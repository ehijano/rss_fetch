<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Feb 2025 02:48:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>plmmr: an R package to fit penalized linear mixed models for genome-wide association data with complex correlation structure</title>
      <link>https://arxiv.org/abs/2502.01577</link>
      <description>arXiv:2502.01577v1 Announce Type: new 
Abstract: Correlation among the observations in high-dimensional regression modeling can be a major source of confounding. We present a new open-source package, plmmr, to implement penalized linear mixed models in R. This R package estimates correlation among observations in high-dimensional data and uses those estimates to improve prediction with the best linear unbiased predictor. The package uses memory-mapping so that genome-scale data can be analyzed on ordinary machines even if the size of data exceeds RAM. We present here the methods, workflow, and file-backing approach upon which plmmr is built, and we demonstrate its computational capabilities with two examples from real GWAS data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01577v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tabitha K. Peter, Anna C. Reisetter, Yujing Lu, Oscar A. Rysavy, Patrick J. Breheny</dc:creator>
    </item>
    <item>
      <title>Decentralized Inference for Distributed Geospatial Data Using Low-Rank Models</title>
      <link>https://arxiv.org/abs/2502.00309</link>
      <description>arXiv:2502.00309v1 Announce Type: cross 
Abstract: Advancements in information technology have enabled the creation of massive spatial datasets, driving the need for scalable and efficient computational methodologies. While offering viable solutions, centralized frameworks are limited by vulnerabilities such as single-point failures and communication bottlenecks. This paper presents a decentralized framework tailored for parameter inference in spatial low-rank models to address these challenges. A key obstacle arises from the spatial dependence among observations, which prevents the log-likelihood from being expressed as a summation-a critical requirement for decentralized optimization approaches. To overcome this challenge, we propose a novel objective function leveraging the evidence lower bound, which facilitates the use of decentralized optimization techniques. Our approach employs a block descent method integrated with multi-consensus and dynamic consensus averaging for effective parameter optimization. We prove the convexity of the new objective function in the vicinity of the true parameters, ensuring the convergence of the proposed method. Additionally, we present the first theoretical results establishing the consistency and asymptotic normality of the estimator within the context of spatial low-rank models. Extensive simulations and real-world data experiments corroborate these theoretical findings, showcasing the robustness and scalability of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00309v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianwei Shi, Sameh Abdulah, Ying Sun, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>Parameter Estimation of State Space Models Using Particle Importance Sampling</title>
      <link>https://arxiv.org/abs/2502.00904</link>
      <description>arXiv:2502.00904v1 Announce Type: cross 
Abstract: State-space models have been used in many applications, including econometrics, engineering, medical research, etc. The maximum likelihood estimation (MLE) of the static parameter of general state-space models is not straightforward because the likelihood function is intractable. It is popular to use the sequential Monte Carlo(SMC) method to perform gradient ascent optimisation in either offline or online fashion. One problem with existing online SMC methods for MLE is that the score estimators are inconsistent, i.e. the bias does not vanish with increasing particle size. In this paper, two SMC algorithms are proposed based on an importance sampling weight function to use each set of generated particles more efficiently. The first one is an offline algorithm that locally approximates the likelihood function using importance sampling, where the locality is adapted by the effective sample size (ESS). The second one is a semi-online algorithm that has a computational cost linear in the particle size and uses score estimators that are consistent. We study its consistency and asymptotic normality. Their computational superiority is illustrated in numerical studies for long time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00904v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxiong Gao, Wentao Li, Rong Chen</dc:creator>
    </item>
    <item>
      <title>Standardized Measurement Approach (SMA) vs Advanced Measurement Approaches (AMA): A Critical Review of Approaches in Operational Risk</title>
      <link>https://arxiv.org/abs/2502.00962</link>
      <description>arXiv:2502.00962v1 Announce Type: cross 
Abstract: The Basel Committee on Banking Supervision proposed replacing all approaches for operational risk capital, including the Advanced Measurement Approach (AMA), with a simplified formula called the Standardized Measurement Approach (SMA). This paper examines and criticizes the weaknesses and failures of SMA, such as instability, insensitivity to risk, superadditivity, and the implicit relationship between the SMA capital model and systemic risk in the banking sector. Furthermore, it discusses the issues of the proposed Operational Risk Capital (OpCar) model by the Basel Committee, a precursor to SMA. The paper concludes by advocating for the maintenance of the AMA internal model framework and suggests a series of standardization recommendations to unify internal operational risk modeling. The findings and viewpoints presented in this paper have been discussed and supported by numerous operational risk professionals and academics from various regions of the world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00962v1</guid>
      <category>q-fin.RM</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Omar Briceno Cruzado</dc:creator>
    </item>
    <item>
      <title>Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models using Markov Chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2403.18072</link>
      <description>arXiv:2403.18072v2 Announce Type: replace 
Abstract: Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the design space using Bayesian optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED method, and illustrate its differences versus conventional non-GO-OED, through various test problems and an application of sensor placement for source inversion in a convection-diffusion field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18072v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Zhong, Wanggang Shen, Tommie Catanach, Xun Huan</dc:creator>
    </item>
    <item>
      <title>Doubly Adaptive Importance Sampling</title>
      <link>https://arxiv.org/abs/2404.18556</link>
      <description>arXiv:2404.18556v2 Announce Type: replace 
Abstract: We propose an adaptive importance sampling scheme for Gaussian approximations of intractable posteriors. Optimization-based approximations like variational inference can be too inaccurate while existing Monte Carlo methods can be too slow. Therefore, we propose a hybrid where, at each iteration, the Monte Carlo effective sample size can be guaranteed at a fixed computational cost by interpolating between natural-gradient variational inference and importance sampling. The amount of damping in the updates adapts to the posterior and guarantees the effective sample size. Gaussianity enables the use of Stein's lemma to obtain gradient-based optimization in the highly damped variational inference regime and a reduction of Monte Carlo error for undamped adaptive importance sampling. The result is a generic, embarrassingly parallel and adaptive posterior approximation method. Numerical studies on simulated and real data show its competitiveness with other, less general methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18556v2</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Willem van den Boom, Andrea Cremaschi, Alexandre H. Thiery</dc:creator>
    </item>
    <item>
      <title>Learning non-Gaussian spatial distributions via Bayesian transport maps with parametric shrinkage</title>
      <link>https://arxiv.org/abs/2409.19208</link>
      <description>arXiv:2409.19208v2 Announce Type: replace 
Abstract: Many applications, including climate-model analysis and stochastic weather generators, require learning or emulating the distribution of a high-dimensional and non-Gaussian spatial field based on relatively few training samples. To address this challenge, a recently proposed Bayesian transport map (BTM) approach consists of a triangular transport map with nonparametric Gaussian-process (GP) components, which is trained to transform the distribution of interest distribution to a Gaussian reference distribution. To improve the performance of this existing BTM, we propose to shrink the map components toward a ``base'' parametric Gaussian family combined with a Vecchia approximation for scalability. The resulting ShrinkTM approach is more accurate than the existing BTM, especially for small numbers of training samples. It can even outperform the ``base'' family when trained on a single sample of the spatial field. We demonstrate the advantage of ShrinkTM though numerical experiments on simulated data and on climate-model output.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19208v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Chakraborty, Matthias Katzfuss</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification in Synthetic Controls with Staggered Treatment Adoption</title>
      <link>https://arxiv.org/abs/2210.05026</link>
      <description>arXiv:2210.05026v5 Announce Type: replace-cross 
Abstract: We propose principled prediction intervals to quantify the uncertainty of a large class of synthetic control predictions (or estimators) in settings with staggered treatment adoption, offering precise non-asymptotic coverage probability guarantees. From a methodological perspective, we provide a detailed discussion of different causal quantities to be predicted, which we call causal predictands, allowing for multiple treated units with treatment adoption at possibly different points in time. From a theoretical perspective, our uncertainty quantification methods improve on prior literature by (i) covering a large class of causal predictands in staggered adoption settings, (ii) allowing for synthetic control methods with possibly nonlinear constraints, (iii) proposing scalable robust conic optimization methods and principled data-driven tuning parameter selection, and (iv) offering valid uniform inference across post-treatment periods. We illustrate our methodology with an empirical application studying the effects of economic liberalization on real GDP per capita for Sub-Saharan African countries. Companion software packages are provided in Python, R, and Stata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.05026v5</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Yingjie Feng, Filippo Palomba, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Exploring Spatial Context: A Comprehensive Bibliography of GWR and MGWR</title>
      <link>https://arxiv.org/abs/2404.16209</link>
      <description>arXiv:2404.16209v3 Announce Type: replace-cross 
Abstract: Local spatial models such as Geographically Weighted Regression (GWR) and Multiscale Geographically Weighted Regression (MGWR) serve as instrumental tools to capture intrinsic contextual effects through the estimates of the local intercepts and behavioral contextual effects through estimates of the local slope parameters. GWR and MGWR provide simple implementation yet powerful frameworks that could be extended to various disciplines that handle spatial data. This bibliography aims to serve as a comprehensive compilation of peer-reviewed papers that have utilized GWR or MGWR as a primary analytical method to conduct spatial analyses and acts as a useful guide to anyone searching the literature for previous examples of local statistical modeling in a wide variety of application fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16209v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Stewart Fotheringham, Chen-Lun Kao, Hanchen Yu, Sarah Bardin, Taylor Oshan, Ziqi Li, Mehak Sachdeva, Wei Luo</dc:creator>
    </item>
    <item>
      <title>tidychangepoint: a unified framework for analyzing changepoint detection in univariate time series</title>
      <link>https://arxiv.org/abs/2407.14369</link>
      <description>arXiv:2407.14369v2 Announce Type: replace-cross 
Abstract: We present tidychangepoint, a new R package for changepoint detection analysis. Most R packages for segmenting univariate time series focus on providing one or two algorithms for changepoint detection that work with a small set of models and penalized objective functions, and all of them return a custom, nonstandard object type. This makes comparing results across various algorithms, models, and penalized objective functions unnecessarily difficult. tidychangepoint solves this problem by wrapping functions from a variety of existing packages and storing the results in a common S3 class called tidycpt. The package then provides functionality for easily extracting comparable numeric or graphical information from a tidycpt object, all in a tidyverse-compliant framework. tidychangepoint is versatile: it supports both deterministic algorithms like PELT (from changepoint), and also flexible, randomized, genetic algorithms (via GA) that -- via new functionality built into tidychangepoint -- can be used with any compliant model-fitting function and any penalized objective function. By bringing all of these disparate tools together in a cohesive fashion, tidychangepoint facilitates comparative analysis of changepoint detection algorithms and models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14369v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin S. Baumer, Biviana Marcela Suarez Sierra</dc:creator>
    </item>
    <item>
      <title>Reliable Pseudo-labeling via Optimal Transport with Attention for Short Text Clustering</title>
      <link>https://arxiv.org/abs/2501.15194</link>
      <description>arXiv:2501.15194v3 Announce Type: replace-cross 
Abstract: Short text clustering has gained significant attention in the data mining community. However, the limited valuable information contained in short texts often leads to low-discriminative representations, increasing the difficulty of clustering. This paper proposes a novel short text clustering framework, called Reliable \textbf{P}seudo-labeling via \textbf{O}ptimal \textbf{T}ransport with \textbf{A}ttention for Short Text Clustering (\textbf{POTA}), that generate reliable pseudo-labels to aid discriminative representation learning for clustering. Specially, \textbf{POTA} first implements an instance-level attention mechanism to capture the semantic relationships among samples, which are then incorporated as a semantic consistency regularization term into an optimal transport problem. By solving this OT problem, we can yield reliable pseudo-labels that simultaneously account for sample-to-sample semantic consistency and sample-to-cluster global structure information. Additionally, the proposed OT can adaptively estimate cluster distributions, making \textbf{POTA} well-suited for varying degrees of imbalanced datasets. Then, we utilize the pseudo-labels to guide contrastive learning to generate discriminative representations and achieve efficient clustering. Extensive experiments demonstrate \textbf{POTA} outperforms state-of-the-art methods. The code is available at: \href{https://github.com/YZH0905/POTA-STC/tree/main}{https://github.com/YZH0905/POTA-STC/tree/main}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15194v3</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihao Yao, Jixuan Yin, Bo Li</dc:creator>
    </item>
  </channel>
</rss>
