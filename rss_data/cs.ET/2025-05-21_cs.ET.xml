<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 May 2025 01:49:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer</title>
      <link>https://arxiv.org/abs/2505.14303</link>
      <description>arXiv:2505.14303v1 Announce Type: new 
Abstract: Using Resistive Random Access Memory (RRAM) crossbars in Computing-in-Memory (CIM) architectures offers a promising solution to overcome the von Neumann bottleneck. Due to non-idealities like cell variability, RRAM crossbars are often operated in binary mode, utilizing only two states: Low Resistive State (LRS) and High Resistive State (HRS). Binary Neural Networks (BNNs) and Ternary Neural Networks (TNNs) are well-suited for this hardware due to their efficient mapping. Existing software projects for RRAM-based CIM typically focus on only one aspect: compilation, simulation, or Design Space Exploration (DSE). Moreover, they often rely on classical 8 bit quantization. To address these limitations, we introduce CIM-Explorer, a modular toolkit for optimizing BNN and TNN inference on RRAM crossbars. CIM-Explorer includes an end-to-end compiler stack, multiple mapping options, and simulators, enabling a DSE flow for accuracy estimation across different crossbar parameters and mappings. CIM-Explorer can accompany the entire design process, from early accuracy estimation for specific crossbar parameters, to selecting an appropriate mapping, and compiling BNNs and TNNs for a finalized crossbar chip. In DSE case studies, we demonstrate the expected accuracy for various mappings and crossbar parameters. CIM-Explorer can be found on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14303v1</guid>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rebecca Pelke, Jos\'e Cubero-Cascante, Nils Bosbach, Niklas Degener, Florian Idrizi, Lennart M. Reimann, Jan Moritz Joseph, Rainer Leupers</dc:creator>
    </item>
    <item>
      <title>Pel, A Programming Language for Orchestrating AI Agents</title>
      <link>https://arxiv.org/abs/2505.13453</link>
      <description>arXiv:2505.13453v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) has opened new frontiers in computing, yet controlling and orchestrating their capabilities beyond simple text generation remains a challenge. Current methods, such as function/tool calling and direct code generation, suffer from limitations in expressiveness, scalability, cost, security, and the ability to enforce fine-grained control. This paper introduces Pel, a novel programming language specifically designed to bridge this gap. Inspired by the strengths of Lisp, Elixir, Gleam, and Haskell, Pel provides a syntactically simple, homoiconic, and semantically rich platform for LLMs to express complex actions, control flow, and inter-agent communication safely and efficiently. Pel's design emphasizes a minimal, easily modifiable grammar suitable for constrained LLM generation, eliminating the need for complex sandboxing by enabling capability control at the syntax level. Key features include a powerful piping mechanism for linear composition, first-class closures enabling easy partial application and functional patterns, built-in support for natural language conditions evaluated by LLMs, and an advanced Read-Eval-Print-Loop (REPeL) with Common Lisp-style restarts and LLM-powered helper agents for automated error correction. Furthermore, Pel incorporates automatic parallelization of independent operations via static dependency analysis, crucial for performant agentic systems. We argue that Pel offers a more robust, secure, and expressive paradigm for LLM orchestration, paving the way for more sophisticated and reliable AI agentic frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13453v1</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Behnam Mohammadi</dc:creator>
    </item>
    <item>
      <title>Learning to Program Quantum Measurements for Machine Learning</title>
      <link>https://arxiv.org/abs/2505.13525</link>
      <description>arXiv:2505.13525v1 Announce Type: cross 
Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML) have sparked significant interest, driving extensive exploration of quantum machine learning (QML) algorithms to address a wide range of complex challenges. The development of high-performance QML models requires expert-level expertise, presenting a key challenge to the widespread adoption of QML. Critical obstacles include the design of effective data encoding strategies and parameterized quantum circuits, both of which are vital for the performance of QML models. Furthermore, the measurement process is often neglected-most existing QML models employ predefined measurement schemes that may not align with the specific requirements of the targeted problem. We propose an innovative framework that renders the observable of a quantum system-specifically, the Hermitian matrix-trainable. This approach employs an end-to-end differentiable learning framework, enabling simultaneous optimization of the neural network used to program the parameterized observables and the standard quantum circuit parameters. Notably, the quantum observable parameters are dynamically programmed by the neural network, allowing the observables to adapt in real time based on the input data stream. Through numerical simulations, we demonstrate that the proposed method effectively programs observables dynamically within variational quantum circuits, achieving superior results compared to existing approaches. Notably, it delivers enhanced performance metrics, such as higher classification accuracy, thereby significantly improving the overall effectiveness of QML models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13525v1</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samual Yen-Chi Chen, Huan-Hsin Tseng, Hsin-Yi Lin, Shinjae Yoo</dc:creator>
    </item>
    <item>
      <title>6G communications through sub-Terahertz CMOS power amplifiers: Design challenges and trends</title>
      <link>https://arxiv.org/abs/2505.13801</link>
      <description>arXiv:2505.13801v1 Announce Type: cross 
Abstract: The fifth-generation (5G) network faces limitations in supporting emerging applications, such as artificial intelligence (AI), virtual reality (VR) and digital twins. To overcome these confines, sub-Terahertz (sub-THz) and Terahertz (THz) technologies are considered to be key enablers of effective 6G wireless communications, offering higher transmission speeds, longer range and wider bandwidth. Achieving these capabilities requires careful engineering of 6G transceivers, with a focus on efficient power amplifiers (PAs) in the front-end, which play a critical role in effectively amplifying and transmitting signals over long distances. Complimentary metal-oxidesemiconductor (CMOS) technology-based PA in sub-THz suffers severe parasitic and limited maximum frequency, however, this has eventually been solved by different design architectures and scaling down of CMOS technology to break through the frequency limitations. In this article, we reviewed the potentials and capabilities of CMOS technology for designing 6G hardware, identified the state-of-art PA designs in the sub-THz band and then examined as well as compared the designs to identify the suitable design strategies for better performance. The circuit optimisation techniques, such as coupled-line, passive gain boosting method, zero-degree power splitting, load-pull matching, diode and capacitor linearisation for better gain, saturated output power and power added efficiency, are considered for the PA design architectures at different sub-THz bands. Furthermore, these methods are summarised and discussed with their advantages and disadvantages in lieu with their performances. The PA design trends, challenges and future perspectives are also presented and discussed. Therefore, this comprehensive review article will serve as a comparative study and reference for future PA designs for radio frequency integrated circuits (RFIC).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13801v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.heliyon.2025.e43393</arxiv:DOI>
      <arxiv:journal_reference>Heliyon, vol. 11, no. 11, May 2025</arxiv:journal_reference>
      <dc:creator>Jun Yan Lee, Duo Wu, Xuanrui Guo, Jian Ding Tan, Teh Jia Yew, Zi Neng Ng, Mohammad Arif Sobhan Bhuiyan, Mahdi H. Miraz</dc:creator>
    </item>
    <item>
      <title>Upgrading Democracies with Fairer Voting Methods</title>
      <link>https://arxiv.org/abs/2505.14349</link>
      <description>arXiv:2505.14349v1 Announce Type: cross 
Abstract: Voting methods are instrumental design element of democracies. Citizens use them to express and aggregate their preferences to reach a collective decision. However, voting outcomes can be as sensitive to voting rules as they are to people's voting choices. Despite the significance and inter-disciplinary scientific progress on voting methods, several democracies keep relying on outdated voting methods that do not fit modern, pluralistic societies well, while lacking social innovation. Here, we demonstrate how one can upgrade real-world democracies, namely by using alternative preferential voting methods such as cumulative voting and the method of equal shares designed for a proportional representation of voters' preferences. By rigorously assessing a new participatory budgeting approach applied in the city of Aarau, Switzerland, we unravel the striking voting outcomes of fair voting methods: more winning projects with the same budget and broader geographic and preference representation of citizens by the elected projects, in particular for voters who used to be under-represented, while promoting novel project ideas. We provide profound causal evidence showing that citizens prefer proportional voting methods, which possess strong legitimacy without the need of very technical specialized explanations. We also reveal strong underlying democratic values exhibited by citizens who support fair voting methods such as altruism and compromise. These findings come with a global momentum to unleash a new and long-awaited participation blueprint of how to upgrade democracies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14349v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Evangelos Pournaras, Srijoni Majumdar, Thomas Wellings, Joshua C. Yang, Fatemeh B. Heravan, Regula H\"anggli Fricker, Dirk Helbing</dc:creator>
    </item>
    <item>
      <title>Design and Evaluation of a Microservices Cloud Framework for Online Travel Platforms</title>
      <link>https://arxiv.org/abs/2505.14508</link>
      <description>arXiv:2505.14508v1 Announce Type: cross 
Abstract: Handling online travel agents globally requires efficient and flexible software solution architectures. When it needs to handle thousands of agents and billions of clients data globally. Microservices architecture is used to break down a large program into numerous, smaller services which can run individually and perform individual tasks. This paper analyses and integrates a unique Microservices Cloud Framework designed to support Online Travel Platforms (MCF-OTP). MCF-OTPs main goal is to increase the performance, flexibility, and maintenance of online travel platforms via cloud computing and microservice technologies. Large-scale travel apps, including managing numerous data sources, dealing with traffic peaks, and providing fault tolerance, can be addressed by the suggested framework. The framework increases good interpretation between flawless data synchronization, microservices, and dynamic scaling based on demand technology. An organization framework that optimizes service borders and minimizes inter-service dependencies is recommended. Thus, this can result in elevated development adaptability. In this research, the principal goal is to evaluate MCF-OTPs efficiency using the indicators of fault tolerance and response time. It is indicated by the findings that the MCF-OTP structure excels traditional monolithic designs in terms of dependability and scalability, managing traffic spikes seamlessly and decreasing downtime. The cost-effective analysis helps ascertain the net gain attained by the startup fees and the ongoing operational costs. The cloud-based environment is used to reduce the fracture cost which also helps to increase the efficiency of resource allocation, according to the research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14508v1</guid>
      <category>cs.SE</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <category>cs.PL</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biman Barua, M. Shamim Kaiser</dc:creator>
    </item>
    <item>
      <title>Towards Verifiability of Total Value Locked (TVL) in Decentralized Finance</title>
      <link>https://arxiv.org/abs/2505.14565</link>
      <description>arXiv:2505.14565v1 Announce Type: cross 
Abstract: Total Value Locked (TVL) aims to measure the aggregate value of cryptoassets deposited in Decentralized Finance (DeFi) protocols. Although blockchain data is public, the way TVL is computed is not well understood. In practice, its calculation on major TVL aggregators relies on self-reports from community members and lacks standardization, making it difficult to verify published figures independently. We thus conduct a systematic study on 939 DeFi projects deployed in Ethereum. We study the methodologies used to compute TVL, examine factors hindering verifiability, and ultimately propose standardization attempts in the field. We find that 10.5% of the protocols rely on external servers; 68 methods alternative to standard balance queries exist, although their use decreased over time; and 240 equal balance queries are repeated on multiple protocols. These findings indicate limits to verifiability and transparency. We thus introduce ``verifiable Total Value Locked'' (vTVL), a metric measuring the TVL that can be verified relying solely on on-chain data and standard balance queries. A case study on 400 protocols shows that our estimations align with published figures for 46.5% of protocols. Informed by these findings, we discuss design guidelines that could facilitate a more verifiable, standardized, and explainable TVL computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14565v1</guid>
      <category>q-fin.GN</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pietro Saggese, Michael Fr\"owis, Stefan Kitzler, Bernhard Haslhofer, Raphael Auer</dc:creator>
    </item>
    <item>
      <title>Federated Hybrid Model Pruning through Loss Landscape Exploration</title>
      <link>https://arxiv.org/abs/2405.10271</link>
      <description>arXiv:2405.10271v3 Announce Type: replace-cross 
Abstract: As the era of connectivity and unprecedented data generation expands, collaborative intelligence emerges as a key driver for machine learning, encouraging global-scale model development. Federated learning (FL) stands at the heart of this transformation, enabling distributed systems to work collectively on complex tasks while respecting strict constraints on privacy and security. Despite its vast potential, specially in the age of complex models, FL encounters challenges such as elevated communication costs, computational constraints, and the heterogeneous data distributions. In this context, we present AutoFLIP, a novel framework that optimizes FL through an adaptive hybrid pruning approach, grounded in a federated loss exploration phase. By jointly analyzing diverse non-IID client loss landscapes, AutoFLIP efficiently identifies model substructures for pruning both at structured and unstructured levels. This targeted optimization fosters a symbiotic intelligence loop, reducing computational burdens and boosting model performance on resource-limited devices for a more inclusive and democratized model usage. Our extensive experiments across multiple datasets and FL tasks show that AutoFLIP delivers quantifiable benefits: a 48.8% reduction in computational overhead, a 35.5% decrease in communication costs, and a notable improvement in global accuracy. By significantly reducing these overheads, AutoFLIP offer the way for efficient FL deployment in real-world applications for a scalable and broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10271v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Intern\`o, Elena Raponi, Niki van Stein, Thomas B\"ack, Markus Olhofer, Yaochu Jin, Barbara Hammer</dc:creator>
    </item>
    <item>
      <title>Efficient Spatio-Temporal Signal Recognition on Edge Devices Using PointLCA-Net</title>
      <link>https://arxiv.org/abs/2411.14585</link>
      <description>arXiv:2411.14585v3 Announce Type: replace-cross 
Abstract: Recent advancements in machine learning, particularly through deep learning architectures like PointNet, have transformed the processing of three-dimensional (3D) point clouds, significantly improving 3D object classification and segmentation tasks. While 3D point clouds provide detailed spatial information, spatio-temporal signals introduce a dynamic element that accounts for changes over time. However, applying deep learning techniques to spatio-temporal signals and deploying them on edge devices presents challenges, including real-time processing, memory capacity, and power consumption. To address these issues, this paper presents a novel approach that combines PointNet's feature extraction with the in-memory computing capabilities and energy efficiency of neuromorphic systems for spatio-temporal signal recognition. The proposed method consists of a two-stage process: in the first stage, PointNet extracts features from the spatio-temporal signals, which are then stored in non-volatile memristor crossbar arrays. In the second stage, these features are processed by a single-layer spiking neural encoder-decoder that employs the Locally Competitive Algorithm (LCA) for efficient encoding and classification. This work integrates the strengths of both PointNet and LCA, enhancing computational efficiency and energy performance on edge devices. PointLCA-Net achieves high recognition accuracy for spatio-temporal data with substantially lower energy burden during both inference and training than comparable approaches, thus advancing the deployment of advanced neural architectures in energy-constrained environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14585v3</guid>
      <category>cs.LG</category>
      <category>cs.ET</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sanaz Mahmoodi Takaghaj</dc:creator>
    </item>
    <item>
      <title>Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems</title>
      <link>https://arxiv.org/abs/2504.20660</link>
      <description>arXiv:2504.20660v2 Announce Type: replace-cross 
Abstract: In this paper, a novel quantum classical hybrid framework is proposed that synergizes quantum with Classical Reinforcement Learning. By leveraging the inherent parallelism of quantum computing, the proposed approach generates robust Q tables and specialized turn cost estimations, which are then integrated with a classical Reinforcement Learning pipeline. The Classical Quantum fusion results in rapid convergence of training, reducing the training time significantly and improved adaptability in scenarios featuring static, dynamic, and moving obstacles. Simulator based evaluations demonstrate significant enhancements in path efficiency, trajectory smoothness, and mission success rates, underscoring the potential of framework for real time, autonomous navigation in complex and unpredictable environments. Furthermore, the proposed framework was tested beyond simulations on practical scenarios, including real world map data such as the IIT Delhi campus, reinforcing its potential for real time, autonomous navigation in complex and unpredictable environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20660v2</guid>
      <category>cs.LG</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sahil Tomar, Shamshe Alam, Sandeep Kumar, Amit Mathur</dc:creator>
    </item>
  </channel>
</rss>
