<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 05:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Mixture models for data with unknown distributions</title>
      <link>https://arxiv.org/abs/2502.19605</link>
      <description>arXiv:2502.19605v1 Announce Type: new 
Abstract: We describe and analyze a broad class of mixture models for real-valued multivariate data in which the probability density of observations within each component of the model is represented as an arbitrary combination of basis functions. Fits to these models give us a way to cluster data with distributions of unknown form, including strongly non-Gaussian or multimodal distributions, and return both a division of the data and an estimate of the distributions, effectively performing clustering and density estimation within each cluster at the same time. We describe two fitting methods, one using an expectation-maximization (EM) algorithm and the other a Bayesian non-parametric method using a collapsed Gibbs sampler. The former is numerically efficient, but gives only point estimates of the probability densities. The latter is more computationally demanding but returns a full Bayesian posterior and also an estimate of the number of components. We demonstrate our methods with a selection of illustrative applications and give code implementing both algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19605v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. E. J. Newman</dc:creator>
    </item>
    <item>
      <title>A Conformalized Empirical Bayes Method for Multiple Testing with Side Information</title>
      <link>https://arxiv.org/abs/2502.19667</link>
      <description>arXiv:2502.19667v1 Announce Type: new 
Abstract: This article presents a Conformalized Locally Adaptive Weighting (CLAW) approach to multiple testing with side information. The proposed method employs innovative data-driven strategies to construct pairwise exchangeable scores, which are integrated into a generic algorithm that leverages a mirror process for controlling the false discovery rate (FDR). By combining principles from empirical Bayes with powerful techniques in conformal inference, CLAW provides a valid and efficient framework for incorporating structural information from both test data and auxiliary covariates. Unlike existing empirical Bayes FDR methods that primarily offer asymptotic validity, often under strong regularity conditions, CLAW controls the FDR in finite samples under weaker conditions. Extensive numerical studies using both simulated and real data demonstrate that CLAW exhibits superior performance compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19667v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zinan Zhao, Wenguang Sun</dc:creator>
    </item>
    <item>
      <title>Modeling Extreme Events in the Presence of Inlier: A Mixture Approach</title>
      <link>https://arxiv.org/abs/2502.19793</link>
      <description>arXiv:2502.19793v1 Announce Type: new 
Abstract: In many random phenomena, such as life-testing experiments and environmental data (like rainfall data), there are often positive values and an excess of zeros, which create modeling challenges. In life testing, immediate failures result in zero lifetimes, often due to defects or poor quality, especially in electronics and clinical trials. These failures, called zero inliers, are difficult to model using standard approaches. When studying extreme values in the above scenarios, a key issue is selecting an appropriate threshold for accurate tail approximation of the population using asymptotic models. While some extreme value mixture models address threshold estimation and tail approximation, conventional parametric and non-parametric bulk and generalised Pareto distribution (GPD) approaches often neglect inliers, leading to suboptimal results. This paper introduces a framework for modeling extreme events and inliers using the GPD, addressing threshold uncertainty and effectively capturing inliers at zero. The model's parameters are estimated using the maximum likelihood estimation (MLE) method, ensuring optimal precision. Through simulation studies and real-world applications, we demonstrate that the proposed model significantly outperforms the traditional methods, which typically neglect inliers at the origin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19793v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivshankar Nila, Ishapathik Das, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>A Principled Approach to Bayesian Transfer Learning</title>
      <link>https://arxiv.org/abs/2502.19796</link>
      <description>arXiv:2502.19796v1 Announce Type: new 
Abstract: Updating $\textit{a priori}$ information given some observed data is the core tenet of Bayesian inference. Bayesian transfer learning extends this idea by incorporating information from a related dataset to improve the inference on the observed data which may have been collected under slightly different settings. The use of related information can be useful when the observed data is scarce, for example. Current Bayesian transfer learning methods that are based on the so-called $\textit{power prior}$ can adaptively transfer information from related data. Unfortunately, it is not always clear under which scenario Bayesian transfer learning performs best or even if it will improve Bayesian inference. Additionally, current power prior methods rely on conjugacy to evaluate the posterior of interest. We propose using leave-one-out cross validation on the target dataset as a means of evaluating Bayesian transfer learning methods. Further, we introduce a new framework, $\textit{transfer sequential Monte Carlo}$, for power prior approaches that efficiently chooses the transfer parameter while avoiding the need for conjugate priors. We assess the performance of our proposed methods in two comprehensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19796v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Bretherton, Joshua J. Bon, David J. Warne, Kerrie Mengersen, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Fast Variational Boosting for Latent Variable Models</title>
      <link>https://arxiv.org/abs/2502.19839</link>
      <description>arXiv:2502.19839v1 Announce Type: new 
Abstract: We consider the problem of estimating complex statistical latent variable models using variational Bayes methods. These methods are used when exact posterior inference is either infeasible or computationally expensive, and they approximate the posterior density with a family of tractable distributions. The parameters of the approximating distribution are estimated using optimisation methods. This article develops a flexible Gaussian mixture variational approximation, where we impose sparsity in the precision matrix of each Gaussian component to reflect the appropriate conditional independence structure in the model. By introducing sparsity in the precision matrix and parameterising it using the Cholesky factor, each Gaussian mixture component becomes parsimonious (with a reduced number of non-zero parameters), while still capturing the dependence in the posterior distribution. Fast estimation methods based on global and local variational boosting moves combined with natural gradients and variance reduction methods are developed. The local boosting moves adjust an existing mixture component, and optimisation is only carried out on a subset of the variational parameters of a new component. The subset is chosen to target improvement of the current approximation in aspects where it is poor. The local boosting moves are fast because only a small number of variational parameters need to be optimised. The efficacy of the approach is illustrated by using simulated and real datasets to estimate generalised linear mixed models and state space models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19839v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Gunawan, David Nott, Robert Kohn</dc:creator>
    </item>
    <item>
      <title>Can a calibration metric be both testable and actionable?</title>
      <link>https://arxiv.org/abs/2502.19851</link>
      <description>arXiv:2502.19851v1 Announce Type: new 
Abstract: Forecast probabilities often serve as critical inputs for binary decision making. In such settings, calibration$\unicode{x2014}$ensuring forecasted probabilities match empirical frequencies$\unicode{x2014}$is essential. Although the common notion of Expected Calibration Error (ECE) provides actionable insights for decision making, it is not testable: it cannot be empirically estimated in many practical cases. Conversely, the recently proposed Distance from Calibration (dCE) is testable but is not actionable since it lacks decision-theoretic guarantees needed for high-stakes applications. We introduce Cutoff Calibration Error, a calibration measure that bridges this gap by assessing calibration over intervals of forecasted probabilities. We show that Cutoff Calibration Error is both testable and actionable and examine its implications for popular post-hoc calibration methods, such as isotonic regression and Platt scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19851v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raphael Rossellini, Jake A. Soloff, Rina Foygel Barber, Zhimei Ren, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>Estimating sample size in dental research</title>
      <link>https://arxiv.org/abs/2502.20009</link>
      <description>arXiv:2502.20009v1 Announce Type: new 
Abstract: Determination of sample size is critical, however not easy to do. Sample size defined as the number of observations in a sample should be big enough to have a high likelihood of detecting a true difference between groups. Practical procedure for determining sample size, using G*power and previous dental articles, is shown in this study. Examples involving independent t-test, paired t-test, one-way analysis of variance(ANOVA), and one-way repeated-measures(RM) ANOVA are used. The purpose of this study is to enable researchers with non-statistical backgrounds to use in practice freely available statistical software G*power to determine sample size and power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20009v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoi-Jeong Lim</dc:creator>
    </item>
    <item>
      <title>Step-by-Step Guide to Conducting Meta-Analysis of Dichotomous Outcomes Using RevMan in Dental Research Step-by-Step Guide to Conducting Meta-Analysis of Dichotomous Outcomes Using RevMan in Dental Research</title>
      <link>https://arxiv.org/abs/2502.20019</link>
      <description>arXiv:2502.20019v1 Announce Type: new 
Abstract: Meta-analysis is a statistical method that combines the results of individual studies on the same topic. This method is becoming popular, due to providing the combined result that individual studies cannot provide and giving a more precise result. Despite meta-analysis having such significance, there are few Korean guides for the use of the Review Manager (RevMan) software. This study will provide a step-by-step guide, using orthodontic mini-screw as a dental example, to help researcher carry out meta-analysis more easily and accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20019v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoi-Jeong Lim, Su-Hyeon Park</dc:creator>
    </item>
    <item>
      <title>Qini curve estimation under clustered network interference</title>
      <link>https://arxiv.org/abs/2502.20097</link>
      <description>arXiv:2502.20097v1 Announce Type: new 
Abstract: Qini curves are a widely used tool for assessing treatment policies under allocation constraints as they visualize the incremental gain of a new treatment policy versus the cost of its implementation. Standard Qini curve estimation assumes no interference between units: that is, that treating one unit does not influence the outcome of any other unit. In many real-life applications such as public policy or marketing, however, the presence of interference is common. Ignoring interference in these scenarios can lead to systematically biased Qini curves that over- or under-estimate a treatment policy's cost-effectiveness. In this paper, we address the problem of Qini curve estimation under clustered network interference, where interfering units form independent clusters. We propose a formal description of the problem setting with an experimental study design under which we can account for clustered network interference. Within this framework, we introduce three different estimation strategies suited for different conditions. Moreover, we introduce a marketplace simulator that emulates clustered network interference in a typical e-commerce setting. From both theoretical and empirical insights, we provide recommendations in choosing the best estimation strategy by identifying an inherent bias-variance trade-off among the estimation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20097v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rickard K. A. Karlsson, Bram van den Akker, Felipe Moraes, Hugo M. Proen\c{c}a, Jesse H. Krijthe</dc:creator>
    </item>
    <item>
      <title>Wavelet-based estimation of long-memory parameter in stochastic volatility models using a robust log-periodogram</title>
      <link>https://arxiv.org/abs/2502.20101</link>
      <description>arXiv:2502.20101v1 Announce Type: new 
Abstract: In this paper, we propose a novel method for estimating the long-memory parameter in time series. By combining the multi-resolution framework of wavelets with the robustness of the Least Absolute Deviations (LAD) criterion, we introduce a periodogram providing a robust alternative to classical methods in the presence of non-Gaussian noise. Incorporating this periodogram into a log-periodogram regression, we develop a new estimator. Simulation studies demonstrate that our estimator outperforms the Geweke and Porter-Hudak (GPH) and Wavelet-Based Log-Periodogram (WBLP) estimators, particularly in terms of mean squared error, across various sample sizes and parameter configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20101v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manganaw N'Daam, Tchilabalo Abozou Kpanzou, Edoh Katchekpele</dc:creator>
    </item>
    <item>
      <title>From the marginal likelihood of a two-way table to ecological inference</title>
      <link>https://arxiv.org/abs/2502.20177</link>
      <description>arXiv:2502.20177v1 Announce Type: new 
Abstract: The paper extends in two directions the work of \cite{Plackett77} who studied how, in a $2\times 2$ table, the likelihood of the column totals depends on the odds ratio. First, we study the marginal likelihood of a single $R\times C$ frequency table when only the marginal frequencies are observed and then consider a collection of, say, $s$ $R\times C$ tables, where only the row and column totals can be observed, which is the basic framework which in applications of Ecological Inference. In the simpler context, we derive the likelihood equations and show that the likelihood has a collection of local maxima which, after a suitable rearrangement of the row and column categories, exhibit the strongest positive association compatible with the marginals, a kind of paradox, considering that the available data are so poor. Next, we derive the likelihood equations for the marginal likelihood of a collection of tow-way tables, under the assumption that they share the same row conditional distributions and derive a necessary condition for the information matrix to be well defined. We also describe a Fisher-scoring algorithm for maximizing the marginal likelihood which, however, can be used only if the number of available replications reaches a given threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20177v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Forcina</dc:creator>
    </item>
    <item>
      <title>Testing Prioritized Composite Endpoint with Multiple Follow-up Time Examinations</title>
      <link>https://arxiv.org/abs/2502.20180</link>
      <description>arXiv:2502.20180v1 Announce Type: new 
Abstract: Composite endpoints are widely used in cardiovascular clinical trials. In recent years, hierarchical composite endpoints-particularly the win ratio approach and its predecessor, the Finkelstein-Schoenfeld (FS) test, also known as the unmatched win ratio test-have gained popularity. These methods involve comparing individuals across multiple endpoints, ranked by priority, with mortality typically assigned the highest priority in many applications. However, these methods have not accounted for varying treatment effects, known as non-constant hazards over time in the context of survival analysis. To address this limitation, we propose an adaptation of the FS test that incorporates progressive follow-up time, which we will refer to as ProFS. This proposed test can jointly evaluate treatment effects at various follow-up time points by incorporating the maximum of several FS test statistics calculated at those specific times. Moreover, ProFS also supports clinical trials with group sequential monitoring strategies, providing flexibility in trial design. As demonstrated through extensive simulations, ProFS offers increased statistical power in scenarios where the treatment effect is mainly in the short term or when the second (non-fatal) layer might be concealed by a lack of effect or weak effect on the top (fatal) layer. We also apply ProFS to the SPRINT clinical trial, illustrating how our proposed method improves the performance of FS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20180v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhan Mou, Haitao Pan, Yu Jiang, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>A New Method for High-Resolution Dating of Radiocarbon Data: The Example of the First Three Centuries B.C</title>
      <link>https://arxiv.org/abs/2502.20211</link>
      <description>arXiv:2502.20211v1 Announce Type: new 
Abstract: Radiocarbon dating poses a challenge in many archaeological contexts due to the limited precision of conventional calibration methods. In this study, we introduce a novel approach to fine-dating that is based on the repeated application of OxCal's R_Simulate function. By constructing extensive reference tables and aggregating measures of central tendency (means and medians), uncalibrated 14C measurements are directly mapped to calendar dates. The method is validated through comprehensive simulations and comparisons with dendrochronologically dated tree rings. Despite challenges in segments of the calibration curve with low gradients, the approach demonstrates that a significant improvement in dating precision is achievable. Limitations and potential avenues for further methodological optimisation are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20211v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian F\"urst</dc:creator>
    </item>
    <item>
      <title>Generalized Multi-Linear Models for Sufficient Dimension Reduction on Tensor Valued Predictors</title>
      <link>https://arxiv.org/abs/2502.20216</link>
      <description>arXiv:2502.20216v1 Announce Type: new 
Abstract: We consider supervised learning (regression/classification) problems with tensor-valued input. We derive multi-linear sufficient reductions for the regression or classification problem by modeling the conditional distribution of the predictors given the response as a member of the quadratic exponential family. We develop estimation procedures of sufficient reductions for both continuous and binary tensor-valued predictors. We prove the consistency and asymptotic normality of the estimated sufficient reduction using manifold theory. For continuous predictors, the estimation algorithm is highly computationally efficient and is also applicable to situations where the dimension of the reduction exceeds the sample size. We demonstrate the superior performance of our approach in simulations and real-world data examples for both continuous and binary tensor-valued predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20216v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Kapla, Efstathia Bura</dc:creator>
    </item>
    <item>
      <title>Semiparametric Triple Difference Estimators</title>
      <link>https://arxiv.org/abs/2502.19788</link>
      <description>arXiv:2502.19788v1 Announce Type: cross 
Abstract: The triple difference causal inference framework is an extension of the well-known difference-in-differences framework. It relaxes the parallel trends assumption of the difference-in-differences framework through leveraging data from an auxiliary domain. Despite being commonly applied in empirical research, the triple difference framework has received relatively limited attention in the statistics literature. Specifically, investigating the intricacies of identification and the design of robust and efficient estimators for this framework has remained largely unexplored. This work aims to address these gaps in the literature. From the identification standpoint, we present outcome regression and weighting methods to identify the average treatment effect on the treated in both panel data and repeated cross-section settings. For the latter, we relax the commonly made assumption of time-invariant covariates. From the estimation perspective, we consider semiparametric estimators for the triple difference framework in both panel data and repeated cross-sections settings. We demonstrate that our proposed estimators are doubly robust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19788v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sina Akbari, Negar Kiyavash, AmirEmad Ghassami</dc:creator>
    </item>
    <item>
      <title>Shared Stochastic Gaussian Process Latent Variable Models: A Multi-modal Generative Model for Quasar Spectra</title>
      <link>https://arxiv.org/abs/2502.19824</link>
      <description>arXiv:2502.19824v1 Announce Type: cross 
Abstract: This work proposes a scalable probabilistic latent variable model based on Gaussian processes (Lawrence, 2004) in the context of multiple observation spaces. We focus on an application in astrophysics where data sets typically contain both observed spectral features and scientific properties of astrophysical objects such as galaxies or exoplanets. In our application, we study the spectra of very luminous galaxies known as quasars, along with their properties, such as the mass of their central supermassive black hole, accretion rate, and luminosity-resulting in multiple observation spaces. A single data point is then characterized by different classes of observations, each with different likelihoods. Our proposed model extends the baseline stochastic variational Gaussian process latent variable model (GPLVM) introduced by Lalchand et al. (2022) to this setting, proposing a seamless generative model where the quasar spectra and scientific labels can be generated simultaneously using a shared latent space as input to different sets of Gaussian process decoders, one for each observation space. Additionally, this framework enables training in a missing data setting where a large number of dimensions per data point may be unknown or unobserved. We demonstrate high-fidelity reconstructions of the spectra and scientific labels during test-time inference and briefly discuss the scientific interpretations of the results, along with the significance of such a generative model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19824v1</guid>
      <category>astro-ph.GA</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Transactions on Machine Learning Research (TMLR), 2025</arxiv:journal_reference>
      <dc:creator>Vidhi Lalchand, Anna-Christina Eilers</dc:creator>
    </item>
    <item>
      <title>Sanity Checking Causal Representation Learning on a Simple Real-World System</title>
      <link>https://arxiv.org/abs/2502.20099</link>
      <description>arXiv:2502.20099v1 Announce Type: cross 
Abstract: We evaluate methods for causal representation learning (CRL) on a simple, real-world system where these methods are expected to work. The system consists of a controlled optical experiment specifically built for this purpose, which satisfies the core assumptions of CRL and where the underlying causal factors (the inputs to the experiment) are known, providing a ground truth. We select methods representative of different approaches to CRL and find that they all fail to recover the underlying causal factors. To understand the failure modes of the evaluated algorithms, we perform an ablation on the data by substituting the real data-generating process with a simpler synthetic equivalent. The results reveal a reproducibility problem, as most methods already fail on this synthetic ablation despite its simple data-generating process. Additionally, we observe that common assumptions on the mixing function are crucial for the performance of some of the methods but do not hold in the real data. Our efforts highlight the contrast between the theoretical promise of the state of the art and the challenges in its application. We hope the benchmark serves as a simple, real-world sanity check to further develop and validate methodology, bridging the gap towards CRL methods that work in practice. We make all code and datasets publicly available at github.com/simonbing/CRLSanityCheck</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20099v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan L. Gamella, Simon Bing, Jakob Runge</dc:creator>
    </item>
    <item>
      <title>Stein's unbiased risk estimate and Hyv\"arinen's score matching</title>
      <link>https://arxiv.org/abs/2502.20123</link>
      <description>arXiv:2502.20123v1 Announce Type: cross 
Abstract: We study two G-modeling strategies for estimating the signal distribution (the empirical Bayesian's prior) from observations corrupted with normal noise. First, we choose the signal distribution by minimizing Stein's unbiased risk estimate (SURE) of the implied Eddington/Tweedie Bayes denoiser, an approach motivated by optimal empirical Bayesian shrinkage estimation of the signals. Second, we select the signal distribution by minimizing Hyv\"arinen's score matching objective for the implied score (derivative of log-marginal density), targeting minimal Fisher divergence between estimated and true marginal densities. While these strategies appear distinct, they are known to be mathematically equivalent. We provide a unified analysis of SURE and score matching under both well-specified signal distribution classes and misspecification. In the classical well-specified setting with homoscedastic noise and compactly supported signal distribution, we establish nearly parametric rates of convergence of the empirical Bayes regret and the Fisher divergence. In a commonly studied misspecified model, we establish fast rates of convergence to the oracle denoiser and corresponding oracle inequalities. Our empirical results demonstrate competitiveness with nonparametric maximum likelihood in well-specified settings, while showing superior performance under misspecification, particularly in settings involving heteroscedasticity and side information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20123v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sulagna Ghosh, Nikolaos Ignatiadis, Frederic Koehler, Amber Lee</dc:creator>
    </item>
    <item>
      <title>Multiple Linked Tensor Factorization</title>
      <link>https://arxiv.org/abs/2502.20286</link>
      <description>arXiv:2502.20286v1 Announce Type: cross 
Abstract: In biomedical research and other fields, it is now common to generate high content data that are both multi-source and multi-way. Multi-source data are collected from different high-throughput technologies while multi-way data are collected over multiple dimensions, yielding multiple tensor arrays. Integrative analysis of these data sets is needed, e.g., to capture and synthesize different facets of complex biological systems. However, despite growing interest in multi-source and multi-way factorization techniques, methods that can handle data that are both multi-source and multi-way are limited. In this work, we propose a Multiple Linked Tensors Factorization (MULTIFAC) method extending the CANDECOMP/PARAFAC (CP) decomposition to simultaneously reduce the dimension of multiple multi-way arrays and approximate underlying signal. We first introduce a version of the CP factorization with L2 penalties on the latent factors, leading to rank sparsity. When extended to multiple linked tensors, the method automatically reveals latent components that are shared across data sources or individual to each data source. We also extend the decomposition algorithm to its expectation-maximization (EM) version to handle incomplete data with imputation. Extensive simulation studies are conducted to demonstrate MULTIFAC's ability to (i) approximate underlying signal, (ii) identify shared and unshared structures, and (iii) impute missing data. The approach yields an interpretable decomposition on multi-way multi-omics data for a study on early-life iron deficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20286v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyu Kang, Raghavendra B. Rao, Eric F. Lock</dc:creator>
    </item>
    <item>
      <title>Bayesian iterative screening in ultra-high dimensional linear regressions</title>
      <link>https://arxiv.org/abs/2107.10175</link>
      <description>arXiv:2107.10175v2 Announce Type: replace 
Abstract: Variable selection in ultra-high dimensional linear regression is often preceded by a screening step to significantly reduce the dimension. Here we develop a Bayesian variable screening method (BITS) guided by the posterior model probabilities. BITS can successfully integrate prior knowledge, if any, on effect sizes, and the number of true variables. BITS iteratively includes potential variables with the highest posterior probability accounting for the already selected variables. It is implemented by a fast Cholesky update algorithm and is shown to have the screening consistency property. BITS is built based on a model with Gaussian errors, yet, the screening consistency is proved to hold under more general tail conditions. The notion of posterior screening consistency allows the resulting model to provide a good starting point for further Bayesian variable selection methods. A new screening consistent stopping rule based on posterior probability is developed. Simulation studies and real data examples are used to demonstrate scalability and fine screening performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.10175v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Run Wang, Somak Dutta, Vivekananda Roy</dc:creator>
    </item>
    <item>
      <title>Joint Mean and Correlation Regression Models for Multivariate Data</title>
      <link>https://arxiv.org/abs/2402.12803</link>
      <description>arXiv:2402.12803v2 Announce Type: replace 
Abstract: We propose a joint mean and correlation regression model for multivariate discrete and (semi-)continuous response data, that simultaneously regresses the mean of each response against a set of covariates, and the correlations between responses against a set of similarity/distance measures. A set of joint estimating equations are formulated to construct an estimator of both the mean regression coefficients and the correlation regression parameters. Under a general setting where the number of responses can tend to infinity, the joint estimator is demonstrated to be consistent and asymptotically normally distributed, with differing rates of convergence due to the mean regression coefficients being heterogeneous across responses. An iterative estimation procedure is developed to obtain parameter estimates in the required (constrained) parameter space. Simulations demonstrate the strong finite sample performance of the proposed estimator in terms of point estimation and inference. We apply the proposed model to a count dataset of 38 Carabidae ground beetle species sampled throughout Scotland, along with information about the environmental conditions of each site and the traits of each species. Results show the relationship between mean abundance and environmental covariates differs across the beetle species, and that beetle total length is important in driving the correlations between species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12803v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Yang Tho, Francis K. C. Hui, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Distribution-Free Online Change Detection for Low-Rank Images</title>
      <link>https://arxiv.org/abs/2406.16136</link>
      <description>arXiv:2406.16136v2 Announce Type: replace 
Abstract: We present a distribution-free CUSUM procedure designed for online change detection in a time series of low-rank images, particularly when the change causes a mean shift. We represent images as matrix data and allow for temporal dependence, in addition to inherent spatial dependence, before and after the change. The marginal distributions are assumed to be general, not limited to any specific parametric distribution. We propose new monitoring statistics that utilize the low-rank structure of the in-control mean matrix. Additionally, we study the properties of the proposed detection procedure, assessing whether the monitoring statistics effectively capture a mean shift and evaluating the rate of increase in the average run length relative to the control limit in both the in-control and out-of-control cases. The effectiveness of our procedure is demonstrated through simulated and real data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16136v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tingnan Gong, Seong-Hee Kim, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Bayesian High-dimensional Linear Regression with Sparse Projection-posterior</title>
      <link>https://arxiv.org/abs/2410.16577</link>
      <description>arXiv:2410.16577v3 Announce Type: replace 
Abstract: We consider a novel Bayesian approach to estimation, uncertainty quantification, and variable selection for a high-dimensional linear regression model under sparsity. The number of predictors can be nearly exponentially large relative to the sample size. We put a conjugate normal prior initially disregarding sparsity, but for making an inference, instead of the original multivariate normal posterior, we use the posterior distribution induced by a map transforming the vector of regression coefficients to a sparse vector obtained by minimizing the sum of squares of deviations plus a suitably scaled $\ell_1$-penalty on the vector. We show that the resulting sparse projection-posterior distribution contracts around the true value of the parameter at the optimal rate adapted to the sparsity of the vector. We show that the true sparsity structure gets a large sparse projection-posterior probability. We further show that an appropriately recentred credible ball has the correct asymptotic frequentist coverage. Finally, we describe how the computational burden can be distributed to many machines, each dealing with only a small fraction of the whole dataset. We conduct a comprehensive simulation study under a variety of settings and found that the proposed method performs well for finite sample sizes. We also apply the method to several real datasets, including the ADNI data, and compare its performance with the state-of-the-art methods. We implemented the method in the \texttt{R} package called \texttt{sparseProj}, and all computations have been carried out using this package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16577v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, Subhashis Ghoshal</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Valid Randomization Tests for Monotone Spillover Effects</title>
      <link>https://arxiv.org/abs/2501.02454</link>
      <description>arXiv:2501.02454v2 Announce Type: replace 
Abstract: Randomization tests have gained popularity for causal inference under network interference because they are finite-sample valid with minimal assumptions. However, existing procedures are limited as they primarily focus on the existence of spillovers through sharp null hypotheses on potential outcomes. In this paper, we expand the scope of randomization procedures in network settings by developing new tests for the monotonicity of spillover effects. These tests offer insights into whether spillover effects increase, decrease, or exhibit ``diminishing returns'' along certain network dimensions of interest. Our approach partitions the network into multiple (possibly overlapping) parts and tests a monotone contrast hypothesis in each sub-network. The test decisions can then be aggregated in various ways depending on how each test is constructed. We demonstrate our method by re-analyzing a large-scale policing experiment in Colombia, which reveals evidence of monotonicity related to the ``crime displacement hypothesis''. Our analysis suggests that crime spillovers on a control street increase with the number of nearby streets receiving more intense policing but diminish at higher exposure levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02454v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shunzhuang Huang, Xinran Li, Panos Toulis</dc:creator>
    </item>
    <item>
      <title>Statistical inference for interacting innovation processes and related general results</title>
      <link>https://arxiv.org/abs/2501.09648</link>
      <description>arXiv:2501.09648v2 Announce Type: replace 
Abstract: Given the importance of understanding how different innovation processes affect each other, we have introduced a model for a finite system of interacting innovation processes. The present work focuses on the second-order asymptotic properties of the model and illustrates how to leverage the theoretical results in order to make statistical inference on the intensity of the interaction. This methodology is presented within a general framework in the supplementary material to ensure its broad applicability across various contexts. We apply the proposed tools to two real data sets (from Reddit and Gutenberg).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09648v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Irene Crimaldi, Andrea Ghiglietti</dc:creator>
    </item>
    <item>
      <title>New methods to compute the generalized chi-square distribution</title>
      <link>https://arxiv.org/abs/2404.05062</link>
      <description>arXiv:2404.05062v3 Announce Type: replace-cross 
Abstract: We present four new mathematical methods, two exact and two approximate, along with open-source software, to compute the cdf, pdf and inverse cdf of the generalized chi-square distribution. Some methods are geared for speed, while others are designed to be accurate far into the tails, using which we can also measure large values of the discriminability index $d'$ between multivariate normal distributions. We compare the accuracy and speed of these and previous methods, characterize their advantages and limitations, and identify the best methods to use in different cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05062v3</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abhranil Das</dc:creator>
    </item>
    <item>
      <title>Redefining the Shortest Path Problem Formulation of the Linear Non-Gaussian Acyclic Model: Pairwise Likelihood Ratios, Prior Knowledge, and Path Enumeration</title>
      <link>https://arxiv.org/abs/2404.11922</link>
      <description>arXiv:2404.11922v2 Announce Type: replace-cross 
Abstract: Effective causal discovery is essential for learning the causal graph from observational data. The linear non-Gaussian acyclic model (LiNGAM) operates under the assumption of a linear data generating process with non-Gaussian noise in determining the causal graph. Its assumption of unmeasured confounders being absent, however, poses practical limitations. In response, empirical research has shown that the reformulation of LiNGAM as a shortest path problem (LiNGAM-SPP) addresses this limitation. Within LiNGAM-SPP, mutual information is chosen to serve as the measure of independence. A challenge is introduced - parameter tuning is now needed due to its reliance on kNN mutual information estimators. The paper proposes a threefold enhancement to the LiNGAM-SPP framework.
  First, the need for parameter tuning is eliminated by using the pairwise likelihood ratio in lieu of kNN-based mutual information. This substitution is validated on a general data generating process and benchmark real-world data sets, outperforming existing methods especially when given a larger set of features. The incorporation of prior knowledge is then enabled by a node-skipping strategy implemented on the graph representation of all causal orderings to eliminate violations based on the provided input of relative orderings. Flexibility relative to existing approaches is achieved. Last among the three enhancements is the utilization of the distribution of paths in the graph representation of all causal orderings. From this, crucial properties of the true causal graph such as the presence of unmeasured confounders and sparsity may be inferred. To some extent, the expected performance of the causal discovery algorithm may be predicted. The refinements above advance the practicality and performance of LiNGAM-SPP, showcasing the potential of graph-search-based methodologies in advancing causal discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11922v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hans Jarett J. Ong, Brian Godwin S. Lim, Renzo Roel P. Tan, Kazushi Ikeda</dc:creator>
    </item>
    <item>
      <title>Discovering overlapping communities in multi-layer directed networks</title>
      <link>https://arxiv.org/abs/2407.16152</link>
      <description>arXiv:2407.16152v2 Announce Type: replace-cross 
Abstract: Community detection in multi-layer undirected networks has attracted considerable attention in recent years. However, multi-layer directed networks are common in the real world, and existing community detection methods often either ignore the asymmetric structure in multi-layer directed networks or assume that every node solely belongs to a single community, significantly limiting their applicability to overlapping multi-layer directed networks, where nodes can belong to multiple communities simultaneously. To fill this gap, this article explores the challenging problem of detecting overlapping communities in multi-layer directed networks. Our goal is to understand the underlying asymmetric overlapping community structure by analyzing the mixed memberships of nodes. We introduce a novel multi-layer mixed membership stochastic co-block model (multi-layer MM-ScBM) to model overlapping multi-layer directed networks. We develop a spectral procedure to estimate nodes' memberships in both sending and receiving patterns. Our method uses a successive projection algorithm on a few leading eigenvectors of two debiased aggregation matrices. To our knowledge, this is the first work to detect asymmetric overlapping communities in multi-layer directed networks. We demonstrate the consistent estimation properties of our method by providing per-node error rates under the multi-layer MM-ScBM framework. Our theoretical analysis reveals that increasing the overall sparsity, the number of nodes, or the number of layers can improve the accuracy of overlapping community detection. Extensive numerical experiments validate these theoretical findings. We also apply our method to one real-world multi-layer directed network, gaining insightful results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16152v2</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Chaos, Solitons and Fractals 2025</arxiv:journal_reference>
      <dc:creator>Huan Qing</dc:creator>
    </item>
    <item>
      <title>Matching $\leq$ Hybrid $\leq$ Difference in Differences</title>
      <link>https://arxiv.org/abs/2411.07952</link>
      <description>arXiv:2411.07952v2 Announce Type: replace-cross 
Abstract: Since LaLonde's (1986) seminal paper, there has been ongoing interest in estimating treatment effects using pre- and post-intervention data. Scholars have traditionally used experimental benchmarks to evaluate the accuracy of alternative econometric methods, including Matching, Difference-in-Differences (DID), and their hybrid forms (e.g., Heckman et al., 1998b; Dehejia and Wahba, 2002; Smith and Todd, 2005). We revisit these methodologies in the evaluation of job training and educational programs using four datasets (LaLonde, 1986; Heckman et al., 1998a; Smith and Todd, 2005; Chetty et al., 2014a; Athey et al., 2020), and show that the inequality relationship, Matching $\leq$ Hybrid $\leq$ DID, appears as a consistent norm, rather than a mere coincidence. We provide a formal theoretical justification for this puzzling phenomenon under plausible conditions such as negative selection, by generalizing the classical bracketing (Angrist and Pischke, 2009, Section 5). Consequently, when treatments are expected to be non-negative, DID tends to provide optimistic estimates, while Matching offers more conservative ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07952v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yechan Park, Yuya Sasaki</dc:creator>
    </item>
    <item>
      <title>Free Anytime Validity by Sequentializing a Test and Optional Continuation with Tests as Future Significance Levels</title>
      <link>https://arxiv.org/abs/2501.03982</link>
      <description>arXiv:2501.03982v3 Announce Type: replace-cross 
Abstract: Anytime valid sequential tests permit us to stop and continue testing based on the current data, without invalidating the inference. Given a maximum number of observations $N$, one may believe this must come at the cost of power when compared to a conventional test that waits until all $N$ observations have arrived. Our first contribution is to show that this is false: for any valid test based on $N$ observations, we derive an anytime valid sequential test that matches it after $N$ observations. Our second contribution is that the outcome of a continuously-interpreted test can be used as a significance level in subsequent testing, leading to an overall procedure that is valid at the original significance level. Combined this shows both the value of continuously-interpreted tests, and the fact that anytime validity and optional continuation are readily available in traditional testing, without requiring explicit use of e-values. We illustrate this by deriving the anytime valid sequentialized $z$-test and $t$-test, which at time $N$ coincide with the traditional $z$-test and $t$-test. Lastly, we show the popular log-optimal sequential $z$-test can be interpreted as desiring a rejection by the traditional $z$-test at some tiny significance level in the distant future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03982v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>Association of normalization, non-differentially expressed genes and data source with machine learning performance in intra-dataset or cross-dataset modelling of transcriptomic and clinical data</title>
      <link>https://arxiv.org/abs/2502.18888</link>
      <description>arXiv:2502.18888v2 Announce Type: replace-cross 
Abstract: Cross-dataset testing is critical for examining machine learning (ML) model's performance. However, most studies on modelling transcriptomic and clinical data only conducted intra-dataset testing. It is also unclear whether normalization and non-differentially expressed genes (NDEG) can improve cross-dataset modeling performance of ML. We thus aim to understand whether normalization, NDEG and data source are associated with performance of ML in cross-dataset testing. The transcriptomic and clinical data shared by the lung adenocarcinoma cases in TCGA and ONCOSG were used. The best cross-dataset ML performance was reached using transcriptomic data alone and statistically better than those using transcriptomic and clinical data. The best balance accuracy, area under curve and accuracy were significantly better in ML algorithms training on TCGA and tested on ONCOSG than those trained on ONCOSG and tested on TCGA (p&lt;0.05 for all). Normalization and NDEG greatly improved intra-dataset ML performances in both datasets, but not in cross-dataset testing. Strikingly, modelling transcriptomic data of ONCOSG alone outperformed modelling transcriptomic and clinical data whereas including clinical data in TCGA did not significantly impact ML performance, suggesting limited clinical data value or an overwhelming influence of transcriptomic data in TCGA. Performance gains in intra-dataset testing were more pronounced for ML models trained on ONCOSG than TCGA. Among the six ML models compared, Support vector machine was the most frequent best-performer in both intra-dataset and cross-dataset testing. Therefore, our data show data source, normalization and NDEG are associated with intra-dataset and cross-dataset ML performance in modelling transcriptomic and clinical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18888v2</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fei Deng, Lanjing Zhang</dc:creator>
    </item>
  </channel>
</rss>
