<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 May 2024 04:02:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Weighted past and paired dynamic varentropy measures, their properties and usefulness</title>
      <link>https://arxiv.org/abs/2405.06428</link>
      <description>arXiv:2405.06428v1 Announce Type: new 
Abstract: We introduce two uncertainty measures, say weighted past varentropy (WPVE) and weighted paired dynamic varentropy (WPDVE). Several properties have been studied for these proposed measures. The effect of the monotone transformation for these measures have been discussed. We have obtained an upper bound of the WPVE using the weighted past Shannon entropy. A lower bound of the WPVE is also obtained. The WPVE has been studied for proportional reversed hazard rate (PRHR) models. Upper and lower bounds of the WPDVE have been derived. We propose non-parametric kernel estimates of the WPVE and WPDVE. Further, maximum likelihood estimation has been employed to estimate WPVE and WPDVE for an exponential population. A numerical simulation is provided to observe the behaviour of the proposed estimates. Finally, we have analysed a real data set and obtain the estimated values of WPVE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06428v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shital Saha, Suchandan Kayal</dc:creator>
    </item>
    <item>
      <title>Generalized van Trees inequality: Local minimax bounds for non-smooth functionals and irregular statistical models</title>
      <link>https://arxiv.org/abs/2405.06437</link>
      <description>arXiv:2405.06437v1 Announce Type: new 
Abstract: In a decision-theoretic framework, minimax lower bound provides the worst-case performance of estimators relative to a given class of statistical models. For parametric and semiparametric models, the H\'{a}jek--Le Cam local asymptotic minimax (LAM) theorem provides the optimal and sharp asymptotic lower bound. Despite its relative generality, this result comes with limitations as it only applies to the estimation of differentiable functionals under regular statistical models. On the other hand, non-asymptotic minimax lower bounds, such as those based on the reduction to hypothesis testing, do not often yield sharp asymptotic constants. Inspired by the recent improvement of the van Trees inequality and related methods in the literature, we provide new non-asymptotic minimax lower bounds under minimal regularity assumptions, which imply sharp asymptotic constants. The proposed lower bounds do not require the differentiability of functionals or regularity of statistical models, extending the efficiency theory to broader situations where standard approaches fail. Additionally, new lower bounds provide non-asymptotic constants, which can shed light on more refined fundamental limits of estimation in finite samples. We demonstrate that new lower bounds recover many classical results, including the LAM theorem and semiparametric efficiency bounds. We also illustrate the use of the new lower bound by deriving the local minimax lower bound for estimating the density at a point and directionally differentiable parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06437v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Takatsu, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Asymptotic Normality of $U$-Statistics is Equivalent to Convergence in the Wasserstein Distance</title>
      <link>https://arxiv.org/abs/2405.06477</link>
      <description>arXiv:2405.06477v1 Announce Type: new 
Abstract: We prove the claim in the title under mild conditions which are usually satisfied when trying to establish asymptotic normality. We assume strictly stationary and absolutely regular data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06477v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Kroll</dc:creator>
    </item>
    <item>
      <title>Integrability-preserving regularizations of Laplacian Growth</title>
      <link>https://arxiv.org/abs/2405.06167</link>
      <description>arXiv:2405.06167v1 Announce Type: cross 
Abstract: The Laplacian Growth (LG) model is known as a universality class of scale-free aggregation models in two dimensions, characterized by classical integrability and featuring finite-time boundary singularity formation. A discrete counterpart, Diffusion-Limited Aggregation (or DLA), has a similar local growth law, but significantly different global behavior. For both LG and DLA, a proper description for the scaling properties of long-time solutions is not available yet. In this note, we outline a possible approach towards finding the correct theory yielding a regularized LG and its relation to DLA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06167v1</guid>
      <category>math-ph</category>
      <category>math.DS</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1051/mmnp/2019032</arxiv:DOI>
      <arxiv:journal_reference>Math. Model. Nat. Phenom. 15 (2020) 9</arxiv:journal_reference>
      <dc:creator>Razvan Teodorescu</dc:creator>
    </item>
    <item>
      <title>Restricted isometric compression of sparse datasets into low-dimensional varieties</title>
      <link>https://arxiv.org/abs/2405.06200</link>
      <description>arXiv:2405.06200v1 Announce Type: cross 
Abstract: This article extends the known restricted isometric projection of sparse datasets in Euclidean spaces $\mathbb{R}^N$ down into low-dimensional subspaces $\mathbb{R}^k, k \ll N,$ to the case of low-dimensional varieties $\mathcal{M} \subset \mathbb{R}^N,$ of codimension $N - k = \omega(N)$. Applications to structured/hierarchical datasets are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06200v1</guid>
      <category>math-ph</category>
      <category>math.FA</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <category>math.RT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasile Pop, Iuliana Teodorescu, Razvan Teodorescu</dc:creator>
    </item>
    <item>
      <title>Statistical divergences in high-dimensional hypothesis testing and a modern technique for estimating them</title>
      <link>https://arxiv.org/abs/2405.06397</link>
      <description>arXiv:2405.06397v1 Announce Type: cross 
Abstract: Hypothesis testing in high dimensional data is a notoriously difficult problem without direct access to competing models' likelihood functions. This paper argues that statistical divergences can be used to quantify the difference between the population distributions of observed data and competing models, justifying their use as the basis of a hypothesis test. We go on to point out how modern techniques for functional optimization let us estimate many divergences, without the need for population likelihood functions, using samples from two distributions alone. We use a physics-based example to show how the proposed two-sample test can be implemented in practice, and discuss the necessary steps required to mature the ideas presented into an experimental framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06397v1</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy J. H. Wilkinson, Christopher G. Lester</dc:creator>
    </item>
    <item>
      <title>Different informational characteristics of cubic transmuted distributions</title>
      <link>https://arxiv.org/abs/2207.13442</link>
      <description>arXiv:2207.13442v4 Announce Type: replace 
Abstract: Cubic transmuted (CT) distributions were introduced recently by \cite{granzotto2017cubic}. In this article, we derive Shannon entropy, Gini's mean difference and Fisher information (matrix) for CT distributions and establish some of their theoretical properties. In addition, we propose cubic transmuted Shannon entropy and cubic transmuted Gini's mean difference. The CT Shannon entropy is expressed in terms of Kullback-Leibler divergences, while the CT Gini's mean difference is shown to be connected with energy distances. We show that the Kullback-Leibler and Chi-square divergences are free of the underlying parent distribution. Finally, we carry out some simulation studies for the proposed information measures from an inferential viewpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.13442v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shital Saha, Suchandan Kayal, N. Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Fundamental Limits of Spectral Clustering in Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2301.09289</link>
      <description>arXiv:2301.09289v3 Announce Type: replace 
Abstract: Spectral clustering has been widely used for community detection in network sciences. While its empirical successes are well-documented, a clear theoretical understanding, particularly for sparse networks where degrees are much smaller than $\log n$, remains unclear. In this paper, we address this significant gap by demonstrating that spectral clustering offers exponentially small error rates when applied to sparse networks under Stochastic Block Models. Our analysis provides sharp characterizations of its performance, backed by matching upper and lower bounds possessing an identical exponent with the same leading constant. The key to our results is a novel truncated $\ell_2$ perturbation analysis for eigenvectors, coupled with a new analysis idea of eigenvectors truncation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.09289v3</guid>
      <category>math.ST</category>
      <category>cs.SI</category>
      <category>math.SP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anderson Ye Zhang</dc:creator>
    </item>
    <item>
      <title>A Novel and Optimal Spectral Method for Permutation Synchronization</title>
      <link>https://arxiv.org/abs/2303.12051</link>
      <description>arXiv:2303.12051v2 Announce Type: replace 
Abstract: Permutation synchronization is an important problem in computer science that constitutes the key step of many computer vision tasks. The goal is to recover $n$ latent permutations from their noisy and incomplete pairwise measurements. In recent years, spectral methods have gained increasing popularity thanks to their simplicity and computational efficiency. Spectral methods utilize the leading eigenspace $U$ of the data matrix and its block submatrices $U_1,U_2,\ldots, U_n$ to recover the permutations. In this paper, we propose a novel and statistically optimal spectral algorithm. Unlike the existing methods which use $\{U_jU_1^\top\}_{j\geq 2}$, ours constructs an anchor matrix $M$ by aggregating useful information from all of the block submatrices and estimates the latent permutations through $\{U_jM^\top\}_{j\geq 1}$. This modification overcomes a crucial limitation of the existing methods caused by the repetitive use of $U_1$ and leads to an improved numerical performance. To establish the optimality of the proposed method, we carry out a fine-grained spectral analysis and obtain a sharp exponential error bound that matches the minimax rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.12051v2</guid>
      <category>math.ST</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.SP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Duc Nguyen, Anderson Ye Zhang</dc:creator>
    </item>
    <item>
      <title>Intrinsic Bayesian Cram\'er-Rao Bound with an Application to Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2311.04748</link>
      <description>arXiv:2311.04748v2 Announce Type: replace 
Abstract: This paper presents a new performance bound for estimation problems where the parameter to estimate lies in a Riemannian manifold (a smooth manifold endowed with a Riemannian metric) and follows a given prior distribution. In this setup, the chosen Riemannian metric induces a geometry for the parameter manifold, as well as an intrinsic notion of the estimation error measure. Performance bound for such error measure were previously obtained in the non-Bayesian case (when the unknown parameter is assumed to deterministic), and referred to as \textit{intrinsic} Cram\'er-Rao bound. The presented result then appears either as: \textit{a}) an extension of the intrinsic Cram\'er-Rao bound to the Bayesian estimation framework; \textit{b}) a generalization of the Van-Trees inequality (Bayesian Cram\'er-Rao bound) that accounts for the aforementioned geometric structures. In a second part, we leverage this formalism to study the problem of covariance matrix estimation when the data follow a Gaussian distribution, and whose covariance matrix is drawn from an inverse Wishart distribution. Performance bounds for this problem are obtained for both the mean squared error (Euclidean metric) and the natural Riemannian distance for Hermitian positive definite matrices (affine invariant metric). Numerical simulation illustrate that assessing the error with the affine invariant metric is revealing of interesting properties of the maximum a posteriori and minimum mean square error estimator, which are not observed when using the Euclidean metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04748v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florent Bouchard, Alexandre Renaux, Guillaume Ginolhac, Arnaud Breloy</dc:creator>
    </item>
    <item>
      <title>Finite Sample Analysis and Bounds of Generalization Error of Gradient Descent in In-Context Linear Regression</title>
      <link>https://arxiv.org/abs/2405.02462</link>
      <description>arXiv:2405.02462v2 Announce Type: replace 
Abstract: Recent studies show that transformer-based architectures emulate gradient descent during a forward pass, contributing to in-context learning capabilities - an ability where the model adapts to new tasks based on a sequence of prompt examples without being explicitly trained or fine tuned to do so. This work investigates the generalization properties of a single step of gradient descent in the context of linear regression with well-specified models. A random design setting is considered and analytical expressions are derived for the statistical properties and bounds of generalization error in a non-asymptotic (finite sample) setting. These expressions are notable for avoiding arbitrary constants, and thus offer robust quantitative information and scaling relationships. These results are contrasted with those from classical least squares regression (for which analogous finite sample bounds are also derived), shedding light on systematic and noise components, as well as optimal step sizes. Additionally, identities involving high-order products of Gaussian random matrices are presented as a byproduct of the analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02462v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Duraisamy</dc:creator>
    </item>
    <item>
      <title>Valid Inference for Machine Learning Model Parameters</title>
      <link>https://arxiv.org/abs/2302.10840</link>
      <description>arXiv:2302.10840v2 Announce Type: replace-cross 
Abstract: The parameters of a machine learning model are typically learned by minimizing a loss function on a set of training data. However, this can come with the risk of overtraining; in order for the model to generalize well, it is of great importance that we are able to find the optimal parameter for the model on the entire population -- not only on the given training sample. In this paper, we construct valid confidence sets for this optimal parameter of a machine learning model, which can be generated using only the training data without any knowledge of the population. We then show that studying the distribution of this confidence set allows us to assign a notion of confidence to arbitrary regions of the parameter space, and we demonstrate that this distribution can be well-approximated using bootstrapping techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10840v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil Dey, Jonathan P. Williams</dc:creator>
    </item>
    <item>
      <title>Probabilistic cellular automata with local transition matrices: synchronization, ergodicity, and inference</title>
      <link>https://arxiv.org/abs/2405.02928</link>
      <description>arXiv:2405.02928v2 Announce Type: replace-cross 
Abstract: We introduce a new class of probabilistic cellular automata that are capable of exhibiting rich dynamics such as synchronization and ergodicity and can be easily inferred from data. The system is a finite-state locally interacting Markov chain on a circular graph. Each site's subsequent state is random, with a distribution determined by its neighborhood's empirical distribution multiplied by a local transition matrix. We establish sufficient and necessary conditions on the local transition matrix for synchronization and ergodicity. Also, we introduce novel least squares estimators for inferring the local transition matrix from various types of data, which may consist of either multiple trajectories, a long trajectory, or ensemble sequences without trajectory information. Under suitable identifiability conditions, we show the asymptotic normality of these estimators and provide non-asymptotic bounds for their accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02928v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Erhan Bayraktar, Fei Lu, Mauro Maggioni, Ruoyu Wu, Sichen Yang</dc:creator>
    </item>
  </channel>
</rss>
