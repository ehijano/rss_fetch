<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ML updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ML" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Oct 2024 01:56:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Limit Theorems for Stochastic Gradient Descent with Infinite Variance</title>
      <link>https://arxiv.org/abs/2410.16340</link>
      <description>arXiv:2410.16340v1 Announce Type: new 
Abstract: Stochastic gradient descent is a classic algorithm that has gained great popularity especially in the last decades as the most common approach for training models in machine learning. While the algorithm has been well-studied when stochastic gradients are assumed to have a finite variance, there is significantly less research addressing its theoretical properties in the case of infinite variance gradients. In this paper, we establish the asymptotic behavior of stochastic gradient descent in the context of infinite variance stochastic gradients, assuming that the stochastic gradient is regular varying with index $\alpha\in(1,2)$. The closest result in this context was established in 1969 , in the one-dimensional case and assuming that stochastic gradients belong to a more restrictive class of distributions. We extend it to the multidimensional case, covering a broader class of infinite variance distributions. As we show, the asymptotic distribution of the stochastic gradient descent algorithm can be characterized as the stationary distribution of a suitably defined Ornstein-Uhlenbeck process driven by an appropriate stable L\'evy process. Additionally, we explore the applications of these results in linear regression and logistic regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16340v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jose Blanchet, Aleksandar Mijatovi\'c, Wenhao Yang</dc:creator>
    </item>
    <item>
      <title>A Simple Model of Inference Scaling Laws</title>
      <link>https://arxiv.org/abs/2410.16377</link>
      <description>arXiv:2410.16377v1 Announce Type: new 
Abstract: Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an "inference loss", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16377v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noam Levi</dc:creator>
    </item>
    <item>
      <title>Data Augmentation of Multivariate Sensor Time Series using Autoregressive Models and Application to Failure Prognostics</title>
      <link>https://arxiv.org/abs/2410.16419</link>
      <description>arXiv:2410.16419v1 Announce Type: new 
Abstract: This work presents a novel data augmentation solution for non-stationary multivariate time series and its application to failure prognostics. The method extends previous work from the authors which is based on time-varying autoregressive processes. It can be employed to extract key information from a limited number of samples and generate new synthetic samples in a way that potentially improves the performance of PHM solutions. This is especially valuable in situations of data scarcity which are very usual in PHM, especially for failure prognostics. The proposed approach is tested based on the CMAPSS dataset, commonly employed for prognostics experiments and benchmarks. An AutoML approach from PHM literature is employed for automating the design of the prognostics solution. The empirical evaluation provides evidence that the proposed method can substantially improve the performance of PHM solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16419v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas Baptista de Souza, Bruno Paes Leao</dc:creator>
    </item>
    <item>
      <title>BI-EqNO: Generalized Approximate Bayesian Inference with an Equivariant Neural Operator Framework</title>
      <link>https://arxiv.org/abs/2410.16420</link>
      <description>arXiv:2410.16420v1 Announce Type: new 
Abstract: Bayesian inference offers a robust framework for updating prior beliefs based on new data using Bayes' theorem, but exact inference is often computationally infeasible, necessitating approximate methods. Though widely used, these methods struggle to estimate marginal likelihoods accurately, particularly due to the rigid functional structures of deterministic models like Gaussian processes and the limitations of small sample sizes in stochastic models like the ensemble Kalman method. In this work, we introduce BI-EqNO, an equivariant neural operator framework for generalized approximate Bayesian inference, designed to enhance both deterministic and stochastic approaches. BI-EqNO transforms priors into posteriors conditioned on observation data through data-driven training. The framework is flexible, supporting diverse prior and posterior representations with arbitrary discretizations and varying numbers of observations. Crucially, BI-EqNO's architecture ensures (1) permutation equivariance between prior and posterior representations, and (2) permutation invariance with respect to observational data. We demonstrate BI-EqNO's utility through two examples: (1) as a generalized Gaussian process (gGP) for regression, and (2) as an ensemble neural filter (EnNF) for sequential data assimilation. Results show that gGP outperforms traditional Gaussian processes by offering a more flexible representation of covariance functions. Additionally, EnNF not only outperforms the ensemble Kalman filter in small-ensemble settings but also has the potential to function as a "super" ensemble filter, capable of representing and integrating multiple ensemble filters for enhanced assimilation performance. This study highlights BI-EqNO's versatility and effectiveness, improving Bayesian inference through data-driven training while reducing computational costs across various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16420v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xu-Hui Zhou, Zhuo-Ran Liu, Heng Xiao</dc:creator>
    </item>
    <item>
      <title>Robust Feature Learning for Multi-Index Models in High Dimensions</title>
      <link>https://arxiv.org/abs/2410.16449</link>
      <description>arXiv:2410.16449v1 Announce Type: new 
Abstract: Recently, there have been numerous studies on feature learning with neural networks, specifically on learning single- and multi-index models where the target is a function of a low-dimensional projection of the input. Prior works have shown that in high dimensions, the majority of the compute and data resources are spent on recovering the low-dimensional projection; once this subspace is recovered, the remainder of the target can be learned independently of the ambient dimension. However, implications of feature learning in adversarial settings remain unexplored. In this work, we take the first steps towards understanding adversarially robust feature learning with neural networks. Specifically, we prove that the hidden directions of a multi-index model offer a Bayes optimal low-dimensional projection for robustness against $\ell_2$-bounded adversarial perturbations under the squared loss, assuming that the multi-index coordinates are statistically independent from the rest of the coordinates. Therefore, robust learning can be achieved by first performing standard feature learning, then robustly tuning a linear readout layer on top of the standard representations. In particular, we show that adversarially robust learning is just as easy as standard learning, in the sense that the additional number of samples needed to robustly learn multi-index models when compared to standard learning, does not depend on dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16449v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alireza Mousavi-Hosseini, Adel Javanmard, Murat A. Erdogdu</dc:creator>
    </item>
    <item>
      <title>Building Conformal Prediction Intervals with Approximate Message Passing</title>
      <link>https://arxiv.org/abs/2410.16493</link>
      <description>arXiv:2410.16493v1 Announce Type: new 
Abstract: Conformal prediction has emerged as a powerful tool for building prediction intervals that are valid in a distribution-free way. However, its evaluation may be computationally costly, especially in the high-dimensional setting where the dimensionality and sample sizes are both large and of comparable magnitudes. To address this challenge in the context of generalized linear regression, we propose a novel algorithm based on Approximate Message Passing (AMP) to accelerate the computation of prediction intervals using full conformal prediction, by approximating the computation of conformity scores. Our work bridges a gap between modern uncertainty quantification techniques and tools for high-dimensional problems involving the AMP algorithm. We evaluate our method on both synthetic and real data, and show that it produces prediction intervals that are close to the baseline methods, while being orders of magnitude faster. Additionally, in the high-dimensional limit and under assumptions on the data distribution, the conformity scores computed by AMP converge to the one computed exactly, which allows theoretical study and benchmarking of conformal methods in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16493v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Clart\'e, Lenka Zdeborov\'a</dc:creator>
    </item>
    <item>
      <title>General Frameworks for Conditional Two-Sample Testing</title>
      <link>https://arxiv.org/abs/2410.16636</link>
      <description>arXiv:2410.16636v1 Announce Type: new 
Abstract: We study the problem of conditional two-sample testing, which aims to determine whether two populations have the same distribution after accounting for confounding factors. This problem commonly arises in various applications, such as domain adaptation and algorithmic fairness, where comparing two groups is essential while controlling for confounding variables. We begin by establishing a hardness result for conditional two-sample testing, demonstrating that no valid test can have significant power against any single alternative without proper assumptions. We then introduce two general frameworks that implicitly or explicitly target specific classes of distributions for their validity and power. Our first framework allows us to convert any conditional independence test into a conditional two-sample test in a black-box manner, while preserving the asymptotic properties of the original conditional independence test. The second framework transforms the problem into comparing marginal distributions with estimated density ratios, which allows us to leverage existing methods for marginal two-sample testing. We demonstrate this idea in a concrete manner with classification and kernel-based methods. Finally, simulation studies are conducted to illustrate the proposed frameworks in finite-sample scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16636v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seongchan Lee, Suman Cha, Ilmun Kim</dc:creator>
    </item>
    <item>
      <title>Lower Bounds for Time-Varying Kernelized Bandits</title>
      <link>https://arxiv.org/abs/2410.16692</link>
      <description>arXiv:2410.16692v1 Announce Type: new 
Abstract: The optimization of black-box functions with noisy observations is a fundamental problem with widespread applications, and has been widely studied under the assumption that the function lies in a reproducing kernel Hilbert space (RKHS). This problem has been studied extensively in the stationary setting, and near-optimal regret bounds are known via developments in both upper and lower bounds. In this paper, we consider non-stationary scenarios, which are crucial for certain applications but are currently less well-understood. Specifically, we provide the first algorithm-independent lower bounds, where the time variations are subject satisfying a total variation budget according to some function norm. Under $\ell_{\infty}$-norm variations, our bounds are found to be close to the state-of-the-art upper bound (Hong \emph{et al.}, 2023). Under RKHS norm variations, the upper and lower bounds are still reasonably close but with more of a gap, raising the interesting open question of whether non-minor improvements in the upper bound are possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16692v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xu Cai, Jonathan Scarlett</dc:creator>
    </item>
    <item>
      <title>Theoretical Convergence Guarantees for Variational Autoencoders</title>
      <link>https://arxiv.org/abs/2410.16750</link>
      <description>arXiv:2410.16750v1 Announce Type: new 
Abstract: Variational Autoencoders (VAE) are popular generative models used to sample from complex data distributions. Despite their empirical success in various machine learning tasks, significant gaps remain in understanding their theoretical properties, particularly regarding convergence guarantees. This paper aims to bridge that gap by providing non-asymptotic convergence guarantees for VAE trained using both Stochastic Gradient Descent and Adam algorithms.We derive a convergence rate of $\mathcal{O}(\log n / \sqrt{n})$, where $n$ is the number of iterations of the optimization algorithm, with explicit dependencies on the batch size, the number of variational samples, and other key hyperparameters. Our theoretical analysis applies to both Linear VAE and Deep Gaussian VAE, as well as several VAE variants, including $\beta$-VAE and IWAE. Additionally, we empirically illustrate the impact of hyperparameters on convergence, offering new insights into the theoretical understanding of VAE training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16750v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sobihan Surendran (LPSM), Antoine Godichon-Baggioni (LPSM), Sylvain Le Corff (LPSM)</dc:creator>
    </item>
    <item>
      <title>Survival Models: Proper Scoring Rule and Stochastic Optimization with Competing Risks</title>
      <link>https://arxiv.org/abs/2410.16765</link>
      <description>arXiv:2410.16765v1 Announce Type: new 
Abstract: When dealing with right-censored data, where some outcomes are missing due to a limited observation period, survival analysis -- known as time-to-event analysis -- focuses on predicting the time until an event of interest occurs. Multiple classes of outcomes lead to a classification variant: predicting the most likely event, a less explored area known as competing risks. Classic competing risks models couple architecture and loss, limiting scalability.To address these issues, we design a strictly proper censoring-adjusted separable scoring rule, allowing optimization on a subset of the data as each observation is evaluated independently. The loss estimates outcome probabilities and enables stochastic optimization for competing risks, which we use for efficient gradient boosting trees. SurvivalBoost not only outperforms 12 state-of-the-art models across several metrics on 4 real-life datasets, both in competing risks and survival settings, but also provides great calibration, the ability to predict across any time horizon, and computation times faster than existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16765v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julie Alberge (SODA), Vincent Maladi\`ere (SODA), Olivier Grisel (SODA), Judith Ab\'ecassis (SODA), Ga\"el Varoquaux (SODA)</dc:creator>
    </item>
    <item>
      <title>Federated Causal Inference: Multi-Centric ATE Estimation beyond Meta-Analysis</title>
      <link>https://arxiv.org/abs/2410.16870</link>
      <description>arXiv:2410.16870v1 Announce Type: new 
Abstract: We study Federated Causal Inference, an approach to estimate treatment effects from decentralized data across centers. We compare three classes of Average Treatment Effect (ATE) estimators derived from the Plug-in G-Formula, ranging from simple meta-analysis to one-shot and multi-shot federated learning, the latter leveraging the full data to learn the outcome model (albeit requiring more communication). Focusing on Randomized Controlled Trials (RCTs), we derive the asymptotic variance of these estimators for linear models. Our results provide practical guidance on selecting the appropriate estimator for various scenarios, including heterogeneity in sample sizes, covariate distributions, treatment assignment schemes, and center effects. We validate these findings with a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16870v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R\'emi Khellaf, Aur\'elien Bellet, Julie Josse</dc:creator>
    </item>
    <item>
      <title>Understanding Transfer Learning via Mean-field Analysis</title>
      <link>https://arxiv.org/abs/2410.17128</link>
      <description>arXiv:2410.17128v2 Announce Type: new 
Abstract: We propose a novel framework for exploring generalization errors of transfer learning through the lens of differential calculus on the space of probability measures. In particular, we consider two main transfer learning scenarios, $\alpha$-ERM and fine-tuning with the KL-regularized empirical risk minimization and establish generic conditions under which the generalization error and the population risk convergence rates for these scenarios are studied. Based on our theoretical results, we show the benefits of transfer learning with a one-hidden-layer neural network in the mean-field regime under some suitable integrability and regularity assumptions on the loss and activation functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17128v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.FA</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gholamali Aminian, {\L}ukasz Szpruch, Samuel N. Cohen</dc:creator>
    </item>
    <item>
      <title>Identification of Mean-Field Dynamics using Transformers</title>
      <link>https://arxiv.org/abs/2410.16295</link>
      <description>arXiv:2410.16295v1 Announce Type: cross 
Abstract: This paper investigates the use of transformer architectures to approximate the mean-field dynamics of interacting particle systems exhibiting collective behavior. Such systems are fundamental in modeling phenomena across physics, biology, and engineering, including gas dynamics, opinion formation, biological networks, and swarm robotics. The key characteristic of these systems is that the particles are indistinguishable, leading to permutation-equivariant dynamics. We demonstrate that transformers, which inherently possess permutation equivariance, are well-suited for approximating these dynamics. Specifically, we prove that if a finite-dimensional transformer can effectively approximate the finite-dimensional vector field governing the particle system, then the expected output of this transformer provides a good approximation for the infinite-dimensional mean-field vector field. Leveraging this result, we establish theoretical bounds on the distance between the true mean-field dynamics and those obtained using the transformer. We validate our theoretical findings through numerical simulations on the Cucker-Smale model for flocking, and the mean-field system for training two-layer neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16295v1</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiba Biswal, Karthik Elamvazhuthi, Rishi Sonthalia</dc:creator>
    </item>
    <item>
      <title>Simplicity Bias via Global Convergence of Sharpness Minimization</title>
      <link>https://arxiv.org/abs/2410.16401</link>
      <description>arXiv:2410.16401v1 Announce Type: cross 
Abstract: The remarkable generalization ability of neural networks is usually attributed to the implicit bias of SGD, which often yields models with lower complexity using simpler (e.g. linear) and low-rank features. Recent works have provided empirical and theoretical evidence for the bias of particular variants of SGD (such as label noise SGD) toward flatter regions of the loss landscape. Despite the folklore intuition that flat solutions are 'simple', the connection with the simplicity of the final trained model (e.g. low-rank) is not well understood. In this work, we take a step toward bridging this gap by studying the simplicity structure that arises from minimizers of the sharpness for a class of two-layer neural networks. We show that, for any high dimensional training data and certain activations, with small enough step size, label noise SGD always converges to a network that replicates a single linear feature across all neurons; thereby, implying a simple rank one feature matrix. To obtain this result, our main technical contribution is to show that label noise SGD always minimizes the sharpness on the manifold of models with zero loss for two-layer networks. Along the way, we discover a novel property -- a local geodesic convexity -- of the trace of Hessian of the loss at approximate stationary points on the manifold of zero loss, which links sharpness to the geometry of the manifold. This tool may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16401v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khashayar Gatmiry, Zhiyuan Li, Sashank J. Reddi, Stefanie Jegelka</dc:creator>
    </item>
    <item>
      <title>Finite-Sample and Distribution-Free Fair Classification: Optimal Trade-off Between Excess Risk and Fairness, and the Cost of Group-Blindness</title>
      <link>https://arxiv.org/abs/2410.16477</link>
      <description>arXiv:2410.16477v1 Announce Type: cross 
Abstract: Algorithmic fairness in machine learning has recently garnered significant attention. However, two pressing challenges remain: (1) The fairness guarantees of existing fair classification methods often rely on specific data distribution assumptions and large sample sizes, which can lead to fairness violations when the sample size is moderate-a common situation in practice. (2) Due to legal and societal considerations, using sensitive group attributes during decision-making (referred to as the group-blind setting) may not always be feasible.
  In this work, we quantify the impact of enforcing algorithmic fairness and group-blindness in binary classification under group fairness constraints. Specifically, we propose a unified framework for fair classification that provides distribution-free and finite-sample fairness guarantees with controlled excess risk. This framework is applicable to various group fairness notions in both group-aware and group-blind scenarios. Furthermore, we establish a minimax lower bound on the excess risk, showing the minimax optimality of our proposed algorithm up to logarithmic factors. Through extensive simulation studies and real data analysis, we further demonstrate the superior performance of our algorithm compared to existing methods, and provide empirical support for our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16477v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaotian Hou, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>Efficient Neural Network Training via Subset Pretraining</title>
      <link>https://arxiv.org/abs/2410.16523</link>
      <description>arXiv:2410.16523v1 Announce Type: cross 
Abstract: In training neural networks, it is common practice to use partial gradients computed over batches, mostly very small subsets of the training set. This approach is motivated by the argument that such a partial gradient is close to the true one, with precision growing only with the square root of the batch size. A theoretical justification is with the help of stochastic approximation theory. However, the conditions for the validity of this theory are not satisfied in the usual learning rate schedules. Batch processing is also difficult to combine with efficient second-order optimization methods. This proposal is based on another hypothesis: the loss minimum of the training set can be expected to be well-approximated by the minima of its subsets. Such subset minima can be computed in a fraction of the time necessary for optimizing over the whole training set. This hypothesis has been tested with the help of the MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks, optionally extended by training data augmentation. The experiments have confirmed that results equivalent to conventional training can be reached. In summary, even small subsets are representative if the overdetermination ratio for the given model parameter set sufficiently exceeds unity. The computing expense can be reduced to a tenth or less.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16523v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Sp\"orer, Bernhard Bermeitinger, Tomas Hrycej, Niklas Limacher, Siegfried Handschuh</dc:creator>
    </item>
    <item>
      <title>A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration</title>
      <link>https://arxiv.org/abs/2410.16540</link>
      <description>arXiv:2410.16540v1 Announce Type: cross 
Abstract: Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance in improving the reasoning capabilities of large language models (LLMs). While theoretical investigations have been conducted to understand CoT, the underlying transformer used in these studies isolates the CoT reasoning process into separated in-context learning steps (Stepwise ICL). In this work, we theoretically show that, compared to Stepwise ICL, the transformer gains better error correction ability and more accurate predictions if the reasoning from earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning changes the behavior of the transformer, we further investigate the sensitivity of the transformer with Coherent CoT when the demonstration examples are corrupted at the inference stage. Our theoretical results indicate that the transformer is more sensitive to errors in intermediate reasoning steps than the final outcome. Building upon this observation, we propose an improvement on CoT by incorporating both correct and incorrect reasoning paths in the demonstration. Our experiments validate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16540v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingqian Cui, Pengfei He, Xianfeng Tang, Qi He, Chen Luo, Jiliang Tang, Yue Xing</dc:creator>
    </item>
    <item>
      <title>Gradient Normalization with(out) Clipping Ensures Convergence of Nonconvex SGD under Heavy-Tailed Noise with Improved Results</title>
      <link>https://arxiv.org/abs/2410.16561</link>
      <description>arXiv:2410.16561v1 Announce Type: cross 
Abstract: This paper investigates Gradient Normalization Stochastic Gradient Descent without Clipping (NSGDC) and its variance reduction variant (NSGDC-VR) for nonconvex optimization under heavy-tailed noise. We present significant improvements in the theoretical results for both algorithms, including the removal of logarithmic factors from the convergence rates and the recovery of the convergence rate to match the deterministic case when the noise variance {\sigma} is zero. Additionally, we demonstrate that gradient normalization alone, assuming individual Lipschitz smoothness, is sufficient to ensure convergence of SGD under heavy-tailed noise, eliminating the need for gradient clipping. Furthermore, we introduce accelerated nonconvex algorithms that utilize second-order Lipschitz smoothness to achieve enhanced convergence rates in the presence of heavy-tailed noise. Our findings offer a deeper understanding of how gradient normalization and variance reduction techniques can be optimized for robust performance in challenging optimization scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16561v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tao Sun, Xinwang Liu, Kun Yuan</dc:creator>
    </item>
    <item>
      <title>Assessing and improving reliability of neighbor embedding methods: a map-continuity perspective</title>
      <link>https://arxiv.org/abs/2410.16608</link>
      <description>arXiv:2410.16608v1 Announce Type: cross 
Abstract: Visualizing high-dimensional data is an important routine for understanding biomedical data and interpreting deep learning models. Neighbor embedding methods, such as t-SNE, UMAP, and LargeVis, among others, are a family of popular visualization methods which reduce high-dimensional data to two dimensions. However, recent studies suggest that these methods often produce visual artifacts, potentially leading to incorrect scientific conclusions. Recognizing that the current limitation stems from a lack of data-independent notions of embedding maps, we introduce a novel conceptual and computational framework, LOO-map, that learns the embedding maps based on a classical statistical idea known as the leave-one-out. LOO-map extends the embedding over a discrete set of input points to the entire input space, enabling a systematic assessment of map continuity, and thus the reliability of the visualizations. We find for many neighbor embedding methods, their embedding maps can be intrinsically discontinuous. The discontinuity induces two types of observed map distortion: ``overconfidence-inducing discontinuity," which exaggerates cluster separation, and ``fracture-inducing discontinuity," which creates spurious local structures. Building upon LOO-map, we propose two diagnostic point-wise scores -- perturbation score and singularity score -- to address these limitations. These scores can help identify unreliable embedding points, detect out-of-distribution data, and guide hyperparameter selection. Our approach is flexible and works as a wrapper around many neighbor embedding algorithms. We test our methods across multiple real-world datasets from computer vision and single-cell omics to demonstrate their effectiveness in enhancing the interpretability and accuracy of visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16608v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhexuan Liu, Rong Ma, Yiqiao Zhong</dc:creator>
    </item>
    <item>
      <title>Parsimonious Dynamic Mode Decomposition: A Robust and Automated Approach for Optimally Sparse Mode Selection in Complex Systems</title>
      <link>https://arxiv.org/abs/2410.16656</link>
      <description>arXiv:2410.16656v1 Announce Type: cross 
Abstract: This paper introduces the Parsimonious Dynamic Mode Decomposition (parsDMD), a novel algorithm designed to automatically select an optimally sparse subset of dynamic modes for both spatiotemporal and purely temporal data. By incorporating time-delay embedding and leveraging Orthogonal Matching Pursuit (OMP), parsDMD ensures robustness against noise and effectively handles complex, nonlinear dynamics. The algorithm is validated on a diverse range of datasets, including standing wave signals, identifying hidden dynamics, fluid dynamics simulations (flow past a cylinder and transonic buffet), and atmospheric sea-surface temperature (SST) data. ParsDMD addresses a significant limitation of the traditional sparsity-promoting DMD (spDMD), which requires manual tuning of sparsity parameters through a rigorous trial-and-error process to balance between single-mode and all-mode solutions. In contrast, parsDMD autonomously determines the optimally sparse subset of modes without user intervention, while maintaining minimal computational complexity. Comparative analyses demonstrate that parsDMD consistently outperforms spDMD by providing more accurate mode identification and effective reconstruction in noisy environments. These advantages render parsDMD an effective tool for real-time diagnostics, forecasting, and reduced-order model construction across various disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16656v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.DS</category>
      <category>physics.data-an</category>
      <category>physics.flu-dyn</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arpan Das, Pier Marzocca, Oleg Levinski</dc:creator>
    </item>
    <item>
      <title>Universal approximation property of ODENet and ResNet with a single activation function</title>
      <link>https://arxiv.org/abs/2410.16709</link>
      <description>arXiv:2410.16709v1 Announce Type: cross 
Abstract: We study a universal approximation property of ODENet and ResNet. The ODENet is a map from an initial value to the final value of an ODE system in a finite interval. It is considered a mathematical model of a ResNet-type deep learning system. We consider dynamical systems with vector fields given by a single composition of the activation function and an affine mapping, which is the most common choice of the ODENet or ResNet vector field in actual machine learning systems. We show that such an ODENet and ResNet with a restricted vector field can uniformly approximate ODENet with a general vector field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16709v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masato Kimura, Kazunori Matsui, Yosuke Mizuno</dc:creator>
    </item>
    <item>
      <title>A class of modular and flexible covariate-based covariance functions for nonstationary spatial modeling</title>
      <link>https://arxiv.org/abs/2410.16716</link>
      <description>arXiv:2410.16716v1 Announce Type: cross 
Abstract: The assumptions of stationarity and isotropy often stated over spatial processes have not aged well during the last two decades, partly explained by the combination of computational developments and the increasing availability of high-resolution spatial data. While a plethora of approaches have been developed to relax these assumptions, it is often a costly tradeoff between flexibility and a diversity of computational challenges. In this paper, we present a class of covariance functions that relies on fixed, observable spatial information that provides a convenient tradeoff while offering an extra layer of numerical and visual representation of the flexible spatial dependencies. This model allows for separate parametric structures for different sources of nonstationarity, such as marginal standard deviation, geometric anisotropy, and smoothness. It simplifies to a Mat\'ern covariance function in its basic form and is adaptable for large datasets, enhancing flexibility and computational efficiency. We analyze the capabilities of the presented model through simulation studies and an application to Swiss precipitation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16716v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Blasi, Reinhard Furrer</dc:creator>
    </item>
    <item>
      <title>Robust Variable Selection for High-dimensional Regression with Missing Data and Measurement Errors</title>
      <link>https://arxiv.org/abs/2410.16722</link>
      <description>arXiv:2410.16722v2 Announce Type: cross 
Abstract: In our paper,we focus on robust variable selection for missing data and measurement error.Missing data and measurement errors can lead to confusing data distribution.We propose an exponential loss function with tuning parameter to apply to Missing and measurement errors data.By adjusting the parameter,the loss functioncan be better and more robust under various different data distributions.We use inverse probability weighting and additivityerrormodels to address missing data and measurement errors.Also,we find that the Atan punishment method works better.We used Monte Carlo simulations to assess the validity of robust variable selection and validated our findings with the breast cancer dataset</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16722v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenhao Zhang</dc:creator>
    </item>
    <item>
      <title>Klein Model for Hyperbolic Neural Networks</title>
      <link>https://arxiv.org/abs/2410.16813</link>
      <description>arXiv:2410.16813v1 Announce Type: cross 
Abstract: Hyperbolic neural networks (HNNs) have been proved effective in modeling complex data structures. However, previous works mainly focused on the Poincar\'e ball model and the hyperboloid model as coordinate representations of the hyperbolic space, often neglecting the Klein model. Despite this, the Klein model offers its distinct advantages thanks to its straight-line geodesics, which facilitates the well-known Einstein midpoint construction, previously leveraged to accompany HNNs in other models. In this work, we introduce a framework for hyperbolic neural networks based on the Klein model. We provide detailed formulation for representing useful operations using the Klein model. We further study the Klein linear layer and prove that the "tangent space construction" of the scalar multiplication and parallel transport are exactly the Einstein scalar multiplication and the Einstein addition, analogous to the M\"obius operations used in the Poincar\'e ball model. We show numerically that the Klein HNN performs on par with the Poincar\'e ball model, providing a third option for HNN that works as a building block for more complicated architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16813v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yidan Mao, Jing Gu, Marcus C. Werner, Dongmian Zou</dc:creator>
    </item>
    <item>
      <title>Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections</title>
      <link>https://arxiv.org/abs/2410.16901</link>
      <description>arXiv:2410.16901v1 Announce Type: cross 
Abstract: Bayesian deep learning all too often underfits so that the Bayesian prediction is less accurate than a simple point estimate. Uncertainty quantification then comes at the cost of accuracy. For linearized models, the null space of the generalized Gauss-Newton matrix corresponds to parameters that preserve the training predictions of the point estimate. We propose to build Bayesian approximations in this null space, thereby guaranteeing that the Bayesian predictive does not underfit. We suggest a matrix-free algorithm for projecting onto this null space, which scales linearly with the number of parameters and quadratically with the number of output dimensions. We further propose an approximation that only scales linearly with parameters to make the method applicable to generative models. An extensive empirical evaluation shows that the approach scales to large models, including vision transformers with 28 million parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16901v1</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Miani, Hrittik Roy, S{\o}ren Hauberg</dc:creator>
    </item>
    <item>
      <title>Optimal Design for Reward Modeling in RLHF</title>
      <link>https://arxiv.org/abs/2410.17055</link>
      <description>arXiv:2410.17055v2 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a popular approach to align language models (LMs) with human preferences. This method involves collecting a large dataset of human pairwise preferences across various text generations and using it to infer (implicitly or explicitly) a reward model. Numerous methods have been proposed to learn the reward model and align a LM with it. However, the costly process of collecting human preferences has received little attention and could benefit from theoretical insights. This paper addresses this issue and aims to formalize the reward training model in RLHF. We frame the selection of an effective dataset as a simple regret minimization task, using a linear contextual dueling bandit method. Given the potentially large number of arms, this approach is more coherent than the best-arm identification setting. We then propose an offline framework for solving this problem. Under appropriate assumptions - linearity of the reward model in the embedding space, and boundedness of the reward parameter - we derive bounds on the simple regret. Finally, we provide a lower bound that matches our upper bound up to constant and logarithmic terms. To our knowledge, this is the first theoretical contribution in this area to provide an offline approach as well as worst-case guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17055v2</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Scheid, Etienne Boursier, Alain Durmus, Michael I. Jordan, Pierre M\'enard, Eric Moulines, Michal Valko</dc:creator>
    </item>
    <item>
      <title>Covariance estimation using Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2410.17147</link>
      <description>arXiv:2410.17147v1 Announce Type: cross 
Abstract: We investigate the complexity of covariance matrix estimation for Gibbs distributions based on dependent samples from a Markov chain. We show that when $\pi$ satisfies a Poincar\'e inequality and the chain possesses a spectral gap, we can achieve similar sample complexity using MCMC as compared to an estimator constructed using i.i.d. samples, with potentially much better query complexity. As an application of our methods, we show improvements for the query complexity in both constrained and unconstrained settings for concrete instances of MCMC. In particular, we provide guarantees regarding isotropic rounding procedures for sampling uniformly on convex bodies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17147v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunbum Kook, Matthew S. Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal Robust Estimation under Local and Global Corruptions: Stronger Adversary and Smaller Error</title>
      <link>https://arxiv.org/abs/2410.17230</link>
      <description>arXiv:2410.17230v1 Announce Type: cross 
Abstract: Algorithmic robust statistics has traditionally focused on the contamination model where a small fraction of the samples are arbitrarily corrupted. We consider a recent contamination model that combines two kinds of corruptions: (i) small fraction of arbitrary outliers, as in classical robust statistics, and (ii) local perturbations, where samples may undergo bounded shifts on average. While each noise model is well understood individually, the combined contamination model poses new algorithmic challenges, with only partial results known. Existing efficient algorithms are limited in two ways: (i) they work only for a weak notion of local perturbations, and (ii) they obtain suboptimal error for isotropic subgaussian distributions (among others). The latter limitation led [NGS24, COLT'24] to hypothesize that improving the error might, in fact, be computationally hard. Perhaps surprisingly, we show that information theoretically optimal error can indeed be achieved in polynomial time, under an even \emph{stronger} local perturbation model (the sliced-Wasserstein metric as opposed to the Wasserstein metric). Notably, our analysis reveals that the entire family of stability-based robust mean estimators continues to work optimally in a black-box manner for the combined contamination model. This generalization is particularly useful in real-world scenarios where the specific form of data corruption is not known in advance. We also present efficient algorithms for distribution learning and principal component analysis in the combined contamination model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17230v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thanasis Pittas, Ankit Pensia</dc:creator>
    </item>
    <item>
      <title>TensorProjection Layer: A Tensor-Based Dimension Reduction Method in Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2004.04454</link>
      <description>arXiv:2004.04454v2 Announce Type: replace 
Abstract: In this paper, we propose a dimension reduction method specifically designed for tensor-structured feature data in deep neural networks. The method is implemented as a hidden layer, called the TensorProjection layer, which transforms input tensors into output tensors with reduced dimensions through mode-wise projections. The projection directions are treated as model parameters of the layer and are optimized during model training. Our method can serve as an alternative to pooling layers for summarizing image data, or to convolutional layers as a technique for reducing the number of channels. We conduct experiments on tasks such as medical image classification and segmentation, integrating the TensorProjection layer into commonly used baseline architectures to evaluate its effectiveness. Numerical experiments indicate that the proposed method can outperform traditional downsampling methods, such as pooling layers, in our tasks, suggesting it as a promising alternative for feature summarization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2004.04454v2</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toshinari Morimoto, Su-Yun Huang</dc:creator>
    </item>
    <item>
      <title>Targeted Separation and Convergence with Kernel Discrepancies</title>
      <link>https://arxiv.org/abs/2209.12835</link>
      <description>arXiv:2209.12835v4 Announce Type: replace 
Abstract: Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this article we derive new sufficient and necessary conditions to ensure (i) and (ii). For MMDs on separable metric spaces, we characterize those kernels that separate Bochner embeddable measures and introduce simple conditions for separating all measures with unbounded kernels and for controlling convergence with bounded kernels. We use these results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD separation and convergence control and to develop the first KSDs known to exactly metrize weak convergence to P. Along the way, we highlight the implications of our results for hypothesis testing, measuring and improving sample quality, and sampling with Stein variational gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.12835v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Barp, Carl-Johann Simon-Gabriel, Mark Girolami, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret</title>
      <link>https://arxiv.org/abs/2306.03372</link>
      <description>arXiv:2306.03372v3 Announce Type: replace 
Abstract: Large tensor learning algorithms are typically computationally expensive and require storing a vast amount of data. In this paper, we propose a unified online Riemannian gradient descent (oRGrad) algorithm for tensor learning, which is computationally efficient, consumes much less memory, and can handle sequentially arriving data while making timely predictions. The algorithm is applicable to both linear and generalized linear models. If the time horizon T is known, oRGrad achieves statistical optimality by choosing an appropriate fixed step size. We find that noisy tensor completion particularly benefits from online algorithms by avoiding the trimming procedure and ensuring sharp entry-wise statistical error, which is often technically challenging for offline methods. The regret of oRGrad is analyzed, revealing a fascinating trilemma concerning the computational convergence rate, statistical error, and regret bound. By selecting an appropriate constant step size, oRGrad achieves an $O(T^{1/2})$ regret. We then introduce the adaptive-oRGrad algorithm, which can achieve the optimal $O(\log T)$ regret by adaptively selecting step sizes, regardless of whether the time horizon is known. The adaptive-oRGrad algorithm can attain a statistically optimal error rate without knowing the horizon. Comprehensive numerical simulations corroborate our theoretical findings. We show that oRGrad significantly outperforms its offline counterpart in predicting the solar F10.7 index with tensor predictors that monitor space weather impacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.03372v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang Li, Jian-Feng Cai, Yang Chen, Dong Xia</dc:creator>
    </item>
    <item>
      <title>Soft ascent-descent as a stable and flexible alternative to flooding</title>
      <link>https://arxiv.org/abs/2310.10006</link>
      <description>arXiv:2310.10006v2 Announce Type: replace 
Abstract: As a heuristic for improving test accuracy in classification, the "flooding" method proposed by Ishida et al. (2020) sets a threshold for the average surrogate loss at training time; above the threshold, gradient descent is run as usual, but below the threshold, a switch to gradient ascent is made. While setting the threshold is non-trivial and is usually done with validation data, this simple technique has proved remarkably effective in terms of accuracy. On the other hand, what if we are also interested in other metrics such as model complexity or average surrogate loss at test time? As an attempt to achieve better overall performance with less fine-tuning, we propose a softened, pointwise mechanism called SoftAD (soft ascent-descent) that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect of flooding, with no additional computational overhead. We contrast formal stationarity guarantees with those for flooding, and empirically demonstrate how SoftAD can realize classification accuracy competitive with flooding (and the more expensive alternative SAM) while enjoying a much smaller loss generalization gap and model norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10006v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew J. Holland, Kosuke Nakatani</dc:creator>
    </item>
    <item>
      <title>Community Detection Guarantees Using Embeddings Learned by Node2Vec</title>
      <link>https://arxiv.org/abs/2310.17712</link>
      <description>arXiv:2310.17712v3 Announce Type: replace 
Abstract: Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of $k$-means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddings for node and link prediction tasks. We demonstrate this result empirically, and examine how this relates to other embedding tools for network data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17712v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Davison, S. Carlyle Morgan, Owen G. Ward</dc:creator>
    </item>
    <item>
      <title>Stochastic Gradient Descent for Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2401.00691</link>
      <description>arXiv:2401.00691v3 Announce Type: replace 
Abstract: This paper introduces an iterative algorithm for training nonparametric additive models that enjoys favorable memory storage and computational requirements. The algorithm can be viewed as the functional counterpart of stochastic gradient descent, applied to the coefficients of a truncated basis expansion of the component functions. We show that the resulting estimator satisfies an oracle inequality that allows for model mis-specification. In the well-specified setting, by choosing the learning rate carefully across three distinct stages of training, we demonstrate that its risk is minimax optimal in terms of the dependence on the dimensionality of the data and the size of the training sample. We also provide polynomial convergence rates even when the covariates do not have full support on their domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00691v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Chen, Jason M. Klusowski</dc:creator>
    </item>
    <item>
      <title>On high-dimensional modifications of the nearest neighbor classifier</title>
      <link>https://arxiv.org/abs/2407.05145</link>
      <description>arXiv:2407.05145v2 Announce Type: replace 
Abstract: Nearest neighbor classifier is arguably the most simple and popular nonparametric classifier available in the literature. However, due to the concentration of pairwise distances and the violation of the neighborhood structure, this classifier often suffers in high-dimension, low-sample size (HDLSS) situations, especially when the scale difference between the competing classes dominates their location difference. Several attempts have been made in the literature to take care of this problem. In this article, we discuss some of these existing methods and propose some new ones. We carry out some theoretical investigations in this regard and analyze several simulated and benchmark datasets to compare the empirical performances of proposed methods with some of the existing ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05145v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annesha Ghosh, Bilol Banerjee, Anil K. Ghosh</dc:creator>
    </item>
    <item>
      <title>The Effect of Personalization in FedProx: A Fine-grained Analysis on Statistical Accuracy and Communication Efficiency</title>
      <link>https://arxiv.org/abs/2410.08934</link>
      <description>arXiv:2410.08934v2 Announce Type: replace 
Abstract: FedProx is a simple yet effective federated learning method that enables model personalization via regularization. Despite remarkable success in practice, a rigorous analysis of how such a regularization provably improves the statistical accuracy of each client's local model hasn't been fully established. Setting the regularization strength heuristically presents a risk, as an inappropriate choice may even degrade accuracy. This work fills in the gap by analyzing the effect of regularization on statistical accuracy, thereby providing a theoretical guideline for setting the regularization strength for achieving personalization. We prove that by adaptively choosing the regularization strength under different statistical heterogeneity, FedProx can consistently outperform pure local training and achieve a nearly minimax-optimal statistical rate. In addition, to shed light on resource allocation, we design an algorithm, provably showing that stronger personalization reduces communication complexity without increasing the computation cost overhead. Finally, our theory is validated on both synthetic and real-world datasets and its generalizability is verified in a non-convex setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08934v2</guid>
      <category>stat.ML</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Yu, Zelin He, Ying Sun, Lingzhou Xue, Runze Li</dc:creator>
    </item>
    <item>
      <title>Low-degree learning and the metric entropy of polynomials</title>
      <link>https://arxiv.org/abs/2203.09659</link>
      <description>arXiv:2203.09659v4 Announce Type: replace-cross 
Abstract: Let $\mathscr{F}_{n,d}$ be the class of all functions $f:\{-1,1\}^n\to[-1,1]$ on the $n$-dimensional discrete hypercube of degree at most $d$. In the first part of this paper, we prove that any (deterministic or randomized) algorithm which learns $\mathscr{F}_{n,d}$ with $L_2$-accuracy $\varepsilon$ requires at least $\Omega((1-\sqrt{\varepsilon})2^d\log n)$ queries for large enough $n$, thus establishing the sharpness as $n\to\infty$ of a recent upper bound of Eskenazis and Ivanisvili (2021). To do this, we show that the $L_2$-packing numbers $\mathsf{M}(\mathscr{F}_{n,d},\|\cdot\|_{L_2},\varepsilon)$ of the concept class $\mathscr{F}_{n,d}$ satisfy the two-sided estimate $$c(1-\varepsilon)2^d\log n \leq \log \mathsf{M}(\mathscr{F}_{n,d},\|\cdot\|_{L_2},\varepsilon) \leq \frac{2^{Cd}\log n}{\varepsilon^4}$$ for large enough $n$, where $c, C&gt;0$ are universal constants. In the second part of the paper, we present a logarithmic upper bound for the randomized query complexity of classes of bounded approximate polynomials whose Fourier spectra are concentrated on few subsets. As an application, we prove new estimates for the number of random queries required to learn approximate juntas of a given degree, functions with rapidly decaying Fourier tails and constant depth circuits of given size. Finally, we obtain bounds for the number of queries required to learn the polynomial class $\mathscr{F}_{n,d}$ without error in the query and random example models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.09659v4</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandros Eskenazis, Paata Ivanisvili, Lauritz Streck</dc:creator>
    </item>
    <item>
      <title>Credal Bayesian Deep Learning</title>
      <link>https://arxiv.org/abs/2302.09656</link>
      <description>arXiv:2302.09656v5 Announce Type: replace-cross 
Abstract: Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian Neural Networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of predictive uncertainty cannot be distinguished properly. We present Credal Bayesian Deep Learning (CBDL). Heuristically, CBDL allows to train an (uncountably) infinite ensemble of BNNs, using only finitely many elements. This is possible thanks to prior and likelihood finitely generated credal sets (FGCSs), a concept from the imprecise probability literature. Intuitively, convex combinations of a finite collection of prior-likelihood pairs are able to represent infinitely many such pairs. After training, CBDL outputs a set of posteriors on the parameters of the neural network. At inference time, such posterior set is used to derive a set of predictive distributions that is in turn utilized to distinguish between (predictive) aleatoric and epistemic uncertainties, and to quantify them. The predictive set also produces either (i) a collection of outputs enjoying desirable probabilistic guarantees, or (ii) the single output that is deemed the best, that is, the one having the highest predictive lower probability -- another imprecise-probabilistic concept. CBDL is more robust than single BNNs to prior and likelihood misspecification, and to distribution shift. We show that CBDL is better at quantifying and disentangling different types of (predictive) uncertainties than single BNNs and ensemble of BNNs. In addition, we apply CBDL to two case studies to demonstrate its downstream tasks capabilities: one, for motion prediction in autonomous driving scenarios, and two, to model blood glucose and insulin dynamics for artificial pancreas control. We show that CBDL performs better when compared to an ensemble of BNNs baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09656v5</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Transaction of Machine Learning Research, 2024, ISSN: 2835-8856</arxiv:journal_reference>
      <dc:creator>Michele Caprio, Souradeep Dutta, Kuk Jin Jang, Vivian Lin, Radoslav Ivanov, Oleg Sokolsky, Insup Lee</dc:creator>
    </item>
    <item>
      <title>Neural Graph Revealers</title>
      <link>https://arxiv.org/abs/2302.13582</link>
      <description>arXiv:2302.13582v3 Announce Type: replace-cross 
Abstract: Sparse graph recovery methods work well where the data follows their assumptions but often they are not designed for doing downstream probabilistic queries. This limits their adoption to only identifying connections among the input variables. On the other hand, the Probabilistic Graphical Models (PGMs) assume an underlying base graph between variables and learns a distribution over them. PGM design choices are carefully made such that the inference \&amp; sampling algorithms are efficient. This brings in certain restrictions and often simplifying assumptions. In this work, we propose Neural Graph Revealers (NGRs), that are an attempt to efficiently merge the sparse graph recovery methods with PGMs into a single flow. The problem setting consists of an input data X with D features and M samples and the task is to recover a sparse graph showing connection between the features and jointly learn a probability distribution over them. NGRs view the neural networks as a `glass box' or more specifically as a multitask learning framework. We introduce `Graph-constrained path norm' that NGRs leverage to learn a graphical model that captures complex non-linear functional dependencies between the features in the form of an undirected sparse graph. Furthermore, NGRs can handle multimodal inputs like images, text, categorical data, embeddings etc. which is not straightforward to incorporate in the existing methods. We show experimental results of doing sparse graph recovery and probabilistic inference on data from Gaussian graphical models and a multimodal infant mortality dataset by Centers for Disease Control and Prevention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13582v3</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Harsh Shrivastava, Urszula Chajewska</dc:creator>
    </item>
    <item>
      <title>Universal approximation property of Banach space-valued random feature models including random neural networks</title>
      <link>https://arxiv.org/abs/2312.08410</link>
      <description>arXiv:2312.08410v3 Announce Type: replace-cross 
Abstract: We introduce a Banach space-valued extension of random feature learning, a data-driven supervised machine learning technique for large-scale kernel approximation. By randomly initializing the feature maps, only the linear readout needs to be trained, which reduces the computational complexity substantially. Viewing random feature models as Banach space-valued random variables, we prove a universal approximation result in the corresponding Bochner space. Moreover, we derive approximation rates and an explicit algorithm to learn an element of the given Banach space by such models. The framework of this paper includes random trigonometric/Fourier regression and in particular random neural networks which are single-hidden-layer feedforward neural networks whose weights and biases are randomly initialized, whence only the linear readout needs to be trained. For the latter, we can then lift the universal approximation property of deterministic neural networks to random neural networks, even within function spaces over non-compact domains, e.g., weighted spaces, $L^p$-spaces, and (weighted) Sobolev spaces, where the latter includes the approximation of the (weak) derivatives. In addition, we analyze when the training costs for approximating a given function grow polynomially in both the input/output dimension and the reciprocal of a pre-specified tolerated approximation error. Furthermore, we demonstrate in a numerical example the empirical advantages of random feature models over their deterministic counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08410v3</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Philipp Schmocker</dc:creator>
    </item>
    <item>
      <title>Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays</title>
      <link>https://arxiv.org/abs/2402.17704</link>
      <description>arXiv:2402.17704v2 Announce Type: replace-cross 
Abstract: With the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences. Often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization. This paper presents a transfer learning design of experiments workflow to make this development feasible. By combining a transfer learning surrogate model with Bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks. We demonstrate the reduction in the number of experiments using data from the development of DNA competitors for use in an amplification-based diagnostic assay. We use cross-validation to compare the predictive accuracy of different transfer learning models, and then compare the performance of the models for both single objective and penalized optimization tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17704v2</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruby Sedgwick, John P. Goertz, Molly M. Stevens, Ruth Misener, Mark van der Wilk</dc:creator>
    </item>
    <item>
      <title>Crowdsourcing with Difficulty: A Bayesian Rating Model for Heterogeneous Items</title>
      <link>https://arxiv.org/abs/2405.19521</link>
      <description>arXiv:2405.19521v2 Announce Type: replace-cross 
Abstract: In applied statistics and machine learning, the "gold standards" used for training are often biased and almost always noisy. Dawid and Skene's justifiably popular crowdsourcing model adjusts for rater (coder, annotator) sensitivity and specificity, but fails to capture distributional properties of rating data gathered for training, which in turn biases training. In this study, we introduce a general purpose measurement-error model with which we can infer consensus categories by adding item-level effects for difficulty, discriminativeness, and guessability. We further show how to constrain the bimodal posterior of these models to avoid (or if necessary, allow) adversarial raters. We validate our model's goodness of fit with posterior predictive checks, the Bayesian analogue of $\chi^2$ tests. Dawid and Skene's model is rejected by goodness of fit tests, whereas our new model, which adjusts for item heterogeneity, is not rejected. We illustrate our new model with two well-studied data sets, binary rating data for caries in dental X-rays and implication in natural language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19521v2</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seong Woo Han, Ozan Ad{\i}g\"uzel, Bob Carpenter</dc:creator>
    </item>
    <item>
      <title>Temporal Graph Rewiring with Expander Graphs</title>
      <link>https://arxiv.org/abs/2406.02362</link>
      <description>arXiv:2406.02362v3 Announce Type: replace-cross 
Abstract: Evolving relations in real-world networks are often modelled by temporal graphs. Temporal Graph Neural Networks (TGNNs) emerged to model evolutionary behaviour of such graphs by leveraging the message passing primitive at the core of Graph Neural Networks (GNNs). It is well-known that GNNs are vulnerable to several issues directly related to the input graph topology, such as under-reaching and over-squashing - we argue that these issues can often get exacerbated in temporal graphs, particularly as the result of stale nodes and edges. While graph rewiring techniques have seen frequent usage in GNNs to make the graph topology more favourable for message passing, they have not seen any mainstream usage on TGNNs. In this work, we propose Temporal Graph Rewiring (TGR), the first approach for graph rewiring on temporal graphs, to the best of our knowledge. TGR constructs message passing highways between temporally distant nodes in a continuous-time dynamic graph by utilizing expander graph propagation, a prominent framework used for graph rewiring on static graphs which makes minimal assumptions on the underlying graph structure. On the challenging TGB benchmark, TGR achieves state-of-the-art results on tgbl-review, tgbl-coin, tgbl-comment and tgbl-flight datasets at the time of writing. For tgbl-review, TGR has 50.5% improvement in MRR over the base TGN model and 22.2% improvement over the base TNCN model. The significant improvement over base models demonstrates clear benefits of temporal graph rewiring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02362v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katarina Petrovi\'c, Shenyang Huang, Farimah Poursafaei, Petar Veli\v{c}kovi\'c</dc:creator>
    </item>
    <item>
      <title>Treeffuser: Probabilistic Predictions via Conditional Diffusions with Gradient-Boosted Trees</title>
      <link>https://arxiv.org/abs/2406.07658</link>
      <description>arXiv:2406.07658v2 Announce Type: replace-cross 
Abstract: Probabilistic prediction aims to compute predictive distributions rather than single point predictions. These distributions enable practitioners to quantify uncertainty, compute risk, and detect outliers. However, most probabilistic methods assume parametric responses, such as Gaussian or Poisson distributions. When these assumptions fail, such models lead to bad predictions and poorly calibrated uncertainty. In this paper, we propose Treeffuser, an easy-to-use method for probabilistic prediction on tabular data. The idea is to learn a conditional diffusion model where the score function is estimated using gradient-boosted trees. The conditional diffusion model makes Treeffuser flexible and non-parametric, while the gradient-boosted trees make it robust and easy to train on CPUs. Treeffuser learns well-calibrated predictive distributions and can handle a wide range of regression tasks -- including those with multivariate, multimodal, and skewed responses. We study Treeffuser on synthetic and real data and show that it outperforms existing methods, providing better calibrated probabilistic predictions. We further demonstrate its versatility with an application to inventory allocation under uncertainty using sales data from Walmart. We implement Treeffuser in https://github.com/blei-lab/treeffuser.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07658v2</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Beltran-Velez, Alessandro Antonio Grande, Achille Nazaret, Alp Kucukelbir, David Blei</dc:creator>
    </item>
    <item>
      <title>Towards Domain Adaptive Neural Contextual Bandits</title>
      <link>https://arxiv.org/abs/2406.09564</link>
      <description>arXiv:2406.09564v2 Announce Type: replace-cross 
Abstract: Contextual bandit algorithms are essential for solving real-world decision making problems. In practice, collecting a contextual bandit's feedback from different domains may involve different costs. For example, measuring drug reaction from mice (as a source domain) and humans (as a target domain). Unfortunately, adapting a contextual bandit algorithm from a source domain to a target domain with distribution shift still remains a major challenge and largely unexplored. In this paper, we introduce the first general domain adaptation method for contextual bandits. Our approach learns a bandit model for the target domain by collecting feedback from the source domain. Our theoretical analysis shows that our algorithm maintains a sub-linear regret bound even adapting across domains. Empirical results show that our approach outperforms the state-of-the-art contextual bandit algorithms on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09564v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyan Wang, Xiaoming Huo, Hao Wang</dc:creator>
    </item>
    <item>
      <title>On metric choice in dimension reduction for Fr\'echet regression</title>
      <link>https://arxiv.org/abs/2410.01783</link>
      <description>arXiv:2410.01783v2 Announce Type: replace-cross 
Abstract: Fr\'echet regression is becoming a mainstay in modern data analysis for analyzing non-traditional data types belonging to general metric spaces. This novel regression method is especially useful in the analysis of complex health data such as continuous monitoring and imaging data. Fr\'echet regression utilizes the pairwise distances between the random objects, which makes the choice of metric crucial in the estimation. In this paper, existing dimension reduction methods for Fr\'echet regression are reviewed, and the effect of metric choice on the estimation of the dimension reduction subspace is explored for the regression between random responses and Euclidean predictors. Extensive numerical studies illustrate how different metrics affect the central and central mean space estimators. Two real applications involving analysis of brain connectivity networks of subjects with and without Parkinson's disease and an analysis of the distributions of glycaemia based on continuous glucose monitoring data are provided, to demonstrate how metric choice can influence findings in real applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01783v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul-Nasah Soale, Congli Ma, Siyu Chen, Obed Koomson</dc:creator>
    </item>
    <item>
      <title>On Divergence Measures for Training GFlowNets</title>
      <link>https://arxiv.org/abs/2410.09355</link>
      <description>arXiv:2410.09355v2 Announce Type: replace-cross 
Abstract: Generative Flow Networks (GFlowNets) are amortized inference models designed to sample from unnormalized distributions over composable objects, with applications in generative modeling for tasks in fields such as causal discovery, NLP, and drug discovery. Traditionally, the training procedure for GFlowNets seeks to minimize the expected log-squared difference between a proposal (forward policy) and a target (backward policy) distribution, which enforces certain flow-matching conditions. While this training procedure is closely related to variational inference (VI), directly attempting standard Kullback-Leibler (KL) divergence minimization can lead to proven biased and potentially high-variance estimators. Therefore, we first review four divergence measures, namely, Renyi-$\alpha$'s, Tsallis-$\alpha$'s, reverse and forward KL's, and design statistically efficient estimators for their stochastic gradients in the context of training GFlowNets. Then, we verify that properly minimizing these divergences yields a provably correct and empirically effective training scheme, often leading to significantly faster convergence than previously proposed optimization. To achieve this, we design control variates based on the REINFORCE leave-one-out and score-matching estimators to reduce the variance of the learning objectives' gradients. Our work contributes by narrowing the gap between GFlowNets training and generalized variational approximations, paving the way for algorithmic ideas informed by the divergence minimization viewpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09355v2</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiago da Silva, Eliezer de Souza da Silva, Diego Mesquita</dc:creator>
    </item>
    <item>
      <title>A Bayesian Framework for Clustered Federated Learning</title>
      <link>https://arxiv.org/abs/2410.15473</link>
      <description>arXiv:2410.15473v2 Announce Type: replace-cross 
Abstract: One of the main challenges of federated learning (FL) is handling non-independent and identically distributed (non-IID) client data, which may occur in practice due to unbalanced datasets and use of different data sources across clients. Knowledge sharing and model personalization are key strategies for addressing this issue. Clustered federated learning is a class of FL methods that groups clients that observe similarly distributed data into clusters, such that every client is typically associated with one data distribution and participates in training a model for that distribution along their cluster peers. In this paper, we present a unified Bayesian framework for clustered FL which associates clients to clusters. Then we propose several practical algorithms to handle the, otherwise growing, data associations in a way that trades off performance and computational complexity. This work provides insights on client-cluster associations and enables client knowledge sharing in new ways. The proposed framework circumvents the need for unique client-cluster associations, which is seen to increase the performance of the resulting models in a variety of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15473v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Wu, Tales Imbiriba, Pau Closas</dc:creator>
    </item>
    <item>
      <title>Diverse Policies Recovering via Pointwise Mutual Information Weighted Imitation Learning</title>
      <link>https://arxiv.org/abs/2410.15910</link>
      <description>arXiv:2410.15910v2 Announce Type: replace-cross 
Abstract: Recovering a spectrum of diverse policies from a set of expert trajectories is an important research topic in imitation learning. After determining a latent style for a trajectory, previous diverse policies recovering methods usually employ a vanilla behavioral cloning learning objective conditioned on the latent style, treating each state-action pair in the trajectory with equal importance. Based on an observation that in many scenarios, behavioral styles are often highly relevant with only a subset of state-action pairs, this paper presents a new principled method in diverse polices recovery. In particular, after inferring or assigning a latent style for a trajectory, we enhance the vanilla behavioral cloning by incorporating a weighting mechanism based on pointwise mutual information. This additional weighting reflects the significance of each state-action pair's contribution to learning the style, thus allowing our method to focus on state-action pairs most representative of that style. We provide theoretical justifications for our new objective, and extensive empirical evaluations confirm the effectiveness of our method in recovering diverse policies from expert data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15910v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanlin Yang, Jian Yao, Weiming Liu, Qing Wang, Hanmin Qin, Hansheng Kong, Kirk Tang, Jiechao Xiong, Chao Yu, Kai Li, Junliang Xing, Hongwu Chen, Juchao Zhuo, Qiang Fu, Yang Wei, Haobo Fu</dc:creator>
    </item>
    <item>
      <title>ExDBN: Exact learning of Dynamic Bayesian Networks</title>
      <link>https://arxiv.org/abs/2410.16100</link>
      <description>arXiv:2410.16100v2 Announce Type: replace-cross 
Abstract: Causal learning from data has received much attention in recent years. One way of capturing causal relationships is by utilizing Bayesian networks. There, one recovers a weighted directed acyclic graph, in which random variables are represented by vertices, and the weights associated with each edge represent the strengths of the causal relationships between them. This concept is extended to capture dynamic effects by introducing a dependency on past data, which may be captured by the structural equation model, which is utilized in the present contribution to formulate a score-based learning approach. A mixed-integer quadratic program is formulated and an algorithmic solution proposed, in which the pre-generation of exponentially many acyclicity constraints is avoided by utilizing the so-called branch-and-cut ("lazy constraint") method. Comparing the novel approach to the state of the art, we show that the proposed approach turns out to produce excellent results when applied to small and medium-sized synthetic instances of up to 25 time-series. Lastly, two interesting applications in bio-science and finance, to which the method is directly applied, further stress the opportunities in developing highly accurate, globally convergent solvers that can handle modest instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16100v2</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavel Rytir, Ales Wodecki, Georgios Korpas, Jakub Marecek</dc:creator>
    </item>
  </channel>
</rss>
