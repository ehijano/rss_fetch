<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Oct 2024 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Using Normalization to Improve SMT Solver Stability</title>
      <link>https://arxiv.org/abs/2410.22419</link>
      <description>arXiv:2410.22419v1 Announce Type: new 
Abstract: In many applications, SMT solvers are used to solve similar or identical tasks over time. When the performance of the solver varies significantly despite only small changes, this leads to frustration for users. This has been called the stability problem, and it represents an important usability challenge for SMT solvers. In this paper, we introduce an approach for mitigating the stability problem based on normalizing solver inputs. We show that a perfect normalizing algorithm exists but is computationally expensive. We then describe an approximate algorithm and evaluate it on a set of benchmarks from related work, as well as a large set of benchmarks sampled from SMT-LIB. Our evaluation shows that our approximate normalizer reduces runtime variability with minimal overhead and is able to normalize a large class of mutated benchmarks to a unique normal form.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22419v1</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daneshvar Amrollahi, Mathias Preiner, Aina Niemetz, Andrew Reynolds, Moses Charikar, Cesare Tinelli, Clark Barrett</dc:creator>
    </item>
    <item>
      <title>A Demonic Outcome Logic for Randomized Nondeterminism</title>
      <link>https://arxiv.org/abs/2410.22540</link>
      <description>arXiv:2410.22540v1 Announce Type: new 
Abstract: Programs increasingly rely on randomization in applications such as cryptography and machine learning. Analyzing randomized programs has been a fruitful research direction, but there is a gap when programs also exploit nondeterminism (for concurrency, efficiency, or algorithmic design). In this paper, we introduce Demonic Outcome Logic for reasoning about programs that exploit both randomization and nondeterminism. The logic includes several novel features, such as reasoning about multiple executions in tandem and manipulating pre- and postconditions using familiar equational laws -- including the distributive law of probabilistic choices over nondeterministic ones. We also give rules for loops that both establish termination and quantify the distribution of final outcomes from a single premise. We illustrate the reasoning capabilities of Demonic Outcome Logic through several case studies, including the Monty Hall problem, an adversarial protocol for simulating fair coins, and a heuristic based probabilistic SAT solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22540v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noam Zilberstein, Dexter Kozen, Alexandra Silva, Joseph Tassarotti</dc:creator>
    </item>
    <item>
      <title>Reactive Synthesis for Expected Impacts</title>
      <link>https://arxiv.org/abs/2410.22760</link>
      <description>arXiv:2410.22760v1 Announce Type: new 
Abstract: As business processes become increasingly complex,  effectively modeling decision points, their likelihood,  and resource consumption is crucial for optimizing operations.  To address this challenge, this paper introduces a formal  extension of the Business Process Model and Notation (BPMN)  that incorporates choices, probabilities, and impacts,  referred to as BPMN+CPI. This extension is motivated  by the growing emphasis on precise control within  business process management, where carefully  selecting decision pathways in repeated instances  is crucial for conforming to certain standards of multiple resource consumption and environmental impacts.  In this context we deal with the problem of synthesizing a  strategy (if any) that guarantees that the expected impacts on repeated execution of the input process  are below a given threshold.  We show that this problem belongs to   PSPACE complexity class; moreover we provide an effective procedure  for computing a strategy (if present).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22760v1</guid>
      <category>cs.LO</category>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.409.7</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 409, 2024, pp. 35-52</arxiv:journal_reference>
      <dc:creator>Emanuele Chini (University "La Sapienza" Rome), Pietro Sala (University of Verona), Andrea Simonetti (University of Verona), Omid Zare (University of Verona)</dc:creator>
    </item>
    <item>
      <title>Epistemic Skills: Logical Dynamics of Knowing and Forgetting</title>
      <link>https://arxiv.org/abs/2410.22763</link>
      <description>arXiv:2410.22763v1 Announce Type: new 
Abstract: We present a type of epistemic logics that encapsulates both the dynamics of acquiring knowledge (knowing) and losing information (forgetting), alongside the integration of group knowledge concepts. Our approach is underpinned by a system of weighted models, which introduces an "epistemic skills" metric to effectively represent the epistemic abilities associated with knowledge update. In this framework, the acquisition of knowledge is modeled as a result of upskilling, whereas forgetting is by downskilling. Additionally, our framework allows us to explore the concept of "knowability," which can be defined as the potential to acquire knowledge through upskilling, and facilitates a nuanced understanding of the distinctions between epistemic de re and de dicto expressions. We study the computational complexity of model checking problems for these logics, providing insights into both the theoretical underpinnings and practical implications of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22763v1</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.409.12</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 409, 2024, pp. 120-137</arxiv:journal_reference>
      <dc:creator>Xiaolong Liang (Shanxi University), Y\`i N. W\'ang (Sun Yat-sen University)</dc:creator>
    </item>
    <item>
      <title>Proceedings Eighth Symposium on Working Formal Methods</title>
      <link>https://arxiv.org/abs/2410.23020</link>
      <description>arXiv:2410.23020v1 Announce Type: new 
Abstract: The Working Formal Methods Symposium (FROM) is a series of workshops that aim to bring together researchers and practitioners who work on formal methods by contributing new theoretical results, methods, techniques, and frameworks, and/or by creating or using software tools that apply theoretical contributions.
</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23020v1</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.410</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 410, 2024</arxiv:journal_reference>
      <dc:creator>Mircea Marin (West University of Timi\c{s}oara), Lauren\c{t}iu Leu\c{s}tean (University of Bucharest)</dc:creator>
    </item>
    <item>
      <title>On Dialectica and Differentiation, via Categories</title>
      <link>https://arxiv.org/abs/2410.22494</link>
      <description>arXiv:2410.22494v1 Announce Type: cross 
Abstract: Godel's Dialectica has been introduced and developed as a logical transformation. Only recently has it been related with the, a priori unrelated, notion of differentiation: we can now read it as a differentiable program transformation. Building on that, we formulate the relation between these two notions categorically, in the framework of differential categories. Moreover, we study the relation between differential categories and Dialectica categories. We do this by taking the point of view of fibrations and (dependent) lenses, which allows us to keep a geometrical intuition in mind by considering reverse tangent categories. The viewpoint we propose opens many interesting further developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22494v1</guid>
      <category>math.CT</category>
      <category>cs.LO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Barbarossa</dc:creator>
    </item>
    <item>
      <title>An Evaluation of Massively Parallel Algorithms for DFA Minimization</title>
      <link>https://arxiv.org/abs/2410.22764</link>
      <description>arXiv:2410.22764v1 Announce Type: cross 
Abstract: We study parallel algorithms for the minimization of Deterministic Finite Automata (DFAs). In particular, we implement four different massively parallel algorithms on Graphics Processing Units (GPUs). Our results confirm the expectations that the algorithm with the theoretically best time complexity is not practically suitable to run on GPUs due to the large amount of resources needed. We empirically verify that parallel partition refinement algorithms from the literature perform better in practice, even though their time complexity is worse. Lastly, we introduce a novel algorithm based on partition refinement with an extra parallel partial transitive closure step and show that on specific benchmarks it has better run-time complexity and performs better in practice.    
</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22764v1</guid>
      <category>cs.DC</category>
      <category>cs.LO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.409.13</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 409, 2024, pp. 138-153</arxiv:journal_reference>
      <dc:creator>Jan Martens, Anton Wijs</dc:creator>
    </item>
    <item>
      <title>Investigations into Proof Structures</title>
      <link>https://arxiv.org/abs/2304.12827</link>
      <description>arXiv:2304.12827v3 Announce Type: replace 
Abstract: We introduce and elaborate a novel formalism for the manipulation and analysis of proofs as objects in a global manner. In this first approach the formalism is restricted to first-order problems characterized by condensed detachment. It is applied in an exemplary manner to a coherent and comprehensive formal reconstruction and analysis of historical proofs of a widely-studied problem due to {\L}ukasiewicz. The underlying approach opens the door towards new systematic ways of generating lemmas in the course of proof search to the effects of reducing the search effort and finding shorter proofs. Among the numerous reported experiments along this line, a proof of {\L}ukasiewicz's problem was automatically discovered that is much shorter than any proof found before by man or machine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12827v3</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10817-024-09711-8</arxiv:DOI>
      <dc:creator>Christoph Wernhard, Wolfgang Bibel</dc:creator>
    </item>
    <item>
      <title>Modular assurance of an Autonomous Ferry using Contract-Based Design and Simulation-based Verification Principles</title>
      <link>https://arxiv.org/abs/2408.03244</link>
      <description>arXiv:2408.03244v2 Announce Type: replace 
Abstract: With the introduction of autonomous technology into our society, e.g. autonomous shipping, it is important to assess and assure the safety of autonomous systems in a real-world context. Simulation-based testing is a common approach to attempt to verify performance of autonomous systems, but assurance also requires formal evidence. This paper introduces the Assurance of Digital Assets (ADA) framework, a structured method for the assurance of digital assets, i.e. novel, complex, or intelligent systems enabled by digital technologies, using contract-based design. Results are shown for an autonomous ferry assurance case, focusing on collision avoidance during the ferry's transit. Further, we discuss the role of simulation-based testing in verifying compliance to contract specifications, to build the necessary evidence for an assurance case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03244v2</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/1742-6596/2867/1/012043</arxiv:DOI>
      <dc:creator>Jon Arne Glomsrud, Stephanie Kemna, Chanjei Vasanthan, Luman Zhao, Dag McGeorge, Tom Arne Pedersen, Tobias Rye Torben, B{\o}rge Rokseth, Dong Trong Nguyen</dc:creator>
    </item>
    <item>
      <title>Unifying Model Execution and Deductive Verification with Interaction Trees in Isabelle/HOL</title>
      <link>https://arxiv.org/abs/2408.15817</link>
      <description>arXiv:2408.15817v2 Announce Type: replace 
Abstract: Model execution allows us to prototype and analyse software engineering models by stepping through their possible behaviours, using techniques like animation and simulation. On the other hand, deductive verification allows us to construct formal proofs demonstrating satisfaction of certain critical properties in support of high-assurance software engineering. To ensure coherent results between execution and proof, we need unifying semantics and automation. In this paper, we mechanise Interaction Trees (ITrees) in Isabelle/HOL to produce an execution and verification framework. ITrees are coinductive structures that allow us to encode infinite labelled transition systems, yet they are inherently executable. We use ITrees to create verification tools for stateful imperative programs, concurrent programs with message passing in the form of the CSP and \Circus languages, and abstract system models in the style of the Z and B methods. We demonstrate how ITrees can account for diverse semantic presentations, such as structural operational semantics, a relational program model, and CSP's failures-divergences trace model. Finally, we demonstrate how ITrees can be executed using the Isabelle code generator to support the animation of models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15817v2</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Foster, Chung-Kil Hur, Jim Woodcock</dc:creator>
    </item>
    <item>
      <title>Masked Hard-Attention Transformers Recognize Exactly the Star-Free Languages</title>
      <link>https://arxiv.org/abs/2310.13897</link>
      <description>arXiv:2310.13897v4 Announce Type: replace-cross 
Abstract: The expressive power of transformers over inputs of unbounded size can be studied through their ability to recognize classes of formal languages. In this paper, we establish exact characterizations of transformers with hard attention (in which all attention is focused on exactly one position) and attention masking (in which each position only attends to positions on one side). With strict masking (each position cannot attend to itself) and without position embeddings, these transformers are expressively equivalent to linear temporal logic (LTL), which defines exactly the star-free languages. A key technique is the use of Boolean RASP as a convenient intermediate language between transformers and LTL. We then take numerous results known for LTL and apply them to transformers, showing how position embeddings, strict masking, and depth all increase expressive power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13897v4</guid>
      <category>cs.FL</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andy Yang, David Chiang, Dana Angluin</dc:creator>
    </item>
    <item>
      <title>Transductive Learning Is Compact</title>
      <link>https://arxiv.org/abs/2402.10360</link>
      <description>arXiv:2402.10360v3 Announce Type: replace-cross 
Abstract: We demonstrate a compactness result holding broadly across supervised learning with a general class of loss functions: Any hypothesis class $H$ is learnable with transductive sample complexity $m$ precisely when all of its finite projections are learnable with sample complexity $m$. We prove that this exact form of compactness holds for realizable and agnostic learning with respect to any proper metric loss function (e.g., any norm on $\mathbb{R}^d$) and any continuous loss on a compact space (e.g., cross-entropy, squared loss). For realizable learning with improper metric losses, we show that exact compactness of sample complexity can fail, and provide matching upper and lower bounds of a factor of 2 on the extent to which such sample complexities can differ. We conjecture that larger gaps are possible for the agnostic case. Furthermore, invoking the equivalence between sample complexities in the PAC and transductive models (up to lower order factors, in the realizable case) permits us to directly port our results to the PAC model, revealing an almost-exact form of compactness holding broadly in PAC learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10360v3</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan, Shang-Hua Teng</dc:creator>
    </item>
    <item>
      <title>Compositional imprecise probability</title>
      <link>https://arxiv.org/abs/2405.09391</link>
      <description>arXiv:2405.09391v2 Announce Type: replace-cross 
Abstract: Imprecise probability is concerned with uncertainty about which probability distributions to use. It has applications in robust statistics and machine learning.
  We look at programming language models for imprecise probability. Our desiderata are that we would like our model to support all kinds of composition, categorical and monoidal; in other words, guided by dataflow diagrams. Another equivalent perspective is that we would like a model of synthetic probability in the sense of Markov categories.
  Imprecise probability can be modelled in various ways, with the leading monad-based approach using convex sets of probability distributions. This model is not fully compositional because the monad involved is not commutative, meaning it does not have a proper monoidal structure. In this work, we provide a new fully compositional account. The key idea is to name the non-deterministic choices. To manage the renamings and disjointness of names, we use graded monads. We show that the resulting compositional model is maximal and relate it with the earlier monadic approach, proving that we obtain tighter bounds on the uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09391v2</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <category>math.CT</category>
      <category>math.PR</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jack Liell-Cock, Sam Staton</dc:creator>
    </item>
    <item>
      <title>LeanAgent: Lifelong Learning for Formal Theorem Proving</title>
      <link>https://arxiv.org/abs/2410.06209</link>
      <description>arXiv:2410.06209v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have been successful in mathematical reasoning tasks such as formal theorem proving when integrated with interactive proof assistants like Lean. Existing approaches involve training or fine-tuning an LLM on a specific dataset to perform well on particular domains, such as undergraduate-level mathematics. These methods struggle with generalizability to advanced mathematics. A fundamental limitation is that these approaches operate on static domains, failing to capture how mathematicians often work across multiple domains and projects simultaneously or cyclically. We present LeanAgent, a novel lifelong learning framework for theorem proving that continuously generalizes to and improves on ever-expanding mathematical knowledge without forgetting previously learned knowledge. LeanAgent introduces several key innovations, including a curriculum learning strategy that optimizes the learning trajectory in terms of mathematical difficulty, a dynamic database for efficient management of evolving mathematical knowledge, and progressive training to balance stability and plasticity. LeanAgent successfully proves 162 theorems previously unproved by humans across 23 diverse Lean repositories, many from advanced mathematics. It performs significantly better than the static LLM baseline, proving challenging theorems in domains like abstract algebra and algebraic topology while showcasing a clear progression of learning from basic concepts to advanced topics. In addition, we analyze LeanAgent's superior performance on key lifelong learning metrics. LeanAgent achieves exceptional scores in stability and backward transfer, where learning new tasks improves performance on previously learned tasks. This emphasizes LeanAgent's continuous generalizability and improvement, explaining its superior theorem-proving performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06209v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, Anima Anandkumar</dc:creator>
    </item>
    <item>
      <title>Effective weak convergence and tightness of measures in computable Polish spaces</title>
      <link>https://arxiv.org/abs/2410.21609</link>
      <description>arXiv:2410.21609v2 Announce Type: replace-cross 
Abstract: Prokhorov's Theorem in probability theory states that a family $\Gamma$ of probability measures on a Polish space is tight if and only if every sequence in $\Gamma$ has a weakly convergent subsequence. Due to the highly non-constructive nature of (relative) sequential compactness, however, the effective content of this theorem has not been studied. To this end, we generalize the effective notions of weak convergence of measures on the real line due to McNicholl and Rojas to computable Polish spaces. Then, we introduce an effective notion of tightness for families of measures on computable Polish spaces. Finally, we prove an effective version of Prokhorov's Theorem for computable sequences of probability measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21609v2</guid>
      <category>math.LO</category>
      <category>cs.LO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diego A. Rojas</dc:creator>
    </item>
  </channel>
</rss>
