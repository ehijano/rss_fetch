<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Apr 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Dr Web: a modern, query-based web data retrieval engine</title>
      <link>https://arxiv.org/abs/2504.05311</link>
      <description>arXiv:2504.05311v1 Announce Type: new 
Abstract: This article introduces the Data Retrieval Web Engine (also referred to as doctor web), a flexible and modular tool for extracting structured data from web pages using a simple query language. We discuss the engineering challenges addressed during its development, such as dynamic content handling and messy data extraction. Furthermore, we cover the steps for making the DR Web Engine public, highlighting its open source potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05311v1</guid>
      <category>cs.DB</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ylli Prifti, Alessandro Provetti, Pasquale de Meo</dc:creator>
    </item>
    <item>
      <title>MicroNN: An On-device Disk-resident Updatable Vector Database</title>
      <link>https://arxiv.org/abs/2504.05573</link>
      <description>arXiv:2504.05573v1 Announce Type: new 
Abstract: Nearest neighbour search over dense vector collections has important applications in information retrieval, retrieval augmented generation (RAG), and content ranking. Performing efficient search over large vector collections is a well studied problem with many existing approaches and open source implementations. However, most state-of-the-art systems are generally targeted towards scenarios using large servers with an abundance of memory, static vector collections that are not updatable, and nearest neighbour search in isolation of other search criteria. We present Micro Nearest Neighbour (MicroNN), an embedded nearest-neighbour vector search engine designed for scalable similarity search in low-resource environments. MicroNN addresses the problem of on-device vector search for real-world workloads containing updates and hybrid search queries that combine nearest neighbour search with structured attribute filters. In this scenario, memory is highly constrained and disk-efficient index structures and algorithms are required, as well as support for continuous inserts and deletes. MicroNN is an embeddable library that can scale to large vector collections with minimal resources. MicroNN is used in production and powers a wide range of vector search use-cases on-device. MicroNN takes less than 7 ms to retrieve the top-100 nearest neighbours with 90% recall on publicly available million-scale vector benchmark while using ~10 MB of memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05573v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Pound, Floris Chabert, Arjun Bhushan, Ankur Goswami, Anil Pacaci, Shihabur Rahman Chowdhury</dc:creator>
    </item>
    <item>
      <title>Simplifying Data Integration: SLM-Driven Systems for Unified Semantic Queries Across Heterogeneous Databases</title>
      <link>https://arxiv.org/abs/2504.05634</link>
      <description>arXiv:2504.05634v1 Announce Type: new 
Abstract: The integration of heterogeneous databases into a unified querying framework remains a critical challenge, particularly in resource-constrained environments. This paper presents a novel Small Language Model(SLM)-driven system that synergizes advancements in lightweight Retrieval-Augmented Generation (RAG) and semantic-aware data structuring to enable efficient, accurate, and scalable query resolution across diverse data formats. By integrating MiniRAG's semantic-aware heterogeneous graph indexing and topology-enhanced retrieval with SLM-powered structured data extraction, our system addresses the limitations of traditional methods in handling Multi-Entity Question Answering (Multi-Entity QA) and complex semantic queries. Experimental results demonstrate superior performance in accuracy and efficiency, while the introduction of semantic entropy as an unsupervised evaluation metric provides robust insights into model uncertainty. This work pioneers a cost-effective, domain-agnostic solution for next-generation database systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05634v1</guid>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teng Lin</dc:creator>
    </item>
    <item>
      <title>ZeroED: Hybrid Zero-shot Error Detection through Large Language Model Reasoning</title>
      <link>https://arxiv.org/abs/2504.05345</link>
      <description>arXiv:2504.05345v1 Announce Type: cross 
Abstract: Error detection (ED) in tabular data is crucial yet challenging due to diverse error types and the need for contextual understanding. Traditional ED methods often rely heavily on manual criteria and labels, making them labor-intensive. Large language models (LLM) can minimize human effort but struggle with errors requiring a comprehensive understanding of data context. In this paper, we propose ZeroED, a novel hybrid zero-shot error detection framework, which combines LLM reasoning ability with the manual label-based ED pipeline. ZeroED operates in four steps, i.e., feature representation, error labeling, training data construction, and detector training. Initially, to enhance error distinction, ZeroED generates rich data representations using error reason-aware binary features, pre-trained embeddings, and statistical features. Then, ZeroED employs LLM to label errors holistically through in-context learning, guided by a two-step reasoning process for detailed error detection guidelines. To reduce token costs, LLMs are applied only to representative data selected via clustering-based sampling. High-quality training data is constructed through in-cluster label propagation and LLM augmentation with verification. Finally, a classifier is trained to detect all errors. Extensive experiments on seven public datasets demonstrate that, ZeroED substantially outperforms state-of-the-art methods by a maximum 30% improvement in F1 score and up to 90% token cost reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05345v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Ni, Kaihang Zhang, Xiaoye Miao, Xiangyu Zhao, Yangyang Wu, Yaoshu Wang, Jianwei Yin</dc:creator>
    </item>
    <item>
      <title>A Comparative Analysis of Modeling Approaches for the Association of FAIR Digital Objects Operations</title>
      <link>https://arxiv.org/abs/2504.05361</link>
      <description>arXiv:2504.05361v1 Announce Type: cross 
Abstract: The concept of FAIR Digital Objects represents a foundational step towards realizing machine-actionable, interoperable data infrastructures across scientific and industrial domains. As digital spaces become increasingly heterogeneous, scalable mechanisms for data processing and interpretability are essential. This paper provides a comparative analysis of various typing mechanisms to associate FAIR Digital Objects with their operations, addressing the pressing need for a structured approach to manage data interactions within the FAIR Digital Objects ecosystem. By examining three core models -- record typing, profile typing, and attribute typing -- this work evaluates each model's complexity, flexibility, versatility, and interoperability, shedding light on their strengths and limitations. With this assessment, we aim to offer insights for adopting FDO frameworks that enhance data automation and promote the seamless exchange of digital resources across domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05361v1</guid>
      <category>cs.DL</category>
      <category>cs.DB</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Blumenr\"ohr, Jana B\"ohm, Philipp Ost, Marco Kul\"uke, Peter Wittenburg, Christophe Blanchi, Sven Bingert, Ulrich Schwardmann</dc:creator>
    </item>
    <item>
      <title>dpBento: Benchmarking DPUs for Data Processing</title>
      <link>https://arxiv.org/abs/2504.05536</link>
      <description>arXiv:2504.05536v1 Announce Type: cross 
Abstract: Data processing units (DPUs, SoC-based SmartNICs) are emerging data center hardware that provide opportunities to address cloud data processing challenges. Their onboard compute, memory, network, and auxiliary storage can be leveraged to offload a variety of data processing tasks. Although recent work shows promising benefits of DPU offloading for specific operations, a comprehensive view of the implications of DPUs for data processing is missing. Benchmarking can help, but existing benchmark tools lack the focus on data processing and are limited to specific DPUs. In this paper, we present dpBento, a benchmark suite that aims to uncover the performance characteristics of different DPU resources and different DPUs, and the performance implications of offloading a wide range of data processing operations and systems to DPUs. It provides an abstraction for automated performance testing and reporting and is easily extensible. We use dpBento to measure recent DPUs, present our benchmarking results, and highlight insights into the potential benefits of DPU offloading for data processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05536v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiasheng Hu, Chihan Cui, Anna Li, Raahil Vora, Yuanfan Chen, Philip A. Bernstein, Jialin Li, Qizhen Zhang</dc:creator>
    </item>
    <item>
      <title>Technical Report: Full Version of Analyzing and Optimizing Perturbation of DP-SGD Geometrically</title>
      <link>https://arxiv.org/abs/2504.05618</link>
      <description>arXiv:2504.05618v1 Announce Type: cross 
Abstract: Differential privacy (DP) has become a prevalent privacy model in a wide range of machine learning tasks, especially after the debut of DP-SGD. However, DP-SGD, which directly perturbs gradients in the training iterations, fails to mitigate the negative impacts of noise on gradient direction. As a result, DP-SGD is often inefficient. Although various solutions (e.g., clipping to reduce the sensitivity of gradients and amplifying privacy bounds to save privacy budgets) are proposed to trade privacy for model efficiency, the root cause of its inefficiency is yet unveiled.
  In this work, we first generalize DP-SGD and theoretically derive the impact of DP noise on the training process. Our analysis reveals that, in terms of a perturbed gradient, only the noise on direction has eminent impact on the model efficiency while that on magnitude can be mitigated by optimization techniques, i.e., fine-tuning gradient clipping and learning rate. Besides, we confirm that traditional DP introduces biased noise on the direction when adding unbiased noise to the gradient itself. Overall, the perturbation of DP-SGD is actually sub-optimal from a geometric perspective. Motivated by this, we design a geometric perturbation strategy GeoDP within the DP framework, which perturbs the direction and the magnitude of a gradient, respectively. By directly reducing the noise on the direction, GeoDP mitigates the negative impact of DP noise on model efficiency with the same DP guarantee. Extensive experiments on two public datasets (i.e., MNIST and CIFAR-10), one synthetic dataset and three prevalent models (i.e., Logistic Regression, CNN and ResNet) confirm the effectiveness and generality of our strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05618v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.DB</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Conference of Data Engineering (ICDE 2025)</arxiv:journal_reference>
      <dc:creator>Jiawei Duan, Haibo Hu, Qingqing Ye, Xinyue Sun</dc:creator>
    </item>
    <item>
      <title>Indexing Strings with Utilities</title>
      <link>https://arxiv.org/abs/2504.05917</link>
      <description>arXiv:2504.05917v1 Announce Type: cross 
Abstract: Applications in domains ranging from bioinformatics to advertising feature strings that come with numerical scores (utilities). The utilities quantify the importance, interest, profit, or risk of the letters occurring at every position of a string. Motivated by the ever-increasing rate of generating such data, as well as by their importance in several domains, we introduce Useful String Indexing (USI), a natural generalization of the classic String Indexing problem. Given a string $S$ (the text) of length $n$, USI asks for preprocessing $S$ into a compact data structure supporting the following queries efficiently: given a shorter string $P$ (the pattern), return the global utility $U(P)$ of $P$ in $S$, where $U$ is a function that maps any string $P$ to a utility score based on the utilities of the letters of every occurrence of $P$ in $S$. Our work also makes the following contributions: (1) We propose a novel and efficient data structure for USI based on finding the top-$K$ frequent substrings of $S$. (2) We propose a linear-space data structure that can be used to mine the top-$K$ frequent substrings of $S$ or to tune the parameters of the USI data structure. (3) We propose a novel space-efficient algorithm for estimating the set of the top-$K$ frequent substrings of $S$, thus improving the construction space of the data structure for USI. (4) We show that popular space-efficient top-$K$ frequent item mining strategies employed by state-of-the-art algorithms do not smoothly translate from items to substrings. (5) Using billion-letter datasets, we experimentally demonstrate that: (i) our top-$K$ frequent substring mining algorithms are accurate and scalable, unlike two state-of-the-art methods; and (ii) our USI data structures are up to $15$ times faster in querying than $4$ nontrivial baselines while occupying the same space with them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05917v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giulia Bernardini, Huiping Chen, Alessio Conte, Roberto Grossi, Veronica Guerrini, Grigorios Loukides, Nadia Pisanti, and Solon P. Pissis</dc:creator>
    </item>
    <item>
      <title>Zerrow: True Zero-Copy Arrow Pipelines in Bauplan</title>
      <link>https://arxiv.org/abs/2504.06151</link>
      <description>arXiv:2504.06151v1 Announce Type: cross 
Abstract: Bauplan is a FaaS-based lakehouse specifically built for data pipelines: its execution engine uses Apache Arrow for data passing between the nodes in the DAG. While Arrow is known as the "zero copy format", in practice, limited Linux kernel support for shared memory makes it difficult to avoid copying entirely. In this work, we introduce several new techniques to eliminate nearly all copying from pipelines: in particular, we implement a new kernel module that performs de-anonymization, thus eliminating a copy to intermediate data. We conclude by sharing our preliminary evaluation on different workloads types, as well as discussing our plan for future improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06151v1</guid>
      <category>cs.OS</category>
      <category>cs.DB</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Dai, Jacopo Tagliabue, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau, Tyler R. Caraza-Harter</dc:creator>
    </item>
    <item>
      <title>Rosetta Statements: Simplifying FAIR Knowledge Graph Construction with a User-Centered Approach</title>
      <link>https://arxiv.org/abs/2407.20007</link>
      <description>arXiv:2407.20007v2 Announce Type: replace 
Abstract: Machines need data and metadata to be machine-actionable and FAIR (findable, accessible, interoperable, reusable) to manage increasing data volumes. Knowledge graphs and ontologies are key to this, but their use is hampered by high access barriers due to required prior knowledge in semantics and data modelling. The Rosetta Statement approach proposes modeling English natural language statements instead of a mind-independent reality. We propose a metamodel for creating semantic schema patterns for simple statement types. The approach supports versioning of statements and provides a detailed editing history. Each Rosetta Statement pattern has a dynamic label for displaying statements as natural language sentences. Implemented in the Open Research Knowledge Graph (ORKG) as a use case, this approach allows domain experts to define data schema patterns without needing semantic knowledge. Future plans include combining Rosetta Statements with semantic units to organize ORKG into meaningful subgraphs, improving usability. A search interface for querying statements without needing SPARQL or Cypher knowledge is also planned, along with tools for data entry and display using Large Language Models. The Rosetta Statement metamodel supports a three-step knowledge graph construction procedure. Domain experts can model semantic content without support from ontology engineers by using Wikidata, lowering entry barriers and increasing cognitive interoperability. The second level involves mapping Wikidata terms to established ontologies, and the third step developing semantic graph patterns for reasoning, requiring collaboration with ontology engineers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20007v2</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars Vogt, Kheir Eddine Farfar, Pallavi Karanth, Marcel Konrad, Allard Oelen, Manuel Prinz, Philip Stroemert</dc:creator>
    </item>
    <item>
      <title>Efficient Data Ingestion in Cloud-based architecture: a Data Engineering Design Pattern Proposal</title>
      <link>https://arxiv.org/abs/2503.16079</link>
      <description>arXiv:2503.16079v2 Announce Type: replace 
Abstract: In today's fast-paced digital world, data has become a critical asset for enterprises across various industries. However, the exponential growth of data presents significant challenges in managing and utilizing the vast amounts of information collected. Data engineering has emerged as a vital discipline addressing these challenges by providing robust platforms for effective data management, processing, and utilization. Data Engineering Patterns (DEP) refer to standardized practices and procedures in data engineering, such as ETL (extract, transform, load) processes, data pipelining, and data streaming management. Data Engineering Design Patterns (DEDP) are best practice solutions to common problems in data engineering, involving established, tested, and optimized approaches. These include architectural decisions, data modeling techniques, and data storage and retrieval strategies. While many researchers and practitioners have identified various DEPs and proposed DEDPs, such as data mesh and lambda architecture, the challenge of high-volume data ingestion remains inadequately addressed. In this paper, we propose a data ingestion design pattern for big data in cloud architecture, incorporating both incremental and full refresh techniques. Our approach leverages a flexible, metadata-driven framework to enhance feasibility and flexibility. This allows for easy changes to the ingestion type, schema modifications, table additions, and the integration of new data sources, all with minimal effort from data engineers. Tested on the Azure cloud architecture, our experiments demonstrate that the proposed techniques significantly reduce data ingestion time. Overall, this paper advances data management practices by presenting a detailed exploration of data ingestion challenges and defining a proposal for an effective design patterns for cloud-based architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16079v2</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Rucco, Antonella Longo, Motaz Saad</dc:creator>
    </item>
  </channel>
</rss>
