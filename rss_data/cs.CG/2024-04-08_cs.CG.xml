<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CG updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CG</link>
    <description>cs.CG updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CG" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Apr 2024 04:02:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Maximum Clique Problem in a Disk Graph Made Easy</title>
      <link>https://arxiv.org/abs/2404.03751</link>
      <description>arXiv:2404.03751v1 Announce Type: new 
Abstract: A disk graph is an intersection graph of disks in $\mathbb{R}^2$. Determining the computational complexity of finding a maximum clique in a disk graph is a long-standing open problem. In 1990, Clark, Colbourn, and Johnson gave a polynomial-time algorithm for computing a maximum clique in a unit disk graph. However, finding a maximum clique when disks are of arbitrary size is widely believed to be a challenging open problem. The problem is open even if we restrict the disks to have at most two different sizes of radii, or restrict the radii to be within $[1,1+\varepsilon]$ for some $\epsilon&gt;0$. In this paper, we provide a new perspective to examine adjacencies in a disk graph that helps obtain the following results.
  - We design an $O(2^k n^{2k} poly(n))$-time algorithm to find a maximum clique in a $n$-vertex disk graph with $k$ different sizes of radii. This is polynomial for every fixed $k$, and thus settles the open question for the case when $k=2$.
  - Given a set of $n$ unit disks, we show how to compute a maximum clique inside each possible axis-aligned rectangle determined by the disk centers in $O(n^5\log n)$-time. This is at least a factor of $n^{4/3}$ faster than applying the fastest known algorithm for finding a maximum clique in a unit disk graph for each rectangle independently.
  - We give an $O(2^kn^{2rk} poly(n,r))$-time algorithm to find a maximum clique in a $n$-vertex ball graph with $k$ different sizes of radii where the ball centers lie on $r$ parallel planes. This is polynomial for every fixed $k$ and $r$, and thus contrasts the previously known NP-hardness result for finding a maximum clique in an arbitrary ball graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03751v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Mark Keil, Debajyoti Mondal</dc:creator>
    </item>
    <item>
      <title>Approximation Schemes for Geometric Knapsack for Packing Spheres and Fat Objects</title>
      <link>https://arxiv.org/abs/2404.03981</link>
      <description>arXiv:2404.03981v1 Announce Type: new 
Abstract: We study the geometric knapsack problem in which we are given a set of $d$-dimensional objects (each with associated profits) and the goal is to find the maximum profit subset that can be packed non-overlappingly into a given $d$-dimensional (unit hypercube) knapsack. Even if $d=2$ and all input objects are disks, this problem is known to be NP-hard [Demaine, Fekete, Lang, 2010]. In this paper, we give polynomial-time $(1+\varepsilon)$-approximation algorithms for the following types of input objects in any constant dimension $d$:
  - disks and hyperspheres,
  - a class of fat convex polygons that generalizes regular $k$-gons for $k\ge 5$ (formally, polygons with a constant number of edges, whose lengths are in a bounded range, and in which each angle is strictly larger than $\pi/2$)
  - arbitrary fat convex objects that are sufficiently small compared to the knapsack.
  We remark that in our \textsf{PTAS} for disks and hyperspheres, we output the computed set of objects, but for a $O_\varepsilon(1)$ of them we determine their coordinates only up to an exponentially small error. However, it is not clear whether there always exists a $(1+\varepsilon)$-approximate solution that uses only rational coordinates for the disks' centers. We leave this as an open problem which is related to well-studied geometric questions in the realm of circle packing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03981v1</guid>
      <category>cs.CG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pritam Acharya, Sujoy Bhore, Aaryan Gupta, Arindam Khan, Bratin Mondal, Andreas Wiese</dc:creator>
    </item>
    <item>
      <title>Discrete Fr\'echet Distance Oracles</title>
      <link>https://arxiv.org/abs/2404.04065</link>
      <description>arXiv:2404.04065v1 Announce Type: new 
Abstract: It is unlikely that the discrete Fr\'echet distance between two curves of length $n$ can be computed in strictly subquadratic time. We thus consider the setting where one of the curves, $P$, is known in advance. In particular, we wish to construct data structures (distance oracles) of near-linear size that support efficient distance queries with respect to $P$ in sublinear time. Since there is evidence that this is impossible for query curves of length $\Theta(n^\alpha)$, for any $\alpha &gt; 0$, we focus on query curves of (small) constant length, for which we are able to devise distance oracles with the desired bounds.
  We extend our tools to handle subcurves of the given curve, and even arbitrary vertex-to-vertex subcurves of a given geometric tree. That is, we construct an oracle that can quickly compute the distance between a short polygonal path (the query) and a path in the preprocessed tree between two query-specified vertices. Moreover, we define a new family of geometric graphs, $t$-local graphs (which strictly contains the family of geometric spanners with constant stretch), for which a similar oracle exists: we can preprocess a graph $G$ in the family, so that, given a query segment and a pair $u,v$ of vertices in $G$, one can quickly compute the smallest discrete Fr\'echet distance between the segment and any $(u,v)$-path in $G$. The answer is exact, if $t=1$, and approximate if $t&gt;1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04065v1</guid>
      <category>cs.CG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boris Aronov, Tsuri Farhana, Matthew J. Katz, Indu Ramesh</dc:creator>
    </item>
    <item>
      <title>Euclidean TSP in Narrow Strips</title>
      <link>https://arxiv.org/abs/2003.09948</link>
      <description>arXiv:2003.09948v2 Announce Type: replace 
Abstract: We investigate how the complexity of Euclidean TSP for point sets $P$ inside the strip $(-\infty,+\infty)\times [0,\delta]$ depends on the strip width $\delta$. We obtain two main results. First, for the case where the points have distinct integer $x$-coordinates, we prove that a shortest bitonic tour (which can be computed in $O(n\log^2 n)$ time using an existing algorithm) is guaranteed to be a shortest tour overall when $\delta\leq 2\sqrt{2}$, a bound which is best possible. Second, we present an algorithm that is fixed-parameter tractable with respect to $\delta$. Our algorithm has running time $2^{O(\sqrt{\delta})} n + O(\delta^2 n^2)$ for sparse point sets, where each $1\times\delta$ rectangle inside the strip contains $O(1)$ points. For random point sets, where the points are chosen uniformly at random from the rectangle $[0,n]\times [0,\delta]$, it has an expected running time of $2^{O(\sqrt{\delta})} n$. These results generalise to point sets $P$ inside a hypercylinder of width $\delta$. In this case, the factors $2^{O(\sqrt{\delta})}$ become $2^{O(\delta^{1-1/d})}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.09948v2</guid>
      <category>cs.CG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00454-023-00609-7</arxiv:DOI>
      <dc:creator>Henk Alkema, Mark de Berg, Remco van der Hofstad, S\'andor Kisfaludi-Bak</dc:creator>
    </item>
    <item>
      <title>Inapproximability of Maximum Diameter Clustering for Few Clusters</title>
      <link>https://arxiv.org/abs/2312.02097</link>
      <description>arXiv:2312.02097v2 Announce Type: replace 
Abstract: In the Max-k-diameter problem, we are given a set of points in a metric space, and the goal is to partition the input points into k parts such that the maximum pairwise distance between points in the same part of the partition is minimized.
  The approximability of the Max-k-diameter problem was studied in the eighties, culminating in the work of Feder and Greene [STOC'88], wherein they showed it is NP-hard to approximate within a factor better than 2 in the $\ell_1$ and $\ell_\infty$ metrics, and NP-hard to approximate within a factor better than 1.969 in the Euclidean metric. This complements the celebrated 2 factor polynomial time approximation algorithm for the problem in general metrics (Gonzalez [TCS'85]; Hochbaum and Shmoys [JACM'86]).
  Over the last couple of decades, there has been increased interest from the algorithmic community to study the approximability of various clustering objectives when the number of clusters is fixed. In this setting, the framework of coresets has yielded PTAS for most popular clustering objectives, including k-means, k-median, k-center, k-minsum, and so on.
  In this paper, rather surprisingly, we prove that even when k=3, the Max-k-diameter problem is NP-hard to approximate within a factor of 1.5 in the $\ell_1$-metric (and Hamming metric) and NP-hard to approximate within a factor of 1.304 in the Euclidean metric.
  Our main conceptual contribution is the introduction of a novel framework called cloud systems which embed hypergraphs into $\ell_p$-metric spaces such that the chromatic number of the hypergraph is related to the quality of the Max-k-diameter clustering of the embedded pointset. Our main technical contributions are the constructions of nontrivial cloud systems in the Euclidean and $\ell_1$-metrics using extremal geometric structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02097v2</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henry Fleischmann, Kyrylo Karlov, Karthik C. S., Ashwin Padaki, Stepan Zharkov</dc:creator>
    </item>
    <item>
      <title>Theoretical and Empirical Analysis of a Fast Algorithm for Extracting Polygons from Signed Distance Bounds</title>
      <link>https://arxiv.org/abs/2111.05778</link>
      <description>arXiv:2111.05778v2 Announce Type: replace-cross 
Abstract: Recently there has been renewed interest in signed distance bound representations due to their unique properties for 3D shape modelling. This is especially the case for deep learning-based bounds. However, it is beneficial to work with polygons in most computer-graphics applications. Thus, in this paper we introduce and investigate an asymptotically fast method for transforming signed distance bounds into polygon meshes. This is achieved by combining the principles of sphere tracing (or ray marching) with traditional polygonization techniques, such as Marching Cubes. We provide theoretical and experimental evidence that this approach is of the $O(N^2\log N)$ computational complexity for a polygonization grid with $N^3$ cells. The algorithm is tested on both a set of primitive shapes as well as signed distance bounds generated from point clouds by machine learning (and represented as neural networks). Given its speed, implementation simplicity and portability, we argue that it could prove useful during the modelling stage as well as in shape compression for storage.
  The code is available here: https://github.com/nenadmarkus/gridhopping</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.05778v2</guid>
      <category>cs.GR</category>
      <category>cs.CG</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.3390/a17040137</arxiv:DOI>
      <dc:creator>Nenad Marku\v{s}, Mirko Su\v{z}njevi\'c</dc:creator>
    </item>
    <item>
      <title>Few-shot point cloud reconstruction and denoising via learned Guassian splats renderings and fine-tuned diffusion features</title>
      <link>https://arxiv.org/abs/2404.01112</link>
      <description>arXiv:2404.01112v3 Announce Type: replace-cross 
Abstract: Existing deep learning methods for the reconstruction and denoising of point clouds rely on small datasets of 3D shapes. We circumvent the problem by leveraging deep learning methods trained on billions of images. We propose a method to reconstruct point clouds from few images and to denoise point clouds from their rendering by exploiting prior knowledge distilled from image-based deep learning models. To improve reconstruction in constraint settings, we regularize the training of a differentiable renderer with hybrid surface and appearance by introducing semantic consistency supervision. In addition, we propose a pipeline to finetune Stable Diffusion to denoise renderings of noisy point clouds and we demonstrate how these learned filters can be used to remove point cloud noise coming without 3D supervision. We compare our method with DSS and PointRadiance and achieved higher quality 3D reconstruction on the Sketchfab Testset and SCUT Dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01112v3</guid>
      <category>cs.CV</category>
      <category>cs.CG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pietro Bonazzi</dc:creator>
    </item>
  </channel>
</rss>
