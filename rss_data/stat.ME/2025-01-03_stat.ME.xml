<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Jan 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>High-Dimensional Markov-switching Ordinary Differential Processes</title>
      <link>https://arxiv.org/abs/2501.00087</link>
      <description>arXiv:2501.00087v1 Announce Type: new 
Abstract: We investigate the parameter recovery of Markov-switching ordinary differential processes from discrete observations, where the differential equations are nonlinear additive models. This framework has been widely applied in biological systems, control systems, and other domains; however, limited research has been conducted on reconstructing the generating processes from observations. In contrast, many physical systems, such as human brains, cannot be directly experimented upon and rely on observations to infer the underlying systems. To address this gap, this manuscript presents a comprehensive study of the model, encompassing algorithm design, optimization guarantees, and quantification of statistical errors. Specifically, we develop a two-stage algorithm that first recovers the continuous sample path from discrete samples and then estimates the parameters of the processes. We provide novel theoretical insights into the statistical error and linear convergence guarantee when the processes are $\beta$-mixing. Our analysis is based on the truncation of the latent posterior processes and demonstrates that the truncated processes approximate the true processes under mixing conditions. We apply this model to investigate the differences in resting-state brain networks between the ADHD group and normal controls, revealing differences in the transition rate matrices of the two groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00087v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katherine Tsai, Mladen Kolar, Sanmi Koyejo</dc:creator>
    </item>
    <item>
      <title>A portmanteau test for multivariate non-stationary functional time series with an increasing number of lags</title>
      <link>https://arxiv.org/abs/2501.00118</link>
      <description>arXiv:2501.00118v1 Announce Type: new 
Abstract: Multivariate locally stationary functional time series provide a flexible framework for modeling complex data structures exhibiting both temporal and spatial dependencies while allowing for time-varying data generating mechanism. In this paper, we introduce a specialized portmanteau-type test tailored for assessing white noise assumptions for multivariate locally stationary functional time series without dimension reduction. A simple bootstrap procedure is proposed to implement the test because the limiting distribution can be non-standard or even does not exist. Our approach is based on a new Gaussian approximation result for a maximum of degenerate $U$-statistics of second-order functional time series, which is of independent interest. Through theoretical analysis and simulation studies, we demonstrate the efficacy and adaptability of the proposed method in detecting departures from white noise assumptions in multivariate locally stationary functional time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00118v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lujia Bai, Holger Dette, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Denoising Data with Measurement Error Using a Reproducing Kernel-based Diffusion Model</title>
      <link>https://arxiv.org/abs/2501.00212</link>
      <description>arXiv:2501.00212v1 Announce Type: new 
Abstract: The ongoing technological revolution in measurement systems enables the acquisition of high-resolution samples in fields such as engineering, biology, and medicine. However, these observations are often subject to errors from measurement devices. Motivated by this challenge, we propose a denoising framework that employs diffusion models to generate denoised data whose distribution closely approximates the unobservable, error-free data, thereby permitting standard data analysis based on the denoised data. The key element of our framework is a novel Reproducing Kernel Hilbert Space-based method that trains the diffusion model with only error-contaminated data, admits a closed-form solution, and achieves a fast convergence rate in terms of estimation error. Furthermore, we verify the effectiveness of our method by deriving an upper bound on the Kullback--Leibler divergence between the distributions of the generated denoised data and the error-free data. A series of conducted simulations also verify the promising empirical performance of the proposed method compared to other state-of-the-art methods. To further illustrate the potential of this denoising framework in a real-world application, we apply it in a digital health context, showing how measurement error in continuous glucose monitors can influence conclusions drawn from a clinical trial on diabetes Mellitus disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00212v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyang Yi, Marcos Matabuena, Ruoyu Wang</dc:creator>
    </item>
    <item>
      <title>Matern and Generalized Wendland correlation models that parameterize hole effect, smoothness, and support</title>
      <link>https://arxiv.org/abs/2501.00558</link>
      <description>arXiv:2501.00558v1 Announce Type: new 
Abstract: A huge literature in statistics and machine learning is devoted to parametric families of correlation functions, where the correlation parameters are used to understand the properties of an associated spatial random process in terms of smoothness and global or compact support. However, most of current parametric correlation functions attain only non-negative values. This work provides two new families that parameterize negative dependencies (aka hole effects), along with smoothness, and global or compact support. They generalize the celebrated Mat\'ern and Generalized Wendland models, respectively, which are attained as special cases. A link between the two new families is also established, showing that a specific reparameterization of the latter includes the former as a special limit case. Their performance in terms of estimation accuracy and goodness of best linear unbiased prediction is illustrated through synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00558v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xavier Emery, Moreno Bevilacqua, Emilio Porcu</dc:creator>
    </item>
    <item>
      <title>Compositional Covariate Importance Testing via Partial Conjunction of Bivariate Hypotheses</title>
      <link>https://arxiv.org/abs/2501.00566</link>
      <description>arXiv:2501.00566v1 Announce Type: new 
Abstract: Compositional data (i.e., data comprising random variables that sum up to a constant) arises in many applications including microbiome studies, chemical ecology, political science, and experimental designs. Yet when compositional data serve as covariates in a regression, the sum constraint renders every covariate automatically conditionally independent of the response given the other covariates, since each covariate is a deterministic function of the others. Since essentially all covariate importance tests and variable selection methods, including parametric ones, are at their core testing conditional independence, they are all completely powerless on regression problems with compositional covariates. In fact, compositionality causes ambiguity in the very notion of relevant covariates. To address this problem, we identify a natural way to translate the typical notion of relevant covariates to the setting with compositional covariates and establish that it is intuitive, well-defined, and unique. We then develop corresponding hypothesis tests and controlled variable selection procedures via a novel connection with \emph{bivariate} conditional independence testing and partial conjunction hypothesis testing. Finally, we provide theoretical guarantees of the validity of our methods, and through numerical experiments demonstrate that our methods are not only valid but also powerful across a range of data-generating scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00566v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritwik Bhaduri, Siyuan Ma, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Robust distribution-free tests for the linear model</title>
      <link>https://arxiv.org/abs/2501.00583</link>
      <description>arXiv:2501.00583v1 Announce Type: new 
Abstract: Recently, there has been growing concern about heavy-tailed and skewed noise in biological data. We introduce RobustPALMRT, a flexible permutation framework for testing the association of a covariate of interest adjusted for control covariates. RobustPALMRT controls type I error rate for finite-samples, even in the presence of heavy-tailed or skewed noise. The new framework expands the scope of state-of-the-art tests in three directions. First, our method applies to robust and quantile regressions, even with the necessary hyper-parameter tuning. Second, by separating model-fitting and model-evaluation, we discover that performance improves when using a robust loss function in the model-evaluation step, regardless of how the model is fit. Third, we allow fitting multiple models to detect specialized features of interest in a distribution. To demonstrate this, we introduce DispersionPALRMT, which tests for differences in dispersion between treatment and control groups. We establish theoretical guarantees, identify settings where our method has greater power than existing methods, and analyze existing immunological data on Long-COVID patients. Using RobustPALMRT, we unveil novel differences between Long-COVID patients and others even in the presence of highly skewed noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00583v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Torey Hilbert, Steven MacEachern, Yuan Zhang</dc:creator>
    </item>
    <item>
      <title>Fiducial inference for partially identified parameters with applications to instrumental variable models</title>
      <link>https://arxiv.org/abs/2501.00837</link>
      <description>arXiv:2501.00837v1 Announce Type: new 
Abstract: In the past two decades, there has been a fast-growing literature on fiducial inference since it was first proposed by R. A. Fisher in the 1930s. However, most of the fiducial inference based methods and related approaches have been developed for point-identified models, i.e., statistical models where the parameters of interest are uniquely determined by the observed data and the model's assumptions. In this paper, we propose a novel fiducial approach for partially identified statistical models. As a leading example, we consider the instrumental variable model with a variety of causal assumptions and estimands. The proposed methods are illustrated through extensive simulations and a data analysis evaluating the effect of consuming Vitamin A supplementation on reducing mortality rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00837v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Cui, Jan Hannig</dc:creator>
    </item>
    <item>
      <title>Penalized weighted GEEs for high-dimensional longitudinal data with informative cluter size</title>
      <link>https://arxiv.org/abs/2501.00839</link>
      <description>arXiv:2501.00839v1 Announce Type: new 
Abstract: High-dimensional longitudinal data have become increasingly prevalent in recent studies, and penalized generalized estimating equations (GEEs) are often used to model such data. However, the desirable properties of the GEE method can break down when the outcome of interest is associated with cluster size, a phenomenon known as informative cluster size. In this article, we address this issue by formulating the effect of informative cluster size and proposing a novel weighted GEE approach to mitigate its impact, while extending the penalized version for high-dimensional settings. We show that the penalized weighted GEE approach achieves consistency in both model selection and estimation. Theoretically, we establish that the proposed penalized weighted GEE estimator is asymptotically equivalent to the oracle estimator, assuming the true model is known. This result indicates that the penalized weighted GEE approach retains the excellent properties of the GEE method and is robust to informative cluster sizes, thereby extending its applicability to highly complex situations. Simulations and a real data application further demonstrate that the penalized weighted GEE outperforms the existing alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00839v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Ma, Haofeng Wang, Xuejun Jiang</dc:creator>
    </item>
    <item>
      <title>A Graphical Approach to State Variable Selection in Off-policy Learning</title>
      <link>https://arxiv.org/abs/2501.00854</link>
      <description>arXiv:2501.00854v1 Announce Type: new 
Abstract: Sequential decision problems are widely studied across many areas of science. A key challenge when learning policies from historical data - a practice commonly referred to as off-policy learning - is how to ``identify'' the impact of a policy of interest when the observed data are not randomized. Off-policy learning has mainly been studied in two settings: dynamic treatment regimes (DTRs), where the focus is on controlling confounding in medical problems with short decision horizons, and offline reinforcement learning (RL), where the focus is on dimension reduction in closed systems such as games. The gap between these two well studied settings has limited the wider application of off-policy learning to many real-world problems. Using the theory for causal inference based on acyclic directed mixed graph (ADMGs), we provide a set of graphical identification criteria in general decision processes that encompass both DTRs and MDPs. We discuss how our results relate to the often implicit causal assumptions made in the DTR and RL literatures and further clarify several common misconceptions. Finally, we present a realistic simulation study for the dynamic pricing problem encountered in container logistics, and demonstrate how violations of our graphical criteria can lead to suboptimal policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00854v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joakim Blach Andersen, Qingyuan Zhao</dc:creator>
    </item>
    <item>
      <title>Optimizing for Rotisserie Fantasy Basketball</title>
      <link>https://arxiv.org/abs/2501.00933</link>
      <description>arXiv:2501.00933v1 Announce Type: new 
Abstract: Previous work on fantasy basketball has established methods for optimizing team construction for head-to-head formats. This has been facilitated by the straightforwardness of calculating the objective function for those formats, given that underlying performance distributions are known. Rotisserie has not been optimized in the same way because even with the assumption that performance distributions are known, directly calculating the most natural objective function is intractable. This work introduces a system for making a tractable approximation of that objective function. The resulting simplified objective function aligns well with the traditional wisdom that balanced teams are preferable for the format, because it contains an implicit mechanism that rewards teams for being balanced. Integrating this new objective function into established optimization methods is shown to perform well in the context of simulated seasons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00933v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Rosenof</dc:creator>
    </item>
    <item>
      <title>A novel unit-asymmetric distribution based on correlated Fr\'echet random variables</title>
      <link>https://arxiv.org/abs/2501.00970</link>
      <description>arXiv:2501.00970v1 Announce Type: new 
Abstract: In this paper, we propose a new distribution with unitary support which can be characterized as a ratio of the type $W=X_1/(X_1+X_2)$, where $(X_1, X_2)^\top$ follows a bivariate extreme distribution with Fr\'echet margins, that is, $X_1$ and $X_2$ are two correlated Fr\'echet random variables. Some mathematical properties such as identifiability, symmetry, stochastic representation, characterization as a ratio, moments, stress-strength probability, quantiles, and the maximum likelihood method are rigorously analyzed. Two applications of the ratio distribution are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00970v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Felipe Quintino</dc:creator>
    </item>
    <item>
      <title>Optimal Sampling for Generalized Linear Model under Measurement Constraint with Surrogate Variables</title>
      <link>https://arxiv.org/abs/2501.00972</link>
      <description>arXiv:2501.00972v1 Announce Type: new 
Abstract: Measurement-constrained datasets, often encountered in semi-supervised learning, arise when data labeling is costly, time-intensive, or hindered by confidentiality or ethical concerns, resulting in a scarcity of labeled data. In certain cases, surrogate variables are accessible across the entire dataset and can serve as approximations to the true response variable; however, these surrogates often contain measurement errors and thus cannot be directly used for accurate prediction. We propose an optimal sampling strategy that effectively harnesses the available information from surrogate variables. This approach provides consistent estimators under the assumption of a generalized linear model, achieving theoretically lower asymptotic variance than existing optimal sampling algorithms that do not leverage surrogate data. By employing the A-optimality criterion from optimal experimental design, our strategy maximizes statistical efficiency. Numerical studies demonstrate that our approach surpasses existing optimal sampling methods, exhibiting reduced empirical mean squared error and enhanced robustness in algorithmic performance. These findings highlight the practical advantages of our strategy in scenarios where measurement constraints exist and surrogates are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00972v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yixin Shen, Yang Ning</dc:creator>
    </item>
    <item>
      <title>Penalized Quasi-likelihood for High-dimensional Longitudinal Data via Within-cluster Resampling</title>
      <link>https://arxiv.org/abs/2501.01021</link>
      <description>arXiv:2501.01021v1 Announce Type: new 
Abstract: The generalized estimating equation (GEE) method is a popular tool for longitudinal data analysis. However, GEE produces biased estimates when the outcome of interest is associated with cluster size, a phenomenon known as informative cluster size (ICS). In this study, we address this issue by formulating the impact of ICS and proposing an integrated approach to mitigate its effects. Our method combines the concept of within-cluster resampling with a penalized quasi-likelihood framework applied to each resampled dataset, ensuring consistency in model selection and estimation. To aggregate the estimators from the resampled datasets, we introduce a penalized mean regression technique, resulting in a final estimator that improves true positive discovery rates while reducing false positives. Simulation studies and an application to yeast cell-cycle gene expression data demonstrate the excellent performance of the proposed penalized quasi-likelihood method via within-cluster resampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01021v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Ma, Haofeng Wang, Xuejun Jiang</dc:creator>
    </item>
    <item>
      <title>The R Package WMAP: Tools for Causal Meta-Analysis by Integrating Multiple Observational Studies</title>
      <link>https://arxiv.org/abs/2501.01041</link>
      <description>arXiv:2501.01041v1 Announce Type: new 
Abstract: ntegrating multiple observational studies for meta-analysis has sparked much interest. The presented R package WMAP (Weighted Meta-Analysis with Pseudo-Population) (Guha et al., 2024) addresses a critical gap in the implementation of integrative weighting approaches for multiple observational studies and causal inferences about various groups of subjects, such as disease subtypes. The package features three weighting approaches, each representing a special case of the unified weighting framework introduced by Guha and Li (2024), which includes an extension of inverse probability weights for data integration settings. It performs meta-analysis on user-inputted datasets as follows: (i) it first estimates the propensity scores for study-group combinations, calculates subject balancing weights, and determines the effective sample size (ESS) for a user-specified weighting method; and (ii) it then estimates various features of multiple counterfactual group outcomes, such as group medians and differences in group means for the mRNA expression of eight genes. Additionally, bootstrap variability estimates are provided. Among the implemented weighting methods, we highlight the FLEXible, O ptimized, and Realistic (FLEXOR) method, which is specifically designed to maximize the ESS within the unified framework. The use of the software is illustrated by simulations as well as a multi-site breast cancer study conducted in seven medical centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01041v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subharup Guha, Mengqi Xu, Kashish Priyam, Yi Li</dc:creator>
    </item>
    <item>
      <title>A Heavily Right Strategy for Integrating Dependent Studies in Any Dimension</title>
      <link>https://arxiv.org/abs/2501.01065</link>
      <description>arXiv:2501.01065v1 Announce Type: new 
Abstract: Recently, there has been a surge of interest in hypothesis testing methods for combining dependent studies without explicitly assessing their dependence. Among these, the Cauchy combination test (CCT) stands out for its approximate validity and power, leveraging a heavy-tail approximation insensitive to dependence. However, CCT is highly sensitive to large $p$-values and inverting it to construct confidence regions can result in regions lacking compactness, convexity, or connectivity. This article proposes a "heavily right" strategy by excluding the left half of the Cauchy distribution in the combination rule, retaining CCT's resilience to dependence while resolving its sensitivity to large $p$-values. Moreover, the Half-Cauchy combination as well as the harmonic mean approach guarantees bounded and convex confidence regions, distinguishing them as the only known combination tests with all such desirable properties. Efficient and accurate algorithms are introduced for implementing both methods. Additionally, we develop a divide-and-combine strategy for constructing confidence regions for high-dimensional mean estimation using the Half-Cauchy method, and empirically illustrate its advantages over the Hotelling $T^2$ approach. To demonstrate the practical utility of our Half-Cauchy approach, we apply it to network meta-analysis, constructing simultaneous confidence intervals for treatment effect comparisons across multiple clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01065v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianle Liu, Xiao-Li Meng, Natesh S. Pillai</dc:creator>
    </item>
    <item>
      <title>A Bayesian Spatial-Temporal Functional Model for Data with Block Structure and Repeated Measures</title>
      <link>https://arxiv.org/abs/2501.01269</link>
      <description>arXiv:2501.01269v1 Announce Type: new 
Abstract: The analysis of spatio-temporal data has been the object of research in several areas of knowledge. One of the main objectives of such research is the need to evaluate the behavior of climate effects in certain regions across a period of time. When certain climate patterns appear for several days or even weeks, causing the areas affected by them to have the same kind of weather for an extended period of time, the use of blocks for these phenomena may be a good strategy. Additionally, having repeated measures for observations within blocks helps to control for differences between observations, thus gaining more statistical power. In view of these perspectives, this study presents a spatio-temporal regression model with block structure with repeated measures incorporating as predictors functional variables of fixed and random nature. To accommodate complex spatial, temporal and block structures, functional components based on random effects were considered in addition to the class Mat\'ern covariance structure, which was responsible to account for spatial covariance. This work is motivated by a precipitation dataset collected monthly from various meteorological stations in Goi\'as State, Brazil, covering the years 1980 to 2001 (21 years). In this framework, spatial effects are represented by individual meteorological stations, temporal effects by months, block effects by climate patterns, and repeated measures by the years within those patterns. The proposed model demonstrated promising results in simulation studies and effectively estimated precipitation using the available data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01269v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David H. da Matta, Mariana R. Motta, Nancy L. Garcia, Alexandre B. Heinemann</dc:creator>
    </item>
    <item>
      <title>Time-dependent Predictive Accuracy Metrics in the Context of Interval Censoring and Competing Risks</title>
      <link>https://arxiv.org/abs/2501.01280</link>
      <description>arXiv:2501.01280v1 Announce Type: new 
Abstract: Evaluating the performance of a prediction model is a common task in medical statistics. Standard accuracy metrics require the observation of the true outcomes. This is typically not possible in the setting with time-to-event outcomes due to censoring. Interval censoring, the presence of time-varying covariates, and competing risks present additional challenges in obtaining those accuracy metrics. In this study, we propose two methods to deal with interval censoring in a time-varying competing risk setting: a model-based approach and the inverse probability of censoring weighting (IPCW) approach, focusing on three key time-dependent metrics: area under the receiver-operating characteristic curve (AUC), Brier score, and expected predictive cross-entropy (EPCE). The evaluation is conducted over a medically relevant time interval of interest, $[t, \Delta t)$. The model-based approach includes all subjects in the risk set, using their predicted risks to contribute to the accuracy metrics. In contrast, the IPCW approach only considers the subset of subjects who are known to be event-free or experience the event within the interval of interest. we performed a simulation study to compare the performance of the two approaches with regard to the three metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01280v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenwei Yang, Dimitris Rizopoulos, Lisa F. Newcomb, Nicole S. Erler</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Customer Support: A Framework for Secure and Scalable Interactions</title>
      <link>https://arxiv.org/abs/2412.07687</link>
      <description>arXiv:2412.07687v2 Announce Type: cross 
Abstract: The growing reliance on artificial intelligence (AI) in customer support has significantly improved operational efficiency and user experience. However, traditional machine learning (ML) approaches, which require extensive local training on sensitive datasets, pose substantial privacy risks and compliance challenges with regulations like the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). Existing privacy-preserving techniques, such as anonymization, differential privacy, and federated learning, address some concerns but face limitations in utility, scalability, and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning (PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates the need for local training on sensitive data by utilizing pre-trained LLMs to generate responses directly. The framework incorporates real-time data anonymization to redact or mask sensitive information, retrieval-augmented generation (RAG) for domain-specific query resolution, and robust post-processing to ensure compliance with regulatory standards. This combination reduces privacy risks, simplifies compliance, and enhances scalability and operational efficiency. Empirical analysis demonstrates that the PP-ZSL framework provides accurate, privacy-compliant responses while significantly lowering the costs and complexities of deploying AI-driven customer support systems. The study highlights potential applications across industries, including financial services, healthcare, e-commerce, legal support, telecommunications, and government services. By addressing the dual challenges of privacy and performance, this framework establishes a foundation for secure, efficient, and regulatory-compliant AI applications in customer interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07687v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anant Prakash Awasthi, Girdhar Gopal Agarwal, Chandraketu Singh, Rakshit Varma, Sanchit Sharma</dc:creator>
    </item>
    <item>
      <title>Post Launch Evaluation of Policies in a High-Dimensional Setting</title>
      <link>https://arxiv.org/abs/2501.00119</link>
      <description>arXiv:2501.00119v1 Announce Type: cross 
Abstract: A/B tests, also known as randomized controlled experiments (RCTs), are the gold standard for evaluating the impact of new policies, products, or decisions. However, these tests can be costly in terms of time and resources, potentially exposing users, customers, or other test subjects (units) to inferior options. This paper explores practical considerations in applying methodologies inspired by "synthetic control" as an alternative to traditional A/B testing in settings with very large numbers of units, involving up to hundreds of millions of units, which is common in modern applications such as e-commerce and ride-sharing platforms. This method is particularly valuable in settings where the treatment affects only a subset of units, leaving many units unaffected. In these scenarios, synthetic control methods leverage data from unaffected units to estimate counterfactual outcomes for treated units. After the treatment is implemented, these estimates can be compared to actual outcomes to measure the treatment effect. A key challenge in creating accurate counterfactual outcomes is interpolation bias, a well-documented phenomenon that occurs when control units differ significantly from treated units. To address this, we propose a two-phase approach: first using nearest neighbor matching based on unit covariates to select similar control units, then applying supervised learning methods suitable for high-dimensional data to estimate counterfactual outcomes. Testing using six large-scale experiments demonstrates that this approach successfully improves estimate accuracy. However, our analysis reveals that machine learning bias -- which arises from methods that trade off bias for variance reduction -- can impact results and affect conclusions about treatment effects. We document this bias in large-scale experimental settings and propose effective de-biasing techniques to address this challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00119v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shima Nassiri, Mohsen Bayati, Joe Cooprider</dc:creator>
    </item>
    <item>
      <title>Sampling the Bayesian Elastic Net</title>
      <link>https://arxiv.org/abs/2501.00594</link>
      <description>arXiv:2501.00594v1 Announce Type: cross 
Abstract: The Bayesian elastic net regression model is characterized by the regression coefficient prior distribution, the negative log density of which corresponds to the elastic net penalty function. While Markov chain Monte Carlo (MCMC) methods exist for sampling from the posterior of the regression coefficients given the penalty parameters, full Bayesian inference that incorporates uncertainty about the penalty parameters remains a challenge due to an intractable integrable in the posterior density function. Though sampling methods have been proposed that avoid computing this integral, all correctly-specified methods for full Bayesian inference that have appeared in the literature involve at least one "Metropolis-within-Gibbs" update, requiring tuning of proposal distributions. The computational landscape is complicated by the fact that two forms of the Bayesian elastic net prior have been introduced, and two representations (with and without data augmentation) of the prior suggest different MCMC algorithms. We review the forms and representations of the prior, discuss all combinations of these different treatments for the first time, and introduce one combination of form and representation that has yet to appear in the literature. We introduce MCMC algorithms for full Bayesian inference for all treatments of the prior. The algorithms allow for direct sampling of all parameters without any "Metropolis-within-Gibbs" steps. The key to the new approach is a careful transformation of the parameter space and an analysis of the resulting full conditional density functions that allows for efficient rejection sampling. We make empirical comparisons between our approaches and existing MCMC samplers for different data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00594v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher M. Hans, Ningyi Liu</dc:creator>
    </item>
    <item>
      <title>Panel Estimation of Taxable Income Elasticities with Heterogeneity and Endogenous Budget Sets</title>
      <link>https://arxiv.org/abs/2501.00633</link>
      <description>arXiv:2501.00633v1 Announce Type: cross 
Abstract: This paper introduces an estimator for the average of heterogeneous elasticities of taxable income (ETI), addressing key econometric challenges posed by nonlinear budget sets. Building on an isoelastic utility framework, we derive a linear-in-logs taxable income specification that incorporates the entire budget set while allowing for individual-specific ETI and productivity growth. To account for endogenous budget sets, we employ panel data and estimate individual-specific ridge regressions, constructing a debiased average of ridge coefficients to obtain the average ETI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00633v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soren Blomquist, Anil Kumar, Whitney K. Newey</dc:creator>
    </item>
    <item>
      <title>An AI-powered Bayesian generative modeling approach for causal inference in observational studies</title>
      <link>https://arxiv.org/abs/2501.00755</link>
      <description>arXiv:2501.00755v1 Announce Type: cross 
Abstract: Causal inference in observational studies with high-dimensional covariates presents significant challenges. We introduce CausalBGM, an AI-powered Bayesian generative modeling approach that captures the causal relationship among covariates, treatment, and outcome variables. The core innovation of CausalBGM lies in its ability to estimate the individual treatment effect (ITE) by learning individual-specific distributions of a low-dimensional latent feature set (e.g., latent confounders) that drives changes in both treatment and outcome. This approach not only effectively mitigates confounding effects but also provides comprehensive uncertainty quantification, offering reliable and interpretable causal effect estimates at the individual level. CausalBGM adopts a Bayesian model and uses a novel iterative algorithm to update the model parameters and the posterior distribution of latent features until convergence. This framework leverages the power of AI to capture complex dependencies among variables while adhering to the Bayesian principles. Extensive experiments demonstrate that CausalBGM consistently outperforms state-of-the-art methods, particularly in scenarios with high-dimensional covariates and large-scale datasets. Its Bayesian foundation ensures statistical rigor, providing robust and well-calibrated posterior intervals. By addressing key limitations of existing methods, CausalBGM emerges as a robust and promising framework for advancing causal inference in modern applications in fields such as genomics, healthcare, and social sciences. CausalBGM is maintained at the website https://causalbgm.readthedocs.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00755v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qiao Liu, Wing Hung Wong</dc:creator>
    </item>
    <item>
      <title>Generalized Heterogeneous Functional Model with Applications to Large-scale Mobile Health Data</title>
      <link>https://arxiv.org/abs/2501.01135</link>
      <description>arXiv:2501.01135v1 Announce Type: cross 
Abstract: Physical activity is crucial for human health. With the increasing availability of large-scale mobile health data, strong associations have been found between physical activity and various diseases. However, accurately capturing this complex relationship is challenging, possibly because it varies across different subgroups of subjects, especially in large-scale datasets. To fill this gap, we propose a generalized heterogeneous functional method which simultaneously estimates functional effects and identifies subgroups within the generalized functional regression framework. The proposed method captures subgroup-specific functional relationships between physical activity and diseases, providing a more nuanced understanding of these associations. Additionally, we introduce a pre-clustering method that enhances computational efficiency for large-scale data through a finer partition of subjects compared to true subgroups. In the real data application, we examine the impact of physical activity on the risk of mental disorders and Parkinson's disease using the UK Biobank dataset, which includes over 79,000 participants. Our proposed method outperforms existing methods in future-day prediction accuracy, identifying four subgroups for mental disorder outcomes and three subgroups for Parkinson's disease diagnosis, with detailed scientific interpretations for each subgroup. We also demonstrate theoretical consistency of our methods. Supplementary materials are available online. Codes implementing the proposed method are available at: https://github.com/xiaojing777/GHFM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01135v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojing Sun, Bingxin Zhao, Fei Xue</dc:creator>
    </item>
    <item>
      <title>Integrative Learning of Intensity Fluctuations of Quantum Dots under Excitation via a Tailored Mixture Hidden Markov Model</title>
      <link>https://arxiv.org/abs/2501.01292</link>
      <description>arXiv:2501.01292v1 Announce Type: cross 
Abstract: Semiconductor nano-crystals, known as quantum dots (QDs), have garnered significant interest in various scientific fields due to their unique fluorescence properties. One captivating characteristic of QDs is their ability to emit photons under continuous excitation. The intensity of photon emission fluctuates during the excitation, and such a fluctuation pattern can vary across different dots even under the same experimental conditions. What adding to the complication is that the processed intensity series are non-Gaussian and truncated due to necessary thresholding and normalization. As such, conventional approaches in the chemistry literature, typified by single-dot analysis of raw intensity data with Gaussian hidden Markov models (HMM), cannot meet the many analytical challenges and may fail to capture any novel yet rare fluctuation patterns among QDs. Collaborating with scientists in the chemistry field, we have developed an integrative learning approach to simultaneously analyzing intensity series of multiple QDs. Our approach still inherits the HMM as the skeleton to model the intensity fluctuations of each dot, and based on the data structure and the hypothesized collective behaviors of the QDs, our approach asserts that (i) under each hidden state, the normalized intensity follows a 0/1 inflated Beta distribution, (ii) the state distributions are shared across all the QDs, and (iii) the patterns of transitions can vary across QDs. These unique features allow for a precise characterization of the intensity fluctuation patterns and facilitate the clustering of the QDs. With experimental data collected on 128 QDs, our methods reveal several QD clusters characterized by unique transition patterns across three intensity states. The results provide deeper insight into QD behaviors and their design/application potentials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01292v1</guid>
      <category>physics.app-ph</category>
      <category>cond-mat.mes-hall</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Yang, Hawi Nyiera, Yonglei Sun, Jing Zhao, Kun Chen</dc:creator>
    </item>
    <item>
      <title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title>
      <link>https://arxiv.org/abs/2501.01324</link>
      <description>arXiv:2501.01324v1 Announce Type: cross 
Abstract: In this work, we develop a scalable approach for a flexible latent factor model for high-dimensional dynamical systems. Each latent factor process has its own correlation and variance parameters, and the orthogonal factor loading matrix can be either fixed or estimated. We utilize an orthogonal factor loading matrix that avoids computing the inversion of the posterior covariance matrix at each time of the Kalman filter, and derive closed-form expressions in an expectation-maximization algorithm for parameter estimation, which substantially reduces the computational complexity without approximation. Our study is motivated by inversely estimating slow slip events from geodetic data, such as continuous GPS measurements. Extensive simulated studies illustrate higher accuracy and scalability of our approach compared to alternatives. By applying our method to geodetic measurements in the Cascadia region, our estimated slip better agrees with independently measured seismic data of tremor events. The substantial acceleration from our method enables the use of massive noisy data for geological hazard quantification and other applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01324v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Deep Discrete Encoders: Identifiable Deep Generative Models for Rich Data with Discrete Latent Layers</title>
      <link>https://arxiv.org/abs/2501.01414</link>
      <description>arXiv:2501.01414v1 Announce Type: cross 
Abstract: In the era of generative AI, deep generative models (DGMs) with latent representations have gained tremendous popularity. Despite their impressive empirical performance, the statistical properties of these models remain underexplored. DGMs are often overparametrized, non-identifiable, and uninterpretable black boxes, raising serious concerns when deploying them in high-stakes applications. Motivated by this, we propose an interpretable deep generative modeling framework for rich data types with discrete latent layers, called Deep Discrete Encoders (DDEs). A DDE is a directed graphical model with multiple binary latent layers. Theoretically, we propose transparent identifiability conditions for DDEs, which imply progressively smaller sizes of the latent layers as they go deeper. Identifiability ensures consistent parameter estimation and inspires an interpretable design of the deep architecture. Computationally, we propose a scalable estimation pipeline of a layerwise nonlinear spectral initialization followed by a penalized stochastic approximation EM algorithm. This procedure can efficiently estimate models with exponentially many latent components. Extensive simulation studies validate our theoretical results and demonstrate the proposed algorithms' excellent performance. We apply DDEs to three diverse real datasets for hierarchical topic modeling, image representation learning, response time modeling in educational testing, and obtain interpretable findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01414v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seunghyun Lee, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>A tree-based model for addressing sparsity and taxa covariance in microbiome compositional count data</title>
      <link>https://arxiv.org/abs/2106.15051</link>
      <description>arXiv:2106.15051v5 Announce Type: replace 
Abstract: Microbiome compositional data are often high-dimensional, sparse, and exhibit pervasive cross-sample heterogeneity. Generative modeling is a popular approach to analyze such data, and effective generative models must accurately characterize these key features. While high-dimensionality and abundance of zeros have received much attention, existing models often lack flexibility in capturing complex cross-sample variability. This limitation can affect statistical efficiency and lead to misleading conclusions in tasks like differential abundance analysis, clustering, and network analysis. We introduce a generative model, the "logistic-tree normal" (LTN) model, which addresses this issue and effectively captures key characteristics of microbiome data, including abundance of zeros. LTN employs a tree-based decomposition to aggregate sparse taxa counts and uses a (multivariate) logistic-normal distribution at tree splits, allowing for flexible covariance adjustments among taxa as needed. The latent Gaussian structure of LTN enables the incorporation of multivariate analysis tools that enforce sparsity or low-rank covariance assumptions. As a versatile, fully generative model, LTN supports a wide range of applications and offers efficient Bayesian inference computational recipes through conjugate blocked Gibbs sampling with P\'olya-Gamma augmentation. We demonstrate application of LTN in a compositional mixed-effects model for differential abundance analysis using numerical experiments and a reanalysis of the infant cohort in the DIABIMMUNE study. Our findings illustrate that LTN, by adequately accounting for cross-sample heterogeneity, appropriately generates the proportion of zeros without requiring an explicit zero-inflation component, confirming a recent viewpoint that "zero-inflation" in count-based sequencing data are often results of unaccounted cross-sample variation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.15051v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoqun Wang, Jialiang Mao, Li Ma</dc:creator>
    </item>
    <item>
      <title>Efficient Generalization and Transportation</title>
      <link>https://arxiv.org/abs/2302.00092</link>
      <description>arXiv:2302.00092v3 Announce Type: replace 
Abstract: When estimating causal effects, it is important to assess external validity, i.e., determine how useful a given study is to inform a practical question for a specific target population. One challenge is that the covariate distribution in the population underlying a study may be different from that in the target population. If some covariates are effect modifiers, the average treatment effect (ATE) may not generalize to the target population. To tackle this problem, we propose new methods to generalize or transport the ATE from a source population to a target population, in the case where the source and target populations have different sets of covariates. When the ATE in the target population is identified, we propose new doubly robust estimators and establish their rates of convergence and limiting distributions. Under regularity conditions, the doubly robust estimators provably achieve the efficiency bound and are locally asymptotic minimax optimal. A sensitivity analysis is provided when the identification assumptions fail. Simulation studies show the advantages of the proposed doubly robust estimator over simple plug-in estimators. Importantly, we also provide minimax lower bounds and higher-order estimators of the target functionals. The proposed methods are applied in transporting causal effects of dietary intake on adverse pregnancy outcomes from an observational study to the whole U.S. pregnant female population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.00092v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenghao Zeng, Edward H. Kennedy, Lisa M. Bodnar, Ashley I. Naimi</dc:creator>
    </item>
    <item>
      <title>Estimation of Long-Range Dependent Models with Missing Data: to Impute or not to Impute?</title>
      <link>https://arxiv.org/abs/2303.04754</link>
      <description>arXiv:2303.04754v3 Announce Type: replace 
Abstract: Among the most important models for long-range dependent time series is the class of ARFIMA$(p,d,q)$ (Autoregressive Fractionally Integrated Moving Average) models. Estimating the long-range dependence parameter $d$ in ARFIMA models is a well-studied problem, but the literature regarding the estimation of $d$ in the presence of missing data is very sparse. There are two basic approaches to dealing with the problem: missing data can be imputed using some plausible method, and then the estimation can proceed as if no data were missing, or we can use a specially tailored methodology to estimate $d$ in the presence of missing data. In this work, we review some of the methods available for both approaches and compare them through a Monte Carlo simulation study. We present a comparison among 35 different setups to estimate $d$, under tenths of different scenarios, considering percentages of missing data ranging from as few as 10\% up to 70\% and several levels of dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04754v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guilherme Pumi, Gladys Choque Ulloa, Taiane Schaedler Prass</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Expected Shortfall Regression</title>
      <link>https://arxiv.org/abs/2307.02695</link>
      <description>arXiv:2307.02695v2 Announce Type: replace 
Abstract: Expected shortfall is defined as the average over the tail below (or above) a certain quantile of a probability distribution. Expected shortfall regression provides powerful tools for learning the relationship between a response variable and a set of covariates while exploring the heterogeneous effects of the covariates. In the health disparity research, for example, the lower/upper tail of the conditional distribution of a health-related outcome, given high-dimensional covariates, is often of importance. Under sparse models, we propose the lasso-penalized expected shortfall regression and establish non-asymptotic error bounds, depending explicitly on the sample size, dimension, and sparsity, for the proposed estimator. To perform statistical inference on a covariate of interest, we propose a debiased estimator and establish its asymptotic normality, from which asymptotically valid tests can be constructed. We illustrate the finite sample performance of the proposed method through numerical studies and a data application on health disparity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02695v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2448860</arxiv:DOI>
      <dc:creator>Shushu Zhang, Xuming He, Kean Ming Tan, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>Estimating and evaluating counterfactual prediction models</title>
      <link>https://arxiv.org/abs/2308.13026</link>
      <description>arXiv:2308.13026v3 Announce Type: replace 
Abstract: Counterfactual prediction methods are required when a model will be deployed in a setting where treatment policies differ from the setting where the model was developed, or when a model provides predictions under hypothetical interventions to support decision-making. However, estimating and evaluating counterfactual prediction models is challenging because, unlike traditional (factual) prediction, one does not observe the full set of potential outcomes for all individuals. Here, we discuss how to fit or tailor a model to target a counterfactual estimand, how to assess the model's performance, and how to perform model and tuning parameter selection. We provide identifiability and estimation results for building a counterfactual prediction model and for multiple measures of counterfactual model performance including loss-based measures, the area under the receiver operating characteristics curve, and calibration. Importantly, our results allow valid estimates of model performance under counterfactual intervention even if the candidate model is misspecified, permitting a wider array of use cases. We illustrate these methods using simulation and apply them to the task of developing a statin-naive risk prediction model for cardiovascular disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13026v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher B. Boyer, Issa J. Dahabreh, Jon A. Steingrimsson</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Quantiles of Hidden Biases in Matched Observational Studies</title>
      <link>https://arxiv.org/abs/2309.06459</link>
      <description>arXiv:2309.06459v2 Announce Type: replace 
Abstract: Causal conclusions from observational studies may be sensitive to unmeasured confounding. In such cases, a sensitivity analysis is often conducted, which tries to infer the minimum amount of hidden biases or the minimum strength of unmeasured confounding needed in order to explain away the observed association between treatment and outcome. If the needed bias is large, then the treatment is likely to have significant effects. The Rosenbaum sensitivity analysis is a modern approach for conducting sensitivity analysis in matched observational studies. It investigates what magnitude the maximum of hidden biases from all matched sets needs to be in order to explain away the observed association. However, such a sensitivity analysis can be overly conservative and pessimistic, especially when investigators suspect that some matched sets may have exceptionally large hidden biases. In this paper, we generalize Rosenbaum's framework to conduct sensitivity analysis on quantiles of hidden biases from all matched sets, which are more robust than the maximum. Moreover, the proposed sensitivity analysis is simultaneously valid across all quantiles of hidden biases and is thus a free lunch added to the conventional sensitivity analysis. The proposed approach works for general outcomes, general matched studies and general test statistics. In addition, we demonstrate that the proposed sensitivity analysis also works for bounded null hypotheses when the test statistic satisfies certain properties. An R package implementing the proposed approach is available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06459v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongxiao Wu, Xinran Li</dc:creator>
    </item>
    <item>
      <title>Gibbs optimal design of experiments</title>
      <link>https://arxiv.org/abs/2310.17440</link>
      <description>arXiv:2310.17440v2 Announce Type: replace 
Abstract: Bayesian optimal design is a well-established approach to planning experiments. A distribution for the responses, i.e. a statistical model, is assumed which is dependent on unknown parameters. A utility function is then specified giving gain in information in estimating the true values of the parameters, using the Bayesian posterior distribution. A Bayesian optimal design is given by maximising expectation of the utility with respect to the distribution implied by statistical model and prior distribution for the true parameter values. The approach accounts for the experimental aim, via specification of the utility, and of assumed sources of uncertainty. However, it is predicated on the statistical model being correct. Recently, a new type of statistical inference, known as Gibbs inference, has been proposed. This is Bayesian-like, i.e. uncertainty for unknown quantities is represented by a posterior distribution, but does not necessarily require specification of a statistical model. The resulting inference is less sensitive to misspecification of the statistical model. This paper introduces Gibbs optimal design: a framework for optimal design of experiments under Gibbs inference. A computational approach to find designs in practice is outlined and the framework is demonstrated on exemplars including linear models, and experiments with count and time-to-event responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17440v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antony M. Overstall, Jacinta Holloway-Brown, James M. McGree</dc:creator>
    </item>
    <item>
      <title>Sequential design for surrogate modeling in Bayesian inverse problems</title>
      <link>https://arxiv.org/abs/2402.16520</link>
      <description>arXiv:2402.16520v3 Announce Type: replace 
Abstract: Sequential design is a highly active field of research in active learning which provides a general framework for designing computer experiments with limited computational budgets. It aims to create efficient surrogate models to replace complex computer codes. Some sequential design strategies can be understood within the Stepwise Uncertainty Reduction (SUR) framework. In the SUR framework, each new design point is chosen by minimizing the expectation of a metric of uncertainty with respect to the yet unknown new data point. These methods offer an accessible framework for sequential experiment design, including almost sure convergence for common uncertainty functionals.
  This paper introduces two strategies. The first one, entitled Constraint Set Query (CSQ) is adapted from D-optimal designs where the search space is constrained in a ball for the Mahalanobis distance around the maximum a posteriori. The second, known as the IP-SUR (Inverse Problem SUR) strategy, uses a weighted-integrated mean squared prediction error as the uncertainty metric and is derived from SUR methods. It is tractable for Gaussian process surrogates with continuous sample paths. It comes with a theoretical guarantee for the almost sure convergence of the uncertainty functional. The premises of this work are highlighted in various test cases, in which these two strategies are compared to other sequential designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16520v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Lartaud, Philippe Humbert, Josselin Garnier</dc:creator>
    </item>
    <item>
      <title>Ancestor regression in structural vector autoregressive models</title>
      <link>https://arxiv.org/abs/2403.03778</link>
      <description>arXiv:2403.03778v2 Announce Type: replace 
Abstract: We present a new method for causal discovery in linear structural vector autoregressive models. We adapt an idea designed for independent observations to the case of time series while retaining its favorable properties, i.e., explicit error control for false causal discovery, at least asymptotically. We apply our method to several real-world bivariate time series datasets and discuss its findings which mostly agree with common understanding. The arrow of time in a model can be interpreted as background knowledge on possible causal mechanisms. Hence, our ideas could be extended to incorporating different background knowledge, even for independent observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03778v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Schultheiss, Markus Ulmer, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>The Mean Shape under the Relative Curvature Condition</title>
      <link>https://arxiv.org/abs/2404.01043</link>
      <description>arXiv:2404.01043v2 Announce Type: replace 
Abstract: Guaranteeing that Fr\'echet means of object populations do not locally self-intersect or are thereby affected is a serious challenge for object representations because the objects' shape space typically includes elements corresponding to geometrically invalid objects. We show how to produce a shape space guaranteeing no local self-intersections for specific but important cases where objects are represented by swept elliptical disks. This representation can model a variety of anatomic objects, such as the colon and hippocampus. Our approach of computing geodesic paths in this shape space enables detailed comparisons of structural variations between groups, such as patients and controls. The guarantee is met by constraining the shape space using the Relative Curvature Condition (RCC) of swept regions. This study introduces the Elliptical Tube Representation (ETRep) framework to provide a systematic approach to ensure valid mean shapes, effectively addressing the challenges of complex non-convex spaces while adhering to the RCC. The ETRep shape space incorporates an intrinsic distance metric defined based on the skeletal coordinate system of the shape space. The proposed methodology is applied to statistical shape analysis, facilitating the development of both global and partial hypothesis testing methods, which were employed to investigate hippocampal structures in early Parkinson's disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01043v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Taheri, Stephen M. Pizer, J\"orn Schulz</dc:creator>
    </item>
    <item>
      <title>From Estimands to Robust Inference of Treatment Effects in Platform Trials</title>
      <link>https://arxiv.org/abs/2411.12944</link>
      <description>arXiv:2411.12944v2 Announce Type: replace 
Abstract: A platform trial is an innovative clinical trial design that uses a master protocol (i.e., one overarching protocol) to evaluate multiple treatments in an ongoing manner and can accelerate the evaluation of new treatments. However, its flexibility introduces inferential challenges, with two fundamental ones being the precise definition of treatment effects and robust, efficient inference on these effects. Central to these challenges is the definition of an appropriate target population for the estimand, as some commonly used populations can be unexpectedly problematic. This article, for the first time, presents a clear framework for constructing a clinically meaningful estimand with precise specificity regarding the population of interest. The proposed estimand defines the treatment effect as a contrast of expected outcomes between two treatments within the entire concurrently eligible (ECE) population - the largest population that preserves the integrity of randomization - establishing a foundation for future research in platform trials. Then, we develop weighting and post-stratification methods for estimation of treatment effects with minimal assumptions. To fully leverage the efficiency potential of platform trials, we also consider a model-assisted approach for baseline covariate adjustment to gain efficiency while maintaining robustness against model misspecification. We derive and compare asymptotic distributions of proposed estimators in theory and propose robust variance estimators. The proposed estimators are empirically evaluated in a simulation study and illustrated in the SIMPLIFY trial, using the R package RobinCID.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12944v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhan Qian, Yifan Yi, Jun Shao, Yanyao Yi, Nicole Mayer-Hamblett, Patrick J. Heagerty, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v3 Announce Type: replace 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>Spline Autoregression Method for Estimation of Quantile Spectrum</title>
      <link>https://arxiv.org/abs/2412.17163</link>
      <description>arXiv:2412.17163v2 Announce Type: replace 
Abstract: The quantile spectrum was introduced in Li (2012; 2014) as an alternative tool for spectral analysis of time series. It has the capability of providing a richer view of time series data than that offered by the ordinary spectrum especially for nonlinear dynamics such as stochastic volatility. A novel method, called spline autoregression (SAR), is proposed in this paper for estimating the quantile spectrum as a bivaraite function of frequency and quantile level, under the assumption that the quantile spectrum varies smoothly with the quantile level. The SAR method is facilitated by the quantile discrete Fourier transform (QDFT) based on trigonometric quantile regression. It is enabled by the resulting time-domain quantile series (QSER) which represents properly scaled oscillatory characteristics of the original time series around a quantile. A functional autoregressive (AR) model is fitted to the QSER on a grid of quantile levels by penalized least-squares with the AR coefficients represented as smoothing splines of the quantile level. While the ordinary AR model is widely used for conventional spectral estimation, the proposed SAR method provides an effective way of estimating the quantile spectrum as a bivariate function in comparison with the alternatives. This is confirmed by a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17163v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ta-Hsin Li</dc:creator>
    </item>
    <item>
      <title>Robust functional PCA for density data</title>
      <link>https://arxiv.org/abs/2412.19004</link>
      <description>arXiv:2412.19004v2 Announce Type: replace 
Abstract: This paper introduces a robust approach to functional principal component analysis (FPCA) for compositional data, particularly density functions. While recent papers have studied density data within the Bayes space framework, there has been limited focus on developing robust methods to effectively handle anomalous observations and large noise. To address this, we extend the Mahalanobis distance concept to Bayes spaces, proposing its regularized version that accounts for the constraints inherent in density data. Based on this extension, we introduce a new method, robust density principal component analysis (RDPCA), for more accurate estimation of functional principal components in the presence of outliers. The method's performance is validated through simulations and real-world applications, showing its ability to improve covariance estimation and principal component analysis compared to traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19004v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Oguamalam, Peter Filzmoser, Karel Hron, Alessandra Menafoglio, Una Radoji\v{c}i\'c</dc:creator>
    </item>
    <item>
      <title>Debiased Nonparametric Regression for Statistical Inference and Distributionally Robustness</title>
      <link>https://arxiv.org/abs/2412.20173</link>
      <description>arXiv:2412.20173v2 Announce Type: replace 
Abstract: This study proposes a debiasing method for smooth nonparametric estimators. While machine learning techniques such as random forests and neural networks have demonstrated strong predictive performance, their theoretical properties remain relatively underexplored. Specifically, many modern algorithms lack assurances of pointwise asymptotic normality and uniform convergence, which are critical for statistical inference and robustness under covariate shift and have been well-established for classical methods like Nadaraya-Watson regression. To address this, we introduce a model-free debiasing method that guarantees these properties for smooth estimators derived from any nonparametric regression approach. By adding a correction term that estimates the conditional expected residual of the original estimator, or equivalently, its estimation error, we obtain a debiased estimator with proven pointwise asymptotic normality, and uniform convergence. These properties enable statistical inference and enhance robustness to covariate shift, making the method broadly applicable to a wide range of nonparametric regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20173v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Identifying average causal effect in regression discontinuity design with auxiliary data</title>
      <link>https://arxiv.org/abs/2412.20840</link>
      <description>arXiv:2412.20840v2 Announce Type: replace 
Abstract: Regression discontinuity designs are widely used when treatment assignment is determined by whether a running variable exceeds a predefined threshold. However, most research focuses on estimating local causal effects at the threshold, leaving the challenge of identifying treatment effects away from the cutoff largely unaddressed. The primary difficulty in this context is that the treatment assignment is deterministically defined by the running variable, violating the commonly assumed positivity assumption. In this paper, we introduce a novel framework for identifying the average causal effect in regression discontinuity designs.Our approach assumes the existence of an auxiliary variable for which the running variable can be seen as a surrogate, and an additional dataset that consists of the running variable and the auxiliary variable alongside the traditional regression discontinuity design setup. Under this framework, we propose three estimation methods for the ATE, which resembles the outcome regression, inverse propensity weighted and doubly robust estimators in classical causal inference literature. Asymptotically valid inference procedures are also provided. To demonstrate the practical application of our method, simulations are conducted to show the good performance of our methods; besides, we use the proposed methods to assess the causal effects of vitamin A supplementation on the severity of autism spectrum disorders in children, where a positive effect is found but with no statistical significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20840v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinqin Feng, Wenjie Hu, Pu Yang, Tingyu Li, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Identification of Dynamic Panel Logit Models with Fixed Effects</title>
      <link>https://arxiv.org/abs/2104.04590</link>
      <description>arXiv:2104.04590v3 Announce Type: replace-cross 
Abstract: We show that identification in a general class of dynamic panel logit models with fixed effects is related to the truncated moment problem from the mathematics literature. We use this connection to show that the identified set for structural parameters and functionals of the distribution of latent individual effects can be characterized by a finite set of conditional moment equalities subject to a certain set of shape constraints on the model parameters. In addition to providing a general approach to identification, the new characterization can deliver informative bounds in cases where competing methods deliver no identifying restrictions, and can deliver point identification in cases where competing methods deliver partial identification. We then present an estimation and inference procedure that uses semidefinite programming methods, is applicable with continuous or discrete covariates, and can be used for models that are either point- or partially-identified. Finally, we illustrate our identification result with a number of examples, and provide an empirical application to employment dynamics using data from the National Longitudinal Survey of Youth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.04590v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Dobronyi, Jiaying Gu, Kyoo il Kim, Thomas M. Russell</dc:creator>
    </item>
    <item>
      <title>A Geometric Condition for Uniqueness of Fr\'echet Means of Persistence Diagrams</title>
      <link>https://arxiv.org/abs/2207.03943</link>
      <description>arXiv:2207.03943v3 Announce Type: replace-cross 
Abstract: The Fr\'echet mean is an important statistical summary and measure of centrality of data; it has been defined and studied for persistent homology captured by persistence diagrams. However, the complicated geometry of the space of persistence diagrams implies that the Fr\'echet mean for a given set of persistence diagrams is not necessarily unique, which prohibits theoretical guarantees for empirical means with respect to population means. In this paper, we derive a variance expression for a set of persistence diagrams exhibiting a multi-matching between the persistence points known as a grouping. Moreover, we propose a condition for groupings, which we refer to as flatness; we prove that sets of persistence diagrams that exhibit flat groupings give rise to unique Fr\'echet means. We derive a finite sample convergence result for general groupings, which results in convergence for Fr\'echet means if the groupings are flat. We then interpret flat groupings in a recently-proposed general framework of Fr\'echet means in Alexandrov geometry. Finally, we show that for manifold-valued data, the persistence diagrams can be truncated to construct flat groupings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.03943v3</guid>
      <category>math.MG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.comgeo.2024.102162</arxiv:DOI>
      <dc:creator>Yueqi Cao, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>Online Joint Assortment-Inventory Optimization under MNL Choices</title>
      <link>https://arxiv.org/abs/2304.02022</link>
      <description>arXiv:2304.02022v3 Announce Type: replace-cross 
Abstract: We study an online joint assortment-inventory optimization problem, in which we assume that the choice behavior of each customer follows the Multinomial Logit (MNL) choice model, and the attraction parameters are unknown a priori. The retailer makes periodic assortment and inventory decisions to dynamically learn from the customer choice observations about the attraction parameters while maximizing the expected total profit over time. In this paper, we propose a novel algorithm that can effectively balance exploration and exploitation in the online decision-making of assortment and inventory. Our algorithm builds on a new estimator for the MNL attraction parameters, an innovative approach to incentivize exploration by adaptively tuning certain known and unknown parameters, and an optimization oracle to static single-cycle assortment-inventory planning problems with given parameters. We establish a regret upper bound for our algorithm and a lower bound for the online joint assortment-inventory optimization problem, suggesting that our algorithm achieves nearly optimal regret rate, provided that the static optimization oracle is exact. Then we incorporate more practical approximate static optimization oracles into our algorithm, and bound from above the impact of static optimization errors on the regret of our algorithm. We perform numerical studies to demonstrate the effectiveness of our proposed algorithm. At last, we extend our study by incorporating inventory carryover and the learning of customer arrival distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.02022v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong Liang, Xiaojie Mao, Shiyuan Wang</dc:creator>
    </item>
    <item>
      <title>Empirical Networks are Sparse: Enhancing Multi-Edge Models with Zero-Inflation</title>
      <link>https://arxiv.org/abs/2406.09169</link>
      <description>arXiv:2406.09169v2 Announce Type: replace-cross 
Abstract: Real-world networks are sparse. As we show in this article, even when a large number of interactions is observed, most node pairs remain disconnected. We demonstrate that classical multi-edge network models, such as the $G(N,p)$, configuration models, and stochastic block models, fail to accurately capture this phenomenon. To mitigate this issue, zero-inflation must be integrated into these traditional models. Through zero-inflation, we incorporate a mechanism that accounts for the excess number of zeroes (disconnected pairs) observed in empirical data. By performing an analysis on all the datasets from the Sociopatterns repository, we illustrate how zero-inflated models more accurately reflect the sparsity and heavy-tailed edge count distributions observed in empirical data. Our findings underscore that failing to account for these ubiquitous properties in real-world networks inadvertently leads to biased models that do not accurately represent complex systems and their dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09169v2</guid>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giona Casiraghi, Georges Andres</dc:creator>
    </item>
    <item>
      <title>Distribution Regression Difference-In-Differences</title>
      <link>https://arxiv.org/abs/2409.02311</link>
      <description>arXiv:2409.02311v2 Announce Type: replace-cross 
Abstract: We provide a simple distribution regression estimator for treatment effects in the difference-in-differences (DiD) design. Our procedure is particularly useful when the treatment effect differs across the distribution of the outcome variable. Our proposed estimator easily incorporates covariates and, importantly, can be extended to settings where the treatment potentially affects the joint distribution of multiple outcomes. Our key identifying restriction is that the counterfactual distribution of the treated in the untreated state has no interaction effect between treatment and time. This assumption results in a parallel trend assumption on a transformation of the distribution. We highlight the relationship between our procedure and assumptions with the changes-in-changes approach of Athey and Imbens (2006). We also reexamine the Card and Krueger (1994) study of the impact of minimum wages on employment to illustrate the utility of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02311v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iv\'an Fern\'andez-Val, Jonas Meier, Aico van Vuuren, Francis Vella</dc:creator>
    </item>
    <item>
      <title>Joint Probability Estimation of Many Binary Outcomes via Localized Adversarial Lasso</title>
      <link>https://arxiv.org/abs/2410.15166</link>
      <description>arXiv:2410.15166v3 Announce Type: replace-cross 
Abstract: In this work we consider estimating the probability of many (possibly dependent) binary outcomes which is at the core of many applications, e.g., multi-level treatments in causal inference, demands for bundle of products, etc. Without further conditions, the probability distribution of an M dimensional binary vector is characterized by exponentially in M coefficients which can lead to a high-dimensional problem even without the presence of covariates. Understanding the (in)dependence structure allows us to substantially improve the estimation as it allows for an effective factorization of the probability distribution. In order to estimate the probability distribution of a M dimensional binary vector, we leverage a Bahadur representation that connects the sparsity of its coefficients with independence across the components. We propose to use regularized and adversarial regularized estimators to obtain an adaptive estimator with respect to the dependence structure which allows for rates of convergence to depend on this intrinsic (lower) dimension. These estimators are needed to handle several challenges within this setting, including estimating nuisance parameters, estimating covariates, and nonseparable moment conditions. Our main results consider the presence of (low dimensional) covariates for which we propose a locally penalized estimator. We provide pointwise rates of convergence addressing several issues in the theoretical analyses as we strive for making a computationally tractable formulation. We apply our results in the estimation of causal effects with multiple binary treatments and show how our estimators can improve the finite sample performance when compared with non-adaptive estimators that try to estimate all the probabilities directly. We also provide simulations that are consistent with our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15166v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Belloni, Yan Chen, Matthew Harding</dc:creator>
    </item>
  </channel>
</rss>
