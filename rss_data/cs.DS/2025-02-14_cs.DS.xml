<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 05:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Planted Spanning Tree Problem</title>
      <link>https://arxiv.org/abs/2502.08790</link>
      <description>arXiv:2502.08790v1 Announce Type: new 
Abstract: We study the problem of detecting and recovering a planted spanning tree $M_n^*$ hidden within a complete, randomly weighted graph $G_n$. Specifically, each edge $e$ has a non-negative weight drawn independently from $P_n$ if $e \in M_n^*$ and from $Q_n$ otherwise, where $P_n \equiv P$ is fixed and $Q_n$ scales with $n$ such that its density at the origin satisfies $\lim_{n\to\infty} n Q'_n(0)=1.$ We consider two representative cases: when $M_n^*$ is either a uniform spanning tree or a uniform Hamiltonian path. We analyze the recovery performance of the minimum spanning tree (MST) algorithm and derive a fixed-point equation that characterizes the asymptotic fraction of edges in $M_n^*$ successfully recovered by the MST as $n \to \infty.$ Furthermore, we establish the asymptotic mean weight of the MST, extending Frieze's $\zeta(3)$ result to the planted model. Leveraging this result, we design an efficient test based on the MST weight and show that it can distinguish the planted model from the unplanted model with vanishing testing error as $n \to \infty.$ Our analysis relies on an asymptotic characterization of the local structure of the planted model, employing the framework of local weak convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08790v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehrdad Moharrami, Cristopher Moore, Jiaming Xu</dc:creator>
    </item>
    <item>
      <title>Scalable Private Partition Selection via Adaptive Weighting</title>
      <link>https://arxiv.org/abs/2502.08878</link>
      <description>arXiv:2502.08878v1 Announce Type: new 
Abstract: In the differentially private partition selection problem (a.k.a. private set union, private key discovery), users hold subsets of items from an unbounded universe. The goal is to output as many items as possible from the union of the users' sets while maintaining user-level differential privacy. Solutions to this problem are a core building block for many privacy-preserving ML applications including vocabulary extraction in a private corpus, computing statistics over categorical data, and learning embeddings over user-provided items.
  We propose an algorithm for this problem, MaximumAdaptiveDegree (MAD), which adaptively reroutes weight from items with weight far above the threshold needed for privacy to items with smaller weight, thereby increasing the probability that less frequent items are output. Our algorithm can be efficiently implemented in massively parallel computation systems allowing scalability to very large datasets. We prove that our algorithm stochastically dominates the standard parallel algorithm for this problem. We also develop a two-round version of our algorithm where results of the computation in the first round are used to bias the weighting in the second round to maximize the number of items output. In experiments, our algorithms provide the best results across the board among parallel algorithms and scale to datasets with hundreds of billions of items, up to three orders of magnitude larger than those analyzed by prior sequential algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08878v1</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Y. Chen, Vincent Cohen-Addad, Alessandro Epasto, Morteza Zadimoghaddam</dc:creator>
    </item>
    <item>
      <title>Incremental Approximate Maximum Flow via Residual Graph Sparsification</title>
      <link>https://arxiv.org/abs/2502.09105</link>
      <description>arXiv:2502.09105v1 Announce Type: new 
Abstract: We give an algorithm that, with high probability, maintains a $(1-\epsilon)$-approximate $s$-$t$ maximum flow in undirected, uncapacitated $n$-vertex graphs undergoing $m$ edge insertions in $\tilde{O}(m+ n F^*/\epsilon)$ total update time, where $F^{*}$ is the maximum flow on the final graph. This is the first algorithm to achieve polylogarithmic amortized update time for dense graphs ($m = \Omega(n^2)$), and more generally, for graphs where $F^*= \tilde{O}(m/n)$.
  At the heart of our incremental algorithm is the residual graph sparsification technique of Karger and Levine [SICOMP '15], originally designed for computing exact maximum flows in the static setting. Our main contributions are (i) showing how to maintain such sparsifiers for approximate maximum flows in the incremental setting and (ii) generalizing the cut sparsification framework of Fung et al. [SICOMP '19] from undirected graphs to balanced directed graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09105v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gramoz Goranci, Monika Henzinger, Harald R\"acke, A. R. Sricharan</dc:creator>
    </item>
    <item>
      <title>A LP-rounding based algorithm for soft capacitated facility location problem with submodular penalties</title>
      <link>https://arxiv.org/abs/2502.09412</link>
      <description>arXiv:2502.09412v1 Announce Type: new 
Abstract: The soft capacitated facility location problem (SCFLP) is a classic combinatorial optimization problem, with its variants widely applied in the fields of operations research and computer science. In the SCFLP, given a set $\mathcal{F}$ of facilities and a set $\mathcal{D}$ of clients, each facility has a capacity and an open cost, allowing to open multiple times, and each client has a demand.
  This problem is to find a subset of facilities in $\mathcal{F}$ and connect each client to the facilities opened, such that the total cost including open cost and connection cost is minimied. SCFLP is a NP-hard problem, which has led to a focus on approximation algorithms. Based on this, we consider a variant, that is, soft capacitated facility location problem with submodular penalties (SCFLPSP), which allows some clients not to be served by accepting the penalty cost. And we consider the integer splittable case of demand, that is, the demand of each client is served by multiple facilities with the integer service amount by each facility. Based on LP-rounding, we propose a $(\lambda R+4)$-approximation algorithm, where $R=\frac{\max_{i \in \mathcal{F} }f_i}{\min_{i \in \mathcal{F} }f_i},\lambda=\frac{R+\sqrt{R^2+8R}}{2R}$. In particular, when the open cost is uniform, the approximation ratio is 6.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09412v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanyin Xiao, Jiaming Zhang, Zhikang Zhang, Weidong Li</dc:creator>
    </item>
    <item>
      <title>Deterministic Independent Sets in the Semi-Streaming Model</title>
      <link>https://arxiv.org/abs/2502.09440</link>
      <description>arXiv:2502.09440v1 Announce Type: new 
Abstract: We consider the independent set problem in the semi-streaming model. For any input graph $G=(V, E)$ with $n$ vertices, an independent set is a set of vertices with no edges between any two elements. In the semi-streaming model, $G$ is presented as a stream of edges and any algorithm must use $\tilde O(n)$ bits of memory to output a large independent set at the end of the stream.
  Prior work has designed various semi-streaming algorithms for finding independent sets. Due to the hardness of finding maximum and maximal independent sets in the semi-streaming model, the focus has primarily been on finding independent sets in terms of certain parameters, such as the maximum degree $\Delta$. In particular, there is a simple randomized algorithm that obtains independent sets of size $\frac n{\Delta+1}$ in expectation, which can also be achieved with high probability using more complicated algorithms. For deterministic algorithms, the best bounds are significantly weaker. In fact, the best we currently know is a straightforward algorithm that finds an $\tilde\Omega\left(\frac n{\Delta^2}\right)$ size independent set.
  We show that this straightforward algorithm is nearly optimal by proving that any deterministic semi-streaming algorithm can only output an $\tilde O\left(\frac n{\Delta^2}\right)$ size independent set. Our result proves a strong separation between the power of deterministic and randomized semi-streaming algorithms for the independent set problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09440v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Ye</dc:creator>
    </item>
    <item>
      <title>Forward-backward Contention Resolution Schemes for Fair Rationing</title>
      <link>https://arxiv.org/abs/2502.09521</link>
      <description>arXiv:2502.09521v1 Announce Type: new 
Abstract: We use contention resolution schemes (CRS) to derive algorithms for the fair rationing of a single resource when agents have stochastic demands. We aim to provide ex-ante guarantees on the level of service provided to each agent, who may measure service in different ways (Type-I, II, or III), calling for CRS under different feasibility constraints (rank-1 matroid or knapsack). We are particularly interested in two-order CRS where the agents are equally likely to arrive in a known forward order or its reverse, which is motivated by online rationing at food banks.
  In particular, we derive a two-order CRS for rank-1 matroids with guarantee $1/(1+e^{-1/2})\approx 0.622$, which we prove is tight. This improves upon the $1/2$ guarantee that is best-possible under a single order (Alaei, SIAM J. Comput. 2014), while achieving separation with the $1-1/e\approx 0.632$ guarantee that is possible for random-order CRS (Lee and Singla, ESA 2018). Because CRS guarantees imply prophet inequalities, this also beats the two-order prophet inequality with ratio $(\sqrt{5}-1)/2\approx 0.618$ from (Arsenis, SODA 2021), which was tight for single-threshold policies. Rank-1 matroids suffice to provide guarantees under Type-II or III service, but Type-I service requires knapsack. Accordingly, we derive a two-order CRS for knapsack with guarantee $1/3$, improving upon the $1/(3+e^{-2})\approx 0.319$ guarantee that is best-possible under a single order (Jiang et al., SODA 2022). To our knowledge, $1/3$ provides the best-known guarantee for knapsack CRS even in the offline setting. Finally, we provide an upper bound of $1/(2+e^{-1})\approx 0.422$ for two-order knapsack CRS, strictly smaller than the upper bound of $(1-e^{-2})/2\approx0.432$ for random-order knapsack CRS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09521v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Will Ma, Calum MacRury, Cliff Stein</dc:creator>
    </item>
    <item>
      <title>Fast Tensor Completion via Approximate Richardson Iteration</title>
      <link>https://arxiv.org/abs/2502.09534</link>
      <description>arXiv:2502.09534v1 Announce Type: new 
Abstract: We study tensor completion (TC) through the lens of low-rank tensor decomposition (TD). Many TD algorithms use fast alternating minimization methods, which solve highly structured linear regression problems at each step (e.g., for CP, Tucker, and tensor-train decompositions). However, such algebraic structure is lost in TC regression problems, making direct extensions unclear. To address this, we propose a lifting approach that approximately solves TC regression problems using structured TD regression algorithms as blackbox subroutines, enabling sublinear-time methods. We theoretically analyze the convergence rate of our approximate Richardson iteration based algorithm, and we demonstrate on real-world tensors that its running time can be 100x faster than direct methods for CP completion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09534v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehrdad Ghadiri, Matthew Fahrbach, Yunbum Kook, Ali Jadbabaie</dc:creator>
    </item>
    <item>
      <title>Stable Hypergraph Matching in Unimodular Hypergraphs</title>
      <link>https://arxiv.org/abs/2502.08827</link>
      <description>arXiv:2502.08827v1 Announce Type: cross 
Abstract: We study the NP-hard Stable Hypergraph Matching (SHM) problem and its generalization allowing capacities, the Stable Hypergraph $b$-Matching (SH$b$M) problem, and investigate their computational properties under various structural constraints. Our study is motivated by the fact that Scarf's Lemma (Scarf, 1967) together with a result of Lov\'asz (1972) guarantees the existence of a stable matching whenever the underlying hypergraph is normal. Furthermore, if the hypergraph is unimodular (i.e., its incidence matrix is totally unimodular), then even a stable $b$-matching is guaranteed to exist. However, no polynomial-time algorithm is known for finding a stable matching or $b$-matching in unimodular hypergraphs.
  We identify subclasses of unimodular hypergraphs where SHM and SH$b$M are tractable such as laminar hypergraphs or so-called subpath hypergraphs with bounded-size hyperedges; for the latter case, even a maximum-weight stable $b$-matching can be found efficiently. We complement our algorithms by showing that optimizing over stable matchings is NP-hard even in laminar hypergraphs. As a practically important special case of SH$b$M for unimodular hypergraphs, we investigate a tripartite stable matching problem with students, schools, and companies as agents, called the University Dual Admission problem, which models real-world scenarios in higher education admissions.
  Finally, we examine a superclass of subpath hypergraphs that are normal but necessarily not unimodular, namely subtree hypergraphs where hyperedges correspond to subtrees of a tree. We establish that for such hypergraphs, stable matchings can be found in polynomial time but, in the setting with capacities, finding a stable $b$-matching is NP-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08827v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P\'eter Bir\'o, Gergely Cs\'aji, Ildik\'o Schlotter</dc:creator>
    </item>
    <item>
      <title>Linear-Time User-Level DP-SCO via Robust Statistics</title>
      <link>https://arxiv.org/abs/2502.08889</link>
      <description>arXiv:2502.08889v1 Announce Type: cross 
Abstract: User-level differentially private stochastic convex optimization (DP-SCO) has garnered significant attention due to the paramount importance of safeguarding user privacy in modern large-scale machine learning applications. Current methods, such as those based on differentially private stochastic gradient descent (DP-SGD), often struggle with high noise accumulation and suboptimal utility due to the need to privatize every intermediate iterate. In this work, we introduce a novel linear-time algorithm that leverages robust statistics, specifically the median and trimmed mean, to overcome these challenges. Our approach uniquely bounds the sensitivity of all intermediate iterates of SGD with gradient estimation based on robust statistics, thereby significantly reducing the gradient estimation noise for privacy purposes and enhancing the privacy-utility trade-off. By sidestepping the repeated privatization required by previous methods, our algorithm not only achieves an improved theoretical privacy-utility trade-off but also maintains computational efficiency. We complement our algorithm with an information-theoretic lower bound, showing that our upper bound is optimal up to logarithmic factors and the dependence on $\epsilon$. This work sets the stage for more robust and efficient privacy-preserving techniques in machine learning, with implications for future research and application in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08889v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Badih Ghazi, Ravi Kumar, Daogao Liu, Pasin Manurangsi</dc:creator>
    </item>
    <item>
      <title>Data Structures for Finite Downsets of Natural Vectors: Theory and Practice</title>
      <link>https://arxiv.org/abs/2502.09189</link>
      <description>arXiv:2502.09189v1 Announce Type: cross 
Abstract: Manipulating downward-closed sets of vectors forms the basis of so-called antichain-based algorithms in verification. In that context, the dimension of the vectors is intimately tied to the size of the input structure to be verified. In this work, we formally analyze the complexity of classical list-based algorithms to manipulate antichains as well as that of Zampuni\'eris's sharing trees and traditional and novel kdtree-based antichain algorithms. In contrast to the existing literature, and to better address the needs of formal verification, our analysis of \kdtree algorithms does not assume that the dimension of the vectors is fixed. Our theoretical results show that kdtrees are asymptotically better than both list- and sharing-tree-based algorithms, as an antichain data structure, when the antichains become exponentially larger than the dimension of the vectors. We evaluate this on applications in the synthesis of reactive systems from linear-temporal logic and parity-objective specifications, and establish empirically that current benchmarks for these computational tasks do not lead to a favorable situation for current implementations of kdtrees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09189v1</guid>
      <category>cs.LO</category>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha\"el Cadilhac, Vanessa Fl\"ugel, Guillermo A. P\'erez, Shrisha Rao</dc:creator>
    </item>
    <item>
      <title>Space-Efficient Quantum Error Reduction without log Factors</title>
      <link>https://arxiv.org/abs/2502.09249</link>
      <description>arXiv:2502.09249v1 Announce Type: cross 
Abstract: Given an algorithm that outputs the correct answer with bounded error, say $1/3$, it is sometimes desirable to reduce this error to some arbitrarily small $\varepsilon$ -- for example, if one wants to call the algorithm many times as a subroutine. The usual method, for both quantum and randomized algorithms, is a procedure called majority voting, which incurs a multiplicative overhead of $O(\log\frac{1}{\varepsilon})$ from calling the algorithm this many times.
  A recent paper introduced a model of quantum computation called \emph{transducers}, and showed how to reduce the ``error'' of a transducer arbitrarily with only constant overhead, using a construction analogous to majority voting called \emph{purification}. Even error-free transducers map to bounded-error quantum algorithms, so this does not let you reduce algorithmic error for free, but it does allow bounded-error quantum algorithms to be composed without incurring log factors.
  In this paper, we present a new highly simplified construction of a purifier, that can be understood as a weighted walk on a line similar to a random walk interpretation of majority voting. In addition to providing a new perspective that is easier to contrast with majority voting, our purifier has exponentially better space complexity than the previous one, and quadratically better dependence on the soundness-completeness gap of the algorithm being purified. Our new purifier has nearly optimal query complexity, even down to the constant, which matters when one composes quantum algorithms to super-constant depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09249v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aleksandrs Belovs, Stacey Jeffery</dc:creator>
    </item>
    <item>
      <title>Moving Matter: Efficient Reconfiguration of Tile Arrangements by a Single Active Robot</title>
      <link>https://arxiv.org/abs/2502.09299</link>
      <description>arXiv:2502.09299v1 Announce Type: cross 
Abstract: We consider the problem of reconfiguring a two-dimensional connected grid arrangement of passive building blocks from a start configuration to a goal configuration, using a single active robot that can move on the tiles, remove individual tiles from a given location and physically move them to a new position by walking on the remaining configuration. The objective is to determine a reconfiguration schedule that minimizes the overall makespan, while ensuring that the tile configuration remains connected. We provide both negative and positive results. (1) We present a generalized version of the problem, parameterized by weighted costs for moving with or without tiles, and show that this is NP-complete. (2) We give a polynomial-time constant-factor approximation algorithm for the case of disjoint start and target bounding boxes. In addition, our approach yields optimal carry distance for 2-scaled instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09299v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <category>cs.RO</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron T. Becker, S\'andor P. Fekete, Jonas Friemel, Ramin Kosfeld, Peter Kramer, Harm Kube, Christian Rieck, Christian Scheffer, Arne Schmidt</dc:creator>
    </item>
    <item>
      <title>Robust Learning of Multi-index Models via Iterative Subspace Approximation</title>
      <link>https://arxiv.org/abs/2502.09525</link>
      <description>arXiv:2502.09525v1 Announce Type: cross 
Abstract: We study the task of learning Multi-Index Models (MIMs) with label noise under the Gaussian distribution. A $K$-MIM is any function $f$ that only depends on a $K$-dimensional subspace. We focus on well-behaved MIMs with finite ranges that satisfy certain regularity properties. Our main contribution is a general robust learner that is qualitatively optimal in the Statistical Query (SQ) model. Our algorithm iteratively constructs better approximations to the defining subspace by computing low-degree moments conditional on the projection to the subspace computed thus far, and adding directions with relatively large empirical moments. This procedure efficiently finds a subspace $V$ so that $f(\mathbf{x})$ is close to a function of the projection of $\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional moments do not help, we prove an SQ lower bound suggesting that no efficient learner exists.
  As applications, we provide faster robust learners for the following concept classes:
  * {\bf Multiclass Linear Classifiers} We give a constant-factor approximate agnostic learner with sample complexity $N = O(d) 2^{\mathrm{poly}(K/\epsilon)}$ and computational complexity $\mathrm{poly}(N ,d)$. This is the first constant-factor agnostic learner for this class whose complexity is a fixed-degree polynomial in $d$.
  * {\bf Intersections of Halfspaces} We give an approximate agnostic learner for this class achieving 0-1 error $K \tilde{O}(\mathrm{OPT}) + \epsilon$ with sample complexity $N=O(d^2) 2^{\mathrm{poly}(K/\epsilon)}$ and computational complexity $\mathrm{poly}(N ,d)$. This is the first agnostic learner for this class with near-linear error dependence and complexity a fixed-degree polynomial in $d$.
  Furthermore, we show that in the presence of random classification noise, the complexity of our algorithm scales polynomially with $1/\epsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09525v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, Nikos Zarifis</dc:creator>
    </item>
    <item>
      <title>The Canadian Traveller Problem on outerplanar graphs</title>
      <link>https://arxiv.org/abs/2403.01872</link>
      <description>arXiv:2403.01872v3 Announce Type: replace 
Abstract: We study the $k$-Canadian Traveller Problem, where a weighted graph $G=(V,E,\omega)$ with a source $s\in V$ and a target $t\in V$ are given. This problem also has a hidden input $E_* \subsetneq E$ of cardinality at most $k$ representing blocked edges. The objective is to travel from $s$ to $t$ with the minimum distance. At the beginning of the walk, the blockages $E_*$ are unknown: the traveller discovers that an edge is blocked when visiting one of its endpoints. Online algorithms, also called strategies, have been proposed for this problem and assessed with the competitive ratio, {\em i.e.}, the ratio between the distance actually traversed by the traveller divided by the distance he would have traversed knowing the blockages in advance.
  Even though the optimal competitive ratio is $2k+1$ even on unit-weighted planar graphs of treewidth 2, we design a polynomial-time strategy achieving competitive ratio 9 on unit-weighted outerplanar graphs. This value 9 also stands as a lower bound for this family of graphs as we prove that, for any $\varepsilon &gt; 0$, no strategy can achieve a competitive ratio $9-\varepsilon$ on it. This comes actually from a strong connexion with another well-known online problem called the cow-path problem.
  Finally, we show that it is not possible to achieve a competitive ratio $e^{W(\frac{\ln k}{2})} - 1$ on arbitrarily weighted outerplanar graphs, where $W$ is the Lambert W function. This lower bound is asymptotically greater than $\frac{\ln k}{\ln \ln k}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01872v3</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Laurent Beaudou, Pierre Berg\'e, Vsevolod Chernyshev, Antoine Dailly, Yan Gerard, Aur\'elie Lagoutte, Vincent Limouzy, Lucas Pastor</dc:creator>
    </item>
    <item>
      <title>Fast and Simple $(1+\epsilon)\Delta$-Edge-Coloring of Dense Graphs</title>
      <link>https://arxiv.org/abs/2408.16692</link>
      <description>arXiv:2408.16692v2 Announce Type: replace 
Abstract: Let $\epsilon \in (0, 1)$ and $n, \Delta \in \mathbb N$ be such that $\Delta = \Omega\left(\max\left\{\frac{\log n}{\epsilon},\, \left(\frac{1}{\epsilon}\log \frac{1}{\epsilon}\right)^2\right\}\right)$. Given an $n$-vertex $m$-edge simple graph $G$ of maximum degree $\Delta$, we present a randomized $O\left(m\,\log^3 \Delta\,/\,\epsilon^2\right)$-time algorithm that computes a proper $(1+\epsilon)\Delta$-edge-coloring of $G$ with high probability. This improves upon the best known results for a wide range of the parameters $\epsilon$, $n$, and $\Delta$. Our approach combines a flagging strategy from earlier work of the author with a shifting procedure employed by Duan, He, and Zhang for dynamic edge-coloring. The resulting algorithm is simple to implement and may be of practical interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16692v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Dhawan</dc:creator>
    </item>
    <item>
      <title>Hyperbolic Random Graphs: Clique Number and Degeneracy with Implications for Colouring</title>
      <link>https://arxiv.org/abs/2410.11549</link>
      <description>arXiv:2410.11549v2 Announce Type: replace 
Abstract: Hyperbolic random graphs inherit many properties that are present in real-world networks. The hyperbolic geometry imposes a scale-free network with a strong clustering coefficient. Other properties like a giant component, the small world phenomena and others follow. This motivates the design of simple algorithms for hyperbolic random graphs.
  In this paper we consider threshold hyperbolic random graphs (HRGs). Greedy heuristics are commonly used in practice as they deliver a good approximations to the optimal solution even though their theoretical analysis would suggest otherwise. A typical example for HRGs are degeneracy-based greedy algorithms [Bl\"asius, Fischbeck; Transactions of Algorithms '24]. In an attempt to bridge this theory-practice gap we characterise the parameter of degeneracy yielding a simple approximation algorithm for colouring HRGs. The approximation ratio of our algorithm ranges from $(2/\sqrt{3})$ to $4/3$ depending on the power-law exponent of the model. We complement our findings for the degeneracy with new insights on the clique number of hyperbolic random graphs. We show that degeneracy and clique number are substantially different and derive an improved upper bound on the clique number. Additionally, we show that the core of HRGs does not constitute the largest clique.
  Lastly we demonstrate that the degeneracy of the closely related standard model of geometric inhomogeneous random graphs behaves inherently different compared to the one of hyperbolic random graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11549v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Baguley, Yannic Maus, Janosch Ruff, George Skretas</dc:creator>
    </item>
    <item>
      <title>Understanding the Kronecker Matrix-Vector Complexity of Linear Algebra</title>
      <link>https://arxiv.org/abs/2502.08029</link>
      <description>arXiv:2502.08029v2 Announce Type: replace 
Abstract: We study the computational model where we can access a matrix $\mathbf{A}$ only by computing matrix-vector products $\mathbf{A}\mathrm{x}$ for vectors of the form $\mathrm{x} = \mathrm{x}_1 \otimes \cdots \otimes \mathrm{x}_q$. We prove exponential lower bounds on the number of queries needed to estimate various properties, including the trace and the top eigenvalue of $\mathbf{A}$. Our proofs hold for all adaptive algorithms, modulo a mild conditioning assumption on the algorithm's queries. We further prove that algorithms whose queries come from a small alphabet (e.g., $\mathrm{x}_i \in \{\pm1\}^n$) cannot test if $\mathbf{A}$ is identically zero with polynomial complexity, despite the fact that a single query using Gaussian vectors solves the problem with probability 1. In steep contrast to the non-Kronecker case, this shows that sketching $\mathbf{A}$ with different distributions of the same subguassian norm can yield exponentially different query complexities. Our proofs follow from the observation that random vectors with Kronecker structure have exponentially smaller inner products than their non-Kronecker counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08029v2</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raphael A. Meyer, William Swartworth, David P. Woodruff</dc:creator>
    </item>
    <item>
      <title>Shortcuts and Transitive-Closure Spanners Approximation</title>
      <link>https://arxiv.org/abs/2502.08032</link>
      <description>arXiv:2502.08032v2 Announce Type: replace 
Abstract: We study polynomial-time approximation algorithms for two closely-related problems, namely computing shortcuts and transitive-closure spanners (TC spanner). For a directed unweighted graph $G=(V, E)$ and an integer $d$, a set of edges $E'\subseteq V\times V$ is called a $d$-TC spanner of $G$ if the graph $H:=(V, E')$ has (i) the same transitive-closure as $G$ and (ii) diameter at most $d.$ The set $E''\subseteq V\times V$ is a $d$-shortcut of $G$ if $E\cup E''$ is a $d$-TC spanner of $G$. Our focus is on the following $(\alpha_D, \alpha_S)$-approximation algorithm: given a directed graph $G$ and integers $d$ and $s$ such that $G$ admits a $d$-shortcut (respectively $d$-TC spanner) of size $s$, find a $(d\alpha_D)$-shortcut (resp. $(d\alpha_D)$-TC spanner) with $s\alpha_S$ edges, for as small $\alpha_S$ and $\alpha_D$ as possible.
  As our main result, we show that, under the Projection Game Conjecture (PGC), there exists a small constant $\epsilon&gt;0$, such that no polynomial-time $(n^{\epsilon},n^{\epsilon})$-approximation algorithm exists for finding $d$-shortcuts as well as $d$-TC spanners of size $s$. Previously, super-constant lower bounds were known only for $d$-TC spanners with constant $d$ and ${\alpha_D}=1$ [Bhattacharyya, Grigorescu, Jung, Raskhodnikova, Woodruff 2009]. Similar lower bounds for super-constant $d$ were previously known only for a more general case of directed spanners [Elkin, Peleg 2000]. No hardness of approximation result was known for shortcuts prior to our result.
  As a side contribution, we complement the above with an upper bound of the form $(n^{\gamma_D}, n^{\gamma_S})$-approximation which holds for $3\gamma_D + 2\gamma_S &gt; 1$ (e.g., $(n^{1/5+o(1)}, n^{1/5+o(1)})$-approximation).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08032v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parinya Chalermsook, Yonggang Jiang, Sagnik Mukhopadhyay, Danupon Nanongkai</dc:creator>
    </item>
    <item>
      <title>Finding a Largest-Area Triangle in a Terrain in Near-Linear Time</title>
      <link>https://arxiv.org/abs/2104.11420</link>
      <description>arXiv:2104.11420v3 Announce Type: replace-cross 
Abstract: A terrain is an $x$-monotone polygon whose lower boundary is a single line segment. We present an algorithm to find in a terrain a triangle of largest area in $O(n \log n)$ time, where $n$ is the number of vertices defining the terrain. The best previous algorithm for this problem has a running time of $O(n^2)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.11420v3</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergio Cabello, Arun Kumar Das, Sandip Das, Joydeep Mukherjee</dc:creator>
    </item>
    <item>
      <title>Quantum Subroutine Composition</title>
      <link>https://arxiv.org/abs/2209.14146</link>
      <description>arXiv:2209.14146v3 Announce Type: replace-cross 
Abstract: An important tool in algorithm design is the ability to build algorithms from other algorithms that run as subroutines. In the case of quantum algorithms, a subroutine may be called on a superposition of different inputs, which complicates things. For example, a classical algorithm that calls a subroutine $Q$ times, where the average probability of querying the subroutine on input $i$ is $p_i$, and the cost of the subroutine on input $i$ is $T_i$, incurs expected cost $Q\sum_i p_i E[T_i]$ from all subroutine queries. While this statement is obvious for classical algorithms, for quantum algorithms, it is much less so, since naively, if we run a quantum subroutine on a superposition of inputs, we need to wait for all branches of the superposition to terminate before we can apply the next operation. We nonetheless show an analogous quantum statement (*): If $q_i$ is the average query weight on $i$ over all queries, the cost from all quantum subroutine queries is $Q\sum_i q_i E[T_i]$. Here the query weight on $i$ for a particular query is the probability of measuring $i$ in the input register if we were to measure right before the query.
  We prove this result using the technique of multidimensional quantum walks, recently introduced in arXiv:2208.13492. We present a more general version of their quantum walk edge composition result, which yields variable-time quantum walks, generalizing variable-time quantum search, by, for example, replacing the update cost with $\sqrt{\sum_{u,v}\pi_u P_{u,v} E[T_{u,v}^2]}$, where $T_{u,v}$ is the cost to move from vertex $u$ to vertex $v$. The same technique that allows us to compose quantum subroutines in quantum walks can also be used to compose in any quantum algorithm, which is how we prove (*).</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.14146v3</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stacey Jeffery</dc:creator>
    </item>
    <item>
      <title>The Robust Bilevel Selection Problem</title>
      <link>https://arxiv.org/abs/2401.03951</link>
      <description>arXiv:2401.03951v3 Announce Type: replace-cross 
Abstract: In bilevel optimization problems, a leader and a follower make their decisions in a hierarchy, and both decisions may influence each other. Usually one assumes that both players have full knowledge also of the other player's data. In a more realistic model, uncertainty can be quantified, e.g., using the robust optimization approach: We assume that the leader does not know the follower's objective precisely, but only up to some uncertainty set, and her aim is to optimize the worst case of the corresponding scenarios. Now the question arises how the complexity of bilevel optimization changes under the additional complications of this uncertainty.
  We make a further step towards answering this question by examining an easy bilevel problem. In the Bilevel Selection Problem (BSP), the leader and the follower each select some items from their own item set, while a common number of items to select in total is given, and each player minimizes the total costs of the selected items, according to different sets of item costs. We show that the BSP can be solved in polynomial time and then investigate its robust version. If the two players' item sets are disjoint, it can still be solved in polynomial time for several types of uncertainty sets. Otherwise, we show that the Robust BSP is NP-hard and present a 2-approximation algorithm and exact exponential-time approaches.
  Furthermore, we investigate variants of the BSP where one or both of the two players take a continuous decision. One variant leads to an example of a bilevel optimization problem whose optimal value may not be attained. For the Robust Continuous BSP, where all variables are continuous, we also develop a new approach for the setting of discrete uncorrelated uncertainty, which gives a polynomial-time algorithm for the Robust Continuous BSP and a pseudopolynomial-time algorithm for the Robust Bilevel Continuous Knapsack Problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03951v3</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dorothee Henke</dc:creator>
    </item>
    <item>
      <title>Improved Algorithms for Contextual Dynamic Pricing</title>
      <link>https://arxiv.org/abs/2406.11316</link>
      <description>arXiv:2406.11316v3 Announce Type: replace-cross 
Abstract: In contextual dynamic pricing, a seller sequentially prices goods based on contextual information. Buyers will purchase products only if the prices are below their valuations. The goal of the seller is to design a pricing strategy that collects as much revenue as possible. We focus on two different valuation models. The first assumes that valuations linearly depend on the context and are further distorted by noise. Under minor regularity assumptions, our algorithm achieves an optimal regret bound of $\tilde{\mathcal{O}}(T^{2/3})$, improving the existing results. The second model removes the linearity assumption, requiring only that the expected buyer valuation is $\beta$-H\"older in the context. For this model, our algorithm obtains a regret $\tilde{\mathcal{O}}(T^{d+2\beta/d+3\beta})$, where $d$ is the dimension of the context space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11316v3</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matilde Tullii, Solenne Gaucher, Nadav Merlis, Vianney Perchet</dc:creator>
    </item>
    <item>
      <title>Beyond Non-Degeneracy: Revisiting Certainty Equivalent Heuristic for Online Linear Programming</title>
      <link>https://arxiv.org/abs/2501.01716</link>
      <description>arXiv:2501.01716v2 Announce Type: replace-cross 
Abstract: The Certainty Equivalent heuristic (CE) is a widely-used algorithm for various dynamic resource allocation problems in OR and OM. Despite its popularity, existing theoretical guarantees of CE are limited to settings satisfying restrictive fluid regularity conditions, particularly, the non-degeneracy conditions, under the widely held belief that the violation of such conditions leads to performance deterioration and necessitates algorithmic innovation beyond CE.
  In this work, we conduct a refined performance analysis of CE within the general framework of online linear programming. We show that CE achieves uniformly near-optimal regret (up to a polylogarithmic factor in $T$) under only mild assumptions on the underlying distribution, without relying on any fluid regularity conditions. Our result implies that, contrary to prior belief, CE effectively beats the curse of degeneracy for a wide range of problem instances with continuous conditional reward distributions, highlighting the distinction of the problem's structure between discrete and non-discrete settings. Our explicit regret bound interpolates between the mild $(\log T)^2$ regime and the worst-case $\sqrt{T}$ regime with a parameter $\beta$ quantifying the minimal rate of probability accumulation of the conditional reward distributions, generalizing prior findings in the multisecretary setting.
  To achieve these results, we develop novel algorithmic analytical techniques. Drawing tools from the empirical processes theory, we establish strong concentration analysis of the solutions to random linear programs, leading to improved regret analysis under significantly relaxed assumptions. These techniques may find potential applications in broader online decision-making contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01716v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilun Chen, Wenjia Wang</dc:creator>
    </item>
  </channel>
</rss>
