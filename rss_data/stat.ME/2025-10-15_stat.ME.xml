<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Oct 2025 01:45:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Monitoring 3D Lattice Structures in Additive Manufacturing Using Topological Data Analysis</title>
      <link>https://arxiv.org/abs/2510.11740</link>
      <description>arXiv:2510.11740v1 Announce Type: new 
Abstract: We present a new method for the statistical process control of lattice structures using tools from Topological Data Analysis. Motivated by applications in additive manufacturing, such as aerospace components and biomedical implants, where hollow lattice geometries are critical, the proposed framework is based on monitoring the persistent homology properties of parts. Specifically, we focus on homological features of dimensions zero and one, corresponding to connected components and one-dimensional loops, to characterize and detect changes in the topology of lattice structures. A nonparametric hypothesis testing procedure and a control charting scheme are introduced to monitor these features during production. Furthermore, we conduct extensive run-length analysis via various simulated but real-life lattice-structured parts. Our results demonstrate that persistent homology is well-suited for detecting topological anomalies in complex geometries and offers a robust, intrinsically geometrical alternative to other SPC methods for mesh and point data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11740v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulin An, Xueqi Zhao, Enrique del Castillo</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing for the dimension of random geometric graph</title>
      <link>https://arxiv.org/abs/2510.11844</link>
      <description>arXiv:2510.11844v1 Announce Type: new 
Abstract: Random geometric graphs (RGGs) offer a powerful tool for analyzing the geometric and dependence structures in real-world networks. For example, it has been observed that RGGs are a good model for protein-protein interaction networks. In RGGs, nodes are randomly distributed over an $m$-dimensional metric space, and edges connect the nodes if and only if their distance is less than some threshold. When fitting RGGs to real-world networks, the first step is probably to input or estimate the dimension $m$. However, it is not clear whether the prespecified dimension is equal to the true dimension. In this paper, we investigate this problem using hypothesis testing. Under the null hypothesis, the dimension is equal to a specific value, while the alternative hypothesis asserts the dimension is not equal to that value. We propose the first statistical test. Under the null hypothesis, the proposed test statistic converges in law to the standard normal distribution, and under the alternative hypothesis, the test statistic is unbounded in probability. We derive the asymptotic distribution by leveraging the asymptotic theory of degenerate U-statistics with kernel function dependent on the number of nodes. This approach differs significantly from prevailing methods used in network hypothesis testing problems. Moreover, we also propose an efficient approach to compute the test statistic based on the adjacency matrix. Simulation studies show that the proposed test performs well. We also apply the proposed test to multiple real-world networks to test their dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11844v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingao Yuan, Feng Yu</dc:creator>
    </item>
    <item>
      <title>Contrastive Dimension Reduction: A Systematic Review</title>
      <link>https://arxiv.org/abs/2510.11847</link>
      <description>arXiv:2510.11847v1 Announce Type: new 
Abstract: Contrastive dimension reduction (CDR) methods aim to extract signal unique to or enriched in a treatment (foreground) group relative to a control (background) group. This setting arises in many scientific domains, such as genomics, imaging, and time series analysis, where traditional dimension reduction techniques such as Principal Component Analysis (PCA) may fail to isolate the signal of interest. In this review, we provide a systematic overview of existing CDR methods. We propose a pipeline for analyzing case-control studies together with a taxonomy of CDR methods based on their assumptions, objectives, and mathematical formulations, unifying disparate approaches under a shared conceptual framework. We highlight key applications and challenges in existing CDR methods, and identify open questions and future directions. By providing a clear framework for CDR and its applications, we aim to facilitate broader adoption and motivate further developments in this emerging field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11847v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Hawke, Eric Zhang, Jiawen Chen, Didong Li</dc:creator>
    </item>
    <item>
      <title>A Martingale Kernel Two-Sample Test</title>
      <link>https://arxiv.org/abs/2510.11853</link>
      <description>arXiv:2510.11853v1 Announce Type: new 
Abstract: The Maximum Mean Discrepancy (MMD) is a widely used multivariate distance metric for two-sample testing. The standard MMD test statistic has an intractable null distribution typically requiring costly resampling or permutation approaches for calibration. In this work we leverage a martingale interpretation of the estimated squared MMD to propose martingale MMD (mMMD), a quadratic-time statistic which has a limiting standard Gaussian distribution under the null. Moreover we show that the test is consistent against any fixed alternative and for large sample sizes, mMMD offers substantial computational savings over the standard MMD test, with only a minor loss in power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11853v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Chatterjee, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>On the permutation invariance principle for causal estimands</title>
      <link>https://arxiv.org/abs/2510.11863</link>
      <description>arXiv:2510.11863v1 Announce Type: new 
Abstract: In many causal inference problems, multiple action variables share the same causal role, such as mediators, factors, network units, or genotypes, yet lack a natural ordering. To avoid ambiguity in interpretation, causal estimands should remain unchanged under relabeling, an implicit principle we refer to as permutation invariance. We formally characterize this principle, analyze its algebraic and combinatorial structure for verification, and present a class of weighted estimands that are permutation-invariant while capturing interactions of all orders. We further provide guidance on selecting weights that yield residual-free estimands, whose inclusion-exclusion sums capture the maximal effect, and extend our results to ratio effect measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11863v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiaqi Tong, Fan Li</dc:creator>
    </item>
    <item>
      <title>Inhomogeneous continuous-time Markov chains to infer flexible time-varying evolutionary rates</title>
      <link>https://arxiv.org/abs/2510.11982</link>
      <description>arXiv:2510.11982v1 Announce Type: new 
Abstract: Reconstructing evolutionary histories and estimating the rate of evolution from molecular sequence data is of central importance in evolutionary biology and infectious disease research. We introduce a flexible Bayesian phylogenetic inference framework that accommodates changing evolutionary rates over time by modeling sequence character substitution processes as inhomogeneous continuous-time Markov chains (ICTMCs) acting along the unknown phylogeny, where the rate remains as an unknown, positive and integrable function of time. The integral of the rate function appears in the finite-time transition probabilities of the ICTMCs that must be efficiently computed for all branches of the phylogeny to evaluate the observed data likelihood. Circumventing computational challenges that arise from a fully nonparametric function, we successfully parameterize the rate function as piecewise constant with a large number of epochs that we call the polyepoch clock model. This makes the transition probability computation relatively inexpensive and continues to flexibly capture rate change over time. We employ a Gaussian Markov random field prior to achieve temporal smoothing of the estimated rate function. Hamiltonian Monte Carlo sampling enabled by scalable gradient evaluation under this model makes our framework computationally efficient. We assess the performance of the polyepoch clock model in recovering the true timescales and rates through simulations under two different evolutionary scenarios. We then apply the polyepoch clock model to examine the rates of West Nile virus, Dengue virus and influenza A/H3N2 evolution, and estimate the time-varying rate of SARS-CoV-2 spread in Europe in 2020.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11982v1</guid>
      <category>stat.ME</category>
      <category>q-bio.PE</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pratyusa Datta, Philippe Lemey, Marc A. Suchard</dc:creator>
    </item>
    <item>
      <title>Robust Functional Logistic Regression</title>
      <link>https://arxiv.org/abs/2510.12048</link>
      <description>arXiv:2510.12048v1 Announce Type: new 
Abstract: Functional logistic regression is a popular model to capture a linear relationship between binary response and functional predictor variables. However, many methods used for parameter estimation in functional logistic regression are sensitive to outliers, which may lead to inaccurate parameter estimates and inferior classification accuracy. We propose a robust estimation procedure for functional logistic regression, in which the observations of the functional predictor are projected onto a set of finite-dimensional subspaces via robust functional principal component analysis. This dimension-reduction step reduces the outlying effects in the functional predictor. The logistic regression coefficient is estimated using an M-type estimator based on binary response and robust principal component scores. In doing so, we provide robust estimates by minimizing the effects of outliers in the binary response and functional predictor variables. Via a series of Monte-Carlo simulations and using hand radiograph data, we examine the parameter estimation and classification accuracy for the response variable. We find that the robust procedure outperforms some existing robust and non-robust methods when outliers are present, while producing competitive results when outliers are absent. In addition, the proposed method is computationally more efficient than some existing robust alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12048v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Berkay Akturk, Ufuk Beyaztas, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Edgington's Method for Random-Effects Meta-Analysis Part I: Estimation</title>
      <link>https://arxiv.org/abs/2510.12301</link>
      <description>arXiv:2510.12301v1 Announce Type: new 
Abstract: Meta-analysis can be formulated as combining $p$-values across studies into a joint $p$-value function, from which point estimates and confidence intervals can be derived. We extend the meta-analytic estimation framework based on combined $p$-value functions to incorporate uncertainty in heterogeneity estimation by employing a confidence distribution approach. Specifically, the confidence distribution of Edgington's method is adjusted according to the confidence distribution of the heterogeneity parameter constructed from the generalized heterogeneity statistic. Simulation results suggest that 95% confidence intervals approach nominal coverage under most scenarios involving more than three studies and heterogeneity. Under no heterogeneity or for only three studies, the confidence interval typically overcovers, but is often narrower than the Hartung-Knapp-Sidik-Jonkman interval. The point estimator exhibits small bias under model misspecification and moderate to large heterogeneity. Edgington's method provides a practical alternative to classical approaches, with adjustment for heterogeneity estimation uncertainty often improving confidence interval coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12301v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kronthaler, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>Optimal Treatment Rules under Missing Predictive Covariates: A Covariate-Balancing Doubly Robust Approach</title>
      <link>https://arxiv.org/abs/2510.12321</link>
      <description>arXiv:2510.12321v1 Announce Type: new 
Abstract: In precision medicine, one of the most important problems is estimating the optimal individualized treatment rules (ITR), which typically involves recommending treatment decisions based on fully observed individual characteristics of patients to maximize overall clinical benefit. In practice, however, there may be missing covariates that are not necessarily confounders, and it remains uncertain whether these missing covariates should be included for learning optimal ITRs. In this paper, we propose a covariate-balancing doubly robust estimator for constructing optimal ITRs, which is particularly suitable for situations with additional predictive covariates. The proposed method is based on two main steps: First, the propensity scores are estimated by solving the covariate-balancing equation. Second, an objective function is minimized to estimate the outcome model, with the function defined by the asymptotic variance under the correctly specified propensity score. The method has three significant advantages: (i) It is doubly robust, ensuring consistency when either the propensity score or outcome model is correctly specified. (ii) It minimizes variance within the class of augmented inverse probability weighted estimators. (iii) When applied to partially observed covariates related to the outcome, the method may further improve estimation efficiency. We demonstrate the proposed method through extensive numerical simulations and two real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12321v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Zhang, Shanshan Luo, Zhi Geng, Yangbo He</dc:creator>
    </item>
    <item>
      <title>Sliding-Window Signatures for Time Series: Application to Electricity Demand Forecasting</title>
      <link>https://arxiv.org/abs/2510.12337</link>
      <description>arXiv:2510.12337v1 Announce Type: new 
Abstract: Nonlinear and delayed effects of covariates often render time series forecasting challenging. To this end, we propose a novel forecasting framework based on ridge regression with signature features calculated on sliding windows. These features capture complex temporal dynamics without relying on learned or hand-crafted representations. Focusing on the discrete-time setting, we establish theoretical guarantees, namely universality of approximation and stationarity of signatures. We introduce an efficient sequential algorithm for computing signatures on sliding windows. The method is evaluated on both synthetic and real electricity demand data. Results show that signature features effectively encode temporal and nonlinear dependencies, yielding accurate forecasts competitive with those based on expert knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12337v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nina Drobac (LPSM), Margaux Br\'eg\`ere (LPSM), Joseph de Vilmarest (LPSM), Olivier Wintenberger (LPSM)</dc:creator>
    </item>
    <item>
      <title>Variational Inference for Count Response Semiparametric Regression: A Convex Solution</title>
      <link>https://arxiv.org/abs/2510.12356</link>
      <description>arXiv:2510.12356v1 Announce Type: new 
Abstract: We develop a version of variational inference for Bayesian count response regression-type models that possesses attractive attributes such as convexity and closed form updates. The convex solution aspect entails numerically stable fitting algorithms, whilst the closed form aspect makes the methodology fast and easy to implement. The essence of the approach is the use of P\'olya-Gamma augmentation of a Negative Binomial likelihood, a finite-valued prior on the shape parameter and the structured mean field variational Bayes paradigm. The approach applies to general count response situations. For concreteness, we focus on generalized linear mixed models within the semiparametric regression class of models. Real-time fitting is also described.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12356v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Virginia Murru, Matt P. Wand</dc:creator>
    </item>
    <item>
      <title>Causal inference of post-transcriptional regulation timelines from long-read sequencing in Arabidopsis thaliana</title>
      <link>https://arxiv.org/abs/2510.12504</link>
      <description>arXiv:2510.12504v1 Announce Type: new 
Abstract: We propose a novel framework for reconstructing the chronology of genetic regulation using causal inference based on Pearl's theory. The approach proceeds in three main stages: causal discovery, causal inference, and chronology construction. We apply it to the ndhB and ndhD genes of the chloroplast in Arabidopsis thaliana, generating four alternative maturation timeline models per gene, each derived from a different causal discovery algorithm (HC, PC, LiNGAM, or NOTEARS). Two methodological challenges are addressed: the presence of missing data, handled via an EM algorithm that jointly imputes missing values and estimates the Bayesian network, and the selection of the $\ell_1$-regularization parameter in NOTEARS, for which we introduce a stability selection strategy. The resulting causal models consistently outperform reference chronologies in terms of both reliability and model fit. Moreover, by combining causal reasoning with domain expertise, the framework enables the formulation of testable hypotheses and the design of targeted experimental interventions grounded in theoretical predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12504v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rub\'en Martos, Christophe Ambroise, Guillem Rigaill</dc:creator>
    </item>
    <item>
      <title>On goodness-of-fit testing for volatility in McKean-Vlasov models</title>
      <link>https://arxiv.org/abs/2510.12607</link>
      <description>arXiv:2510.12607v1 Announce Type: new 
Abstract: This paper develops a statistical framework for goodness-of-fit testing of volatility functions in McKean-Vlasov stochastic differential equations, which describe large systems of interacting particles with distribution-dependent dynamics. While integrated volatility estimation in classical SDEs is now well established, formal model validation and goodness-of-fit testing for McKean-Vlasov systems remain largely unexplored, particularly in regimes with both large particle limits and high-frequency sampling. We propose a test statistic based on discrete observations of particle systems, analysed in a joint regime where both the number of particles and the sampling frequency increase. The estimators involved are proven to be consistent, and the test statistic is shown to satisfy a central limit theorem, converging in distribution to a centred Gaussian law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12607v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akram Heidari, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>The $\alpha$--regression for compositional data: a unified framework for standard, spatially-lagged, and geographically-weighted regression models</title>
      <link>https://arxiv.org/abs/2510.12663</link>
      <description>arXiv:2510.12663v1 Announce Type: new 
Abstract: Compositional data-vectors of non--negative components summing to unity--frequently arise in scientific applications where covariates influence the relative proportions of components, yet traditional regression approaches struggle with the unit-sum constraint and zero values. This paper revisits the $\alpha$--regression framework, which uses a flexible power transformation parameterized by $\alpha$ to interpolate between raw data analysis and log-ratio methods, naturally handling zeros without imputation while allowing data-driven transformation selection. We formulate $\alpha$--regression as a non-linear least squares problem, provide efficient estimation via the Levenberg-Marquardt algorithm with explicit gradient and Hessian derivations, establish asymptotic normality of the estimators, and derive marginal effects for interpretation. The framework is extended to spatial settings through two models: the $\alpha$--spatially lagged X regression model, which incorporates spatial spillover effects via spatially lagged covariates with decomposition into direct and indirect effects, and the geographically weighted $\alpha$--regression, which allows coefficients to vary spatially for capturing local relationships. Application to Greek agricultural land-use data demonstrates that spatial extensions substantially improve predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12663v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>The Robustness of Differentiable Causal Discovery in Misspecified Scenarios</title>
      <link>https://arxiv.org/abs/2510.12503</link>
      <description>arXiv:2510.12503v1 Announce Type: cross 
Abstract: Causal discovery aims to learn causal relationships between variables from targeted data, making it a fundamental task in machine learning. However, causal discovery algorithms often rely on unverifiable causal assumptions, which are usually difficult to satisfy in real-world data, thereby limiting the broad application of causal discovery in practical scenarios. Inspired by these considerations, this work extensively benchmarks the empirical performance of various mainstream causal discovery algorithms, which assume i.i.d. data, under eight model assumption violations. Our experimental results show that differentiable causal discovery methods exhibit robustness under the metrics of Structural Hamming Distance and Structural Intervention Distance of the inferred graphs in commonly used challenging scenarios, except for scale variation. We also provide the theoretical explanations for the performance of differentiable causal discovery methods. Finally, our work aims to comprehensively benchmark the performance of recent differentiable causal discovery methods under model assumption violations, and provide the standard for reasonable evaluation of causal discovery, as well as to further promote its application in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12503v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiyang Yi, Yanyan He, Duxin Chen, Mingyu Kang, He Wang, Wenwu Yu</dc:creator>
    </item>
    <item>
      <title>Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of Experts: Consistency without Model Sweeps</title>
      <link>https://arxiv.org/abs/2510.12744</link>
      <description>arXiv:2510.12744v1 Announce Type: cross 
Abstract: We develop a unified statistical framework for softmax-gated Gaussian mixture of experts (SGMoE) that addresses three long-standing obstacles in parameter estimation and model selection: (i) non-identifiability of gating parameters up to common translations, (ii) intrinsic gate-expert interactions that induce coupled differential relations in the likelihood, and (iii) the tight numerator-denominator coupling in the softmax-induced conditional density. Our approach introduces Voronoi-type loss functions aligned with the gate-partition geometry and establishes finite-sample convergence rates for the maximum likelihood estimator (MLE). In over-specified models, we reveal a link between the MLE's convergence rate and the solvability of an associated system of polynomial equations characterizing near-nonidentifiable directions. For model selection, we adapt dendrograms of mixing measures to SGMoE, yielding a consistent, sweep-free selector of the number of experts that attains pointwise-optimal parameter rates under overfitting while avoiding multi-size training. Simulations on synthetic data corroborate the theory, accurately recovering the expert count and achieving the predicted rates for parameter estimation while closely approximating the regression function. Under model misspecification (e.g., $\epsilon$-contamination), the dendrogram selection criterion is robust, recovering the true number of mixture components, while the Akaike information criterion, the Bayesian information criterion, and the integrated completed likelihood tend to overselect as sample size grows. On a maize proteomics dataset of drought-responsive traits, our dendrogram-guided SGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes the likelihood early, and yields interpretable genotype-phenotype maps, outperforming standard criteria without multi-size training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12744v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Do Tien Hai, Trung Nguyen Mai, TrungTin Nguyen, Nhat Ho, Binh T. Nguyen, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Sample-Efficient Omniprediction for Proper Losses</title>
      <link>https://arxiv.org/abs/2510.12769</link>
      <description>arXiv:2510.12769v1 Announce Type: cross 
Abstract: We consider the problem of constructing probabilistic predictions that lead to accurate decisions when employed by downstream users to inform actions. For a single decision maker, designing an optimal predictor is equivalent to minimizing a proper loss function corresponding to the negative utility of that individual. For multiple decision makers, our problem can be viewed as a variant of omniprediction in which the goal is to design a single predictor that simultaneously minimizes multiple losses. Existing algorithms for achieving omniprediction broadly fall into two categories: 1) boosting methods that optimize other auxiliary targets such as multicalibration and obtain omniprediction as a corollary, and 2) adversarial two-player game based approaches that estimate and respond to the ``worst-case" loss in an online fashion. We give lower bounds demonstrating that multicalibration is a strictly more difficult problem than omniprediction and thus the former approach must incur suboptimal sample complexity. For the latter approach, we discuss how these ideas can be used to obtain a sample-efficient algorithm through an online-to-batch conversion. This conversion has the downside of returning a complex, randomized predictor. We improve on this method by designing a more direct, unrandomized algorithm that exploits structural elements of the set of proper losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12769v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac Gibbs, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>General Bayesian L2 calibration of mathematical models</title>
      <link>https://arxiv.org/abs/2103.01132</link>
      <description>arXiv:2103.01132v4 Announce Type: replace 
Abstract: A mathematical model is a function taking certain arguments and returning a theoretical prediction of a feature of a physical system. The arguments to the mathematical model can be split into two groups; (a) controllable variables of the system; and (b) calibration parameters: unknown characteristics of the physical system that cannot be controlled or directly measured. Of interest is the estimation of the calibration parameter using physical observations. Since the mathematical model will be an inexact representation of the physical system: the aim is to estimate values for the calibration parameters to make the mathematical model ``close" to the physical system. Closeness is defined as the squared $L^2$ norm of the difference between the mathematical model and the physical system. Different Bayesian and general Bayesian methods are introduced, developed and compared for this task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.01132v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antony M. Overstall, James M. McGree</dc:creator>
    </item>
    <item>
      <title>The Power of Prognosis: Improving Covariate Balance Tests with Outcome Information</title>
      <link>https://arxiv.org/abs/2205.10478</link>
      <description>arXiv:2205.10478v2 Announce Type: replace 
Abstract: Scholars frequently use covariate balance tests to test the validity of natural experiments and related designs. Unfortunately, when measured covariates are unrelated to potential outcomes, balance is uninformative about key identification conditions. We show that balance tests can then lead to erroneous conclusions. To build stronger tests, researchers should identify covariates that are jointly predictive of potential outcomes; formally measure and report covariate prognosis; and prioritize the most individually informative variables in tests. Building on prior research on ``prognostic scores," we develop bootstrap balance tests that upweight covariates associated with the outcome. We adapt this approach for regression-discontinuity designs and use simulations to compare weighting methods based on linear regression and more flexible methods, including machine learning. The results show how prognosis weighting can avoid both false negatives and false positives. To illustrate key points, we study empirical examples from a sample of published studies, including an important debate over close elections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.10478v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clara Bicalho, Adam Bouyamourn, Thad Dunning</dc:creator>
    </item>
    <item>
      <title>Inference on the state process of periodically inhomogeneous hidden Markov models for animal behavior</title>
      <link>https://arxiv.org/abs/2312.14583</link>
      <description>arXiv:2312.14583v3 Announce Type: replace 
Abstract: Over the last decade, hidden Markov models (HMMs) have become increasingly popular in statistical ecology, where they constitute natural tools for studying animal behavior based on complex sensor data. Corresponding analyses sometimes explicitly focus on - and in any case need to take into account - periodic variation, for example by quantifying the activity distribution over the daily cycle or seasonal variation such as migratory behavior. For HMMs including periodic components, we establish important mathematical properties that allow for comprehensive statistical inference related to periodic variation, thereby also providing guidance for model building and model checking. Specifically, we derive the periodically varying unconditional state distribution as well as the time-varying and overall state dwell-time distributions - all of which are of key interest when the inferential focus lies on the dynamics of the state process. We use the associated novel inference and model-checking tools to investigate changes in the diel activity patterns of fruit flies in response to changing light conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14583v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik, Carlina C. Feldmann, Sina Mews, Rouven Michels, Roland Langrock</dc:creator>
    </item>
    <item>
      <title>Quantification of vaccine waning as a challenge effect</title>
      <link>https://arxiv.org/abs/2405.01336</link>
      <description>arXiv:2405.01336v3 Announce Type: replace 
Abstract: Knowing whether vaccine protection wanes over time is important for health policy and drug development. However, quantifying waning effects is difficult. A simple contrast of vaccine efficacy at two different times compares different populations of individuals: those who were uninfected at the first time versus those who remain uninfected until the second time. Thus, the contrast of vaccine efficacy at early and late times can not be interpreted as a causal effect. We propose to quantify vaccine waning using the challenge effect, which is a contrast of outcomes under controlled exposures to the infectious agent following vaccination. We identify sharp bounds on the challenge effect under non-parametric assumptions that are broadly applicable in vaccine trials using routinely collected data. We demonstrate that the challenge effect can differ substantially from the conventional vaccine efficacy due to depletion of susceptible individuals from the risk set over time. Finally, we apply the methods to derive bounds on the waning of the BNT162b2 COVID-19 vaccine using data from a placebo-controlled randomized trial. Our estimates of the challenge effect suggest waning protection after 2 months beyond administration of the second vaccine dose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01336v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2408776</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association, 120:549, 96-106 (2025)</arxiv:journal_reference>
      <dc:creator>Matias Janvin, Mats J. Stensrud</dc:creator>
    </item>
    <item>
      <title>Bayesian Calibration for Prediction in a Multi-Output Transposition Context</title>
      <link>https://arxiv.org/abs/2410.00116</link>
      <description>arXiv:2410.00116v4 Announce Type: replace 
Abstract: Numerical simulations are widely used to predict the behavior of physical systems, with Bayesian approaches being particularly well suited for this purpose. However, experimental observations are necessary to calibrate certain simulator parameters for the prediction. In this work, we use a multi-output simulator to predict all its outputs, including those that have never been experimentally observed. This situation is referred to as the transposition context. To accurately quantify the discrepancy between model outputs and real data in this context, conventional methods cannot be applied, and the Bayesian calibration must be augmented by incorporating a joint model error across all outputs. To achieve this, the proposed method is to consider additional numerical input parameters within a hierarchical Bayesian model, which includes hyperparameters for the prior distribution of the calibration variables. This approach is applied on a computer code with three outputs that models the Taylor cylinder impact test with a small number of observations. The outputs are considered as the observed variables one at a time, to work with three different transposition situations. The proposed method is compared with other approaches that embed model errors to demonstrate the significance of the hierarchical formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00116v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1615/Int.J.UncertaintyQuantification.2025056586</arxiv:DOI>
      <arxiv:journal_reference>International Journal for Uncertainty Quantification, Volume 15, Issue 6, 2025, pp. 37-59</arxiv:journal_reference>
      <dc:creator>Charlie Sire, Josselin Garnier, C\'edric Durantin, Baptiste Kerleguer, Gilles Defaux, Guillaume Perrin</dc:creator>
    </item>
    <item>
      <title>Bootstrap tests for almost goodness-of-fit</title>
      <link>https://arxiv.org/abs/2410.20918</link>
      <description>arXiv:2410.20918v2 Announce Type: replace 
Abstract: We introduce the \textit{almost goodness-of-fit} test, a procedure to assess whether a (parametric) model provides a good representation of the probability distribution generating the observed sample. Specifically, given a distribution function $F$ and a parametric family $\mathcal{G}=\{ G(\boldsymbol{\theta}) : \boldsymbol{\theta} \in \Theta\}$, we consider the testing problem \[ H_0: \| F - G(\boldsymbol{\theta}_F) \|_p \geq \epsilon \quad \text{vs} \quad H_1: \| F - G(\boldsymbol{\theta}_F) \|_p &lt; \epsilon, \] where $\epsilon&gt;0$ is a margin of error and $G(\boldsymbol{\theta}_F)$ denotes a representative of $F$ within the parametric class. The approximate model is determined via an M-estimator of the parameters. %The objective is the approximate validation of a distribution or an entire parametric family up to a pre-specified threshold value. The methodology also quantifies the percentage improvement of the proposed model relative to a non-informative (constant) benchmark. The test statistic is the $\mathrm{L}^p$-distance between the empirical distribution function and that of the estimated model. We present two consistent, easy-to-implement, and flexible bootstrap schemes to carry out the test. The performance of the proposal is illustrated through simulation studies and analysis and real-data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20918v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Statistics and Computing, 2025</arxiv:journal_reference>
      <dc:creator>Amparo Ba\'illo, Javier C\'arcamo</dc:creator>
    </item>
    <item>
      <title>Regularized Sparse Optimal Discriminant Clustering</title>
      <link>https://arxiv.org/abs/2501.10147</link>
      <description>arXiv:2501.10147v2 Announce Type: replace 
Abstract: We propose a new method based on sparse optimal discriminant clustering (SODC), incorporating a penalty term into the scoring matrix based on convex clustering. With the addition of this penalty term, it is expected to improve the accuracy of cluster identification by pulling points within the same cluster closer together and points from different clusters further apart. When the estimation results are visualized, the clustering structure can be depicted more clearly. Moreover, we develop a novel algorithm to derive the updated formula of this scoring matrix using a majorizing function. The scoring matrix is updated using the alternating direction method of multipliers (ADMM), which is often employed to calculate the parameters of the objective function in the convex clustering. In the proposed method, as in the conventional SODC, the scoring matrix is subject to an orthogonal constraint. Therefore, it is necessary to satisfy the orthogonal constraint on the scoring matrix while maintaining the clustering structure. Using a majorizing function, we adress the challenge of enforcing both orthogonal constraint and the clustering structure within the scoring matrix. We demonstrate numerical simulations and an application to real data to assess the performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10147v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mayu Hiraishi, Kensuke Tanioka, Hiroshi Yadohisa</dc:creator>
    </item>
    <item>
      <title>A Principled Approach to Bayesian Transfer Learning</title>
      <link>https://arxiv.org/abs/2502.19796</link>
      <description>arXiv:2502.19796v3 Announce Type: replace 
Abstract: Updating $\textit{a priori}$ information given some observed data is the core tenet of Bayesian inference. Bayesian transfer learning extends this idea by incorporating information from a related dataset to improve the inference on the observed target dataset which may have been collected under slightly different settings. The use of related information can be useful when the target dataset is scarce, for example. There exist various Bayesian transfer learning methods that decide how to incorporate the related data in different ways. Unfortunately, there is no principled approach for comparing Bayesian transfer methods in real data settings. Additionally, some Bayesian transfer learning methods, such as the so-called power prior approaches, rely on conjugacy or costly specialised techniques. In this paper, we find an effective approach to compare Bayesian transfer learning methods is to apply leave-one-out cross validation on the target dataset. Further, we introduce a new framework, $\textit{transfer sequential Monte Carlo}$, that efficiently implements power prior methods in an automated fashion. We demonstrate the performance of our proposed methods in two comprehensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19796v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Bretherton, Joshua J. Bon, David J. Warne, Kerrie Mengersen, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random</title>
      <link>https://arxiv.org/abs/2505.19093</link>
      <description>arXiv:2505.19093v2 Announce Type: replace 
Abstract: Model-based clustering integrated with variable selection is a powerful tool for uncovering latent structures within complex data. However, its effectiveness is often hindered by challenges such as identifying relevant variables that define heterogeneous subgroups and handling data that are missing not at random, a prevalent issue in fields like transcriptomics. While several notable methods have been proposed to address these problems, they typically tackle each issue in isolation, thereby limiting their flexibility and adaptability. This paper introduces a unified framework designed to address these challenges simultaneously. Our approach incorporates a data-driven penalty matrix into penalized clustering to enable more flexible variable selection, along with a mechanism that explicitly models the relationship between missingness and latent class membership. We demonstrate that, under certain regularity conditions, the proposed framework achieves both asymptotic consistency and selection consistency, even in the presence of missing data. This unified strategy significantly enhances the capability and efficiency of model-based clustering, advancing methodologies for identifying informative variables that define homogeneous subgroups in the presence of complex missing data patterns. The performance of the framework, including its computational efficiency, is evaluated through simulations and demonstrated using both synthetic and real-world transcriptomic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19093v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>39th Conference on Neural Information Processing Systems (NeurIPS 2025)</arxiv:journal_reference>
      <dc:creator>Binh H. Ho, Long Nguyen Chi, TrungTin Nguyen, Binh T. Nguyen, Van Ha Hoang, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Disentangling network dependence among multiple variables</title>
      <link>https://arxiv.org/abs/2506.20974</link>
      <description>arXiv:2506.20974v2 Announce Type: replace 
Abstract: When two variables depend on the same or similar underlying network, their shared network dependence structure can lead to spurious associations. While statistical associations between two variables sampled from interconnected subjects are a common inferential goal across various fields, little research has focused on how to disentangle shared dependence for valid statistical inference. We revisit two different approaches from distinct fields that may address shared network dependence: the pre-whitening approach, commonly used in time series analysis to remove the shared temporal dependence, and the network autocorrelation model, widely used in network analysis often to examine or account for autocorrelation of the outcome variable. We demonstrate how each approach implicitly entails assumptions about how a variable of interest propagates among nodes via network ties given the network structure. We further propose adaptations of existing pre-whitening methods to the network setting by explicitly reflecting underlying assumptions about "level of interaction" that induce network dependence, while accounting for its unique complexities. Our simulation studies demonstrate the effectiveness of the two approaches in reducing spurious associations due to shared network dependence when their respective assumptions hold. However, the results also show the sensitivity to assumption violations, underscoring the importance of correctly specifying the shared dependence structure based on available network information and prior knowledge about the interactions driving dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20974v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhejia Dong, Corwin Zigler, Youjin Lee</dc:creator>
    </item>
    <item>
      <title>Is 1:1 Always Most Powerful? Why Careful Determination of Allocation Ratios Matters in Trial Design</title>
      <link>https://arxiv.org/abs/2507.13036</link>
      <description>arXiv:2507.13036v2 Announce Type: replace 
Abstract: The principle of allocating an equal number of patients to each arm in a randomized controlled trial remains widely believed to be optimal for maximising statistical power. However, this long-held belief only holds true if the treatment groups have equal outcome variances, a condition that is often not met or, is simply not assessed in practice. This paper reasserts the fact that a departure from a 1:1 ratio can maintain or improve statistical power while increasing the benefits to participants. The benefit is particularly self-evident for binary and time-to-event endpoints, where variances are determined by the assumed success or event rates. To illustrate this, we present two case studies: a small-scale metastatic melanoma trial with a binary endpoint and a larger trial evaluating virtual reality for pain reduction with a continuous endpoint. Our simulations compare equal randomisation, preplanned fixed unequal randomisation, and response-adaptive randomisation targeting Neyman allocation. Results show that unequal allocation can increase the proportion of patients receiving the superior treatment without reducing power, with modest power gains observed in both binary and continuous settings, highlighting the practical relevance of optimised allocation strategies across trial types and sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13036v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Stef Baas, David S. Robertson, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>Model-robust standardization in stepped wedge designs</title>
      <link>https://arxiv.org/abs/2507.17190</link>
      <description>arXiv:2507.17190v2 Announce Type: replace 
Abstract: Stepped-wedge cluster-randomized trials (SW-CRTs) are widely used in healthcare and implementation science, providing an ethical advantage by ensuring all clusters eventually receive the intervention. The staggered rollout of treatment introduces complexities in defining and estimating treatment effect estimands, particularly under informative sizes. Traditional model-based methods, including generalized estimating equations (GEE) and linear mixed models (LMM), produce estimates that depend on implicit weighting schemes and parametric assumptions, leading to bias for different types of estimands in the presence of informative sizes. While recent methods have attempted to provide robust estimation in SW-CRTs, they are restrictive on modeling assumptions or lack of general framework for consistent estimating multiple estimands under informative size. In this article, we propose a model-robust standardization framework for SW-CRTs that generalizes existing methods from parallel-arm CRTs. We define causal estimands including horizontal-individual, horizontal-cluster, vertical-individual, and vertical-cluster average treatment effects under a super population framework and introduce an augmented standardization estimator that standardizes parametric and semiparametric working models while maintaining robustness to informative cluster size under arbitrary misspecification. We evaluate the finite-sample properties of our proposed estimators through extensive simulation studies, assessing their performance under various SW-CRT designs. Finally, we illustrate the practical application of model-robust estimation through a reanalysis of two real-world SW-CRTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17190v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Fang, Xueqi Wang, Patrick J. Heagerty, Bingkai Wang, Fan Li</dc:creator>
    </item>
    <item>
      <title>Generalized Jeffreys's approximate objective Bayes factor: Model-selection consistency, finite-sample accuracy, and statistical evidence in 71,126 clinical trial findings</title>
      <link>https://arxiv.org/abs/2510.10358</link>
      <description>arXiv:2510.10358v2 Announce Type: replace 
Abstract: Concerns about the misuse and misinterpretation of p-values and statistical significance have motivated alternatives for quantifying evidence. We define a generalized form of Jeffreys's approximate objective Bayes factor (eJAB), a one-line calculation that is a function of the p-value, sample size, and parameter dimension. We establish conditions under which eJAB is model-selection consistent and verify them for ten statistical tests. We assess finite-sample accuracy by comparing eJAB with Markov chain Monte Carlo computed Bayes factors in 12 simulation studies. We then apply eJAB to 71,126 results from ClinicalTrials.gov (CTG) and find that the proportion of findings with $\text{p-value} \le \alpha$ yet $eJAB_{01}&gt;1$ (favoring the null) closely tracks the significance level $\alpha$, suggesting that such contradictions are pointing to the type I errors. We catalog 4,088 such candidate type I errors and provide details for 131 with reported $\text{p-value} \le 0.01$. We also identify 487 instances of the Jeffreys-Lindley paradox. Finally, we estimate that 75% (6%) of clinical trial plans from CTG set $\alpha \ge 0.05$ as the target evidence threshold, and that 35.5% (0.22%) of results significant at $\alpha =0.05$ correspond to evidence that is no stronger than anecdotal under eJAB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10358v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Puneet Velidi, Zhengxiao Wei, Shreena Nisha Kalaria, Yimeng Liu, C\'eline M. Laumont, Brad H. Nelson, Farouk S. Nathoo</dc:creator>
    </item>
    <item>
      <title>Semi-parametric Markov models for multi-type point patterns</title>
      <link>https://arxiv.org/abs/2510.11226</link>
      <description>arXiv:2510.11226v2 Announce Type: replace 
Abstract: Multi-type Markov point processes offer a flexible framework for modelling complex multi-type point patterns where it is pertinent to capture both interactions between points as well as large scale trends depending on observed covariates. However, estimation of interaction and covariate effects may be seriously biased in the presence of unobserved spatial confounders. In this paper we introduce a new class of semi-parametric Markov point processes that adjusts for spatial confounding through a non-parametric factor that accommodates effects of latent spatial variables common to all types of points. We introduce a conditional pseudo likelihood for parameter estimation and show that the resulting estimator has desirable asymptotic properties. Our methodology not least has great potential in studies of industry agglomeration and we apply it to study spatial patterns of locations of two types of banks in France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11226v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ib Thorsgaard Jensen, Jean-Fran\c{c}ois Coeurjolly, Rasmus Waagepetersen</dc:creator>
    </item>
    <item>
      <title>Identifiability and Falsifiability: Two Challenges for Bayesian Model Expansion</title>
      <link>https://arxiv.org/abs/2307.14545</link>
      <description>arXiv:2307.14545v2 Announce Type: replace-cross 
Abstract: We study the identifiability of parameters and falsifiability of predictions under the process of model expansion in a Bayesian setting. Identifiability is represented by the closeness of the posterior to the prior distribution and falsifiability by the power of posterior predictive tests against alternatives. To study these two concepts formally, we develop information-theoretic proxies, which we term the identifiability and falsifiability mutual information. We argue that these are useful indicators, with lower values indicating a risk of poor parameter inference and underpowered model checks, respectively. Our main result establishes that a sufficiently complex expansion of a base statistical model forces a trade-off between these two mutual information quantities -- at least one of the two must decrease relative to the base model. We illustrate our result in three worked examples and extract implications for model expansion in practice. In particular, we show as an implication of our result that the negative impacts of model expansion can be limited by offsetting complexity in the likelihood with sufficiently constraining prior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14545v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Collin Cademartori</dc:creator>
    </item>
    <item>
      <title>Beyond the noise: intrinsic dimension estimation with optimal neighbourhood identification</title>
      <link>https://arxiv.org/abs/2405.15132</link>
      <description>arXiv:2405.15132v3 Announce Type: replace-cross 
Abstract: The Intrinsic Dimension (ID) is a key concept in unsupervised learning and feature selection, as it is a lower bound to the number of variables which are necessary to describe a system. However, in almost any real-world dataset the ID depends on the scale at which the data are analysed. Quite typically at a small scale, the ID is very large, as the data are affected by measurement errors. At large scale, the ID can also be erroneously large, due to the curvature and the topology of the manifold containing the data. In this work, we introduce an automatic protocol to select the sweet spot, namely the correct range of scales in which the ID is meaningful and useful. This protocol is based on imposing that for distances smaller than the correct scale the density of the data is constant. In the presented framework, to estimate the density it is necessary to know the ID, therefore, this condition is imposed self-consistently. We illustrate the usefulness and robustness of this procedure by benchmarks on artificial and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15132v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Di Noia, Iuri Macocco, Aldo Glielmo, Alessandro Laio, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>Constrained Identifiability of Causal Effects</title>
      <link>https://arxiv.org/abs/2412.02869</link>
      <description>arXiv:2412.02869v2 Announce Type: replace-cross 
Abstract: We study the identification of causal effects in the presence of different types of constraints (e.g., logical constraints) in addition to the causal graph. These constraints impose restrictions on the models (parameterizations) induced by the causal graph, reducing the set of models considered by the identifiability problem. We formalize the notion of constrained identifiability, which takes a set of constraints as another input to the classical definition of identifiability. We then introduce a framework for testing constrained identifiability by employing tractable Arithmetic Circuits (ACs), which enables us to accommodate constraints systematically. We show that this AC-based approach is at least as complete as existing algorithms (e.g., do-calculus) for testing classical identifiability, which only assumes the constraint of strict positivity. We use examples to demonstrate the effectiveness of this AC-based approach by showing that unidentifiable causal effects may become identifiable under different types of constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02869v2</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizuo Chen, Adnan Darwiche</dc:creator>
    </item>
    <item>
      <title>An Introduction to Sliced Optimal Transport</title>
      <link>https://arxiv.org/abs/2508.12519</link>
      <description>arXiv:2508.12519v2 Announce Type: replace-cross 
Abstract: Sliced Optimal Transport (SOT) is a rapidly developing branch of optimal transport (OT) that exploits the tractability of one-dimensional OT problems. By combining tools from OT, integral geometry, and computational statistics, SOT enables fast and scalable computation of distances, barycenters, and kernels for probability measures, while retaining rich geometric structure. This paper provides a comprehensive review of SOT, covering its mathematical foundations, methodological advances, computational methods, and applications. We discuss key concepts of OT and one-dimensional OT, the role of tools from integral geometry such as Radon transform in projecting measures, and statistical techniques for estimating sliced distances. The paper further explores recent methodological advances, including non-linear projections, improved Monte Carlo approximations, statistical estimation techniques for one-dimensional optimal transport, weighted slicing techniques, and transportation plan estimation methods. Variational problems, such as minimum sliced Wasserstein estimation, barycenters, gradient flows, kernel constructions, and embeddings are examined alongside extensions to unbalanced, partial, multi-marginal, and Gromov-Wasserstein settings. Applications span machine learning, statistics, computer graphics and computer visions, highlighting SOT's versatility as a practical computational tool. This work will be of interest to researchers and practitioners in machine learning, data sciences, and computational disciplines seeking efficient alternatives to classical OT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12519v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen</dc:creator>
    </item>
  </channel>
</rss>
