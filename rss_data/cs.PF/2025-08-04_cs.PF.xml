<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Aug 2025 02:31:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>DGEMM without FP64 Arithmetic -- using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme</title>
      <link>https://arxiv.org/abs/2508.00441</link>
      <description>arXiv:2508.00441v1 Announce Type: new 
Abstract: Since AI computations require low-precision matrix multiplications, processors with enhanced performance for these operations are increasing along with the growing demand for AI computations. However, it is difficult to use these operations directly for scientific computations. The Ozaki scheme, an accurate matrix multiplication method proposed by Ozaki et al. in 2012, enables FP64 matrix multiplication (DGEMM) using low-precision floating-point operations such as FP16. The method was subsequently extended to utilize integer arithmetic. The use of integer operations reduces computational cost compared to the floating-point based approach. It has also demonstrated higher performance than hardware FP64 operations on GPUs with fast INT8 Tensor Cores for AI workloads. However, the latest hardware tends to enhance low-precision floating-point operation performance such as FP8 instead of INT8. This study revisits the utilization of low-precision floating-point operations in the Ozaki scheme, considering the latest AI hardware. Specifically, we consider the use of FP6 and FP8 Tensor Cores. Moreover, for processors that support very slow FP64 operations or do not support them at all, we consider the use of the FP64 emulation based on integer arithmetic. We also examine a new blocking strategy. We demonstrate the effectiveness of these methods by evaluating the performance of DGEMM using FP8 Tensor Cores and FP64 emulation on a Blackwell architecture GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00441v1</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <category>cs.MS</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daichi Mukunoki</dc:creator>
    </item>
    <item>
      <title>Systematic Evaluation of Optimization Techniques for Long-Context Language Models</title>
      <link>https://arxiv.org/abs/2508.00305</link>
      <description>arXiv:2508.00305v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel across diverse natural language processing tasks but face resource demands and limited context windows. Although techniques like pruning, quantization, and token dropping can mitigate these issues, their efficacy in long-context scenarios and system evaluation remains underexplored. This paper systematically benchmarks these optimizations, characterizing memory usage, latency, and throughput, and studies how these methods impact the quality of text generation. We first analyze individual optimization methods for two LLM architectures supporting long context and then systematically evaluate combinations of these techniques to assess how this deeper analysis impacts performance metrics. We subsequently study the scalability of individual optimization methods on a larger variant with 70 billion-parameter model. Our novel insights reveal that naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, as compared to their smaller counterparts. Experiments show that relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks. By integrating system-level profiling with task-specific insights, this study helps LLM practitioners and researchers explore and balance efficiency, accuracy, and scalability across tasks and hardware configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00305v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ammar Ahmed, Sheng Di, Franck Cappello, Zirui Liu, Jingoo Han, Ali Anwar</dc:creator>
    </item>
    <item>
      <title>Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach</title>
      <link>https://arxiv.org/abs/2508.00629</link>
      <description>arXiv:2508.00629v1 Announce Type: cross 
Abstract: The transition toward softwarized Radio Access Networks (RANs), driven by the Open RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through disaggregation and virtualization of base station functions. However, this shift introduces new challenges in managing CPU resources efficiently under strict real-time constraints. In particular, the interplay between latency-sensitive RAN workloads and general-purpose Operating System (OS) schedulers often leads to sub-optimal performance and unnecessary energy consumption. This work proposes a lightweight, programmable distributed application (dApp) deployed at the Distributed Unit (DU) level to dynamically orchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging thread-level telemetry like context switches, Instructions Per Cycle (IPC), and cache metrics, to adapt CPU thread affinity, core isolation, and frequency scaling in real time. Unlike existing solutions, it requires no access to proprietary RAN software, hardware-specific features, or kernel modifications. Fully compliant with the O-RAN architecture and agnostic to the underlying RAN stack, the proposed solution introduces negligible overhead while improving energy efficiency and CPU utilization. Experimental results using a commercial-grade srsRAN deployment demonstrate consistent power savings without compromising real-time processing performance, highlighting the potential of low-latency dApps for fine-grained resource control in next-generation networks</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00629v1</guid>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco Crespo, Javier Villegas, Carlos Baena, Eduardo Baena, Sergio Fortes, Raquel Barco</dc:creator>
    </item>
    <item>
      <title>Efficient Solving of Large Single Input Superstate Decomposable Markovian Decision Process</title>
      <link>https://arxiv.org/abs/2508.00816</link>
      <description>arXiv:2508.00816v1 Announce Type: cross 
Abstract: Solving Markov Decision Processes (MDPs) remains a central challenge in sequential decision-making, especially when dealing with large state spaces and long-term optimization criteria. A key step in Bellman dynamic programming algorithms is the policy evaluation, which becomes computationally demanding in infinite-horizon settings such as average-reward or discounted-reward formulations. In the context of Markov chains, aggregation and disaggregation techniques have for a long time been used to reduce complexity by exploiting structural decompositions. In this work, we extend these principles to a structured class of MDPs. We define the Single-Input Superstate Decomposable Markov Decision Process (SISDMDP), which combines Chiu's single-input decomposition with Robertazzi's single-cycle recurrence property. When a policy induces this structure, the resulting transition graph can be decomposed into interacting components with centralized recurrence. We develop an exact and efficient policy evaluation method based on this structure. This yields a scalable solution applicable to both average and discounted reward MDPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00816v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Youssef Ait El Mahjoub, Jean-Michel Fourneau, Salma Alouah</dc:creator>
    </item>
    <item>
      <title>Fast solvers for Tokamak fluid models with PETSC</title>
      <link>https://arxiv.org/abs/2506.16676</link>
      <description>arXiv:2506.16676v5 Announce Type: replace-cross 
Abstract: This work begins the development of fast, scalable solvers for scientific and engineering-relevant magnetohydrodynamics (MHD) models of tokamaks using multigrid methods. These models are characterized by a distinguished direction following the magnetic field around the torus, which dominates the plasma dynamics. All tokamak models exploit this structure, for example, NIMROD uses 2D, unstructured, high-order finite elements in the poloidal plane with Fourier modes in the toroidal coordinate, and the 3D, extended MHD code M3D-C1 uses 2D, unstructured C1 elements in the poloidal plane with cubic Hermite functions in the toroidal direction and a regular grid that is partially aligned with the magnetic field. This structure suggests addressing the toroidal coordinate first, which NIMROD does at the formulation level, but M3D-C1 uses a full 3D discretization. The resulting algebraic system is solved at each time step in an implicit time integrator. This work addressed the toroidal coordinate in the M3D-C1 velocity solve by adding semi-coarsening multigrid to the existing PETSC - Portable, Extensible Toolkit for Scientific Computation - block Jacobi solver, with the addition of little new code. Competitive performance of this new solver configuration is demonstrated on a self-consistent runaway electron model of a SPARC4 disruption, and the next steps in the development of this solver are outlined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16676v5</guid>
      <category>physics.plasm-ph</category>
      <category>cs.PF</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mark F. Adams, Jin Chen, Benjamin Sturdevant</dc:creator>
    </item>
  </channel>
</rss>
