<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Oct 2024 01:58:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The CEKG: A Tool for Constructing Event Graphs in the Care Pathways of Multi-Morbid Patients</title>
      <link>https://arxiv.org/abs/2410.10827</link>
      <description>arXiv:2410.10827v1 Announce Type: new 
Abstract: One of the challenges in healthcare processes, especially those related to multi-morbid patients who suffer from multiple disorders simultaneously, is not connecting the disorders in patients to process events and not linking events' activities to globally accepted terminology. Addressing this challenge introduces a new entity to the clinical process. On the other hand, it facilitates that the process is interpretable and analyzable across different healthcare systems. This paper aims to introduce a tool named CEKG that uses event logs, diagnosis data, ICD-10, SNOMED-CT, and mapping functions to satisfy these challenges by constructing event graphs for multi-morbid patients' care pathways automatically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10827v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milad Naeimaei Aali, Felix Mannhardt, Pieter Jelle Toussaint</dc:creator>
    </item>
    <item>
      <title>Test Case-Informed Knowledge Tracing for Open-ended Coding Tasks</title>
      <link>https://arxiv.org/abs/2410.10829</link>
      <description>arXiv:2410.10829v1 Announce Type: new 
Abstract: Open-ended coding tasks, which ask students to construct programs according to certain specifications, are common in computer science education. Student modeling can be challenging since their open-ended nature means that student code can be diverse. Traditional knowledge tracing (KT) models that only analyze response correctness may not fully capture nuances in student knowledge from student code. In this paper, we introduce Test case-Informed Knowledge Tracing for Open-ended Coding (TIKTOC), a framework to simultaneously analyze and predict both open-ended student code and whether the code passes each test case. We augment the existing CodeWorkout dataset with the test cases used for a subset of the open-ended coding questions, and propose a multi-task learning KT method to simultaneously analyze and predict 1) whether a student's code submission passes each test case and 2) the student's open-ended code, using a large language model as the backbone. We quantitatively show that these methods outperform existing KT methods for coding that only use the overall score a code submission receives. We also qualitatively demonstrate how test case information, combined with open-ended code, helps us gain fine-grained insights into student knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10829v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhangqi Duan, Nigel Fernandez, Alexander Hicks, Andrew Lan</dc:creator>
    </item>
    <item>
      <title>A Cloud collaborative approach for managing patients wellness</title>
      <link>https://arxiv.org/abs/2410.10837</link>
      <description>arXiv:2410.10837v1 Announce Type: new 
Abstract: Patients with chronic diseases or people with special health care needs are typically monitored by various health experts that address the problem from several perspectives. These experts usually do not interact directly between them; therefore, the instructions given to the patient by one of them are provided disregarding advices and instructions provided by the others. The collaboration between different health experts in a real context is mandatory to ensure the proper monitoring of the patient. This kind of collaboration, supported by technology, benefits the users' health condition and helps the patient achieve their goals in terms of wellbeing. This paper presents an approach for collaborative management of events related to activity supervisions and monitoring of these patients types. It also introduces a model and a mobile application to support this collaborative work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10837v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angel Ruiz-Zafra, Manuel Noguera, Kawtar Benghazi, Sergio F. Ochoa</dc:creator>
    </item>
    <item>
      <title>Evaluating the Effectiveness of SIWES Programs for Computer Science Students at Moshood Abiola Polytechnic, Abeokuta</title>
      <link>https://arxiv.org/abs/2410.10838</link>
      <description>arXiv:2410.10838v1 Announce Type: new 
Abstract: This study examines the effectiveness of the Students Industrial Work Experience Scheme (SIWES) for Computer Science students at Moshood Abiola Polytechnic, Abeokuta. Through correlational analysis of various assessment components, including employer evaluations, student logbooks, and technical reports, the research aims to identify key factors contributing to student performance and skill development. The findings reveal strong correlations between employer evaluations, logbook quality, and overall performance, highlighting the importance of practical experience and documentation in the SIWES program. This research contributes to the ongoing discourse on the role of industrial training in higher education and provides recommendations for enhancing the SIWES experience for computer science students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10838v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ganiyu Oladimeji</dc:creator>
    </item>
    <item>
      <title>A discrete event simulator for policy evaluation in liver allocation in Eurotransplant</title>
      <link>https://arxiv.org/abs/2410.10840</link>
      <description>arXiv:2410.10840v1 Announce Type: new 
Abstract: We present the ELAS simulator, a discrete event simulator built for the Eurotransplant (ET) Liver Allocation System (ELAS). Eurotransplant uses ELAS to allocate deceased donor livers in eight European countries. The simulator is made publicly available to be transparent on which model Eurotransplant uses to evaluate liver allocation policies, and to facilitate collaborations with policymakers, scientists and other stakeholders in evaluating alternative liver allocation policies. This paper describes the design and modules of the ELAS simulator. One of the included modules is the obligation module, which is instrumental in ensuring that international cooperation in liver allocation benefits all ET member countries.
  By default, the ELAS simulator simulates liver allocation according to the actual ET allocation rules. Stochastic processes, such as graft offer acceptance behavior and listing for a repeat transplantation, are approximated with statistical models which were calibrated to data from the ET registry. We validate the ELAS simulator by comparing simulated waitlist outcomes to historically observed waitlist outcomes between 2016 and 2019.
  The modular design of the ELAS simulator gives end users maximal control over the rules and assumptions under which ET liver allocation is simulated, which makes the simulator useful for policy evaluation. We illustrate this with two clinically motivated case studies, for which we collaborated with hepatologists and transplantation surgeons from two liver advisory committees affiliated with Eurotransplant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10840v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hans de Ferrante, Marieke de Rosner-Van Rosmalen, Bart Smeulders, Frits C. R. Spieksma, Serge Vogelaar</dc:creator>
    </item>
    <item>
      <title>Characterizing the MrDeepFakes Sexual Deepfake Marketplace</title>
      <link>https://arxiv.org/abs/2410.11100</link>
      <description>arXiv:2410.11100v1 Announce Type: new 
Abstract: The prevalence of sexual deepfake material has exploded over the past several years. Attackers create and utilize deepfakes for many reasons: to seek sexual gratification, to harass and humiliate targets, or to exert power over an intimate partner. In tandem with this growth, several markets have emerged to support the buying and selling of sexual deepfake material. In this paper, we systematically characterize the most prominent and mainstream marketplace, MrDeepFakes. We analyze the marketplace economics, the targets of created media, and user discussions of how to create deepfakes, which we use to understand the current state-of-the-art in deepfake creation. Our work uncovers little enforcement of posted rules (e.g., limiting targeting to well-established celebrities), previously undocumented attacker motivations, and unexplored attacker tactics for acquiring resources to create sexual deepfakes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11100v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Catherine Han, Anne Li, Deepak Kumar, Zakir Durumeric</dc:creator>
    </item>
    <item>
      <title>Data-driven Design of Randomized Control Trials with Guaranteed Treatment Effects</title>
      <link>https://arxiv.org/abs/2410.11212</link>
      <description>arXiv:2410.11212v1 Announce Type: new 
Abstract: Randomized controlled trials (RCTs) can be used to generate guarantees on treatment effects. However, RCTs often spend unnecessary resources exploring sub-optimal treatments, which can reduce the power of treatment guarantees. To address these concerns, we develop a two-stage RCT where, first on a data-driven screening stage, we prune low-impact treatments, while in the second stage, we develop high probability lower bounds on the treatment effect. Unlike existing adaptive RCT frameworks, our method is simple enough to be implemented in scenarios with limited adaptivity. We derive optimal designs for two-stage RCTs and demonstrate how we can implement such designs through sample splitting. Empirically, we demonstrate that two-stage designs improve upon single-stage approaches, especially in scenarios where domain knowledge is available in the form of a prior. Our work is thus, a simple, yet effective, method to estimate high probablility certificates for high performant treatment effects on a RCT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11212v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santiago Cortes-Gomez, Naveen Raman, Aarti Singh, Bryan Wilder</dc:creator>
    </item>
    <item>
      <title>Before &amp; After: The Effect of EU's 2022 Code of Practice on Disinformation</title>
      <link>https://arxiv.org/abs/2410.11369</link>
      <description>arXiv:2410.11369v1 Announce Type: new 
Abstract: Over the past few years, the European Commission has made significant steps to reduce disinformation in cyberspace. One of those steps has been the introduction of the 2022 "Strengthened Code of Practice on Disinformation". Signed by leading online platforms, this Strengthened Code of Practice on Disinformation is an attempt to combat disinformation on the Web. The Code of Practice includes a variety of measures including the demonetization of disinformation, urging, for example, advertisers "to avoid the placement of advertising next to Disinformation content".
  In this work, we set out to explore what was the impact of the Code of Practice and especially to explore to what extent ad networks continue to advertise on dis-/mis-information sites. We perform a historical analysis and find that, although at a hasty glance things may seem to be improving, there is really no significant reduction in the amount of advertising relationships among popular misinformation websites and major ad networks. In fact, we show that ad networks have withdrawn mostly from unpopular misinformation websites with very few visitors, but still form relationships with highly unreliable websites that account for the majority of misinformation traffic. To make matters worse, we show that ad networks continue to place advertisements of legitimate companies next to misinformation content. In fact, major ad networks place ads in almost 400 misinformation websites of our dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11369v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emmanouil Papadogiannakis, Panagiotis Papadopoulos, Nicolas Kourtellis, Evangelos P. Markatos</dc:creator>
    </item>
    <item>
      <title>Report on Female Participation in Informatics degrees in Europe</title>
      <link>https://arxiv.org/abs/2410.11431</link>
      <description>arXiv:2410.11431v1 Announce Type: new 
Abstract: This study aims to enrich and leverage data from the Informatics Europe Higher Education (IEHE) data portal to extract and analyze trends in female participation in Informatics across Europe. The research examines the proportion of female students, first-year enrollments, and degrees awarded to women in the field. The issue of low female participation in Informatics has long been recognized as a persistent challenge and remains a critical area of scholarly inquiry. Furthermore, existing literature indicates that socio-economic factors can unpredictably influence female participation, complicating efforts to address the gender gap.
  The analysis focuses on participation data from research universities at various academic levels, including Bachelors, Masters, and PhD programs, and seeks to uncover potential correlations between female participation and geographical or economic zones. The dataset was first enriched by integrating additional information, such as each country's GDP and relevant geographical data, sourced from various online repositories. Subsequently, the data was cleaned to ensure consistency and eliminate incomplete time series. A final set of complete time series was selected for further analysis.
  We then used the data collected from the internet to assign countries to different clusters. Specifically, we employed Economic Zone, Geographical Area, and GDP quartile to cluster countries and compare their temporal trends both within and between clusters. We analyze the results for each classification and derive conclusions based on the available data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11431v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea D'Angelo, Tiziana Catarci, Antinisca Di Marco, Monica Landoni, Enrico Nardelli, Giovanni Stilo</dc:creator>
    </item>
    <item>
      <title>Towards a Healthy AI Tradition: Lessons from Biology and Biomedical Science</title>
      <link>https://arxiv.org/abs/2410.11590</link>
      <description>arXiv:2410.11590v1 Announce Type: new 
Abstract: AI is a magnificent field that directly and profoundly touches on numerous disciplines ranging from philosophy, computer science, engineering, mathematics, decision and data science and economics, to cognitive science, neuroscience and more. The number of applications and impact of AI is second to none and the potential of AI to broadly impact future science developments is particularly thrilling. While attempts to understand knowledge, reasoning, cognition and learning go back centuries, AI remains a relatively new field. In part due to the fact it has so many wide-ranging overlaps with other disparate fields it appears to have trouble developing a robust identity and culture. Here we suggest that contrasting the fast-moving AI culture to biological and biomedical sciences is both insightful and useful way to inaugurate a healthy tradition needed to envision and manage our ascent to AGI and beyond (independent of the AI Platforms used). The co-evolution of AI and Biomedical Science offers many benefits to both fields. In a previous perspective, we suggested that biomedical laboratories or centers can usefully embrace logistic traditions in AI labs that will allow them to be highly collaborative, improve the reproducibility of research, reduce risk aversion and produce faster mentorship pathways for PhDs and fellows. This perspective focuses on the benefits to AI by adapting features of biomedical science at higher, primarily cultural levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11590v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Simon Kasif</dc:creator>
    </item>
    <item>
      <title>AI Rules? Characterizing Reddit Community Policies Towards AI-Generated Content</title>
      <link>https://arxiv.org/abs/2410.11698</link>
      <description>arXiv:2410.11698v1 Announce Type: new 
Abstract: How are Reddit communities responding to AI-generated content? We explored this question through a large-scale analysis of subreddit community rules and their change over time. We collected the metadata and community rules for over 300,000 public subreddits and measured the prevalence of rules governing AI. We labeled subreddits and AI rules according to existing taxonomies from the HCI literature and a new taxonomy we developed specific to AI rules. While rules about AI are still relatively uncommon, the number of subreddits with these rules almost doubled over the course of a year. AI rules are also more common in larger subreddits and communities focused on art or celebrity topics, and less common in those focused on social support. These rules often focus on AI images and evoke, as justification, concerns about quality and authenticity. Overall, our findings illustrate the emergence of varied concerns about AI, in different community contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11698v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Travis Lloyd, Jennah Gosciak, Tung Nguyen, Mor Naaman</dc:creator>
    </item>
    <item>
      <title>On the Reliability of Large Language Models to Misinformed and Demographically-Informed Prompts</title>
      <link>https://arxiv.org/abs/2410.10850</link>
      <description>arXiv:2410.10850v1 Announce Type: cross 
Abstract: We investigate and observe the behaviour and performance of Large Language Model (LLM)-backed chatbots in addressing misinformed prompts and questions with demographic information within the domains of Climate Change and Mental Health. Through a combination of quantitative and qualitative methods, we assess the chatbots' ability to discern the veracity of statements, their adherence to facts, and the presence of bias or misinformation in their responses. Our quantitative analysis using True/False questions reveals that these chatbots can be relied on to give the right answers to these close-ended questions. However, the qualitative insights, gathered from domain experts, shows that there are still concerns regarding privacy, ethical implications, and the necessity for chatbots to direct users to professional services. We conclude that while these chatbots hold significant promise, their deployment in sensitive areas necessitates careful consideration, ethical oversight, and rigorous refinement to ensure they serve as a beneficial augmentation to human expertise rather than an autonomous solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10850v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toluwani Aremu, Oluwakemi Akinwehinmi, Chukwuemeka Nwagu, Syed Ishtiaque Ahmed, Rita Orji, Pedro Arnau Del Amo, Abdulmotaleb El Saddik</dc:creator>
    </item>
    <item>
      <title>Superficial Safety Alignment Hypothesis</title>
      <link>https://arxiv.org/abs/2410.10862</link>
      <description>arXiv:2410.10862v1 Announce Type: cross 
Abstract: As large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe and aligned responses is a pressing need. Previous research on alignment has largely focused on general instruction-following but has often overlooked the unique properties and challenges of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment should teach an otherwise unsafe model to choose the correct reasoning direction - interpreted as a specialized binary classification task - and incorporate a refusal mechanism with multiple reserved fallback options. Furthermore, through SSAH, we hypothesize that safety guardrails in LLMs can be established by just a small number of essential components. To verify this, we conduct an ablation study and successfully identify four types of attribute-critical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components 7.5\% during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Additionally, we show that leveraging redundant units 20\% in the pre-trained model as an ``alignment budget'' can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated. We believe this work contributes to the foundation of efficient and scalable safety alignment for future LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10862v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianwei Li, Jung-Eun Kim</dc:creator>
    </item>
    <item>
      <title>AuditWen:An Open-Source Large Language Model for Audit</title>
      <link>https://arxiv.org/abs/2410.10873</link>
      <description>arXiv:2410.10873v1 Announce Type: cross 
Abstract: Intelligent auditing represents a crucial advancement in modern audit practices, enhancing both the quality and efficiency of audits within the realm of artificial intelligence. With the rise of large language model (LLM), there is enormous potential for intelligent models to contribute to audit domain. However, general LLMs applied in audit domain face the challenges of lacking specialized knowledge and the presence of data biases. To overcome these challenges, this study introduces AuditWen, an open-source audit LLM by fine-tuning Qwen with constructing instruction data from audit domain. We first outline the application scenarios for LLMs in the audit and extract requirements that shape the development of LLMs tailored for audit purposes. We then propose an audit LLM, called AuditWen, by fine-tuning Qwen with constructing 28k instruction dataset from 15 audit tasks and 3 layers. In evaluation stage, we proposed a benchmark with 3k instructions that covers a set of critical audit tasks derived from the application scenarios. With the benchmark, we compare AuditWen with other existing LLMs from information extraction, question answering and document generation. The experimental results demonstrate superior performance of AuditWen both in question understanding and answer generation, making it an immediately valuable tool for audit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10873v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiajia Huang, Haoran Zhu, Chao Xu, Tianming Zhan, Qianqian Xie, Jimin Huang</dc:creator>
    </item>
    <item>
      <title>Performance in a dialectal profiling task of LLMs for varieties of Brazilian Portuguese</title>
      <link>https://arxiv.org/abs/2410.10991</link>
      <description>arXiv:2410.10991v1 Announce Type: cross 
Abstract: Different of biases are reproduced in LLM-generated responses, including dialectal biases. A study based on prompt engineering was carried out to uncover how LLMs discriminate varieties of Brazilian Portuguese, specifically if sociolinguistic rules are taken into account in four LLMs: GPT 3.5, GPT-4o, Gemini, and Sabi.-2. The results offer sociolinguistic contributions for an equity fluent NLP technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10991v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raquel Meister Ko Freitag, T\'ulio Sousa de Gois</dc:creator>
    </item>
    <item>
      <title>Generative Image Steganography Based on Point Cloud</title>
      <link>https://arxiv.org/abs/2410.11673</link>
      <description>arXiv:2410.11673v1 Announce Type: cross 
Abstract: In deep steganography, the model size is usually related to the underlying mesh resolution, and a separate neural network needs to be trained as a message extractor. In this paper, we propose a generative image steganography based on point cloud representation, which represents image data as a point cloud, learns the distribution of the point cloud data, and represents it in the form of a continuous function. This method breaks through the limitation of the image resolution, and can generate images with arbitrary resolution according to the actual need, and omits the need for explicit data for image steganography. At the same time, using a fixed point cloud extractor transfers the training of the network to the point cloud data, which saves the training time and avoids the risk of exposing the steganography behavior caused by the transmission of the message extractor. Experiments prove that the steganographic images generated by the scheme have very high image quality and the accuracy of message extraction reaches more than 99%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11673v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhong Yangjie, Liu Jia, Liu Meiqi, Ke Yan</dc:creator>
    </item>
    <item>
      <title>Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI</title>
      <link>https://arxiv.org/abs/2308.07213</link>
      <description>arXiv:2308.07213v3 Announce Type: replace-cross 
Abstract: While many Natural Language Processing (NLP) techniques have been proposed for fact-checking, both academic research and fact-checking organizations report limited adoption of such NLP work due to poor alignment with fact-checker practices, values, and needs. To address this, we investigate a co-design method, Matchmaking for AI, to enable fact-checkers, designers, and NLP researchers to collaboratively identify what fact-checker needs should be addressed by technology, and to brainstorm ideas for potential solutions. Co-design sessions we conducted with 22 professional fact-checkers yielded a set of 11 design ideas that offer a "north star", integrating fact-checker criteria into novel NLP design concepts. These concepts range from pre-bunking misinformation, efficient and personalized monitoring misinformation, proactively reducing fact-checker potential biases, and collaborative writing fact-check reports. Our work provides new insights into both human-centered fact-checking research and practice and AI co-design research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07213v3</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Houjiang Liu, Anubrata Das, Alexander Boltz, Didi Zhou, Daisy Pinaroc, Matthew Lease, Min Kyung Lee</dc:creator>
    </item>
    <item>
      <title>Words as Trigger Points in Social Media Discussions</title>
      <link>https://arxiv.org/abs/2405.10213</link>
      <description>arXiv:2405.10213v2 Announce Type: replace-cross 
Abstract: Trigger points, introduced by Mau et al . [30], are rooted in theories of affective political identity and relate to deeply lying beliefs about moral expectations and social dispositions. Examining trigger points in online discussions helps understand why and when social media users engage in disagreements or affective political deliberations. This opens the door to modelling social media user engagement more effectively and studying the conditions and causal mechanisms that lead to adverse reactions, hate speech, and abusive language in online debates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10213v2</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimosthenis Antypas, Christian Arnold, Jose Camacho-Collados, Nedjma Ousidhoum, Carla Perez Almendros</dc:creator>
    </item>
    <item>
      <title>Plurals: A System for Guiding LLMs Via Simulated Social Ensembles</title>
      <link>https://arxiv.org/abs/2409.17213</link>
      <description>arXiv:2409.17213v3 Announce Type: replace-cross 
Abstract: Recent debates raised concerns that language models may favor certain viewpoints. But what if the solution is not to aim for a 'view from nowhere' but rather to leverage different viewpoints? We introduce Plurals, a system and Python library for pluralistic AI deliberation. Plurals consists of Agents (LLMs, optionally with personas) which deliberate within customizable Structures, with Moderators overseeing deliberation. Plurals is a generator of simulated social ensembles. Plurals integrates with government datasets to create nationally representative personas, includes deliberation templates inspired by democratic deliberation theory, and allows users to customize both information-sharing structures and deliberation behavior within Structures. Six case studies demonstrate fidelity to theoretical constructs and efficacy. Three randomized experiments show simulated focus groups produced output resonant with an online sample of the relevant audiences (chosen over zero-shot generation in 75% of trials). Plurals is both a paradigm and a concrete system for pluralistic AI. The Plurals library is available at https://github.com/josh-ashkinaze/plurals and will be continually updated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17213v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Ashkinaze, Emily Fry, Narendra Edara, Eric Gilbert, Ceren Budak</dc:creator>
    </item>
    <item>
      <title>Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation</title>
      <link>https://arxiv.org/abs/2410.09318</link>
      <description>arXiv:2410.09318v2 Announce Type: replace-cross 
Abstract: While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at understanding the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09318v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Saiful Islam Salim, Rubin Yuchan Yang, Alexander Cooper, Suryashree Ray, Saumya Debray, Sazzadur Rahaman</dc:creator>
    </item>
    <item>
      <title>The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning</title>
      <link>https://arxiv.org/abs/2410.09600</link>
      <description>arXiv:2410.09600v2 Announce Type: replace-cross 
Abstract: Fairness metrics are a core tool in the fair machine learning literature (FairML), used to determine that ML models are, in some sense, "fair". Real-world data, however, are typically plagued by various measurement biases and other violated assumptions, which can render fairness assessments meaningless. We adapt tools from causal sensitivity analysis to the FairML context, providing a general framework which (1) accommodates effectively any combination of fairness metric and bias that can be posed in the "oblivious setting"; (2) allows researchers to investigate combinations of biases, resulting in non-linear sensitivity; and (3) enables flexible encoding of domain-specific constraints and assumptions. Employing this framework, we analyze the sensitivity of the most common parity metrics under 3 varieties of classifier across 14 canonical fairness datasets. Our analysis reveals the striking fragility of fairness assessments to even minor dataset biases. We show that causal sensitivity analysis provides a powerful and necessary toolkit for gauging the informativeness of parity metric evaluations. Our repository is available here: https://github.com/Jakefawkes/fragile_fair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09600v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jake Fawkes, Nic Fishman, Mel Andrews, Zachary C. Lipton</dc:creator>
    </item>
  </channel>
</rss>
