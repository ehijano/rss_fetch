<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Oct 2024 04:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Characterizing and Testing Principal Minor Equivalence of Matrices</title>
      <link>https://arxiv.org/abs/2410.01961</link>
      <description>arXiv:2410.01961v1 Announce Type: new 
Abstract: Two matrices are said to be principal minor equivalent if they have equal corresponding principal minors of all orders. We give a characterization of principal minor equivalence and a deterministic polynomial time algorithm to check if two given matrices are principal minor equivalent. Earlier such results were known for certain special cases like symmetric matrices, skew-symmetric matrices with {0, 1, -1}-entries, and matrices with no cuts (i.e., for any non-trivial partition of the indices, the top right block or the bottom left block must have rank more than 1).
  As an immediate application, we get an algorithm to check if the determinantal point processes corresponding to two given kernel matrices (not necessarily symmetric) are the same. As another application, we give a deterministic polynomial-time test to check equality of two multivariate polynomials, each computed by a symbolic determinant with a rank 1 constraint on coefficient matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01961v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhranil Chatterjee, Sumanta Ghosh, Rohit Gurjar, Roshan Raj</dc:creator>
    </item>
    <item>
      <title>Inapproximability of Sparsest Vector in a Real Subspace</title>
      <link>https://arxiv.org/abs/2410.02636</link>
      <description>arXiv:2410.02636v1 Announce Type: new 
Abstract: We establish strong inapproximability for finding the sparsest nonzero vector in a real subspace. We show that it is NP-Hard (under randomized reductions) to approximate the sparsest vector in a subspace within any constant factor (or almost polynomial factors in quasipolynomial time). We recover as a corollary state of the art inapproximability for the shortest vector problem (SVP), a foundational problem in lattice based cryptography. Our proof is surprisingly simple, bypassing even the PCP theorem. We are inspired by the homogenization framework from the inapproximability theory of minimum distance problems (MDC) in integer lattices and error correcting codes. We use a combination of (a) \emph{product testing via tensor codes} and (b) \emph{encoding an assignment as a coset of a random code in higher dimensional space} in order to embed non-homogeneous quadratic equations into the sparsest vector problem. (a) is inspired by Austrin and Khot's simplified proof of hardness of MDC over finite fields, and (b) is inspired by Micciancio's semi-derandomization of hardness of SVP. Our reduction involves the challenge of performing (a) over the reals. We prove that tensoring of the kernel of a +1/-1 random matrix furnishes an adequate product test (while still allowing (b)). The proof exposes a connection to Littlewood-Offord theory and relies on a powerful anticoncentration result of Rudelson and Vershynin. Our main motivation in this work is the development of inapproximability theory for problems over the reals. Analytic variants of sparsest vector have connections to small set expansion, quantum separability and polynomial maximization over convex sets, all of which cause similar barriers to inapproximability. The approach we develop could lead to progress on the hardness of some of these problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02636v1</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vijay Bhattiprolu, Euiwoong Lee</dc:creator>
    </item>
    <item>
      <title>Is uniform expressivity too restrictive? Towards efficient expressivity of graph neural networks</title>
      <link>https://arxiv.org/abs/2410.01910</link>
      <description>arXiv:2410.01910v1 Announce Type: cross 
Abstract: Uniform expressivity guarantees that a Graph Neural Network (GNN) can express a query without the parameters depending on the size of the input graphs. This property is desirable in applications in order to have number of trainable parameters that is independent of the size of the input graphs. Uniform expressivity of the two variable guarded fragment (GC2) of first order logic is a well-celebrated result for Rectified Linear Unit (ReLU) GNNs [Barcelo &amp; al., 2020]. In this article, we prove that uniform expressivity of GC2 queries is not possible for GNNs with a wide class of Pfaffian activation functions (including the sigmoid and tanh), answering a question formulated by [Grohe, 2021]. We also show that despite these limitations, many of those GNNs can still efficiently express GC2 queries in a way that the number of parameters remains logarithmic on the maximal degree of the input graphs. Furthermore, we demonstrate that a log-log dependency on the degree is achievable for a certain choice of activation function. This shows that uniform expressivity can be successfully relaxed by covering large graphs appearing in practical applications. Our experiments illustrates that our theoretical estimates hold in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01910v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sammy Khalife, Josu\'e Tonelli-Cueto</dc:creator>
    </item>
    <item>
      <title>The Role of piracy in quantum proofs</title>
      <link>https://arxiv.org/abs/2410.02228</link>
      <description>arXiv:2410.02228v1 Announce Type: cross 
Abstract: A well-known feature of quantum information is that it cannot, in general, be cloned. Recently, a number of quantum-enabled information-processing tasks have demonstrated various forms of uncloneability; among these forms, piracy is an adversarial model that gives maximal power to the adversary, in controlling both a cloning-type attack, as well as the evaluation/verification stage. Here, we initiate the study of anti-piracy proof systems, which are proof systems that inherently prevent piracy attacks. We define anti-piracy proof systems, demonstrate such a proof system for an oracle problem, and also describe a candidate anti-piracy proof system for NP. We also study quantum proof systems that are cloneable and settle the famous QMA vs. QMA(2) debate in this setting. Lastly, we discuss how one can approach the QMA vs. QCMA question, by studying its cloneable variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02228v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anne Broadbent, Alex B. Grilo, Supartha Podder, Jamie Sikora</dc:creator>
    </item>
    <item>
      <title>Approximate Degrees of Multisymmetric Properties with Application to Quantum Claw Detection</title>
      <link>https://arxiv.org/abs/2410.02243</link>
      <description>arXiv:2410.02243v1 Announce Type: cross 
Abstract: The claw problem is central in the fields of theoretical computer science as well as cryptography. The optimal quantum query complexity of the problem is known to be $\Omega\left(\sqrt{G}+(FG)^{1/3} \right)$ for input functions $f\colon [F]\to Z$ and $g\colon [G]\to Z$. However, the lower bound was proved when the range $Z$ is sufficiently large (i.e., $|{Z}|=\Omega(FG)$). The current paper proves the lower bound holds even for every smaller range $Z$ with $|{Z}|\ge F+G$. This implies that $\Omega\left(\sqrt{G}+(FG)^{1/3} \right)$ is tight for every such range. In addition, the lower bound $\Omega\left(\sqrt{G}+F^{1/3}G^{1/6}M^{1/6}\right)$ is provided for even smaller range $Z=[M]$ with every $M\in [2,F+G]$ by reducing the claw problem for $|{Z}|= F+G$. The proof technique is general enough to apply to any $k$-symmetric property (e.g., the $k$-claw problem), i.e., the Boolean function $\Phi$ on the set of $k$ functions with different-size domains and a common range such that $\Phi$ is invariant under the permutations over each domain and the permutations over the range. More concretely, it generalizes Ambainis's argument [Theory of Computing, 1(1):37-46] to the multiple-function case by using the notion of multisymmetric polynomials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02243v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seiichiro Tani</dc:creator>
    </item>
    <item>
      <title>Can You Link Up With Treewidth?</title>
      <link>https://arxiv.org/abs/2410.02606</link>
      <description>arXiv:2410.02606v1 Announce Type: cross 
Abstract: A central result of Marx [ToC '10] proves that there are $k$-vertex graphs $H$ of maximum degree $3$ such that $n^{o(k /\log k)}$ time algorithms for detecting colorful $H$-subgraphs would refute the Exponential-Time Hypothesis (ETH). This result is widely used to obtain almost-tight conditional lower bounds for parameterized problems under ETH.
  Our first contribution is a new and fully self-contained proof of this result that further simplifies a recent work by Karthik et al. [SOSA 2024]. Towards this end, we introduce a novel graph parameter, the linkage capacity $\gamma(H)$, and show with an elementary proof that detecting colorful $H$-subgraphs in time $n^{o(\gamma(H))}$ refutes ETH. Then, we use a simple construction of communication networks credited to Bene\v{s} to obtain $k$-vertex graphs of maximum degree $3$ and linkage capacity $\Omega(k / \log k)$, avoiding the use of expander graphs. We also show that every graph $H$ of treewidth $t$ has linkage capacity $\Omega(t / \log t)$, thus recovering the stronger result of Marx [ToC '10] with a simplified proof.
  Additionally, we obtain new tight lower bounds for certain types of patterns by analyzing their linkage capacity. For example, we prove that almost all $k$-vertex graphs of polynomial average degree $\Omega(k^{\beta})$ for some $\beta &gt; 0$ have linkage capacity $\Theta(k)$, which implies tight lower bounds for such patterns $H$. As an application of these results, we also obtain tight lower bounds for counting small induced subgraphs having a certain property $\Phi$, improving bounds from [Roth et al., FOCS 2020].</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02606v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu Curticapean, Simon D\"oring, Daniel Neuen, Jiaheng Wang</dc:creator>
    </item>
    <item>
      <title>Parameterised distance to local irregularity</title>
      <link>https://arxiv.org/abs/2307.04583</link>
      <description>arXiv:2307.04583v3 Announce Type: replace 
Abstract: A graph $G$ is \emph{locally irregular} if no two of its adjacent vertices have the same degree. In [Fioravantes et al. Complexity of finding maximum locally irregular induced subgraph. {\it SWAT}, 2022], the authors introduced and studied the problem of finding a locally irregular induced subgraph of a given a graph $G$ of maximum order, or, equivalently, computing a subset $S$ of $V(G)$ of minimum order, whose deletion from $G$ results in a locally irregular graph; $S$ is denoted as an \emph{optimal vertex-irregulator of $G$}. In this work we provide an in-depth analysis of the parameterised complexity of computing an optimal vertex-irregulator of a given graph $G$. Moreover, we introduce and study a variation of this problem, where $S$ is a substet of the edges of $G$; in this case, $S$ is denoted as an \emph{optimal edge-irregulator of $G$}. In particular, we prove that computing an optimal vertex-irregulator of a graph $G$ is in FPT when parameterised by the vertex integrity, neighborhood diversity or cluster deletion number of $G$, while it is $W[1]$-hard when parameterised by the feedback vertex set number or the treedepth of $G$. In the case of computing an optimal edge-irregulator of a graph $G$, we prove that this problem is in FPT when parameterised by the vertex integrity of $G$, while it is NP-hard even if $G$ is a planar bipartite graph of maximum degree $4$, and $W[1]$-hard when parameterised by the size of the solution, the feedback vertex set or the treedepth of $G$. Our results paint a comprehensive picture of the tractability of both problems studied here, considering most of the standard graph-structural parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04583v3</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Foivos Fioravantes, Nikolaos Melissinos, Theofilos Triommatis</dc:creator>
    </item>
    <item>
      <title>The NFA Acceptance Hypothesis: Non-Combinatorial and Dynamic Lower Bounds</title>
      <link>https://arxiv.org/abs/2311.10204</link>
      <description>arXiv:2311.10204v2 Announce Type: replace 
Abstract: We pose the fine-grained hardness hypothesis that the textbook algorithm for the NFA Acceptance problem is optimal up to subpolynomial factors, even for dense NFAs and fixed alphabets.
  We show that this barrier appears in many variations throughout the algorithmic literature by introducing a framework of Colored Walk problems. These yield fine-grained equivalent formulations of the NFA Acceptance problem as problems concerning detection of an $s$-$t$-walk with a prescribed color sequence in a given edge- or node-colored graph. For NFA Acceptance on sparse NFAs (or equivalently, Colored Walk in sparse graphs), a tight lower bound under the Strong Exponential Time Hypothesis has been rediscovered several times in recent years. We show that our hardness hypothesis, which concerns dense NFAs, has several interesting implications:
  - It gives a tight lower bound for Context-Free Language Reachability. This proves conditional optimality for the class of 2NPDA-complete problems, explaining the cubic bottleneck of interprocedural program analysis.
  - It gives a tight $(n+nm^{1/3})^{1-o(1)}$ lower bound for the Word Break problem on strings of length $n$ and dictionaries of total size $m$.
  - It implies the popular OMv hypothesis. Since the NFA acceptance problem is a static (i.e., non-dynamic) problem, this provides a static reason for the hardness of many dynamic problems.
  Thus, a proof of the NFA Acceptance hypothesis would resolve several interesting barriers. Conversely, a refutation of the NFA Acceptance hypothesis may lead the way to attacking the current barriers observed for Context-Free Language Reachability, the Word Break problem and the growing list of dynamic problems proven hard under the OMv hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10204v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.24.22</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 3 (2024), Article 22, 1-35</arxiv:journal_reference>
      <dc:creator>Karl Bringmann, Allan Gr{\o}nlund, Marvin K\"unnemann, Kasper Green Larsen</dc:creator>
    </item>
    <item>
      <title>On the Existence of Seedless Condensers: Exploring the Terrain</title>
      <link>https://arxiv.org/abs/2312.15087</link>
      <description>arXiv:2312.15087v3 Announce Type: replace 
Abstract: We prove several new results for seedless condensers in the context of three related classes of sources: Non-Oblivious Symbol Fixing (NOSF) sources, online NOSF (oNOSF) sources [AORSV, EUROCRYPT'20], and adversarial Chor-Goldreich (aCG) source [DMOZ, STOC'23]. We think of these sources as a sequence of random variables $\mathbf{X}=\mathbf{X}_1,\dots,\mathbf{X}_\ell$ on $\ell$ symbols where at least $g$ out of these $\ell$ symbols are "good" (i.e., have some min-entropy requirement), denoted as a $(g,\ell)$-source, and the remaining "bad" $\ell-g$ symbols may adversarially depend on these $g$ good blocks. The difference between each of these sources is realized by restrictions on the power of the adversary. Prior to our work, the only known seedless condenser upper or lower bound in these settings is due to [DMOZ, STOC'23], where they explicitly construct a seedless condenser for a restricted subset of $(g,\ell)$-aCG sources.
  We show:
  1) oNOSF sources
  a) When $g\leq\ell/2$, we prove that condensing with error 0.99 above rate $\frac{1}{\lfloor \ell/g \rfloor}$ is impossible. In fact, we show that this is tight.
  b) For $g&gt; \ell/2$, we show the existence of excellent condensers for uniform oNOSF sources. In addition, we show the existence of similar condensers for oNOSF sources with only logarithmic min-entropy.
  2) aCG sources
  a) We observe that uniform aCG sources are equivalent to uniform oNOSF sources and consequently inherit the same results.
  b) We show that one cannot condense beyond the min-entropy gap of each block or condense low min-entropy CG sources above rate $1/2$.
  3) NOSF sources
  a) We show that condensing with constant error above rate $\frac{g}{\ell}$ is impossible for uniform NOSF sources for any $g$ and $\ell$, thus ruling out the possibility of any non-trivial condensing. This shows a distinction between NOSF sources and oNOSF sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15087v3</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eshan Chattopadhyay, Mohit Gurumukhani, Noam Ringach</dc:creator>
    </item>
    <item>
      <title>Quantum Channel Testing in Average-Case Distance</title>
      <link>https://arxiv.org/abs/2409.12566</link>
      <description>arXiv:2409.12566v2 Announce Type: replace-cross 
Abstract: We study the complexity of testing properties of quantum channels. First, we show that testing identity to any channel $\mathcal N: \mathbb C^{d_{\mathrm{in}} \times d_{\mathrm{in}}} \to \mathbb C^{d_{\mathrm{out}} \times d_{\mathrm{out}}}$ in diamond norm distance requires $\Omega(\sqrt{d_{\mathrm{in}}} / \varepsilon)$ queries, even in the strongest algorithmic model that admits ancillae, coherence, and adaptivity. This is due to the worst-case nature of the distance induced by the diamond norm.
  Motivated by this limitation and other theoretical and practical applications, we introduce an average-case analogue of the diamond norm, which we call the average-case imitation diamond (ACID) norm. In the weakest algorithmic model without ancillae, coherence, or adaptivity, we prove that testing identity to certain types of channels in ACID distance can be done with complexity independent of the dimensions of the channel, while for other types of channels the complexity depends on both the input and output dimensions. Building on previous work, we also show that identity to any fixed channel can be tested with $\tilde O(d_{\mathrm{in}} d_{\mathrm{out}}^{3/2} / \varepsilon^2)$ queries in ACID distance and $\tilde O(d_{\mathrm{in}}^2 d_{\mathrm{out}}^{3/2} / \varepsilon^2)$ queries in diamond distance in this model. Finally, we prove tight bounds on the complexity of channel tomography in ACID distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12566v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Aaronson, Gregory Rosenthal, Sathyawageeswar Subramanian, Animesh Datta, Tom Gur</dc:creator>
    </item>
  </channel>
</rss>
