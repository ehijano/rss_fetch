<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Nov 2024 02:38:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>US COVID-19 school closure was not cost-effective, but other measures were</title>
      <link>https://arxiv.org/abs/2411.12016</link>
      <description>arXiv:2411.12016v1 Announce Type: new 
Abstract: Non-pharmaceutical interventions (NPIs) in response to the COVID-19 pandemic necessitated a trade-off between the health impacts of viral spread and the social and economic costs of restrictions. We conduct a cost-effectiveness analysis of NPI policies enacted at the state level in the United States in 2020. Although school closures reduced viral transmission, their social impact in terms of student learning loss was too costly, depriving the nation of \$2 trillion (USD2020), conservatively, in future GDP. Moreover, this marginal trade-off between school closure and COVID deaths was not inescapable: a combination of other measures would have been enough to maintain similar or lower mortality rates without incurring such profound learning loss. Optimal policies involve consistent implementation of mask mandates, public test availability, contact tracing, social distancing orders, and reactive workplace closures, with no closure of schools beyond the usual 16 weeks of break per year. Their use would have reduced the gross impact of the pandemic in the US in 2020 from \$4.6 trillion to \$1.9 trillion and, with high probability, saved over 100,000 lives. Our results also highlight the need to address the substantial global learning deficit incurred during the pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12016v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas J. Irons, Adrian E. Raftery</dc:creator>
    </item>
    <item>
      <title>Hierarchical Spatio-Temporal Uncertainty Quantification for Distributed Energy Adoption</title>
      <link>https://arxiv.org/abs/2411.12193</link>
      <description>arXiv:2411.12193v1 Announce Type: new 
Abstract: The rapid deployment of distributed energy resources (DER) has introduced significant spatio-temporal uncertainties in power grid management, necessitating accurate multilevel forecasting methods. However, existing approaches often produce overly conservative uncertainty intervals at individual spatial units and fail to properly capture uncertainties when aggregating predictions across different spatial scales. This paper presents a novel hierarchical spatio-temporal model based on the conformal prediction framework to address these challenges. Our approach generates circuit-level DER growth predictions and efficiently aggregates them to the substation level while maintaining statistical validity through a tailored non-conformity score. Applied to a decade of DER installation data from a local utility network, our method demonstrates superior performance over existing approaches, particularly in reducing prediction interval widths while maintaining coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12193v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenbin Zhou, Shixiang Zhu, Feng Qiu, Xuan Wu</dc:creator>
    </item>
    <item>
      <title>E-STGCN: Extreme Spatiotemporal Graph Convolutional Networks for Air Quality Forecasting</title>
      <link>https://arxiv.org/abs/2411.12258</link>
      <description>arXiv:2411.12258v1 Announce Type: new 
Abstract: Modeling and forecasting air quality plays a crucial role in informed air pollution management and protecting public health. The air quality data of a region, collected through various pollution monitoring stations, display nonlinearity, nonstationarity, and highly dynamic nature and detain intense stochastic spatiotemporal correlation. Geometric deep learning models such as Spatiotemporal Graph Convolutional Networks (STGCN) can capture spatial dependence while forecasting temporal time series data for different sensor locations. Another key characteristic often ignored by these models is the presence of extreme observations in the air pollutant levels for severely polluted cities worldwide. Extreme value theory is a commonly used statistical method to predict the expected number of violations of the National Ambient Air Quality Standards for air pollutant concentration levels. This study develops an extreme value theory-based STGCN model (E-STGCN) for air pollution data to incorporate extreme behavior across pollutant concentrations. Along with spatial and temporal components, E-STGCN uses generalized Pareto distribution to investigate the extreme behavior of different air pollutants and incorporate it inside graph convolutional networks. The proposal is then applied to analyze air pollution data (PM2.5, PM10, and NO2) of 37 monitoring stations across Delhi, India. The forecasting performance for different test horizons is evaluated compared to benchmark forecasters (both temporal and spatiotemporal). It was found that E-STGCN has consistent performance across all the seasons in Delhi, India, and the robustness of our results has also been evaluated empirically. Moreover, combined with conformal prediction, E-STGCN can also produce probabilistic prediction intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12258v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madhurima Panja, Tanujit Chakraborty, Anubhab Biswas, Soudeep Deb</dc:creator>
    </item>
    <item>
      <title>O-MAGIC: Online Change-Point Detection for Dynamic Systems</title>
      <link>https://arxiv.org/abs/2411.12277</link>
      <description>arXiv:2411.12277v1 Announce Type: new 
Abstract: The capture of changes in dynamic systems, especially ordinary differential equations (ODEs), is an important and challenging task, with multiple applications in biomedical research and other scientific areas. This article proposes a fast and mathematically rigorous online method, called ODE-informed MAnifold-constrained Gaussian process Inference for Change point detection(O-MAGIC), to detect changes of parameters in the ODE system using noisy and sparse observation data. O-MAGIC imposes a Gaussian process prior to the time series of system components with a latent manifold constraint, induced by restricting the derivative process to satisfy ODE conditions. To detect the parameter changes from the observation, we propose a procedure based on a two-sample generalized likelihood ratio (GLR) test that can detect multiple change points in the dynamic system automatically. O-MAGIC bypasses conventional numerical integration and achieves substantial savings in computation time. By incorporating the ODE structures through manifold constraints, O-MAGIC enjoys a significant advantage in detection delay, while following principled statistical construction under the Bayesian paradigm, which further enables it to handle systems with missing data or unobserved components. O-MAGIC can also be applied to general nonlinear systems. Simulation studies on three challenging examples: SEIRD model, Lotka-Volterra model and Lorenz model are provided to illustrate the robustness and efficiency of O-MAGIC, compared with numerical integration and other popular time-series-based change point detection benchmark methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12277v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Sun, Yeping Wang, Zhaohui Li, Shihao Yang</dc:creator>
    </item>
    <item>
      <title>A Comparison of Zero-Inflated Models for Modern Biomedical Data</title>
      <link>https://arxiv.org/abs/2411.12086</link>
      <description>arXiv:2411.12086v1 Announce Type: cross 
Abstract: Many data sets cannot be accurately described by standard probability distributions due to the excess number of zero values present. For example, zero-inflation is prevalent in microbiome data and single-cell RNA sequencing data, which serve as our real data examples. Several models have been proposed to address zero-inflated datasets including the zero-inflated negative binomial, hurdle negative binomial model, and the truncated latent Gaussian copula model. This study aims to compare various models and determine which one performs optimally under different conditions using both simulation studies and real data analyses. We are particularly interested in investigating how dependence among the variables, level of zero-inflation or deflation, and variance of the data affects model selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12086v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Beveridge, Zach Goldstein, Hee Cheol Chung</dc:creator>
    </item>
    <item>
      <title>Sensor-fusion based Prognostics Framework for Complex Engineering Systems Exhibiting Multiple Failure Modes</title>
      <link>https://arxiv.org/abs/2411.12159</link>
      <description>arXiv:2411.12159v1 Announce Type: cross 
Abstract: Complex engineering systems are often subject to multiple failure modes. Developing a remaining useful life (RUL) prediction model that does not consider the failure mode causing degradation is likely to result in inaccurate predictions. However, distinguishing between causes of failure without manually inspecting the system is nontrivial. This challenge is increased when the causes of historically observed failures are unknown. Sensors, which are useful for monitoring the state-of-health of systems, can also be used for distinguishing between multiple failure modes as the presence of multiple failure modes results in discriminatory behavior of the sensor signals. When systems are equipped with multiple sensors, some sensors may exhibit behavior correlated with degradation, while other sensors do not. Furthermore, which sensors exhibit this behavior may differ for each failure mode. In this paper, we present a simultaneous clustering and sensor selection approach for unlabeled training datasets of systems exhibiting multiple failure modes. The cluster assignments and the selected sensors are then utilized in real-time to first diagnose the active failure mode and then to predict the system RUL. We validate the complete pipeline of the methodology using a simulated dataset of systems exhibiting two failure modes and on a turbofan degradation dataset from NASA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12159v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Peters, Ayush Mohanty, Xiaolei Fang, Stephen K. Robinson, Nagi Gebraeel</dc:creator>
    </item>
    <item>
      <title>Left-truncated discrete lifespans: The AFiD enterprise panel</title>
      <link>https://arxiv.org/abs/2411.12367</link>
      <description>arXiv:2411.12367v1 Announce Type: cross 
Abstract: Our model for the lifespan of an enterprise is the geometric distribution. We do not formulate a model for enterprise foundation, but assume that foundations and lifespans are independent. We aim to fit the model to information about foundation and closure of German enterprises in the AFiD panel. The lifespan for an enterprise that has been founded before the first wave of the panel is either left truncated, when the enterprise is contained in the panel, or missing, when it already closed down before the first wave. Marginalizing the likelihood to that part of the enterprise history after the first wave contributes to the aim of a closed-form estimate and standard error. Invariance under the foundation distribution is achived by conditioning on observability of the enterprises. The conditional marginal likelihood can be written as a function of a martingale. The later arises when calculating the compensator, with respect some filtration, of a process that counts the closures. The estimator itself can then also be written as a martingale transform and consistency as well as asymptotic normality are easily proven. The life expectancy of German enterprises, estimated from the demographic information about 1.4 million enterprises for the years 2018 and 2019, are ten years. The width of the confidence interval are two months. Closure after the last wave is taken into account as right censored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12367v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Scholz, Rafael Wei{\ss}bach</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian causal estimation for causal inference in medical diagnosis</title>
      <link>https://arxiv.org/abs/2411.12477</link>
      <description>arXiv:2411.12477v1 Announce Type: cross 
Abstract: Causal effect estimation is a critical task in statistical learning that aims to find the causal effect on subjects by identifying causal links between a number of predictor (or, explanatory) variables and the outcome of a treatment. In a regressional framework, we assign a treatment and outcome model to estimate the average causal effect. Additionally, for high dimensional regression problems, variable selection methods are also used to find a subset of predictor variables that maximises the predictive performance of the underlying model for better estimation of the causal effect. In this paper, we propose a different approach. We focus on the variable selection aspects of high dimensional causal estimation problem. We suggest a cautious Bayesian group LASSO (least absolute shrinkage and selection operator) framework for variable selection using prior sensitivity analysis. We argue that in some cases, abstaining from selecting (or, rejecting) a predictor is beneficial and we should gather more information to obtain a more decisive result. We also show that for problems with very limited information, expert elicited variable selection can give us a more stable causal effect estimation as it avoids overfitting. Lastly, we carry a comparative study with synthetic dataset and show the applicability of our method in real-life situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12477v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tathagata Basu, Matthias C. M. Troffaes</dc:creator>
    </item>
    <item>
      <title>Germany's Tax Revenue and its Total Administrative Cost</title>
      <link>https://arxiv.org/abs/2411.12543</link>
      <description>arXiv:2411.12543v1 Announce Type: cross 
Abstract: Tax administrative cost reduction is an economically and socially desirable goal for public policy. This article proposes total administrative cost as percentage of total tax revenue as a vivid measurand, also useful for cross-jurisdiction comparisons. Statistical data, surveys and a novel approach demonstrate: Germany's 2021 tax administrative costs likely exceeded 20% of total tax revenue, indicating need for improvement of Germany's taxation system - and for the many jurisdictions with similar tax regimes. In addition, this article outlines possible reasons for and implications of the seemingly high tax administrative burden as well as solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12543v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Originally published in an earlier version in the peer-reviewed Journal of Multidisciplinary Research, ISSN: 1947-2900(p), 1947-2919(e) in 2024-09 (Vol 16 No 1), found at t1p.de/nweix</arxiv:journal_reference>
      <dc:creator>Christopher Mantzaris, Ajda Fo\v{s}ner</dc:creator>
    </item>
    <item>
      <title>Semiparametric quantile functional regression analysis of adolescent physical activity distributions in the presence of missing data</title>
      <link>https://arxiv.org/abs/2411.12585</link>
      <description>arXiv:2411.12585v1 Announce Type: cross 
Abstract: In the age of digital healthcare, passively collected physical activity profiles from wearable sensors are a preeminent tool for evaluating health outcomes. In order to fully leverage the vast amounts of data collected through wearable accelerometers, we propose to use quantile functional regression to model activity profiles as distributional outcomes through quantile responses, which can be used to evaluate activity level differences across covariates based on any desired distributional summary. Our proposed framework addresses two key problems not handled in existing distributional regression literature. First, we use spline mixed model formulations in the basis space to model nonparametric effects of continuous predictors on the distributional response. Second, we address the underlying missingness problem that is common in these types of wearable data but typically not addressed. We show that the missingness can induce bias in the subject-specific distributional summaries that leads to biased distributional regression estimates and even bias the frequently used scalar summary measures, and introduce a nonparametric function-on-function modeling approach that adjusts for each subject's missingness profile to address this problem. We evaluate our nonparametric modeling and missing data adjustment using simulation studies based on realistically simulated activity profiles and use it to gain insights into adolescent activity profiles from the Teen Environment and Neighborhood study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12585v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benny Ren, Ian Barnett, Haochang Shou, Jeremy Rubin, Hongxiao Zhu, Terry Conway, Kelli Cain, Brian Saelens, Karen Glanz, James Sallis, Jeffrey S. Morris</dc:creator>
    </item>
    <item>
      <title>A Hybrid Data-Driven Multi-Stage Deep Learning Framework for Enhanced Nuclear Reactor Power Prediction</title>
      <link>https://arxiv.org/abs/2211.13157</link>
      <description>arXiv:2211.13157v3 Announce Type: replace 
Abstract: The accurate and efficient modeling of nuclear reactor transients is crucial for ensuring safe and optimal reactor operation. Traditional physics-based models, while valuable, can be computationally intensive and may not fully capture the complexities of real-world reactor behavior. This paper introduces a novel multi-stage deep learning framework that addresses these limitations, offering a faster and more robust solution for predicting the final steady-state power of reactor transients. By leveraging a combination of feed-forward neural networks with both classification and regression stages, and training on a unique dataset that integrates real-world measurements of reactor power and controls state from the Missouri University of Science and Technology Reactor (MSTR) with noise-enhanced simulated data, our approach achieves remarkable accuracy (96% classification, 2.3% MAPE). The incorporation of simulated data with noise significantly improves the model's generalization capabilities, mitigating the risk of overfitting. This innovative solution not only enables rapid and precise prediction of reactor behavior but also has the potential to revolutionize nuclear reactor operations, facilitating enhanced safety protocols, optimized performance, and streamlined decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.13157v3</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Daniell, Kazuma Kobayashi, Ayodeji Alajo, Syed Bahauddin Alam</dc:creator>
    </item>
    <item>
      <title>Smoothing Quantile Regression Averaging: A new approach to probabilistic forecasting of electricity prices</title>
      <link>https://arxiv.org/abs/2302.00411</link>
      <description>arXiv:2302.00411v3 Announce Type: replace 
Abstract: Accurate short-term price forecasting is essential for daily operations in electricity markets. This article introduces a new method, called Smoothing Quantile Regression (SQR) Averaging, that improves upon well-performing probabilistic forecasting schemes. To demonstrate its utility, a comprehensive study is conducted on two electricity markets, including recent data covering the COVID-19 pandemic and the Russian invasion of Ukraine. The performance of SQR Averaging is evaluated both in terms of reliability and sharpness measures, and economic benefits from a trading strategy. The latter utilizes battery storage and sets limit orders using selected quantiles of the predictive distribution. SQR Averaging leads to profit increases of up to 3.5\% on average compared to the benchmark strategy based solely on point forecasts. This is strong evidence for the practical value of using probabilistic forecasts in day-ahead power trading, even in the face of the COVID-19 pandemic and geopolitical disruptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.00411v3</guid>
      <category>stat.AP</category>
      <category>q-fin.CP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bartosz Uniejewski</dc:creator>
    </item>
    <item>
      <title>Probabilistic forecasting with a hybrid Factor-QRA approach: Application to electricity trading</title>
      <link>https://arxiv.org/abs/2303.08565</link>
      <description>arXiv:2303.08565v2 Announce Type: replace 
Abstract: This paper presents a novel hybrid approach for constricting probabilistic forecasts that combines both the Quantile Regression Averaging (QRA) method and the factor-based averaging scheme. The performance of the approach is evaluated on data sets from two European energy markets - the German EPEX SPOT and the Polish Power Exchange (TGE). The results show that the newly proposed method outperforms literature benchmarks in terms of statistical measures: the empirical coverage and the Christoffersen test for conditional coverage. Moreover, in line with recent literature trends, the economic value of forecasts is evaluated based on the trading strategy using probabilistic price predictions to optimize the operation of an energy storage system. The results suggest that apart from the use of statistical measures, there is a need for the economic evaluation of forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.08565v2</guid>
      <category>stat.AP</category>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.epsr.2024.110541</arxiv:DOI>
      <arxiv:journal_reference>Electric Power Systems Research 2024, 234, 110541</arxiv:journal_reference>
      <dc:creator>Katarzyna Maciejowska, Tomasz Serafin, Bartosz Uniejewski</dc:creator>
    </item>
    <item>
      <title>Zero-Truncated Modelling in a Meta-Analysis on Suicide Data after Bariatric Surgery</title>
      <link>https://arxiv.org/abs/2305.01277</link>
      <description>arXiv:2305.01277v2 Announce Type: replace 
Abstract: Meta-analysis is a well-established method for integrating results from several independent studies to estimate a common quantity of interest. However, meta-analysis is prone to selection bias, notably when particular studies are systematically excluded. This can lead to bias in estimating the quantity of interest. Motivated by a meta-analysis to estimate the rate of completed-suicide after bariatric surgery, where studies which reported no suicides were excluded, a novel zero-truncated count modelling approach was developed. This approach addresses heterogeneity, both observed and unobserved, through covariate and overdispersion modelling, respectively. Additionally, through the Horvitz-Thompson estimator, an approach is developed to estimate the number of excluded studies, a quantity of potential interest for researchers. Uncertainty quantification for both estimation of suicide rates and number of excluded studies is achieved through a parametric bootstrapping approach.\end{abstract}</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.01277v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Layna Charlie Dennett, Antony Overstall, Dankmar Boehning</dc:creator>
    </item>
    <item>
      <title>Monitoring time to event in registry data using CUSUMs based on excess hazard models</title>
      <link>https://arxiv.org/abs/2411.09353</link>
      <description>arXiv:2411.09353v2 Announce Type: replace 
Abstract: An aspect of interest in surveillance of diseases is whether the survival time distribution changes over time. By following data in health registries over time, this can be monitored, either in real time or retrospectively. With relevant risk factors registered, these can be taken into account in the monitoring as well. A challenge in monitoring survival times based on registry data is that data on cause of death might either be missing or uncertain. To quantify the burden of disease in such cases, excess hazard methods can be used, where the total hazard is modelled as the population hazard plus the excess hazard due to the disease.
  We propose a CUSUM procedure for monitoring for changes in the survival time distribution in cases where use of excess hazard models is relevant. The procedure is based on a survival log-likelihood ratio and extends previously suggested methods for monitoring of time to event to the excess hazard setting. The procedure takes into account changes in the population risk over time, as well as changes in the excess hazard which is explained by observed covariates. Properties, challenges and an application to cancer registry data will be presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09353v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jimmy Huy Tran, Jan Terje Kval{\o}y, Hartwig K{\o}rner</dc:creator>
    </item>
    <item>
      <title>A Probabilistic Framework for Estimating the Modal Age at Death</title>
      <link>https://arxiv.org/abs/2411.09800</link>
      <description>arXiv:2411.09800v2 Announce Type: replace 
Abstract: We present a novel method for estimating the probability distribution of the modal age at death - the age at which the highest number of deaths occurs in a population. Traditional demographic methods often relies on point estimates derived from parametric models or smoothing techniques, which may overlook the inherent variability and uncertainty in mortality data. By contrast, our approach models death counts across age intervals as outcomes of a multinomial distribution, aligning with the categorical nature of mortality data. By applying a Gaussian approximation, we make the computation of modal age probabilities feasible. While this probabilistic method offers a robust approach to analyzing mortality data, we acknowledge its limitations, particularly the assumption of independent deaths, which may not hold during events like epidemics or when social factors significantly influence mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09800v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silvio C. Patricio</dc:creator>
    </item>
    <item>
      <title>Machine learning-based probabilistic forecasting of solar irradiance in Chile</title>
      <link>https://arxiv.org/abs/2411.11073</link>
      <description>arXiv:2411.11073v2 Announce Type: replace 
Abstract: By the end of 2023, renewable sources cover 63.4% of the total electric power demand of Chile, and in line with the global trend, photovoltaic (PV) power shows the most dynamic increase. Although Chile's Atacama Desert is considered the sunniest place on Earth, PV power production, even in this area, can be highly volatile. Successful integration of PV energy into the country's power grid requires accurate short-term PV power forecasts, which can be obtained from predictions of solar irradiance and related weather quantities. Nowadays, in weather forecasting, the state-of-the-art approach is the use of ensemble forecasts based on multiple runs of numerical weather prediction models. However, ensemble forecasts still tend to be uncalibrated or biased, thus requiring some form of post-processing. The present work investigates probabilistic forecasts of solar irradiance for Regions III and IV in Chile. For this reason, 8-member short-term ensemble forecasts of solar irradiance for calendar year 2021 are generated using the Weather Research and Forecasting (WRF) model, which are then calibrated using the benchmark ensemble model output statistics (EMOS) method based on a censored Gaussian law, and its machine learning-based distributional regression network (DRN) counterpart. Furthermore, we also propose a neural network-based post-processing method resulting in improved 8-member ensemble predictions. All forecasts are evaluated against station observations for 30 locations, and the skill of post-processed predictions is compared to the raw WRF ensemble. Our case study confirms that all studied post-processing methods substantially improve both the calibration of probabilistic- and the accuracy of point forecasts. Among the methods tested, the corrected ensemble exhibits the best overall performance. Additionally, the DRN model generally outperforms the corresponding EMOS approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11073v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'andor Baran, Julio C. Mar\'in, Omar Cuevas, Mailiu D\'iaz, Marianna Szab\'o, Orietta Nicolis, M\'aria Lakatos</dc:creator>
    </item>
    <item>
      <title>Robust Pareto Set Identification with Contaminated Bandit Feedback</title>
      <link>https://arxiv.org/abs/2206.02666</link>
      <description>arXiv:2206.02666v2 Announce Type: replace-cross 
Abstract: We consider the Pareto set identification (PSI) problem in multi-objective multi-armed bandits (MO-MAB) with contaminated reward observations. At each arm pull, with some fixed probability, the true reward samples are replaced with the samples from an arbitrary contamination distribution chosen by an adversary. We consider ({\alpha}, {\delta})-PAC PSI and propose a sample median-based multi-objective adaptive elimination algorithm that returns an ({\alpha}, {\delta})- PAC Pareto set upon termination with a sample complexity bound that depends on the contamination probability. As the contamination probability decreases, we recover the wellknown sample complexity results in MO-MAB. We compare the proposed algorithm with a mean-based method from MO-MAB literature, as well as an extended version that uses median estimators, on several PSI problems under adversarial corruptions, including review bombing and diabetes management. Our numerical results support our theoretical findings and demonstrate that robust algorithm design is crucial for accurate PSI under contaminated reward observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02666v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\.Ilter Onat Korkmaz, Efe Eren Ceyani, Kerem Bozgan, Cem Tekin</dc:creator>
    </item>
    <item>
      <title>Aggregating Dependent Signals with Heavy-Tailed Combination Tests</title>
      <link>https://arxiv.org/abs/2310.20460</link>
      <description>arXiv:2310.20460v2 Announce Type: replace-cross 
Abstract: Combining dependent p-values to evaluate the global null hypothesis presents a longstanding challenge in statistical inference, particularly when aggregating results from diverse methods to boost signal detection. P-value combination tests using heavy-tailed distribution based transformations, such as the Cauchy combination test and the harmonic mean p-value, have recently garnered significant interest for their potential to efficiently handle arbitrary p-value dependencies. Despite their growing popularity in practical applications, there is a gap in comprehensive theoretical and empirical evaluations of these methods. This paper conducts an extensive investigation, revealing that, theoretically, while these combination tests are asymptotically valid for pairwise quasi-asymptotically independent test statistics, such as bivariate normal variables, they are also asymptotically equivalent to the Bonferroni test under the same conditions. However, extensive simulations unveil their practical utility, especially in scenarios where stringent type-I error control is not necessary and signals are dense. Both the heaviness of the distribution and its support substantially impact the tests' non-asymptotic validity and power, and we recommend using a truncated Cauchy distribution in practice. Moreover, we show that under the violation of quasi-asymptotic independence among test statistics, these tests remain valid and, in fact, can be considerably less conservative than the Bonferroni test. We also present two case studies in genetics and genomics, showcasing the potential of the combination tests to significantly enhance statistical power while effectively controlling type-I errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20460v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Gui, Yuchao Jiang, Jingshu Wang</dc:creator>
    </item>
    <item>
      <title>SportsNGEN: Sustained Generation of Realistic Multi-player Sports Gameplay</title>
      <link>https://arxiv.org/abs/2403.12977</link>
      <description>arXiv:2403.12977v3 Announce Type: replace-cross 
Abstract: We present a transformer decoder based sports simulation engine, SportsNGEN, trained on sports player and ball tracking sequences, that is capable of generating sustained gameplay and accurately mimicking the decision making of real players. By training on a large database of professional tennis tracking data, we demonstrate that simulations produced by SportsNGEN can be used to predict the outcomes of rallies, determine the best shot choices at any point, and evaluate counterfactual or what if scenarios to inform coaching decisions and elevate broadcast coverage. By combining the generated simulations with a shot classifier and logic to start and end rallies, the system is capable of simulating an entire tennis match. We evaluate SportsNGEN by comparing statistics of the simulations with those of real matches between the same players. We show that the model output sampling parameters are crucial to simulation realism and that SportsNGEN is probabilistically well-calibrated to real data. In addition, a generic version of SportsNGEN can be customized to a specific player by fine-tuning on the subset of match data that includes that player. Finally, we show qualitative results indicating the same approach works for football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12977v3</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lachlan Thorpe, Lewis Bawden, Karanjot Vendal, John Bronskill, Richard E. Turner</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Polychoric Correlation</title>
      <link>https://arxiv.org/abs/2407.18835</link>
      <description>arXiv:2407.18835v3 Announce Type: replace-cross 
Abstract: Polychoric correlation is often an important building block in the analysis of rating data, particularly for structural equation models. However, the commonly employed maximum likelihood (ML) estimator is highly susceptible to misspecification of the polychoric correlation model, for instance through violations of latent normality assumptions. We propose a novel estimator that is designed to be robust to partial misspecification of the polychoric model, that is, the model is only misspecified for an unknown fraction of observations, for instance (but not limited to) careless respondents. In contrast to existing literature, our estimator makes no assumption on the type or degree of model misspecification. It furthermore generalizes ML estimation, is consistent as well as asymptotically normally distributed, and comes at no additional computational cost. We demonstrate the robustness and practical usefulness of our estimator in simulation studies and an empirical application on a Big Five administration. In the latter, the polychoric correlation estimates of our estimator and ML differ substantially, which, after further inspection, is likely due to the presence of careless respondents that the estimator helps identify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18835v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Welz, Patrick Mair, Andreas Alfons</dc:creator>
    </item>
  </channel>
</rss>
