<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Aug 2024 01:38:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bootstrap Matching: a robust and efficient correction for non-random A/B test, and its applications</title>
      <link>https://arxiv.org/abs/2408.05297</link>
      <description>arXiv:2408.05297v1 Announce Type: new 
Abstract: A/B testing, a widely used form of Randomized Controlled Trial (RCT), is a fundamental tool in business data analysis and experimental design. However, despite its intent to maintain randomness, A/B testing often faces challenges that compromise this randomness, leading to significant limitations in practice. In this study, we introduce Bootstrap Matching, an innovative approach that integrates Bootstrap resampling, Matching techniques, and high-dimensional hypothesis testing to address the shortcomings of A/B tests when true randomization is not achieved. Unlike traditional methods such as Difference-in-Differences (DID) and Propensity Score Matching (PSM), Bootstrap Matching is tailored for large-scale datasets, offering enhanced robustness and computational efficiency. We illustrate the effectiveness of this methodology through a real-world application in online advertising and further discuss its potential applications in digital marketing, empirical economics, clinical trials, and high-dimensional bioinformatics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05297v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Zheng, Carol Liu</dc:creator>
    </item>
    <item>
      <title>Debiased Estimating Equation Method for Versatile and Efficient Mendelian Randomization Using Large Numbers of Correlated Weak and Invalid Instruments</title>
      <link>https://arxiv.org/abs/2408.05386</link>
      <description>arXiv:2408.05386v1 Announce Type: new 
Abstract: Mendelian randomization (MR) is a widely used tool for causal inference in the presence of unobserved confounders, which uses single nucleotide polymorphisms (SNPs) as instrumental variables (IVs) to estimate causal effects. However, SNPs often have weak effects on complex traits, leading to bias and low statistical efficiency in existing MR analysis due to weak instruments that are often used. The linkage disequilibrium (LD) among SNPs poses additional statistical hurdles. Specifically, existing MR methods often restrict analysis to independent SNPs via LD clumping and result in efficiency loss in estimating the causal effect. To address these issues, we propose the Debiased Estimating Equation Method (DEEM), a summary statistics-based MR approach that incorporates a large number of correlated weak-effect and invalid SNPs. DEEM not only effectively eliminates the weak IV bias but also improves the statistical efficiency of the causal effect estimation by leveraging information from many correlated SNPs. DEEM is a versatile method that allows for pleiotropic effects, adjusts for Winner's curse, and is applicable to both two-sample and one-sample MR analyses. Asymptotic analyses of the DEEM estimator demonstrate its attractive theoretical properties. Through extensive simulations and two real data examples, we demonstrate that DEEM improves the efficiency and robustness of MR analysis compared with existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05386v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyu Wang, Haoyu Zhang, Xihong Lin</dc:creator>
    </item>
    <item>
      <title>Addressing Outcome Reporting Bias in Meta-analysis: A Selection Model Perspective</title>
      <link>https://arxiv.org/abs/2408.05747</link>
      <description>arXiv:2408.05747v1 Announce Type: new 
Abstract: Outcome Reporting Bias (ORB) poses significant threats to the validity of meta-analytic findings. It occurs when researchers selectively report outcomes based on the significance or direction of results, potentially leading to distorted treatment effect estimates. Despite its critical implications, ORB remains an under-recognized issue, with few comprehensive adjustment methods available. The goal of this research is to investigate ORB-adjustment techniques through a selection model lens, thereby extending some of the existing methodological approaches available in the literature. To gain a better insight into the effects of ORB in meta-analysis of clinical trials, specifically in the presence of heterogeneity, and to assess the effectiveness of ORB-adjustment techniques, we apply the methodology to real clinical data affected by ORB and conduct a simulation study focusing on treatment effect estimation with a secondary interest in heterogeneity quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05747v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandra Gaia Saracini, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>BOP2-TE: Bayesian Optimal Phase 2 Design for Jointly Monitoring Efficacy and Toxicity with Application to Dose Optimization</title>
      <link>https://arxiv.org/abs/2408.05816</link>
      <description>arXiv:2408.05816v1 Announce Type: new 
Abstract: We propose a Bayesian optimal phase 2 design for jointly monitoring efficacy and toxicity, referred to as BOP2-TE, to improve the operating characteristics of the BOP2 design proposed by Zhou et al. (2017). BOP2-TE utilizes a Dirichlet-multinomial model to jointly model the distribution of toxicity and efficacy endpoints, making go/no-go decisions based on the posterior probability of toxicity and futility. In comparison to the original BOP2 and other existing designs, BOP2-TE offers the advantage of providing rigorous type I error control in cases where the treatment is toxic and futile, effective but toxic, or safe but futile, while optimizing power when the treatment is effective and safe. As a result, BOP2-TE enhances trial safety and efficacy. We also explore the incorporation of BOP2-TE into multiple-dose randomized trials for dose optimization, and consider a seamless design that integrates phase I dose finding with phase II randomized dose optimization. BOP2-TE is user-friendly, as its decision boundary can be determined prior to the trial's onset. Simulations demonstrate that BOP2-TE possesses desirable operating characteristics. We have developed a user-friendly web application as part of the BOP2 app, which is freely available at www.trialdesign.org.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05816v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kai Chen, Heng Zhou, J. Jack Lee, Ying Yuan</dc:creator>
    </item>
    <item>
      <title>Censored and extreme losses: functional convergence and applications to tail goodness-of-fit</title>
      <link>https://arxiv.org/abs/2408.05862</link>
      <description>arXiv:2408.05862v1 Announce Type: new 
Abstract: This paper establishes the functional convergence of the Extreme Nelson--Aalen and Extreme Kaplan--Meier estimators, which are designed to capture the heavy-tailed behaviour of censored losses. The resulting limit representations can be used to obtain the distributions of pathwise functionals with respect to the so-called tail process. For instance, we may recover the convergence of a censored Hill estimator, and we further investigate two goodness-of-fit statistics for the tail of the loss distribution. Using the the latter limit theorems, we propose two rules for selecting a suitable number of order statistics, both based on test statistics derived from the functional convergence results. The effectiveness of these selection rules is investigated through simulations and an application to a real dataset comprised of French motor insurance claim sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05862v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Christoffer {\O}hlenschl{\ae}ger</dc:creator>
    </item>
    <item>
      <title>Statistically Optimal Uncertainty Quantification for Expensive Black-Box Models</title>
      <link>https://arxiv.org/abs/2408.05887</link>
      <description>arXiv:2408.05887v1 Announce Type: new 
Abstract: Uncertainty quantification, by means of confidence interval (CI) construction, has been a fundamental problem in statistics and also important in risk-aware decision-making. In this paper, we revisit the basic problem of CI construction, but in the setting of expensive black-box models. This means we are confined to using a low number of model runs, and without the ability to obtain auxiliary model information such as gradients. In this case, there exist classical methods based on data splitting, and newer methods based on suitable resampling. However, while all these resulting CIs have similarly accurate coverage in large sample, their efficiencies in terms of interval length differ, and a systematic understanding of which method and configuration attains the shortest interval appears open. Motivated by this, we create a theoretical framework to study the statistical optimality on CI tightness under computation constraint. Our theory shows that standard batching, but also carefully constructed new formulas using uneven-size or overlapping batches, batched jackknife, and the so-called cheap bootstrap and its weighted generalizations, are statistically optimal. Our developments build on a new bridge of the classical notion of uniformly most accurate unbiasedness with batching and resampling, by viewing model runs as asymptotically Gaussian "data", as well as a suitable notion of homogeneity for CIs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05887v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyi He, Henry Lam</dc:creator>
    </item>
    <item>
      <title>Scalable recommender system based on factor analysis</title>
      <link>https://arxiv.org/abs/2408.05896</link>
      <description>arXiv:2408.05896v1 Announce Type: new 
Abstract: Recommender systems have become crucial in the modern digital landscape, where personalized content, products, and services are essential for enhancing user experience. This paper explores statistical models for recommender systems, focusing on crossed random effects models and factor analysis. We extend the crossed random effects model to include random slopes, enabling the capture of varying covariate effects among users and items. Additionally, we investigate the use of factor analysis in recommender systems, particularly for settings with incomplete data. The paper also discusses scalable solutions using the Expectation Maximization (EM) and variational EM algorithms for parameter estimation, highlighting the application of these models to predict user-item interactions effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05896v1</guid>
      <category>stat.ME</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Disha Ghandwani, Trevor Hastie</dc:creator>
    </item>
    <item>
      <title>Cross-Spectral Analysis of Bivariate Graph Signals</title>
      <link>https://arxiv.org/abs/2408.05961</link>
      <description>arXiv:2408.05961v1 Announce Type: new 
Abstract: With the advancements in technology and monitoring tools, we often encounter multivariate graph signals, which can be seen as the realizations of multivariate graph processes, and revealing the relationship between their constituent quantities is one of the important problems. To address this issue, we propose a cross-spectral analysis tool for bivariate graph signals. The main goal of this study is to extend the scope of spectral analysis of graph signals to multivariate graph signals. In this study, we define joint weak stationarity graph processes and introduce graph cross-spectral density and coherence for multivariate graph processes. We propose several estimators for the cross-spectral density and investigate the theoretical properties of the proposed estimators. Furthermore, we demonstrate the effectiveness of the proposed estimators through numerical experiments, including simulation studies and a real data application. Finally, as an interesting extension, we discuss robust spectral analysis of graph signals in the presence of outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05961v1</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyusoon Kim, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>Identifying Total Causal Effects in Linear Models under Partial Homoscedasticity</title>
      <link>https://arxiv.org/abs/2408.06046</link>
      <description>arXiv:2408.06046v1 Announce Type: new 
Abstract: A fundamental challenge of scientific research is inferring causal relations based on observed data. One commonly used approach involves utilizing structural causal models that postulate noisy functional relations among interacting variables. A directed graph naturally represents these models and reflects the underlying causal structure. However, classical identifiability results suggest that, without conducting additional experiments, this causal graph can only be identified up to a Markov equivalence class of indistinguishable models. Recent research has shown that focusing on linear relations with equal error variances can enable the identification of the causal structure from mere observational data. Nonetheless, practitioners are often primarily interested in the effects of specific interventions, rendering the complete identification of the causal structure unnecessary. In this work, we investigate the extent to which less restrictive assumptions of partial homoscedasticity are sufficient for identifying the causal effects of interest. Furthermore, we construct mathematically rigorous confidence regions for total causal effects under structure uncertainty and explore the performance gain of relying on stricter error assumptions in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06046v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Strieder, Mathias Drton</dc:creator>
    </item>
    <item>
      <title>Extreme-based causal effect learning with endogenous exposures and a light-tailed error</title>
      <link>https://arxiv.org/abs/2408.06211</link>
      <description>arXiv:2408.06211v1 Announce Type: new 
Abstract: Endogeneity poses significant challenges in causal inference across various research domains. This paper proposes a novel approach to identify and estimate causal effects in the presence of endogeneity. We consider a structural equation with endogenous exposures and an additive error term. Assuming the light-tailedness of the error term, we show that the causal effect can be identified by contrasting extreme conditional quantiles of the outcome given the exposures. Unlike many existing results, our identification approach does not rely on additional parametric assumptions or auxiliary variables. Building on the identification result, we develop an EXtreme-based Causal Effect Learning (EXCEL) method that estimates the causal effect using extreme quantile regression. We establish the consistency of the EXCEL estimator under a general additive structural equation and demonstrate its asymptotic normality in the linear model setting. These results reveal that extreme quantile regression is invulnerable to endogeneity when the error term is light-tailed, which is not appreciated in the literature to our knowledge. The EXCEL method is applied to causal inference problems with invalid instruments to construct a valid confidence set for the causal effect. Simulations and data analysis of an automobile sale dataset show the effectiveness of our method in addressing endogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06211v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyu Wang, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Optimal Integrative Estimation for Distributed Precision Matrices with Heterogeneity Adjustment</title>
      <link>https://arxiv.org/abs/2408.06263</link>
      <description>arXiv:2408.06263v1 Announce Type: new 
Abstract: Distributed learning offers a practical solution for the integrative analysis of multi-source datasets, especially under privacy or communication constraints. However, addressing prospective distributional heterogeneity and ensuring communication efficiency pose significant challenges on distributed statistical analysis. In this article, we focus on integrative estimation of distributed heterogeneous precision matrices, a crucial task related to joint precision matrix estimation where computation-efficient algorithms and statistical optimality theories are still underdeveloped. To tackle these challenges, we introduce a novel HEterogeneity-adjusted Aggregating and Thresholding (HEAT) approach for distributed integrative estimation. HEAT is designed to be both communication- and computation-efficient, and we demonstrate its statistical optimality by establishing the convergence rates and the corresponding minimax lower bounds under various integrative losses. To enhance the optimality of HEAT, we further propose an iterative HEAT (IteHEAT) approach. By iteratively refining the higher-order errors of HEAT estimators through multi-round communications, IteHEAT achieves geometric contraction rates of convergence. Extensive simulations and real data applications validate the numerical performance of HEAT and IteHEAT methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06263v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinrui Sun, Yin Xia</dc:creator>
    </item>
    <item>
      <title>Infer-and-widen versus split-and-condition: two tales of selective inference</title>
      <link>https://arxiv.org/abs/2408.06323</link>
      <description>arXiv:2408.06323v1 Announce Type: new 
Abstract: Recent attention has focused on the development of methods for post-selection inference. However, the connections between these methods, and the extent to which one might be preferred to another, remain unclear. In this paper, we classify existing methods for post-selection inference into one of two frameworks: infer-and-widen or split-and-condition. The infer-and-widen framework produces confidence intervals whose midpoints are biased due to selection, and must be wide enough to account for this bias. By contrast, split-and-condition directly adjusts the intervals' midpoints to account for selection. We compare the two frameworks in three vignettes: the winner's curse, maximal contrasts, and inference after the lasso. Our results are striking: in each of these examples, a split-and-condition strategy leads to confidence intervals that are much narrower than the state-of-the-art infer-and-widen proposal, when methods are tuned to yield identical selection events. Furthermore, even an ``oracle" infer-and-widen confidence interval -- the narrowest possible interval that could be theoretically attained via infer-and-widen -- is not necessarily narrower than a feasible split-and-condition method. Taken together, these results point to split-and-condition as the most promising framework for post-selection inference in real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06323v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronan Perry, Zichun Xu, Olivia McGough, Daniela Witten</dc:creator>
    </item>
    <item>
      <title>Generalized Encouragement-Based Instrumental Variables for Counterfactual Regression</title>
      <link>https://arxiv.org/abs/2408.05428</link>
      <description>arXiv:2408.05428v1 Announce Type: cross 
Abstract: In causal inference, encouragement designs (EDs) are widely used to analyze causal effects, when randomized controlled trials (RCTs) are impractical or compliance to treatment cannot be perfectly enforced. Unlike RCTs, which directly allocate treatments, EDs randomly assign encouragement policies that positively motivate individuals to engage in a specific treatment. These random encouragements act as instrumental variables (IVs), facilitating the identification of causal effects through leveraging exogenous perturbations in discrete treatment scenarios. However, real-world applications of encouragement designs often face challenges such as incomplete randomization, limited experimental data, and significantly fewer encouragements compared to treatments, hindering precise causal effect estimation. To address this, this paper introduces novel theories and algorithms for identifying the Conditional Average Treatment Effect (CATE) using variations in encouragement. Further, by leveraging both observational and encouragement data, we propose a generalized IV estimator, named Encouragement-based Counterfactual Regression (EnCounteR), to effectively estimate the causal effects. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of EnCounteR over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05428v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anpeng Wu, Kun Kuang, Ruoxuan Xiong, Xiangwei Chen, Zexu Sun, Fei Wu, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Testing Elliptical Models in High Dimensions</title>
      <link>https://arxiv.org/abs/2408.05514</link>
      <description>arXiv:2408.05514v1 Announce Type: cross 
Abstract: Due to the broad applications of elliptical models, there is a long line of research on goodness-of-fit tests for empirically validating them. However, the existing literature on this topic is generally confined to low-dimensional settings, and to the best of our knowledge, there are no established goodness-of-fit tests for elliptical models that are supported by theoretical guarantees in high dimensions. In this paper, we propose a new goodness-of-fit test for this problem, and our main result shows that the test is asymptotically valid when the dimension and sample size diverge proportionally. Remarkably, it also turns out that the asymptotic validity of the test requires no assumptions on the population covariance matrix. With regard to numerical performance, we confirm that the empirical level of the test is close to the nominal level across a range of conditions, and that the test is able to reliably detect non-elliptical distributions. Moreover, when the proposed test is specialized to the problem of testing normality in high dimensions, we show that it compares favorably with a state-of-the-art method, and hence, this way of using the proposed test is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05514v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyao Wang, Miles E. Lopes</dc:creator>
    </item>
    <item>
      <title>Dynamical causality under invisible confounders</title>
      <link>https://arxiv.org/abs/2408.05584</link>
      <description>arXiv:2408.05584v1 Announce Type: cross 
Abstract: Causality inference is prone to spurious causal interactions, due to the substantial confounders in a complex system. While many existing methods based on the statistical methods or dynamical methods attempt to address misidentification challenges, there remains a notable lack of effective methods to infer causality, in particular in the presence of invisible/unobservable confounders. As a result, accurately inferring causation with invisible confounders remains a largely unexplored and outstanding issue in data science and AI fields. In this work, we propose a method to overcome such challenges to infer dynamical causality under invisible confounders (CIC method) and further reconstruct the invisible confounders from time-series data by developing an orthogonal decomposition theorem in a delay embedding space. The core of our CIC method lies in its ability to decompose the observed variables not in their original space but in their delay embedding space into the common and private subspaces respectively, thereby quantifying causality between those variables both theoretically and computationally. This theoretical foundation ensures the causal detection for any high-dimensional system even with only two observed variables under many invisible confounders, which is actually a long-standing problem in the field. In addition to the invisible confounder problem, such a decomposition actually makes the intertwined variables separable in the embedding space, thus also solving the non-separability problem of causal inference. Extensive validation of the CIC method is carried out using various real datasets, and the experimental results demonstrates its effectiveness to reconstruct real biological networks even with unobserved confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05584v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinling Yan, Shao-Wu Zhang, Chihao Zhang, Weitian Huang, Jifan Shi, Luonan Chen</dc:creator>
    </item>
    <item>
      <title>Controlling for discrete unmeasured confounding in nonlinear causal models</title>
      <link>https://arxiv.org/abs/2408.05647</link>
      <description>arXiv:2408.05647v1 Announce Type: cross 
Abstract: Unmeasured confounding is a major challenge for identifying causal relationships from non-experimental data. Here, we propose a method that can accommodate unmeasured discrete confounding. Extending recent identifiability results in deep latent variable models, we show theoretically that confounding can be detected and corrected under the assumption that the observed data is a piecewise affine transformation of a latent Gaussian mixture model and that the identity of the mixture components is confounded. We provide a flow-based algorithm to estimate this model and perform deconfounding. Experimental results on synthetic and real-world data provide support for the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05647v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Burauel, Frederick Eberhardt, Michel Besserve</dc:creator>
    </item>
    <item>
      <title>On the Robustness of Kernel Goodness-of-Fit Tests</title>
      <link>https://arxiv.org/abs/2408.05854</link>
      <description>arXiv:2408.05854v1 Announce Type: cross 
Abstract: Goodness-of-fit testing is often criticized for its lack of practical relevance; since ``all models are wrong'', the null hypothesis that the data conform to our model is ultimately always rejected when the sample size is large enough. Despite this, probabilistic models are still used extensively, raising the more pertinent question of whether the model is good enough for a specific task. This question can be formalized as a robust goodness-of-fit testing problem by asking whether the data were generated by a distribution corresponding to our model up to some mild perturbation. In this paper, we show that existing kernel goodness-of-fit tests are not robust according to common notions of robustness including qualitative and quantitative robustness. We also show that robust techniques based on tilted kernels from the parameter estimation literature are not sufficient for ensuring both types of robustness in the context of goodness-of-fit testing. We therefore propose the first robust kernel goodness-of-fit test which resolves this open problem using kernel Stein discrepancy balls, which encompass perturbation models such as Huber contamination models and density uncertainty bands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05854v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xing Liu, Fran\c{c}ois-Xavier Briol</dc:creator>
    </item>
    <item>
      <title>Method-of-Moments Inference for GLMs and Doubly Robust Functionals under Proportional Asymptotics</title>
      <link>https://arxiv.org/abs/2408.06103</link>
      <description>arXiv:2408.06103v1 Announce Type: cross 
Abstract: In this paper, we consider the estimation of regression coefficients and signal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models (GLMs), and explore their implications in inferring popular estimands such as average treatment effects in high-dimensional observational studies. Under the ``proportional asymptotic'' regime and Gaussian covariates with known (population) covariance $\Sigma$, we derive Consistent and Asymptotically Normal (CAN) estimators of our targets of inference through a Method-of-Moments type of estimators that bypasses estimation of high dimensional nuisance functions and hyperparameter tuning altogether. Additionally, under non-Gaussian covariates, we demonstrate universality of our results under certain additional assumptions on the regression coefficients and $\Sigma$. We also demonstrate that knowing $\Sigma$ is not essential to our proposed methodology when the sample covariance matrix estimator is invertible. Finally, we complement our theoretical results with numerical experiments and comparisons with existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06103v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Chen, Lin Liu, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Multi-marginal Schr\"odinger Bridges with Iterative Reference</title>
      <link>https://arxiv.org/abs/2408.06277</link>
      <description>arXiv:2408.06277v1 Announce Type: cross 
Abstract: Practitioners frequently aim to infer an unobserved population trajectory using sample snapshots at multiple time points. For instance, in single-cell sequencing, scientists would like to learn how gene expression evolves over time. But sequencing any cell destroys that cell. So we cannot access any cell's full trajectory, but we can access snapshot samples from many cells. Stochastic differential equations are commonly used to analyze systems with full individual-trajectory access; since here we have only sample snapshots, these methods are inapplicable. The deep learning community has recently explored using Schr\"odinger bridges (SBs) and their extensions to estimate these dynamics. However, these methods either (1) interpolate between just two time points or (2) require a single fixed reference dynamic within the SB, which is often just set to be Brownian motion. But learning piecewise from adjacent time points can fail to capture long-term dependencies. And practitioners are typically able to specify a model class for the reference dynamic but not the exact values of the parameters within it. So we propose a new method that (1) learns the unobserved trajectories from sample snapshots across multiple time points and (2) requires specification only of a class of reference dynamics, not a single fixed one. In particular, we suggest an iterative projection method inspired by Schr\"odinger bridges; we alternate between learning a piecewise SB on the unobserved trajectories and using the learned SB to refine our best guess for the dynamics within the reference class. We demonstrate the advantages of our method via a well-known simulated parametric model from ecology, simulated and real data from systems biology, and real motion-capture data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06277v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunyi Shen, Renato Berlinghieri, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Feature Selection in High-dimensional Spaces Using Graph-Based Methods</title>
      <link>https://arxiv.org/abs/2108.12682</link>
      <description>arXiv:2108.12682v3 Announce Type: replace 
Abstract: High-dimensional feature selection is a central problem in a variety of application domains such as machine learning, image analysis, and genomics. In this paper, we propose graph-based tests as a useful basis for feature selection. We describe an algorithm for selecting informative features in high-dimensional data, where each observation comes from one of $K$ different distributions. Our algorithm can be applied in a completely nonparametric setup without any distributional assumptions on the data, and it aims at outputting those features in the data, that contribute the most to the overall distributional variation. At the heart of our method is the recursive application of distribution-free graph-based tests on subsets of the feature set, located at different depths of a hierarchical clustering tree constructed from the data. Our algorithm recovers all truly contributing features with high probability, while ensuring optimal control on false-discovery. We show the superior performance of our method over other existing ones through synthetic data, and demonstrate the utility of this method on several real-life datasets from the domains of climate change and biology, wherein our algorithm is not only able to detect known features expected to be associated with the underlying process, but also discovers novel targets that can be subsequently studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.12682v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Swarnadip Ghosh, Somabha Mukherjee, Divyansh Agarwal, Yichen He, Mingzhi Song, Xuejiao Pei</dc:creator>
    </item>
    <item>
      <title>Powerful Spatial Multiple Testing via Borrowing Neighboring Information</title>
      <link>https://arxiv.org/abs/2210.17121</link>
      <description>arXiv:2210.17121v2 Announce Type: replace 
Abstract: Clustered effects are often encountered in multiple hypothesis testing of spatial signals. In this paper, we propose a new method, termed \textit{two-dimensional spatial multiple testing} (2d-SMT) procedure, to control the false discovery rate (FDR) and improve the detection power by exploiting the spatial information encoded in neighboring observations. The proposed method provides a novel perspective of utilizing spatial information by gathering signal patterns and spatial dependence into an auxiliary statistic. 2d-SMT rejects the null when a primary statistic at the location of interest and the auxiliary statistic constructed based on nearby observations are greater than their corresponding cutoffs. 2d-SMT can also be combined with different variants of the weighted BH procedures to improve the detection power further. A fast algorithm is developed to accelerate the search for optimal cutoffs in 2d-SMT. In theory, we establish the asymptotic FDR control of 2d-SMT under weak spatial dependence. Extensive numerical experiments demonstrate that the 2d-SMT method combined with various weighted BH procedures achieves the most competitive performance in FDR and power trade-off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.17121v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linsui Deng, Kejun He, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>A Moment-assisted Approach for Improving Subsampling-based MLE with Large-scale data</title>
      <link>https://arxiv.org/abs/2309.09872</link>
      <description>arXiv:2309.09872v3 Announce Type: replace 
Abstract: The maximum likelihood estimation is computationally demanding for large datasets, particularly when the likelihood function includes integrals. Subsampling can reduce the computational burden, but it often results in efficiency loss. This paper proposes a moment-assisted subsampling (MAS) method that can improve the estimation efficiency of existing subsampling-based maximum likelihood estimators. The motivation behind this approach stems from the fact that sample moments can be efficiently computed even if the sample size of the whole data set is huge. Through the generalized method of moments, the proposed method incorporates informative sample moments of the whole data. The MAS estimator can be computed rapidly and is asymptotically normal with a smaller asymptotic variance than the corresponding estimator without incorporating sample moments of the whole data. The asymptotic variance of the proposed estimator depends on the specific sample moments incorporated. We derive the optimal moment that minimizes the resulting asymptotic variance in terms of Loewner order. The proposed MAS estimator can achieve the same estimation efficiency as the whole data-based estimator when the optimal moment is incorporated. Numerical results demonstrate the promising performance of the proposed method in both estimation and computational efficiency compared with existing subsampling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.09872v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miaomiao Su, Qihua Wang, Ruoyu Wang</dc:creator>
    </item>
    <item>
      <title>Variable Selection and Minimax Prediction in High-dimensional Functional Linear Model</title>
      <link>https://arxiv.org/abs/2310.14419</link>
      <description>arXiv:2310.14419v3 Announce Type: replace 
Abstract: High-dimensional functional data have become increasingly prevalent in modern applications such as high-frequency financial data and neuroimaging data analysis. We investigate a class of high-dimensional linear regression models, where each predictor is a random element in an infinite-dimensional function space, and the number of functional predictors $p$ can potentially be ultra-high. Assuming that each of the unknown coefficient functions belongs to some reproducing kernel Hilbert space (RKHS), we regularize the fitting of the model by imposing a group elastic-net type of penalty on the RKHS norms of the coefficient functions. We show that our loss function is Gateaux sub-differentiable, and our functional elastic-net estimator exists uniquely in the product RKHS. Under suitable sparsity assumptions and a functional version of the irrepresentable condition, we derive a non-asymptotic tail bound for variable selection consistency of our method. Allowing the number of true functional predictors $q$ to diverge with the sample size, we also show a post-selection refined estimator can achieve the oracle minimax optimal prediction rate. The proposed methods are illustrated through simulation studies and a real-data application from the Human Connectome Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14419v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingche Guo, Yehua Li, Tailen Hsing</dc:creator>
    </item>
    <item>
      <title>Enhancing Scalability in Bayesian Nonparametric Factor Analysis of Spatiotemporal Data</title>
      <link>https://arxiv.org/abs/2312.05802</link>
      <description>arXiv:2312.05802v5 Announce Type: replace 
Abstract: This manuscript puts forward novel practicable spatiotemporal Bayesian factor analysis frameworks computationally feasible for moderate to large data. Our models exhibit significantly enhanced computational scalability and storage efficiency, deliver high overall modeling performances, and possess powerful inferential capabilities for adequately predicting outcomes at future time points or new spatial locations and satisfactorily clustering spatial locations into regions with similar temporal trajectories, a frequently encountered crucial task. We integrate on top of a baseline separable factor model with temporally dependent latent factors and spatially dependent factor loadings under a probit stick breaking process (PSBP) prior a new slice sampling algorithm that permits unknown varying numbers of spatial mixture components across all factors and guarantees them to be non-increasing through the MCMC iterations, thus considerably enhancing model flexibility, efficiency, and scalability. We further introduce a novel spatial latent nearest-neighbor Gaussian process (NNGP) prior and new sequential updating algorithms for the spatially varying latent variables in the PSBP prior, thereby attaining high spatial scalability. The markedly accelerated posterior sampling and spatial prediction as well as the great modeling and inferential performances of our models are substantiated by our simulation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05802v5</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Cheng, Cheng Li</dc:creator>
    </item>
    <item>
      <title>Proximal Causal Inference for Conditional Separable Effects</title>
      <link>https://arxiv.org/abs/2402.11020</link>
      <description>arXiv:2402.11020v2 Announce Type: replace 
Abstract: Scientists regularly pose questions about treatment effects on outcomes conditional on a post-treatment event. However, defining, identifying, and estimating causal effects conditional on post-treatment events requires care, even in perfectly executed randomized experiments. Recently, the conditional separable effect (CSE) was proposed as an interventionist estimand, corresponding to scientifically meaningful questions in these settings. However, while being a single-world estimand, which can be queried experimentally, existing identification results for the CSE require no unmeasured confounding between the outcome and post-treatment event. This assumption can be violated in many applications. In this work, we address this concern by developing new identification and estimation results for the CSE in the presence of unmeasured confounding. We establish nonparametric identification of the CSE in both observational and experimental settings when certain proxy variables are available for hidden common causes of the post-treatment event and outcome. For inference, we characterize the efficient influence function for the CSE under a semiparametric model in which nuisance functions are a priori unrestricted. Moreover, we develop a consistent, asymptotically linear, and locally semiparametric efficient estimator of the CSE using modern machine learning theory. We illustrate our framework with simulation studies and a real-world cancer therapy trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11020v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chan Park, Mats Stensrud, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Decomposition-Based Intrinsic Modeling of Shape-Constrained Functional Data</title>
      <link>https://arxiv.org/abs/2406.12817</link>
      <description>arXiv:2406.12817v2 Announce Type: replace 
Abstract: Shape-constrained functional data encompass a wide array of application fields, such as activity profiling, growth curves, healthcare and mortality. Most existing methods for general functional data analysis often ignore that such data are subject to inherent shape constraints, while some specialized techniques rely on strict distributional assumptions. We propose an approach for modeling such data that harnesses the intrinsic geometry of functional trajectories by decomposing them into size and shape components. We focus on the two most prevalent shape constraints, positivity and monotonicity, and develop individual-level estimators for the size and shape components. Furthermore, we demonstrate the applicability of our approach by conducting subsequent analyses involving Fr\'{e}chet mean and Fr\'{e}chet regression and establish rates of convergence for the empirical estimators. Illustrative examples include simulations and data applications for activity profiles for Mediterranean fruit flies during their entire lifespan and for data from the Z\"{u}rich longitudinal growth study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12817v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Poorbita Kundu, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Evaluating and Utilizing Surrogate Outcomes in Covariate-Adjusted Response-Adaptive Designs</title>
      <link>https://arxiv.org/abs/2408.02667</link>
      <description>arXiv:2408.02667v2 Announce Type: replace 
Abstract: This manuscript explores the intersection of surrogate outcomes and adaptive designs in statistical research. While surrogate outcomes have long been studied for their potential to substitute long-term primary outcomes, current surrogate evaluation methods do not directly account for the potential benefits of using surrogate outcomes to adapt randomization probabilities in adaptive randomized trials that aim to learn and respond to treatment effect heterogeneity. In this context, surrogate outcomes can benefit participants in the trial directly (i.e. improve expected outcome of newly-enrolled participants) by allowing for more rapid adaptation of randomization probabilities, particularly when surrogates enable earlier detection of heterogeneous treatment effects and/or indicate the optimal (individualized) treatment with stronger signals. Our study introduces a novel approach for surrogate evaluation that quantifies both of these benefits in the context of sequential adaptive experiment designs. We also propose a new Covariate-Adjusted Response-Adaptive (CARA) design that incorporates an Online Superlearner to assess and adaptively choose surrogate outcomes for updating treatment randomization probabilities. We introduce a Targeted Maximum Likelihood Estimator that addresses data dependency challenges in adaptively collected data and achieves asymptotic normality under reasonable assumptions without relying on parametric model assumptions. The robust performance of our adaptive design with Online Superlearner is presented via simulations. Our framework not only contributes a method to more comprehensively quantifying the benefits of candidate surrogate outcomes and choosing between them, but also offers an easily generalizable tool for evaluating various adaptive designs and making inferences, providing insights into alternative choices of designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02667v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Zhang, Aaron Hudson, Maya Petersen, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>E-backtesting</title>
      <link>https://arxiv.org/abs/2209.00991</link>
      <description>arXiv:2209.00991v4 Announce Type: replace-cross 
Abstract: In the recent Basel Accords, the Expected Shortfall (ES) replaces the Value-at-Risk (VaR) as the standard risk measure for market risk in the banking sector, making it the most important risk measure in financial regulation. One of the most challenging tasks in risk modeling practice is to backtest ES forecasts provided by financial institutions. To design a model-free backtesting procedure for ES, we make use of the recently developed techniques of e-values and e-processes. Backtest e-statistics are introduced to formulate e-processes for risk measure forecasts, and unique forms of backtest e-statistics for VaR and ES are characterized using recent results on identification functions. For a given backtest e-statistic, a few criteria for optimally constructing the e-processes are studied. The proposed method can be naturally applied to many other risk measures and statistical quantities. We conduct extensive simulation studies and data analysis to illustrate the advantages of the model-free backtesting method, and compare it with the ones in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00991v4</guid>
      <category>q-fin.RM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qiuqi Wang, Ruodu Wang, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Estimating Lagged (Cross-)Covariance Operators of $L^p$-$m$-approximable Processes in Cartesian Product Hilbert Spaces</title>
      <link>https://arxiv.org/abs/2402.08110</link>
      <description>arXiv:2402.08110v4 Announce Type: replace-cross 
Abstract: Estimating parameters of functional ARMA, GARCH and invertible processes requires estimating lagged covariance and cross-covariance operators of Cartesian product Hilbert space-valued processes. Asymptotic results have been derived in recent years, either less generally or under a strict condition. This article derives upper bounds of the estimation errors for such operators based on the mild condition Lp-m-approximability for each lag, Cartesian power(s) and sample size, where the two processes can take values in different spaces in the context of lagged cross-covariance operators. Implications of our results on eigenelements, parameters in functional AR(MA) models and other general situations are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08110v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian K\"uhnert</dc:creator>
    </item>
    <item>
      <title>Asymptotic utility of spectral anonymization</title>
      <link>https://arxiv.org/abs/2405.20779</link>
      <description>arXiv:2405.20779v2 Announce Type: replace-cross 
Abstract: In the contemporary data landscape characterized by multi-source data collection and third-party sharing, ensuring individual privacy stands as a critical concern. While various anonymization methods exist, their utility preservation and privacy guarantees remain challenging to quantify. In this work, we address this gap by studying the utility and privacy of the spectral anonymization (SA) algorithm, particularly in an asymptotic framework. Unlike conventional anonymization methods that directly modify the original data, SA operates by perturbing the data in a spectral basis and subsequently reverting them to their original basis. Alongside the original version $\mathcal{P}$-SA, employing random permutation transformation, we introduce two novel SA variants: $\mathcal{J}$-spectral anonymization and $\mathcal{O}$-spectral anonymization, which employ sign-change and orthogonal matrix transformations, respectively. We show how well, under some practical assumptions, these SA algorithms preserve the first and second moments of the original data. Our results reveal, in particular, that the asymptotic efficiency of all three SA algorithms in covariance estimation is exactly 50% when compared to the original data. To assess the applicability of these asymptotic results in practice, we conduct a simulation study with finite data and also evaluate the privacy protection offered by these algorithms using distance-based record linkage. Our research reveals that while no method exhibits clear superiority in finite-sample utility, $\mathcal{O}$-SA distinguishes itself for its exceptional privacy preservation, never producing identical records, albeit with increased computational complexity. Conversely, $\mathcal{P}$-SA emerges as a computationally efficient alternative, demonstrating unmatched efficiency in mean estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20779v2</guid>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Katariina Perkonoja, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Improving Bias Correction Standards by Quantifying its Effects on Treatment Outcomes</title>
      <link>https://arxiv.org/abs/2407.14861</link>
      <description>arXiv:2407.14861v2 Announce Type: replace-cross 
Abstract: With the growing access to administrative health databases, retrospective studies have become crucial evidence for medical treatments. Yet, non-randomized studies frequently face selection biases, requiring mitigation strategies. Propensity score matching (PSM) addresses these biases by selecting comparable populations, allowing for analysis without further methodological constraints. However, PSM has several drawbacks. Different matching methods can produce significantly different Average Treatment Effects (ATE) for the same task, even when meeting all validation criteria. To prevent cherry-picking the best method, public authorities must involve field experts and engage in extensive discussions with researchers.
  To address this issue, we introduce a novel metric, A2A, to reduce the number of valid matches. A2A constructs artificial matching tasks that mirror the original ones but with known outcomes, assessing each matching method's performance comprehensively from propensity estimation to ATE estimation. When combined with Standardized Mean Difference, A2A enhances the precision of model selection, resulting in a reduction of up to 50% in ATE estimation errors across synthetic tasks and up to 90% in predicted ATE variability across both synthetic and real-world datasets. To our knowledge, A2A is the first metric capable of evaluating outcome correction accuracy using covariates not involved in selection.
  Computing A2A requires solving hundreds of PSMs, we therefore automate all manual steps of the PSM pipeline. We integrate PSM methods from Python and R, our automated pipeline, a new metric, and reproducible experiments into popmatch, our new Python package, to enhance reproducibility and accessibility to bias correction methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14861v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alexandre Abraham, Andr\'es Hoyos Idrobo</dc:creator>
    </item>
  </channel>
</rss>
