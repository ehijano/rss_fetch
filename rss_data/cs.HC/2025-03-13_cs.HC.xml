<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Mar 2025 02:04:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Iterative, User-Centered Design of a Clinical Decision Support System for Critical Care Assessments: Co-Design Sessions with ICU Clinical Providers</title>
      <link>https://arxiv.org/abs/2503.08814</link>
      <description>arXiv:2503.08814v1 Announce Type: new 
Abstract: This study reports the findings of qualitative interview sessions conducted with ICU clinicians for the co-design of a system user interface of an artificial intelligence (AI)-driven clinical decision support (CDS) system. This system integrates medical record data with wearable sensor, video, and environmental data into a real-time dynamic model that quantifies patients' risk of clinical decompensation and risk of developing delirium, providing actionable alerts to augment clinical decision-making in the ICU setting. Co-design sessions were conducted as semi-structured focus groups and interviews with ICU clinicians, including physicians, mid-level practitioners, and nurses. Study participants were asked about their perceptions on AI-CDS systems, their system preferences, and were asked to provide feedback on the current user interface prototype. Session transcripts were qualitatively analyzed to identify key themes related to system utility, interface design features, alert preferences, and implementation considerations. Ten clinicians participated in eight sessions. The analysis identified five themes: (1) AI's computational utility, (2) workflow optimization, (3) effects on patient care, (4) technical considerations, and (5) implementation considerations. Clinicians valued the CDS system's multi-modal continuous monitoring and AI's capacity to process large volumes of data in real-time to identify patient risk factors and suggest action items. Participants underscored the system's unique value in detecting delirium and promoting non-pharmacological delirium prevention measures. The actionability and intuitive interpretation of the presented information was emphasized. ICU clinicians recognize the potential of an AI-driven CDS system for ICU delirium and acuity to improve patient outcomes and clinical workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08814v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrea E. Davidson, Jessica M. Ray, Ayush K. Patel, Yulia Strekalova Levites, Parisa Rashidi, Azra Bihorac</dc:creator>
    </item>
    <item>
      <title>A Critical Analysis of the Usage of Dimensionality Reduction in Four Domains</title>
      <link>https://arxiv.org/abs/2503.08836</link>
      <description>arXiv:2503.08836v1 Announce Type: new 
Abstract: Dimensionality reduction is used as an important tool for unraveling the complexities of high-dimensional datasets in many fields of science, such as cell biology, chemical informatics, and physics. Visualizations of the dimensionally reduced data enable scientists to delve into the intrinsic structures of their datasets and align them with established hypotheses. Visualization researchers have thus proposed many dimensionality reduction methods and interactive systems designed to uncover latent structures. At the same time, different scientific domains have formulated guidelines or common workflows for using dimensionality reduction techniques and visualizations for their respective fields. In this work, we present a critical analysis of the usage of dimensionality reduction in scientific domains outside of computer science. First, we conduct a bibliometric analysis of 21,249 academic publications that use dimensionality reduction to observe differences in the frequency of techniques across fields. Next, we conduct a survey of a 71-paper sample from four fields: biology, chemistry, physics, and business. Through this survey, we uncover common workflows, processes, and usage patterns, including the mixed use of confirmatory data analysis to validate a dataset and projection method and exploratory data analysis to then generate more hypotheses. We also find that misinterpretations and inappropriate usage is common, particularly in the visual interpretation of the resulting dimensionally reduced view. Lastly, we compare our observations with recent works in the visualization community in order to match work within our community to potential areas of impact outside our community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08836v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Cashman, Mark Keller, Hyeon Jeon, Bum Chul Kwon, Qianwen Wang</dc:creator>
    </item>
    <item>
      <title>"New" Challenges for Future C2: Commanding Soldier-Machine Partnerships</title>
      <link>https://arxiv.org/abs/2503.08844</link>
      <description>arXiv:2503.08844v1 Announce Type: new 
Abstract: Future warfare will occur in more complex, fast-paced, ill-structured, and demanding conditions that will stress current Command and Control (C2) systems. Without modernization, these C2 systems may fail to maintain overmatch against adversaries. We previously proposed robust partnerships between humans and artificial intelligence systems, and directly focusing on C2, we introduced how intelligent technologies could provide future overmatch through streamlining the C2 operations process, maintaining unity of effort across formations, and developing collective knowledge systems that adapt to battlefield dynamics across missions. Future C2 systems must seamlessly integrate human and machine intelligence to achieve decision advantage over adversaries while overcoming "new" challenges due to the technological advances driving fundamental changes in effective teaming, unity of effort, and meaningful human control. Here, we describe "new" C2 challenges and discuss pathways to transcend them, such as AI-enabled systems with effective human machine interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08844v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Madison, Kaleb McDowell, Vinicius G. Goecks, Jeff Hansberger, Ceili M. Olney, Claire Ahern, Amar Marathe, Nicholas Waytowich, Christian Kenney, Christopher Kelshaw</dc:creator>
    </item>
    <item>
      <title>The Detection of Saccadic Eye Movements and Per-Eye Comparisons using Virtual Reality Eye Tracking Devices</title>
      <link>https://arxiv.org/abs/2503.08926</link>
      <description>arXiv:2503.08926v1 Announce Type: new 
Abstract: Eye tracking has been found to be useful in various tasks including diagnostic and screening tools. However, traditional eye trackers had a complicated setup and operated at a higher frequency to measure eye movements. The use of more commonly available eye trackers such as those in head-mounted virtual reality (VR) headsets greatly expands the utility of these eye trackers for research and analytical purposes. In this study, the research question is focused on detecting saccades, which is a common task when analyzing eye tracking data, but it is not well-established for VR headset-mounted eye trackers. The aim is to determine how accurately saccadic eye movements can be detected using an eye tracker that operates at 60 or 90Hz. The study involves VR eye tracking technology and neuroscience with respect to saccadic eye movements. The goal is to build prototype software implemented using VR eye tracking technology to detect saccadic eye movements, and per-eye differences in an individual. It is anticipated that the software will be able to accurately detect when saccades occur and analyze the differences in saccadic eye movements per-eye. The field of research surrounding VR eye tracking software is still developing rapidly, specifically its applications to neuroscience. Since previous methods of eye tracking involved specialized equipment, using commercially and consumer available VR eye tracking technology to assist in the detection of saccades and per-eye differences would be novel. This project will impact the field of neuroscience by providing a tool that can be used to detect saccadic eye movements and neurological and neurodegenerative disorders. However, this project is limited by the short time frame and that the eye tracker used in this study operates at a maximum frequency of 90Hz.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08926v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teran Bukenberger, Brent Davis</dc:creator>
    </item>
    <item>
      <title>PassAI: explainable artificial intelligence algorithm for soccer pass analysis using multimodal information resources</title>
      <link>https://arxiv.org/abs/2503.08945</link>
      <description>arXiv:2503.08945v1 Announce Type: new 
Abstract: This study developed a new explainable artificial intelligence algorithm called PassAI, which classifies successful or failed passes in a soccer game and explains its rationale using both tracking and passer's seasonal stats information. This study aimed to address two primary challenges faced by artificial intelligence and machine learning algorithms in the sports domain: how to use different modality data for the analysis and how to explain the rationale of the outcome from multimodal perspectives. To address these challenges, PassAI has two processing streams for multimodal information: tracking image data and passer's stats and classifying pass success and failure. After completing the classification, it provides a rationale by either calculating the relative contribution between the different modality data or providing more detailed contribution factors within the modality. The results of the experiment with 6,349 passes of data obtained from professional soccer games revealed that PassAI showed higher classification performance than state-of-the-art algorithms by &gt;5% and could visualize the rationale of the pass success/failure for both tracking and stats data. These results highlight the importance of using multimodality data in the sports domain to increase the performance of the artificial intelligence algorithm and explainability of the outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08945v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryota Takamido, Jun Ota, Hiroki Nakamoto</dc:creator>
    </item>
    <item>
      <title>StratIncon Detector: Analyzing Strategy Inconsistencies Between Real-Time Strategy and Preferred Professional Strategy in MOBA Esports</title>
      <link>https://arxiv.org/abs/2503.09060</link>
      <description>arXiv:2503.09060v1 Announce Type: new 
Abstract: MOBA (Multiplayer Online Battle Arena) games require a delicate interplay of strategic planning and real-time decision-making, particularly in professional esports, where players exhibit varying levels of skill and strategic insight. While team strategies have been widely studied, analyzing inconsistencies in professional matches remains a significant challenge. The complexity lies in defining and quantifying the difference between real-time and preferred professional strategies, as well as understanding the disparities between them. Establishing direct causal links between specific strategic decisions and game outcomes also demands a comprehensive analysis of the entire match progression. To tackle these challenges, we present the StratIncon Detector, a visual analytics system designed to assist professional players and coaches in efficiently identifying strategic inconsistencies. The system detects real-time strategies, predicts preferred professional strategies, extracts relevant human factors, and uncovers their impact on subsequent game phases. Findings from a case study, a user study with 24 participants, and expert interviews suggest that, compared to traditional methods, the StratIncon Detector enables users to more comprehensively and efficiently identify inconsistencies, infer their causes, evaluate their effects on subsequent game outcomes, and gain deeper insights into team collaboration-ultimately enhancing future teamwork.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09060v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruofei Ma, Yu Zhao, Yuheng Shao, Yunjie Yao, Quan Li</dc:creator>
    </item>
    <item>
      <title>DancingBoard: Streamlining the Creation of Motion Comics to Enhance Narratives</title>
      <link>https://arxiv.org/abs/2503.09061</link>
      <description>arXiv:2503.09061v1 Announce Type: new 
Abstract: Motion comics, a digital animation format that enhances comic book narratives, have wide applications in storytelling, education, and advertising. However, their creation poses significant challenges for amateur creators, primarily due to the need for specialized skills and complex workflows. To address these issues, we conducted an exploratory survey (N=58) to understand the challenges associated with creating motion comics, and an expert interview (N=4) to identify a typical workflow for creation. We further analyzed $95$ online motion comics to gain insights into the design space of character and object actions. Based on our findings, we proposed DancingBoard, an integrated authoring tool designed to simplify the creation process. This tool features a user-friendly interface and a guided workflow, providing comprehensive support throughout each step of the creation process. A user study involving 23 creators showed that, compared to professional tools, DancingBoard is easily comprehensible and provides improved guidance and support, requiring less effort from users. Additionally, a separate study with $18$ audience members confirmed the tool's effectiveness in conveying the story to its viewers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09061v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Longfei Chen, Shengxin Li, Ziang Li, Quan Li</dc:creator>
    </item>
    <item>
      <title>TSConnect: An Enhanced MOOC Platform for Bridging Communication Gaps Between Instructors and Students in Light of the Curse of Knowledge</title>
      <link>https://arxiv.org/abs/2503.09062</link>
      <description>arXiv:2503.09062v1 Announce Type: new 
Abstract: Knowledge dissemination in educational settings is profoundly influenced by the curse of knowledge, a cognitive bias that causes experts to underestimate the challenges faced by learners due to their own in-depth understanding of the subject. This bias can hinder effective knowledge transfer and pedagogical effectiveness, and may be exacerbated by inadequate instructor-student communication. To encourage more effective feedback and promote empathy, we introduce TSConnect, a bias-aware, adaptable interactive MOOC (Massive Open Online Course) learning system, informed by a need-finding survey involving 129 students and 6 instructors. TSConnect integrates instructors, students, and Artificial Intelligence (AI) into a cohesive platform, facilitating diverse and targeted communication channels while addressing previously overlooked information needs. A notable feature is its dynamic knowledge graph, which enhances learning support and fosters a more interconnected educational experience. We conducted a between-subjects user study with 30 students comparing TSConnect to a baseline system. Results indicate that TSConnect significantly encourages students to provide more feedback to instructors. Additionally, interviews with 4 instructors reveal insights into how they interpret and respond to this feedback, potentially leading to improvements in teaching strategies and the development of broader pedagogical skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09062v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qianyu Liu, Xinran Li, Xiaocong Du, Quan Li</dc:creator>
    </item>
    <item>
      <title>Impact of Short-Duration Aerobic Exercise Intensity on Executive Function and Sleep</title>
      <link>https://arxiv.org/abs/2503.09077</link>
      <description>arXiv:2503.09077v1 Announce Type: new 
Abstract: IoT-based devices and wearable sensors are now common in daily life, with smartwatches, smartphones, and other digital tools tracking physical activity and health data. This lifelogging process provides valuable insights into people's lives. This paper analyzes a publicly available lifelog dataset of 14 individuals to explore how exercise affects mood and, in turn, executive function. Results show that moderate physical activity significantly improves mood, reduces stress, and enhances cognitive functions like decision-making and focus. Improved mood not only boosts exercise performance but also strengthens executive function, suggesting exercise benefits both emotional and cognitive well-being. This opens the door for personalized exercise plans tailored to emotional states to optimize brain function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09077v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Peng, Guoqing Zhang, Huadong Pang</dc:creator>
    </item>
    <item>
      <title>The effect of intelligent monitoring of physical exercise on executive function in children with ADHD</title>
      <link>https://arxiv.org/abs/2503.09079</link>
      <description>arXiv:2503.09079v1 Announce Type: new 
Abstract: Children with ADHD often struggle with executive function (EF) and motor skills, impacting their academics and social life. While medications are commonly used, they have side effects, leading to interest in non-drug treatments. Physical activity (PA) has shown promise in improving cognitive and motor skills in children with ADHD. This study examined the short- and long-term effects of three PA interventions: a specific skill training group (EG1), a low-demand exercise group (EG2), and a control group (CG) over 12 weeks. EG1 showed significant improvements in motor tasks and working memory (15\% improvement, p&lt;0.05), while EG2 and CG showed smaller changes. Long-term PA improved working memory, but short-term PA had limited effects on balance and manual dexterity. These findings suggest that skill training has an immediate impact on motor performance, while more complex motor skills require longer interventions. Smart devices tracked progress, confirming sustained engagement and improvement in EG1. This research highlights PA as a promising non-pharmacological treatment for ADHD, warranting further exploration of its effects on other cognitive domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09079v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liwen Lin, Nan Lib, Shuchen Zhao</dc:creator>
    </item>
    <item>
      <title>"I Like Your Story!": A Co-Creative Story-Crafting Game with a Persona-Driven Character Based on Generative AI</title>
      <link>https://arxiv.org/abs/2503.09102</link>
      <description>arXiv:2503.09102v1 Announce Type: new 
Abstract: While generative AI is advancing writing support tools, creative writing is often seen as the exclusive domain of skilled writers. This paper introduces "1001 Nights", a co-creative story-crafting game that transforms writing into a playful and rewarding activity. In this game, the AI agent takes on the role of a "moody" king with distinct storytelling preferences, not merely assisting but actively influencing the narrative. Players engage with the king agent through strategic storytelling, guiding him to mention weapon-related keywords, which materialize as battle equipment. The king agent provides dynamic feedback, expressing satisfaction or displeasure, prompting players to adjust their approach. By combining storytelling, game mechanics, and AI-driven responses, our system motivates creativity through playful constraints. Inspired by Oulipo's literary techniques, this approach demonstrates how AI-powered game experiences can make creative writing more accessible and engaging, encouraging players to explore their creative potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09102v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3721163</arxiv:DOI>
      <dc:creator>Jiaying Fu, Xiruo Wang, Zhouyi Li, Kate Vi, Chuyan Xu, Yuqian Sun</dc:creator>
    </item>
    <item>
      <title>Spiritus: An AI-Assisted Tool for Creating 2D Characters and Animations</title>
      <link>https://arxiv.org/abs/2503.09127</link>
      <description>arXiv:2503.09127v1 Announce Type: new 
Abstract: This research presents Spiritus, an AI-assisted creation tool designed to streamline 2D character animation creation while enhancing creative flexibility. By integrating natural language processing and diffusion models, users can efficiently transform natural language descriptions into personalized 2D characters and animations. The system employs automated segmentation, layered costume techniques, and dynamic mesh-skeleton binding solutions to support flexible adaptation of complex costumes and additional components. Spiritus further achieves real-time animation generation and efficient animation resource reuse between characters through the integration of BVH data and motion diffusion models. Experimental results demonstrate Spiritus's effectiveness in reducing technical barriers, enhancing creative freedom, and supporting resource universality. Future work will focus on optimizing user experience and further exploring the system's human-computer collaboration potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09127v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qirui Sun, Yunyi Ni, Teli Yuan, Jingjing Zhang, Fan Yang, Zhihao Yao, Haipeng Mi</dc:creator>
    </item>
    <item>
      <title>AdaptAI: A Personalized Solution to Sense Your Stress, Fix Your Mess, and Boost Productivity</title>
      <link>https://arxiv.org/abs/2503.09150</link>
      <description>arXiv:2503.09150v1 Announce Type: new 
Abstract: Personalization is a critical yet often overlooked factor in boosting productivity and wellbeing in knowledge-intensive workplaces to better address individual preferences. Existing tools typically offer uniform guidance whether auto-generating email responses or prompting break reminders without accounting for individual behavioral patterns or stress triggers. We introduce AdaptAI, a multimodal AI solution combining egocentric vision and audio, heart and motion activities, and the agentic workflow of Large Language Models LLMs to deliver highly personalized productivity support and context-aware well-being interventions. AdaptAI not only automates peripheral tasks (e.g. drafting succinct document summaries, replying to emails etc.) but also continuously monitors the users unique physiological and situational indicators to dynamically tailor interventions such as micro-break suggestions or exercise prompts, at the exact point of need. In a preliminary study with 15 participants, AdaptAI demonstrated significant improvements in task throughput and user satisfaction by anticipating user stressors and streamlining daily workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09150v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rushiraj Gadhvi, Soham Petkar, Priyansh Desai, Shreyas Ramachandran, Siddharth Siddharth</dc:creator>
    </item>
    <item>
      <title>PromptMap: An Alternative Interaction Style for AI-Based Image Generation</title>
      <link>https://arxiv.org/abs/2503.09436</link>
      <description>arXiv:2503.09436v1 Announce Type: new 
Abstract: Recent technological advances popularized the use of image generation among the general public. Crafting effective prompts can, however, be difficult for novice users. To tackle this challenge, we developed PromptMap, a new interaction style for text-to-image AI that allows users to freely explore a vast collection of synthetic prompts through a map-like view with semantic zoom. PromptMap groups images visually by their semantic similarity, allowing users to discover relevant examples. We evaluated PromptMap in a between-subject online study ($n=60$) and a qualitative within-subject study ($n=12$). We found that PromptMap supported users in crafting prompts by providing them with examples. We also demonstrated the feasibility of using LLMs to create vast example collections. Our work contributes a new interaction style that supports users unfamiliar with prompting in achieving a satisfactory image output.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09436v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3708359.3712150</arxiv:DOI>
      <dc:creator>Krzysztof Adamkiewicz, Pawe{\l} W. Wo\'zniak, Julia Dominiak, Andrzej Romanowski, Jakob Karolus, Stanislav Frolov</dc:creator>
    </item>
    <item>
      <title>Dubito Ergo Sum: Exploring AI Ethics</title>
      <link>https://arxiv.org/abs/2503.06788</link>
      <description>arXiv:2503.06788v1 Announce Type: cross 
Abstract: We paraphrase Descartes' famous dictum in the area of AI ethics where the "I doubt and therefore I am" is suggested as a necessary aspect of morality. Therefore AI, which cannot doubt itself, cannot possess moral agency. Of course, this is not the end of the story. We explore various aspects of the human mind that substantially differ from AI, which includes the sensory grounding of our knowing, the act of understanding, and the significance of being able to doubt ourselves. The foundation of our argument is the discipline of ethics, one of the oldest and largest knowledge projects of human history, yet, we seem only to be beginning to get a grasp of it. After a couple of thousand years of studying the ethics of humans, we (humans) arrived at a point where moral psychology suggests that our moral decisions are intuitive, and all the models from ethics become relevant only when we explain ourselves. This recognition has a major impact on what and how we can do regarding AI ethics. We do not offer a solution, we explore some ideas and leave the problem open, but we hope somewhat better understood than before our study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06788v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.24251/HICSS.2024.671</arxiv:DOI>
      <dc:creator>Viktor Dorfler, Giles Cuthbert</dc:creator>
    </item>
    <item>
      <title>Integrating UX Design in Astronomical Software Development: A Case Study</title>
      <link>https://arxiv.org/abs/2503.08766</link>
      <description>arXiv:2503.08766v1 Announce Type: cross 
Abstract: In 2023, ASTRON took the step of incorporating a dedicated User Experience (UX) designer into its software development process. This decision aimed to enhance the accessibility and usability of services providing access to the data holdings from the telescopes we are developing.
  The field of astronomical software development has historically under emphasized UX design. ASTRON's initiative not only improves our own tools, but can also be used to demonstrate to the broader community the value of integrating UX expertise into development teams.
  We discuss how we integrate the UX designer at the start of our software development lifecycle. We end with providing some considerations on how other projects could make use of UX knowledge in their development process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08766v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan G. Grange (ASTRON), Kevin Tai (ASTRON)</dc:creator>
    </item>
    <item>
      <title>Toward a Corpus Study of the Dynamic Gradual Type</title>
      <link>https://arxiv.org/abs/2503.08928</link>
      <description>arXiv:2503.08928v1 Announce Type: cross 
Abstract: Gradually-typed languages feature a dynamic type that supports implicit coercions, greatly weakening the type system but making types easier to adopt. Understanding how developers use this dynamic type is a critical question for the design of useful and usable type systems. This paper reports on an in-progress corpus study of the dynamic type in Python, targeting 221 GitHub projects that use the mypy type checker. The study reveals eight patterns-of-use for the dynamic type, which have implications for future refinements of the mypy type system and for tool support to encourage precise type annotations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08928v1</guid>
      <category>cs.PL</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dibri Nsofor, Ben Greenman</dc:creator>
    </item>
    <item>
      <title>I Felt Pressured to Give 100% All the Time: How Are Neurodivergent Professionals Being Included in Software Development Teams?</title>
      <link>https://arxiv.org/abs/2503.09001</link>
      <description>arXiv:2503.09001v1 Announce Type: cross 
Abstract: Context: As the demand for digital solutions adapted to different user profiles increases, creating more inclusive and diverse software development teams becomes an important initiative to improve software product accessibility. Problem: However, neurodivergent professionals are underrepresented in this area, encountering obstacles from difficulties in communication and collaboration to inadequate software tools, which directly impact their productivity and well-being. Solution: This study seeks to understand the work experiences of neurodivergent professionals acting in different software development roles. A better understanding of their challenges and strategies to deal with them can collaborate to create more inclusive software development teams. IS Theory: We applied the Sociotechnical Theory (STS) to investigate how the social structures of organizations and their respective work technologies influence the inclusion of these professionals. Method: To address this study, we conducted semi-structured interviews with nine neurodivergent professionals in the Software Engineering field and analyzed the results by applying a continuous comparison coding strategy. Results: The results highlighted issues faced by interviewees, the main ones related to difficulties in communication, social interactions, and prejudice related to their diagnosis. Additionally, excessive in work tools became a significant challenge, leading toconstant distractions and cognitive overload. This scenario negatively impacts their concentration and overall performance. Contributions and Impact in the IS area: As a contribution,this study presents empirically based recommendations to overcome sociotechnical challenges faced by neurodivergent individuals working in software development teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09001v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicoly da Silva Menezes, Thayssa \'Aguila da Rocha, Lucas Samuel Santiago Camelo, Marcelle Pereira Mota</dc:creator>
    </item>
    <item>
      <title>Investigating User Perspectives on Differentially Private Text Privatization</title>
      <link>https://arxiv.org/abs/2503.09338</link>
      <description>arXiv:2503.09338v1 Announce Type: cross 
Abstract: Recent literature has seen a considerable uptick in $\textit{Differentially Private Natural Language Processing}$ (DP NLP). This includes DP text privatization, where potentially sensitive input texts are transformed under DP to achieve privatized output texts that ideally mask sensitive information $\textit{and}$ maintain original semantics. Despite continued work to address the open challenges in DP text privatization, there remains a scarcity of work addressing user perceptions of this technology, a crucial aspect which serves as the final barrier to practical adoption. In this work, we conduct a survey study with 721 laypersons around the globe, investigating how the factors of $\textit{scenario}$, $\textit{data sensitivity}$, $\textit{mechanism type}$, and $\textit{reason for data collection}$ impact user preferences for text privatization. We learn that while all these factors play a role in influencing privacy decisions, users are highly sensitive to the utility and coherence of the private output texts. Our findings highlight the socio-technical factors that must be considered in the study of DP NLP, opening the door to further user-based investigations going forward.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09338v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Meisenbacher, Alexandra Klymenko, Alexander Karpp, Florian Matthes</dc:creator>
    </item>
    <item>
      <title>Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People</title>
      <link>https://arxiv.org/abs/2403.15604</link>
      <description>arXiv:2403.15604v2 Announce Type: replace 
Abstract: "Scene description" applications that describe visual content in a photo are useful daily tools for blind and low vision (BLV) people. Researchers have studied their use, but they have only explored those that leverage remote sighted assistants; little is known about applications that use AI to generate their descriptions. Thus, to investigate their use cases, we conducted a two-week diary study where 16 BLV participants used an AI-powered scene description application we designed. Through their diary entries and follow-up interviews, users shared their information goals and assessments of the visual descriptions they received. We analyzed the entries and found frequent use cases, such as identifying visual features of known objects, and surprising ones, such as avoiding contact with dangerous objects. We also found users scored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for satisfaction and 2.43 out of 4 (SD=1.16) for trust, showing that descriptions still need significant improvements to deliver satisfying and trustworthy experiences. We discuss future opportunities for AI as it becomes a more powerful accessibility tool for BLV users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15604v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642211</arxiv:DOI>
      <dc:creator>Ricardo Gonzalez, Jazmin Collins, Shiri Azenkot, Cynthia Bennett</dc:creator>
    </item>
    <item>
      <title>Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making</title>
      <link>https://arxiv.org/abs/2403.16812</link>
      <description>arXiv:2403.16812v2 Announce Type: replace 
Abstract: In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans' appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16812v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuai Ma, Qiaoyi Chen, Xinru Wang, Chengbo Zheng, Zhenhui Peng, Ming Yin, Xiaojuan Ma</dc:creator>
    </item>
    <item>
      <title>EmBARDiment: an Embodied AI Agent for Productivity in XR</title>
      <link>https://arxiv.org/abs/2408.08158</link>
      <description>arXiv:2408.08158v2 Announce Type: replace 
Abstract: XR devices running chat-bots powered by Large Language Models (LLMs) have the to become always-on agents that enable much better productivity scenarios. Current screen based chat-bots do not take advantage of the the full-suite of natural inputs available in XR, including inward facing sensor data, instead they over-rely on explicit voice or text prompts, sometimes paired with multi-modal data dropped as part of the query. We propose a solution that leverages an attention framework that derives context implicitly from user actions, eye-gaze, and contextual memory within the XR environment. Our work minimizes the need for engineered explicit prompts, fostering grounded and intuitive interactions that glean user insights for the chat-bot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08158v2</guid>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE Virtual Reality Conference 2025</arxiv:journal_reference>
      <dc:creator>Riccardo Bovo, Steven Abreu, Karan Ahuja, Eric J Gonzalez, Li-Te Cheng, Mar Gonzalez-Franco</dc:creator>
    </item>
    <item>
      <title>Understanding How Psychological Distance Influences User Preferences in Conversational Versus Web Search</title>
      <link>https://arxiv.org/abs/2409.19982</link>
      <description>arXiv:2409.19982v2 Announce Type: replace 
Abstract: Conversational search offers an easier and faster alternative to conventional web search, while having downsides like lack of source verification. Research has examined performance disparities between these two systems in different settings. However, little work has considered the effects of variations within a given search task. We hypothesize that psychological distance - one's perceived closeness to a target event - affects information needs in search tasks, and investigate the corresponding effects on user preferences between web and conversational search systems. We find that with greater psychological distances, users perceive conversational search as more credible, useful, enjoyable, and easy to use, and demonstrate increased preference for this system. We reveal qualitative reasons for these differences and provide design implications for search system designers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19982v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yitian Yang, Yugin Tan, Yang Chen Lin, Jung-Tai King, Zihan Liu, Yi-Chieh Lee</dc:creator>
    </item>
    <item>
      <title>AI Conversational Interviewing: Transforming Surveys with LLMs as Adaptive Interviewers</title>
      <link>https://arxiv.org/abs/2410.01824</link>
      <description>arXiv:2410.01824v2 Announce Type: replace 
Abstract: Traditional methods for eliciting people's opinions face a trade-off between depth and scale: structured surveys enable large-scale data collection but limit respondents' ability to voice their opinions in their own words, while conversational interviews provide deeper insights but are resource-intensive. This study explores the potential of replacing human interviewers with large language models (LLMs) to conduct scalable conversational interviews. Our goal is to assess the performance of AI Conversational Interviewing and to identify opportunities for improvement in a controlled environment. We conducted a small-scale, in-depth study with university students who were randomly assigned to a conversational interview by either AI or human interviewers, both employing identical questionnaires on political topics. Various quantitative and qualitative measures assessed interviewer adherence to guidelines, response quality, participant engagement, and overall interview efficacy. The findings indicate the viability of AI Conversational Interviewing in producing quality data comparable to traditional methods, with the added benefit of scalability. We publish our data and materials for re-use and present specific recommendations for effective implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01824v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>LaTeCH-CLfL2025</arxiv:journal_reference>
      <dc:creator>Alexander Wuttke, Matthias A{\ss}enmacher, Christopher Klamm, Max M. Lang, Quirin W\"urschinger, Frauke Kreuter</dc:creator>
    </item>
    <item>
      <title>The Interaction Layer: An Exploration for Co-Designing User-LLM Interactions in Parental Wellbeing Support Systems</title>
      <link>https://arxiv.org/abs/2411.01228</link>
      <description>arXiv:2411.01228v2 Announce Type: replace 
Abstract: Parenting brings emotional and physical challenges, from balancing work, childcare, and finances to coping with exhaustion and limited personal time. Yet, one in three parents never seek support. AI systems potentially offer stigma-free, accessible, and affordable solutions. Yet, user adoption often fails due to issues with explainability and reliability. To see if these issues could be solved using a co-design approach, we developed and tested NurtureBot, a wellbeing support assistant for new parents. 32 parents co-designed the system through Asynchronous Remote Communities method, identifying the key challenge as achieving a "successful chat." As part of co-design, parents role-played as NurtureBot, rewriting its dialogues to improve user understanding, control, and outcomes. The refined prototype, featuring an Interaction Layer, was evaluated by 32 initial and 46 new parents, showing improved user experience and usability, with final CUQ score of 91.3/100, demonstrating successful interaction patterns. Our process revealed useful interaction design lessons for effective AI parenting support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01228v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714088</arxiv:DOI>
      <dc:creator>Sruthi Viswanathan, Seray Ibrahim, Ravi Shankar, Reuben Binns, Max Van Kleek, Petr Slovak</dc:creator>
    </item>
    <item>
      <title>Predicting Workload in Virtual Flight Simulations using EEG Features (Including Post-hoc Analysis in Appendix)</title>
      <link>https://arxiv.org/abs/2412.12428</link>
      <description>arXiv:2412.12428v2 Announce Type: replace 
Abstract: Effective cognitive workload management has a major impact on the safety and performance of pilots. Integrating brain-computer interfaces (BCIs) presents an opportunity for real-time workload assessment. Leveraging cognitive workload data from high-fidelity virtual reality (VR) flight simulations allows for dynamic adjustments to training scenarios. While prior studies have predominantly concentrated on EEG spectral power for workload prediction, delving into intra-brain connectivity may yield deeper insights. This study assessed the predictive value of EEG spectral and connectivity features in distinguishing high vs. low workload periods during simulated flight in VR and Desktop conditions. Using an ensemble approach, a stacked classifier was trained to predict workload from the EEG signals of 52 participants. Results showed that the mean accuracy of the model incorporating both spectral and connectivity features improved by 28% compared to the model that solely relied on spectral features. Further research on other connectivity metrics and deep learning models in a large sample of pilots is essential to validate the potential of a real-time workload-prediction BCI. This could contribute to the development of an adaptive training system for safety-critical operational environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12428v2</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/AIxVR63409.2025.00019</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality (AIxVR), pp. 82-90, Lisbon, Portugal</arxiv:journal_reference>
      <dc:creator>Bas Verkennis, Evy van Weelden, Francesca L. Marogna, Maryam Alimardani, Travis J. Wiltshire, Max M. Louwerse</dc:creator>
    </item>
    <item>
      <title>OptiCarVis: Improving Automated Vehicle Functionality Visualizations Using Bayesian Optimization to Enhance User Experience</title>
      <link>https://arxiv.org/abs/2501.06757</link>
      <description>arXiv:2501.06757v3 Announce Type: replace 
Abstract: Automated vehicle (AV) acceptance relies on their understanding via feedback. While visualizations aim to enhance user understanding of AV's detection, prediction, and planning functionalities, establishing an optimal design is challenging. Traditional "one-size-fits-all" designs might be unsuitable, stemming from resource-intensive empirical evaluations. This paper introduces OptiCarVis, a set of Human-in-the-Loop (HITL) approaches using Multi-Objective Bayesian Optimization (MOBO) to optimize AV feedback visualizations. We compare conditions using eight expert and user-customized designs for a Warm-Start HITL MOBO. An online study (N=117) demonstrates OptiCarVis's efficacy in significantly improving trust, acceptance, perceived safety, and predictability without increasing cognitive load. OptiCarVis facilitates a comprehensive design space exploration, enhancing in-vehicle interfaces for optimal passenger experiences and broader applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06757v3</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713514</arxiv:DOI>
      <dc:creator>Pascal Jansen, Mark Colley, Svenja Krau{\ss}, Daniel Hirschle, Enrico Rukzio</dc:creator>
    </item>
    <item>
      <title>Understanding Children's Avatar Making in Social Online Games</title>
      <link>https://arxiv.org/abs/2502.18705</link>
      <description>arXiv:2502.18705v2 Announce Type: replace 
Abstract: Social online games like Minecraft and Roblox have become increasingly integral to children's daily lives. Our study explores how children aged 8 to 13 create and customize avatars in these virtual environments. Through semi-structured interviews and gameplay observations with 48 participants, we investigate the motivations behind children's avatar-making. Our findings show that children's avatar creation is motivated by self-representation, experimenting with alter ego identities, fulfilling social needs, and improving in-game performance. In addition, designed monetization strategies play a role in shaping children's avatars. We identify the ''wardrobe effect,'' where children create multiple avatars but typically use only one favorite consistently. We discuss the impact of cultural consumerism and how social games can support children's identity exploration while balancing self-expression and social conformity. This work contributes to understanding how avatar shapes children's identity growth in social online games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18705v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yue Fu, Samuel Schwamm, Amanda Baughan, Nicole M Powell, Zoe Kronberg, Alicia Owens, Emily Renee Izenman, Dania Alsabeh, Elizabeth Hunt, Michael Rich, David Bickham, Jenny Radesky, Alexis Hiniker</dc:creator>
    </item>
    <item>
      <title>Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based Telephone Survey System at Scale</title>
      <link>https://arxiv.org/abs/2502.20140</link>
      <description>arXiv:2502.20140v2 Announce Type: replace 
Abstract: Telephone surveys remain a valuable tool for gathering insights but typically require substantial resources in training and coordinating human interviewers. This work presents an AI-driven telephone survey system integrating text-to-speech (TTS), a large language model (LLM), and speech-to-text (STT) that mimics the versatility of human-led interviews (full-duplex dialogues) at scale.
  We tested the system across two populations, a pilot study in the United States (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting participants via web-based links and contacting them via direct phone calls. The AI agent successfully administered open-ended and closed-ended questions, handled basic clarifications, and dynamically navigated branching logic, allowing fast large-scale survey deployment without interviewer recruitment or training.
  Our findings demonstrate that while the AI system's probing for qualitative depth was more limited than human interviewers, overall data quality approached human-led standards for structured items. This study represents one of the first successful large-scale deployments of an LLM-based telephone interviewer in a real-world survey context. The AI-powered telephone survey system has the potential for expanding scalable, consistent data collecting across market research, social science, and public opinion studies, thus improving operational efficiency while maintaining appropriate data quality for research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20140v2</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Max M. Lang, Sol Eskenazi</dc:creator>
    </item>
    <item>
      <title>PinchCatcher: Enabling Multi-selection for Gaze+Pinch</title>
      <link>https://arxiv.org/abs/2503.05456</link>
      <description>arXiv:2503.05456v2 Announce Type: replace 
Abstract: This paper investigates multi-selection in XR interfaces based on eye and hand interaction. We propose enabling multi-selection using different variations of techniques that combine gaze with a semi-pinch gesture, allowing users to select multiple objects, while on the way to a full-pinch. While our exploration is based on the semi-pinch mode for activating a quasi-mode, we explore four methods for confirming subselections in multi-selection mode, varying in effort and complexity: dwell-time (SemiDwell), swipe (SemiSwipe), tilt (SemiTilt), and non-dominant hand input (SemiNDH), and compare them to a baseline technique. In the user study, we evaluate their effectiveness in reducing task completion time, errors, and effort. The results indicate the strengths and weaknesses of each technique, with SemiSwipe and SemiDwell as the most preferred methods by participants. We also demonstrate their utility in file managing and RTS gaming application scenarios. This study provides valuable insights to advance 3D input systems in XR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05456v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713530</arxiv:DOI>
      <dc:creator>Jinwook Kim, Sangmin Park, Qiushi Zhou, Mar Gonzalez-Franco, Jeongmi Lee, Ken Pfeuffer</dc:creator>
    </item>
    <item>
      <title>Facilitating Daily Practice in Intangible Cultural Heritage through Virtual Reality: A Case Study of Traditional Chinese Flower Arrangement</title>
      <link>https://arxiv.org/abs/2503.06122</link>
      <description>arXiv:2503.06122v2 Announce Type: replace 
Abstract: The essence of intangible cultural heritage (ICH) lies in the living knowledge and skills passed down through generations. Daily practice plays a vital role in revitalizing ICH by fostering continuous learning and improvement. However, limited resources and accessibility pose significant challenges to sustaining such practice. Virtual reality (VR) has shown promise in supporting extensive skill training. Unlike technical skill training, ICH daily practice prioritizes cultivating a deeper understanding of cultural meanings and values. This study explores VR's potential in facilitating ICH daily practice through a case study of Traditional Chinese Flower Arrangement (TCFA). By investigating TCFA learners' challenges and expectations, we designed and evaluated FloraJing, a VR system enriched with cultural elements to support sustained TCFA practice. Findings reveal that FloraJing promotes progressive reflection, and continuous enhances technical improvement and cultural understanding. We further propose design implications for VR applications aimed at fostering ICH daily practice in both knowledge and skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06122v2</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713409</arxiv:DOI>
      <dc:creator>Yingna Wang, Qingqin Liu, Xiaoying Wei, Mingming Fan</dc:creator>
    </item>
    <item>
      <title>Multimodal Programming in Computer Science with Interactive Assistance Powered by Large Language Model</title>
      <link>https://arxiv.org/abs/2503.06552</link>
      <description>arXiv:2503.06552v2 Announce Type: replace 
Abstract: LLM chatbot interfaces allow students to get instant, interactive assistance with homework, but doing so carelessly may not advance educational objectives. In this study, an interactive homework help system based on DeepSeek R1 is developed and first implemented for students enrolled in a large computer science beginning programming course. In addition to an assist button in a well-known code editor, our assistant also has a feedback option in our command-line automatic evaluator. It wraps student work in a personalized prompt that advances our educational objectives without offering answers straight away. We have discovered that our assistant can recognize students' conceptual difficulties and provide ideas, plans, and template code in pedagogically appropriate ways. However, among other mistakes, it occasionally incorrectly labels the correct student code as incorrect or encourages students to use correct-but-lesson-inappropriate approaches, which can lead to long and frustrating journeys for the students. After discussing many development and deployment issues, we provide our conclusions and future actions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06552v2</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rajan Das Gupta, Md. Tanzib Hosain, M. F. Mridha, Salah Uddin Ahmed</dc:creator>
    </item>
    <item>
      <title>Creating and Evaluating Privacy and Security Micro-Lessons for Elementary School Children</title>
      <link>https://arxiv.org/abs/2503.07427</link>
      <description>arXiv:2503.07427v2 Announce Type: replace 
Abstract: The growing use of technology in K--8 classrooms highlights a parallel need for formal learning opportunities aimed at helping children use technology safely and protect their personal information. Even the youngest students are now using tablets, laptops, and apps to support their learning; however, there are limited curricular materials available for elementary and middle school children on digital privacy and security topics. To bridge this gap, we developed a series of micro-lessons to help K--8 children learn about digital privacy and security at school. We first conducted a formative study by interviewing elementary school teachers to identify the design needs for digital privacy and security lessons. We then developed micro-lessons -- multiple 15-20 minute activities designed to be easily inserted into the existing curriculum -- using a co-design approach with multiple rounds of developing and revising the micro-lessons in collaboration with teachers. Throughout the process, we conducted evaluation sessions where teachers implemented or reviewed the micro-lessons. Our study identifies strengths, challenges, and teachers' tailoring strategies when incorporating micro-lessons for K--8 digital privacy and security topics, providing design implications for facilitating learning about these topics in school classrooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07427v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lan Gao, Elana B Blinder, Abigail Barnes, Kevin Song, Tamara Clegg, Jessica Vitak, Marshini Chetty</dc:creator>
    </item>
    <item>
      <title>Personality Traits in Large Language Models</title>
      <link>https://arxiv.org/abs/2307.00184</link>
      <description>arXiv:2307.00184v4 Announce Type: replace-cross 
Abstract: The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly powerconversational agents used by the general public world-wide, the synthetic personality traits embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a novel and comprehensive psychometrically valid and reliable methodology for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method to 18 LLMs, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss the application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00184v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Greg Serapio-Garc\'ia, Mustafa Safdari, Cl\'ement Crepy, Luning Sun, Stephen Fitz, Peter Romero, Marwa Abdulhai, Aleksandra Faust, Maja Matari\'c</dc:creator>
    </item>
    <item>
      <title>Behind the Smile: Mental Health Implications of Mother-Infant Interactions Revealed Through Smile Analysis</title>
      <link>https://arxiv.org/abs/2408.01434</link>
      <description>arXiv:2408.01434v2 Announce Type: replace-cross 
Abstract: Mothers of infants have specific demands in fostering emotional bonds with their children, characterized by dynamics that are different from adult-adult interactions, notably requiring heightened maternal emotional regulation. In this study, we analyzed maternal emotional state by modeling maternal emotion regulation reflected in smiles. The dataset comprises N=94 videos of approximately 3 plus or minus 1-minutes, capturing free play interactions between 6 and 12-month-old infants and their mothers. Corresponding demographic details of self-reported maternal mental health provide variables for determining mothers' relations to emotions measured during free play. In this work, we employ diverse methodological approaches to explore the temporal evolution of maternal smiles. Our findings reveal a correlation between the temporal dynamics of mothers' smiles and their emotional state. Furthermore, we identify specific smile features that correlate with maternal emotional state, thereby enabling informed inferences with existing literature on general smile analysis. This study offers insights into emotional labor, defined as the management of one's own emotions for the benefit of others, and emotion regulation entailed in mother-infant interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01434v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACII63134.2024.00010</arxiv:DOI>
      <dc:creator>Adi Dust, Pat Levitt, Maja Matari\'c</dc:creator>
    </item>
    <item>
      <title>3DArticCyclists: Generating Synthetic Articulated 8D Pose-Controllable Cyclist Data for Computer Vision Applications</title>
      <link>https://arxiv.org/abs/2410.10782</link>
      <description>arXiv:2410.10782v2 Announce Type: replace-cross 
Abstract: In Autonomous Driving (AD) Perception, cyclists are considered safety-critical scene objects. Commonly used publicly-available AD datasets typically contain large amounts of car and vehicle object instances but a low number of cyclist instances, usually with limited appearance and pose diversity. This cyclist training data scarcity problem not only limits the generalization of deep-learning perception models for cyclist semantic segmentation, pose estimation, and cyclist crossing intention prediction, but also limits research on new cyclist-related tasks such as fine-grained cyclist pose estimation and spatio-temporal analysis under complex interactions between humans and articulated objects. To address this data scarcity problem, in this paper we propose a framework to generate synthetic dynamic 3D cyclist data assets that can be used to generate training data for different tasks. In our framework, we designed a methodology for creating a new part-based multi-view articulated synthetic 3D bicycle dataset that we call 3DArticBikes that we use to train a 3D Gaussian Splatting (3DGS)-based reconstruction and image rendering method. We then propose a parametric bicycle 3DGS composition model to assemble 8-DoF pose-controllable 3D bicycles. Finally, using dynamic information from cyclist videos, we build a complete synthetic dynamic 3D cyclist (rider pedaling a bicycle) by re-posing a selectable synthetic 3D person, while automatically placing the rider onto one of our new articulated 3D bicycles using a proposed 3D Keypoint optimization-based Inverse Kinematics pose refinement. We present both, qualitative and quantitative results where we compare our generated cyclists against those from a recent stable diffusion-based method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10782v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo R. Corral-Soto, Yang Liu, Tongtong Cao, Yuan Ren, Liu Bingbing</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models generalize analogy solving like people can?</title>
      <link>https://arxiv.org/abs/2411.02348</link>
      <description>arXiv:2411.02348v2 Announce Type: replace-cross 
Abstract: When we solve an analogy we transfer information from a known context to a new one through abstract rules and relational similarity. In people, the ability to solve analogies such as "body : feet :: table : ?" emerges in childhood, and appears to transfer easily to other domains, such as the visual domain "( : ) :: &lt; : ?". Recent research shows that large language models (LLMs) can solve various forms of analogies. However, can LLMs generalize analogy solving to new domains like people can? To investigate this, we had children, adults, and LLMs solve a series of letter-string analogies (e.g., a b : a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek alphabet), and a far transfer domain (list of symbols). As expected, children and adults easily generalized their knowledge to unfamiliar domains, whereas LLMs did not. This key difference between human and AI performance is evidence that these LLMs still struggle with robust human-like analogical transfer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02348v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Claire E. Stevenson, Alexandra Pafford, Han L. J. van der Maas, Melanie Mitchell</dc:creator>
    </item>
    <item>
      <title>Validating LLM-as-a-Judge Systems in the Absence of Gold Labels</title>
      <link>https://arxiv.org/abs/2503.05965</link>
      <description>arXiv:2503.05965v2 Announce Type: replace-cross 
Abstract: The LLM-as-a-judge paradigm, in which a judge LLM system replaces human raters in rating the outputs of other generative AI (GenAI) systems, has come to play a critical role in scaling and standardizing GenAI evaluations. To validate judge systems, evaluators collect multiple human ratings for each item in a validation corpus, and then aggregate the ratings into a single, per-item gold label rating. High agreement rates between these gold labels and judge system ratings are then taken as a sign of good judge system performance. In many cases, however, items or rating criteria may be ambiguous, or there may be principled disagreement among human raters. In such settings, gold labels may not exist for many of the items. In this paper, we introduce a framework for LLM-as-a-judge validation in the absence of gold labels. We present a theoretical analysis drawing connections between different measures of judge system performance under different rating elicitation and aggregation schemes. We also demonstrate empirically that existing validation approaches can select judge systems that are highly suboptimal, performing as much as 34% worse than the systems selected by alternative approaches that we describe. Based on our findings, we provide concrete recommendations for developing more reliable approaches to LLM-as-a-judge validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05965v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Guerdan, Solon Barocas, Kenneth Holstein, Hanna Wallach, Zhiwei Steven Wu, Alexandra Chouldechova</dc:creator>
    </item>
    <item>
      <title>AI-native Memory 2.0: Second Me</title>
      <link>https://arxiv.org/abs/2503.08102</link>
      <description>arXiv:2503.08102v2 Announce Type: replace-cross 
Abstract: Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. A significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating a more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents a critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08102v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiale Wei, Xiang Ying, Tao Gao, Fangyi Bao, Felix Tao, Jingbo Shang</dc:creator>
    </item>
  </channel>
</rss>
