<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Oct 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>MambaCPU: Enhanced Correlation Mining with State Space Models for CPU Performance Prediction</title>
      <link>https://arxiv.org/abs/2410.19297</link>
      <description>arXiv:2410.19297v1 Announce Type: new 
Abstract: CPU performance prediction, which involves forecasting the performance scores of a CPU based on its hardware characteristics during the operation process, is a critical technology for computational system design and resource management. However, this research field currently faces two significant challenges. First, collecting real-world data is challenging due to the wide variety of CPU products on the market and the highly specialized nature of relevant hardware characteristics. Second, existing methods based on hardware simulation models or machine learning exhibit notable shortcomings, such as lengthy simulation test cycles, low prediction accuracy, and the ignoration of characteristic correlations. To bridge these gaps, we first collect, preprocess, and standardize historical data from the 4th Generation Intel Xeon Scalable Processors across multiple benchmark suites to create a new dataset, named PerfCastDB. Subsequently, we design a novel network MambaCPU (MaC) as the baseline for the PerfCastDB dataset. This model leverages the mamba structure to mine the global dependencies and correlations between multiple characteristics. The intra- and inter-group attention mechanisms are subsequently utilized to refine the correlations within and across the characteristic type groups. These techniques enhance the analysis and mining capability of Mac for the complex multivariate correlations. Comparative experiments on the PerfCastDB dataset demonstrate that MaC achieves superior results compared to existing methods, validating its effectiveness. Furthermore, we have open-sourced part of the dataset and the MaC code at \url{https://github.com/xiaoman-liu/MaC} to facilitate the subsequent research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19297v1</guid>
      <category>cs.PF</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoman Liu</dc:creator>
    </item>
    <item>
      <title>Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware Neuron Management</title>
      <link>https://arxiv.org/abs/2410.19274</link>
      <description>arXiv:2410.19274v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success across various domains, yet deploying them on mobile devices remains an arduous challenge due to their extensive computational and memory demands. While lightweight LLMs have been developed to fit mobile environments, they suffer from degraded model accuracy. In contrast, sparsity-based techniques minimize DRAM usage by selectively transferring only relevant neurons to DRAM while retaining the full model in external storage, such as flash. However, such approaches are critically limited by numerous I/O operations, particularly on smartphones with severe IOPS constraints.
  In this paper, we propose Ripple, a novel approach that accelerates LLM inference on smartphones by optimizing neuron placement in flash memory. Ripple leverages the concept of Neuron Co-Activation, where neurons frequently activated together are linked to facilitate continuous read access and optimize data transfer efficiency. Our approach incorporates a two-stage solution: an offline stage that reorganizes neuron placement based on co-activation patterns, and an online stage that employs tailored data access and caching strategies to align well with hardware characteristics. Evaluations conducted on a variety of smartphones and LLMs demonstrate that Ripple achieves up to 5.93x improvements in I/O latency compared to the state-of-the-art. As the first solution to optimize storage placement under sparsity, Ripple explores a new optimization space at the intersection of sparsity-driven algorithm and storage-level system co-design in LLM inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19274v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuowei Wang, Ruwen Fan, Minxing Huang, Zixu Hao, Kun Li, Ting Cao, Youyou Lu, Yaoxue Zhang, Ju Ren</dc:creator>
    </item>
    <item>
      <title>Accelerating AI Performance using Anderson Extrapolation on GPUs</title>
      <link>https://arxiv.org/abs/2410.19460</link>
      <description>arXiv:2410.19460v1 Announce Type: cross 
Abstract: We present a novel approach for accelerating AI performance by leveraging Anderson extrapolation, a vector-to-vector mapping technique based on a window of historical iterations. By identifying the crossover point where a mixing penalty is incurred, the method focuses on reducing iterations to convergence, with fewer more compute-intensive but generally cacheable iterations, balancing speed and memory usage with accuracy and algorithmic stability, respectively. We demonstrate significant improvements, in both training and inference, motivated by scalability and efficiency extensions to the realm of high-performance computing (HPC).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19460v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>math.NA</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Neural Information Processing Systems (NeurIPS). Machine Learning with New Compute Paradigms (MLNCP) Workshop, October 2024</arxiv:journal_reference>
      <dc:creator>Saleem Abdul Fattah Ahmed Al Dajani, David E. Keyes</dc:creator>
    </item>
  </channel>
</rss>
