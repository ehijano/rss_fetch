<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Aug 2024 01:36:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>GPU-Accelerated Counterfactual Regret Minimization</title>
      <link>https://arxiv.org/abs/2408.14778</link>
      <description>arXiv:2408.14778v1 Announce Type: new 
Abstract: Counterfactual regret minimization (CFR) is a family of algorithms of no-regret learning dynamics capable of solving large-scale imperfect information games. There has been a notable lack of work on making CFR more computationally efficient. We propose implementing this algorithm as a series of dense and sparse matrix and vector operations, thereby making it highly parallelizable for a graphical processing unit. Our experiments show that our implementation performs up to about 352.5 times faster than OpenSpiel's Python implementation and up to about 22.2 times faster than OpenSpiel's C++ implementation and the speedup becomes more pronounced as the size of the game being solved grows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14778v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juho Kim</dc:creator>
    </item>
    <item>
      <title>Effective Anonymous Messaging: the Role of Altruism</title>
      <link>https://arxiv.org/abs/2408.14980</link>
      <description>arXiv:2408.14980v1 Announce Type: new 
Abstract: Anonymous messaging and payments have gained momentum recently due to their impact on individuals, society, and the digital landscape. Fuzzy Message Detection (FMD) is a privacy-preserving protocol where an untrusted server performs message anonymously filtering for its clients. To prevent the server from linking the sender and the receiver, the latter can set how much cover traffic they should download along with genuine messages. This could cause unwanted messages to appear on the user's end, thereby creating a need to balance one's bandwidth cost with the desired level of unlinkability. Previous work showed that FMD is not viable with selfish users. In this paper, we model and analyze FMD using the tools of empirical game theory and show that the system needs at least a few altruistic users to operate properly. Utilizing real-world communication datasets, we characterize the emerging equilibria, quantify the impact of different types and levels of altruism, and assess the efficiency of potential outcomes versus socially optimal allocations. Moreover, taking a mechanism design approach, we show how the betweenness centrality (BC) measure can be utilized to achieve the social optimum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14980v1</guid>
      <category>cs.GT</category>
      <category>cs.NI</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcell Frank, Balazs Pejo, Gergely Biczok</dc:creator>
    </item>
    <item>
      <title>Exploiting Approximate Symmetry for Efficient Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2408.15173</link>
      <description>arXiv:2408.15173v1 Announce Type: new 
Abstract: Mean-field games (MFG) have become significant tools for solving large-scale multi-agent reinforcement learning problems under symmetry. However, the assumption of exact symmetry limits the applicability of MFGs, as real-world scenarios often feature inherent heterogeneity. Furthermore, most works on MFG assume access to a known MFG model, which might not be readily available for real-world finite-agent games. In this work, we broaden the applicability of MFGs by providing a methodology to extend any finite-player, possibly asymmetric, game to an "induced MFG". First, we prove that $N$-player dynamic games can be symmetrized and smoothly extended to the infinite-player continuum via explicit Kirszbraun extensions. Next, we propose the notion of $\alpha,\beta$-symmetric games, a new class of dynamic population games that incorporate approximate permutation invariance. For $\alpha,\beta$-symmetric games, we establish explicit approximation bounds, demonstrating that a Nash policy of the induced MFG is an approximate Nash of the $N$-player dynamic game. We show that TD learning converges up to a small bias using trajectories of the $N$-player game with finite-sample guarantees, permitting symmetrized learning without building an explicit MFG model. Finally, for certain games satisfying monotonicity, we prove a sample complexity of $\widetilde{\mathcal{O}}(\varepsilon^{-6})$ for the $N$-agent game to learn an $\varepsilon$-Nash up to symmetrization bias. Our theory is supported by evaluations on MARL benchmarks with thousands of agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15173v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Batuhan Yardim, Niao He</dc:creator>
    </item>
    <item>
      <title>Effect of Adaptation Rate and Cost Display in a Human-AI Interaction Game</title>
      <link>https://arxiv.org/abs/2408.14640</link>
      <description>arXiv:2408.14640v1 Announce Type: cross 
Abstract: As interactions between humans and AI become more prevalent, it is critical to have better predictors of human behavior in these interactions. We investigated how changes in the AI's adaptive algorithm impact behavior predictions in two-player continuous games. In our experiments, the AI adapted its actions using a gradient descent algorithm under different adaptation rates while human participants were provided cost feedback. The cost feedback was provided by one of two types of visual displays: (a) cost at the current joint action vector, or (b) cost in a local neighborhood of the current joint action vector. Our results demonstrate that AI adaptation rate can significantly affect human behavior, having the ability to shift the outcome between two game theoretic equilibrium. We observed that slow adaptation rates shift the outcome towards the Nash equilibrium, while fast rates shift the outcome towards the human-led Stackelberg equilibrium. The addition of localized cost information had the effect of shifting outcomes towards Nash, compared to the outcomes from cost information at only the current joint action vector. Future work will investigate other effects that influence the convergence of gradient descent games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14640v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason T. Isa, Bohan Wu, Qirui Wang, Yilin Zhang, Samuel A. Burden, Lillian J. Ratliff, Benjamin J. Chasnov</dc:creator>
    </item>
    <item>
      <title>On Controlling Knockout Tournaments Without Perfect Information</title>
      <link>https://arxiv.org/abs/2408.15068</link>
      <description>arXiv:2408.15068v1 Announce Type: cross 
Abstract: Over the last decade, extensive research has been conducted on the algorithmic aspects of designing single-elimination (SE) tournaments. Addressing natural questions of algorithmic tractability, we identify key properties of input instances that enable the tournament designer to efficiently schedule the tournament in a way that maximizes the chances of a preferred player winning. Much of the prior algorithmic work on this topic focuses on the perfect (complete and deterministic) information scenario, especially in the context of fixed-parameter algorithm design. Our contributions constitute the first fixed-parameter tractability results applicable to more general settings of SE tournament design with potential imperfect information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15068v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V\'aclav Bla\v{z}ej, M. S. Ramanujan, Peter Strulo, Sushmita Gupta</dc:creator>
    </item>
    <item>
      <title>Tractable Equilibrium Computation in Markov Games through Risk Aversion</title>
      <link>https://arxiv.org/abs/2406.14156</link>
      <description>arXiv:2406.14156v2 Announce Type: replace 
Abstract: A significant roadblock to the development of principled multi-agent reinforcement learning is the fact that desired solution concepts like Nash equilibria may be intractable to compute. To overcome this obstacle, we take inspiration from behavioral economics and show that -- by imbuing agents with important features of human decision-making like risk aversion and bounded rationality -- a class of risk-averse quantal response equilibria (RQE) become tractable to compute in all $n$-player matrix and finite-horizon Markov games. In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degree of risk-aversion and bounded rationality. To validate the richness of this class of solution concepts we show that it captures peoples' patterns of play in a number of 2-player matrix games previously studied in experimental economics. Furthermore, we give a first analysis of the sample complexity of computing these equilibria in finite-horizon Markov games when one has access to a generative model and validate our findings on a simple multi-agent reinforcement learning benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14156v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Eric Mazumdar, Kishan Panaganti, Laixi Shi</dc:creator>
    </item>
    <item>
      <title>Fair Division of Indivisible Chores via Earning Restricted Equilibria</title>
      <link>https://arxiv.org/abs/2407.03318</link>
      <description>arXiv:2407.03318v3 Announce Type: replace 
Abstract: We study fair division of $m$ indivisible chores among $n$ agents with additive preferences. We consider the desirable fairness notions of envy-freeness up to any chore (EFX) and envy-freeness up to $k$ chores (EF$k$), alongside the efficiency notion of Pareto optimality (PO). We present the first constant approximations of these notions, showing the existence of:
  - 4-EFX allocations, which improves the best-known factor of $O(n^2)$-EFX.
  - 2-EF2 and PO allocations, which improves the best-known factor of EF$m$ and PO. In particular, we show the existence of an allocation that is PO and for every agent, either EF2 or 2-EF1.
  - 3-EFX and PO allocations for the special case of bivalued instances, which improves the best-known factor of $O(n)$-EFX without any efficiency guarantees.
  A notable contribution of our work is the introduction of the novel concept of earning-restricted (ER) competitive equilibrium for fractional allocations, which limits agents' earnings from each chore. Technically, our work addresses two main challenges: proving the existence of an ER equilibrium and designing algorithms that leverage ER equilibria to achieve the above results. To tackle the first challenge, we formulate a linear complementarity problem (LCP) formulation that captures all ER equilibria and show that the classic complementary pivot algorithm on the LCP must terminate at an ER equilibrium. For the second challenge, we carefully set the earning limits and use properties of ER equilibria to design sophisticated procedures that involve swapping and merging bundles to meet the desired fairness and efficiency criteria. We expect that the concept of ER equilibrium will be instrumental in deriving further results on related problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03318v3</guid>
      <category>cs.GT</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jugal Garg, Aniket Murhekar, John Qin</dc:creator>
    </item>
    <item>
      <title>Tracking Truth with Liquid Democracy</title>
      <link>https://arxiv.org/abs/2107.11868</link>
      <description>arXiv:2107.11868v3 Announce Type: replace-cross 
Abstract: The dynamics of random transitive delegations on a graph are of particular interest when viewed through the lens of an emerging voting paradigm, liquid democracy. This paradigm allows voters to choose between directly voting and transitively delegating their votes to other voters, so that those selected cast a vote weighted by the number of delegations they received. In the epistemic setting, where voters decide on a binary issue for which there is a ground truth, previous work showed that a few voters may amass such a large amount of influence that liquid democracy is less likely to identify the ground truth than direct voting. We quantify the amount of permissible concentration of power and examine more realistic delegation models, showing they behave well by ensuring that (with high probability) there is a permissible limit on the maximum number of delegations received. Our theoretical results demonstrate that the delegation process is similar to well-known processes on random graphs that are sufficiently bounded for our purposes. Along the way, we prove new bounds on the size of the largest component in an infinite P\'olya urn process, which may be of independent interest. In addition, we empirically validate the theoretical results, running six experiments (for a total of $N=168$ participants, $62$ delegation graphs and over $11k$ votes collected). We find that empirical delegation behaviors meet the conditions for our positive theoretical guarantees. Overall, our work alleviates concerns raised about liquid democracy and bolsters the case for the applicability of this emerging paradigm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.11868v3</guid>
      <category>cs.DM</category>
      <category>cs.GT</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Berinsky, Daniel Halpern, Joseph Y. Halpern, Ali Jadbabaie, Elchanan Mossel, Ariel D. Procaccia, Manon Revel</dc:creator>
    </item>
  </channel>
</rss>
