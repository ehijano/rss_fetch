<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Aug 2025 04:01:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Density Estimation from Aggregated Data with Integrated Auxiliary Information: Estimating Population Densities with Geospatial Data</title>
      <link>https://arxiv.org/abs/2508.03610</link>
      <description>arXiv:2508.03610v1 Announce Type: new 
Abstract: Density estimation for geospatial data ideally relies on precise geocoordinates, typically defined by longitude and latitude. However, such detailed information is often unavailable due to confidentiality constraints. As a result, analysts frequently work with spatially aggregated data, commonly visualized through choropleth maps. Approaches that reverse the aggregation process using measurement error models in the context of kernel density estimation have been proposed in the literature. From a methodological perspective, we extend this line of work by incorporating auxiliary information to improve the precision of density estimates derived from aggregated data. Our approach employs a correlation-based weighting scheme to combine the auxiliary density with the estimate obtained from aggregated data. We evaluate the method through a series of model-based simulation scenarios reflecting varying conditions of auxiliary data quality. From an applied perspective, we demonstrate the utility of our method in two real-world case studies: (1) estimating population densities from the 2022 German Census in Bavaria, using satellite imagery of nighttime light emissions as auxiliary data; and (2) analyzing brown hare hunting bag data in the German state of Lower Saxony. Overall, our results show that integrating auxiliary information into the estimation process leads to more precise density estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03610v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael M\"uhlbauer, Timo Schmid</dc:creator>
    </item>
    <item>
      <title>Optimized imaging prefiltering for enhanced image segmentation</title>
      <link>https://arxiv.org/abs/2508.03653</link>
      <description>arXiv:2508.03653v1 Announce Type: new 
Abstract: The Box-Cox transformation, introduced in 1964, is a widely used statistical tool for stabilizing variance and improving normality in data analysis. Its application in image processing, particularly for image enhancement, has gained increasing attention in recent years. This paper investigates the use of the Box-Cox transformation as a preprocessing step for image segmentation, with a focus on the estimation of the transformation parameter. We evaluate the effectiveness of the transformation by comparing various segmentation methods, highlighting its advantages for traditional machine learning techniques-especially in situations where no training data is available. The results demonstrate that the transformation enhances feature separability and computational efficiency, making it particularly beneficial for models like discriminant analysis. In contrast, deep learning models did not show consistent improvements, underscoring the differing impacts of the transformation across model types and image characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03653v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronny Vallejos, Felipe Osorio, Sebastian Vidal, Grisel Britos</dc:creator>
    </item>
    <item>
      <title>Random Effects Models for Understanding Variability and Association between Brain Functional and Structural Connectivity</title>
      <link>https://arxiv.org/abs/2508.02908</link>
      <description>arXiv:2508.02908v1 Announce Type: cross 
Abstract: The human brain is organized as a complex network, where connections between regions are characterized by both functional connectivity (FC) and structural connectivity (SC). While previous studies have primarily focused on network-level FC-SC correlations (i.e., the correlation between FC and SC across all edges within a predefined network), edge-level correlations (i.e., the correlation between FC and SC across subjects at each edge) has received comparatively little attention. In this study, we systematically analyze both network-level and edge-level FC-SC correlations, demonstrating that they lead to divergent conclusions about the strength of brain function-structure association. To explain these discrepancies, we introduce new random effects models that decompose FC and SC variability into different sources: subject effects, edge effects, and their interactions. Our results reveal that network-level and edge-level FC-SC correlations are influenced by different effects, each contributing differently to the total variability in FC and SC. This modeling framework provides the first statistical approach for disentangling and quantitatively assessing different sources of FC and SC variability and yields new insights into the relationship between functional and structural brain networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02908v1</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingyi Peng, Qiaochu Wang, Yaotian Wang, Jie He, Xu Zou, Shuoran Li, Dana L. Tudorascu, David J. Schaeffer, Lauren Schaeffer, Diego Szczupak, Emily S. Rothwell, Stacey J. Sukoff Rizzo, Gregory W. Carter, Afonso C. Silva, Tingting Zhang</dc:creator>
    </item>
    <item>
      <title>LLM-based IR-system for Bank Supervisors</title>
      <link>https://arxiv.org/abs/2508.02945</link>
      <description>arXiv:2508.02945v1 Announce Type: cross 
Abstract: Bank supervisors face the complex task of ensuring that new measures are consistently aligned with historical precedents. To address this challenge, we introduce a novel Information Retrieval (IR) System tailored to assist supervisors in drafting both consistent and effective measures. This system ingests findings from on-site investigations. It then retrieves the most relevant historical findings and their associated measures from a comprehensive database, providing a solid basis for supervisors to write well-informed measures for new findings. Utilizing a blend of lexical, semantic, and Capital Requirements Regulation (CRR) fuzzy set matching techniques, the IR system ensures the retrieval of findings that closely align with current cases. The performance of this system, particularly in scenarios with partially labeled data, is validated through a Monte Carlo methodology, showcasing its robustness and accuracy. Enhanced by a Transformer-based Denoising AutoEncoder for fine-tuning, the final model achieves a Mean Average Precision (MAP@100) of 0.83 and a Mean Reciprocal Rank (MRR@100) of 0.92. These scores surpass those of both standalone lexical models such as BM25 and semantic BERT-like models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02945v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.knosys.2024.112914</arxiv:DOI>
      <arxiv:journal_reference>Journal-ref: Knowledge-Based Systems 310 (2025) 112914</arxiv:journal_reference>
      <dc:creator>Ilias Aarab</dc:creator>
    </item>
    <item>
      <title>Poisson Inventory Models with Many Items: An Empirical Bayes Approach</title>
      <link>https://arxiv.org/abs/2508.03074</link>
      <description>arXiv:2508.03074v1 Announce Type: cross 
Abstract: We consider inventory decisions with many items, each of which has Poisson demand. The rate of demand for individual items is estimated on the basis of observations of past demand. The problem is to determine the items to hold in stock and the amount of each one. Our setting provides a natural framework for the application of the empirical Bayes methodology. We show how to do this in practice and demonstrate the importance of making posterior estimates of different demand levels, rather than just estimating the Poisson rate. We also address the question of when it is beneficial to separately analyse a group of items which are distinguished in some way. An example occurs when looking at inventory for a book retailer, who may find it advantageous to look separately at certain types of book (e.g. biographies). The empirical Bayes methodology is valuable when dealing with items having Poisson demand, and can be effective even with relatively small numbers of distinct items (e.g. 100). We discuss the best way to apply an empirical Bayes methodology in this context, and also show that doing this in the wrong way will reduce or eliminate the potential benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03074v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Anderson, Nam Ho-Nguyen, Peter Radchenko</dc:creator>
    </item>
    <item>
      <title>Likelihood Matching for Diffusion Models</title>
      <link>https://arxiv.org/abs/2508.03636</link>
      <description>arXiv:2508.03636v1 Announce Type: cross 
Abstract: We propose a Likelihood Matching approach for training diffusion models by first establishing an equivalence between the likelihood of the target data distribution and a likelihood along the sample path of the reverse diffusion. To efficiently compute the reverse sample likelihood, a quasi-likelihood is considered to approximate each reverse transition density by a Gaussian distribution with matched conditional mean and covariance, respectively. The score and Hessian functions for the diffusion generation are estimated by maximizing the quasi-likelihood, ensuring a consistent matching of both the first two transitional moments between every two time points. A stochastic sampler is introduced to facilitate computation that leverages on both the estimated score and Hessian information. We establish consistency of the quasi-maximum likelihood estimation, and provide non-asymptotic convergence guarantees for the proposed sampler, quantifying the rates of the approximation errors due to the score and Hessian estimation, dimensionality, and the number of diffusion steps. Empirical and simulation evaluations demonstrate the effectiveness of the proposed Likelihood Matching and validate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03636v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Qian, Wu Su, Yanqi Huang, Song Xi Chen</dc:creator>
    </item>
    <item>
      <title>A New Approach to Partial Conjunction Analysis in Neuroimaging</title>
      <link>https://arxiv.org/abs/2508.03675</link>
      <description>arXiv:2508.03675v1 Announce Type: cross 
Abstract: The problem of identifying the brain regions activated through a particular cognitive task is pivotal in neuroimaging. This problem becomes even more complex if we have several cognitive tasks or several subjects. In this paper, we view this problem as a partial conjunction (PC) hypotheses testing problem, i.e., we are testing whether a specific brain region is activated in at least $\gamma$ (for some pre-fixed $\gamma$) subjects. We propose the application of a recent advance in the simultaneous statistical inference literature to activation localization in neuroimaging. We apply the recently proposed CoFilter method to neuroimaging data to discover brain regions activated in at least $\gamma$ subjects. Our proposal has two distinct advantages. First, it alleviates the conservativeness displayed by the traditional multiple testing procedures in testing PC hypotheses by eliminating many of the conservative PC $p$-values. Second, it is especially suitable for several high-dimensional studies, each of which examines a large number of null hypotheses. We also compare the performance of our proposal with existing methods for testing PC hypotheses through extensive simulation studies on neuroimaging data and a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03675v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monitirtha Dey, Anna Vesely, Thorsten Dickhaus</dc:creator>
    </item>
    <item>
      <title>Quantifying Grid Resilience Against Extreme Weather Using Large-Scale Customer Power Outage Data</title>
      <link>https://arxiv.org/abs/2109.09711</link>
      <description>arXiv:2109.09711v3 Announce Type: replace 
Abstract: In recent decades, the weather around the world has become more irregular and extreme, often causing large-scale extended power outages. Resilience -- the capability of withstanding, adapting to, and recovering from a large-scale disruption -- has become a top priority for the power sector. However, the understanding of power grid resilience still stays on the conceptual level mostly or focuses on particular components, yielding no actionable results or revealing few insights on the system level. This study provides a quantitatively measurable definition of power grid resilience, using a statistical model inspired by patterns observed from data and domain knowledge. We analyze a large-scale quarter-hourly historical electricity customer outage data and the corresponding weather records, and draw connections between the model and industry resilience practice. We showcase the resilience analysis using three major service territories on the east coast of the United States. Our analysis suggests that cumulative weather effects play a key role in causing immediate, sustained outages, and these outages can propagate and cause secondary outages in neighboring areas. The proposed model also provides some interesting insights into grid resilience enhancement planning. For example, our simulation results indicate that enhancing the power infrastructure in a small number of critical locations can reduce nearly half of the number of customer power outages in Massachusetts. In addition, we have shown that our model achieves promising accuracy in predicting the progress of customer power outages throughout extreme weather events, which can be very valuable for system operators and federal agencies to prepare disaster response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.09711v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shixiang Zhu, Rui Yao, Yao Xie, Feng Qiu, Yueming Qiu, Xuan Wu</dc:creator>
    </item>
    <item>
      <title>Tomographic reconstruction of a disease transmission landscape via GPS recorded random paths</title>
      <link>https://arxiv.org/abs/2404.04455</link>
      <description>arXiv:2404.04455v4 Announce Type: replace 
Abstract: Identifying areas in a landscape where individuals have a higher likelihood of disease infection is key to managing diseases. Unlike conventional methods relying on ecological assumptions, we perform a novel epidemiological tomography for the estimation of landscape propensity to disease infection, using GPS animal tracks in a manner analogous to tomographic techniques in positron emission tomography (PET). Treating tracking data as random Radon transforms, we analyze Cervid movements in a game preserve, paired with antibody levels for epizootic hemorrhagic disease virus (EHDV) -- a vector-borne disease transmitted by biting midges. After discretizing the field and building the regression matrix of the time spent by each deer (row) at each point of the lattice (column), we model the binary response (infected or not) as a binomial linear inverse problem where spatial coherence is enforced with a total variation regularization. The smoothness of the reconstructed propensity map is selected by the quantile universal threshold. To address limitations of small sample sizes and evaluate significance of our estimates, we quantify uncertainty using a bootstrap-based data augmentation procedure. Our method outperforms alternative ones when using simulated and real data. This tomographic framework is novel, with no established statistical methods tailored for such data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04455v4</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jairo Diaz-Rodriguez, Juan Pablo Gomez, Jeremy P. Orange, Nathan D. Burkett-Cadena, Samantha M. Wisely, Jason K. Blackburn, Sylvain Sardy</dc:creator>
    </item>
    <item>
      <title>The Impact of Question Framing on the Performance of Automatic Occupation Coding</title>
      <link>https://arxiv.org/abs/2501.05584</link>
      <description>arXiv:2501.05584v2 Announce Type: replace 
Abstract: Occupational data play a vital role in research, official statistics, and policymaking, yet their collection and accurate classification remain a challenge. This study investigates the effects of occupational question wording on data variability and the performance of automatic coding tools. We conducted and replicated a split-ballot survey experiment in Germany using two common occupational question formats: one focusing on 'job title' (Berufsbezeichnung) and another on 'occupational tasks' (berufliche T\"atigkeit). Our analysis reveals that automatic coding tools, such as CASCOT and OccuCoDe, exhibit sensitivity to the form and origin of the data. Specifically, these tools were more efficient when coding responses to the job title question format compared with the occupational task format, suggesting a potential way to improve the respective questions for many German surveys. In a subsequent 'detailed tasks and duties' question, providing a guiding example prompted respondents to give longer answers without broadening the range of unique words they used. These findings highlight the importance of harmonising survey questions and of ensuring that automatic coding tools are robust to differences in question wording. We emphasise the need for further research to optimise question design and coding tools for greater accuracy and applicability in occupational data collection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05584v2</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Olga Kononykhina, Frauke Kreuter, Malte Schierholz</dc:creator>
    </item>
    <item>
      <title>A Hybrid Mixture of $t$-Factor Analyzers for Clustering High-dimensional Data</title>
      <link>https://arxiv.org/abs/2504.21120</link>
      <description>arXiv:2504.21120v2 Announce Type: replace-cross 
Abstract: This paper develops a novel hybrid approach for estimating the mixture model of $t$-factor analyzers (MtFA) that employs multivariate $t$-distribution and factor model to cluster and characterize grouped data. The traditional estimation method for MtFA faces computational challenges, particularly in high-dimensional settings, where the eigendecomposition of large covariance matrices and the iterative nature of Expectation-Maximization (EM) algorithms lead to scalability issues. We propose a computational scheme that integrates a profile likelihood method into the EM framework to efficiently obtain the model parameter estimates. The effectiveness of our approach is demonstrated through simulations showcasing its superior computational efficiency compared to the existing method, while preserving clustering accuracy and resilience against outliers. Our method is applied to cluster the Gamma-ray bursts, reinforcing several claims in the literature that Gamma-ray bursts have heterogeneous subpopulations and providing characterizations of the estimated groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21120v2</guid>
      <category>stat.ME</category>
      <category>astro-ph.HE</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazeem Kareem, Fan Dai</dc:creator>
    </item>
  </channel>
</rss>
