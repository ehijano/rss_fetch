<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Nov 2025 02:47:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Note on the Parameterised Complexity of Coverability in Vector Addition Systems</title>
      <link>https://arxiv.org/abs/2511.19212</link>
      <description>arXiv:2511.19212v1 Announce Type: new 
Abstract: We investigate the parameterised complexity of the classic coverability problem for vector addition systems (VAS): given a finite set of vectors $V \subseteq\mathbb{Z}^d$, an initial configuration $s\in\mathbb{N}^d$, and a target configuration $t\in\mathbb{N}^d$, decide whether starting from $s$, one can iteratively add vectors from $V$ to ultimately arrive at a configuration that is larger than or equal to $t$ on every coordinate, while not observing any negative value on any coordinate along the way. We consider two natural parameters for the problem: the dimension $d$ and the size of $V$, defined as the total bitsize of its encoding. We present several results charting the complexity of those two parameterisations, among which the highlight is that coverability for VAS parameterised by the dimension and with all the numbers in the input encoded in unary is complete for the class XNL under PL-reductions. We also discuss open problems in the topic, most notably the question about fixed-parameter tractability for the parameterisation by the size of $V$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19212v1</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha{\l} Pilipczuk, Sylvain Schmitz, Henry Sinclair-Banks</dc:creator>
    </item>
    <item>
      <title>Reconstructing Sets of Strings from Their k-way Projections: Algorithms &amp; Complexity</title>
      <link>https://arxiv.org/abs/2511.17707</link>
      <description>arXiv:2511.17707v1 Announce Type: cross 
Abstract: Graphs are a powerful tool for analyzing large data sets, but many real-world phenomena involve interactions that go beyond the simple pairwise relationships captured by a graph. In this paper we introduce and study a simple combinatorial model to capture higher order dependencies from an algorithms and computational complexity perspective. Specifically, we introduce the String Set Reconstruction problem, which asks when a set of strings can be reconstructed from seeing only the k-way projections of strings in the set. This problem is distinguished from genetic reconstruction problems in that we allow projections from any k indices and we maintain knowledge of those indices, but not which k-mer came from which string. We give several results on the complexity of this problem, including hardness results, inapproximability, and parametrized complexity.
  Our main result is the introduction of a new algorithm for this problem using a modified version of overlap graphs from genetic reconstruction algorithms. A key difference we must overcome is that in our setting the k-mers need not be contiguous, unlike the setting of genetic reconstruction. We exhibit our algorithm's efficiency in a variety of experiments, and give high-level explanations for how its complexity is observed to scale with various parameters. We back up these explanation with analytic approximations. We also consider the related problems of: whether a single string can be reconstructed from the k-way projections of a given set of strings, and finding the largest k at which we get no information about the original data set from its k-way projections (i.e., the largest $k$ for which it is "k-wise independent").</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17707v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elise Tate, Joshua A. Grochow</dc:creator>
    </item>
    <item>
      <title>Smoothed Agnostic Learning of Halfspaces over the Hypercube</title>
      <link>https://arxiv.org/abs/2511.17782</link>
      <description>arXiv:2511.17782v1 Announce Type: cross 
Abstract: Agnostic learning of Boolean halfspaces is a fundamental problem in computational learning theory, but it is known to be computationally hard even for weak learning. Recent work [CKKMK24] proposed smoothed analysis as a way to bypass such hardness, but existing frameworks rely on additive Gaussian perturbations, making them unsuitable for discrete domains. We introduce a new smoothed agnostic learning framework for Boolean inputs, where perturbations are modeled via random bit flips. This defines a natural discrete analogue of smoothed optimality generalizing the Gaussian case. Under strictly subexponential assumptions on the input distribution, we give an efficient algorithm for learning halfspaces in this model, with runtime and sample complexity approximately n raised to a poly(1/(sigma * epsilon)) factor. Previously, such algorithms were known only with strong structural assumptions for the discrete hypercube, for example, independent coordinates or symmetric distributions. Our result provides the first computationally efficient guarantee for smoothed agnostic learning of halfspaces over the Boolean hypercube, bridging the gap between worst-case intractability and practical learnability in discrete settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17782v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiwen Kou, Raghu Meka</dc:creator>
    </item>
    <item>
      <title>Quantum Algorithm for Estimating Gibbs Free Energy and Entropy via Energy Derivatives</title>
      <link>https://arxiv.org/abs/2511.17821</link>
      <description>arXiv:2511.17821v1 Announce Type: cross 
Abstract: Estimating vibrational entropy is a significant challenge in thermodynamics and statistical mechanics due to its reliance on quantum mechanical properties. This paper introduces a quantum algorithm designed to estimate vibrational entropy via energy derivatives. Our approach block encodes the exact expression for the second derivative of the energy and uses quantum linear systems algorithms to deal with the reciprocal powers of the gaps that appear in the expression. We further show that if prior knowledge about the values of the second derivative is used then our algorithm can $\epsilon$-approximate the entropy using a number of queries that scales with the condition number $\kappa$, the temperature $T$, error tolerance $\epsilon$ and an analogue of the partition function $\mathcal{Z}$, as $\widetilde{O}\left(\frac{\mathcal{Z}\kappa^2 }{\epsilon T}\right)$. We show that if sufficient prior knowledge is given about the second derivative then the query scales quadratically better than these results. This shows that, under reasonable assumptions of the temperature and a quantum computer can be used to compute the vibrational contributions to the entropy faster than analogous classical algorithms would be capable of. Our findings highlight the potential of quantum algorithms to enhance the prediction of thermodynamic properties, paving the way for advancements in fields such as material science, molecular biology, and chemical engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17821v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shangjie Guo, Corneliu Buda, Nathan Wiebe</dc:creator>
    </item>
    <item>
      <title>Computational complexity of Berry phase estimation in topological phases of matter</title>
      <link>https://arxiv.org/abs/2509.13423</link>
      <description>arXiv:2509.13423v2 Announce Type: replace-cross 
Abstract: The Berry phase is a fundamental quantity in the classification of topological phases of matter. In this paper, we present a new quantum algorithm and several complexity-theoretical results for the Berry phase estimation (BPE) problems. Our new quantum algorithm achieves BPE in a more general setting than previously known quantum algorithms, with a theoretical guarantee. For the complexity-theoretic results, we consider three cases. First, we prove $\mathsf{BQP}$-completeness when we are given a guiding state that has a large overlap with the ground state. This result establishes an exponential quantum speedup for estimating the Berry phase. Second, we prove $\mathsf{dUQMA}$-completeness when we have $\textit{a priori}$ bound for ground state energy. Here, $\mathsf{dUQMA}$ is a variant of the unique witness version of $\mathsf{QMA}$ (i.e., $\mathsf{UQMA}$), which we introduce in this paper, and this class precisely captures the complexity of BPE without the known guiding state. Remarkably, this problem turned out to be the first natural problem contained in both $\mathsf{UQMA}$ and $\mathsf{co}$-$\mathsf{UQMA}$. Third, we show $\mathsf{P}^{\mathsf{dUQMA[log]}}$-hardness and containment in $\mathsf{P}^{\mathsf{PGQMA[log]}}$ when we have no additional assumption. These results advance the role of quantum computing in the study of topological phases of matter and provide a pathway for clarifying the connection between topological phases of matter and computational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13423v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.str-el</category>
      <category>cs.CC</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryu Hayakawa, Kazuki Sakamoto, Chusei Kiumi</dc:creator>
    </item>
  </channel>
</rss>
