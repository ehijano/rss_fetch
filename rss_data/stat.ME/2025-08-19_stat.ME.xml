<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Aug 2025 01:22:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical analysis of multivariate planar curves and applications to X-ray classification</title>
      <link>https://arxiv.org/abs/2508.11780</link>
      <description>arXiv:2508.11780v1 Announce Type: new 
Abstract: Recent developments in computer vision have enabled the availability of segmented images across various domains, such as medicine, where segmented radiography images play an important role in diagnosis-making. As prediction problems are common in medical image analysis, this work explores the use of segmented images (through the associated contours they highlight) as predictors in a supervised classification context. Consequently, we develop a new approach for image analysis that takes into account the shape of objects within images. For this aim, we introduce a new formalism that extends the study of single random planar curves to the joint analysis of multiple planar curves-referred to here as multivariate planar curves. In this framework, we propose a solution to the alignment issue in statistical shape analysis. The obtained multivariate shape variables are then used in functional classification methods through tangent projections. Detection of cardiomegaly in segmented X-rays and numerical experiments on synthetic data demonstrate the appeal and robustness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11780v1</guid>
      <category>stat.ME</category>
      <category>cs.CV</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moindji\'e Issam-Ali, Descary Marie-H\'el\`ene, Beaulac C\'edric</dc:creator>
    </item>
    <item>
      <title>Simulation-based validation of Bayes factor computation</title>
      <link>https://arxiv.org/abs/2508.11814</link>
      <description>arXiv:2508.11814v1 Announce Type: new 
Abstract: We propose and evaluate two methods that validate the computation of Bayes factors: one based on an improved variant of simulation-based calibration checking (SBC) and one based on calibration metrics for binary predictions. We show that in theory, binary prediction calibration is equivalent to a special case of SBC, but with finite resources, binary prediction calibration is typically more sensitive. With well-designed test quantities, SBC can however detect all possible problems in computation, including some that cannot be uncovered by binary prediction calibration.
  Previous work on Bayes factor validation includes checks based on the data-averaged posterior and the Good check method. We demonstrate that both checks miss many problems in Bayes factor computation detectable with SBC and binary prediction calibration. Moreover, we find that the Good check as originally described fails to control its error rates. Our proposed checks also typically use simulation results more efficiently than data-averaged posterior checks. Finally, we show that a special approach based on posterior SBC is necessary when checking Bayes factor computation under improper priors and we validate several models with such priors.
  We recommend that novel methods for Bayes factor computation be validated with SBC and binary prediction calibration with at least several hundred simulations. For all the models we tested, the bridgesampling and BayesFactor R packages satisfy all available checks and thus are likely safe to use in standard scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11814v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Martin Modr\'ak, Sebastian Stroppel, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Post-selection inference with a single realization of a network</title>
      <link>https://arxiv.org/abs/2508.11843</link>
      <description>arXiv:2508.11843v1 Announce Type: new 
Abstract: Given a dataset consisting of a single realization of a network, we consider conducting inference on a parameter selected from the data. In particular, we focus on the setting where the parameter of interest is a linear combination of the mean connectivities within and between estimated communities. Inference in this setting poses a challenge, since the communities are themselves estimated from the data. Furthermore, since only a single realization of the network is available, sample splitting is not possible. In this paper, we show that it is possible to split a single realization of a network consisting of $n$ nodes into two (or more) networks involving the same $n$ nodes; the first network can be used to select a data-driven parameter, and the second to conduct inference on that parameter. In the case of weighted networks with Poisson or Gaussian edges, we obtain two independent realizations of the network; by contrast, in the case of Bernoulli edges, the two realizations are dependent, and so extra care is required. We establish the theoretical properties of our estimators, in the sense of confidence intervals that attain the nominal (selective) coverage, and demonstrate their utility in numerical simulations and in application to a dataset representing the relationships among dolphins in Doubtful Sound, New Zealand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11843v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ethan Ancell, Daniela Witten, Daniel Kessler</dc:creator>
    </item>
    <item>
      <title>Likelihood confidence intervals for misspecified Cox models</title>
      <link>https://arxiv.org/abs/2508.11851</link>
      <description>arXiv:2508.11851v1 Announce Type: new 
Abstract: The robust Wald confidence interval (CI) for the Cox model is commonly used when the model may be misspecified or when weights are applied. However it can perform poorly when there are few events in one or both treatment groups, as may occur when the event of interest is rare or when the experimental arm is highly efficacious. For instance, if we artificially remove events (assuming more events are unfavorable) from the experimental group, the resulting upper CI may increase. This is clearly counter-intuitive as a small number of events in the experimental arm represents stronger evidence for efficacy. It is well known that, when the sample size is small to moderate, likelihood CIs are better than Wald CIs in terms of actual coverage probabilities closely matching nominal levels. However, a robust version of the likelihood CI for the Cox model remains an open problem. For example, in the SAS procedure PHREG, the likelihood CI provided in the outputs is still the regular version, even when the robust option is specified. This is obviously undesirable as a user may mistakenly assume that the CI is the robust version. In this article we demonstrate that the likelihood ratio test statistic of the Cox model converges to a weighted chi-square distribution when the model is misspecified. The robust likelihood CI is then obtained by inverting the robust likelihood ratio test. The proposed CIs are evaluated through simulation studies and illustrated using real data from an HIV prevention trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11851v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yongwu Shao, Xu Guo</dc:creator>
    </item>
    <item>
      <title>A novel approach to generate distributions</title>
      <link>https://arxiv.org/abs/2508.11861</link>
      <description>arXiv:2508.11861v1 Announce Type: new 
Abstract: A novel approach to adding two additional parameters to a family of distributions for better adaptability has been put forth. This approach yields a versatile class of distributions supported on the positive real line. We proceed to analyze its mathematical characteristics, such as critical points, modality, stochastic representation, identifiability, quantiles, moments, and truncated moments. We present a new regression model for unimodal continuous data based on a submodel of the newly proposed family of distributions, in which the distribution of the response variable is reparameterized in terms of the median. We use the maximum likelihood method to estimate the parameters, which was implemented through the gamlss package in R. The proposed regression model was applied to a real dataset, and its adequacy was validated through quantile residual analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11861v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhankar Dutta, Roberto Vila, Terezinha K. A. Ribeiro</dc:creator>
    </item>
    <item>
      <title>Penalized Spline M-Estimators for Discretely Sampled Functional Data: Existence and Asymptotics</title>
      <link>https://arxiv.org/abs/2508.12000</link>
      <description>arXiv:2508.12000v1 Announce Type: new 
Abstract: Location estimation is a central problem in functional data analysis. In this paper, we investigate penalized spline estimators of location for discretely sampled functional data under a broad class of convex loss functions. Our framework generalizes and extends previously derived results for non-robust estimators to a broad penalized M-estimation framework. The analysis is built on two general-purpose non-asymptotic theoretical tools: (i) a non-asymptotic existence result for penalized spline estimators under minimal design conditions, and (ii) a localization lemma that captures both stochastic variability and approximation error. Under mild assumptions, we establish optimal convergence rates and identify the small- and large-knot regimes, along with the critical breakpoint known from penalized spline theory in nonparametric regression. Our results imply that parametric rates are attainable even with discretely sampled data and numerical experiments demonstrate that our estimators match or exceed the robustness of competing estimators while being considerably more computationally attractive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12000v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ioannis Kalogridis</dc:creator>
    </item>
    <item>
      <title>Homogeneity Test of Proportions for Combined Unilateral and Bilateral Data via GEE and MLE Approaches</title>
      <link>https://arxiv.org/abs/2508.12008</link>
      <description>arXiv:2508.12008v1 Announce Type: new 
Abstract: In clinical trials involving paired organs such as eyes, ears, and kidneys, binary outcomes may be collected bilaterally or unilaterally. In such combined datasets, bilateral outcomes exhibit intra-subject correlation, while unilateral outcomes are assumed independent. We investigate the generalized Estimating Equations (GEE) approach for testing homogeneity of proportions across multiple groups for the combined unilateral and bilateral data, and compare it with three likelihood-based statistics (likelihood ratio, Wald-type, and score) under Rosner's constant $R$ model and Donner's equal correlation $\rho$ model. Monte Carlo simulations evaluate empirical type I error and power under varied sample sizes and parameter settings. The GEE and score tests show superior type I error control, outperforming likelihood ratio and Wald-type tests. Applications to two real datasets in otolaryngologic and ophthalmologic studies illustrate the methods. We recommend the GEE and score tests for homogeneity testing, and suggest GEE for more complex models with covariates, while favoring the score statistic for small sample exact tests due to its computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12008v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jia Zhou, Chang-Xing Ma</dc:creator>
    </item>
    <item>
      <title>A Severity-Aware Reliability Index for Risk-Informed Structural Design</title>
      <link>https://arxiv.org/abs/2508.12068</link>
      <description>arXiv:2508.12068v1 Announce Type: new 
Abstract: Classical measures of structural reliability, such as the probability of failure and the related reliability index, are still widely applied in practice. However, these measures are frequency-based only, and they do not give information about the severity of failure once it happens. This missing aspect can cause underestimation of risks, in particular when rare events produce very undesirable consequences. In this paper, a new reliability framework is proposed to address this issue. The framework is based on a new concept, called the Expected Failure Deficit (EFD), which is defined as the average deficiency of the system response when failure occurs. From this quantity, a new reliability index is introduced, called the Severity-Aware Reliability Index, which evaluates the consequence of failure in comparison with the Gaussian benchmark. The mathematical formulation is derived and it is shown that the inverse mapping exists in a restricted domain, which can be interpreted as an indicator of excessive tail risks. A Severity Classification System with five levels is also proposed and calibrated from analytical expressions. Numerical examples, including Gaussian, mildly nonlinear, and heavy-tailed cases, demonstrate that the proposed framework agrees with classical measures in standard situations, while being able to detect hidden severity in more complex cases. The method can therefore be used not only to quantify severity of failure, but also to classify risks in support of engineering design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12068v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moussa Leblouba, Samer Barakat, Raghad Awad</dc:creator>
    </item>
    <item>
      <title>Unified Conformalized Multiple Testing with Full Data Efficiency</title>
      <link>https://arxiv.org/abs/2508.12085</link>
      <description>arXiv:2508.12085v1 Announce Type: new 
Abstract: Conformalized multiple testing offers a model-free way to control predictive uncertainty in decision-making. Existing methods typically use only part of the available data to build score functions tailored to specific settings. We propose a unified framework that puts data utilization at the center: it uses all available data-null, alternative, and unlabeled-to construct scores and calibrate p-values through a full permutation strategy. This unified use of all available data significantly improves power by enhancing non-conformity score quality and maximizing calibration set size while rigorously controlling the false discovery rate. Crucially, our framework provides a systematic design principle for conformal testing and enables automatic selection of the best conformal procedure among candidates without extra data splitting. Extensive numerical experiments demonstrate that our enhanced methods deliver superior efficiency and adaptability across diverse scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12085v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuyang Huo, Xiaoyang Wu, Changliang Zou, Haojie Ren</dc:creator>
    </item>
    <item>
      <title>A Systematic Particle Filter for Estimating Time-Varying Parameters in Advection-Diffusion Equations with Source Terms</title>
      <link>https://arxiv.org/abs/2508.12155</link>
      <description>arXiv:2508.12155v1 Announce Type: new 
Abstract: Many real-world systems modeled using partial differential equations (PDEs) involve unknown parameters that must be estimated from limited, noisy system observations. While typically assumed to be constants, some of these unobserved parameters may vary with time. This work proposes a two-phase, offline-online numerical procedure for systematically estimating and quantifying uncertainty in time-varying parameters (TVPs) in time-dependent PDEs, specifically focusing on advection-diffusion models with TVPs involved in the source terms. Numerical results on a set of one-dimensional test problems demonstrate the effectiveness of the proposed estimation procedure in tracking unknown TVPs of different forms, while simultaneously estimating variance parameters affecting the TVP evolution model, from partial, noisy observations of the solution at discrete spatial locations and times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12155v1</guid>
      <category>stat.ME</category>
      <category>math.DS</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Arnold</dc:creator>
    </item>
    <item>
      <title>A unified method for generating closed-form point estimators for exponential families: An example with the beta distribution applied to proportions of land used for farming</title>
      <link>https://arxiv.org/abs/2508.12169</link>
      <description>arXiv:2508.12169v1 Announce Type: new 
Abstract: We show that, after a simple power-transform reparameterization of the (vector) exponential family, the solutions to the likelihood equations coincide with moment-type estimating equations. This equivalence enables a unified route to closed-form point estimators for multi-parameter models that typically lack explicit maximum likelihood (ML) solutions. Within this framework we (i) recover, as special cases, several recent closed-form estimators from the literature; (ii) derive new families of estimators indexed by monotone transformations $g$; and (iii) establish strong consistency and asymptotic normality under mild regularity, including a dominated differentiation condition. As a detailed illustration, we derive closed-form estimators for parameters that index the beta distribution. A Monte Carlo simulation study is carried out to evaluate and compare the performance of proposed and existing estimators. Finally, we illustrate the approach with a novel municipal data set regarding proportions of land used for farming in Roraima (Brazil), which, to the best of our knowledge, has not been analyzed in the literature before, demonstrating the method's practical usefulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12169v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>Model positive and unlabeled data with a generalized additive density ratio model</title>
      <link>https://arxiv.org/abs/2508.12446</link>
      <description>arXiv:2508.12446v1 Announce Type: new 
Abstract: We address learning from positive and unlabeled (PU) data, a common setting in which only some positives are labeled and the rest are mixed with negatives. Classical exponential tilting models guarantee identifiability by assuming a linear structure, but they can be badly misspecified when relationships are nonlinear. We propose a generalized additive density-ratio framework that retains identifiability while allowing smooth, feature-specific effects. The approach comes with a practical fitting algorithm and supporting theory that enables estimation and inference for the mixture proportion and other quantities of interest. In simulations and analyses of benchmark datasets, the proposed method matches the standard exponential tilting method when the linear model is correct and delivers clear gains when it is not. Overall, the framework strikes a useful balance between flexibility and interpretability for PU learning and provides principled tools for estimation, prediction, and uncertainty assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12446v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peijun Sang, Yifan Sun, Qinglong Tian, Donglin Zeng, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>Simultaneous estimation of connectivity and dimensionality in samples of networks</title>
      <link>https://arxiv.org/abs/2508.12483</link>
      <description>arXiv:2508.12483v1 Announce Type: new 
Abstract: An overarching objective in contemporary statistical network analysis is extracting salient information from datasets consisting of multiple networks. To date, considerable attention has been devoted to node and network clustering, while comparatively less attention has been devoted to downstream connectivity estimation and parsimonious embedding dimension selection. Given a sample of potentially heterogeneous networks, this paper proposes a method to simultaneously estimate a latent matrix of connectivity probabilities and its embedding dimensionality or rank after first pre-estimating the number of communities and the node community memberships. The method is formulated as a convex optimization problem and solved using an alternating direction method of multipliers algorithm. We establish estimation error bounds under the Frobenius norm and nuclear norm for settings in which observable networks have blockmodel structure, even when node memberships are imperfectly recovered. When perfect membership recovery is possible and dimensionality is much smaller than the number of communities, the proposed method outperforms conventional averaging-based methods for estimating connectivity and dimensionality. Numerical studies empirically demonstrate the accuracy of our method across various scenarios. Additionally, analysis of a primate brain dataset demonstrates that posited connectivity is not necessarily full rank in practice, illustrating the need for flexible methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12483v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenlong Jiang, Chris McKennan, Jes\'us Arroyo, Joshua Cape</dc:creator>
    </item>
    <item>
      <title>A Generalized Genetic Random Field Method for the Genetic Association Analysis of Sequencing Data</title>
      <link>https://arxiv.org/abs/2508.12617</link>
      <description>arXiv:2508.12617v1 Announce Type: new 
Abstract: With the advance of high-throughput sequencing technologies, it has become feasible to investigate the influence of the entire spectrum of sequencing variations on complex human diseases. Although association studies utilizing the new sequencing technologies hold great promise to unravel novel genetic variants, especially rare genetic variants that contribute to human diseases, the statistical analysis of high-dimensional sequencing data remains a challenge. Advanced analytical methods are in great need to facilitate high-dimensional sequencing data analyses. In this article, we propose a generalized genetic random field (GGRF) method for association analyses of sequencing data. Like other similarity-based methods (e.g., SIMreg and SKAT), the new method has the advantages of avoiding the need to specify thresholds for rare variants and allowing for testing multiple variants acting in different directions and magnitude of effects. The method is built on the generalized estimating equation framework and thus accommodates a variety of disease phenotypes (e.g., quantitative and binary phenotypes). Moreover, it has a nice asymptotic property, and can be applied to small-scale sequencing data without need for small-sample adjustment. Through simulations, we demonstrate that the proposed GGRF attains an improved or comparable power over a commonly used method, SKAT, under various disease scenarios, especially when rare variants play a significant role in disease etiology. We further illustrate GGRF with an application to a real dataset from the Dallas Heart Study. By using GGRF, we were able to detect the association of two candidate genes, ANGPTL3 and ANGPTL4, with serum triglyceride.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12617v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/gepi.21790</arxiv:DOI>
      <dc:creator>Ming Li, Zihuai He, Min Zhang, Xiaowei Zhan, Changshuai Wei, Robert C Elston, Qing Lu</dc:creator>
    </item>
    <item>
      <title>A self-supervised learning approach for denoising autoregressive models with additive noise: finite and infinite variance cases</title>
      <link>https://arxiv.org/abs/2508.12970</link>
      <description>arXiv:2508.12970v1 Announce Type: new 
Abstract: The autoregressive time series model is a popular second-order stationary process, modeling a wide range of real phenomena. However, in applications, autoregressive signals are often corrupted by additive noise. Further, the autoregressive process and the corruptive noise may be highly impulsive, stemming from an infinite-variance distribution. The model estimation techniques that account for additional noise tend to show reduced efficacy when there is very strong noise present in the data, especially when the noise is heavy-tailed. Moreover, identification of a model corrupted with heavy-tailed, particularly infinite-variance noise, can be a very challenging task. In this paper, we propose a novel self-supervised learning method to denoise the additive noise-corrupted autoregressive model. Our approach is motivated by recent work in computer vision and does not require full knowledge of the noise distribution. We use the proposed method to recover exemplary finite- and infinite-variance autoregressive signals, namely, Gaussian- and alpha-stable distributed signals, respectively, from their noise-corrupted versions. The simulation study conducted on both synthetic and semi-synthetic data demonstrates the efficiency of our method compared to several baseline methods, particularly when the corruption is significant and impulsive in nature. Finally, we apply the presented methodology to forecast the pure autoregressive signal from the noise-corrupted data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12970v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sayantan Banerjee, Agnieszka Wylomanska, Sundar S</dc:creator>
    </item>
    <item>
      <title>Dynamic Latent Class Structural Equation Modeling: A Hands-On Tutorial for Modeling Intensive Longitudinal Data</title>
      <link>https://arxiv.org/abs/2508.12983</link>
      <description>arXiv:2508.12983v1 Announce Type: new 
Abstract: In this tutorial, we provide a hands-on guideline on how to implement complex Dynamic Latent Class Structural Equation Models (DLCSEM) in the Bayesian software JAGS. We provide building blocks starting with simple Confirmatory Factor and Time Series analysis, and then extend these blocks to Multilevel Models and Dynamic Structural Equation Models (DSEM). Leading through the tutorial is an example from clinical psychology using data on a generalized anxiety treatment that includes scales on anxiety symptoms and the Working Alliance Inventory that measures alliance between therapists and patients. Within each block, we provide an overview, specific hypotheses, we want to test, the resulting model and its implementation as well as an interpretation of the results. The aim of this tutorial is to provide a step-by-step guide for applied researchers that enables them use this flexible DLCSEM framework for their own analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12983v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Faleh, Sofia Morelli, Vivato Andriamiarana, Zachary J. Roman, Christoph Fl\"uckiger, Holger Brandt</dc:creator>
    </item>
    <item>
      <title>An Improved Solution to the Two Normal Means Problem via Regularization</title>
      <link>https://arxiv.org/abs/2508.13012</link>
      <description>arXiv:2508.13012v1 Announce Type: new 
Abstract: The many-normal-means problem is a classic example that motivates the development of many important inferential procedures in the history of statistics. In this short note, we consider a further special case of the problem, which involves only two normally distributed data points with a constraint that the pair of means are not too far apart from one another. Starting with a regularized ML estimator, we construct a novel possibilistic IM for marginal inference on one of the two means. Not only does the new IM remain valid, it is also more efficient than the standard marginal inference ignoring the a priori information about the closeness of means, as well as the partial conditioning IM solution recently proposed in Yang et al. (2023).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13012v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Jonathan P. Williams</dc:creator>
    </item>
    <item>
      <title>Surrogate-based Bayesian calibration methods for climate models: a comparison of traditional and non-traditional approaches</title>
      <link>https://arxiv.org/abs/2508.13071</link>
      <description>arXiv:2508.13071v1 Announce Type: new 
Abstract: Parameter calibration is crucial for reducing uncertainty and improving simulation accuracy in physics-based models, yet computational constraints pose significant challenges. Bayesian calibration methods offer a principled framework for combining prior knowledge with data while rigorously quantifying uncertainty. In this work, we compare four emulator-based Bayesian calibration methods: Calibrate-Emulate-Sample (CES), History Matching (HM), Bayesian Optimal Experimental Design (BOED), and a novel Goal-Oriented BOED (GBOED) approach, using the Lorenz '96 multiscale system as a testbed. Our GBOED formulation explicitly targets calibration-relevant quantities and leverages information-theoretic criteria for data selection. We assess each method in terms of calibration accuracy, uncertainty quantification, computational cost, and convergence behavior. We evaluate each method's performance in balancing computational cost, implementation complexity, and uncertainty quantification (UQ), with additional insights into convergence behavior as model evaluations increase. We find CES offers excellent performance but at high computational expense, while GBOED achieves comparable accuracy using fewer model evaluations. Standard BOED underperforms with respect to calibration accuracy, and HM shows moderate effectiveness but can be useful as a precursor. Our results highlight trade-offs among Bayesian strategies and demonstrate the promise of goal-oriented design in calibration workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13071v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maike F. Holthuijzen, Atlanta Chakraborty, Elizabeth Krath, Tommie Catanach</dc:creator>
    </item>
    <item>
      <title>A note on simulation methods for the Dirichlet-Laplace prior</title>
      <link>https://arxiv.org/abs/2508.11982</link>
      <description>arXiv:2508.11982v1 Announce Type: cross 
Abstract: Bhattacharya et al. (2015, Journal of the American Statistical Association 110(512): 1479-1490) introduce a novel prior, the Dirichlet-Laplace (DL) prior, and propose a Markov chain Monte Carlo (MCMC) method to simulate posterior draws under this prior in a conditionally Gaussian setting. The original algorithm samples from conditional distributions in the wrong order, i.e., it does not correctly sample from the joint posterior distribution of all latent variables. This note details the issue and provides two simple solutions: A correction to the original algorithm and a new algorithm based on an alternative, yet equivalent, formulation of the prior. This corrigendum does not affect the theoretical results in Bhattacharya et al. (2015).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11982v1</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2540256</arxiv:DOI>
      <arxiv:journal_reference>Correction. (2025). Journal of the American Statistical Association</arxiv:journal_reference>
      <dc:creator>Luis Gruber, Gregor Kastner, Anirban Bhattacharya, Debdeep Pati, Natesh Pillai, David Dunson</dc:creator>
    </item>
    <item>
      <title>Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset</title>
      <link>https://arxiv.org/abs/2508.12253</link>
      <description>arXiv:2508.12253v1 Announce Type: cross 
Abstract: Time-series forecasting underpins critical decisions across aviation, energy, retail and health. Classical autoregressive integrated moving average (ARIMA) models offer interpretability via coefficients but struggle with nonlinearities, whereas tree-based machine-learning models such as XGBoost deliver high accuracy but are often opaque. This paper presents a unified framework for interpreting time-series forecasts using local interpretable model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We convert a univariate series into a leakage-free supervised learning problem, train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc explainability. Using the Air Passengers dataset as a case study, we show that a small set of lagged features -- particularly the twelve-month lag -- and seasonal encodings explain most forecast variance. We contribute: (i) a methodology for applying LIME and SHAP to time series without violating chronology; (ii) theoretical exposition of the underlying algorithms; (iii) empirical evaluation with extensive analysis; and (iv) guidelines for practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12253v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manish Shukla</dc:creator>
    </item>
    <item>
      <title>Asymptotic confidence bands for the histogram regression estimator</title>
      <link>https://arxiv.org/abs/2508.12391</link>
      <description>arXiv:2508.12391v1 Announce Type: cross 
Abstract: In a multivariate nonparametric regression model with a H\"older continuous regression function and heteroscedastic noise asymptotic uniform confidence bands are constructed based on the histogram estimator. The radius of the confidence bands does not depend on an extreme value distribution, but instead can be numerically calculated for the chosen binning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12391v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalie Neumeyer, Jan Rabe, Mathias Trabs</dc:creator>
    </item>
    <item>
      <title>A statistician's guide to weak-instrument-robust inference in instrumental variables regression with illustrations in Python</title>
      <link>https://arxiv.org/abs/2508.12474</link>
      <description>arXiv:2508.12474v1 Announce Type: cross 
Abstract: We provide an overview of results relating to estimation and weak-instrument-robust inference in instrumental variables regression. Methods are implemented in the ivmodels software package for Python, which we use to illustrate results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12474v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malte Londschien</dc:creator>
    </item>
    <item>
      <title>An Iterative Bayesian Robbins--Monro Sequence</title>
      <link>https://arxiv.org/abs/2508.12478</link>
      <description>arXiv:2508.12478v1 Announce Type: cross 
Abstract: This study introduces an iterative Bayesian Robbins--Monro (IBRM) sequence, which unites the classical Robbins--Monro sequence with statistical estimation for faster root-finding under noisy observations. Although the standard Robbins--Monro method iteratively approaches solutions, its convergence speed is limited by noisy measurements and naivety to any prior information about the objective function. The proposed Bayesian sequence dynamically updates the prior distribution with newly obtained observations to accelerate convergence rates and robustness. The paper demonstrates almost sure convergence of the sequence and analyses its convergence rates for both one-dimensional and multi-dimensional problems. We evaluate the method in a practical application that suffers from large variability and allows only a few function evaluations, specifically estimating thresholds in noninvasive brain stimulation, where the method is more robust and accurate than conventional alternatives. Simulations involving 25,000 virtual subjects illustrate reduced error margins and decreased outlier frequency with direct impact on clinical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12478v1</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siwei Liu, Ke Ma, Stephan M. Goetz</dc:creator>
    </item>
    <item>
      <title>Sparsity of the Main Effect Matrix Factor Model</title>
      <link>https://arxiv.org/abs/2508.12510</link>
      <description>arXiv:2508.12510v1 Announce Type: cross 
Abstract: We introduce sparsity detection and estimation in main effect matrix factor models for matrix-valued time series. A carefully chosen set of identification conditions for the common component and the potentially nonstationary main effects is proposed to strengthen the interpretations of sparse main effects, while estimators of all model components are presented. Sparse estimation of the latent main effects is proposed using a doubly adaptive fused lasso estimation to allow for sparse sub-block detection, with theoretical guarantees and rates of convergence spelt out for the final estimators. Sparse block consistency for the main effects is also proved as a result. A realized Mallow's $C_p$ is developed for tuning parameter selection, with practical implementation described. Simulation experiments are performed under a variety of settings, showing our proposed estimators work well. A set of NYC taxi traffic data is analyzed, clearly showing the effects of Covid-19 lockdown, with prolonged sparse main effects detected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12510v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Kaixin Liu, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>An Introduction to Sliced Optimal Transport</title>
      <link>https://arxiv.org/abs/2508.12519</link>
      <description>arXiv:2508.12519v1 Announce Type: cross 
Abstract: Sliced Optimal Transport (SOT) is a rapidly developing branch of optimal transport (OT) that exploits the tractability of one-dimensional OT problems. By combining tools from OT, integral geometry, and computational statistics, SOT enables fast and scalable computation of distances, barycenters, and kernels for probability measures, while retaining rich geometric structure. This paper provides a comprehensive review of SOT, covering its mathematical foundations, methodological advances, computational methods, and applications. We discuss key concepts of OT and one-dimensional OT, the role of tools from integral geometry such as Radon transform in projecting measures, and statistical techniques for estimating sliced distances. The paper further explores recent methodological advances, including non-linear projections, improved Monte Carlo approximations, statistical estimation techniques for one-dimensional optimal transport, weighted slicing techniques, and transportation plan estimation methods. Variational problems, such as minimum sliced Wasserstein estimation, barycenters, gradient flows, kernel constructions, and embeddings are examined alongside extensions to unbalanced, partial, multi-marginal, and Gromov-Wasserstein settings. Applications span machine learning, statistics, computer graphics and computer visions, highlighting SOT's versatility as a practical computational tool. This work will be of interest to researchers and practitioners in machine learning, data sciences, and computational disciplines seeking efficient alternatives to classical OT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12519v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen</dc:creator>
    </item>
    <item>
      <title>On computing and the complexity of computing higher-order $U$-statistics, exactly</title>
      <link>https://arxiv.org/abs/2508.12627</link>
      <description>arXiv:2508.12627v1 Announce Type: cross 
Abstract: Higher-order $U$-statistics abound in fields such as statistics, machine learning, and computer science, but are known to be highly time-consuming to compute in practice. Despite their widespread appearance, a comprehensive study of their computational complexity is surprisingly lacking. This paper aims to fill that gap by presenting several results related to the computational aspect of $U$-statistics. First, we derive a useful decomposition from an $m$-th order $U$-statistic to a linear combination of $V$-statistics with orders not exceeding $m$, which are generally more feasible to compute. Second, we explore the connection between exactly computing $V$-statistics and Einstein summation, a tool often used in computational mathematics, quantum computing, and quantum information sciences for accelerating tensor computations. Third, we provide an optimistic estimate of the time complexity for exactly computing $U$-statistics, based on the treewidth of a particular graph associated with the $U$-statistic kernel. The above ingredients lead to a new, much more runtime-efficient algorithm of exactly computing general higher-order $U$-statistics. We also wrap our new algorithm into an open-source Python package called $\texttt{u-stats}$. We demonstrate via three statistical applications that $\texttt{u-stats}$ achieves impressive runtime performance compared to existing benchmarks. This paper aspires to achieve two goals: (1) to capture the interest of researchers in both statistics and other related areas further to advance the algorithmic development of $U$-statistics, and (2) to offer the package $\texttt{u-stats}$ as a valuable tool for practitioners, making the implementation of methods based on higher-order $U$-statistics a more delightful experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12627v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingyu Chen, Ruiqi Zhang, Lin Liu</dc:creator>
    </item>
    <item>
      <title>Bivariate Distribution Regression; Theory, Estimation and an Application to Intergenerational Mobility</title>
      <link>https://arxiv.org/abs/2508.12716</link>
      <description>arXiv:2508.12716v1 Announce Type: cross 
Abstract: We employ distribution regression (DR) to estimate the joint distribution of two outcome variables conditional on chosen covariates. While Bivariate Distribution Regression (BDR) is useful in a variety of settings, it is particularly valuable when some dependence between the outcomes persists after accounting for the impact of the covariates. Our analysis relies on a result from Chernozhukov et al. (2018) which shows that any conditional joint distribution has a local Gaussian representation. We describe how BDR can be implemented and present some associated functionals of interest. As modeling the unexplained dependence is a key feature of BDR, we focus on functionals related to this dependence. We decompose the difference between the joint distributions for different groups into composition, marginal and sorting effects. We provide a similar decomposition for the transition matrices which describe how location in the distribution in one of the outcomes is associated with location in the other. Our theoretical contributions are the derivation of the properties of these estimated functionals and appropriate procedures for inference. Our empirical illustration focuses on intergenerational mobility. Using the Panel Survey of Income Dynamics data, we model the joint distribution of parents' and children's earnings. By comparing the observed distribution with constructed counterfactuals, we isolate the impact of observable and unobservable factors on the observed joint distribution. We also evaluate the forces responsible for the difference between the transition matrices of sons' and daughters'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12716v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Iv\'an Fern\'andez-Val, Jonas Meier, Aico van Vuuren, Francis Vella</dc:creator>
    </item>
    <item>
      <title>Estimation in linear models with clustered data</title>
      <link>https://arxiv.org/abs/2508.12860</link>
      <description>arXiv:2508.12860v1 Announce Type: cross 
Abstract: We study linear regression models with clustered data, high-dimensional controls, and a complicated structure of exclusion restrictions. We propose a correctly centered internal IV estimator that accommodates a variety of exclusion restrictions and permits within-cluster dependence. The estimator has a simple leave-out interpretation and remains computationally tractable. We derive a central limit theorem for its quadratic form and propose a robust variance estimator. We also develop inference methods that remain valid under weak identification. Our framework extends classical dynamic panel methods to more general clustered settings. An empirical application of a large-scale fiscal intervention in rural Kenya with spatial interference illustrates the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12860v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Mikusheva, Mikkel S{\o}lvsten, Baiyun Jing</dc:creator>
    </item>
    <item>
      <title>Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption</title>
      <link>https://arxiv.org/abs/2508.12896</link>
      <description>arXiv:2508.12896v1 Announce Type: cross 
Abstract: We formalize three design axioms for sustained adoption of agent-centric AI systems executing multi-step tasks: (A1) Reliability &gt; Novelty; (A2) Embed &gt; Destination; (A3) Agency &gt; Chat. We model adoption as a sum of a decaying novelty term and a growing utility term and derive the phase conditions for troughs/overshoots with full proofs. We introduce: (i) an identifiability/confounding analysis for $(\alpha,\beta,N_0,U_{\max})$ with delta-method gradients; (ii) a non-monotone comparator (logistic-with-transient-bump) evaluated on the same series to provide additional model comparison; (iii) ablations over hazard families $h(\cdot)$ mapping $\Delta V \to \beta$; (iv) a multi-series benchmark (varying trough depth, noise, AR structure) reporting coverage (type-I error, power); (v) calibration of friction proxies against time-motion/survey ground truth with standard errors; (vi) residual analyses (autocorrelation and heteroskedasticity) for each fitted curve; (vii) preregistered windowing choices for pre/post estimation; (viii) Fisher information &amp; CRLB for $(\alpha,\beta)$ under common error models; (ix) microfoundations linking $\mathcal{T}$ to $(N_0,U_{\max})$; (x) explicit comparison to bi-logistic, double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$ heterogeneity. Figures and tables are reflowed for readability, and the bibliography restores and extends non-logistic/Bass adoption references (Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All code and logs necessary to reproduce the synthetic analyses are embedded as LaTeX listings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12896v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faruk Alpay, Taylan Alpay</dc:creator>
    </item>
    <item>
      <title>The purpose of an estimator is what it does: Misspecification, estimands, and over-identification</title>
      <link>https://arxiv.org/abs/2508.13076</link>
      <description>arXiv:2508.13076v1 Announce Type: cross 
Abstract: In over-identified models, misspecification -- the norm rather than exception -- fundamentally changes what estimators estimate. Different estimators imply different estimands rather than different efficiency for the same target. A review of recent applications of generalized method of moments in the American Economic Review suggests widespread acceptance of this fact: There is little formal specification testing and widespread use of estimators that would be inefficient were the model correct, including the use of "hand-selected" moments and weighting matrices. Motivated by these observations, we review and synthesize recent results on estimation under model misspecification, providing guidelines for transparent and robust empirical research. We also provide a new theoretical result, showing that Hansen's J-statistic measures, asymptotically, the range of estimates achievable at a given standard error. Given the widespread use of inefficient estimators and the resulting researcher degrees of freedom, we thus particularly recommend the broader reporting of J-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13076v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaiah Andrew, Jiafeng Chen, Otavio Tecchio</dc:creator>
    </item>
    <item>
      <title>Reasonable uncertainty: Confidence intervals in empirical Bayes discrimination detection</title>
      <link>https://arxiv.org/abs/2508.13110</link>
      <description>arXiv:2508.13110v1 Announce Type: cross 
Abstract: We revisit empirical Bayes discrimination detection, focusing on uncertainty arising from both partial identification and sampling variability. While prior work has mostly focused on partial identification, we find that some empirical findings are not robust to sampling uncertainty. To better connect statistical evidence to the magnitude of real-world discriminatory behavior, we propose a counterfactual odds-ratio estimand with a attractive properties and interpretation. Our analysis reveals the importance of careful attention to uncertainty quantification and downstream goals in empirical Bayes analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13110v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaying Gu, Nikolaos Ignatiadis, Azeem M. Shaikh</dc:creator>
    </item>
    <item>
      <title>Alpha Estimation via Sample Splitting: A Two-Sample Framework for Stable-like Distributions</title>
      <link>https://arxiv.org/abs/1705.09840</link>
      <description>arXiv:1705.09840v2 Announce Type: replace 
Abstract: Stable distributions provide a flexible framework for modeling heavy-tailed and skewed data, with the stability index $\alpha$ quantifying tail heaviness. We propose a new semiparametric estimator for $\alpha$ that leverages the two-sum closure property of stable distributions within a location-scale framework. The method transforms a single sample into two pseudo-independent samples via repeated random splitting and estimates $\alpha$ using weighted least squares applied to empirical quantiles. This approach avoids intractable likelihood calculations, offers computational advantages over maximum likelihood estimation, and remains robust to skewness. We establish consistency and asymptotic properties of the estimator and assess its finite-sample performance via simulation. Results indicate competitive accuracy, particularly in small samples and heavy-tailed settings, with substantial computational savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:1705.09840v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cornelis J. Potgieter, Jacques van Appel, Sudharshan Samaratunga</dc:creator>
    </item>
    <item>
      <title>Multiply robust estimation for causal survival analysis with treatment noncompliance</title>
      <link>https://arxiv.org/abs/2305.13443</link>
      <description>arXiv:2305.13443v3 Announce Type: replace 
Abstract: Comparative effectiveness research frequently addresses a time-to-event outcome and can require unique considerations in the presence of treatment noncompliance. Motivated by the challenges in addressing noncompliance in the ADAPTABLE pragmatic clinical trial, we develop a multiply robust estimator to estimate the principal survival causal effects under the principal ignorability and monotonicity. The multiply robust estimator is consistent even if one, and sometimes two, of the required models are misspecified. We apply the multiply robust method in the ADAPTABLE trial to evaluate the effect of low- versus high-dose aspirin assignment on patients' death and hospitalization from cardiovascular diseases. We find that, comparing to low-dose assignment, assignment to the high-dose leads to differential effects among always high-dose takers, compliers, and always low-dose takers. Such treatment effect heterogeneity contributes to the null intention-to-treatment effect. We further perform a formal sensitivity analysis for investigating the robustness of our causal conclusions under violation of two identification assumptions specific to noncompliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13443v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Cheng, Bo Liu, Lisa Wruck, Fan Li, Fan Li</dc:creator>
    </item>
    <item>
      <title>A More Robust Approach to Multivariable Mendelian Randomization</title>
      <link>https://arxiv.org/abs/2402.00307</link>
      <description>arXiv:2402.00307v5 Announce Type: replace 
Abstract: Multivariable Mendelian randomization (MVMR) uses genetic variants as instrumental variables to infer the direct effects of multiple exposures on an outcome. However, unlike univariable Mendelian randomization, MVMR often faces greater challenges with many weak instruments, which can lead to bias not necessarily toward zero and inflation of type I errors. In this work, we introduce a new asymptotic regime that allows exposures to have varying degrees of instrument strength, providing a more accurate theoretical framework for studying MVMR estimators. Under this regime, our analysis of the widely used multivariable inverse-variance weighted method shows that it is often biased and tends to produce misleadingly narrow confidence intervals in the presence of many weak instruments. To address this, we propose a simple, closed-form modification to the multivariable inverse-variance weighted estimator to reduce bias from weak instruments, and additionally introduce a novel spectral regularization technique to improve finite-sample performance. We show that the resulting spectral-regularized estimator remains consistent and asymptotically normal under many weak instruments. Through simulations and real data applications, we demonstrate that our proposed estimator and asymptotic framework can enhance the robustness of MVMR analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00307v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/biomet/asaf053</arxiv:DOI>
      <arxiv:journal_reference>Biometrika, asaf053 (2025)</arxiv:journal_reference>
      <dc:creator>Yinxiang Wu, Hyunseung Kang, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Jacobi Prior: An Alternative Bayesian Method for Supervised Learning</title>
      <link>https://arxiv.org/abs/2404.11345</link>
      <description>arXiv:2404.11345v3 Announce Type: replace 
Abstract: The Jacobi prior offers an alternative Bayesian framework for predictive modelling, designed to achieve superior computational efficiency without compromising predictive performance. This scalable method is suitable for image classification and other computationally intensive tasks. Compared to widely used methods such as Lasso, Ridge, Elastic Net, the MCMC-based Horseshoe prior, and non-Bayesian machine learning methods including Support Vector Machines (SVM), Random Forests, and Extreme Gradient Boosting (XGBoost), the Jacobi prior achieves competitive or better accuracy with significantly reduced computational cost. The method is well suited to distributed computing environments, as it naturally accommodates partitioned data across multiple servers. We propose a parallelisable Monte Carlo algorithm to quantify the uncertainty in the estimated coefficients. We establish the theoretical foundations of the Jacobi estimator by studying its asymptotic properties. In particular, we prove a Bernstein--von Mises theorem for the Jacobi posterior. To demonstrate its practical utility, we conduct a comprehensive simulation study comprising seven experiments focused on statistical consistency, prediction accuracy, scalability, sensitivity analysis and robustness study. In the spine classification task, we extract last-layer features from a fine-tuned ResNet-50 model and evaluate multiple classifiers, including Jacobi-Multinomial logit regression, SVM, and Random Forest. The Jacobi prior achieves state-of-the-art results in recall and predictive stability, especially when paired with domain-specific features. This highlights its potential for scalable, high-dimensional learning in medical image analysis.
  All code and datasets used in this paper are available at: https://github.com/sourish-cmi/Jacobi-Prior/</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11345v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sourish Das, Shouvik Sardar</dc:creator>
    </item>
    <item>
      <title>EFECT: A Method to Quantify the Reproducibility of Stochastic Simulations</title>
      <link>https://arxiv.org/abs/2406.16820</link>
      <description>arXiv:2406.16820v2 Announce Type: replace 
Abstract: Reproducibility is a fundamental requirement for validating scientific claims in computational research. Stochastic computational models are widely used in fields such as systems biology, financial modeling and environmental sciences. However, achieving reproducibility in stochastic simulations remains challenging, as each run can produce different outcomes. Existing infrastructure and software tools do not address independent reproduction of simulation results. Without independent reproducibility, results and conclusions lack credibility, as it remains unclear whether observed findings reflect model behavior or are artifacts of stochastic variation or an underpowered study. To bridge this gap, we introduce the Empirical Characteristic Function Equality Convergence Test (EFECT), a data-driven method to quantify the reproducibility of stochastic simulation results. EFECT employs empirical characteristic functions to compare reported results with those independently generated by assessing distributional inequality, termed EFECT error. Additionally, we establish the EFECT convergence point, a quantitative metric for determining the required number of simulation runs to achieve an EFECT error value of a priori significance. EFECT is applicable to all bounded, real-valued outputs, regardless of the model type or simulation method that produced them. We tested EFECT with over 40 use cases to demonstrate its broad applicability and effectiveness. EFECT standardizes stochastic simulation reproducibility, establishing a workflow that guarantees reliable results, supporting a wide range of stakeholders, and thereby enhancing validation of stochastic simulation studies, across a model's lifecycle. To promote standardization, we are developing the open-source software library libSSR in multiple programming languages for easy integration of EFECT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16820v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. J. Sego, Matthias K\"onig, Luis L. Fonseca, Dilan Pathirana, Frank T. Bergmann, Hern\'an E. Grecco, Mauro Silberberg, Subhasis Ray, Baylor Fain, Adam C. Knapp, Krishna Tiwari, Henning Hermjakob, Herbert M. Sauro, James A. Glazier, Reinhard C. Laubenbacher, Rahuman S. Malik-Sheriff</dc:creator>
    </item>
    <item>
      <title>Bayesian Mapping of Mortality Clusters</title>
      <link>https://arxiv.org/abs/2407.19135</link>
      <description>arXiv:2407.19135v3 Announce Type: replace 
Abstract: Disease mapping analyses the distribution of several disease outcomes within a territory. Primary goals include identifying areas with unexpected changes in mortality rates, studying the relation among multiple diseases, and dividing the analysed territory into clusters based on the observed levels of disease incidence or mortality. In this work, we focus on detecting spatial mortality clusters, that occur when neighbouring areas within a territory exhibit similar mortality levels due to one or more diseases. When multiple causes of death are examined together, it is relevant to identify not only the spatial boundaries of the clusters but also the diseases that lead to their formation. However, existing methods in literature struggle to address this dual problem effectively and simultaneously. To overcome these limitations, we introduce Perla, a multivariate Bayesian model that clusters areas in a territory according to the observed mortality rates of multiple causes of death, also exploiting the information of external covariates. Our model incorporates the spatial structure of data directly into the clustering probabilities by leveraging the stick-breaking formulation of the multinomial distribution. Additionally, it exploits suitable global-local shrinkage priors to ensure that the detection of clusters depends on diseases showing concrete increases or decreases in mortality levels, while excluding uninformative diseases. We propose an MCMC algorithm for posterior inference that consists of closed-form Gibbs sampling moves for nearly every model parameter. To demonstrate the flexibility and effectiveness of our methodology, we validate Perla with a series of simulation experiments and two extensive case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19135v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrea Sottosanti, Enrico Bovo, Pietro Belloni, Giovanna Boccuzzo</dc:creator>
    </item>
    <item>
      <title>Adaptive sparsening and smoothing of the treatment model for longitudinal causal inference using outcome-adaptive LASSO and marginal fused LASSO</title>
      <link>https://arxiv.org/abs/2410.08283</link>
      <description>arXiv:2410.08283v2 Announce Type: replace 
Abstract: Causal variable selection in time-varying treatment settings is challenging due to evolving confounding effects. Existing methods mainly focus on time-fixed exposures and are not directly applicable to time-varying scenarios. We propose a novel two-step procedure for variable selection when modeling the treatment probability at each time point. We first introduce a novel approach to longitudinal confounder selection using a Longitudinal Outcome Adaptive LASSO (LOAL) that will data-adaptively select covariates with theoretical justification of variance reduction of the estimator of the causal effect. We then propose an Adaptive Fused LASSO that can collapse treatment model parameters over time points with the goal of simplifying the models in order to improve the efficiency of the estimator while minimizing model misspecification bias compared with naive pooled logistic regression models. Our simulation studies highlight the need for and usefulness of the proposed approach in practice. We implemented our method on data from the Nicotine Dependence in Teens study to estimate the effect of the timing of alcohol initiation during adolescence on depressive symptoms in early adulthood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08283v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mireille E Schnitzer, Denis Talbot, Yan Liu, David Berger, Guanbo Wang, Jennifer O'Loughlin, Marie-Pierre Sylvestre, Ashkan Ertefaie</dc:creator>
    </item>
    <item>
      <title>Differentially Private Covariate Balancing Causal Inference</title>
      <link>https://arxiv.org/abs/2410.14789</link>
      <description>arXiv:2410.14789v2 Announce Type: replace 
Abstract: Differential privacy is the leading mathematical framework for privacy protection, providing a probabilistic guarantee that safeguards individuals' private information when publishing statistics from a dataset. This guarantee is achieved by applying a randomized algorithm to the original data, which introduces unique challenges in data analysis by distorting inherent patterns. In particular, causal inference using observational data in privacy-sensitive contexts is challenging because it requires covariate balance between treatment groups, yet checking the true covariates is prohibited to prevent leakage of sensitive information. In this article, we present a differentially private two-stage covariate balancing weighting estimator to infer causal effects from observational data. Our algorithm produces both point and interval estimators with statistical guarantees, such as consistency and rate optimality, under a given privacy budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14789v2</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuki Ohnishi, Jordan Awan</dc:creator>
    </item>
    <item>
      <title>Estimands and Their Implications for Evidence Synthesis for Oncology: A Simulation Study of Treatment Switching in Meta-Analysis</title>
      <link>https://arxiv.org/abs/2411.14323</link>
      <description>arXiv:2411.14323v2 Announce Type: replace 
Abstract: The ICH E9(R1) addendum provides guidelines on accounting for intercurrent events in clinical trials using the estimands framework. However, there has been limited attention to the estimands framework for meta-analysis. Using treatment switching, a well-known intercurrent event that occurs frequently in oncology, we conducted a simulation study to explore the bias introduced by pooling together estimates targeting different estimands in a meta-analysis of randomized clinical trials (RCTs) that allowed treatment switching. We simulated overall survival data of a collection of RCTs that allowed patients in the control group to switch to the intervention treatment after disease progression under fixed-effects and random-effects models. For each RCT, we calculated effect estimates for a treatment policy estimand that ignored treatment switching, and a hypothetical estimand that accounted for treatment switching either by fitting rank-preserving structural failure time models or by censoring switchers. Then, we performed random-effects and fixed-effects meta-analyses to pool together RCT effect estimates while varying the proportions of trials providing treatment policy and hypothetical effect estimates. We compared the results of meta-analyses that pooled different types of effect estimates with those that pooled only treatment policy or hypothetical estimates. We found that pooling estimates targeting different estimands results in pooled estimators that do not target any estimand of interest, and that pooling estimates of varying estimands can generate misleading results, even under a random-effects model. Adopting the estimands framework for meta-analysis may improve alignment between meta-analytic results and the clinical research question of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14323v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rebecca K. Metcalfe, Antonio Remiro-Az\'ocar, Quang Vuong, Anders Gorst-Rasmussen, Oliver Keene, Shomoita Alam, Jay J. H. Park</dc:creator>
    </item>
    <item>
      <title>Chain-linked multiple matrix integration via embedding alignment</title>
      <link>https://arxiv.org/abs/2412.02791</link>
      <description>arXiv:2412.02791v4 Announce Type: replace 
Abstract: Motivated by the increasing demand for multi-source data integration in various scientific fields, in this paper we study matrix completion in scenarios where the data exhibits certain block-wise missing structures -- specifically, where only a few noisy submatrices representing (overlapping) parts of the full matrix are available. We propose the Chain-linked Multiple Matrix Integration (CMMI) procedure to efficiently combine the information that can be extracted from these individual noisy submatrices. CMMI begins by deriving entity embeddings for each observed submatrix, then aligns these embeddings using overlapping entities between pairs of submatrices, and finally aggregates them to reconstruct the entire matrix of interest. We establish, under mild regularity conditions, entrywise error bounds and normal approximations for the CMMI estimates. Simulation studies and real data applications show that CMMI is computationally efficient and effective in recovering the full matrix, even when overlaps between the observed submatrices are minimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02791v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runbing Zheng, Minh Tang</dc:creator>
    </item>
    <item>
      <title>Estimands for Early Phase Dose Optimization Trials in Oncology</title>
      <link>https://arxiv.org/abs/2501.18930</link>
      <description>arXiv:2501.18930v2 Announce Type: replace 
Abstract: Phase I dose escalation trials in oncology generally aim to find the maximum tolerated dose (MTD). However, with the advent of molecular targeted therapies and antibody drug conjugates, dose limiting toxicities are less frequently observed, giving rise to the concept of optimal biological dose (OBD), which considers both efficacy and toxicity. The Estimand framework presented in the addendum of the ICH E9(R1) guidelines strengthens the dialogue between different stakeholders by bringing in greater clarity in the clinical trial objectives and by providing alignment between the targeted estimand under consideration and the statistical analysis methods. However, there lacks clarity in implementing this framework in early phase dose optimization studies. This manuscript aims at discussing the Estimand framework for dose optimization trials in oncology considering efficacy and toxicity through utility functions. Such trials should include Pharmacokinetics (PK) data, toxicity data, and efficacy data. Based on these data, the analysis methods used to identify the optimized dose/s are also described. Focusing on optimizing the utility function to estimate the OBD, the population-level summary measure should reflect only the properties used for the estimating this utility function. A detailed strategy recommendation for intercurrent events has been provided using a real-life oncology case study. Key recommendations regarding the estimand attributes include that in a seamless Phase I/II dose optimization trial, the treatment attribute should start when the subject receives the first dose. We argue that such a framework brings in additional clarity to dose optimization trial objectives and strengthens the understanding of the drug under consideration that would enable the correct dose to move to Phase II of clinical development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18930v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayon Mukherjee, Jonathan L. Moscovici, Zheng Liu</dc:creator>
    </item>
    <item>
      <title>Stability and performance guarantees for misspecified multivariate score-driven filters</title>
      <link>https://arxiv.org/abs/2502.05021</link>
      <description>arXiv:2502.05021v4 Announce Type: replace 
Abstract: Can stochastic gradient methods track a moving target? We address the problem of tracking multivariate time-varying parameters under noisy observations and potential model misspecification. Specifically, we examine implicit and explicit score-driven (ISD and ESD) filters, which update parameter predictions using the gradient of the logarithmic postulated observation density (commonly referred to as the score). For both filter types, we derive novel sufficient conditions that ensure the exponential stability of the filtered parameter path and the existence of a finite mean squared error (MSE) bound relative to the pseudo-true parameter path. Our (non-)asymptotic MSE bounds rely on mild moment conditions on the data-generating process, while our stability results are agnostic about the true process. For the ISD filter, concavity of the postulated log density combined with simple parameter restrictions is sufficient to guarantee stability. In contrast, the ESD filter additionally requires the score to be Lipschitz continuous and the learning rate to be sufficiently small. We validate our theoretical findings through simulation studies, showing that ISD filters outperform ESD filters in terms of accuracy and stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05021v4</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Donker van Heel, Rutger-Jan Lange, Bram van Os, Dick van Dijk</dc:creator>
    </item>
    <item>
      <title>Distilling heterogeneous treatment effects: Stable subgroup estimation in causal inference</title>
      <link>https://arxiv.org/abs/2502.07275</link>
      <description>arXiv:2502.07275v3 Announce Type: replace 
Abstract: Recent methodological developments have introduced new black-box approaches to better estimate heterogeneous treatment effects; however, these methods fall short of providing interpretable characterizations of the underlying individuals who may be most at risk or benefit most from receiving the treatment, thereby limiting their practical utility. In this work, we introduce \textit{causal distillation trees} (CDT) to estimate interpretable subgroups. CDT allows researchers to fit any machine learning model to estimate the heterogeneous treatment effect, and then leverages a simple, second-stage tree-based model to "distill" the estimated treatment effect into meaningful subgroups. As a result, CDT inherits the improvements in predictive performance from black-box machine learning models while preserving the interpretability of a simple decision tree. We derive theoretical guarantees for the consistency of the estimated subgroups using CDT, and introduce stability-driven diagnostics for researchers to evaluate the quality of the estimated subgroups. We illustrate our proposed method on a randomized controlled trial of antiretroviral treatment for HIV from the AIDS Clinical Trials Group Study 175 and show that CDT out-performs state-of-the-art approaches in constructing stable, clinically relevant subgroups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07275v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melody Huang, Tiffany M. Tang, Ana M. Kenney</dc:creator>
    </item>
    <item>
      <title>PanelMatch: Matching Methods for Causal Inference with Time-Series Cross-Section Data</title>
      <link>https://arxiv.org/abs/2503.02073</link>
      <description>arXiv:2503.02073v2 Announce Type: replace 
Abstract: Analyzing time-series cross-sectional (also known as longitudinal or panel) data is an important process across a number of fields, including the social sciences, economics, finance, and medicine. PanelMatch is an R package that implements a set of tools enabling researchers to apply matching methods for causal inference with time-series cross-sectional data. Relative to other commonly used methods for longitudinal analyses, like regression with fixed effects, the matching-based approach implemented in PanelMatch makes fewer parametric assumptions and offers more diagnostics. In this paper, we discuss the PanelMatch package, showing users a recommended pipeline for doing causal inference analysis with it and highlighting useful diagnostic and visualization tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02073v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Rauh, In Song Kim, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>On "confirmatory" methodological research in statistics and related fields</title>
      <link>https://arxiv.org/abs/2503.08124</link>
      <description>arXiv:2503.08124v3 Announce Type: replace 
Abstract: Empirical substantive research, such as in the life or social sciences, is commonly categorized into the two modes exploratory and confirmatory, both of which are essential to scientific progress. The former is also referred to as hypothesis-generating or data-contingent research, while the latter is also called hypothesis-testing research. In the context of empirical methodological research in statistics, however, the exploratory-confirmatory distinction has received very little attention so far. Our paper aims to fill this gap. First, we revisit the concept of empirical methodological research through the lens of the exploratory-confirmatory distinction. Secondly, we examine current practice with respect to this distinction through a literature survey including 115 articles from the field of biostatistics. Thirdly, we provide practical recommendations towards more appropriate design, interpretation, and reporting of empirical methodological research in light of this distinction. In particular, we argue that both modes of research are crucial to methodological progress, but that most published studies -- even if sometimes disguised as confirmatory -- are essentially exploratory in nature. We emphasize that it may be adequate to consider empirical methodological research as a continuum between "pure" exploration and "strict" confirmation, recommend transparently reporting the mode of conducted research within the spectrum between exploratory and confirmatory, and stress the importance of study protocols written before conducting the study, especially in confirmatory methodological research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08124v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. J. D. Lange, Juliane C. Wilcke, Sabine Hoffmann, Moritz Herrmann, Anne-Laure Boulesteix</dc:creator>
    </item>
    <item>
      <title>Two-sample comparison through additive tree models for density ratios</title>
      <link>https://arxiv.org/abs/2508.03059</link>
      <description>arXiv:2508.03059v2 Announce Type: replace 
Abstract: The ratio of two densities characterizes their differences. We consider learning the density ratio given i.i.d. observations from each of the two distributions. We propose additive tree models for the density ratio along with efficient algorithms for training these models using a new loss function called the balancing loss. With this loss, additive tree models for the density ratio can be trained using algorithms original designed for supervised learning. Specifically, they can be trained from both an optimization perspective that parallels tree boosting and from a (generalized) Bayesian perspective that parallels Bayesian additive regression trees (BART). For the former, we present two boosting algorithms -- one based on forward-stagewise fitting and the other based on gradient boosting, both of which produce a point estimate for the density ratio function. For the latter, we show that due to the loss function's resemblance to an exponential family kernel, the new loss can serve as a pseudo-likelihood for which conjugate priors exist, thereby enabling effective generalized Bayesian inference on the density ratio using backfitting samplers designed for BART. The resulting uncertainty quantification on the inferred density ratio is critical for applications involving high-dimensional and complex distributions in which uncertainty given limited data can often be substantial. We provide insights on the balancing loss through its close connection to the exponential loss in binary classification and to the variational form of f-divergence, in particular that of the squared Hellinger distance. Our numerical experiments demonstrate the accuracy of the proposed approach while providing unique capabilities in uncertainty quantification. We demonstrate the application of our method in a case study involving assessing the quality of generative models for microbiome compositional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03059v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoki Awaya, Yuliang Xu, Li Ma</dc:creator>
    </item>
    <item>
      <title>Efficient Statistical Estimation for Sequential Adaptive Experiments with Implications for Adaptive Designs</title>
      <link>https://arxiv.org/abs/2508.09135</link>
      <description>arXiv:2508.09135v2 Announce Type: replace 
Abstract: Adaptive experimental designs have gained popularity in clinical trials and online experiments. Unlike traditional, fixed experimental designs, adaptive designs can dynamically adjust treatment randomization probabilities and other design features in response to data accumulated sequentially during the experiment. These adaptations are useful to achieve diverse objectives, including reducing uncertainty in the estimation of causal estimands or increasing participants' chances of receiving better treatments during the experiment. At the end of the experiment, it is often desirable to answer causal questions from the observed data. However, the adaptive nature of such experiments and the resulting dependence among observations pose significant challenges to providing valid statistical inference and efficient estimation of causal estimands. Building upon the Targeted Maximum Likelihood Estimator (TMLE) framework tailored for adaptive designs (van der Laan, 2008), we introduce a new Adaptive-Design-Likelihood-based TMLE (ADL-TMLE) to estimate a wide class of causal estimands from adaptive experiment data, including the average treatment effect as our primary example. We establish asymptotic normality and semiparametric efficiency of ADL-TMLE under relaxed positivity and design stabilization assumptions for adaptive experiments. Motivated by these results, we further propose a novel adaptive design aimed at minimizing the variance of the estimator based on data generated under that design. Simulations show that ADL-TMLE demonstrates superior variance-reduction performance across different types of adaptive experiments, and that the proposed adaptive design attains lower variance than the standard efficiency-oriented adaptive design. Finally, we generalize our framework to broader settings, including those with longitudinal structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09135v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Zhang, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Modelling Skewed and Heavy-Tailed Errors in Bayesian Mediation Analysis</title>
      <link>https://arxiv.org/abs/2508.09311</link>
      <description>arXiv:2508.09311v2 Announce Type: replace 
Abstract: Traditional mediation models in both the frequentist and Bayesian frameworks typically assume normality of the error terms. Violations of this assumption can impair the estimation and hypothesis testing of the mediation effect in conventional approaches. This study addresses the non-normality issue by explicitly modelling skewed and heavy-tailed error terms within the Bayesian mediation framework. Building on the work of Fern\'andez and Steel (1998), this study introduces a novel family of distributions, termed the Centred Two-Piece Student $t$ Distribution (CTPT). The new distribution incorporates a skewness parameter into the Student t distribution and centres it to have a mean of zero, enabling flexible modelling of error terms in Bayesian regression and mediation analysis. A class of standard improper priors is employed, and conditions for the existence of the posterior distribution and posterior moments are established, while enabling inference on both skewness and tail parameters. Simulation studies are conducted to examine parameter recovery accuracy and statistical power in testing mediation effects. Compared to traditional Bayesian and frequentist methods, particularly bootstrap-based approaches, our method gives greater statistical power when correctly specified, while maintaining robustness against model misspecification. The application of the proposed approach is illustrated through real data analysis. Additionally, we have developed an R package FlexBayesMed to implement our methods in linear regression and mediation analysis, available at https://github.com/Zongyu-Li/FlexBayesMed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09311v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongyu Li, Mark Steel, Zhiyong Zhang</dc:creator>
    </item>
    <item>
      <title>Noisy, Non-Smooth, Non-Convex Estimation of Moment Condition Models</title>
      <link>https://arxiv.org/abs/2301.07196</link>
      <description>arXiv:2301.07196v3 Announce Type: replace-cross 
Abstract: A practical challenge for structural estimation is the requirement to accurately minimize a sample objective function which is often non-smooth, non-convex, or both. This paper proposes a simple algorithm designed to find accurate solutions without performing an exhaustive search. It augments each iteration from a new Gauss-Newton algorithm with a grid search step. A finite sample analysis derives its optimization and statistical properties simultaneously using only econometric assumptions. After a finite number of iterations, the algorithm automatically transitions from global to fast local convergence, producing accurate estimates with high probability. Simulated examples and an empirical application illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07196v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Jacques Forneron</dc:creator>
    </item>
    <item>
      <title>Quantile Importance Sampling</title>
      <link>https://arxiv.org/abs/2305.03158</link>
      <description>arXiv:2305.03158v3 Announce Type: replace-cross 
Abstract: In Bayesian inference, the approximation of integrals of the form $\psi = \mathbb{E}_{F}{l(X)} = \int_{\chi} l(\mathbf{x}) d F(\mathbf{x})$ is a fundamental challenge. Such integrals are crucial for evidence estimation, which is important for various purposes, including model selection and numerical analysis. The existing strategies for evidence estimation are classified into four categories: deterministic approximation, density estimation, importance sampling, and vertical representation (Llorente et al., 2020). In this paper, we show that the Riemann sum estimator due to Yakowitz (1978) can be used in the context of nested sampling (Skilling, 2006) to achieve a $O(n^{-4})$ rate of convergence, faster than the usual Ergodic Central Limit Theorem. We provide a brief overview of the literature on the Riemann sum estimators and the nested sampling algorithm and its connections to vertical likelihood Monte Carlo. We provide theoretical and numerical arguments to show how merging these two ideas may result in improved and more robust estimators for evidence estimation, especially in higher dimensional spaces. We also briefly discuss the idea of simulating the Lorenz curve that avoids the problem of intractable $\Lambda$ functions, essential for the vertical representation and nested sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.03158v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Nicholas G. Polson</dc:creator>
    </item>
    <item>
      <title>A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models</title>
      <link>https://arxiv.org/abs/2309.06230</link>
      <description>arXiv:2309.06230v2 Announce Type: replace-cross 
Abstract: Analysis of high-dimensional data has led to increased interest in both single index models (SIMs) and the best-subset selection. SIMs provide an interpretable and flexible modeling framework for high-dimensional data, while the best-subset selection aims to find a sparse model from a large set of predictors. However, the best-subset selection in high-dimensional models is known to be computationally intractable. Existing proxy algorithms are appealing but do not yield the bestsubset solution. In this paper, we directly tackle the intractability by proposing a provably scalable algorithm for the best-subset selection in high-dimensional SIMs. We directly proved the subset selection consistency and oracle property for our algorithmic solution, distinguishing it from other state-of-the-art support recovery methods in SIMs. The algorithm comprises a generalized information criterion to determine the support size of the regression coefficients, eliminating the model selection tuning. Moreover, our method does not assume an error distribution or a specific link function and hence is flexible to apply. Extensive simulation results demonstrate that our method is not only computationally efficient but also able to exactly recover the best subset in various settings (e.g., linear regression, Poisson regression, heteroscedastic models).</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06230v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Borui Tang, Jin Zhu, Junxian Zhu, Xueqin Wang, Heping Zhang</dc:creator>
    </item>
    <item>
      <title>TRIALSCOPE: A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models</title>
      <link>https://arxiv.org/abs/2311.01301</link>
      <description>arXiv:2311.01301v3 Announce Type: replace-cross 
Abstract: The rapid digitization of real-world data presents an unprecedented opportunity to optimize healthcare delivery and accelerate biomedical discovery. However, these data are often found in unstructured forms such as clinical notes in electronic medical records (EMRs), and is typically plagued by confounders, making it challenging to generate robust real-world evidence (RWE). Therefore, we present TRIALSCOPE, a framework designed to distil RWE from population level observational data at scale. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to address common confounders in treatment effect estimation. Extensive experiments were conducted on a large-scale dataset of over one million cancer patients from a single large healthcare network in the United States. TRIALSCOPE was shown to automatically curate high-quality structured patient data, expanding the dataset and incorporating key patient attributes only available in unstructured form. The framework reduces confounding in treatment effect estimation, generating comparable results to randomized controlled lung cancer trials. Additionally, we demonstrate simulations of unconducted clinical trials - including a pancreatic cancer trial with varying eligibility criteria - using a suite of validation tests to ensure robustness. Thorough ablation studies were conducted to better understand key components of TRIALSCOPE and establish best practices for RWE generation from EMRs. TRIALSCOPE was able to extract data cancer treatment data from EMRs, overcoming limitations of manual curation. We were also able to show that TRIALSCOPE could reproduce results of lung and pancreatic cancer clinical trials from the extracted real world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01301v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javier Gonz\'alez, Risa Ueno, Cliff Wong, Zelalem Gero, Jass Bagga, Isabel Chien, Eduard Oravkin, Emre Kiciman, Aditya Nori, Roshanthi Weerasinghe, Rom S. Leidner, Brian Piening, Tristan Naumann, Carlo Bifulco, Hoifung Poon</dc:creator>
    </item>
    <item>
      <title>Non-parametric estimators of scaled cash flows</title>
      <link>https://arxiv.org/abs/2408.13176</link>
      <description>arXiv:2408.13176v2 Announce Type: replace-cross 
Abstract: In multi-state life insurance, incidental policyholder behavior gives rise to expected cash flows that are not easily targeted by classic non-parametric estimators if data is subject to sampling effects. We introduce a scaled version of the classic Aalen--Johansen estimator that overcomes this challenge. Strong uniform consistency and asymptotic normality are established under entirely random right-censoring, subject to lax moment conditions on the multivariate counting process. In a simulation study, the estimator outperforms earlier proposals from the literature. Finally, we showcase the potential of the presented method to other areas of actuarial science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13176v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>T. Bathke, C. Furrer</dc:creator>
    </item>
    <item>
      <title>Towards Optimal Environmental Policies: Policy Learning under Arbitrary Bipartite Network Interference</title>
      <link>https://arxiv.org/abs/2410.08362</link>
      <description>arXiv:2410.08362v3 Announce Type: replace-cross 
Abstract: The substantial effect of air pollution on cardiovascular disease and mortality burdens is well-established. Emissions-reducing interventions on coal-fired power plants -- a major source of hazardous air pollution -- have proven to be an effective, but costly, strategy for reducing pollution-related health burdens. Targeting the power plants that achieve maximum health benefits while satisfying realistic cost constraints is challenging. The primary difficulty lies in quantifying the health benefits of intervening at particular plants. This is further complicated because interventions are applied on power plants, while health impacts occur in potentially distant communities, a setting known as bipartite network interference (BNI). In this paper, we introduce novel policy learning methods based on Q- and A-Learning to determine the optimal policy under arbitrary BNI. We derive asymptotic properties and demonstrate finite sample efficacy in simulations. We apply our novel methods to a comprehensive dataset of Medicare claims, power plant data, and pollution transport networks. Our goal is to determine the optimal strategy for installing power plant scrubbers to minimize ischemic heart disease (IHD) hospitalizations under various cost constraints. We find that annual IHD hospitalization rates could be reduced in a range from 23.37-55.30 per 10,000 person-years through optimal policies under different cost constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08362v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raphael C. Kim, Falco J. Bargagli-Stoffi, Kevin L. Chen, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Reverse Markov Learning: Multi-Step Generative Models for Complex Distributions</title>
      <link>https://arxiv.org/abs/2502.13747</link>
      <description>arXiv:2502.13747v2 Announce Type: replace-cross 
Abstract: Learning complex distributions is a fundamental challenge in contemporary applications. Shen and Meinshausen (2024) introduced engression, a generative approach based on scoring rules that maps noise (and covariates, if available) directly to data. While effective, engression can struggle with highly complex distributions, such as those encountered in image data. In this work, we propose reverse Markov learning (RML), a framework that defines a general forward process transitioning from the target distribution to a known distribution (e.g., Gaussian) and then learns a reverse Markov process using multiple engression models. This reverse process reconstructs the target distribution step by step. This framework accommodates general forward processes, allows for dimension reduction, and naturally discretizes the generative process. In the special case of diffusion-based forward processes, RML provides an efficient discretization strategy for both training and inference in diffusion models. We further introduce an alternating sampling scheme to enhance post-training performance. Our statistical analysis establishes error bounds for RML and elucidates its advantages in estimation efficiency and flexibility in forward process design. Empirical results on simulated and climate data corroborate the theoretical findings, demonstrating the effectiveness of RML in capturing complex distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13747v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinwei Shen, Nicolai Meinshausen, Tong Zhang</dc:creator>
    </item>
    <item>
      <title>Cosmic Strings-induced CMB anisotropies in light of Weighted Morphology</title>
      <link>https://arxiv.org/abs/2503.00758</link>
      <description>arXiv:2503.00758v3 Announce Type: replace-cross 
Abstract: Motivated by the morphological measures in assessing the geometrical and topological properties of a generic cosmological stochastic field, we propose an extension of the weighted morphological measures, specifically the $n$th conditional moments of derivative (cmd-$n$). This criterion assigns a distinct weight to each excursion set point based on the associated field. We apply the cmd-$n$ on the Cosmic Microwave Background (CMB) to identify the cosmic string networks (CSs) through their unique Gott-Kaiser-Stebbins effect on the temperature anisotropies. We also formulate the perturbative expansion of cmd-$n$ for the weak non-Gaussian regime up to $\mathcal{O}(\sigma_0^3)$. We propose a comprehensive pipeline designed to analyze the morphological properties of string-induced CMB maps within the flat sky approximation. To evaluate the robustness of our proposed criteria, we employ string-induced high-resolution flat-sky CMB simulated patches of $7.2$ deg$^2$ size with a resolution of $0.42$ arc-minutes. Our results demonstrate that the minimum detectable value of cosmic string tension is $G\mu\gtrsim 1.9\times 10^{-7}$ when a noise-free map is analyzed with normalized cmd-$n$. Whereas for the ACT, CMB-S4, and Planck-like experiments at 95.45\% confidence level, the normalized cmd-$n$ can distinguish the CSs network for $G\mu\gtrsim2.9 \times 10^{-7}$, $G\mu\gtrsim 2.4\times 10^{-7}$ and $G\mu\gtrsim 5.8\times 10^{-7}$, respectively. The normalized cmd-$n$ exhibits a significantly enhanced capability in detecting CSs relative to the Minkowski Functionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00758v3</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>hep-ph</category>
      <category>physics.comp-ph</category>
      <category>stat.ME</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/mnras/staf1110</arxiv:DOI>
      <arxiv:journal_reference>MNRAS, Volume 541, Issue 4, August 2025, Pages 3851-3868</arxiv:journal_reference>
      <dc:creator>Adeela Afzal, M. Alakhras, M. H. Jalali Kanafi, S. M. S. Movahed</dc:creator>
    </item>
  </channel>
</rss>
