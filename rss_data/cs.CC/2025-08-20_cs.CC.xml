<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 04:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Approximating 1-in-3 SAT by linearly ordered hypergraph 3-colouring is NP-hard</title>
      <link>https://arxiv.org/abs/2508.14606</link>
      <description>arXiv:2508.14606v1 Announce Type: new 
Abstract: Given a satisfiable instance of 1-in-3 SAT, it is NP-hard to find a satisfying assignment for it, but it may be possible to efficiently find a solution subject to a weaker (not necessarily Boolean) predicate than `1-in-3'. There is a folklore conjecture predicting which choices of weaker predicates lead to tractability and for which the task remains \NP-hard. One specific predicate, corresponding to the problem of linearly ordered $3$-colouring of 3-uniform hypergraphs, has been mentioned in several recent papers as an obstacle to further progress in proving this conjecture. We prove that the problem for this predicate is NP-hard, as predicted by the conjecture.
  We use the Promise CSP framework, where the complexity analysis is performed via the algebraic approach, by studying the structure of polymorphisms, which are multidimensional invariants of the problem at hand. The analysis of polymorphisms is in general a highly non-trivial task, and topological combinatorics was recently discovered to provide a useful tool for this. There are two distinct ways in which it was used: one is based on variations of the Borsuk-Ulam theorem, and the other aims to classify polymorphisms up to certain reconfigurations (homotopy). Our proof, whilst combinatorial in nature, shows that our problem is the first example where the features behind the two uses of topology appear together. Thus, it is likely to be useful in guiding further development of the topological method aimed at classifying Promise CSPs. An easy consequence of our result is the hardness of another specific Promise CSP, which was recently proved by Filakovsk\'y et al. by employing a deep topological analysis of polymorphisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14606v1</guid>
      <category>cs.CC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrei Krokhin, Danny Vagnozzi</dc:creator>
    </item>
    <item>
      <title>$TIME[t] \subseteq SPACE[O(\sqrt{t})]$ via Tree Height Compression</title>
      <link>https://arxiv.org/abs/2508.14831</link>
      <description>arXiv:2508.14831v1 Announce Type: new 
Abstract: We prove a square-root space simulation for deterministic multitape Turing machines, showing $\TIME[t] \subseteq \SPACE[O(\sqrt{t})]$. The key step is a Height Compression Theorem that uniformly (and in logspace) reshapes the canonical left-deep succinct computation tree for a block-respecting run into a binary tree whose evaluation-stack depth along any DFS path is $O(\log T)$ for $T = \lceil t/b \rceil$, while preserving $O(b)$ work at leaves, $O(1)$ at internal nodes, and edges that are logspace-checkable; semantic correctness across merges is witnessed by an exact $O(b)$ window replay at the unique interface. The proof uses midpoint (balanced) recursion, a per-path potential that bounds simultaneously active interfaces by $O(\log T)$, and an indegree-capping replacement of multiway merges by balanced binary combiners. Algorithmically, an Algebraic Replay Engine with constant-degree maps over a constant-size field, together with pointerless DFS and index-free streaming, ensures constant-size per-level tokens and eliminates wide counters, yielding the additive tradeoff $S(b)=O(b + \log(t/b))$ for block sizes $b \ge b_0$ with $b_0 = \Theta(\log t)$, which at the canonical choice $b = \Theta(\sqrt{t})$ gives $O(\sqrt{t})$ space; the $b_0$ threshold rules out degenerate blocks where addressing scratch would dominate the window footprint. The construction is uniform, relativizes, and is robust to standard model choices. Consequences include branching-program upper bounds $2^{O(\sqrt{s})}$ for size-$s$ bounded-fan-in circuits, tightened quadratic-time lower bounds for $\SPACE[n]$-complete problems via the standard hierarchy argument, and $O(\sqrt{t})$-space certifying interpreters; under explicit locality assumptions, the framework extends to geometric $d$-dimensional models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14831v1</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Logan Nye</dc:creator>
    </item>
    <item>
      <title>Sublinear-Time Approximation for Graph Frequency Vectors in Hyperfinite Graphs</title>
      <link>https://arxiv.org/abs/2508.14324</link>
      <description>arXiv:2508.14324v1 Announce Type: cross 
Abstract: In this work, we address the problem of approximating the $k$-disc distribution (``frequency vector") of a bounded-degree graph in sublinear-time under the assumption of hyperfiniteness. We revisit the partition-oracle framework of Hassidim, Kelner, Nguyen, and Onak \cite{hassidim2009local}, and provide a concise, self-contained analysis that explicitly separates the two sources of error: (i) the cut error, controlled by hyperfiniteness parameter $\phi$, which incurs at most $\varepsilon/2$ in $\ell_1$-distance by removing at most $\phi |V|$ edges; and (ii) the sampling error, controlled by the accuracy parameter $\varepsilon$, bounded by $\varepsilon/2$ via $N=\Theta(\varepsilon^{-2})$ random vertex queries and a Chernoff and union bound argument. Combining these yields an overall $\ell_1$-error of $\varepsilon$ with high probability. Algorithmically, we show that by sampling $N=\lceil C\varepsilon^{-2} \rceil$ vertices and querying the local partition oracle, one can in time $poly(d,k,\varepsilon^{-1})$ construct a summary graph $H$ of size $|H|=poly(d^k,1/\varepsilon)$ whose $k$-disc frequency vector approximates that of the original graph within $\varepsilon$ in $\ell_1$-distance. Our approach clarifies the dependence of both runtime and summary-size on the parameter $d$,$k$, and $\varepsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14324v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Moroie</dc:creator>
    </item>
    <item>
      <title>Minimum Stable Cut and Treewidth</title>
      <link>https://arxiv.org/abs/2104.13097</link>
      <description>arXiv:2104.13097v3 Announce Type: replace 
Abstract: A stable or locally-optimal cut of a graph is a cut whose weight cannot be increased by changing the side of a single vertex. In this paper we study Minimum Stable Cut, the problem of finding a stable cut of minimum weight. Since this problem is NP-hard, we study its complexity on graphs of low treewidth, low degree, or both. We begin by showing that the problem remains weakly NP-hard on severely restricted trees, so bounding treewidth alone cannot make it tractable. We match this hardness with a pseudo-polynomial DP algorithm solving the problem in time $(\Delta\cdot W)^{O(tw)}n^{O(1)}$, where $tw$ is the treewidth, $\Delta$ the maximum degree, and $W$ the maximum weight. On the other hand, bounding $\Delta$ is also not enough, as the problem is NP-hard for unweighted graphs of bounded degree. We therefore parameterize Minimum Stable Cut by both $tw$ and $\Delta$ and obtain an FPT algorithm running in time $2^{O(\Delta tw)}(n+\log W)^{O(1)}$. Our main result for the weighted problem is to provide a reduction showing that both aforementioned algorithms are essentially optimal, even if we replace treewidth by pathwidth: if there exists an algorithm running in $(nW)^{o(pw)}$ or $2^{o(\Delta pw)}(n+\log W)^{O(1)}$, then the ETH is false. Complementing this, we show that we can, however, obtain an FPT approximation scheme parameterized by treewidth, if we consider almost-stable solutions, that is, solutions where no single vertex can unilaterally increase the weight of its incident cut edges by more than a factor of $(1+\varepsilon)$.
  Motivated by these mostly negative results, we consider Unweighted Minimum Stable Cut. Here our results already imply a much faster exact algorithm running in time $\Delta^{O(tw)}n^{O(1)}$. We show that this is also probably essentially optimal: an algorithm running in $n^{o(pw)}$ would contradict the ETH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.13097v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lampis</dc:creator>
    </item>
    <item>
      <title>Graph-Based Deterministic Polynomial Algorithm for NP Problems</title>
      <link>https://arxiv.org/abs/2508.13166</link>
      <description>arXiv:2508.13166v2 Announce Type: replace 
Abstract: The P = NP problem asks whether every problem whose solution can be verified in polynomial time (NP) can also be solved in polynomial time (P). In this paper, we present a proof that P = NP, demonstrating that every NP problem can be solved deterministically in polynomial time. We introduce a new Computation Model that enables the simulation of a Turing machine, and show that NP problems can be simulated efficiently within this framework. By introducing the concept of a Feasible Graph, we ensure that the simulation can be performed in polynomial time, providing a direct path to resolving the P = NP question. Our result has significant implications for fields such as cryptography, optimization, and artificial intelligence, where NP-complete problems play a central role.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13166v2</guid>
      <category>cs.CC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changryeol Lee (Department of Software, Yonsei University, Mirae Campus)</dc:creator>
    </item>
    <item>
      <title>First Order Logic on Pathwidth Revisited Again</title>
      <link>https://arxiv.org/abs/2210.09899</link>
      <description>arXiv:2210.09899v2 Announce Type: replace-cross 
Abstract: Courcelle's celebrated theorem states that all MSO-expressible properties can be decided in linear time on graphs of bounded treewidth. Unfortunately, the hidden constant implied by this theorem is a tower of exponentials whose height increases with each quantifier alternation in the formula. More devastatingly, this cannot be improved, under standard assumptions, even if we consider the much more restricted problem of deciding FO-expressible properties on trees.
  In this paper we revisit this well-studied topic and identify a natural special case where the dependence of Courcelle's theorem can, in fact, be improved. Specifically, we show that all FO-expressible properties can be decided with an elementary dependence on the input formula, if the input graph has bounded pathwidth (rather than treewidth). This is a rare example of treewidth and pathwidth having different complexity behaviors. Our result is also in sharp contrast with MSO logic on graphs of bounded pathwidth, where it is known that the dependence has to be non-elementary, under standard assumptions. Our work builds upon, and generalizes, a corresponding meta-theorem by Gajarsk{\'{y}} and Hlin{\v{e}}n{\'{y}} for the more restricted class of graphs of bounded tree-depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.09899v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lampis</dc:creator>
    </item>
    <item>
      <title>Regenerative Ulam-von Neumann Algorithm: An Innovative Markov chain Monte Carlo Method for Matrix Inversion</title>
      <link>https://arxiv.org/abs/2407.16661</link>
      <description>arXiv:2407.16661v3 Announce Type: replace-cross 
Abstract: This paper presents a regenerative variant of the classical Ulam-von Neumann Markov chain Monte Carlo algorithm for the approximation of the matrix inverse. The algorithm presented in this paper, termed regenerative Ulam-von Neumann algorithm, utilizes the regenerative structure of classical, non-truncated Neumann series defined by a non-singular matrix and produces an estimator of the matrix inverse via ratios of unbiased estimators of the regenerative quantities. The accuracy of the proposed algorithm depends on a single parameter that controls the total number of simulated Markov transitions, thus avoiding the challenge of balancing between the total number of Markov chain replications and their length as in the classical Ulam-von Neumann algorithm. To efficiently utilize Markov chain transition samples in the calculation of the regenerative variables, the proposed algorithm automatically quantifies the contribution of each Markov transition to all regenerative quantities by a carefully designed updating scheme that utilized three separate matrices containing the current weights, total weights, and regenerative cycle count, respectively. A probabilistic analysis of the performance of the algorithm, including the variance of the estimator, is provided. Finally, numerical experiments verify the effectiveness of the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16661v3</guid>
      <category>math.NA</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.NA</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyadip Ghosh, Lior Horesh, Vassilis Kalantzis, Yingdong Lu, Tomasz Nowicki</dc:creator>
    </item>
  </channel>
</rss>
