<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Nov 2024 02:44:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>About the Kannan-Bachem algorithm</title>
      <link>https://arxiv.org/abs/2411.02422</link>
      <description>arXiv:2411.02422v1 Announce Type: new 
Abstract: The Smith reduction is a basic tool when analyzing integer matrices up to equivalence, and the Kannan-Bachem (KB) algorithm is the first polynomial algorithm computing such a reduction. Using this algorithm in complicated situations where the rank of the studied matrix is not maximal revealed an unexpected obstacle in the algorithm. This difficulty is described, analyzed, a simple solution is given to overcome it, finally leading to a general organization of the KB algorithm, simpler than the original one, efficient and having a general scope.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02422v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francis Sergeraert</dc:creator>
    </item>
    <item>
      <title>A Linear Time Gap-ETH-Tight Approximation Scheme for TSP in the Euclidean Plane</title>
      <link>https://arxiv.org/abs/2411.02585</link>
      <description>arXiv:2411.02585v1 Announce Type: new 
Abstract: The Traveling Salesman Problem (TSP) in the two-dimensional Euclidean plane is among the oldest and most famous NP-hard optimization problems. In breakthrough works, Arora [J. ACM 1998] and Mitchell [SICOMP 1999] gave the first polynomial time approximation schemes. The running time of their approximation schemes was improved by Rao and Smith [STOC 1998] to $(1/\varepsilon)^{O(1/\varepsilon)} n \log n$. Bartal and Gottlieb [FOCS 2013] gave an approximation scheme of running time $2^{(1/\varepsilon)^{O(1)}} n$, which is optimal in $n$. Recently, Kisfaludi-Bak, Nederlof, and W\k{e}grzycki [FOCS 2021] gave a $2^{O(1/\varepsilon)} n \log n$ time approximation scheme, achieving the optimal running time in $\varepsilon$ under the Gap-ETH conjecture. In our work, we give a $2^{O(1/\varepsilon)} n$ time approximation scheme, achieving the optimal running time both in $n$ and in $\varepsilon$ under the Gap-ETH conjecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02585v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias M\"omke, Hang Zhou</dc:creator>
    </item>
    <item>
      <title>Linear-Time Algorithms for k-Edge-Connected Components, k-Lean Tree Decompositions, and More</title>
      <link>https://arxiv.org/abs/2411.02658</link>
      <description>arXiv:2411.02658v1 Announce Type: new 
Abstract: We present $k^{O(k^2)} m$ time algorithms for various problems about decomposing a given undirected graph by edge cuts or vertex separators of size $&lt;k$ into parts that are ``well-connected'' with respect to cuts or separators of size $&lt;k$; here, $m$ is the total number of vertices and edges of the graph. As an application of our results, we obtain for every fixed $k$ a linear-time algorithm for computing the $k$-edge-connected components of a given graph, solving a long-standing open problem. More generally, we obtain a $k^{O(k^2)} m$ time algorithm for computing a $k$-Gomory-Hu tree of a given graph, which is a structure representing pairwise minimum cuts of size $&lt;k$.
  Our main technical result, from which the other results follow, is a $k^{O(k^2)} m$ time algorithm for computing a $k$-lean tree decomposition of a given graph. This is a tree decomposition with adhesion size $&lt;k$ that captures the existence of separators of size $&lt;k$ between subsets of its bags. A $k$-lean tree decomposition is also an unbreakable tree decomposition with optimal unbreakability parameters for the adhesion size bound $k$.
  As further applications, we obtain $k^{O(k^2)} m$ time algorithms for $k$-vertex connectivity and for element connectivity $k$-Gomory-Hu tree. All of our algorithms are deterministic.
  Our techniques are inspired by the tenth paper of the Graph Minors series of Robertson and Seymour and by Bodlaender's parameterized linear-time algorithm for treewidth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02658v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tuukka Korhonen</dc:creator>
    </item>
    <item>
      <title>Sensitivity Lower Bounds for Approximaiton Algorithms</title>
      <link>https://arxiv.org/abs/2411.02744</link>
      <description>arXiv:2411.02744v2 Announce Type: new 
Abstract: Sensitivity measures how much the output of an algorithm changes, in terms of Hamming distance, when part of the input is modified. While approximation algorithms with low sensitivity have been developed for many problems, no sensitivity lower bounds were previously known for approximation algorithms. In this work, we establish the first polynomial lower bound on the sensitivity of (randomized) approximation algorithms for constraint satisfaction problems (CSPs) by adapting the probabilistically checkable proof (PCP) framework to preserve sensitivity lower bounds. From this, we derive polynomial sensitivity lower bounds for approximation algorithms for a variety of problems, including maximum clique, minimum vertex cover, and maximum cut.
  Given the connection between sensitivity and distributed algorithms, our sensitivity lower bounds also allow us to recover various round complexity lower bounds for distributed algorithms in the LOCAL model. Additionally, we present new lower bounds for distributed CSPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02744v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noah Fleming, Yuichi Yoshida</dc:creator>
    </item>
    <item>
      <title>Sampling Permutations Satisfying Constraints within and beyond the Local Lemma Regime</title>
      <link>https://arxiv.org/abs/2411.02750</link>
      <description>arXiv:2411.02750v1 Announce Type: new 
Abstract: Sampling a random permutation with restricted positions, or equivalently approximating the permanent of a 0-1 matrix, is a fundamental problem in computer science, with several notable results attained through the years. In this paper, we first improves the running time of the algorithms for a single permutation. We propose a fast approximation algorithm for the permanent of $\gamma$-dense 0-1 matrix, with an expected running time of $\tilde{O}\left(n^{2+(1-\gamma)/(2\gamma - 1)}\right)$. Our result removes the $n^4$ term from the previous best runtime and provides an improvement for $\gamma \geq 0.6$. When $\gamma = o(1)$, our runtime is $\tilde{\Theta}(n^2)$, which is nearly optimal for this problem. The core of our proof is to demonstrate that the Sinkhorn algorithm, a fundamental tool in matrix scaling, can achieve maximum accuracy of $1/\text{poly}(n)$ for dense matrices in $O(\log n)$ iterations.
  We further introduce a general model called permutations with disjunctive constraints (PDC) for handling multiple constrained permutations. We propose a novel Markov chain-based algorithm for sampling nearly uniform solutions of PDC within a Lov${\'a}$sz Local Lemma (LLL)-like regime by a novel sampling framework called correlated factorization. For uniform PDC formulas, where all constraints are of the same length and all permutations are of equal size, our algorithm runs in nearly linear time with respect to the number of variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02750v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun He, Guoliang Qiu, Xiaoming Sun</dc:creator>
    </item>
    <item>
      <title>Fast, robust approximate message passing</title>
      <link>https://arxiv.org/abs/2411.02764</link>
      <description>arXiv:2411.02764v1 Announce Type: new 
Abstract: We give a fast, spectral procedure for implementing approximate-message passing (AMP) algorithms robustly. For any quadratic optimization problem over symmetric matrices $X$ with independent subgaussian entries, and any separable AMP algorithm $\mathcal A$, our algorithm performs a spectral pre-processing step and then mildly modifies the iterates of $\mathcal A$. If given the perturbed input $X + E \in \mathbb R^{n \times n}$ for any $E$ supported on a $\varepsilon n \times \varepsilon n$ principal minor, our algorithm outputs a solution $\hat v$ which is guaranteed to be close to the output of $\mathcal A$ on the uncorrupted $X$, with $\|\mathcal A(X) - \hat v\|_2 \le f(\varepsilon) \|\mathcal A(X)\|_2$ where $f(\varepsilon) \to 0$ as $\varepsilon \to 0$ depending only on $\varepsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02764v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Misha Ivkov, Tselil Schramm</dc:creator>
    </item>
    <item>
      <title>Faster Exact and Parameterized Algorithm for Feedback Vertex Set in Bipartite Tournaments</title>
      <link>https://arxiv.org/abs/2411.02821</link>
      <description>arXiv:2411.02821v1 Announce Type: new 
Abstract: A {\em bipartite tournament} is a directed graph $T:=(A \cup B, E)$ such that every pair of vertices $(a,b), a\in A,b\in B$ are connected by an arc, and no arc connects two vertices of $A$ or two vertices of $B$. A {\em feedback vertex set} is a set $S$ of vertices in $T$ such that $T - S$ is acyclic. In this article we consider the {\sc Feedback Vertex Set} problem in bipartite tournaments. Here the input is a bipartite tournament $T$ on $n$ vertices together with an integer $k$, and the task is to determine whether $T$ has a feedback vertex set of size at most $k$. We give a new algorithm for {\sc Feedback Vertex Set in Bipartite Tournaments}. The running time of our algorithm is upper-bounded by $O(1.6181^k + n^{O(1)})$, improving over the previously best known algorithm with running time $2^kk^{O(1)} + n^{O(1)}$ [Hsiao, ISAAC 2011]. As a by-product, we also obtain the fastest currently known exact exponential-time algorithm for the problem, with running time $O(1.3820^n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02821v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.FSTTCS.2016.24</arxiv:DOI>
      <dc:creator>Mithilesh Kumar, Daniel Lokshtanov</dc:creator>
    </item>
    <item>
      <title>Max-Distance Sparsification for Diversification and Clustering</title>
      <link>https://arxiv.org/abs/2411.02845</link>
      <description>arXiv:2411.02845v1 Announce Type: new 
Abstract: Let $\mathcal{D}$ be a set family that is the solution domain of some combinatorial problem. The \emph{max-min diversification problem on $\mathcal{D}$} is the problem to select $k$ sets from $\mathcal{D}$ such that the Hamming distance between any two selected sets is at least $d$. FPT algorithms parameterized by $k,l:=\max_{D\in \mathcal{D}}|D|$ and $k,d$ have been actively studied recently for several specific domains.
  This paper provides unified algorithmic frameworks to solve this problem. Specifically, for each parameterization $k,l$ and $k,d$, we provide an FPT oracle algorithm for the max-min diversification problem using oracles related to $\mathcal{D}$. We then demonstrate that our frameworks generalize most of the existing domain-specific tractability results and provide the first FPT algorithms for several domains.
  Our main technical breakthrough is introducing the notion of \emph{max-distance sparsifier} of $\mathcal{D}$, a domain on which the max-min diversification problem is equivalent to the same problem on the original domain $\mathcal{D}$. The core of our framework is to design FPT oracle algorithms that construct a constant-size max-distance sparsifier of $\mathcal{D}$. Using max-distance sparsifiers, we provide FPT algorithms for the max-min and max-sum diversification problems on $\mathcal{D}$, as well as $k$-center and $k$-sum-of-radii clustering problems on $\mathcal{D}$, which are also natural problems in the context of diversification and have their own interests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02845v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soh Kumabe</dc:creator>
    </item>
    <item>
      <title>Counting random $k$-SAT near the satisfiability threshold</title>
      <link>https://arxiv.org/abs/2411.02980</link>
      <description>arXiv:2411.02980v1 Announce Type: new 
Abstract: We present efficient counting and sampling algorithms for random $k$-SAT when the clause density satisfies $\alpha \le \frac{2^k}{\mathrm{poly}(k)}.$ In particular, the exponential term $2^k$ matches the satisfiability threshold $\Theta(2^k)$ for the existence of a solution and the (conjectured) algorithmic threshold $2^k (\ln k) / k$ for efficiently finding a solution. Previously, the best-known counting and sampling algorithms required far more restricted densities $\alpha\lesssim 2^{k/3}$ [He, Wu, Yang, SODA '23]. Notably, our result goes beyond the lower bound $d\gtrsim 2^{k/2}$ for worst-case $k$-SAT with bounded-degree $d$ [Bez\'akov\'a et al, SICOMP '19], showing that for counting and sampling, the average-case random $k$-SAT model is computationally much easier than the worst-case model.
  At the heart of our approach is a new refined analysis of the recent novel coupling procedure by [Wang, Yin, FOCS '24], utilizing the structural properties of random constraint satisfaction problems (CSPs). Crucially, our analysis avoids reliance on the $2$-tree structure used in prior works, which cannot extend beyond the worst-case threshold $2^{k/2}$. Instead, we employ a witness tree similar to that used in the analysis of the Moser-Tardos algorithm [Moser, Tardos, JACM '10] for the Lov\'{a}sz Local lemma, which may be of independent interest. Our new analysis provides a universal framework for efficient counting and sampling for random atomic CSPs, including, for example, random hypergraph colorings. At the same time, it immediately implies as corollaries several structural and probabilistic properties of random CSPs that have been widely studied but rarely justified, including replica symmetry and non-reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02980v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongchen Chen, Aditya Lonkar, Chunyang Wang, Kuan Yang, Yitong Yin</dc:creator>
    </item>
    <item>
      <title>Top-k Stabbing Interval Queries</title>
      <link>https://arxiv.org/abs/2411.03037</link>
      <description>arXiv:2411.03037v1 Announce Type: new 
Abstract: We investigate a weighted variant of the interval stabbing problem, where the goal is to design an efficient data structure for a given set $\mathcal{I}$ of weighted intervals such that, for a query point $q$ and an integer $k&gt;0$, we can report the $k$ intervals with largest weights among those stabbed by $q$. In this paper, we present a linear space solution with $O(\log n + k)$ query time. Moreover, we also present another trade-off for the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03037v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Waseem Akram, Sanjeev Saxena</dc:creator>
    </item>
    <item>
      <title>Multi-dimensional Approximate Counting</title>
      <link>https://arxiv.org/abs/2411.03071</link>
      <description>arXiv:2411.03071v1 Announce Type: new 
Abstract: The celebrated Morris counter uses $\log_2\log_2 n + O(\log_2 \sigma^{-1})$ bits to count up to $n$ with a relative error $\sigma$, where if $\hat{\lambda}$ is the estimate of the current count $\lambda$, then $\mathbb{E}|\hat{\lambda}-\lambda|^2 &lt;\sigma^2\lambda^2$. A natural generalization is \emph{multi-dimensional} approximate counting. Let $d\geq 1$ be the dimension. The count vector $x\in \mathbb{N}^d$ is incremented entry-wisely over a stream of coordinates $(w_1,\ldots,w_n)\in [d]^n$, where upon receiving $w_k\in[d]$, $x_{w_k}\gets x_{w_k}+1$. A \emph{$d$-dimensional approximate counter} is required to count $d$ coordinates simultaneously and return an estimate $\hat{x}$ of the count vector $x$. Aden-Ali, Han, Nelson, and Yu \cite{aden2022amortized} showed that the trivial solution of using $d$ Morris counters that track $d$ coordinates separately is already optimal in space, \emph{if each entry only allows error relative to itself}, i.e., $\mathbb{E}|\hat{x}_j-x_j|^2&lt;\sigma^2|x_j|^2$ for each $j\in [d]$. However, for another natural error metric -- the \emph{Euclidean mean squared error} $\mathbb{E} |\hat{x}-x|^2$ -- we show that using $d$ separate Morris counters is sub-optimal.
  In this work, we present a simple and optimal $d$-dimensional counter with Euclidean relative error $\sigma$, i.e., $\mathbb{E} |\hat{x}-x|^2 &lt;\sigma^2|x|^2$ where $|x|=\sqrt{\sum_{j=1}^d x_j^2}$, with a matching lower bound. The upper and lower bounds are proved with ideas that are strikingly simple. The upper bound is constructed with a certain variable-length integer encoding and the lower bound is derived from a straightforward volumetric estimation of sphere covering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03071v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dingyu Wang</dc:creator>
    </item>
    <item>
      <title>On Differentially Private Linear Algebra</title>
      <link>https://arxiv.org/abs/2411.03087</link>
      <description>arXiv:2411.03087v1 Announce Type: new 
Abstract: We introduce efficient differentially private (DP) algorithms for several linear algebraic tasks, including solving linear equalities over arbitrary fields, linear inequalities over the reals, and computing affine spans and convex hulls. As an application, we obtain efficient DP algorithms for learning halfspaces and affine subspaces. Our algorithms addressing equalities are strongly polynomial, whereas those addressing inequalities are weakly polynomial. Furthermore, this distinction is inevitable: no DP algorithm for linear programming can be strongly polynomial-time efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03087v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haim Kaplan, Yishay Mansour, Shay Moran, Uri Stemmer, Nitzan Tur</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic $k$-Median with Near-Optimal Update Time and Recourse</title>
      <link>https://arxiv.org/abs/2411.03121</link>
      <description>arXiv:2411.03121v1 Announce Type: new 
Abstract: In metric $k$-clustering, we are given as input a set of $n$ points in a general metric space, and we have to pick $k$ centers and cluster the input points around these chosen centers, so as to minimize an appropriate objective function. In recent years, significant effort has been devoted to the study of metric $k$-clustering problems in a dynamic setting, where the input keeps changing via updates (point insertions/deletions), and we have to maintain a good clustering throughout these updates. The performance of such a dynamic algorithm is measured in terms of three parameters: (i) Approximation ratio, which signifies the quality of the maintained solution, (ii) Recourse, which signifies how stable the maintained solution is, and (iii) Update time, which signifies the efficiency of the algorithm.
  We consider the metric $k$-median problem, where the objective is the sum of the distances of the points to their nearest centers. We design the first dynamic algorithm for this problem with near-optimal guarantees across all three performance measures (up to a constant factor in approximation ratio, and polylogarithmic factors in recourse and update time). Specifically, we obtain a $O(1)$-approximation algorithm for dynamic metric $k$-median with $\tilde{O}(1)$ recourse and $\tilde{O}(k)$ update time. Prior to our work, the state-of-the-art here was the recent result of [Bhattacharya et al., FOCS'24], who obtained $O(\epsilon^{-1})$-approximation ratio with $\tilde{O}(k^{\epsilon})$ recourse and $\tilde{O}(k^{1+\epsilon})$ update time.
  We achieve our results by carefully synthesizing the concept of robust centers introduced in [Fichtenberger et al., SODA'21] along with the randomized local search subroutine from [Bhattacharya et al., FOCS'24], in addition to several key technical insights of our own.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03121v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sayan Bhattacharya, Mart\'in Costa, Ermiya Farokhnejad</dc:creator>
    </item>
    <item>
      <title>Tight Sampling Bounds for Eigenvalue Approximation</title>
      <link>https://arxiv.org/abs/2411.03227</link>
      <description>arXiv:2411.03227v1 Announce Type: new 
Abstract: We consider the problem of estimating the spectrum of a symmetric bounded entry (not necessarily PSD) matrix via entrywise sampling. This problem was introduced by [Bhattacharjee, Dexter, Drineas, Musco, Ray '22], where it was shown that one can obtain an $\epsilon n$ additive approximation to all eigenvalues of $A$ by sampling a principal submatrix of dimension $\frac{\text{poly}(\log n)}{\epsilon^3}$. We improve their analysis by showing that it suffices to sample a principal submatrix of dimension $\tilde{O}(\frac{1}{\epsilon^2})$ (with no dependence on $n$). This matches known lower bounds and therefore resolves the sample complexity of this problem up to $\log\frac{1}{\epsilon}$ factors. Using similar techniques, we give a tight $\tilde{O}(\frac{1}{\epsilon^2})$ bound for obtaining an additive $\epsilon\|A\|_F$ approximation to the spectrum of $A$ via squared row-norm sampling, improving on the previous best $\tilde{O}(\frac{1}{\epsilon^{8}})$ bound. We also address the problem of approximating the top eigenvector for a bounded entry, PSD matrix $A.$ In particular, we show that sampling $O(\frac{1}{\epsilon})$ columns of $A$ suffices to produce a unit vector $u$ with $u^T A u \geq \lambda_1(A) - \epsilon n$. This matches what one could achieve via the sampling bound of [Musco, Musco'17] for the special case of approximating the top eigenvector, but does not require adaptivity.
  As additional applications, we observe that our sampling results can be used to design a faster eigenvalue estimation sketch for dense matrices resolving a question of [Swartworth, Woodruff'23], and can also be combined with [Musco, Musco'17] to achieve $O(1/\epsilon^3)$ (adaptive) sample complexity for approximating the spectrum of a bounded entry PSD matrix to $\epsilon n$ additive error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03227v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Swartworth, David P. Woodruff</dc:creator>
    </item>
    <item>
      <title>Concurrent Composition for Continual Mechanisms</title>
      <link>https://arxiv.org/abs/2411.03299</link>
      <description>arXiv:2411.03299v1 Announce Type: new 
Abstract: A series of recent works by Lyu, Wang, Vadhan, and Zhang (TCC `21, NeurIPS `22, STOC `23) showed that composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of interactive differentially private mechanism, when differential privacy is measured using $f$-DP and the adversary is adaptive. We extend their work to the $\textit{continual observation setting,}$ where the data is arriving online in a potentially adaptive manner. More specifically, we show that all composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of continual differentially private mechanism, where the adversary is adaptive. We show this result for $f$-DP, which also implies the result for pure DP and $(\epsilon, \delta)$-DP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03299v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monika Henzinger, Roodabeh Safavi, Salil Vadhan</dc:creator>
    </item>
    <item>
      <title>Practical, optimal preparation of general quantum state with exponentially improved robustness</title>
      <link>https://arxiv.org/abs/2411.02782</link>
      <description>arXiv:2411.02782v2 Announce Type: cross 
Abstract: Quantum state preparation, as a general process of loading classical data to quantum device, is essential for end-to-end implementation of quantum algorithms. Yet, existing methods suffer from either high circuit depth or complicated hardware, limiting their practicality and robustness. In this work, we overcome these limitations with a bucket-brigade approach. The tree architectures of our hardware represents the simplest connectivity required for achieving sub-exponential circuit depth. Leveraging the bucket-brigade mechanism that can suppress the error propagation between different branches, our approach exhibit exponential improvement on the robustness compared to existing depth-optimal methods. More specifically, the infidelity scales as $O(\text{polylog}(N))$ with data size $N$, as oppose to $O(N)$ for conventional methods. Moreover, our approach is the first to simultaneously achieve linear Clifford$+T$ circuit depth, gate count number, and space-time allocation. These advancements offer the opportunity for processing big data in both near-term and fault-tolerant quantum devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02782v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiao-Ming Zhang</dc:creator>
    </item>
    <item>
      <title>Constant Approximation for Weighted Nash Social Welfare with Submodular Valuations</title>
      <link>https://arxiv.org/abs/2411.02942</link>
      <description>arXiv:2411.02942v1 Announce Type: cross 
Abstract: We study the problem of assigning items to agents so as to maximize the \emph{weighted} Nash Social Welfare (NSW) under submodular valuations. The best-known result for the problem is an $O(nw_{\max})$-approximation due to Garg, Husic, Li, Vega, and Vondrak~\cite{GHL23}, where $w_{\max}$ is the maximum weight over all agents. Obtaining a constant approximation algorithm is an open problem in the field that has recently attracted considerable attention.
  We give the first such algorithm for the problem, thus solving the open problem in the affirmative. Our algorithm is based on the natural Configuration LP for the problem, which was introduced recently by Feng and Li~\cite{FL24} for the additive valuation case. Our rounding algorithm is similar to that of Li \cite{Li25} developed for the unrelated machine scheduling problem to minimize weighted completion time. Roughly speaking, we designate the largest item in each configuration as a large item and the remaining items as small items. So, every agent gets precisely 1 fractional large item in the configuration LP solution. With the rounding algorithm in \cite{Li25}, we can ensure that in the obtained solution, every agent gets precisely 1 large item, and the assignments of small items are negatively correlated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02942v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuda Feng, Yang Hu, Shi Li, Ruilong Zhang</dc:creator>
    </item>
    <item>
      <title>Reconstructing edge-deleted unicyclic graphs</title>
      <link>https://arxiv.org/abs/2411.03133</link>
      <description>arXiv:2411.03133v1 Announce Type: cross 
Abstract: The Harary reconstruction conjecture states that any graph with more than four edges can be uniquely reconstructed from its set of maximal edge-deleted subgraphs. In 1977, M\"uller verified the conjecture for graphs with $n$ vertices and $n \log_2(n)$ edges, improving on Lov\'as's bound of $\log(n^2-n)/4$. Here, we show that the reconstruction conjecture holds for graphs which have exactly one cycle and and three non-isomorphic subtrees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03133v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony E. Pizzimenti, Umarkhon Rakhimov</dc:creator>
    </item>
    <item>
      <title>Discovering Data Structures: Nearest Neighbor Search and Beyond</title>
      <link>https://arxiv.org/abs/2411.03253</link>
      <description>arXiv:2411.03253v1 Announce Type: cross 
Abstract: We propose a general framework for end-to-end learning of data structures. Our framework adapts to the underlying data distribution and provides fine-grained control over query and space complexity. Crucially, the data structure is learned from scratch, and does not require careful initialization or seeding with candidate data structures/algorithms. We first apply this framework to the problem of nearest neighbor search. In several settings, we are able to reverse-engineer the learned data structures and query algorithms. For 1D nearest neighbor search, the model discovers optimal distribution (in)dependent algorithms such as binary search and variants of interpolation search. In higher dimensions, the model learns solutions that resemble k-d trees in some regimes, while in others, they have elements of locality-sensitive hashing. The model can also learn useful representations of high-dimensional data and exploit them to design effective data structures. We also adapt our framework to the problem of estimating frequencies over a data stream, and believe it could also be a powerful discovery tool for new problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03253v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Salemohamed, Laurent Charlin, Shivam Garg, Vatsal Sharan, Gregory Valiant</dc:creator>
    </item>
    <item>
      <title>Error Interference in Quantum Simulation</title>
      <link>https://arxiv.org/abs/2411.03255</link>
      <description>arXiv:2411.03255v1 Announce Type: cross 
Abstract: Understanding algorithmic error accumulation in quantum simulation is crucial due to its fundamental significance and practical applications in simulating quantum many-body system dynamics. Conventional theories typically apply the triangle inequality to provide an upper bound for the error. However, these often yield overly conservative and inaccurate estimates as they neglect error interference -- a phenomenon where errors in different segments can destructively interfere. Here, we introduce a novel method that directly estimates the long-time algorithmic errors with multiple segments, thereby establishing a comprehensive framework for characterizing algorithmic error interference. We identify the sufficient and necessary condition for strict error interference and introduce the concept of approximate error interference, which is more broadly applicable to scenarios such as power-law interaction models, the Fermi-Hubbard model, and higher-order Trotter formulas. Our work demonstrates significant improvements over prior ones and opens new avenues for error analysis in quantum simulation, offering potential advancements in both theoretical algorithm design and experimental implementation of Hamiltonian simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03255v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boyang Chen, Jue Xu, Qi Zhao, Xiao Yuan</dc:creator>
    </item>
    <item>
      <title>Online Bipartite Matching with Advice: Tight Robustness-Consistency Tradeoffs for the Two-Stage Model</title>
      <link>https://arxiv.org/abs/2206.11397</link>
      <description>arXiv:2206.11397v3 Announce Type: replace 
Abstract: Two-stage bipartite matching is a fundamental problem of optimization under uncertainty introduced by Feng, Niazadeh, and Saberi (2021), who study it under the stochastic and adversarial paradigms of uncertainty. We propose a method to interpolate between these paradigms, using the Algorithms with Predictions (ALPS) framework. To elaborate, given some form of information (e.g. a distributional prediction) about the uncertainty, we consider the optimal decision assuming that information is correct to be some "advice", whose accuracy is unknown. In the ALPS framework, we define Consistency to be an algorithm's performance relative to the advice, and Robustness to be an algorithm's performance relative to the hindsight-optimal decision. We characterize the tight tradeoff between Consistency and Robustness for four settings of two-stage matching: unweighted, vertex-weighted, edge-weighted, and fractional budgeted allocation. Additionally, we show our algorithm achieves state-of-the-art performance in both synthetic and real-data simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.11397v3</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Billy Jin, Will Ma</dc:creator>
    </item>
    <item>
      <title>Accelerating Matroid Optimization through Fast Imprecise Oracles</title>
      <link>https://arxiv.org/abs/2402.02774</link>
      <description>arXiv:2402.02774v2 Announce Type: replace 
Abstract: Querying complex models for precise information (e.g. traffic models, database systems, large ML models) often entails intense computations and results in long response times. Thus, weaker models which give imprecise results quickly can be advantageous, provided inaccuracies can be resolved using few queries to a stronger model. In the fundamental problem of computing a maximum-weight basis of a matroid, a well-known generalization of many combinatorial optimization problems, algorithms have access to a clean oracle to query matroid information. We additionally equip algorithms with a fast but dirty oracle modelling an unknown, potentially different matroid. We design and analyze practical algorithms which only use few clean queries w.r.t. the quality of the dirty oracle, while maintaining robustness against arbitrarily poor dirty matroids, approaching the performance of classic algorithms for the given problem. Notably, we prove that our algorithms are, in many respects, best-possible. Further, we outline extensions to other matroid oracle types, non-free dirty oracles and other matroid problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02774v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franziska Eberle, Felix Hommelsheim, Alexander Lindermayr, Zhenwei Liu, Nicole Megow, Jens Schl\"oter</dc:creator>
    </item>
    <item>
      <title>Harmonic Decomposition in Data Sketches</title>
      <link>https://arxiv.org/abs/2403.15366</link>
      <description>arXiv:2403.15366v3 Announce Type: replace 
Abstract: In the turnstile streaming model, a dynamic vector $\mathbf{x}=(\mathbf{x}_1,\ldots,\mathbf{x}_n)\in \mathbb{Z}^n$ is updated by a stream of entry-wise increments/decrements. Let $f\colon\mathbb{Z}\to \mathbb{R}_+$ be a symmetric function with $f(0)=0$. The \emph{$f$-moment} of $\mathbf{x}$ is defined to be $f(\mathbf{x}) := \sum_{v\in[n]}f(\mathbf{x}_v)$. We revisit the problem of constructing a \emph{universal sketch} that can estimate many different $f$-moments. Previous constructions of universal sketches rely on the technique of sampling with respect to the $L_0$-mass (uniform samples) or $L_2$-mass ($L_2$-heavy-hitters), whose universality comes from being able to evaluate the function $f$ over the samples. In this work we take a new approach to constructing a universal sketch that does not use \emph{any} explicit samples but relies on the \emph{harmonic structure} of the target function $f$. The new sketch ($\textsf{SymmetricPoissonTower}$) \emph{embraces} hash collisions instead of avoiding them, which saves multiple $\log n$ factors in space, e.g., when estimating all $L_p$-moments ($f(z) = |z|^p,p\in[0,2]$). For many nearly periodic functions, the new sketch is \emph{exponentially} more efficient than sampling-based methods. We conjecture that the $\textsf{SymmetricPoissonTower}$ sketch is \emph{the} universal sketch that can estimate every tractable function $f$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15366v3</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dingyu Wang</dc:creator>
    </item>
    <item>
      <title>Differentially Private Kernel Density Estimation</title>
      <link>https://arxiv.org/abs/2409.01688</link>
      <description>arXiv:2409.01688v2 Announce Type: replace 
Abstract: We introduce a refined differentially private (DP) data structure for kernel density estimation (KDE), offering not only improved privacy-utility tradeoff but also better efficiency over prior results. Specifically, we study the mathematical problem: given a similarity function $f$ (or DP KDE) and a private dataset $X \subset \mathbb{R}^d$, our goal is to preprocess $X$ so that for any query $y\in\mathbb{R}^d$, we approximate $\sum_{x \in X} f(x, y)$ in a differentially private fashion. The best previous algorithm for $f(x,y) =\| x - y \|_1$ is the node-contaminated balanced binary tree by [Backurs, Lin, Mahabadi, Silwal, and Tarnawski, ICLR 2024]. Their algorithm requires $O(nd)$ space and time for preprocessing with $n=|X|$. For any query point, the query time is $d \log n$, with an error guarantee of $(1+\alpha)$-approximation and $\epsilon^{-1} \alpha^{-0.5} d^{1.5} R \log^{1.5} n$.
  In this paper, we improve the best previous result [Backurs, Lin, Mahabadi, Silwal, and Tarnawski, ICLR 2024] in three aspects:
  - We reduce query time by a factor of $\alpha^{-1} \log n$.
  - We improve the approximation ratio from $\alpha$ to 1.
  - We reduce the error dependence by a factor of $\alpha^{-0.5}$.
  From a technical perspective, our method of constructing the search tree differs from previous work [Backurs, Lin, Mahabadi, Silwal, and Tarnawski, ICLR 2024]. In prior work, for each query, the answer is split into $\alpha^{-1} \log n$ numbers, each derived from the summation of $\log n$ values in interval tree countings. In contrast, we construct the tree differently, splitting the answer into $\log n$ numbers, where each is a smart combination of two distance values, two counting values, and $y$ itself. We believe our tree structure may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01688v2</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Erzhi Liu, Jerry Yao-Chieh Hu, Alex Reneau, Zhao Song, Han Liu</dc:creator>
    </item>
    <item>
      <title>Optimal Scalarizations for Sublinear Hypervolume Regret</title>
      <link>https://arxiv.org/abs/2307.03288</link>
      <description>arXiv:2307.03288v4 Announce Type: replace-cross 
Abstract: Scalarization is a general, parallizable technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, yet some have dismissed this versatile approach because linear scalarizations cannot explore concave regions of the Pareto frontier. To that end, we aim to find simple non-linear scalarizations that provably explore a diverse set of $k$ objectives on the Pareto frontier, as measured by the dominated hypervolume. We show that hypervolume scalarizations with uniformly random weights achieves an optimal sublinear hypervolume regret bound of $O(T^{-1/k})$, with matching lower bounds that preclude any algorithm from doing better asymptotically. For the setting of multiobjective stochastic linear bandits, we utilize properties of hypervolume scalarizations to derive a novel non-Euclidean analysis to get regret bounds of $\tilde{O}( d T^{-1/2} + T^{-1/k})$, removing unnecessary $\text{poly}(k)$ dependencies. We support our theory with strong empirical performance of using non-linear scalarizations that outperforms both their linear counterparts and other standard multiobjective algorithms in a variety of natural settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03288v4</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qiuyi Zhang (Richard)</dc:creator>
    </item>
    <item>
      <title>An Algorithm for Fast and Correct Computation of Reeb Spaces for PL Bivariate Fields</title>
      <link>https://arxiv.org/abs/2403.06564</link>
      <description>arXiv:2403.06564v2 Announce Type: replace-cross 
Abstract: Reeb space is an important tool (data-structure) for topological data analysis that captures the quotient space topology of a multi-field or multiple scalar fields. For piecewise-linear (PL) bivariate fields, the Reeb spaces are $2$-dimensional polyhedrons while for PL scalar fields, the Reeb graphs (or Reeb spaces) are of dimension $1$. Efficient algorithms have been designed for computing Reeb graphs, however, computing correct Reeb spaces for PL bivariate fields, is a challenging open problem. In the current paper, we propose a novel algorithm for fast and correct computation of the Reeb space corresponding to a generic PL bivariate field defined on a triangulation $\mathbb{M}$ of a $3$-manifold without boundary, leveraging the fast algorithms for computing Reeb graphs in the literature.
  Our algorithm is based on the computation of a Multi-Dimensional Reeb Graph (MDRG) which is first proved to be homeomorphic with the Reeb space. For the correct computation of the MDRG, we compute the Jacobi set of the PL bivariate field and its projection into the Reeb space, called the Jacobi structure. Finally, the correct Reeb space is obtained by computing a net-like structure embedded in the Reeb space and then computing its $2$-sheets in the net-like structure. The time complexity of our algorithm is $\mathcal{O}(n^2 + n(c_{int})\log (n) + nc_L^2)$, where $n$ is the total number of simplices in $\mathbb{M}$, $c_{int}$ is the number of intersections of the projections of the non-adjacent Jacobi set edges on the range of the bivariate field and $c_L$ is the upper bound on the number of simplices in the link of an edge of $\mathbb{M}$. This complexity is comparable with the fastest algorithm available in the literature. Moreover, we claim to provide the first algorithm to compute the topologically correct Reeb space without using range quantization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06564v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amit Chattopadhyay, Yashwanth Ramamurthi, Osamu Saeki</dc:creator>
    </item>
    <item>
      <title>Optimal Matrix Sketching over Sliding Windows</title>
      <link>https://arxiv.org/abs/2405.07792</link>
      <description>arXiv:2405.07792v2 Announce Type: replace-cross 
Abstract: Matrix sketching, aimed at approximating a matrix $\boldsymbol{A} \in \mathbb{R}^{N\times d}$ consisting of vector streams of length $N$ with a smaller sketching matrix $\boldsymbol{B} \in \mathbb{R}^{\ell\times d}, \ell \ll N$, has garnered increasing attention in fields such as large-scale data analytics and machine learning. A well-known deterministic matrix sketching method is the Frequent Directions algorithm, which achieves the optimal $O\left(\frac{d}{\varepsilon}\right)$ space bound and provides a covariance error guarantee of $\varepsilon = \lVert \boldsymbol{A}^\top \boldsymbol{A} - \boldsymbol{B}^\top \boldsymbol{B} \rVert_2/\lVert \boldsymbol{A} \rVert_F^2$. The matrix sketching problem becomes particularly interesting in the context of sliding windows, where the goal is to approximate the matrix $\boldsymbol{A}_W$, formed by input vectors over the most recent $N$ time units. However, despite recent efforts, whether achieving the optimal $O\left(\frac{d}{\varepsilon}\right)$ space bound on sliding windows is possible has remained an open question.
  In this paper, we introduce the DS-FD algorithm, which achieves the optimal $O\left(\frac{d}{\varepsilon}\right)$ space bound for matrix sketching over row-normalized, sequence-based sliding windows. We also present matching upper and lower space bounds for time-based and unnormalized sliding windows, demonstrating the generality and optimality of \dsfd across various sliding window models. This conclusively answers the open question regarding the optimal space bound for matrix sketching over sliding windows. Furthermore, we conduct extensive experiments with both synthetic and real-world datasets, validating our theoretical claims and thus confirming the correctness and effectiveness of our algorithm, both theoretically and empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07792v2</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.14778/3665844.3665847</arxiv:DOI>
      <dc:creator>Hanyan Yin, Dongxie Wen, Jiajun Li, Zhewei Wei, Xiao Zhang, Zengfeng Huang, Feifei Li</dc:creator>
    </item>
    <item>
      <title>Sample Complexity of Posted Pricing for a Single Item</title>
      <link>https://arxiv.org/abs/2406.00819</link>
      <description>arXiv:2406.00819v2 Announce Type: replace-cross 
Abstract: Selling a single item to $n$ self-interested buyers is a fundamental problem in economics, where the two objectives typically considered are welfare maximization and revenue maximization. Since the optimal mechanisms are often impractical and do not work for sequential buyers, posted pricing mechanisms, where fixed prices are set for the item for different buyers, have emerged as a practical and effective alternative. This paper investigates how many samples are needed from buyers' value distributions to find near-optimal posted prices, considering both independent and correlated buyer distributions, and welfare versus revenue maximization. We obtain matching upper and lower bounds (up to logarithmic factors) on the sample complexity for all these settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00819v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Billy Jin, Thomas Kesselheim, Will Ma, Sahil Singla</dc:creator>
    </item>
    <item>
      <title>Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex composite losses</title>
      <link>https://arxiv.org/abs/2407.05237</link>
      <description>arXiv:2407.05237v2 Announce Type: replace-cross 
Abstract: Differentially-private stochastic gradient descent (DP-SGD) is a family of iterative machine learning training algorithms that privatize gradients to generate a sequence of differentially-private (DP) model parameters. It is also the standard tool used to train DP models in practice, even though most users are only interested in protecting the privacy of the final model. Tight DP accounting for the last iterate would minimize the amount of noise required while maintaining the same privacy guarantee and potentially increasing model utility. However, last-iterate accounting is challenging, and existing works require strong assumptions not satisfied by most implementations. These include assuming (i) the global sensitivity constant is known - to avoid gradient clipping; (ii) the loss function is Lipschitz or convex; and (iii) input batches are sampled randomly.
  In this work, we forego any unrealistic assumptions and provide privacy bounds for the most commonly used variant of DP-SGD, in which data is traversed cyclically, gradients are clipped, and only the last model is released. More specifically, we establish new Renyi differential privacy (RDP) upper bounds for the last iterate under realistic assumptions of small stepsize and Lipschitz smoothness of the loss function. Our general bounds also recover the special-case convex bounds when the weak-convexity parameter of the objective function approaches zero and no clipping is performed. The approach itself leverages optimal transport techniques for last iterate bounds, which is a nontrivial task when the data is traversed cyclically and the loss function is nonconvex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05237v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiwei Kong, M\'onica Ribero</dc:creator>
    </item>
    <item>
      <title>Improved Explicit Near-Optimal Codes in the High-Noise Regimes</title>
      <link>https://arxiv.org/abs/2410.15506</link>
      <description>arXiv:2410.15506v2 Announce Type: replace-cross 
Abstract: We study uniquely decodable codes and list decodable codes in the high-noise regime, specifically codes that are uniquely decodable from $\frac{1-\varepsilon}{2}$ fraction of errors and list decodable from $1-\varepsilon$ fraction of errors. We present several improved explicit constructions that achieve near-optimal rates, as well as efficient or even linear-time decoding algorithms. Our contributions are as follows.
  1. Explicit Near-Optimal Linear Time Uniquely Decodable Codes: We construct a family of explicit $\mathbb{F}_2$-linear codes with rate $\Omega(\varepsilon)$ and alphabet size $2^{\mathrm{poly} \log(1/\varepsilon)}$, that are capable of correcting $e$ errors and $s$ erasures whenever $2e + s &lt; (1 - \varepsilon)n$ in linear-time.
  2. Explicit Near-Optimal List Decodable Codes: We construct a family of explicit list decodable codes with rate $\Omega(\varepsilon)$ and alphabet size $2^{\mathrm{poly} \log(1/\varepsilon)}$, that are capable of list decoding from $1-\varepsilon$ fraction of errors with a list size $L = \exp\exp\exp(\log^{\ast}n)$ in polynomial time.
  3. List Decodable Code with Near-Optimal List Size: We construct a family of explicit list decodable codes with an optimal list size of $O(1/\varepsilon)$, albeit with a suboptimal rate of $O(\varepsilon^2)$, capable of list decoding from $1-\varepsilon$ fraction of errors in polynomial time. Furthermore, we introduce a new combinatorial object called multi-set disperser, and use it to give a family of list decodable codes with near-optimal rate $\frac{\varepsilon}{\log^2(1/\varepsilon)}$ and list size $\frac{\log^2(1/\varepsilon)}{\varepsilon}$, that can be constructed in probabilistic polynomial time and decoded in deterministic polynomial time.
  We also introduce new decoding algorithms that may prove valuable for other graph-based codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15506v2</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Li, Songtao Mao</dc:creator>
    </item>
    <item>
      <title>Some easy optimization problems have the overlap-gap property</title>
      <link>https://arxiv.org/abs/2411.01836</link>
      <description>arXiv:2411.01836v2 Announce Type: replace-cross 
Abstract: We show that the shortest $s$-$t$ path problem has the overlap-gap property in (i) sparse $\mathbf{G}(n,p)$ graphs and (ii) complete graphs with i.i.d. Exponential edge weights. Furthermore, we demonstrate that in sparse $\mathbf{G}(n,p)$ graphs, shortest path is solved by $O(\log n)$-degree polynomial estimators, and a uniform approximate shortest path can be sampled in polynomial time. This constitutes the first example in which the overlap-gap property is not predictive of algorithmic intractability for a (non-algebraic) average-case optimization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01836v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuangping Li, Tselil Schramm</dc:creator>
    </item>
    <item>
      <title>An Exponential Separation Between Quantum and Quantum-Inspired Classical Algorithms for Machine Learning</title>
      <link>https://arxiv.org/abs/2411.02087</link>
      <description>arXiv:2411.02087v2 Announce Type: replace-cross 
Abstract: Achieving a provable exponential quantum speedup for an important machine learning task has been a central research goal since the seminal HHL quantum algorithm for solving linear systems and the subsequent quantum recommender systems algorithm by Kerenidis and Prakash. These algorithms were initially believed to be strong candidates for exponential speedups, but a lower bound ruling out similar classical improvements remained absent. In breakthrough work by Tang, it was demonstrated that this lack of progress in classical lower bounds was for good reasons. Concretely, she gave a classical counterpart of the quantum recommender systems algorithm, reducing the quantum advantage to a mere polynomial. Her approach is quite general and was named quantum-inspired classical algorithms. Since then, almost all the initially exponential quantum machine learning speedups have been reduced to polynomial via new quantum-inspired classical algorithms. From the current state-of-affairs, it is unclear whether we can hope for exponential quantum speedups for any natural machine learning task.
  In this work, we present the first such provable exponential separation between quantum and quantum-inspired classical algorithms. We prove the separation for the basic problem of solving a linear system when the input matrix is well-conditioned and has sparse rows and columns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02087v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allan Gr{\o}nlund, Kasper Green Larsen</dc:creator>
    </item>
  </channel>
</rss>
