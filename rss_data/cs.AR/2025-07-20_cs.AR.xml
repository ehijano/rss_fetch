<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Jul 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning</title>
      <link>https://arxiv.org/abs/2507.13355</link>
      <description>arXiv:2507.13355v1 Announce Type: new 
Abstract: Leveraging artificial intelligence (AI)-driven electronic design and automation (EDA) tools, high-performance computing, and parallelized algorithms are essential for next-generation microprocessor innovation, ensuring continued progress in computing, AI, and semiconductor technology. Machine learning-based design rule checking (DRC) and lithography hotspot detection can improve first-pass silicon success. However, conventional ML and neural network (NN)-based models use supervised learning and require a large balanced dataset (in terms of positive and negative classes) and training time. This research addresses those key challenges by proposing the first-ever unsupervised DRC violation prediction methodology. The proposed model can be built using any unbalanced dataset using only one class and set a threshold for it, then fitting any new data querying if they are within the boundary of the model for classification. This research verified the proposed model by implementing different computational cores using CMOS 28 nm technology and Synopsys Design Compiler and IC Compiler II tools. Then, layouts were divided into virtual grids to collect about 60k data for analysis and verification. The proposed method has 99.95% prediction test accuracy, while the existing support vector machine (SVM) and neural network (NN) models have 85.44\% and 98.74\% accuracy, respectively. In addition, the proposed methodology has about 26.3x and up to 6003x lower training times compared to SVM and NN-models, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13355v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riadul Islam, Dhandeep Challagundla</dc:creator>
    </item>
    <item>
      <title>VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation</title>
      <link>https://arxiv.org/abs/2507.13369</link>
      <description>arXiv:2507.13369v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are gaining popularity for hardware design automation, particularly through Register Transfer Level (RTL) code generation. In this work, we examine the current literature on RTL generation using LLMs and identify key requirements for training and fine-tuning datasets. We construct a robust Verilog dataset through an automated three-pronged process involving database (DB) creation and management with PostgreSQL, data collection from code hosting sites like OpenCores and GitHub, and data preprocessing to verify the codes' syntax, run logic synthesis, and extract relevant module metadata. We implement a scalable and efficient DB infrastructure to support analysis and detail our preprocessing pipeline to enforce high-quality data before DB insertion. The resulting dataset comprises 20,392 Verilog samples, 751 MB of Verilog code data, which is the largest high-quality Verilog dataset for LLM fine-tuning to our knowledge. We further evaluate the dataset, address associated challenges, and explore potential applications for future research and development in LLM-based hardware generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13369v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul E. Calzada, Zahin Ibnat, Tanvir Rahman, Kamal Kandula, Danyu Lu, Sujan Kumar Saha, Farimah Farahmandi, Mark Tehranipoor</dc:creator>
    </item>
    <item>
      <title>GAP-LA: GPU-Accelerated Performance-Driven Layer Assignment</title>
      <link>https://arxiv.org/abs/2507.13375</link>
      <description>arXiv:2507.13375v1 Announce Type: new 
Abstract: Layer assignment is critical for global routing of VLSI circuits. It converts 2D routing paths into 3D routing solutions by determining the proper metal layer for each routing segments to minimize congestion and via count. As different layers have different unit resistance and capacitance, layer assignment also has significant impacts to timing and power. With growing design complexity, it becomes increasingly challenging to simultaneously optimize timing, power, and congestion efficiently. Existing studies are mostly limited to a subset of objectives. In this paper, we propose a GPU-accelerated performance-driven layer assignment framework, GAP-LA, for holistic optimization the aforementioned objectives. Experimental results demonstrate that we can achieve 0.3%-9.9% better worst negative slack (WNS) and 2.0%-5.4% better total negative slack (TNS) while maintaining power and congestion with competitive runtime compared with ISPD 2025 contest winners, especially on designs with up to 12 millions of nets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13375v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunyuan Zhao, Zizheng Guo, Zuodong Zhang, Yibo Lin</dc:creator>
    </item>
    <item>
      <title>4T2R X-ReRAM CiM Array for Variation-tolerant, Low-power, Massively Parallel MAC Operation</title>
      <link>https://arxiv.org/abs/2507.13631</link>
      <description>arXiv:2507.13631v1 Announce Type: new 
Abstract: Computation-in-Memory (CiM) is attracting attention as a technology that can perform MAC calculations required for AI accelerators, at high speed with low power consumption. However, there is a problem regarding power consumption and device-derived errors that increase as row parallelism increases. In this paper, a 4T2R ReRAM cell and an 8T SRAM CiM suitable for CiM is proposed. It is shown that adopting the proposed 4T2R ReRAM cell reduces the errors due to variation in ReRAM devices compared to conventional 4T4R ReRAM cells.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13631v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fuyuki Kihara, Seiji Uenohara, Satoshi Awamura, Naoko Misawa, Chihiro Matsui, Ken Takeuchi</dc:creator>
    </item>
    <item>
      <title>An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC</title>
      <link>https://arxiv.org/abs/2507.13736</link>
      <description>arXiv:2507.13736v1 Announce Type: cross 
Abstract: This work presents a multi-layer DNN scheduling framework as an extension of OctopuScheduler, providing an end-to-end flow from PyTorch models to inference on a single SpiNNaker2 chip. Together with a front-end comprised of quantization and lowering steps, the proposed framework enables the edge-based execution of large and complex DNNs up to transformer scale using the neuromorphic platform SpiNNaker2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13736v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Jobst, Tim Langer, Chen Liu, Mehmet Alici, Hector A. Gonzalez, Christian Mayr</dc:creator>
    </item>
    <item>
      <title>Fast Graph Vector Search via Hardware Acceleration and Delayed-Synchronization Traversal</title>
      <link>https://arxiv.org/abs/2406.12385</link>
      <description>arXiv:2406.12385v2 Announce Type: replace 
Abstract: Vector search systems are indispensable in large language model (LLM) serving, search engines, and recommender systems, where minimizing online search latency is essential. Among various algorithms, graph-based vector search (GVS) is particularly popular due to its high search performance and quality. However, reducing GVS latency by intra-query parallelization remains challenging due to limitations imposed by both existing hardware architectures (CPUs and GPUs) and the inherent difficulty of parallelizing graph traversals. To efficiently serve low-latency GVS, we co-design hardware and algorithm by proposing Falcon and Delayed-Synchronization Traversal (DST). Falcon is a hardware GVS accelerator that implements efficient GVS operators, pipelines these operators, and reduces memory accesses by tracking search states with an on-chip Bloom filter. DST is an efficient graph traversal algorithm that simultaneously improves search performance and quality by relaxing traversal orders to maximize accelerator utilization. Evaluation across various graphs and datasets shows that Falcon, prototyped on FPGAs, together with DST, achieves up to 4.3x and 19.5x lower latency and up to 8.0x and 26.9x improvements in energy efficiency over CPU- and GPU-based GVS systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12385v2</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.14778/3749646.3749655</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the VLDB Endowment Volume 18, 2025</arxiv:journal_reference>
      <dc:creator>Wenqi Jiang, Hang Hu, Torsten Hoefler, Gustavo Alonso</dc:creator>
    </item>
    <item>
      <title>LASANA: Large-Scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration</title>
      <link>https://arxiv.org/abs/2507.10748</link>
      <description>arXiv:2507.10748v2 Announce Type: replace 
Abstract: Neuromorphic systems using in-memory or event-driven computing are motivated by the need for more energy-efficient processing of artificial intelligence workloads. Emerging neuromorphic architectures aim to combine traditional digital designs with the computational efficiency of analog computing and novel device technologies. A crucial problem in the rapid exploration and co-design of such architectures is the lack of tools for fast and accurate modeling and simulation. Typical mixed-signal design tools integrate a digital simulator with an analog solver like SPICE, which is prohibitively slow for large systems. By contrast, behavioral modeling of analog components is faster, but existing approaches are fixed to specific architectures with limited energy and performance modeling. In this paper, we propose LASANA, a novel approach that leverages machine learning to derive data-driven surrogate models of analog sub-blocks in a digital backend architecture. LASANA uses SPICE-level simulations of a circuit to train ML models that predict circuit energy, performance, and behavior at analog/digital interfaces. Such models can provide energy and performance annotation on top of existing behavioral models or function as replacements to analog simulation. We apply LASANA to an analog crossbar array and a spiking neuron circuit. Running MNIST and spiking MNIST, LASANA surrogates demonstrate up to three orders of magnitude speedup over SPICE, with energy, latency, and behavioral error less than 7%, 8%, and 2%, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10748v2</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason Ho, James A. Boyle, Linshen Liu, Andreas Gerstlauer</dc:creator>
    </item>
  </channel>
</rss>
