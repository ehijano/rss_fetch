<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 01:42:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A minimalistic and general weighted averaging method for inconsistent data</title>
      <link>https://arxiv.org/abs/2406.08293</link>
      <description>arXiv:2406.08293v1 Announce Type: new 
Abstract: The weighted average of inconsistent data is a common and tedious problem that many scientists have encountered. The standard weighted average is not recommended for these cases, and different alternative methods are proposed in the literature. Here, we introduce a new method based on Bayesian statistics for a broad application that keeps the number of assumptions to a minimum. The uncertainty associated with each input value is considered just a lower bound of the true unknown uncertainty. By assuming a non-informative (Jeffreys') prior for true uncertainty and marginalising over its value, a modified Gaussian distribution is obtained with smoothly decreasing wings, which allows for a better treatment of scattered data and outliers. The proposed method is tested on a series of data sets: simulations, CODATA recommended value of the Newtonian gravitational constant, and some particle properties from the Particle Data Group, including the proton charge radius and the mass of the W boson. For the latter in particular, contrary to other works, our prediction lies in good agreement with the Standard Model. A freely available Python library is also provided for a simple implementation of our averaging method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08293v1</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martino Trassinelli, Marleen Maxton</dc:creator>
    </item>
    <item>
      <title>High-Precision Surrogate Modeling for Uncertainty Quantification in Complex Slurry Flows</title>
      <link>https://arxiv.org/abs/2406.07758</link>
      <description>arXiv:2406.07758v1 Announce Type: cross 
Abstract: Slurry transportation via pipelines is essential for global industries, offering efficiency and environmental benefits. Specifically, the precise calibration of physical parameters for transporting raw phosphate material to fertilizer plants is crucial to minimize energy losses and ensure secure operations. Computational fluid dynamics (CFD) is commonly employed to understand solid concentration, velocity distributions, and flow pressure along the pipeline. However, numerical solutions for slurry flows often entail uncertainties from initial and boundary conditions, emphasizing the need for quantification. This study addresses the challenge by proposing a framework that combines proper orthogonal decomposition and polynomial chaos expansions to quantify uncertainties in two-dimensional phosphate slurry flow simulations. The use of surrogate modeling methods, like polynomial chaos expansion, proves effective in reducing computational costs associated with direct stochastic simulations, especially for complex flows with high spatial variability, as observed in phosphate slurries. Numerical results demonstrate the accuracy of the non-intrusive reduction method in reproducing mean and variance distributions. Moreover, the uncertainty quantification analysis shows that the reduced-order model significantly reduces computational costs compared to the full-order model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07758v1</guid>
      <category>physics.flu-dyn</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marwane Elkarii, Radouan Boukharfane, Nabil El Mo\c{c}ayd</dc:creator>
    </item>
    <item>
      <title>Deep Learning of Structural Morphology Imaged by Scanning X-ray Diffraction Microscopy</title>
      <link>https://arxiv.org/abs/2406.07761</link>
      <description>arXiv:2406.07761v1 Announce Type: cross 
Abstract: Scanning X-ray nanodiffraction microscopy is a powerful technique for spatially resolving nanoscale structural morphologies by diffraction contrast. One of the critical challenges in experimental nanodiffraction data analysis is posed by the convergence angle of nanoscale focusing optics which creates simultaneous dependency of the far-field scattering data on three independent components of the local strain tensor - corresponding to dilation and two potential rigid body rotations of the unit cell. All three components are in principle resolvable through a spatially mapped sample tilt series however traditional data analysis is computationally expensive and prone to artifacts. In this study, we implement NanobeamNN, a convolutional neural network specifically tailored to the analysis of scanning probe X-ray microscopy data. NanobeamNN learns lattice strain and rotation angles from simulated diffraction of a focused X-ray nanobeam by an epitaxial thin film and can directly make reasonable predictions on experimental data without the need for additional fine-tuning. We demonstrate that this approach represents a significant advancement in computational speed over conventional methods, as well as a potential improvement in accuracy over the current standard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07761v1</guid>
      <category>physics.app-ph</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>physics.ins-det</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aileen Luo, Tao Zhou, Martin V. Holt, Andrej Singer, Mathew J. Cherukara</dc:creator>
    </item>
  </channel>
</rss>
