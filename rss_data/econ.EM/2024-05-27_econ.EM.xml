<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 May 2024 04:01:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Modularity, Higher-Order Recombination, and New Venture Success</title>
      <link>https://arxiv.org/abs/2405.15042</link>
      <description>arXiv:2405.15042v1 Announce Type: new 
Abstract: Modularity is critical for the emergence and evolution of complex social, natural, and technological systems robust to exploratory failure. We consider this in the context of emerging business organizations, which can be understood as complex systems. We build a theory of organizational emergence as higher-order, modular recombination wherein successful start-ups assemble novel combinations of successful modular components, rather than engage in the lower-order combination of disparate, singular components. Lower-order combinations are critical for long-term socio-economic transformation, but manifest diffuse benefits requiring support as public goods. Higher-order combinations facilitate rapid experimentation and attract private funding. We evaluate this with U.S. venture-funded start-ups over 45 years using company descriptions. We build a dynamic semantic space with word embedding models constructed from evolving business discourse, which allow us to measure the modularity of and distance between new venture components. Using event history models, we demonstrate how ventures more likely achieve successful IPOs and high-priced acquisitions when they combine diverse modules of clustered components. We demonstrate how higher-order combination enables venture success by accelerating firm development and diversifying investment, and we reflect on its implications for social innovation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15042v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Likun Cao, Ziwen Chen, James Evans</dc:creator>
    </item>
    <item>
      <title>Generating density nowcasts for U.S. GDP growth with deep learning: Bayes by Backprop and Monte Carlo dropout</title>
      <link>https://arxiv.org/abs/2405.15579</link>
      <description>arXiv:2405.15579v1 Announce Type: new 
Abstract: Recent results in the literature indicate that artificial neural networks (ANNs) can outperform the dynamic factor model (DFM) in terms of the accuracy of GDP nowcasts. Compared to the DFM, the performance advantage of these highly flexible, nonlinear estimators is particularly evident in periods of recessions and structural breaks. From the perspective of policy-makers, however, nowcasts are the most useful when they are conveyed with uncertainty attached to them. While the DFM and other classical time series approaches analytically derive the predictive (conditional) distribution for GDP growth, ANNs can only produce point nowcasts based on their default training procedure (backpropagation). To fill this gap, first in the literature, we adapt two different deep learning algorithms that enable ANNs to generate density nowcasts for U.S. GDP growth: Bayes by Backprop and Monte Carlo dropout. The accuracy of point nowcasts, defined as the mean of the empirical predictive distribution, is evaluated relative to a naive constant growth model for GDP and a benchmark DFM specification. Using a 1D CNN as the underlying ANN architecture, both algorithms outperform those benchmarks during the evaluation period (2012:Q1 -- 2022:Q4). Furthermore, both algorithms are able to dynamically adjust the location (mean), scale (variance), and shape (skew) of the empirical predictive distribution. The results indicate that both Bayes by Backprop and Monte Carlo dropout can effectively augment the scope and functionality of ANNs, rendering them a fully compatible and competitive alternative for classical time series approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15579v1</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Krist\'of N\'emeth, D\'aniel Hadh\'azi</dc:creator>
    </item>
    <item>
      <title>Empirical Crypto Asset Pricing</title>
      <link>https://arxiv.org/abs/2405.15716</link>
      <description>arXiv:2405.15716v1 Announce Type: new 
Abstract: We motivate the study of the crypto asset class with eleven empirical facts, and study the drivers of crypto asset returns through the lens of univariate factors. We argue crypto assets are a new, attractive, and independent asset class. In a novel and rigorously built panel of crypto assets, we examine pricing ability of sixty three asset characteristics to find rich signal content across the characteristics and at several future horizons. Only univariate financial factors (i.e., functions of previous returns) were associated with statistically significant long-short strategies, suggestive of speculatively driven returns as opposed to more fundamental pricing factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15716v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adam Baybutt</dc:creator>
    </item>
    <item>
      <title>Dynamic Latent-Factor Model with High-Dimensional Asset Characteristics</title>
      <link>https://arxiv.org/abs/2405.15721</link>
      <description>arXiv:2405.15721v1 Announce Type: new 
Abstract: We develop novel estimation procedures with supporting econometric theory for a dynamic latent-factor model with high-dimensional asset characteristics, that is, the number of characteristics is on the order of the sample size. Utilizing the Double Selection Lasso estimator, our procedure employs regularization to eliminate characteristics with low signal-to-noise ratios yet maintains asymptotically valid inference for asset pricing tests. The crypto asset class is well-suited for applying this model given the limited number of tradable assets and years of data as well as the rich set of available asset characteristics. The empirical results present out-of-sample pricing abilities and risk-adjusted returns for our novel estimator as compared to benchmark methods. We provide an inference procedure for measuring the risk premium of an observable nontradable factor, and employ this to find that the inflation-mimicking portfolio in the crypto asset class has positive risk compensation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15721v1</guid>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adam Baybutt</dc:creator>
    </item>
    <item>
      <title>YUI: Day-ahead Electricity Price Forecasting Using Invariance Simplified Supply and Demand Curve</title>
      <link>https://arxiv.org/abs/2405.14893</link>
      <description>arXiv:2405.14893v1 Announce Type: cross 
Abstract: In day-ahead electricity market, it is crucial for all market participants to have access to reliable and accurate price forecasts for their decision-making processes. Forecasting methods currently utilized in industrial applications frequently neglect the underlying mechanisms of price formation, while economic research from the perspective of supply and demand have stringent data collection requirements, making it difficult to apply in actual markets. Observing the characteristics of the day-ahead electricity market, we introduce two invariance assumptions to simplify the modeling of supply and demand curves. Upon incorporating the time invariance assumption, we can forecast the supply curve using the market equilibrium points from multiple time slots in the recent period. By introducing the price insensitivity assumption, we can approximate the demand curve using a straight line. The point where these two curves intersect provides us with the forecast price. The proposed model, forecasting suppl\textbf{Y} and demand cUrve simplified by Invariance, termed as YUI, is more efficient than state-of-the-art methods. Our experiment results in Shanxi day-ahead electricity market show that compared with existing methods, YUI can reduce forecast error by 13.8\% in MAE and 28.7\% in sMAPE. Code is publicly available at https://github.com/wangln19/YUI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14893v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linian Wang, Anlan Yu, Jianghong Liu, Huibing Zhang, Leye Wang</dc:creator>
    </item>
    <item>
      <title>Transfer Learning for Spatial Autoregressive Models</title>
      <link>https://arxiv.org/abs/2405.15600</link>
      <description>arXiv:2405.15600v1 Announce Type: cross 
Abstract: The spatial autoregressive (SAR) model has been widely applied in various empirical economic studies to characterize the spatial dependence among subjects. However, the precision of estimating the SAR model diminishes when the sample size of the target data is limited. In this paper, we propose a new transfer learning framework for the SAR model to borrow the information from similar source data to improve both estimation and prediction. When the informative source data sets are known, we introduce a two-stage algorithm, including a transferring stage and a debiasing stage, to estimate the unknown parameters and also establish the theoretical convergence rates for the resulting estimators. If we do not know which sources to transfer, a transferable source detection algorithm is proposed to detect informative sources data based on spatial residual bootstrap to retain the necessary spatial dependence. Its detection consistency is also derived. Simulation studies demonstrate that using informative source data, our transfer learning algorithm significantly enhances the performance of the classical two-stage least squares estimator. In the empirical application, we apply our method to the election prediction in swing states in the 2020 U.S. presidential election, utilizing polling data from the 2016 U.S. presidential election along with other demographic and geographical data. The empirical results show that our method outperforms traditional estimation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15600v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Zeng, Wei Zhong, Xingbai Xu</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Partial Effects in Nonlinear Semiparametric Panel Models</title>
      <link>https://arxiv.org/abs/2105.12891</link>
      <description>arXiv:2105.12891v5 Announce Type: replace 
Abstract: Average partial effects (APEs) are often not point identified in panel models with unrestricted unobserved individual heterogeneity, such as a binary response panel model with fixed effects and logistic errors as a special case. This lack of point identification occurs despite the identification of these models' common coefficients. We provide a unified framework to establish the point identification of various partial effects in a wide class of nonlinear semiparametric models under an index sufficiency assumption on the unobserved heterogeneity, even when the error distribution is unspecified and non-stationary. This assumption does not impose parametric restrictions on the unobserved heterogeneity and idiosyncratic errors. We also present partial identification results when the support condition fails. We then propose three-step semiparametric estimators for APEs, average structural functions, and average marginal effects, and show their consistency and asymptotic normality. Finally, we illustrate our approach in a study of determinants of married women's labor supply.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.12891v5</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Liu, Alexandre Poirier, Ji-Liang Shiu</dc:creator>
    </item>
    <item>
      <title>The Chained Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2301.01085</link>
      <description>arXiv:2301.01085v3 Announce Type: replace 
Abstract: This paper studies the identification, estimation, and inference of long-term (binary) treatment effect parameters when balanced panel data is not available, or consists of only a subset of the available data. We develop a new estimator: the chained difference-in-differences, which leverages the overlapping structure of many unbalanced panel data sets. This approach consists in aggregating a collection of short-term treatment effects estimated on multiple incomplete panels. Our estimator accommodates (1) multiple time periods, (2) variation in treatment timing, (3) treatment effect heterogeneity, (4) general missing data patterns, and (5) sample selection on observables. We establish the asymptotic properties of the proposed estimator and discuss identification and efficiency gains in comparison to existing methods. Finally, we illustrate its relevance through (i) numerical simulations, and (ii) an application about the effects of an innovation policy in France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.01085v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bell\'ego, David Benatia, Vincent Dortet-Bernardet</dc:creator>
    </item>
    <item>
      <title>Sharp and Robust Estimation of Partially Identified Discrete Response Models</title>
      <link>https://arxiv.org/abs/2310.02414</link>
      <description>arXiv:2310.02414v3 Announce Type: replace 
Abstract: Semiparametric discrete choice models are widely used in a variety of practical applications. While these models are point identified in the presence of continuous covariates, they can become partially identified when covariates are discrete. In this paper we find that classic estimators, including the maximum score estimator, (Manski (1975)), loose their attractive statistical properties without point identification. First of all, they are not sharp with the estimator converging to an outer region of the identified set, (Komarova (2013)), and in many discrete designs it weakly converges to a random set. Second, they are not robust, with their distribution limit discontinuously changing with respect to the parameters of the model. We propose a novel class of estimators based on the concept of a quantile of a random set, which we show to be both sharp and robust. We demonstrate that our approach extends from cross-sectional settings to class static and dynamic discrete panel data models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02414v3</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shakeeb Khan, Tatiana Komarova, Denis Nekipelov</dc:creator>
    </item>
    <item>
      <title>Real-time Prediction of the Great Recession and the Covid-19 Recession</title>
      <link>https://arxiv.org/abs/2310.08536</link>
      <description>arXiv:2310.08536v5 Announce Type: replace 
Abstract: This paper uses standard and penalized logistic regression models to predict the Great Recession and the Covid-19 recession in the US in real time. It examines the predictability of various macroeconomic and financial indicators with respect to the NBER recession indicator. The findings strongly support the use of penalized logistic regression models in recession forecasting. These models, particularly the ridge logistic regression model, outperform the standard logistic regression model in predicting the Great Recession in the US across different forecast horizons. The study also confirms the traditional significance of the term spread as an important recession indicator. However, it acknowledges that the Covid-19 recession remains unpredictable due to the unprecedented nature of the pandemic. The results are validated by creating a recession indicator through principal component analysis (PCA) on selected variables, which strongly correlates with the NBER recession indicator and is less affected by publication lags.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08536v5</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seulki Chung</dc:creator>
    </item>
    <item>
      <title>Optimal Categorical Instrumental Variables</title>
      <link>https://arxiv.org/abs/2311.17021</link>
      <description>arXiv:2311.17021v2 Announce Type: replace 
Abstract: This paper discusses estimation with a categorical instrumental variable in settings with potentially few observations per category. The proposed categorical instrumental variable estimator (CIV) leverages a regularization assumption that implies existence of a latent categorical variable with fixed finite support achieving the same first stage fit as the observed instrument. In asymptotic regimes that allow the number of observations per category to grow at arbitrary small polynomial rate with the sample size, I show that when the cardinality of the support of the optimal instrument is known, CIV is root-n asymptotically normal, achieves the same asymptotic variance as the oracle IV estimator that presumes knowledge of the optimal instrument, and is semiparametrically efficient under homoskedasticity. Under-specifying the number of support points reduces efficiency but maintains asymptotic normality. In an application that leverages judge fixed effects as instruments, CIV compares favorably to commonly used jackknife-based instrumental variable estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17021v2</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Wiemann</dc:creator>
    </item>
  </channel>
</rss>
