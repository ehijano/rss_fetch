<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Oct 2025 01:48:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Extending Prediction-Powered Inference through Conformal Prediction</title>
      <link>https://arxiv.org/abs/2510.16166</link>
      <description>arXiv:2510.16166v1 Announce Type: new 
Abstract: Prediction-powered inference is a recent methodology for the safe use of black-box ML models to impute missing data, strengthening inference of statistical parameters. However, many applications require strong properties besides valid inference, such as privacy, robustness or validity under continuous distribution shifts; deriving prediction-powered methods with such guarantees is generally an arduous process, and has to be done case by case. In this paper, we resolve this issue by connecting prediction-powered inference with conformal prediction: by performing imputation through a calibrated conformal set-predictor, we attain validity while achieving additional guarantees in a natural manner. We instantiate our procedure for the inference of means, Z- and M-estimation, as well as e-values and e-value-based procedures. Furthermore, in the case of e-values, ours is the first general prediction-powered procedure that operates off-line. We demonstrate these advantages by applying our method on private and time-series data. Both tasks are nontrivial within the standard prediction-powered framework but become natural under our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16166v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Csillag, Pedro Dall'Antonia, Claudio Jos\'e Struchiner, Guilherme Tegoni Goedert</dc:creator>
    </item>
    <item>
      <title>Estimate Time-Varying Exposure Effects via Ensemble Learning-based Marginal Structural Model with Application to Adolescent Cognitive Development Study</title>
      <link>https://arxiv.org/abs/2510.16298</link>
      <description>arXiv:2510.16298v1 Announce Type: new 
Abstract: Evaluating the effects of time-varying exposures is essential for longitudinal studies. The effect estimation becomes increasingly challenging when dealing with hundreds of time-dependent confounders. We propose a Marginal Structure Ensemble Learning Model (MASE) to provide a marginal structure model (MSM)-based robust estimator under the longitudinal setting. The proposed model integrates multiple machine learning algorithms to model propensity scores and a sequence of conditional outcome means such that it becomes less sensitive to model mis-specification due to any single algorithm and allows many confounders with potential non-linear confounding effects to reduce the risk of inconsistent estimation. Extensive simulation analysis demonstrates the superiority of MASE over benchmark methods (e.g., MSM, G-computation, Targeted maximum likelihood), yielding smaller estimation bias and improved inference accuracy. We apply MASE to the adolescent cognitive development study to investigate the time-varying effects of sleep insufficiency on cognitive performance. The results reveal an aggregated negative impact of insufficient sleep on cognitive development among youth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16298v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiwei Zhao, Chixiang Chen, Shuo Chen</dc:creator>
    </item>
    <item>
      <title>Heterogeneity-Aware Federated Causal Inference Leveraging Effect-Measure Transportability</title>
      <link>https://arxiv.org/abs/2510.16317</link>
      <description>arXiv:2510.16317v1 Announce Type: new 
Abstract: Federated learning of causal estimands offers a powerful strategy to improve estimation efficiency by leveraging data from multiple study sites while preserving privacy. Existing literature has primarily focused on the average treatment effect using single data source, whereas our work addresses a broader class of causal measures across multiple sources. We derive and compare semiparametrically efficient estimators under two transportability assumptions, which impose different restrictions on the data likelihood and illustrate the efficiency-robustness tradeoff. This estimator also permits the incorporation of flexible machine learning algorithms for nuisance functions while maintaining parametric convergence rates and nominal coverage. To further handle scenarios where some source sites violate transportability, we propose a Post-Federated Weighting Selection (PFWS) framework, which is a two-step procedure that adaptively identifies compatible sites and achieves the semiparametric efficiency bound asymptotically. This framework mitigates the efficiency loss of weighting methods and the instability and computational burden of direct site selection in finite samples. Through extensive simulations and real-data analysis, we demonstrate that our PFWS framework achieves superior variance efficiency compared with the target-only analyses across diverse transportability scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16317v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siqi Cao, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Common-Individual Embedding for Dynamic Networks with Temporal Group Structure</title>
      <link>https://arxiv.org/abs/2510.16337</link>
      <description>arXiv:2510.16337v1 Announce Type: new 
Abstract: We propose STANE (Shared and Time-specific Adaptive Network Embedding), a new joint embedding framework for dynamic networks that captures both stable global structures and localized temporal variations. To further improve the model's adaptability to transient changes, we introduce Sparse STANE, which models time-specific changes as sparse perturbations, thereby improving interpretability. Unlike existing methods that either overlook cross-time similarities or enforce overly smooth evolution, Sparse STANE integrates temporal clustering with sparse deviation modeling to strike a flexible balance between persistence and change. We also provide non-asymptotic error guarantees of embeddings and show that our estimator can reliably identify changed node pairs when deviations are sparse. On synthetic and real-world political conflict networks, STANE and its extensions improve temporal clustering accuracy and structural recovery, outperforming state-of-the-art baselines. These results highlight the potential of STANE in applications such as international relations modeling, where both persistent and transient connections matter. Our findings underscore the power of structured dynamic embeddings for revealing interpretable patterns in network evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16337v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hairi Bai, Xinyan Fan, Kuangnan Fang, Yan Zhang</dc:creator>
    </item>
    <item>
      <title>A Semiparametric Gaussian Mixture Model with Spatial Dependence and Its Application to Whole-Slide Image Clustering Analysis</title>
      <link>https://arxiv.org/abs/2510.16421</link>
      <description>arXiv:2510.16421v1 Announce Type: new 
Abstract: We develop here a semiparametric Gaussian mixture model (SGMM) for unsupervised learning with valuable spatial information taken into consideration. Specifically, we assume for each instance a random location. Then, conditional on this random location, we assume for the feature vector a standard Gaussian mixture model (GMM). The proposed SGMM allows the mixing probability to be nonparametrically related to the spatial location. Compared with a classical GMM, SGMM is considerably more flexible and allows the instances from the same class to be spatially clustered. To estimate the SGMM, novel EM algorithms are developed and rigorous asymptotic theories are established. Extensive numerical simulations are conducted to demonstrate our finite sample performance. For a real application, we apply our SGMM method to the CAMELYON16 dataset of whole-slide images (WSIs) for breast cancer detection. The SGMM method demonstrates outstanding clustering performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16421v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Baichen Yu, Jin Liu, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Spatial Scalar-on-Function Quantile Regression Model</title>
      <link>https://arxiv.org/abs/2510.16429</link>
      <description>arXiv:2510.16429v1 Announce Type: new 
Abstract: This paper introduces a novel spatial scalar-on-function quantile regression model that extends classical scalar-on-function models to account for spatial dependence and heterogeneous conditional distributions. The proposed model incorporates spatial autocorrelation through a spatially lagged response and characterizes the entire conditional distribution of a scalar outcome given a functional predictor. To address the endogeneity induced by the spatial lag term, we develop two robust estimation procedures based on instrumental variable strategies. $\sqrt{n}$-consistency and asymptotic normality of the proposed estimators are established under mild regularity conditions. We demonstrate through extensive Monte Carlo simulations that the proposed estimators outperform existing mean-based and robust alternatives, particularly in settings with strong spatial dependence and outlier contamination. We apply our method to high-resolution environmental data from the Lombardy region in Italy, using daily ozone trajectories to predict daily mean particulate matter with a diameter of less than 2.5 micrometers concentrations. The empirical results confirm the superiority of our approach in predictive accuracy, robustness, and interpretability across various quantile levels. Our method has been implemented in the \texttt{ssofqrm} R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16429v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Muge Mutis, Ufuk Beyaztas, Filiz Karaman, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Rank-based concordance for zero-inflated data: New representations, estimators, and sharp bounds</title>
      <link>https://arxiv.org/abs/2510.16504</link>
      <description>arXiv:2510.16504v1 Announce Type: new 
Abstract: Quantifying concordance between two random variables is crucial in applications. Traditional estimation techniques for commonly used concordance measures, such as Gini's gamma or Spearman's rho, often fail when data contain ties. This is particularly problematic for zero-inflated data, characterized by a combination of discrete mass in zero and a continuous component, which frequently appear in insurance, weather forecasting, and biomedical applications. This study provides a new formulation of Gini's gamma and Spearman's footrule, two rank-based concordance measures that incorporate absolute rank differences, tailored to zero-inflated continuous distributions. Along the way, we correct an expression of Spearman's rho for zero-inflated data previously presented in the literature. The best-possible upper and lower bounds for these measures in zero-inflated continuous settings are established, making the estimators useful and interpretable in practice. We pair our theoretical results with simulations and two real-life applications in insurance and weather forecasting, respectively. Our results illustrate the impact of zero inflation on dependence estimation, emphasizing the benefits of appropriately adjusted zero-inflated measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16504v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasper Arends, Guanjie Lyu, Mhamed Mesfioui, Elisa Perrone, Julien Trufin</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis to Unobserved Confounders: A Comparative Review to Estimate Confounding Strength in Sensitivity Models</title>
      <link>https://arxiv.org/abs/2510.16560</link>
      <description>arXiv:2510.16560v1 Announce Type: new 
Abstract: Causal inference is only valid when its underlying assumptions are satisfied, one of the most central being the ignorability assumption (also known as unconfoundedness or exogeneity). In practice, however, this assumption is often unrealistic in observational studies, as some confounding variables may remain unobserved. To address this limitation, sensitivity models for Inverse Probability Weighting (IPW) estimators, known as Marginal Sensitivity Models, have been introduced, allowing for a controlled relaxation of ignorability. Over the past decades, a substantial body of literature has emerged around these models, aiming to derive sharp and robust bounds for both binary and continuous treatment effects. A key element of these approaches is the specification of a sensitivity parameter, sometimes referred to as the "confounding strength", which quantifies the extent of deviation from ignorability. Yet, determining an appropriate value for this parameter is challenging, and the final interpretation of sensitivity analyses can be unclear. We believe these difficulties represent major obstacles to the adoption of such methods in practice. In this review, after introducing sensitivity analyses for IPW estimators, we focus on different strategies to estimate or lower bound the confounding strength, select the most suitable approach, and avoid common pitfalls in the interpretation of results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16560v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jean-Baptiste Baitairian, Bernard Sebastien, Rana Jreich, Sandrine Katsahian, Agathe Guilloux</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of causal mechanisms in cluster-randomized trials with post-treatment confounding using Bayesian nonparametrics</title>
      <link>https://arxiv.org/abs/2510.16673</link>
      <description>arXiv:2510.16673v1 Announce Type: new 
Abstract: Causal mediation analysis in cluster-randomized trials (CRTs) is essential for explaining how cluster-level interventions affect individual outcomes, yet it is complicated by interference, post-treatment confounding, and hierarchical covariate adjustment. We develop a Bayesian nonparametric framework that simultaneously accommodates interference and a post-treatment confounder that precedes the mediator. Identification is achieved through a multivariate Gaussian copula that replaces cross-world independence with a single dependence parameter, yielding a built-in sensitivity analysis to residual post-treatment confounding. For estimation, we introduce a nested common atoms enriched Dirichlet process (CA-EDP) prior that integrates the Common Atoms Model (CAM) to share information across clusters while capturing between- and within-cluster heterogeneity, and an Enriched Dirichlet Process (EDP) structure delivering robust covariate adjustment without impacting the outcome model. We provide formal theoretical support for our prior by deriving the model's key distributional properties, including its partially exchangeable partition structure, and by establishing convergence guarantees for the practical truncation-based posterior inference strategy. We demonstrate the performance of the proposed methods in simulations and provide further illustration through a reanalysis of a completed CRT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16673v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Ohnishi, Michael J. Daniels, Lei Yang, Fan Li</dc:creator>
    </item>
    <item>
      <title>Correlation of divergency: c-delta. Being different in a similar way or not</title>
      <link>https://arxiv.org/abs/2510.16717</link>
      <description>arXiv:2510.16717v1 Announce Type: new 
Abstract: This paper introduces the correlation-of-divergency coefficient, c-delta, a custom statistical measure designed to quantify the similarity of internal divergence patterns between two groups of values. Unlike conventional correlation coefficients such as Pearson or Spearman, which assess the association between paired values, c-delta evaluates whether the way values differ within one group is mirrored in another. The method involves calculating, for each value, its divergence from all other values in its group, and then comparing these patterns across the two groups (e.g., human vs machine intelligence). The coefficient is normalised by the average root mean square divergence within each group, ensuring scale invariance. Potential applications of c-delta span quantum physics, where it can compare the spread of measurement outcomes between quantum systems, as well as fields such as genetics, ecology, psychometrics, manufacturing, machine learning, and social network analysis. The measure is particularly useful for benchmarking, clustering validation, and assessing the similarity of variability structures. While c-delta is not bounded between -1 and 1 and may be sensitive to outliers (but so is PMCC), it offers a new perspective for analysing internal variability and divergence. The article discusses the mathematical formulation, potential adaptations for complex data, and the interpretative considerations relevant to this alternative approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16717v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>quant-ph</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Johan F. Hoorn</dc:creator>
    </item>
    <item>
      <title>Causal inference for calibrated scaling interventions on time-to-event processes</title>
      <link>https://arxiv.org/abs/2510.16798</link>
      <description>arXiv:2510.16798v1 Announce Type: new 
Abstract: This work studies stochastic interventions in continuous-time event-history settings formulated as multiplicative scalings of the observed intensity governing an intermediate event process. This gives rise to a family of causal estimands indexed by a scalar parameter {\alpha}, which changes the event rate while preserving the temporal and covariate structure of the data-generating process. We introduce calibrated interventions, where \(\alpha\) is chosen to achieve a pre-specified goal, such as a desired level of cumulative risk of the intermediate event, and define corresponding composite target parameters capturing the resulting effects on the outcome process. Our proposal enables practical yet statistically principled intervention analysis in survival and longitudinal settings, which offers a flexible alternative to deterministic or static interventions that are often ill-defined. The framework applies broadly to causal questions involving time-to-event treatments or mediators, and offers a pragmatic analogue to indirect/direct effect decompositions. We present the efficient influence curves for various versions of target parameters under a nonparametric statistical model, discuss their double robustness properties, and propose an estimation procedure based on targeted maximum likelihood estimation (TMLE). The proposed estimands are illustrated through examples of event-history scenarios addressing distinct causal questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16798v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Helene Charlotte Wiese Rytgaard, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Causal Variance Decompositions for Measuring Health Inequalities</title>
      <link>https://arxiv.org/abs/2510.16975</link>
      <description>arXiv:2510.16975v1 Announce Type: new 
Abstract: Recent causal inference literature has introduced causal effect decompositions to quantify sources of observed inequalities or disparities in outcomes but usually limiting this to pairwise comparisons. In the context of hospital profiling, comparison of hospital performance may reveal inequalities in healthcare delivery between sociodemographic groups, which may be explained by access/selection or actual effect modification. We consider the case of polytomous exposures in hospital profiling where the comparison is often to the system wide average performance, and decompose the observed variance in care delivery as the quantity of interest. For this, we formulate a new eight-way causal variance decomposition where we attribute the observed variation to components describing the main effects of hospital and group membership, modification of the hospital effect by group membership, hospital access/selection, effect of case-mix covariates and residual variance. We discuss the causal interpretation of the components, formulate parametric and nonparametric model based estimators and study the properties of these estimators through simulation. Finally, we illustrate our method by an example of cancer care delivery using data from the SEER database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16975v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Yu, Zhihui Liu, Kathy Han, Olli Saarela</dc:creator>
    </item>
    <item>
      <title>Functional principal component analysis for functional data with detection limits</title>
      <link>https://arxiv.org/abs/2510.16992</link>
      <description>arXiv:2510.16992v1 Announce Type: new 
Abstract: When measurements fall below or above a detection threshold, the resulting data are missing not at random (MNAR), posing challenges for statistical analysis. For example, in longitudinal biomarker studies, observations may be subject to detection limits. Functional principal component analysis (FPCA) is commonly used method for dimension reduction of dense and sparse data measured along a continuum, but standard approaches typically ignore MNAR mechanisms by imputing detection limit values, leading to biased estimates of principal components and scores.
  Building on recent work by Liu and Houwing-Duistermaat (2022, 2023), who proposed estimators for the mean and covariance functions under detection limits, we extend FPCA to accommodate functional data affected by such limits. We derive the asymptotic properties of the resulting estimators and assess their performance through simulations, comparing them to standard methods. Finally, we illustrate our approach using longitudinal biomarker data subject to detection limits.
  Our method yields more accurate estimates of functional principal components and scores, enhancing the reliability of functional data analysis in the presence of detection limits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16992v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiyan Liu, Jeanine Houwing-Duistermaat</dc:creator>
    </item>
    <item>
      <title>Variable Selection with Broken Adaptive Ridge Regression for Interval-Censored Competing Risks Data</title>
      <link>https://arxiv.org/abs/2510.17084</link>
      <description>arXiv:2510.17084v1 Announce Type: new 
Abstract: Competing risks data refer to situations where the occurrence of one event pre- cludes the possibility of other events happening, resulting in multiple mutually exclusive events. This data type is commonly encountered in medical research and clinical trials, exploring the interplay between different events and informing decision-making in fields such as healthcare and epidemiology. We develop a penal- ized variable selection procedure to handle such complex data in an interval-censored setting. We consider a broad class of semiparametric transformation regression mod- els, including popular models such as proportional and non-proportional hazards models. To promote sparsity and select variables specific to each event, we employ the broken adaptive ridge (BAR) penalty. This approach allows us to simultane- ously select important risk factors and estimate their effects for each event under investigation. We establish the oracle property of the BAR procedure and evaluate its performance through simulation studies. The proposed method is applied to a real-life HIV cohort dataset, further validating its applicability in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17084v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatemeh Mahmoudi, Chenxi Li, Kaida Cai, Xuewen Lu</dc:creator>
    </item>
    <item>
      <title>Discovering Causal Relationships using Proxy Variables under Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2510.17167</link>
      <description>arXiv:2510.17167v1 Announce Type: new 
Abstract: Inferring causal relationships between variable pairs in the observational study is crucial but challenging, due to the presence of unmeasured confounding. While previous methods employed the negative controls to adjust for the confounding bias, they were either restricted to the discrete setting (i.e., all variables are discrete) or relied on strong assumptions for identification. To address these problems, we develop a general nonparametric approach that accommodates both discrete and continuous settings for testing causal hypothesis under unmeasured confounders. By using only a single negative control outcome (NCO), we establish a new identification result based on a newly proposed integral equation that links the outcome and NCO, requiring only the completeness and mild regularity conditions. We then propose a kernel-based testing procedure that is more efficient than existing moment-restriction methods. We derive the asymptotic level and power properties for our tests. Furthermore, we examine cases where our procedure using only NCO fails to achieve identification, and introduce a new procedure that incorporates a negative control exposure (NCE) to restore identifiability. We demonstrate the effectiveness of our approach through extensive simulations and real-world data from the Intensive Care Data and World Values Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17167v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yong Wu, Yanwei Fu, Shouyan Wang, Yizhou Wang, Xinwei Sun</dc:creator>
    </item>
    <item>
      <title>On Misspecified Error Distributions in Bayesian Functional Clustering: Consequences and Remedies</title>
      <link>https://arxiv.org/abs/2510.17215</link>
      <description>arXiv:2510.17215v1 Announce Type: new 
Abstract: Nonparametric Bayesian approaches provide a flexible framework for clustering without pre-specifying the number of groups, yet they are well known to overestimate the number of clusters, especially for functional data. We show that a fundamental cause of this phenomenon lies in misspecification of the error structure: errors are conventionally assumed to be independent across observed points in Bayesian functional models. Through high-dimensional clustering theory, we demonstrate that ignoring the underlying correlation leads to excess clusters regardless of the flexibility of prior distributions. Guided by this theory, we propose incorporating the underlying correlation structures via Gaussian processes and also present its scalable approximation with principled hyperparameter selection. Numerical experiments illustrate that even simple clustering based on Dirichlet processes performs well once error dependence is properly modeled.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17215v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fumiya Iwashige, Tomoya Wakayama, Shonosuke Sugasawa, Shintaro Hashimoto</dc:creator>
    </item>
    <item>
      <title>Bridging the gap between experimental burden and statistical power for quantiles equivalence testing</title>
      <link>https://arxiv.org/abs/2510.17514</link>
      <description>arXiv:2510.17514v1 Announce Type: new 
Abstract: Testing the equivalence of multiple quantiles between two populations is important in many scientific applications, such as clinical trials, where conventional mean-based methods may be inadequate. This is particularly relevant in bridging studies that compare drug responses across different experimental conditions or patient populations. These studies often aim to assess whether a proposed dose for a target population achieves pharmacokinetic levels comparable to those of a reference population where efficacy and safety have been established. The focus is on extreme quantiles which directly inform both efficacy and safety assessments. When analyzing heterogeneous Gaussian samples, where a single quantile of interest is estimated, the existing Two One-Sided Tests method for quantile equivalence testing (qTOST) tends to be overly conservative. To mitigate this behavior, we introduce $\alpha$-qTOST, a finite-sample adjustment that achieves uniformly higher power compared to qTOST while maintaining the test size at the nominal level. Moreover, we extend the quantile equivalence framework to simultaneously assess equivalence across multiple quantiles. Through theoretical guarantees and an extensive simulation study, we demonstrate that $\alpha$-qTOST offers substantial improvements, especially when testing extreme quantiles under heteroskedasticity and with small, unbalanced sample sizes. We illustrate these advantages through two case studies, one in HIV drug development, where a bridging clinical trial examines exposure distributions between male and female populations with unbalanced sample sizes, and another in assessing the reproducibility of an identical experimental protocol performed by different operators for generating biodistribution profiles of topically administered and locally acting products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17514v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jun Wu, St\'ephane Guerrier, Si Gou, Yogeshvar N. Kalia, Luca Insolia</dc:creator>
    </item>
    <item>
      <title>Defining Utility as a Measure of Preference Under Uncertainty in Phase I-II Oncology Dose Finding Trials</title>
      <link>https://arxiv.org/abs/2510.17550</link>
      <description>arXiv:2510.17550v1 Announce Type: new 
Abstract: The main objective of dose finding trials is to find an optimal dose amongst a candidate set for further research. The trial design in oncology proceeds in stages with a decision as to how to treat the next group of patients made at every stage until a final sample size is reached or the trial stopped early.
  This work applies a Bayesian decision-theoretic approach to the problem, proposing a new utility function based on both efficacy and toxicity and grounded in von Neumann-Morgenstern (VNM) utility theory. Our proposed framework seeks to better capture real clinical judgements by allowing attitudes to risk to vary when the judgements are of gains or losses, which are defined with respect to an intermediate outcome known as a reference point. We call this method Reference Dependent Decision Theoretic dose finding (R2DT).
  A simulation study demonstrates that the framework can perform well and produce good operating characteristics. The simulation results demonstrate that R2DT is better at detecting the optimal dose in scenarios where candidate doses are around minimum acceptable efficacy and maximum acceptable toxicity thresholds.
  The proposed framework shows that a flexible utility function, which better captures clinician beliefs, can lead to trials with good operating characteristics, including a high probability of finding the optimal dose. Our work demonstrates proof-of-concept for this framework, which should be evaluated in a broader range of settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17550v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Hall, Duncan Wilson, Stuart Barber, Sarah R Brown</dc:creator>
    </item>
    <item>
      <title>Relaxing the Assumption of Strongly Non-Informative Linkage Error in Secondary Regression Analysis of Linked Files</title>
      <link>https://arxiv.org/abs/2510.17553</link>
      <description>arXiv:2510.17553v1 Announce Type: new 
Abstract: Data analysis of files that are a result of linking records from multiple sources are often affected by linkage errors. Records may be linked incorrectly, or their links may be missed. In consequence, it is essential that such errors are taken into account to ensure valid post-linkage inference. Here, we propose an extension to a general framework for regression with linked covariates and responses based on a two-component mixture model, which was developed in prior work. This framework addresses the challenging case of secondary analysis in which only the linked data is available and information about the record linkage process is limited. The extension considered herein relaxes the assumption of strongly non-informative linkage in the framework according to which linkage does not depend on the covariates used in the analysis, which may be limiting in practice. The effectiveness of the proposed extension is investigated by simulations and a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17553v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Priyanjali Bukke, Martin Slawski</dc:creator>
    </item>
    <item>
      <title>The modified odd Burr XII-G family of distributions: Properties and Applications</title>
      <link>https://arxiv.org/abs/2510.17567</link>
      <description>arXiv:2510.17567v1 Announce Type: new 
Abstract: The modified odd Burr XII-G family is developed, capable of incorporating bimodal and bathtub shapes in its baseline distributions, with properties derived from the exponentiated-G class. A regression model is developed within this family. The parameters are estimated by maximum likelihood, and simulations are performed to verify their consistency. The usefulness of the proposals is demonstrated by means of three real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17567v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexsandro A. Ferreira, Gauss M. Cordeiro</dc:creator>
    </item>
    <item>
      <title>Institutional Differences, Crisis Shocks, and Volatility Structure: A By-Window EGARCH/TGARCH Analysis of ASEAN Stock Markets</title>
      <link>https://arxiv.org/abs/2510.16010</link>
      <description>arXiv:2510.16010v1 Announce Type: cross 
Abstract: This study examines how institutional differences and external crises shape volatility dynamics in emerging Asian stock markets. Using daily stock index returns for Indonesia, Malaysia, and the Philippines from 2010 to 2024, we estimate EGARCH(1,1) and TGARCH(1,1) models in a by-window design. The sample is split into the 2013 Taper Tantrum, the 2020-2021 COVID-19 period, the 2022-2023 rate-hike cycle, and tranquil phases. Prior work typically studies a single market or a static period; to our knowledge no study unifies institutional comparison with multi-crisis dynamics within one GARCH framework. We address this gap and show that all three markets display strong volatility persistence and fat-tailed returns. During crises both persistence and asymmetry increase, while tail thickness rises, implying more frequent extreme moves. After crises, parameters revert toward pre-shock levels. Cross-country evidence indicates a buffering role of institutional maturity: Malaysias stronger regulatory and information systems dampen amplification and speed recovery, whereas the Philippines thinner market structure prolongs instability. We conclude that crises amplify volatility structures, while institutional robustness governs recovery speed. The results provide policy guidance on transparency, macroprudential communication, and liquidity support to reduce volatility persistence during global shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16010v1</guid>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junlin Yang</dc:creator>
    </item>
    <item>
      <title>Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?</title>
      <link>https://arxiv.org/abs/2510.16060</link>
      <description>arXiv:2510.16060v1 Announce Type: cross 
Abstract: The recent development of foundation models for time series data has generated considerable interest in using such models across a variety of applications. Although foundation models achieve state-of-the-art predictive performance, their calibration properties remain relatively underexplored, despite the fact that calibration can be critical for many practical applications. In this paper, we investigate the calibration-related properties of five recent time series foundation models and two competitive baselines. We perform a series of systematic evaluations assessing model calibration (i.e., over- or under-confidence), effects of varying prediction heads, and calibration under long-term autoregressive forecasting. We find that time series foundation models are consistently better calibrated than baseline models and tend not to be either systematically over- or under-confident, in contrast to the overconfidence often seen in other deep learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16060v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Coen Adler, Yuxin Chang, Felix Draxler, Samar Abdi, Padhraic Smyth</dc:creator>
    </item>
    <item>
      <title>COWs and their Hybrids: A Statistical View of Custom Orthogonal Weights</title>
      <link>https://arxiv.org/abs/2510.16174</link>
      <description>arXiv:2510.16174v1 Announce Type: cross 
Abstract: A recurring challenge in high energy physics is inference of the signal component from a distribution for which observations are assumed to be a mixture of signal and background events. A standard assumption is that there exists information encoded in a discriminant variable that is effective at separating signal and background. This can be used to assign a signal weight to each event, with these weights used in subsequent analyses of one or more control variables of interest. The custom orthogonal weights (COWs) approach of Dembinski, et al.(2022), a generalization of the sPlot approach of Barlow (1987) and Pivk and Le Diberder (2005), is tailored to address this objective. The problem, and this method, present interesting and novel statistical issues. Here we formalize the assumptions needed and the statistical properties, while also considering extensions and alternative approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16174v1</guid>
      <category>stat.AP</category>
      <category>hep-ex</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chad Schafer, Larry Wasserman, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>On Quantile Treatment Effects, Rank Similarity,and Variation of Instrumental Variables</title>
      <link>https://arxiv.org/abs/2510.16681</link>
      <description>arXiv:2510.16681v1 Announce Type: cross 
Abstract: This paper develops a nonparametric framework to identify and estimate distributional treatment effects under nonseparable endogeneity. We begin by revisiting the widely adopted \emph{rank similarity} (RS) assumption and characterizing it by the relationship it imposes between observed and counterfactual potential outcome distributions. The characterization highlights the restrictiveness of RS, motivating a weaker identifying condition. Under this alternative, we construct identifying bounds on the distributional treatment effects of interest through a linear semi-infinite programming (SILP) formulation. Our identification strategy also clarifies how richer exogenous instrument variation, such as multi-valued or multiple instruments, can further tighten these bounds. Finally, exploiting the SILP's saddle-point structure and Karush-Kuhn-Tucker (KKT) conditions, we establish large-sample properties for the empirical SILP: consistency and asymptotic distribution results for the estimated bounds and associated solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16681v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukjin Han, Haiqing Xu</dc:creator>
    </item>
    <item>
      <title>On the Granularity of Causal Effect Identifiability</title>
      <link>https://arxiv.org/abs/2510.16703</link>
      <description>arXiv:2510.16703v1 Announce Type: cross 
Abstract: The classical notion of causal effect identifiability is defined in terms of treatment and outcome variables. In this note, we consider the identifiability of state-based causal effects: how an intervention on a particular state of treatment variables affects a particular state of outcome variables. We demonstrate that state-based causal effects may be identifiable even when variable-based causal effects may not. Moreover, we show that this separation occurs only when additional knowledge -- such as context-specific independencies and conditional functional dependencies -- is available. We further examine knowledge that constrains the states of variables, and show that such knowledge does not improve identifiability on its own but can improve both variable-based and state-based identifiability when combined with other knowledge such as context-specific independencies. Our findings highlight situations where causal effects of interest may be estimable from observational data and this identifiability may be missed by existing variable-based frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16703v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizuo Chen, Adnan Darwiche</dc:creator>
    </item>
    <item>
      <title>Local regression on path spaces with signature metrics</title>
      <link>https://arxiv.org/abs/2510.16728</link>
      <description>arXiv:2510.16728v1 Announce Type: cross 
Abstract: We study nonparametric regression and classification for path-valued data. We introduce a functional Nadaraya-Watson estimator that combines the signature transform from rough path theory with local kernel regression. The signature transform provides a principled way to encode sequential data through iterated integrals, enabling direct comparison of paths in a natural metric space. Our approach leverages signature-induced distances within the classical kernel regression framework, achieving computational efficiency while avoiding the scalability bottlenecks of large-scale kernel matrix operations. We establish finite-sample convergence bounds demonstrating favorable statistical properties of signature-based distances compared to traditional metrics in infinite-dimensional settings. We propose robust signature variants that provide stability against outliers, enhancing practical performance. Applications to both synthetic and real-world data - including stochastic differential equation learning and time series classification - demonstrate competitive accuracy while offering significant computational advantages over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16728v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Bayer, Davit Gogolashvili, Luca Pelizzari</dc:creator>
    </item>
    <item>
      <title>Surrogate Modeling and Explainable Artificial Intelligence for Complex Systems: A Workflow for Automated Simulation Exploration</title>
      <link>https://arxiv.org/abs/2510.16742</link>
      <description>arXiv:2510.16742v1 Announce Type: cross 
Abstract: Complex systems are increasingly explored through simulation-driven engineering workflows that combine physics-based and empirical models with optimization and analytics. Despite their power, these workflows face two central obstacles: (1) high computational cost, since accurate exploration requires many expensive simulator runs; and (2) limited transparency and reliability when decisions rely on opaque blackbox components. We propose a workflow that addresses both challenges by training lightweight emulators on compact designs of experiments that (i) provide fast, low-latency approximations of expensive simulators, (ii) enable rigorous uncertainty quantification, and (iii) are adapted for global and local Explainable Artificial Intelligence (XAI) analyses. This workflow unifies every simulation-based complex-system analysis tool, ranging from engineering design to agent-based models for socio-environmental understanding. In this paper, we proposea comparative methodology and practical recommendations for using surrogate-based explainability tools within the proposed workflow. The methodology supports continuous and categorical inputs, combines global-effect and uncertainty analyses with local attribution, and evaluates the consistency of explanations across surrogate models, thereby diagnosing surrogate adequacy and guiding further data collection or model refinement. We demonstrate the approach on two contrasting case studies: a multidisciplinary design analysis of a hybrid-electric aircraft and an agent-based model of urban segregation. Results show that the surrogate model and XAI coupling enables large-scale exploration in seconds, uncovers nonlinear interactions and emergent behaviors, identifies key design and policy levers, and signals regions where surrogates require more data or alternative architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16742v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Saves, Pramudita Satria Palar, Muhammad Daffa Robani, Nicolas Verstaevel, Moncef Garouani, Julien Aligon, Benoit Gaudou, Koji Shimoyama, Joseph Morlier</dc:creator>
    </item>
    <item>
      <title>Kernel-Based Nonparametric Tests For Shape Constraints</title>
      <link>https://arxiv.org/abs/2510.16745</link>
      <description>arXiv:2510.16745v2 Announce Type: cross 
Abstract: We develop a reproducing kernel Hilbert space (RKHS) framework for nonparametric mean-variance optimization and inference on shape constraints of the optimal rule. We derive statistical properties of the sample estimator and provide rigorous theoretical guarantees, such as asymptotic consistency, a functional central limit theorem, and a finite-sample deviation bound that matches the Monte Carlo rate up to regularization. Building on these findings, we introduce a joint Wald-type statistic to test for shape constraints over finite grids. The approach comes with an efficient computational procedure based on a pivoted Cholesky factorization, facilitating scalability to large datasets. Empirical tests suggest favorably of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16745v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Sen</dc:creator>
    </item>
    <item>
      <title>Prediction-Augmented Trees for Reliable Statistical Inference</title>
      <link>https://arxiv.org/abs/2510.16937</link>
      <description>arXiv:2510.16937v1 Announce Type: cross 
Abstract: The remarkable success of machine learning (ML) in predictive tasks has led scientists to incorporate ML predictions as a core component of the scientific discovery pipeline. This was exemplified by the landmark achievement of AlphaFold (Jumper et al. (2021)). In this paper, we study how ML predictions can be safely used in statistical analysis of data towards scientific discovery. In particular, we follow the framework introduced by Angelopoulos et al. (2023). In this framework, we assume access to a small set of $n$ gold-standard labeled samples, a much larger set of $N$ unlabeled samples, and a ML model that can be used to impute the labels of the unlabeled data points. We introduce two new learning-augmented estimators: (1) Prediction-Augmented Residual Tree (PART), and (2) Prediction-Augmented Quadrature (PAQ). Both estimators have significant advantages over existing estimators like PPI and PPI++ introduced by Angelopoulos et al. (2023) and Angelopoulos et al. (2024), respectively. PART is a decision-tree based estimator built using a greedy criterion. We first characterize PART's asymptotic distribution and demonstrate how to construct valid confidence intervals. Then we show that PART outperforms existing methods in real-world datasets from ecology, astronomy, and census reports, among other domains. This leads to estimators with higher confidence, which is the result of using both the gold-standard samples and the machine learning predictions. Finally, we provide a formal proof of the advantage of PART by exploring PAQ, an estimation that arises when considering the limit of PART when the depth its tree grows to infinity. Under appropriate assumptions in the input data we show that the variance of PAQ shrinks at rate of $O(N^{-1} + n^{-4})$, improving significantly on the $O(N^{-1}+n^{-1})$ rate of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16937v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vikram Kher, Argyris Oikonomou, Manolis Zampetakis</dc:creator>
    </item>
    <item>
      <title>A Unified Approach to Statistical Estimation Under Nonlinear Observations: Tensor Estimation and Matrix Factorization</title>
      <link>https://arxiv.org/abs/2510.16965</link>
      <description>arXiv:2510.16965v1 Announce Type: cross 
Abstract: We consider the estimation of some parameter $\mathbf{x}$ living in a cone from the nonlinear observations of the form $\{y_i=f_i(\langle\mathbf{a}_i,\mathbf{x}\rangle)\}_{i=1}^m$. We develop a unified approach that first constructs a gradient from the data and then establishes the restricted approximate invertibility condition (RAIC), a condition that quantifies how well the gradient aligns with the ideal descent step. We show that RAIC yields linear convergence guarantees for the standard projected gradient descent algorithm, a Riemannian gradient descent algorithm for low Tucker-rank tensor estimation, and a factorized gradient descent algorithm for asymmetric low-rank matrix estimation. Under Gaussian designs, we establish sharp RAIC for the canonical statistical estimation problems of single index models, generalized linear models, noisy phase retrieval, and one-bit compressed sensing. Combining the convergence guarantees and the RAIC, we obtain a set of optimal statistical estimation results, including, to our knowledge, the first minimax-optimal and computationally efficient algorithms for tensor single index models, tensor logistic regression, (local) noisy tensor phase retrieval, and one-bit tensor sensing. Moreover, several other results are new or match the best known guarantees. We also provide simulations and a real-data experiment to illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16965v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junren Chen, Lijun Ding, Dong Xia, Ming Yuan</dc:creator>
    </item>
    <item>
      <title>DFNN: A Deep Fr\'echet Neural Network Framework for Learning Metric-Space-Valued Responses</title>
      <link>https://arxiv.org/abs/2510.17072</link>
      <description>arXiv:2510.17072v1 Announce Type: cross 
Abstract: Regression with non-Euclidean responses -- e.g., probability distributions, networks, symmetric positive-definite matrices, and compositions -- has become increasingly important in modern applications. In this paper, we propose deep Fr\'echet neural networks (DFNNs), an end-to-end deep learning framework for predicting non-Euclidean responses -- which are considered as random objects in a metric space -- from Euclidean predictors. Our method leverages the representation-learning power of deep neural networks (DNNs) to the task of approximating conditional Fr\'echet means of the response given the predictors, the metric-space analogue of conditional expectations, by minimizing a Fr\'echet risk. The framework is highly flexible, accommodating diverse metrics and high-dimensional predictors. We establish a universal approximation theorem for DFNNs, advancing the state-of-the-art of neural network approximation theory to general metric-space-valued responses without making model assumptions or relying on local smoothing. Empirical studies on synthetic distributional and network-valued responses, as well as a real-world application to predicting employment occupational compositions, demonstrate that DFNNs consistently outperform existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17072v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyum Kim, Yaqing Chen, Paromita Dubey</dc:creator>
    </item>
    <item>
      <title>The synthetic instrument: From sparse association to sparse causation</title>
      <link>https://arxiv.org/abs/2304.01098</link>
      <description>arXiv:2304.01098v3 Announce Type: replace 
Abstract: In many observational studies, researchers are often interested in studying the effects of multiple exposures on a single outcome. Standard approaches for high-dimensional data such as the lasso assume the associations between the exposures and the outcome are sparse. These methods, however, do not estimate the causal effects in the presence of unmeasured confounding. In this paper, we consider an alternative approach that assumes the causal effects in view are sparse. We show that with sparse causation, the causal effects are identifiable even with unmeasured confounding. At the core of our proposal is a novel device, called the synthetic instrument, that in contrast to standard instrumental variables, can be constructed using the observed exposures directly. We show that under linear structural equation models, the problem of causal effect estimation can be formulated as an $\ell_0$-penalization problem, and hence can be solved efficiently using off-the-shelf software. Simulations show that our approach outperforms state-of-art methods in both low-dimensional and high-dimensional settings. We further illustrate our method using a mouse obesity dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01098v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dingke Tang, Dehan Kong, Linbo Wang</dc:creator>
    </item>
    <item>
      <title>Sliced Wasserstein Regression</title>
      <link>https://arxiv.org/abs/2306.10601</link>
      <description>arXiv:2306.10601v3 Announce Type: replace 
Abstract: While statistical modeling of distributional data has gained increased attention, the case of multivariate distributions has been somewhat neglected despite its relevance in various applications. This is because the Wasserstein distance, commonly used in distributional data analysis, poses challenges for multivariate distributions. A promising alternative is the sliced Wasserstein distance, which offers a computationally simpler solution. We propose distributional regression models with multivariate distributions as responses paired with Euclidean vector predictors. The foundation of our methodology is a slicing transform from the multivariate distribution space to the sliced distribution space for which we establish a theoretical framework, with the Radon transform as a prominent example. We introduce and study the asymptotic properties of sample-based estimators for two regression approaches, one based on utilizing the sliced Wasserstein distance directly in the multivariate distribution space, and a second approach based on a new slice-wise distance, employing a univariate distribution regression for each slice. Both global and local Fr\'echet regression methods are deployed for these approaches and illustrated in simulations and through applications. These include joint distributions of excess winter death rates and winter temperature anomalies in European countries as a function of base winter temperature and also data from finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10601v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Chen, Yidong Zhou, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal Besov Priors for Bayesian Inverse Problems</title>
      <link>https://arxiv.org/abs/2306.16378</link>
      <description>arXiv:2306.16378v3 Announce Type: replace 
Abstract: Fast development in science and technology has driven the need for proper statistical tools to capture special data features such as abrupt changes or sharp contrast. Many inverse problems in data science require spatiotemporal solutions derived from a sequence of time-dependent objects with these spatial features, e.g., the dynamic reconstruction of computerized tomography (CT) images with edges. Conventional methods based on Gaussian processes (GP) often fall short in providing satisfactory solutions since they tend to offer oversmooth priors. Recently, the Besov process (BP), defined by wavelet expansions with random coefficients, has emerged as a more suitable prior for Bayesian inverse problems of this nature. While BP excels in handling spatial inhomogeneity, it does not automatically incorporate temporal correlation inherited in the dynamically changing objects. In this paper, we generalize BP to a novel spatiotemporal Besov process (STBP) by replacing the random coefficients in the series expansion with stochastic time functions as Q-exponential process (Q-EP) which governs the temporal correlation structure. We thoroughly investigate the mathematical and statistical properties of STBP. Simulations, two limited-angle CT reconstruction examples, a highly non-linear inverse problem involving Navier-Stokes equation, and a spatiotemporal temperature imputation problem are used to demonstrate the advantage of the proposed STBP compared with the classic STGP and a time-uncorrelated approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16378v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2560688</arxiv:DOI>
      <arxiv:journal_reference>Shiwei Lan, Mirjeta Pasha, Shuyi Li, and Weining Shen, Journal of the American Statistical Association (Theory and Methods), 2025</arxiv:journal_reference>
      <dc:creator>Shiwei Lan, Mirjeta Pasha, Shuyi Li, Weining Shen</dc:creator>
    </item>
    <item>
      <title>A Double Machine Learning Approach to Combining Experimental and Observational Data</title>
      <link>https://arxiv.org/abs/2307.01449</link>
      <description>arXiv:2307.01449v4 Announce Type: replace 
Abstract: Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework proposes a falsification test for external validity and ignorability under milder assumptions. We provide consistent treatment effect estimators even when one of the assumptions is violated. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. Through comparative analyses, we show our framework's superiority over existing data fusion methods. The practical utility of our approach is further exemplified by three real-world case studies, underscoring its potential for widespread application in empirical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01449v4</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Harsh Parikh, Marco Morucci, Vittorio Orlandi, Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky</dc:creator>
    </item>
    <item>
      <title>Scalable solution to crossed random effects model with random slopes</title>
      <link>https://arxiv.org/abs/2307.12378</link>
      <description>arXiv:2307.12378v4 Announce Type: replace 
Abstract: The crossed random effects model is widely used, finding applications in various fields such as longitudinal studies, e-commerce, and recommender systems, among others. However, these models encounter scalability challenges, as the computational time for standard algorithms grows superlinearly with the number N of observations in the data set, commonly $\Omega(N^{3/2})$ or worse. Recent published works present scalable methods for crossed random effects in linear models and some generalized linear models, but those methods only allow for random intercepts. In this paper, we devise scalable algorithms for models that include random slopes. This addition brings substantial difficulty in estimating the random-effect covariance matrices in a scalable way. We address this issue by using a variational EM algorithm. Our proposed approach accommodates both diagonal covariance matrices and cases where no structure is assumed-a scenario common in fields such as psychology and neuroscience. In simulations, the proposed method is substantially faster than standard methods for large $N$. It is also more efficient than ordinary least squares which has a problem of greatly underestimating the sampling uncertainty in parameter estimates. We illustrate the new method on a MovieLens dataset, as well as a large data set (five million observations) from the online retailer Stitch Fix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12378v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Disha Ghandwani, Swarnadip Ghosh, Trevor Hastie, Art B. Owen</dc:creator>
    </item>
    <item>
      <title>Mitigating dimensionality effects with robust graph constructions for testing</title>
      <link>https://arxiv.org/abs/2307.15205</link>
      <description>arXiv:2307.15205v4 Announce Type: replace 
Abstract: Dimensionality effects pose major challenges in high-dimensional and non-Euclidean data analysis. Graph-based two-sample tests and change-point detection are particularly attractive in this context, as they make minimal distributional assumptions and perform well across a wide range of scenarios. These methods rely on similarity graphs constructed from data, with $K$-nearest neighbor graphs and $K$-minimum spanning trees among the most effective and widely used. However, in high-dimensional and non-Euclidean regimes such graphs often produce hubs -- nodes with disproportionately high degrees -- to which graph-based methods are especially sensitive. To mitigate these dimensionality effects, we propose a robust graph construction that is far less prone to hub formation. Incorporating this construction substantially improves the power of graph-based methods across diverse settings. We further establish a theoretical foundation by proving its consistency under fixed alternatives in both low- and high-dimensional regimes. The effectiveness of the approach is demonstrated through real-world applications, including comparisons of correlation matrices for brain regions, gene expression profiles of T cells, and temporal changes in New York City taxi travel patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15205v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yejiong Zhu, Hao Chen</dc:creator>
    </item>
    <item>
      <title>Deep Partially Linear Transformation Model for Right-Censored Survival Data</title>
      <link>https://arxiv.org/abs/2412.07611</link>
      <description>arXiv:2412.07611v3 Announce Type: replace 
Abstract: Although the Cox proportional hazards model is well established and extensively used in the analysis of survival data, the proportional hazards (PH) assumption may not always hold in practical scenarios. The class of semiparametric transformation models extends the Cox model and also includes many other survival models as special cases. This paper introduces a deep partially linear transformation model (DPLTM) as a general and flexible regression framework for right-censored data. The proposed method is capable of avoiding the curse of dimensionality while still retaining the interpretability of some covariates of interest. We derive the overall convergence rate of the maximum likelihood estimators, the minimax lower bound of the nonparametric deep neural network (DNN) estimator, and the asymptotic normality and the semiparametric efficiency of the parametric estimator. Comprehensive simulation studies demonstrate the impressive performance of the proposed estimation procedure in terms of both the estimation accuracy and the predictive power, which is further validated by an application to a real-world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07611v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junkai Yin, Yue Zhang, Zhangsheng Yu</dc:creator>
    </item>
    <item>
      <title>P3LS: Point Process Partial Least Squares</title>
      <link>https://arxiv.org/abs/2412.11267</link>
      <description>arXiv:2412.11267v2 Announce Type: replace 
Abstract: Many studies collect data that can be considered as a realization of a point process. Included are medical imaging data where photon counts are recorded by a gamma camera from patients being injected with a gamma emitting tracer. It is of interest to develop analytic methods that can help with diagnosis as well as in the training of inexpert radiologists. Partial least squares (PLS) is a popular analytic approach that combines features from linear modeling as well as dimension reduction to provide parsimonious prediction and classification. However, existing PLS methodologies do not include the analysis of point process predictors. In this article, we introduce point process PLS (P3LS) for analyzing latent time-varying intensity functions from collections of inhomogeneous point processes. A novel estimation procedure for $P^3LS$ is developed that utilizes the properties of log-Gaussian Cox processes, and its empirical properties are examined in simulation studies. The method is used to analyze kidney functionality in patients with renal disease in order to aid in the diagnosis of kidney obstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11267v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jamshid Namdari, Robert T Krafty, Amita Manatunga</dc:creator>
    </item>
    <item>
      <title>Detection and estimation of vertex-wise latent position shifts across networks</title>
      <link>https://arxiv.org/abs/2502.01947</link>
      <description>arXiv:2502.01947v2 Announce Type: replace 
Abstract: Pairwise network comparison is essential for various applications, including neuroscience, disease research, and dynamic network analysis. While existing literature primarily focuses on comparing entire network structures, we address a vertex-wise comparison problem where two random networks share the same set of vertices but allow for structural variations in some vertices, enabling a more detailed and flexible analysis of network differences. In our framework, some vertices retain their latent positions between networks, while others undergo shifts. To identify the shifted and unshifted vertices and estimate their latent position shifts, we propose a method that first derives vertex embeddings in a low-rank Euclidean space for each network, then aligns these estimated vertex latent positions into a common space to resolve potential non-identifiability, and finally tests whether each vertex is shifted or not and estimates the vertex shifts. Our theoretical results establish the test statistic for the algorithms, guide parameter selection, and provide performance guarantees. Simulation studies and real data applications, including a case-control study in disease research and dynamic network analysis, demonstrate that the proposed algorithms are both computationally efficient and effective in extracting meaningful insights from network comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01947v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runbing Zheng</dc:creator>
    </item>
    <item>
      <title>Model-assisted inference for dynamic causal effects in staggered rollout cluster randomized experiments</title>
      <link>https://arxiv.org/abs/2502.10939</link>
      <description>arXiv:2502.10939v2 Announce Type: replace 
Abstract: Staggered rollout cluster randomized experiments (SR-CREs) involve sequential treatment adoption across clusters, requiring analysis methods that address a general class of dynamic causal effects, anticipation, and non-ignorable cluster-period sizes. Without imposing any outcome modeling assumptions, we study regression estimators using individual data, cluster-period averages, and scaled cluster-period totals, with and without covariate adjustment from a design-based perspective. We establish consistency and asymptotic normality of each estimator under a randomization-based framework and prove that the associated variance estimators are asymptotically conservative in the L\"{o}wner ordering. Furthermore, we conduct a unified efficiency comparison of the estimators and provide recommendations. We highlight the efficiency advantage of using estimators based on scaled cluster-period totals with covariate adjustment over their counterparts using individual-level data and cluster-period averages. Our results rigorously justify linear regression estimators as model-assisted methods to address an entire class of dynamic causal effects in SR-CREs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10939v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Fan Li</dc:creator>
    </item>
    <item>
      <title>Neural Bayes estimation and selection for complex bivariate extremal dependence models</title>
      <link>https://arxiv.org/abs/2503.23156</link>
      <description>arXiv:2503.23156v2 Announce Type: replace 
Abstract: Likelihood-free approaches are appealing for performing inference on complex dependence models, either because it is not possible to formulate a likelihood function, or its evaluation is very computationally costly. This is the case for several models available in the multivariate extremes literature, particularly for the most flexible tail models, including those that interpolate between the two key dependence classes of `asymptotic dependence' and `asymptotic independence'. We focus on approaches that leverage neural networks to approximate Bayes estimators. In particular, we explore the properties of neural Bayes estimators for parameter inference for several flexible but computationally expensive models to fit, with a view to aiding their routine implementation. Owing to the absence of likelihood evaluation in the inference procedure, classical information criteria such as the Bayesian information criterion cannot be used to select the most appropriate model. Instead, we propose using neural networks as neural Bayes classifiers for model selection. Our goal is to provide a toolbox for simple, fast fitting and comparison of complex extreme-value dependence models, where the best model is selected for a given data set and its parameters subsequently estimated using neural Bayes estimation. We apply our classifiers and estimators to analyse the pairwise extremal behaviour of changes in horizontal geomagnetic field fluctuations at three different locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23156v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'idia M. Andr\'e, Jennifer L. Wadsworth, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>Prenatal phthalate exposures and adiposity outcomes trajectories: a multivariate Bayesian factor regression approach</title>
      <link>https://arxiv.org/abs/2506.02518</link>
      <description>arXiv:2506.02518v2 Announce Type: replace 
Abstract: Experimental animal evidence and a growing body of observational studies suggest that prenatal exposure to phthalates may be a risk factor for childhood obesity. Using data from the Mount Sinai Children's Environmental Health Study (MSCEHS), which measured urinary phthalate metabolites (including MEP, MnBP, MiBP, MCPP, MBzP, MEHP, MEHHP, MEOHP, and MECPP) during the third trimester of pregnancy (between 25 and 40 weeks) of 382 mothers, we examined adiposity outcomes: body mass index (BMI), fat mass percentage, waist-to-hip ratio, and waist circumference, of 180 children between ages 4 and 9. We aimed to assess the effects of prenatal exposure to phthalates on these adiposity outcomes, with potential time-varying and sex-specific effects. We applied a novel Bayesian multivariate factor regression (BMFR) that (1) represents phthalate mixtures as latent factors, a DEHP and a non-DEHP factor, (2) borrows information across highly correlated adiposity outcomes to improve estimation precision, (3) models potentially non-linear time-varying effects of the latent factors on adiposity outcomes, and (4) fully quantifies uncertainty using state-of-the-art prior specifications. The results show that in boys, at younger ages (4-6), all phthalate components are associated with lower adiposity outcomes; however, after age 7, they are associated with higher outcomes. In girls, there is no evidence of associations between phthalate factors and adiposity outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02518v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phuc H. Nguyen, Stephanie M. Engel, Amy H. Herring</dc:creator>
    </item>
    <item>
      <title>Sufficient digits and density estimation: A Bayesian nonparametric approach using generalized finite P\'olya trees</title>
      <link>https://arxiv.org/abs/2506.09437</link>
      <description>arXiv:2506.09437v2 Announce Type: replace 
Abstract: This paper proposes a novel approach for statistical modelling of a continuous random variable $X$ on $[0, 1)$, based on its digit representation $X=.X_1X_2\ldots$. In general, $X$ can be coupled with a latent random variable $N$ so that $(X_1,\ldots,X_N)$ becomes a sufficient statistics and $.X_{N+1}X_{N+2}\ldots$ is uniformly distributed. In line with this fact, and focusing on binary digits for simplicity, we propose a family of generalized finite P{\'o}lya trees that induces a random density for a sample, which becomes a flexible tool for density estimation. Here, the digit system may be random and learned from the data. We provide a detailed Bayesian analysis, including closed form expression for the posterior distribution which sidesteps the need of MCMC methods for posterior inference. We analyse the frequentist properties as the sample size increases, and provide sufficient conditions for consistency of the posterior distributions of the random density and $N$. We consider an extension to data spanning multiple orders of magnitude, and propose a prior distribution that encodes the so-called extended Newcomb-Benford law. Such a model shows promising results for density estimation of human-activity data. Our methodology is illustrated on several synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09437v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Jesper M{\o}ller</dc:creator>
    </item>
    <item>
      <title>Optimal Targeting in Dynamic Systems</title>
      <link>https://arxiv.org/abs/2507.00312</link>
      <description>arXiv:2507.00312v2 Announce Type: replace 
Abstract: Modern treatment targeting methods often rely on estimating the conditional average treatment effect (CATE) using machine learning tools. While effective in identifying who benefits from treatment on the individual level, these approaches typically overlook system-level dynamics that may arise when treatments induce strain on shared capacity. We study the problem of targeting in Markovian systems, where treatment decisions must be made one at a time as units arrive, and early decisions can impact later outcomes through delayed or limited access to resources. We show that optimal policies in such settings compare CATE-like quantities to state-specific thresholds, where each threshold reflects the expected cumulative impact on the system of treating an additional individual in the given state. We propose an algorithm that augments standard CATE estimation with off-policy evaluation techniques to estimate these thresholds from observational data. Theoretical results establish consistency and convergence guarantees, and empirical studies demonstrate that our method improves long-run outcomes considerably relative to individual-level CATE targeting rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00312v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Hu, Shuangning Li, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Enhanced Lepage-type test statistics for location-scale shifts with right-skewed data</title>
      <link>https://arxiv.org/abs/2509.19126</link>
      <description>arXiv:2509.19126v3 Announce Type: replace 
Abstract: Detecting simultaneous shifts in location and scale between two populations is a common challenge in statistical inference, particularly in fields like biomedicine where right-skewed data distributions are prevalent. The classical Lepage test, which combines the Wilcoxon-Mann-Whitney and Ansari-Bradley tests, can be suboptimal under these conditions due to its restrictive assumptions of equal variances and medians. This study systematically evaluates enhanced Lepage-type test statistics that incorporate modern robust components for improved performance with right-skewed data. We combine the Fligner-Policello test and Fong-Huang variance estimator for the location component with a novel empirical variance estimator for the Ansari-Bradley scale component, relaxing assumptions of equal variances and medians. Extensive Monte Carlo simulations across exponential, gamma, chi-square, lognormal, and Weibull distributions demonstrate that tests incorporating both robust components achieve power improvements of 10-25\% over the classical Lepage test while maintaining reasonable Type I error control. The practical utility is demonstrated through analyses of four real-world biomedical datasets, where the tests successfully detect significant location-scale shifts. We provide practical guidance for test selection and discuss implementation considerations, making these methods accessible for practitioners in biomedical research and other disciplines where right-skewed data are common.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19126v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abid Hussain, Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Robust Semiparametric Inference for Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2509.24634</link>
      <description>arXiv:2509.24634v2 Announce Type: replace 
Abstract: We develop a semiparametric framework for inference on the mean response in missing-data settings using a corrected posterior distribution. Our approach is tailored to Bayesian Additive Regression Trees (BART), which is a powerful predictive method but whose nonsmoothness complicate asymptotic theory with multi-dimensional covariates. When using BART combined with Bayesian bootstrap weights, we establish a new Bernstein-von Mises theorem and show that the limit distribution generally contains a bias term. To address this, we introduce RoBART, a posterior bias-correction that robustifies BART for valid inference on the mean response. Monte Carlo studies support our theory, demonstrating reduced bias and improved coverage relative to existing procedures using BART.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24634v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Breunig, Ruixuan Liu, Zhengfei Yu</dc:creator>
    </item>
    <item>
      <title>Remote Auditing: Design-based Tests of Randomization, Selection, and Missingness with Broadly Accessible Satellite Imagery</title>
      <link>https://arxiv.org/abs/2510.00128</link>
      <description>arXiv:2510.00128v2 Announce Type: replace 
Abstract: Randomized controlled trials (RCTs) are the benchmark for causal inference, yet field implementation can drift from the registered design or, by chance, yield imbalances. We introduce a remote audit -- a preregistrable, design-based diagnostic that uses strictly pre-treatment, publicly available satellite imagery to test whether assignment is independent of local conditions. The audit implements a conditional randomization test that asks whether treatment is more predictable from pre-treatment features than under the registered mechanism, delivering a finite-sample-valid, nonparametric check that honors blocks and clusters and controls multiplicity across image models, resolutions, and patch sizes via a max-statistic. The same preregistered procedure can be run before baseline data collection to guide implementation and, after assignments are realized, to audit the actual allocation. In two illustrations -- Uganda's Youth Opportunities Program (randomization corroborated) and a school-based experiment in Bangladesh (assignment predictable relative to the design, consistent with independent concerns) -- the audit can surface potential problems early, before costly scientific investments. We also provide descriptive diagnostics for selection into the study and for missingness. Because it is low-cost and can be implemented rapidly in a unified way across diverse global administrative jurisdictions, the remote audit complements balance tests, strengthens preregistration, and enables rapid design checks when conventional data collection is slow, expensive, or infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00128v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Connor T. Jerzak, Adel Daoud</dc:creator>
    </item>
    <item>
      <title>Revisiting Madigan and Mosurski: Collapsibility via Minimal Separators</title>
      <link>https://arxiv.org/abs/2510.09024</link>
      <description>arXiv:2510.09024v2 Announce Type: replace 
Abstract: Collapsibility provides a principled approach for dimension reduction in contingency tables and graphical models. Madigan and Mosurski (1990) pioneered the study of minimal collapsible sets in decomposable models, but existing algorithms for general graphs remain computationally demanding. We show that a model is collapsible onto a target set precisely when that set contains all minimal separators between its non-adjacent vertices. This insight motivates the Close Minimal Separator Absorption (CMSA) algorithm, which constructs minimal collapsible sets using only local separator searches at very low costs. Simulations confirm substantial efficiency gains, making collapsibility analysis practical in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09024v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pei Heng, Yi Sun, Shiyuan He, Jianhua Guo</dc:creator>
    </item>
    <item>
      <title>Multiply Robust Estimation of Conditional Survival Probability with Time-Varying Covariates</title>
      <link>https://arxiv.org/abs/2510.10372</link>
      <description>arXiv:2510.10372v2 Announce Type: replace 
Abstract: It is often of interest to study the association between covariates and the cumulative incidence of a time-to-event outcome, but a common challenge is right-censoring. When time-varying covariates are measured on a fixed discrete time scale, it is desirable to account for these more up-to-date covariates when addressing censoring. For example, in vaccine trials, it is of interest to study the association between immune response levels after administering the vaccine and the cumulative incidence of the endpoint, while accounting for loss to follow-up explained by immune response levels measured at multiple post-vaccination visits. Existing methods rely on stringent parametric assumptions, do not account for informative censoring due to time-varying covariates when time is continuous, only estimate a marginal survival probability, or do not fully use the discrete-time structure of post-treatment covariates. In this paper, we propose a nonparametric estimator of the continuous-time survival probability conditional on covariates, accounting for censoring due to time-varying covariates measured on a fixed discrete time scale. We show that the estimator is multiply robust: it is consistent if, within each time window between adjacent visits, at least one of the time-to-event distribution and the censoring distribution is consistently estimated. We demonstrate the superior performance of this estimator in a numerical simulation, and apply the method to a COVID-19 vaccine efficacy trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10372v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxiang Qiu, Marco Carone, Alex Luedtke, Peter B. Gilbert</dc:creator>
    </item>
    <item>
      <title>Edgington's Method for Random-Effects Meta-Analysis Part I: Estimation</title>
      <link>https://arxiv.org/abs/2510.12301</link>
      <description>arXiv:2510.12301v2 Announce Type: replace 
Abstract: Meta-analysis can be formulated as combining $p$-values across studies into a joint $p$-value function, from which point estimates and confidence intervals can be derived. We extend the meta-analytic estimation framework based on combined $p$-value functions to incorporate uncertainty in heterogeneity estimation by employing a confidence distribution approach. Specifically, the confidence distribution of Edgington's method is adjusted according to the confidence distribution of the heterogeneity parameter constructed from the generalized heterogeneity statistic. Simulation results suggest that 95% confidence intervals approach nominal coverage under most scenarios involving more than three studies and heterogeneity. Under no heterogeneity or for only three studies, the confidence interval typically overcovers, but is often narrower than the Hartung-Knapp-Sidik-Jonkman interval. The point estimator exhibits small bias under model misspecification and moderate to large heterogeneity. Edgington's method provides a practical alternative to classical approaches, with adjustment for heterogeneity estimation uncertainty often improving confidence interval coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12301v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kronthaler, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>Transfer Q-learning</title>
      <link>https://arxiv.org/abs/2202.04709</link>
      <description>arXiv:2202.04709v2 Announce Type: replace-cross 
Abstract: Time-inhomogeneous finite-horizon Markov decision processes (MDP) are frequently employed to model decision-making in dynamic treatment regimes and other statistical reinforcement learning (RL) scenarios. These fields, especially healthcare and business, often face challenges such as high-dimensional state spaces and time-inhomogeneity of the MDP process, compounded by insufficient sample availability which complicates informed decision-making. To overcome these challenges, we investigate knowledge transfer within time-inhomogeneous finite-horizon MDP by leveraging data from both a target RL task and several related source tasks. We have developed transfer learning (TL) algorithms that are adaptable for both batch and online $Q$-learning, integrating valuable insights from offline source studies. The proposed transfer $Q$-learning algorithm contains a novel {\em re-targeting} step that enables {\em cross-stage transfer} along multiple stages in an RL task, besides the usual {\em cross-task transfer} for supervised learning. We establish the first theoretical justifications of TL in RL tasks by showing a faster rate of convergence of the $Q^*$-function estimation in the offline RL transfer, and a lower regret bound in the offline-to-online RL transfer under stage-wise reward similarity and mild design similarity across tasks. Empirical evidence from both synthetic and real datasets is presented to evaluate the proposed algorithm and support our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.04709v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The Electronic Journal of Statistics, 2025</arxiv:journal_reference>
      <dc:creator>Elynn Chen, Sai Li, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Online Quantile Regression</title>
      <link>https://arxiv.org/abs/2402.04602</link>
      <description>arXiv:2402.04602v4 Announce Type: replace-cross 
Abstract: This paper addresses the challenge of integrating sequentially arriving data within the quantile regression framework, where the number of features is allowed to grow with the number of observations, the horizon is unknown, and memory is limited. We employ stochastic sub-gradient descent to minimize the empirical check loss and study its statistical properties and regret performance. In our analysis, we unveil the delicate interplay between updating iterates based on individual observations versus batches of observations, revealing distinct regularity properties in each scenario. Our method ensures long-term optimal estimation irrespective of the chosen update strategy. Importantly, our contributions go beyond prior works by achieving exponential-type concentration inequalities and attaining optimal regret and error rates that exhibit only \textsf{ short-term} sensitivity to initial errors. A key insight from our study is the delicate statistical analyses and the revelation that appropriate stepsize schemes significantly mitigate the impact of initial errors on subsequent errors and regrets. This underscores the robustness of stochastic sub-gradient descent in handling initial uncertainties, emphasizing its efficacy in scenarios where the sequential arrival of data introduces uncertainties regarding both the horizon and the total number of observations. Additionally, when the initial error rate is well-controlled, there is a trade-off between short-term error rate and long-term optimality. Due to the lack of delicate statistical analysis for squared loss, we also briefly discuss its properties and proper schemes. Extensive simulations support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04602v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Shen, Dong Xia, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>Estimating Treatment Effects under Recommender Interference: A Structured Neural Networks Approach</title>
      <link>https://arxiv.org/abs/2406.14380</link>
      <description>arXiv:2406.14380v4 Announce Type: replace-cross 
Abstract: Recommender systems are essential for content-sharing platforms by curating personalized content. To improve recommender systems, platforms frequently rely on creator-side randomized experiments to evaluate algorithm updates. We show that commonly adopted difference-in-means estimators can lead to severely biased estimates due to recommender interference, where treated and control creators compete for exposure. This bias can result in incorrect business decisions. To address this, we propose a ``recommender choice model'' that explicitly represents the interference pathway. The approach combines a structural choice framework with neural networks to account for rich viewer-content heterogeneity. Building on this foundation, we develop a debiased estimator using the double machine learning (DML) framework to adjust for errors from nuisance component estimation. We show that the estimator is $\sqrt{n}$-consistent and asymptotically normal, and we extend the DML theory to handle correlated data, which arise in our context due to overlapped items. We validate our method with a large-scale field experiment on Weixin short-video platform, using a costly double-sided randomization design to obtain an interference-free ground truth. Our results show that the proposed estimator successfully recovers this ground truth, whereas benchmark estimators exhibit substantial bias, and in some cases, yield reversed signs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14380v4</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruohan Zhan, Shichao Han, Yuchen Hu, Zhenling Jiang</dc:creator>
    </item>
    <item>
      <title>The Gauss-Markov Adjunction Provides Categorical Semantics of Residuals in Supervised Learning</title>
      <link>https://arxiv.org/abs/2507.02442</link>
      <description>arXiv:2507.02442v3 Announce Type: replace-cross 
Abstract: Enhancing the intelligibility and interpretability of machine learning is a crucial task in responding to the demand for Explicability as an AI principle, and in promoting the better social implementation of AI. The aim of our research is to contribute to this improvement by reformulating machine learning models through the lens of category theory, thereby developing a semantic framework for structuring and understanding AI systems. Our categorical modeling in this paper clarifies and formalizes the structural interplay between residuals and parameters in supervised learning. The present paper focuses on the multiple linear regression model, which represents the most basic form of supervised learning. By defining two Lawvere-enriched categories corresponding to parameters and data, along with an adjoint pair of functors between them, we introduce our categorical formulation of supervised learning. We show that the essential structure of this framework is captured by what we call the Gauss-Markov Adjunction. Within this setting, the dual flow of information can be explicitly described as a correspondence between variations in parameters and residuals. The ordinary least squares estimator for the parameters and the minimum residual are related via the preservation of limits by the right adjoint functor. Furthermore, we position this formulation as an instance of extended denotational semantics for supervised learning, and propose applying a semantic perspective developed in theoretical computer science as a formal foundation for Explicability in AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02442v3</guid>
      <category>cs.AI</category>
      <category>math.CT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moto Kamiura</dc:creator>
    </item>
    <item>
      <title>Selecting the Best Arm in One-Shot Multi-Arm RCTs: The Asymptotic Minimax-Regret Decision Framework for the Best-Population Selection Problem</title>
      <link>https://arxiv.org/abs/2509.03796</link>
      <description>arXiv:2509.03796v2 Announce Type: replace-cross 
Abstract: We develop a frequentist decision-theoretic framework for selecting the best arm in one-shot, multi-arm randomized controlled trials (RCTs). Our approach characterizes the minimax-regret (MMR) optimal decision rule for any multivariate location family reward distribution with full support. We show that the MMR rule is deterministic, unique, and computationally tractable. We then specialize to the case of multivariate normal (MVN) rewards with an arbitrary covariance matrix, and establish the local asymptotic minimaxity of a plug-in version of the rule when only estimated means and covariances are available. This asymptotic MMR (AMMR) procedure maps a covariance-matrix estimate directly into decision boundaries, allowing straightforward implementation in practice. Our analysis highlights a sharp contrast between two-arm and multi-arm designs. With two arms, the "pick-the-winner" empirical success rule remains MMR-optimal, regardless of the arm-specific variances. By contrast, with three or more arms and heterogeneous variances, the empirical success rule is no longer optimal: the MMR decision boundaries become nonlinear and systematically penalize high-variance arms, requiring stronger evidence to select them. Our multi-arm AMMR framework offers a rigorous foundation that leads to practical criteria for comparing multiple policies simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03796v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joonhwi Joo</dc:creator>
    </item>
    <item>
      <title>Spatial and Temporal Boundaries in Difference-in-Differences: A Framework from Navier-Stokes Equation</title>
      <link>https://arxiv.org/abs/2510.11013</link>
      <description>arXiv:2510.11013v2 Announce Type: replace-cross 
Abstract: This paper develops a unified framework for identifying spatial and temporal boundaries of treatment effects in difference-in-differences designs. Starting from fundamental fluid dynamics equations (Navier-Stokes), we derive conditions under which treatment effects decay exponentially in space and time, enabling researchers to calculate explicit boundaries beyond which effects become undetectable. The framework encompasses both linear (pure diffusion) and nonlinear (advection-diffusion with chemical reactions) regimes, with testable scope conditions based on dimensionless numbers from physics (P\'eclet and Reynolds numbers). We demonstrate the framework's diagnostic capability using air pollution from coal-fired power plants. Analyzing 791 ground-based PM$_{2.5}$ monitors and 189,564 satellite-based NO$_2$ grid cells in the Western United States over 2019-2021, we find striking regional heterogeneity: within 100 km of coal plants, both pollutants show positive spatial decay (PM$_{2.5}$: $\kappa_s = 0.00200$, $d^* = 1,153$ km; NO$_2$: $\kappa_s = 0.00112$, $d^* = 2,062$ km), validating the framework. Beyond 100 km, negative decay parameters correctly signal that urban sources dominate and diffusion assumptions fail. Ground-level PM$_{2.5}$ decays approximately twice as fast as satellite column NO$_2$, consistent with atmospheric transport physics. The framework successfully diagnoses its own validity in four of eight analyzed regions, providing researchers with physics-based tools to assess whether their spatial difference-in-differences setting satisfies diffusion assumptions before applying the estimator. Our results demonstrate that rigorous boundary detection requires both theoretical derivation from first principles and empirical validation of underlying physical assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11013v2</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>math.ST</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Nonparametric Identification of Spatial Treatment Effect Boundaries: Evidence from Bank Branch Consolidation</title>
      <link>https://arxiv.org/abs/2510.13148</link>
      <description>arXiv:2510.13148v2 Announce Type: replace-cross 
Abstract: I develop a nonparametric framework for identifying spatial boundaries of treatment effects without imposing parametric functional form restrictions. The method employs local linear regression with data-driven bandwidth selection to flexibly estimate spatial decay patterns and detect treatment effect boundaries. Monte Carlo simulations demonstrate that the nonparametric approach exhibits lower bias and correctly identifies the absence of boundaries when none exist, unlike parametric methods that may impose spurious spatial patterns. I apply this framework to bank branch openings during 2015--2020, matching 5,743 new branches to 5.9 million mortgage applications across 14,209 census tracts. The analysis reveals that branch proximity significantly affects loan application volume (8.5\% decline per 10 miles) but not approval rates, consistent with branches stimulating demand through local presence while credit decisions remain centralized. Examining branch survival during the digital transformation era (2010--2023), I find a non-monotonic relationship with area income: high-income areas experience more closures despite conventional wisdom. This counterintuitive pattern reflects strategic consolidation of redundant branches in over-banked wealthy urban areas rather than discrimination against poor neighborhoods. Controlling for branch density, urbanization, and competition, the direct income effect diminishes substantially, with branch density emerging as the primary determinant of survival. These findings demonstrate the necessity of flexible nonparametric methods for detecting complex spatial patterns that parametric models would miss, and challenge simplistic narratives about banking deserts by revealing the organizational complexity underlying spatial consolidation decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13148v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effect Boundaries: A Continuous Functional Framework from Navier-Stokes Equations</title>
      <link>https://arxiv.org/abs/2510.14409</link>
      <description>arXiv:2510.14409v2 Announce Type: replace-cross 
Abstract: I develop a comprehensive theoretical framework for dynamic spatial treatment effect boundaries using continuous functional definitions grounded in Navier-Stokes partial differential equations. Rather than discrete treatment effect estimators, the framework characterizes treatment intensity as a continuous function $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of propagation dynamics, boundary evolution, and cumulative exposure patterns. Building on exact self-similar solutions expressible through Kummer confluent hypergeometric and modified Bessel functions, I establish that treatment effects follow scaling laws $\tau(d, t) = t^{-\alpha} f(d/t^\beta)$ where exponents characterize diffusion mechanisms. Empirical validation using 42 million TROPOMI satellite observations of NO$_2$ pollution from U.S. coal-fired power plants demonstrates strong exponential spatial decay ($\kappa_s = 0.004$ per km, $R^2 = 0.35$) with detectable boundaries at 572 km. Monte Carlo simulations confirm superior performance over discrete parametric methods in boundary detection and false positive avoidance (94\% vs 27\% correct rejection). Regional heterogeneity analysis validates diagnostic capability: positive decay parameters within 100 km confirm coal plant dominance; negative parameters beyond 100 km correctly signal when urban sources dominate. The continuous functional perspective unifies spatial econometrics with mathematical physics, providing theoretically grounded methods for boundary detection, exposure quantification, and policy evaluation across environmental economics, banking, and healthcare applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14409v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effects as Continuous Functionals: Theory and Evidence from Healthcare Access</title>
      <link>https://arxiv.org/abs/2510.15324</link>
      <description>arXiv:2510.15324v2 Announce Type: replace-cross 
Abstract: I develop a continuous functional framework for spatial treatment effects grounded in Navier-Stokes partial differential equations. Rather than discrete treatment parameters, the framework characterizes treatment intensity as continuous functions $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of boundary evolution, spatial gradients, and cumulative exposure. Empirical validation using 32,520 U.S. ZIP codes demonstrates exponential spatial decay for healthcare access ($\kappa = 0.002837$ per km, $R^2 = 0.0129$) with detectable boundaries at 37.1 km. The framework successfully diagnoses when scope conditions hold: positive decay parameters validate diffusion assumptions near hospitals, while negative parameters correctly signal urban confounding effects. Heterogeneity analysis reveals 2-13 $\times$ stronger distance effects for elderly populations and substantial education gradients. Model selection strongly favors logarithmic decay over exponential ($\Delta \text{AIC} &gt; 10,000$), representing a middle ground between exponential and power-law decay. Applications span environmental economics, banking, and healthcare policy. The continuous functional framework provides predictive capability ($d^*(t) = \xi^* \sqrt{t}$), parameter sensitivity ($\partial d^*/\partial \nu$), and diagnostic tests unavailable in traditional difference-in-differences approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15324v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
  </channel>
</rss>
