<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Oct 2025 14:03:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quantifying the AI Gap: A Comparative Index of Development in the United States and Chinese Regions</title>
      <link>https://arxiv.org/abs/2510.21832</link>
      <description>arXiv:2510.21832v1 Announce Type: new 
Abstract: This study develops a comprehensive Artificial Intelligence (AI) Index with seven primary dimensions, designed for provincial-level and industry-specific analysis. We employ an anchor point method for data normalization, using fixed upper and lower bounds as benchmarks, and devise a hierarchical indicator weighting system that combines expert judgment with objective data. The index draws from authoritative data sources across domains including official statistics, patents and research outputs, education and talent, industrial economy, policy and governance, and social impact. The China-US comparison indicates that under a unified framework, the US composite score (68.1) exceeds China's (59.4). We further dissect China into seven main areas to form a sub-national index. The findings reveal stark regional disparities in China's AI development: the North, East, and South regions lead in composite scores, whereas central and western regions lag significantly, underscoring the effects of regional concentration of innovation and industry resources. This research provides an academic reference and decision support tool for government agencies and research institutions, informing more targeted regional AI development strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21832v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuanxi Li, Lei Yin</dc:creator>
    </item>
    <item>
      <title>A quality of mercy is not trained: the imagined vs. the practiced in healthcare process-specialized AI development</title>
      <link>https://arxiv.org/abs/2510.21843</link>
      <description>arXiv:2510.21843v1 Announce Type: new 
Abstract: In high stakes organizational contexts like healthcare, artificial intelligence (AI) systems are increasingly being designed to augment complex coordination tasks. This paper investigates how the ethical stakes of such systems are shaped by their epistemic framings: what aspects of work they represent, and what they exclude. Drawing on an embedded study of AI development for operating room (OR) scheduling at a Canadian hospital, we compare scheduling-as-imagined in the AI design process: rule-bound, predictable, and surgeon-centric, with scheduling-as-practiced as a fluid, patient-facing coordination process involving ethical discretion. We show how early representational decisions narrowed what the AI could support, resulting in epistemic foreclosure: the premature exclusion of key ethical dimensions from system design. Our findings surface the moral consequences of abstraction and call for a more situated approach to designing healthcare process-specialized artificial intelligence systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21843v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anand Bhardwaj, Samer Faraj</dc:creator>
    </item>
    <item>
      <title>Enhancing Student Performance Prediction In CS1 Via In-Class Coding</title>
      <link>https://arxiv.org/abs/2510.21848</link>
      <description>arXiv:2510.21848v1 Announce Type: new 
Abstract: Computer science's increased recognition as a prominent field of study has attracted students with diverse academic backgrounds. This has significantly increased the already high failure rates in introductory courses. To address this challenge, it is essential to identify struggling students early on. Incorporating in-class coding exercises in these courses not only offers additional practice opportunities to students but may also reveal their abilities and help teachers identify those in need of assistance. In this work, we seek to determine the extent to which the practice of using in-class coding exercises enhances the ability to predict student performance, especially early in the semester. Based on data obtained in a CS1 course taught at a mid-size American university, we found that in-class exercises could improve the prediction of students' eventual performance. In particular, we found relatively accurately predictions as early as academic weeks 3 through 5, making it possible to devise early intervention strategies. This work can benefit future studies on the impact of in-class exercises as well as intervention strategies throughout the semester.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21848v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Hics, Vinhthuy Phan, Kriangsiri Malasri</dc:creator>
    </item>
    <item>
      <title>Data-Driven Approach to Capitation Reform in Rwanda</title>
      <link>https://arxiv.org/abs/2510.21851</link>
      <description>arXiv:2510.21851v1 Announce Type: new 
Abstract: As part of Rwanda's transition toward universal health coverage, the national Community-Based Health Insurance (CBHI) scheme is moving from retrospective fee-for-service reimbursements to prospective capitation payments for public primary healthcare providers. This report outlines a data-driven approach to designing, calibrating, and monitoring the capitation model using individual-level claims data from the Intelligent Health Benefits System (IHBS). We introduce a transparent, interpretable formula for allocating payments to Health Centers and their affiliated Health Posts. The formula is based on catchment population, service utilization patterns, and patient inflows, with parameters estimated via regression models calibrated on national claims data. Repeated validation exercises show the payment scheme closely aligns with historical spending while promoting fairness and adaptability across diverse facilities. In addition to payment design, the same dataset enables actionable behavioral insights. We highlight the use case of monitoring antibiotic prescribing patterns, particularly in pediatric care, to flag potential overuse and guideline deviations. Together, these capabilities lay the groundwork for a learning health financing system: one that connects digital infrastructure, resource allocation, and service quality to support continuous improvement and evidence-informed policy reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21851v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Babaniyi Olaniyi, Ina Kalisa, Ana Fern\'andez del R\'io, Jean Marie Vianney Hakizayezu, Enric Jan\'e, Eniola Olaleye, Juan Francisco Garamendi, Ivan Nazarov, Aditya Rastogi, Mateo Diaz-Quiroz, \'Africa Peri\'a\~nez, Regis Hitimana</dc:creator>
    </item>
    <item>
      <title>What Exactly is a Deepfake?</title>
      <link>https://arxiv.org/abs/2510.22128</link>
      <description>arXiv:2510.22128v1 Announce Type: new 
Abstract: Deepfake technologies are often associated with deception, misinformation, and identity fraud, raising legitimate societal concerns. Yet such narratives may obscure a key insight: deepfakes embody sophisticated capabilities for sensory manipulation that can alter human perception, potentially enabling beneficial applications in domains such as healthcare and education. Realizing this potential, however, requires understanding how the technology is conceptualized across disciplines. This paper analyzes 826 peer-reviewed publications from 2017 to 2025 to examine how deepfakes are defined and understood in the literature. Using large language models for content analysis, we categorize deepfake conceptualizations along three dimensions: Identity Source (the relationship between original and generated content), Intent (deceptive versus non-deceptive purposes), and Manipulation Granularity (holistic versus targeted modifications). Results reveal substantial heterogeneity that challenges simplified public narratives. Notably, a subset of studies discuss non-deceptive applications, highlighting an underexplored potential for social good. Temporal analysis shows an evolution from predominantly threat-focused views (2017 to 2019) toward recognition of beneficial applications (2022 to 2025). This study provides an empirical foundation for developing nuanced governance and research frameworks that distinguish applications warranting prohibition from those deserving support, showing that, with safeguards, deepfakes' realism can serve important social purposes beyond deception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22128v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizhi Liu, Balaji Padmanabhan, Siva Viswanathan</dc:creator>
    </item>
    <item>
      <title>Surface Reading LLMs: Synthetic Text and its Styles</title>
      <link>https://arxiv.org/abs/2510.22162</link>
      <description>arXiv:2510.22162v1 Announce Type: new 
Abstract: Despite a potential plateau in ML advancement, the societal impact of large language models lies not in approaching superintelligence but in generating text surfaces indistinguishable from human writing. While Critical AI Studies provides essential material and socio-technical critique, it risks overlooking how LLMs phenomenologically reshape meaning-making. This paper proposes a semiotics of "surface integrity" as attending to the immediate plane where LLMs inscribe themselves into human communication. I distinguish three knowledge interests in ML research (epistemology, epist\=em\=e, and epistemics) and argue for integrating surface-level stylistic analysis alongside depth-oriented critique. Through two case studies examining stylistic markers of synthetic text, I argue how attending to style as a semiotic phenomenon reveals LLMs as cultural actors that transform the conditions of meaning emergence and circulation in contemporary discourse, independent of questions about machine consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22162v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannes Bajohr</dc:creator>
    </item>
    <item>
      <title>The AI Tutor in Engineering Education: Design, Results, and Redesign of an Experience in Hydrology at an Argentine University</title>
      <link>https://arxiv.org/abs/2510.22279</link>
      <description>arXiv:2510.22279v1 Announce Type: new 
Abstract: The emergence of Generative Artificial Intelligence (GenAI) has reshaped higher education, presenting both opportunities and ethical-pedagogical challenges. This article presents an empirical case study on the complete cycle (design, initial failure, redesign, and re-evaluation) of an intervention using an AI Tutor (ChatGPT) in the "Hydrology and Hydraulic Works" course (Civil Engineering, UTN-FRT, Argentina). The study documents two interventions in the same cohort (n=23). The first resulted in widespread failure (0% pass rate) due to superficial use and serious academic integrity issues (65% similarity, copies &gt; 80%). This failure forced a comprehensive methodological redesign. The second intervention, based on a redesigned prompt (Prompt V2) with strict evidence controls (mandatory Appendix A with exported chat, minimum time $\geq$ 120 minutes, verifiable numerical exercise) and a refined rubric (Rubric V2), showed significantly better results: a median score of 88/100 and verifiable compliance with genuine interaction processes. Using a mixed-methods approach (reproducible document analysis and rubric analysis), the impact of the redesign on integrity and technical performance is evaluated. The results demonstrate that, without explicit process controls, students prioritize efficiency over deep learning, submitting documents without real traceability. A transferable assessment protocol for STEM courses is proposed, centered on "auditable personal zones," to foster higher-order thinking. The study provides key empirical evidence from the context of a public Latin American university.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22279v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Roger Paz</dc:creator>
    </item>
    <item>
      <title>Hybrid Instructor Ai Assessment In Academic Projects: Efficiency, Equity, And Methodological Lessons</title>
      <link>https://arxiv.org/abs/2510.22286</link>
      <description>arXiv:2510.22286v1 Announce Type: new 
Abstract: In technical subjects characterized by high enrollment, such as Basic Hydraulics, the assessment of reports necessitates superior levels of objectivity, consistency, and formative feedback; goals often compromised by faculty workload. This study presents the implementation of a generative artificial intelligence (AI) assisted assessment system, supervised by instructors, to grade 33 hydraulics reports. The central objective was to quantify its impact on the efficiency, quality, and fairness of the process. The employed methodology included the calibration of the Large Language Model (LLM) with a detailed rubric, the batch processing of assignments, and a human-in-the-loop validation phase. The quantitative results revealed a noteworthy 88% reduction in grading time (from 50 to 6 minutes per report, including verification) and a 733% increase in productivity. The quality of feedback was substantially improved, evidenced by 100% rubric coverage and a 150% increase in the anchoring of comments to textual evidence. The system proved to be equitable, exhibiting no bias related to report length, and highly reliable post-calibration (r = 0.96 between scores). It is concluded that the hybrid AI-instructor model optimizes the assessment process, thereby liberating time for high-value pedagogical tasks and enhancing the fairness and quality of feedback, in alignment with UNESCO's principles on the ethical use of AI in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22286v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Roger Paz</dc:creator>
    </item>
    <item>
      <title>Stop the Nonconsensual Use of Nude Images in Research</title>
      <link>https://arxiv.org/abs/2510.22423</link>
      <description>arXiv:2510.22423v1 Announce Type: new 
Abstract: In order to train, test, and evaluate nudity detection models, machine learning researchers typically rely on nude images scraped from the Internet. Our research finds that this content is collected and, in some cases, subsequently distributed by researchers without consent, leading to potential misuse and exacerbating harm against the subjects depicted. This position paper argues that the distribution of nonconsensually collected nude images by researchers perpetuates image-based sexual abuse and that the machine learning community should stop the nonconsensual use of nude images in research. To characterize the scope and nature of this problem, we conducted a systematic review of papers published in computing venues that collect and use nude images. Our results paint a grim reality: norms around the usage of nude images are sparse, leading to a litany of problematic practices like distributing and publishing nude images with uncensored faces, and intentionally collecting and sharing abusive content. We conclude with a call-to-action for publishing venues and a vision for research in nudity detection that balances user agency with concrete research objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22423v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Princessa Cintaqia, Arshia Arya, Elissa M Redmiles, Deepak Kumar, Allison McDonald, Lucy Qin</dc:creator>
    </item>
    <item>
      <title>TLSQKT: A Question-Aware Dual-Channel Transformer for Literacy Tracing from Learning Sequences</title>
      <link>https://arxiv.org/abs/2510.22488</link>
      <description>arXiv:2510.22488v1 Announce Type: new 
Abstract: Knowledge tracing (KT) supports personalized learning by modeling how students' knowledge states evolve over time. However, most KT models emphasize mastery of discrete knowledge components, limiting their ability to characterize broader literacy development. We reframe the task as Literacy Tracing (LT), which models the growth of higher-order cognitive abilities and literacy from learners' interaction sequences, and we instantiate this paradigm with a Transformer-based model, TLSQKT (Transformer for Learning Sequences with Question-Aware Knowledge Tracing). TLSQKT employs a dual-channel design that jointly encodes student responses and item semantics, while question-aware interaction and self-attention capture long-range dependencies in learners' evolving states. Experiments on three real-world datasets - one public benchmark, one private knowledge-component dataset, and one private literacy dataset - show that TLSQKT consistently outperforms strong KT baselines on literacy-oriented metrics and reveals interpretable developmental trajectories of learners' literacy. Transfer experiments further indicate that knowledge-tracing signals can be leveraged for literacy tracing, offering a practical route when dedicated literacy labels are limited. These findings position literacy tracing as a scalable component of intelligent educational systems and lay the groundwork for literacy evaluation in future large-scale educational models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22488v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhifeng Wang, Yaowei Dong, Chunyan Zeng</dc:creator>
    </item>
    <item>
      <title>Women upskilling or reskilling to an ICT career: A systematic review of drivers and barriers</title>
      <link>https://arxiv.org/abs/2510.22508</link>
      <description>arXiv:2510.22508v1 Announce Type: new 
Abstract: Demand for technology focused STEM professionals will increase globally over the coming decade, with many countries finding it difficult to meet growing demand. Compounding this are difficulties in attracting and retaining female technology-focused professionals. Research seeking to address this gender imbalance and workforce shortage focuses on increasing participation among school leavers. However, there is a paucity of research around the potential for females to upskill or reskill into an ICT career. As a starting point, this review asks the question: "What potential drivers and barriers have been identified that impact on female intentions or choices to reskill or upskill to a technology focused STEM career". Results indicate dissatisfaction in a first career, combined with positive computing experiences in the workplace can rouse interest in computing professions. Learning of job opportunities, especially from salient referents, is also a key driver. Results indicate women must overcome negative identity and academic beliefs, as well as self-doubt to make the switch. In summary, it is possible to increase and diversify the tech workforce by leveraging women's latent interest in computing. This review provides a roadmap for research to support educational institutions, employers, and women to benefit from upskilling or reskilling opportunities</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22508v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shondell Williams, Karen Blackmore, Regina Berretta, Michelle Mansfield</dc:creator>
    </item>
    <item>
      <title>Barriers to Integrating Low-Power IoT in Engineering Education: A Survey of the Literature</title>
      <link>https://arxiv.org/abs/2510.22522</link>
      <description>arXiv:2510.22522v1 Announce Type: new 
Abstract: Low-power Internet of Things (IoT) technologies are becoming increasingly important in engineering education as a tool to help students connect theory to real applications. However, many institutions face barriers that slow down their adoption in courses and labs. This paper reviews recent studies to understand these barriers and organizes them into three groups: technical, organizational, and curricular/pedagogical. Technical barriers include energy management, scalability, and integration issues. Organizational barriers are related to cost, planning, and the need for trained staff. Curricular and pedagogical barriers include gaps in student readiness, limited lab time, and platform choices that depend on budget. By detailing these barriers with practical examples, this paper aims to help educators and academic leaders develop more effective strategies to adopt low-power IoT in engineering programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22522v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V. Sanchez Padilla, Albert Espinal, Jose Cordova-Garcia, Lisa Schibelius</dc:creator>
    </item>
    <item>
      <title>How Can AI Augment Access to Justice? Public Defenders' Perspectives on AI Adoption</title>
      <link>https://arxiv.org/abs/2510.22933</link>
      <description>arXiv:2510.22933v1 Announce Type: new 
Abstract: Public defenders are asked to do more with less: representing clients deserving of adequate counsel while facing overwhelming caseloads and scarce resources. While artificial intelligence (AI) and large language models (LLMs) are promoted as tools to alleviate this burden, such proposals are detached from the lived realities of public defenders. This study addresses that gap through semi-structured interviews with fourteen practitioners across the United States to examine their experiences with AI, anticipated applications, and ethical concerns. We find that AI adoption is constrained by costs, restrictive office norms, confidentiality risks, and unsatisfactory tool quality. To clarify where AI can and cannot contribute, we propose a task-level map of public defense. Public defenders view AI as most useful for evidence investigation to analyze overwhelming amounts of digital records, with narrower roles in legal research &amp; writing, and client communication. Courtroom representation and defense strategy are considered least compatible with AI assistance, as they depend on contextual judgment and trust. Public defenders emphasize safeguards for responsible use, including mandatory human verification, limits on overreliance, and the preservation of relational aspect of lawyering. Building on these findings, we outline a research agenda that promotes equitable access to justice by prioritizing open-source models, domain-specific datasets and evaluation, and participatory design that incorporates defenders' perspectives into system development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22933v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Inyoung Cheong, Patty Liu, Dominik Stammbach, Peter Henderson</dc:creator>
    </item>
    <item>
      <title>From Perceived Effectiveness to Measured Impact: Identity-Aware Evaluation of Automated Counter-Stereotypes</title>
      <link>https://arxiv.org/abs/2510.23523</link>
      <description>arXiv:2510.23523v1 Announce Type: new 
Abstract: We investigate the effect of automatically generated counter-stereotypes on gender bias held by users of various demographics on social media. Building on recent NLP advancements and social psychology literature, we evaluate two counter-stereotype strategies -- counter-facts and broadening universals (i.e., stating that anyone can have a trait regardless of group membership) -- which have been identified as the most potentially effective in previous studies. We assess the real-world impact of these strategies on mitigating gender bias across user demographics (gender and age), through the Implicit Association Test and the self-reported measures of explicit bias and perceived utility. Our findings reveal that actual effectiveness does not align with perceived effectiveness, and the former is a nuanced and sometimes divergent phenomenon across demographic groups. While overall bias reduction was limited, certain groups (e.g., older, male participants) exhibited measurable improvements in implicit bias in response to some interventions. Conversely, younger participants, especially women, showed increasing bias in response to the same interventions. These results highlight the complex and identity-sensitive nature of stereotype mitigation and call for dynamic and context-aware evaluation and mitigation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23523v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Svetlana Kiritchenko, Anna Kerkhof, Isar Nejadgholi, Kathleen C. Fraser</dc:creator>
    </item>
    <item>
      <title>Exploring the Applications of Generative AI in High School STEM Education</title>
      <link>https://arxiv.org/abs/2510.21718</link>
      <description>arXiv:2510.21718v1 Announce Type: cross 
Abstract: In recent years, ChatGPT \cite{openai_2023_gpt4} along with Microsoft Copilot have become subjects of great discourse, particularly in the field of education. Prior research has hypothesized on potential impacts these tools could have on student learning and performance. These have primarily relied on trends from prior applications of technology in education and an understanding of the limitations and strengths of Generative AI in other applications. This study utilizes an experimental approach to analyze the impacts of Generative AI on high school STEM education (physics in particular). In accordance with most findings, generative AI does have some positive impact on student performance. However, our findings have shown that the most significant impact is an increase in student engagement with the subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21718v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ishaan Masilamony</dc:creator>
    </item>
    <item>
      <title>GAMER PAT: Research as a Serious Game</title>
      <link>https://arxiv.org/abs/2510.21719</link>
      <description>arXiv:2510.21719v1 Announce Type: cross 
Abstract: As generative AI increasingly outperforms students in producing academic writing, a critical question arises: how can we preserve the motivation, creativity, and intellectual growth of novice researchers in an age of automated academic achievement? This paper introduces GAMER PAT (GAme MastER, Paper Authoring Tutor), a prompt-engineered AI chatbot that reframes research paper writing as a serious game. Through role-playing mechanics, users interact with a co-author NPC and anonymous reviewer NPCs, turning feedback into "missions" and advancing through a narrative-driven writing process.
  Our study reports on 26+ gameplay chat logs, including both autoethnography and use by graduate students under supervision. Using qualitative log analysis with SCAT (Steps for Coding and Theorization), we identified an emergent four-phase scaffolding pattern: (1) question posing, (2) meta-perspective, (3) structuring, and (4) recursive reflection. These results suggest that GAMER PAT supports not only the structural development of research writing but also reflective and motivational aspects.
  We present this work as a descriptive account of concept and process, not a causal evaluation. We also include a speculative outlook envisioning how humans may continue to cultivate curiosity and agency alongside AI-driven research. This arXiv version thus provides both a descriptive report of design and usage, and a forward-looking provocation for future empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21719v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kenji Saito, Rei Tadika</dc:creator>
    </item>
    <item>
      <title>Butterfly: glo-cal effects of data, energy and industry, New Media and Performance Exhibition Catalogue</title>
      <link>https://arxiv.org/abs/2510.21893</link>
      <description>arXiv:2510.21893v1 Announce Type: cross 
Abstract: The exhibition Butterfly: Glo-cal Effects of Data, Energy, and Industry is, at its core, a meditation on entanglement-between the global and the local, the ecological and the digital, the material and the virtual. It asks how we might reframe the infrastructures that shape our lives not only as technologies of efficiency or convenience, but as ecosystems themselves: dynamic, interdependent, and in need of care. It emerges from a prior exhibition, EcoDigital Futures, presented as part of Melbourne Design Week 2024, an Australian initiative of Creative Victoria and the National Gallery of Australia. That landmark exhibition spotlighted the growing imperative to align our digital futures with ecological sensibilities. Butterfly carries forward this vision, but also intensifies it by shifting the focus from speculation to activation, from theory to lived intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21893v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebekah Rousi, Toija Cinque, Katey O'Sullivan</dc:creator>
    </item>
    <item>
      <title>We Need Accountability in Human-AI Agent Relationships</title>
      <link>https://arxiv.org/abs/2510.21967</link>
      <description>arXiv:2510.21967v1 Announce Type: cross 
Abstract: We argue that accountability mechanisms are needed in human-AI agent relationships to ensure alignment with user and societal interests. We propose a framework according to which AI agents' engagement is conditional on appropriate user behaviour. The framework incorporates design-strategies such as distancing, disengaging, and discouraging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21967v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Lange, Geoff Keeling, Arianna Manzini, Amanda McCroskery</dc:creator>
    </item>
    <item>
      <title>Do You Trust the Process?: Modeling Institutional Trust for Community Adoption of Reinforcement Learning Policies</title>
      <link>https://arxiv.org/abs/2510.22017</link>
      <description>arXiv:2510.22017v1 Announce Type: cross 
Abstract: Many governmental bodies are adopting AI policies for decision-making. In particular, Reinforcement Learning has been used to design policies that citizens would be expected to follow if implemented. Much RL work assumes that citizens follow these policies, and evaluate them with this in mind. However, we know from prior work that without institutional trust, citizens will not follow policies put in place by governments. In this work, we develop a trust-aware RL algorithm for resource allocation in communities. We consider the case of humanitarian engineering, where the organization is aiming to distribute some technology or resource to community members. We use a Deep Deterministic Policy Gradient approach to learn a resource allocation that fits the needs of the organization. Then, we simulate resource allocation according to the learned policy, and model the changes in institutional trust of community members. We investigate how this incorporation of institutional trust affects outcomes, and ask how effectively an organization can learn policies if trust values are private. We find that incorporating trust into RL algorithms can lead to more successful policies, specifically when the organization's goals are less certain. We find more conservative trust estimates lead to increased fairness and average community trust, though organization success suffers. Finally, we explore a strategy to prevent unfair outcomes to communities. We implement a quota system by an external entity which decreases the organization's utility when it does not serve enough community members. We find this intervention can improve fairness and trust among communities in some cases, while decreasing the success of the organization. This work underscores the importance of institutional trust in algorithm design and implementation, and identifies a tension between organization success and community well-being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22017v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naina Balepur, Xingrui Pei, Hari Sundaram</dc:creator>
    </item>
    <item>
      <title>Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model</title>
      <link>https://arxiv.org/abs/2510.22057</link>
      <description>arXiv:2510.22057v1 Announce Type: cross 
Abstract: With the rise of online and virtual learning, monitoring and enhancing student engagement have become an important aspect of effective education. Traditional methods of assessing a student's involvement might not be applicable directly to virtual environments. In this study, we focused on this problem and addressed the need to develop an automated system to detect student engagement levels during online learning. We proposed a novel training method which can discourage a model from leveraging sensitive features like gender for its predictions. The proposed method offers benefits not only in the enforcement of ethical standards, but also to enhance interpretability of the model predictions. We applied an attribute-orthogonal regularization technique to a split-model classifier, which uses multiple transfer learning strategies to achieve effective results in reducing disparity in the distribution of prediction for sensitivity groups from a Pearson correlation coefficient of 0.897 for the unmitigated model, to 0.999 for the mitigated model. The source code for this project is available on https://github.com/ashiskb/elearning-engagement-study .</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22057v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Thiering, Tarun Sethupat Radha Krishna, Dylan Zelkin, Ashis Kumer Biswas</dc:creator>
    </item>
    <item>
      <title>A Multi-level Analysis of Factors Associated with Student Performance: A Machine Learning Approach to the SAEB Microdata</title>
      <link>https://arxiv.org/abs/2510.22266</link>
      <description>arXiv:2510.22266v1 Announce Type: cross 
Abstract: Identifying the factors that influence student performance in basic education is a central challenge for formulating effective public policies in Brazil. This study introduces a multi-level machine learning approach to classify the proficiency of 9th-grade and high school students using microdata from the System of Assessment of Basic Education (SAEB). Our model uniquely integrates four data sources: student socioeconomic characteristics, teacher professional profiles, school indicators, and director management profiles. A comparative analysis of four ensemble algorithms confirmed the superiority of a Random Forest model, which achieved 90.2% accuracy and an Area Under the Curve (AUC) of 96.7%. To move beyond prediction, we applied Explainable AI (XAI) using SHAP, which revealed that the school's average socioeconomic level is the most dominant predictor, demonstrating that systemic factors have a greater impact than individual characteristics in isolation. The primary conclusion is that academic performance is a systemic phenomenon deeply tied to the school's ecosystem. This study provides a data-driven, interpretable tool to inform policies aimed at promoting educational equity by addressing disparities between schools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22266v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rodrigo Tertulino, Ricardo Almeida</dc:creator>
    </item>
    <item>
      <title>Machine Learning Enabled Early Warning System For Financial Distress Using Real-Time Digital Signals</title>
      <link>https://arxiv.org/abs/2510.22287</link>
      <description>arXiv:2510.22287v1 Announce Type: cross 
Abstract: The growing instability of both global and domestic economic environments has increased the risk of financial distress at the household level. However, traditional econometric models often rely on delayed and aggregated data, limiting their effectiveness. This study introduces a machine learning-based early warning system that utilizes real-time digital and macroeconomic signals to identify financial distress in near real-time. Using a panel dataset of 750 households tracked over three monitoring rounds spanning 13 months, the framework combines socioeconomic attributes, macroeconomic indicators (such as GDP growth, inflation, and foreign exchange fluctuations), and digital economy measures (including ICT demand and market volatility). Through data preprocessing and feature engineering, we introduce lagged variables, volatility measures, and interaction terms to capture both gradual and sudden changes in financial stability. We benchmark baseline classifiers, such as logistic regression and decision trees, against advanced ensemble models including random forests, XGBoost, and LightGBM. Our results indicate that the engineered features from the digital economy significantly enhance predictive accuracy. The system performs reliably for both binary distress detection and multi-class severity classification, with SHAP-based explanations identifying inflation volatility and ICT demand as key predictors. Crucially, the framework is designed for scalable deployment in national agencies and low-bandwidth regional offices, ensuring it is accessible for policymakers and practitioners. By implementing machine learning in a transparent and interpretable manner, this study demonstrates the feasibility and impact of providing near-real-time early warnings of financial distress. This offers actionable insights that can strengthen household resilience and guide preemptive intervention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22287v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laxmi pant, Syed Ali Reza, Md Khalilor Rahman, MD Saifur Rahman, Shamima Sharmin, Md Fazlul Huq Mithu, Kazi Nehal Hasnain, Adnan Farabi, Mahamuda khanom, Raisul Kabir</dc:creator>
    </item>
    <item>
      <title>Predicting Metabolic Dysfunction-Associated Steatotic Liver Disease using Machine Learning Methods</title>
      <link>https://arxiv.org/abs/2510.22293</link>
      <description>arXiv:2510.22293v1 Announce Type: cross 
Abstract: Background: Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD) affects ~33% of U.S. adults and is the most common chronic liver disease. Although often asymptomatic, progression can lead to cirrhosis. Early detection is important, as lifestyle interventions can prevent disease progression. We developed a fair, rigorous, and reproducible MASLD prediction model and compared it to prior methods using a large electronic health record database.
  Methods: We evaluated LASSO logistic regression, random forest, XGBoost, and a neural network for MASLD prediction using clinical feature subsets, including the top 10 SHAP-ranked features. To reduce disparities in true positive rates across racial and ethnic subgroups, we applied an equal opportunity postprocessing method.
  Results: This study included 59,492 patients in the training data, 24,198 in the validating data, and 25,188 in the testing data. The LASSO logistic regression model with the top 10 features was selected for its interpretability and comparable performance. Before fairness adjustment, the model achieved AUROC of 0.84, accuracy of 78%, sensitivity of 72%, specificity of 79%, and F1-score of 0.617. After equal opportunity postprocessing, accuracy modestly increased to 81% and specificity to 94%, while sensitivity decreased to 41% and F1-score to 0.515, reflecting the fairness trade-off.
  Conclusions: We developed the MASER prediction model (MASLD Static EHR Risk Prediction), a LASSO logistic regression model which achieved competitive performance for MASLD prediction (AUROC 0.836, accuracy 77.6%), comparable to previously reported ensemble and tree-based models. Overall, this approach demonstrates that interpretable models can achieve a balance of predictive performance and fairness in diverse patient populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22293v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mary E. An, Paul Griffin, Jonathan G. Stine, Ramakrishna Balakrishnan, Ram Sriram, Soundar Kumara</dc:creator>
    </item>
    <item>
      <title>Bias Begins with Data: The FairGround Corpus for Robust and Reproducible Research on Algorithmic Fairness</title>
      <link>https://arxiv.org/abs/2510.22363</link>
      <description>arXiv:2510.22363v1 Announce Type: cross 
Abstract: As machine learning (ML) systems are increasingly adopted in high-stakes decision-making domains, ensuring fairness in their outputs has become a central challenge. At the core of fair ML research are the datasets used to investigate bias and develop mitigation strategies. Yet, much of the existing work relies on a narrow selection of datasets--often arbitrarily chosen, inconsistently processed, and lacking in diversity--undermining the generalizability and reproducibility of results.
  To address these limitations, we present FairGround: a unified framework, data corpus, and Python package aimed at advancing reproducible research and critical data studies in fair ML classification. FairGround currently comprises 44 tabular datasets, each annotated with rich fairness-relevant metadata. Our accompanying Python package standardizes dataset loading, preprocessing, transformation, and splitting, streamlining experimental workflows. By providing a diverse and well-documented dataset corpus along with robust tooling, FairGround enables the development of fairer, more reliable, and more reproducible ML models. All resources are publicly available to support open and collaborative research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22363v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Simson, Alessandro Fabris, Cosima Fr\"ohner, Frauke Kreuter, Christoph Kern</dc:creator>
    </item>
    <item>
      <title>Cross-Platform Short-Video Diplomacy: Topic and Sentiment Analysis of China-US Relations on Douyin and TikTok</title>
      <link>https://arxiv.org/abs/2510.22415</link>
      <description>arXiv:2510.22415v1 Announce Type: cross 
Abstract: We examine discussions surrounding China-U.S. relations on the Chinese and American social media platforms \textit{Douyin} and \textit{TikTok}. Both platforms, owned by \textit{ByteDance}, operate under different regulatory and cultural environments, providing a unique perspective for analyzing China-U.S. public discourse. This study analyzed 4,040 videos and 338,209 user comments to assess the public discussions and sentiments on social media regarding China-U.S. relations. Through topic clustering and sentiment analysis, we identified key themes, including economic strength, technological and industrial interdependence, cultural cognition and value pursuits, and responses to global challenges. There are significant emotional differences between China and the US on various themes. Since April 2022, the Chinese government has implemented a new regulation requiring all social media accounts to disclose their provincial-level geolocation information. Utilizing this publicly available data, along with factors such as GDP per capita, minority index, and internet penetration rate, we investigate the changes in sentiment towards the U.S. in mainland China. This study links socioeconomic indicators with online discussions, deeply analyzing how regional and economic factors influence Chinese comments on their views of the US, providing important insights for China-U.S. relationship research and policy making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22415v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheng Wei, Mingchen Li, Junxiang Liao, Zeyu Yang, Xiaoyu Yang, Yixuan Xie, Pan Hui, Huamin Qu</dc:creator>
    </item>
    <item>
      <title>Group size effects and collective misalignment in LLM multi-agent systems</title>
      <link>https://arxiv.org/abs/2510.22422</link>
      <description>arXiv:2510.22422v1 Announce Type: cross 
Abstract: Multi-agent systems of large language models (LLMs) are rapidly expanding across domains, introducing dynamics not captured by single-agent evaluations. Yet, existing work has mostly contrasted the behavior of a single agent with that of a collective of fixed size, leaving open a central question: how does group size shape dynamics? Here, we move beyond this dichotomy and systematically explore outcomes across the full range of group sizes. We focus on multi-agent misalignment, building on recent evidence that interacting LLMs playing a simple coordination game can generate collective biases absent in individual models. First, we show that collective bias is a deeper phenomenon than previously assessed: interaction can amplify individual biases, introduce new ones, or override model-level preferences. Second, we demonstrate that group size affects the dynamics in a non-linear way, revealing model-dependent dynamical regimes. Finally, we develop a mean-field analytical approach and show that, above a critical population size, simulations converge to deterministic predictions that expose the basins of attraction of competing equilibria. These findings establish group size as a key driver of multi-agent dynamics and highlight the need to consider population-level effects when deploying LLM-based systems at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22422v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Flint, Luca Maria Aiello, Romualdo Pastor-Satorras, Andrea Baronchelli</dc:creator>
    </item>
    <item>
      <title>Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance</title>
      <link>https://arxiv.org/abs/2510.22602</link>
      <description>arXiv:2510.22602v1 Announce Type: cross 
Abstract: Building on decades of success in digital infrastructure and biomedical innovation, we propose the Personal Care Utility (PCU) - a cybernetic system for lifelong health guidance. PCU is conceived as a global, AI-powered utility that continuously orchestrates multimodal data, knowledge, and services to assist individuals and populations alike. Drawing on multimodal agents, event-centric modeling, and contextual inference, it offers three essential capabilities: (1) trusted health information tailored to the individual, (2) proactive health navigation and behavior guidance, and (3) ongoing interpretation of recovery and treatment response after medical events. Unlike conventional episodic care, PCU functions as an ambient, adaptive companion - observing, interpreting, and guiding health in real time across daily life. By integrating personal sensing, experiential computing, and population-level analytics, PCU promises not only improved outcomes for individuals but also a new substrate for public health and scientific discovery. We describe the architecture, design principles, and implementation challenges of this emerging paradigm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22602v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahyar Abbasian, Ramesh Jain</dc:creator>
    </item>
    <item>
      <title>Modeling Political Discourse with Sentence-BERT and BERTopic</title>
      <link>https://arxiv.org/abs/2510.22904</link>
      <description>arXiv:2510.22904v1 Announce Type: cross 
Abstract: Social media has reshaped political discourse, offering politicians a platform for direct engagement while reinforcing polarization and ideological divides. This study introduces a novel topic evolution framework that integrates BERTopic-based topic modeling with Moral Foundations Theory (MFT) to analyze the longevity and moral dimensions of political topics in Twitter activity during the 117th U.S. Congress. We propose a methodology for tracking dynamic topic shifts over time and measuring their association with moral values and quantifying topic persistence. Our findings reveal that while overarching themes remain stable, granular topics tend to dissolve rapidly, limiting their long-term influence. Moreover, moral foundations play a critical role in topic longevity, with Care and Loyalty dominating durable topics, while partisan differences manifest in distinct moral framing strategies. This work contributes to the field of social network analysis and computational political discourse by offering a scalable, interpretable approach to understanding moral-driven topic evolution on social media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22904v1</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Margarida Mendonca, Alvaro Figueira</dc:creator>
    </item>
    <item>
      <title>LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models</title>
      <link>https://arxiv.org/abs/2510.23011</link>
      <description>arXiv:2510.23011v1 Announce Type: cross 
Abstract: Language educators strive to create a rich experience for learners, while they may be restricted in the extend of feedback and practice they can provide. We present the design and development of LangLingual, a conversational agent built using the LangChain framework and powered by Large Language Models. The system is specifically designed to provide real-time, grammar-focused feedback, generate context-aware language exercises and track learner proficiency over time. The paper discusses the architecture, implementation and evaluation of LangLingual in detail. The results indicate strong usability, positive learning outcomes and encouraging learner engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23011v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sammriddh Gupta, Sonit Singh, Aditya Joshi, Mira Kim</dc:creator>
    </item>
    <item>
      <title>Tailoring Chatbots for Higher Education: Some Insights and Experiences</title>
      <link>https://arxiv.org/abs/2409.06717</link>
      <description>arXiv:2409.06717v2 Announce Type: replace 
Abstract: The general availability of general-purpose Large Language Models continues to impact on higher education, yet they may not always be useful for specialized tasks. When using these models, oftentimes the need for particular domain knowledge becomes quickly apparent, and the desire for customized bots arises. Customization holds the promise of leading to more accurate and contextually relevant responses, enhancing the educational experience. This report relates insights and experiences from one particular technical university in Switzerland, ETH Zurich, to describe what "customizing" Large Language Models means in practical terms for higher education institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06717v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gerd Kortemeyer</dc:creator>
    </item>
    <item>
      <title>Secure and Confidential Certificates of Online Fairness</title>
      <link>https://arxiv.org/abs/2410.02777</link>
      <description>arXiv:2410.02777v2 Announce Type: replace 
Abstract: The black-box service model enables ML service providers to serve clients while keeping their intellectual property and client data confidential. Confidentiality is critical for delivering ML services legally and responsibly, but makes it difficult for outside parties to verify important model properties such as fairness. Existing methods that assess model fairness confidentially lack either (i) reliability because they certify fairness with respect to a static set of data, and therefore fail to guarantee fairness in the presence of distribution shift or service provider malfeasance; and/or (ii) scalability due to the computational overhead of confidentiality-preserving cryptographic primitives. We address these problems by introducing online fairness certificates, which verify that a model is fair with respect to data received by the service provider online during deployment. We then present OATH, a deployably efficient and scalable zero-knowledge proof protocol for confidential online group fairness certification. OATH exploits statistical properties of group fairness via a cut-and-choose style protocol, enabling scalability improvements over baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02777v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>39th Conference on Neural Information Processing Systems (NeurIPS 2025)</arxiv:journal_reference>
      <dc:creator>Olive Franzese, Ali Shahin Shamsabadi, Carter Luck, Hamed Haddadi</dc:creator>
    </item>
    <item>
      <title>SafeCOMM: A Study on Safety Degradation in Fine-Tuned Telecom Large Language Models</title>
      <link>https://arxiv.org/abs/2506.00062</link>
      <description>arXiv:2506.00062v2 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) on telecom datasets is a common practice to adapt general-purpose models to the telecom domain. However, little attention has been paid to how this process may compromise model safety. Recent research has shown that even benign fine-tuning can degrade the safety alignment of LLMs, causing them to respond to harmful or unethical user queries. In this paper, we investigate this issue by fine-tuning LLMs on three representative telecom datasets and show that safety degrades even for light telecom domain adaptation. To this end, we introduce TeleHarm, the first telecom-specific red-teaming benchmark, which we use alongside established Direct-Harm and HexPhi datasets to systematically assess harmful behavior. We further extend our analysis to publicly available TeleLLMs that were continually pre-trained on large telecom corpora, revealing that safety alignment is severely lacking, primarily due to the omission of safety-focused instruction tuning. To address these issues, we evaluate three realignment defenses: SafeInstruct, SafeLoRA, SafeMERGE. We show that, across all settings, the proposed defenses can effectively restore safety without compromising telecom task performance, leading to Safe teleCOMMunication (SafeCOMM) models. Our work serves as both a diagnostic study and practical guide for safety realignment in telecom-tuned LLMs, underscoring the need for safety-aware instruction and fine-tuning in the telecom domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00062v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Syed Zawad, Fernando Koch, Walid Saad, Holger Boche</dc:creator>
    </item>
    <item>
      <title>Evidence Without Injustice: A New Counterfactual Test for Fair Algorithms</title>
      <link>https://arxiv.org/abs/2510.12822</link>
      <description>arXiv:2510.12822v2 Announce Type: replace 
Abstract: The growing philosophical literature on algorithmic fairness has examined statistical criteria such as equalized odds and calibration, causal and counterfactual approaches, and the role of structural and compounding injustices. Yet an important dimension has been overlooked: whether the evidential value of an algorithmic output itself depends on structural injustice. We contrast a predictive policing algorithm, which relies on historical crime data, with a camera-based system that records ongoing offenses, where both are designed to guide police deployment. In evaluating the moral acceptability of acting on a piece of evidence, we must ask not only whether the evidence is probative in the actual world, but also whether it would remain probative in nearby worlds without the relevant injustices. The predictive policing algorithm fails this test, but the camera-based system passes it. When evidence fails the test, it is morally problematic to use it punitively, more so than evidence that passes the test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12822v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michele Loi, Marcello Di Bello, Nicol\`o Cangiotti</dc:creator>
    </item>
    <item>
      <title>Censoring chemical data to mitigate dual use risk</title>
      <link>https://arxiv.org/abs/2304.10510</link>
      <description>arXiv:2304.10510v2 Announce Type: replace-cross 
Abstract: Machine learning models have dual-use potential, potentially serving both beneficial and malicious purposes. The development of open-source models in chemistry has specifically surfaced dual-use concerns around toxicological data and chemical warfare agents. We discuss a chain risk framework identifying three misuse pathways and corresponding mitigation strategies: inference-level, model-level, and data-level. At the data level, we introduce a model-agnostic noising method to increase prediction error in specific desired regions (sensitive regions). Our results show that selective noise induces variance and attenuation bias, whereas simply omitting sensitive data fails to prevent extrapolation. These findings hold for both molecular feature multilayer perceptrons and graph neural networks. Thus, noising molecular structures can enable open sharing of potential dual-use molecular data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10510v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>physics.chem-ph</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quintina L. Campbell, Jonathan Herington, Andrew D. White</dc:creator>
    </item>
    <item>
      <title>Refusal as Silence: Gendered Disparities in Vision-Language Model Responses</title>
      <link>https://arxiv.org/abs/2406.08222</link>
      <description>arXiv:2406.08222v3 Announce Type: replace-cross 
Abstract: Refusal behavior by Large Language Models is increasingly visible in content moderation, yet little is known about how refusals vary by the identity of the user making the request. This study investigates refusal as a sociotechnical outcome through a counterfactual persona design that varies gender identity--including male, female, non-binary, and transgender personas--while keeping the classification task and visual input constant. Focusing on a vision-language model (GPT-4V), we examine how identity-based language cues influence refusal in binary gender classification tasks. We find that transgender and non-binary personas experience significantly higher refusal rates, even in non-harmful contexts. Our findings also provide methodological implications for equity audits and content analysis using LLMs. Our findings underscore the importance of modeling identity-driven disparities and caution against uncritical use of AI systems for content coding. This study advances algorithmic fairness by reframing refusal as a communicative act that may unevenly regulate epistemic access and participation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08222v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sha Luo, Sang Jung Kim, Zening Duan, Kaiping Chen</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models Unlock Novel Scientific Research Ideas?</title>
      <link>https://arxiv.org/abs/2409.06185</link>
      <description>arXiv:2409.06185v2 Announce Type: replace-cross 
Abstract: The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives. This study examines the ability of Large Language Models (LLMs) to generate future research ideas from scientific papers. Unlike tasks such as summarization or translation, idea generation lacks a clearly defined reference set or structure, making manual evaluation the default standard. However, human evaluation in this setting is extremely challenging ie: it requires substantial domain expertise, contextual understanding of the paper, and awareness of the current research landscape. This makes it time-consuming, costly, and fundamentally non-scalable, particularly as new LLMs are being released at a rapid pace. Currently, there is no automated evaluation metric specifically designed for this task. To address this gap, we propose two automated evaluation metrics: Idea Alignment Score (IAScore) and Idea Distinctness Index. We further conducted human evaluation to assess the novelty, relevance, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. We make our datasets and codes publicly available</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06185v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal</dc:creator>
    </item>
    <item>
      <title>Integrated Design and Governance of Agentic AI Systems through Adaptive Information Modulation</title>
      <link>https://arxiv.org/abs/2409.10372</link>
      <description>arXiv:2409.10372v4 Announce Type: replace-cross 
Abstract: Modern engineered systems increasingly involve complex sociotechnical environments where multiple agents, including humans and the emerging paradigm of agentic AI powered by large language models, must navigate social dilemmas that pit individual interests against collective welfare. As engineered systems evolve toward multi-agent architectures with autonomous LLM-based agents, traditional governance approaches using static rules or fixed network structures fail to address the dynamic uncertainties inherent in real-world operations. This paper presents a novel framework that integrates adaptive governance mechanisms directly into the design of sociotechnical systems through a unique separation of agent interaction networks from information flow networks. We introduce a system comprising strategic LLM-based system agents that engage in repeated interactions and a reinforcement learning-based governing agent that dynamically modulates information transparency. Unlike conventional approaches that require direct structural interventions or payoff modifications, our framework preserves agent autonomy while promoting cooperation through adaptive information governance. The governing agent learns to strategically adjust information disclosure at each timestep, determining what contextual or historical information each system agent can access. Experimental results demonstrate that this RL-based governance significantly enhances cooperation compared to static information-sharing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10372v4</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiliang Chen, Sepehr Ilami, Nunzio Lore, Babak Heydari</dc:creator>
    </item>
    <item>
      <title>Stronger together? The homophily trap in networks</title>
      <link>https://arxiv.org/abs/2412.20158</link>
      <description>arXiv:2412.20158v2 Announce Type: replace-cross 
Abstract: While homophily -- the tendency to link with similar others -- may nurture a sense of belonging and shared values, it can also hinder diversity and widen inequalities. Here, we unravel this trade-off analytically, revealing homophily traps for minority groups: scenarios where increased homophilic interaction among minorities negatively affects their structural opportunities within a network. We demonstrate that homophily traps arise when minority size falls below 25% of a network, at which point homophily comes at the expense of lower structural visibility for the minority group. Our work reveals that social groups require a critical size to benefit from homophily without incurring structural costs, providing insights into core processes underlying the emergence of group inequality in networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20158v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcos Oliveira, Leonie Neuhauser, Fariba Karimi</dc:creator>
    </item>
    <item>
      <title>Validating LLM-as-a-Judge Systems under Rating Indeterminacy</title>
      <link>https://arxiv.org/abs/2503.05965</link>
      <description>arXiv:2503.05965v4 Announce Type: replace-cross 
Abstract: The LLM-as-a-judge paradigm, in which a judge LLM system replaces human raters in rating the outputs of other generative AI (GenAI) systems, plays a critical role in scaling and standardizing GenAI evaluations. To validate such judge systems, evaluators assess human--judge agreement by first collecting multiple human ratings for each item in a validation corpus, then aggregating the ratings into a single, per-item gold label rating. For many items, however, rating criteria may admit multiple valid interpretations, so a human or LLM rater may deem multiple ratings "reasonable" or "correct." We call this condition rating indeterminacy. Problematically, many rating tasks that contain rating indeterminacy rely on forced-choice elicitation, whereby raters are instructed to select only one rating for each item. In this paper, we introduce a framework for validating LLM-as-a-judge systems under rating indeterminacy. We draw theoretical connections between different measures of judge system performance under different human--judge agreement metrics, and different rating elicitation and aggregation schemes. We demonstrate that differences in how humans and LLMs resolve rating indeterminacy when responding to forced-choice rating instructions can heavily bias LLM-as-a-judge validation. Through extensive experiments involving 11 real-world rating tasks and 9 commercial LLMs, we show that standard validation approaches that rely upon forced-choice ratings select judge systems that are highly suboptimal, performing as much as 31% worse than judge systems selected by our approach that uses multi-label "response set" ratings to account for rating indeterminacy. We conclude with concrete recommendations for more principled approaches to LLM-as-a-judge validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05965v4</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Guerdan, Solon Barocas, Kenneth Holstein, Hanna Wallach, Zhiwei Steven Wu, Alexandra Chouldechova</dc:creator>
    </item>
    <item>
      <title>StereoDetect: Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings</title>
      <link>https://arxiv.org/abs/2504.03352</link>
      <description>arXiv:2504.03352v3 Announce Type: replace-cross 
Abstract: Stereotypes are known to have very harmful effects, making their detection critically important. However, current research predominantly focuses on detecting and evaluating stereotypical biases, thereby leaving the study of stereotypes in its early stages. Our study revealed that many works have failed to clearly distinguish between stereotypes and stereotypical biases, which has significantly slowed progress in advancing research in this area. Stereotype and Anti-stereotype detection is a problem that requires social knowledge; hence, it is one of the most difficult areas in Responsible AI. This work investigates this task, where we propose a five-tuple definition and provide precise terminologies disentangling stereotypes, anti-stereotypes, stereotypical bias, and general bias. We provide a conceptual framework grounded in social psychology for reliable detection. We identify key shortcomings in existing benchmarks for this task of stereotype and anti-stereotype detection. To address these gaps, we developed StereoDetect, a well curated, definition-aligned benchmark dataset designed for this task. We show that sub-10B language models and GPT-4o frequently misclassify anti-stereotypes and fail to recognize neutral overgeneralizations. We demonstrate StereoDetect's effectiveness through multiple qualitative and quantitative comparisons with existing benchmarks and models fine-tuned on them. The dataset and code is available at https://github.com/KaustubhShejole/StereoDetect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03352v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaustubh Shivshankar Shejole, Pushpak Bhattacharyya</dc:creator>
    </item>
    <item>
      <title>Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False Presuppositions</title>
      <link>https://arxiv.org/abs/2504.11373</link>
      <description>arXiv:2504.11373v2 Announce Type: replace-cross 
Abstract: Cancer patients are increasingly turning to large language models (LLMs) for medical information, making it critical to assess how well these models handle complex, personalized questions. However, current medical benchmarks focus on medical exams or consumer-searched questions and do not evaluate LLMs on real patient questions with patient details. In this paper, we first have three hematology-oncology physicians evaluate cancer-related questions drawn from real patients. While LLM responses are generally accurate, the models frequently fail to recognize or address false presuppositions in the questions, posing risks to safe medical decision-making. To study this limitation systematically, we introduce Cancer-Myth, an expert-verified adversarial dataset of 585 cancer-related questions with false presuppositions. On this benchmark, no frontier LLM -- including GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet -- corrects these false presuppositions more than $43\%$ of the time. To study mitigation strategies, we further construct a 150-question Cancer-Myth-NFP set, in which physicians confirm the absence of false presuppositions. We find typical mitigation strategies, such as adding precautionary prompts with GEPA optimization, can raise accuracy on Cancer-Myth to $80\%$, but at the cost of misidentifying presuppositions in $41\%$ of Cancer-Myth-NFP questions and causing a $10\%$ relative performance drop on other medical benchmarks. These findings highlight a critical gap in the reliability of LLMs, show that prompting alone is not a reliable remedy for false presuppositions, and underscore the need for more robust safeguards in medical AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11373v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wang Bill Zhu, Tianqi Chen, Xinyan Velocity Yu, Ching Ying Lin, Jade Law, Mazen Jizzini, Jorge J. Nieva, Ruishan Liu, Robin Jia</dc:creator>
    </item>
    <item>
      <title>Scaling Laws For Scalable Oversight</title>
      <link>https://arxiv.org/abs/2504.18530</link>
      <description>arXiv:2504.18530v3 Announce Type: replace-cross 
Abstract: Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, we propose a framework that quantifies the probability of successful oversight as a function of the capabilities of the overseer and the system being overseen. Specifically, our framework models oversight as a game between capability-mismatched players; the players have oversight-specific Elo scores that are a piecewise-linear function of their general intelligence, with two plateaus corresponding to task incompetence and task saturation. We validate our framework with a modified version of the game Nim and then apply it to four oversight games: Mafia, Debate, Backdoor Code and Wargames. For each game, we find scaling laws that approximate how domain performance depends on general AI system capability. We then build on our findings in a theoretical study of Nested Scalable Oversight (NSO), a process in which trusted models oversee untrusted stronger models, which then become the trusted models in the next step. We identify conditions under which NSO succeeds and derive numerically (and in some cases analytically) the optimal number of oversight levels to maximize the probability of oversight success. We also apply our theory to our four oversight games, where we find that NSO success rates at a general Elo gap of 400 are 13.5% for Mafia, 51.7% for Debate, 10.0% for Backdoor Code, and 9.4% for Wargames; these rates decline further when overseeing stronger systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18530v3</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>NeurIPS 2025 (Spotlight)</arxiv:journal_reference>
      <dc:creator>Joshua Engels, David D. Baek, Subhash Kantamneni, Max Tegmark</dc:creator>
    </item>
    <item>
      <title>Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy</title>
      <link>https://arxiv.org/abs/2507.06969</link>
      <description>arXiv:2507.06969v2 Announce Type: replace-cross 
Abstract: Differentially private (DP) mechanisms are difficult to interpret and calibrate because existing methods for mapping standard privacy parameters to concrete privacy risks -- re-identification, attribute inference, and data reconstruction -- are both overly pessimistic and inconsistent. In this work, we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that bounds on attack success can take the same unified form across re-identification, attribute inference, and data reconstruction risks. Our unified bounds are (1) consistent across a multitude of attack settings, and (2) tunable, enabling practitioners to evaluate risk with respect to arbitrary, including worst-case, levels of baseline risk. Empirically, our results are tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated DP. As a result, calibrating noise using our bounds can reduce the required noise by 20% at the same risk level, which yields, e.g., an accuracy increase from 52% to 70% in a text classification task. Overall, this unifying perspective provides a principled framework for interpreting and calibrating the degree of protection in DP against specific levels of re-identification, attribute inference, or data reconstruction risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06969v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bogdan Kulynych, Juan Felipe Gomez, Georgios Kaissis, Jamie Hayes, Borja Balle, Flavio du Pin Calmon, Jean Louis Raisaro</dc:creator>
    </item>
    <item>
      <title>What Is Your AI Agent Buying? Evaluation, Implications and Emerging Questions for Agentic E-Commerce</title>
      <link>https://arxiv.org/abs/2508.02630</link>
      <description>arXiv:2508.02630v2 Announce Type: replace-cross 
Abstract: Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, AI agents can parse webpages or interact through APIs to evaluate products, and transact. This raises a fundamental question: what do AI agents buy-and why? We develop ACES, a sandbox environment that pairs a platform-agnostic agent with a fully programmable mock marketplace to study this. We first explore aggregate choices, revealing that modal choices can differ across models, with AI agents sometimes concentrating on a few products, raising competition questions. We then analyze the drivers of choices through rationality checks and randomized experiments on product positions and listing attributes. Models show sizeable and heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal ``top'' rank. They penalize sponsored tags, reward endorsements, and sensitivities to price, ratings, and reviews are directionally as expected, but vary sharply across models. Finally, we find that a seller-side agent that makes minor tweaks to product descriptions can deliver substantial market-share gains by targeting AI buyer preferences. Our findings reveal how AI agents behave in e-commerce, and surface concrete seller strategy, platform design, and regulatory questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02630v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amine Allouah, Omar Besbes, Josu\'e D Figueroa, Yash Kanoria, Akshit Kumar</dc:creator>
    </item>
    <item>
      <title>The Role of AI in Facilitating Interdisciplinary Collaboration: Evidence from AlphaFold</title>
      <link>https://arxiv.org/abs/2508.13234</link>
      <description>arXiv:2508.13234v2 Announce Type: replace-cross 
Abstract: The acceleration of artificial intelligence (AI) in science is recognized and many scholars have begun to explore its role in interdisciplinary collaboration. However, the mechanisms and extent of this impact are still unclear. This study, using AlphaFold's impact on structural biologists, examines how AI technologies influence interdisciplinary collaborative patterns. By analyzing 1,247 AlphaFold-related papers and 7,700 authors from Scopus, we employ bibliometric analysis and causal inference to compare interdisciplinary collaboration between AlphaFold adopters and non-adopters. Contrary to the widespread belief that AI facilitates interdisciplinary collaboration, our findings show that AlphaFold increased structural biology-computer science collaborations by just 0.48%, with no measurable effect on other disciplines. Specifically, AI creates interdisciplinary collaboration demands with specific disciplines due to its technical characteristics, but this demand is weakened by technological democratization and other factors. These findings demonstrate that artificial intelligence (AI) alone has limited efficacy in bridging disciplinary divides or fostering meaningful interdisciplinary collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13234v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naixuan Zhao, Chunli Wei, Xinyan Zhang, Jiang Li</dc:creator>
    </item>
    <item>
      <title>SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors</title>
      <link>https://arxiv.org/abs/2510.17516</link>
      <description>arXiv:2510.17516v3 Announce Type: replace-cross 
Abstract: Large language model (LLM) simulations of human behavior have the potential to revolutionize the social and behavioral sciences, if and only if they faithfully reflect real human behaviors. Current evaluations are fragmented, based on bespoke tasks and metrics, creating a patchwork of incomparable results. To address this, we introduce SimBench, the first large-scale, standardized benchmark for a robust, reproducible science of LLM simulation. By unifying 20 diverse datasets covering tasks from moral decision-making to economic choice across a large global participant pool, SimBench provides the necessary foundation to ask fundamental questions about when, how, and why LLM simulations succeed or fail. We show that, while even the best LLMs today have limited simulation ability (score: 40.80/100), performance scales log-linearly with model size. Simulation performance is not improved by increased inference-time compute. We demonstrate an alignment-simulation trade-off: instruction-tuning improves performance on low-entropy (consensus) questions but degrades it on high-entropy (diverse) ones. Models particularly struggle when simulating specific demographic groups. Finally, we demonstrate that simulation ability correlates most strongly with deep, knowledge-intensive reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to accelerate the development of more faithful LLM simulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17516v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiancheng Hu, Joachim Baumann, Lorenzo Lupo, Nigel Collier, Dirk Hovy, Paul R\"ottger</dc:creator>
    </item>
  </channel>
</rss>
