<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Nov 2025 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Beat Frequency Induced Transitions in Synchronization Dynamics</title>
      <link>https://arxiv.org/abs/2511.15985</link>
      <description>arXiv:2511.15985v1 Announce Type: new 
Abstract: In neurosciences, the brain processes information via the firing patterns of connected neurons operating across a spectrum of frequencies. To better understand the effects of these frequencies in the neuron dynamics, we have simulated a neuronal network of Izhikevich neurons to examine the interaction between frequency allocation and intermittent phase synchronization dynamics. As the synchronized population of neurons passes through a bifurcation, an additional frequency mode emerges, enabling a match in the mean frequency while retaining distinct most probable frequencies among neurons. Subsequently, the network intermittently transits between two patterns, one partially synchronized and the other unsynchronized. Through our analysis, we demonstrate that the frequency changes on the network lead to characteristic transition times between synchronization states. Moreover, these transitions adhere to beat frequency statistics when the neurons' frequencies differ by multiples of a frequency gap. Finally, our results can improve the performance in predicting transitions on problems where the beat frequency strongly influences the dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15985v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.PS</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cnsns.2024.108243</arxiv:DOI>
      <arxiv:journal_reference>Communications in Nonlinear Science and Numerical Simulation (2024) 128, 108243</arxiv:journal_reference>
      <dc:creator>Gabriel Marghoti, Thiago L. Prado, Miguel A. F. Sanju\'an, Sergio R. Lopes</dc:creator>
    </item>
    <item>
      <title>From generative AI to the brain: five takeaways</title>
      <link>https://arxiv.org/abs/2511.16432</link>
      <description>arXiv:2511.16432v1 Announce Type: cross 
Abstract: The big strides seen in generative AI are not based on somewhat obscure algorithms, but due to clearly defined generative principles. The resulting concrete implementations have proven themselves in large numbers of applications. We suggest that it is imperative to thoroughly investigate which of these generative principles may be operative also in the brain, and hence relevant for cognitive neuroscience. In addition, ML research led to a range of interesting characterizations of neural information processing systems. We discuss five examples, the shortcomings of world modelling, the generation of thought processes, attention, neural scaling laws, and quantization, that illustrate how much neuroscience could potentially learn from ML research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16432v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudius Gros</dc:creator>
    </item>
    <item>
      <title>Mesoscale tissue properties and electric fields in brain stimulation - bridging the macroscopic and microscopic scales</title>
      <link>https://arxiv.org/abs/2511.16465</link>
      <description>arXiv:2511.16465v1 Announce Type: cross 
Abstract: Accurate simulations of electric fields (E-fields) in brain stimulation depend on tissue conductivity representations that link macroscopic assumptions with underlying microscopic tissue structure. Mesoscale conductivity variations can produce meaningful changes in E-fields and neural activation thresholds but remain largely absent from standard macroscopic models. Recent microscopic models have suggested substantial local E-field perturbations and could, in principle, inform mesoscale conductivity. However, the quantitative validity of microscopic models is limited by fixation-related tissue distortion and incomplete extracellular-space reconstruction. We outline approaches that bridge macro- and microscales to derive consistent mesoscale conductivity distributions, providing a foundation for accurate multiscale models of E-fields and neural activation in brain stimulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16465v1</guid>
      <category>physics.bio-ph</category>
      <category>physics.app-ph</category>
      <category>physics.med-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boshuo Wang, Torge Worbs, Minhaj A. Hussain, Aman S. Aberra, Axel Thielscher, Warren M. Grill, Angel V. Peterchev</dc:creator>
    </item>
    <item>
      <title>Clarifying the conceptual dimensions of representation in neuroscience</title>
      <link>https://arxiv.org/abs/2403.14046</link>
      <description>arXiv:2403.14046v3 Announce Type: replace 
Abstract: Despite the centrality of the notion of representation in neuroscience, the field lacks a unified framework for the concepts used to characterize representation, leading to disparate use of both terminology and measures associated with representation. To offer clarification, we propose a core set of conceptual dimensions that characterize representations in neuroscience. These dimensions describe relations between a neural response, features that may be represented, and downstream effects of the neural response. A neural response may be shown to be sensitive and specific to a feature, invariant to other features, and functional, which means that it is used downstream in the brain. We use information-theoretic measures to introduce these conceptual dimensions unambiguously and explain how data analysis methods such as correlational analyses, decoding and encoding models, representational similarity analysis, and tests of statistical dependence or adaptation relate to our framework. We consider several canonical examples, including the representation of orientation, numerosity, and spatial location, which illustrate how the evidence put forth in support or criticism of representational conclusions is systematized by our framework. By offering a unified conceptual framework we hope to aid the comparison and integration of results across studies and research groups and to help determine when evidence for a representational conclusion is strong.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14046v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephan Pohl, Edgar Y. Walker, David L. Barack, Jennifer Lee, Rachel N. Denison, Ned Block, Florent Meyniel, Wei Ji Ma</dc:creator>
    </item>
    <item>
      <title>Brain's Statistical Learning and its Embodied Mechanisms Towards Understanding Creativity and Individuality in Music</title>
      <link>https://arxiv.org/abs/2504.10875</link>
      <description>arXiv:2504.10875v2 Announce Type: replace 
Abstract: Music is a universal feature of human culture, linked to embodied cognitive functions that drive learning, action, and the emergence of creativity and individuality. Evidence highlights the critical role of statistical learning an implicit cognitive process of the brain in musical creativity and individuality. Despite its significance, the precise neural and computational mechanisms underpinning these dynamic and embodied cognitive processes re-main poorly understood. This paper discusses how individuality and creativity emerge within the framework of the brain's statistical learning, drawing on a series of neural and computational studies. This work offers perspectives on the mechanisms driving the heterogeneous nature of statistical learning abilities and embodied mechanisms and provides a framework to explain the paradoxical phenomenon where individuals with specific cognitive traits that limit certain perceptual abilities excel in creative domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10875v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuya Daikoku</dc:creator>
    </item>
    <item>
      <title>A Tripartite Framework for Understanding the Embodied Mind: A Comprehensive Review of Bodily Map Study</title>
      <link>https://arxiv.org/abs/2504.14865</link>
      <description>arXiv:2504.14865v2 Announce Type: replace 
Abstract: People often speak of being "moved beyond words" when witnessing awe-inspiring natural beauty, experiencing profound moral elevation, or encountering powerful works of art and music. Such moments evoke intensely embodied sensations - a swelling in the chest, a quickened heartbeat, or a sudden stillness that floods the entire body. These experiences are difficult to articulate, yet they are vividly real and spatially anchored in bodily experience. This review examines the emerging method of body mapping, which captures how individuals subjectively localize emotional sensations across the body. Drawing on over two dozen empirical studies, we propose a tripartite framework for understanding the origins of emotional bodily maps: (1) bottom-up physiological signals, (2) top-down behavioral engagement (including motor actions), and (3) conceptual and metaphorical constructions. Crucially, we argue that bodily maps are not mere reflections of physiological monitoring, but expressive interfaces between brain, body, and conceptualization. Because they rely on introspective sensation rather than verbal articulation, bodily maps offer a promising language-independent tool for assessing emotional experience - particularly in cross-cultural research, developmental studies, and clinical contexts where verbal reporting may be limited or unreliable. Ultimately, bodily maps reveal the hidden geography of emotion-those feelings that elude precise definition or verbalization, like being moved beyond words, yet are undeniably real and embodied - offering a compelling window into how emotions are formed, shared, and lived through the body.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14865v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuya Daikoku, Maiko Minatoya, Masaki Tanaka</dc:creator>
    </item>
    <item>
      <title>System Filter-Based Common Components Modeling for Cross-Subject EEG Decoding</title>
      <link>https://arxiv.org/abs/2507.05268</link>
      <description>arXiv:2507.05268v2 Announce Type: replace 
Abstract: Brain-computer interface (BCI) technology enables direct communication between the brain and external devices through electroencephalography (EEG) signals. However, existing decoding models often mix common and personalized components, leading to interference from individual variability that limits cross-subject decoding performance. To address this issue, this paper proposes a system filter that extends the concept of signal filtering to the system level. The method expands a system into its spectral representation, selectively removes unnecessary components, and reconstructs the system from the retained target components, thereby achieving explicit system-level decomposition and filtering. We further integrate the system filter into a Cross-Subject Decoding framework based on the System Filter (CSD-SF) and evaluate it on the four-class motor imagery (MI) task of the BCIC IV 2a dataset. Personalized models are transformed into relation spectrums, and statistical testing across subjects is used to remove personalized components. The remaining stable relations, representing common components across subjects, are then used to construct a common model for cross-subject decoding. Experimental results show an average improvement of 3.28% in decoding accuracy over baseline methods, demonstrating that the proposed system filter effectively isolates stable common components and enhances model robustness and generalizability in cross-subject EEG decoding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05268v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xiaoyuan Li, Xinru Xue, Bohan Zhang, Ye Sun, Shoushuo Xi, Gang Liu</dc:creator>
    </item>
    <item>
      <title>Maximum entropy models of neuronal populations at and off criticality</title>
      <link>https://arxiv.org/abs/2511.14872</link>
      <description>arXiv:2511.14872v2 Announce Type: replace 
Abstract: Empirical evidence of scaling behaviors in neuronal avalanches suggests that neuronal populations in the brain operate near criticality. Departure from scaling in neuronal avalanches has been used as a measure of distance to criticality and linked to brain disorders. A distinct line of evidence for brain criticality has come from thermodynamic signatures in maximum entropy (ME) models. Both of these approaches have been widely applied to the analysis of neuronal data. However, the relationship between deviations from avalanche criticality and thermodynamics of ME models of neuronal populations remains poorly understood. To address this question, we study spontaneous activity of organotypic rat cortex slice cultures in physiological and drug-induced hypo- or hyper-excitable conditions, which are classified as critical, subcritical and supercritical based on avalanche dynamics. We find that ME models inferred from critical cultures show signatures of criticality in thermodynamic quantities, e.g. specific heat. However, such signatures are also present, and equally strong, in models inferred from supercritical cultures -- despite their altered dynamics and poor functional performance. On the contrary, ME models inferred from subcritical cultures do not show thermodynamic hints of criticality. Importantly, we confirm these results using an interpretable neural network model that can be tuned to and away from avalanche criticality. Our findings indicate that maximum entropy models correctly distinguish subcritical from critical/supercritical systems. However, they may not be able to discriminate between avalanche criticality and supercriticality, although they may still capture a number of important features from neuronal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14872v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. S. A. N. Sim\~oes, F. Lombardi, D. Plenz, H. J. Herrmann, L. de Arcangelis</dc:creator>
    </item>
    <item>
      <title>Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2410.03972</link>
      <description>arXiv:2410.03972v3 Announce Type: replace-cross 
Abstract: Task-trained recurrent neural networks (RNNs) are widely used in neuroscience and machine learning to model dynamical computations. To gain mechanistic insight into how neural systems solve tasks, prior work often reverse-engineers individual trained networks. However, different RNNs trained on the same task and achieving similar performance can exhibit strikingly different internal solutions, a phenomenon known as solution degeneracy. Here, we develop a unified framework to systematically quantify and control solution degeneracy across three levels: behavior, neural dynamics, and weight space. We apply this framework to 3,400 RNNs trained on four neuroscience-relevant tasks: flip-flop memory, sine wave generation, delayed discrimination, and path integration, while systematically varying task complexity, learning regime, network size, and regularization. We find that higher task complexity and stronger feature learning reduce degeneracy in neural dynamics but increase it in weight space, with mixed effects on behavior. In contrast, larger networks and structural regularization reduce degeneracy at all three levels. These findings empirically validate the Contravariance Principle and provide practical guidance for researchers seeking to tune the variability of RNN solutions, either to uncover shared neural mechanisms or to model the individual variability observed in biological systems. This work provides a principled framework for quantifying and controlling solution degeneracy in task-trained RNNs, offering new tools for building more interpretable and biologically grounded models of neural computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03972v3</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NE</category>
      <category>math.IT</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ann Huang, Satpreet H. Singh, Flavio Martinelli, Kanaka Rajan</dc:creator>
    </item>
  </channel>
</rss>
