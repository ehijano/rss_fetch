<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 04:02:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Modeling pandemics</title>
      <link>https://arxiv.org/abs/2508.15125</link>
      <description>arXiv:2508.15125v1 Announce Type: new 
Abstract: We review several models for pandemics that plagued the USA and the world in the past decade. Methods of data fitting are reviewed and several types of microscopic and rate equation models are discussed and numerically solved. This paper was written in March of 2021 and newer data is now available; however some of the models and techniques we used to numerically study these models are still of interest. Several appendices discuss in detail these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15125v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John F. Dawson, Fred Cooper, Efstathios G. Charalampidis</dc:creator>
    </item>
    <item>
      <title>Post-processing of ensemble photovoltaic power forecasts with distributional and quantile regression methods</title>
      <link>https://arxiv.org/abs/2508.15508</link>
      <description>arXiv:2508.15508v1 Announce Type: new 
Abstract: Accurate and reliable forecasting of photovoltaic (PV) power generation is crucial for grid operations, electricity markets, and energy planning, as solar systems now contribute a significant share of the electricity supply in many countries. PV power forecasts are often generated by converting forecasts of relevant weather variables to power predictions via a model chain. The use of ensemble simulations from numerical weather prediction models results in probabilistic PV forecasts in the form of a forecast ensemble. However, weather forecasts often exhibit systematic errors that propagate through the model chain, leading to biased and/or uncalibrated PV power predictions. These deficiencies can be mitigated by statistical post-processing. Using PV production data and corresponding short-term PV power ensemble forecasts at seven utility-scale PV plants in Hungary, we systematically evaluate and compare seven state-of-the-art methods for post-processing PV power forecasts. These include both parametric and non-parametric techniques, as well as statistical and machine learning-based approaches. Our results show that compared to the raw PV power ensemble, any form of statistical post-processing significantly improves the predictive performance. Non-parametric methods outperform parametric models, with advanced nonlinear quantile regression models showing the best results. Furthermore, machine learning-based approaches surpass their traditional statistical counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15508v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin J\'anos Mayer, \'Agnes Baran, Sebastian Lerch, Nina Horat, Dazhi Yang, S\'andor Baran</dc:creator>
    </item>
    <item>
      <title>A Classification-Driven Likelihood Ratio Method for Familial DNA Testing</title>
      <link>https://arxiv.org/abs/2508.15579</link>
      <description>arXiv:2508.15579v1 Announce Type: new 
Abstract: Background: Familial DNA search relies on statistical methods to infer genetic relationships, often using the likelihood ratio (LR) as the test statistic. This approach involves calculating the probability of observed DNA profiles under various scenarios, traditionally assuming a uniform genetic background for all individuals. However, this assumption is unrealistic in practice due to population substructure, where allele frequencies differ among subpopulations. To address this, modified LR-based approaches have been proposed, including methods such as calculating likelihood ratios using average allele frequencies (LRLAF) or the maximum, minimum, and average likelihood ratios (LRMAX, LRMIN, and LRAVG). Despite these developments, further advancements are needed to improve the testing of genetic relationships in the presence of subpopulation structures. Results: We propose a novel LR-based statistic designed to account for population substructure by incorporating a classification step to address nuisance parameters. Specifically, two DNA profiles with unknown subpopulation origins are simultaneously classified into one group. Following this classification, the likelihood ratio (LRCLASS) is applied. Our analysis demonstrates that the proposed LRCLASS statistic, when paired with Naive Bayes classification, exhibits higher statistical power than existing methods for testing full-sibling relationships in the Thai population. Conclusion: The proposed LRCLASS statistic enhances the power of familial DNA searches by effectively addressing population substructure through a novel classification-based approach. This method provides a robust alternative to existing LR-based strategies and underscores the potential for integrating advanced classification techniques into genetic relationship testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15579v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akaraphon Jantaraphum, Chanagarn Laoiam, Budsaba Rerkamnuaychoke, Jittima Shotivaranon, Monchai Kooakachai</dc:creator>
    </item>
    <item>
      <title>Joint Classification of Haze and Dust Events Using Factorial Hidden Markov Model Framework</title>
      <link>https://arxiv.org/abs/2508.15661</link>
      <description>arXiv:2508.15661v1 Announce Type: new 
Abstract: Haze and dust pollution events have significant adverse impacts on human health and ecosystems. Their formation-impact interactions are complex, creating substantial modeling and computational challenges for joint classification. To address the state-space explosion faced by conventional Hidden Markov Models in multivariate dynamic settings, this study develops a classification framework based on the Factorial Hidden Markov Model. The framework assumes statistical independence across multiple latent chains and applies the Walsh-Hadamard transform to reduce computational and memory costs. A Gaussian copula decouples marginal distributions from dependence to capture nonlinear correlations among meteorological and pollution indicators. Algorithmically, mutual information weights the observational variables to increase the sensitivity of Viterbi decoding to salient features, and a single global weight hyperparameter balances emission and transition contributions in the decoding objective. In an empirical application, the model attains a Micro-F1 of 0.9459; for the low-frequency classes Dust prevalence below 1\% and Haze prevalence below 10\%, the F1-scores improve from 0.19 and 0.32 under a baseline FHMM to 0.75 and 0.68. The framework provides a scalable pathway for statistical modeling of complex air-pollution events and supplies quantitative evidence for decision-making in outdoor activity management and fine-grained environmental governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15661v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianhao Zhang, Yixin Zhang, Liang Guo, Xiaoqiang Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Methods for Surveillance of Cervical Dystonia Treatments</title>
      <link>https://arxiv.org/abs/2508.15762</link>
      <description>arXiv:2508.15762v1 Announce Type: new 
Abstract: Cervical dystonia, a debilitating neurological disorder marked by involuntary muscle contractions and chronic pain, presents significant treatment challenges despite advances in botulinum toxin therapy. While botulinum toxin type B has emerged as one of the leading treatments, comparative efficacy across doses and the influence of demographic factors for personalized medicine remain understudied. This study aimed to: (1) compare the efficacy of different botulinum toxin type B doses using Bayesian methods, (2) evaluate demographic and clinical factors affecting treatment response, and (3) establish a probabilistic framework for personalized cervical dystonia management. We analyzed data from a multicenter randomized controlled trial involving 109 patients assigned to placebo, 5,000 units, or 10,000 units of botulinum toxin type B groups. The primary outcome was the Toronto Western Spasmodic Torticollis Rating Scale measured over 16 weeks. Bayesian hierarchical modeling assessed treatment effects while accounting for patient heterogeneity. Lower botulinum toxin type B doses (5,000 units) showed greater overall Toronto Western Spasmodic Torticollis Rating Scale score reductions (treatment effect: -2.39, 95% Probability Interval: -4.10 to -0.70). Male patients demonstrated better responses (5.2% greater improvement) than female patients. Substantial between-patient variability and site-specific effects were observed, highlighting the need for personalized protocols. The study confirms botulinum toxin type B's dose-dependent efficacy while identifying key modifiable factors in treatment response. Bayesian methods provided nuanced insights into uncertainty and heterogeneity, paving the way for personalized medicine in cervical dystonia management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15762v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. Baidoo, E. Kubuafor, S. F. Osarfo, F. A. Agyei-Owusu, J. A. Frimpong, R. Amevor, A. Duah, F. Aboagye</dc:creator>
    </item>
    <item>
      <title>Can synthetic data reproduce real-world findings in epidemiology? A replication study using tree-based generative AI</title>
      <link>https://arxiv.org/abs/2508.14936</link>
      <description>arXiv:2508.14936v1 Announce Type: cross 
Abstract: Generative artificial intelligence for synthetic data generation holds substantial potential to address practical challenges in epidemiology. However, many current methods suffer from limited quality, high computational demands, and complexity for non-experts. Furthermore, common evaluation strategies for synthetic data often fail to directly reflect statistical utility. Against this background, a critical underexplored question is whether synthetic data can reliably reproduce key findings from epidemiological research. We propose the use of adversarial random forests (ARF) as an efficient and convenient method for synthesizing tabular epidemiological data. To evaluate its performance, we replicated statistical analyses from six epidemiological publications and compared original with synthetic results. These publications cover blood pressure, anthropometry, myocardial infarction, accelerometry, loneliness, and diabetes, based on data from the German National Cohort (NAKO Gesundheitsstudie), the Bremen STEMI Registry U45 Study, and the Guelph Family Health Study. Additionally, we assessed the impact of dimensionality and variable complexity on synthesis quality by limiting datasets to variables relevant for individual analyses, including necessary derivations. Across all replicated original studies, results from multiple synthetic data replications consistently aligned with original findings. Even for datasets with relatively low sample size-to-dimensionality ratios, the replication outcomes closely matched the original results across various descriptive and inferential analyses. Reducing dimensionality and pre-deriving variables further enhanced both quality and stability of the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14936v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Kapar, Kathrin G\"unther, Lori Ann Vallis, Klaus Berger, Nadine Binder, Hermann Brenner, Stefanie Castell, Beate Fischer, Volker Harth, Bernd Holleczek, Timm Intemann, Till Ittermann, Andr\'e Karch, Thomas Keil, Lilian Krist, Berit Lange, Michael F. Leitzmann, Katharina Nimptsch, Nadia Obi, Iris Pigeot, Tobias Pischon, Tamara Schikowski, B\"orge Schmidt, Carsten Oliver Schmidt, Anja M. Sedlmair, Justine Tanoey, Harm Wienbergen, Andreas Wienke, Claudia Wigmann, Marvin N. Wright</dc:creator>
    </item>
    <item>
      <title>LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa</title>
      <link>https://arxiv.org/abs/2508.15110</link>
      <description>arXiv:2508.15110v1 Announce Type: cross 
Abstract: In this work, we highlight the transformative potential of Artificial Intelligence (AI), particularly Large Language Models (LLMs) and agentic AI, in the insurance sector. We consider and emphasize the unique opportunities, challenges, and potential pathways in insurance amid rapid performance improvements, increased open-source access, decreasing deployment costs, and the complexity of LLM or agentic AI frameworks. To bring it closer to home, we identify critical gaps in the African insurance market and highlight key local efforts, players, and partnership opportunities. Finally, we call upon actuaries, insurers, regulators, and tech leaders to a collaborative effort aimed at creating inclusive, sustainable, and equitable AI strategies and solutions: by and for Africans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15110v1</guid>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Graham Hill, JingYuan Gong, Thulani Babeli, Moseli Mots'oehli, James Gachomo Wanjiku</dc:creator>
    </item>
    <item>
      <title>CSTEapp: An interactive R-Shiny application of the covariate-specific treatment effect curve for visualizing individualized treatment rule</title>
      <link>https://arxiv.org/abs/2508.15265</link>
      <description>arXiv:2508.15265v1 Announce Type: cross 
Abstract: In precision medicine, deriving the individualized treatment rule (ITR) is crucial for recommending the optimal treatment based on patients' baseline covariates. The covariate-specific treatment effect (CSTE) curve presents a graphical method to visualize an ITR within a causal inference framework. Recent advancements have enhanced the causal interpretation of the CSTE curves and provided methods for deriving simultaneous confidence bands for various study types. To facilitate the implementation of these methods and make ITR estimation more accessible, we developed CSTEapp, a web-based application built on the R Shiny framework. CSTEapp allows users to upload data and create CSTE curves through simple point and click operations, making it the first application for estimating the ITRs. CSTEapp simplifies the analytical process by providing interactive graphical user interfaces with dynamic results, enabling users to easily report optimal treatments for individual patients based on their covariates information. Currently, CSTEapp is applicable to studies with binary and time-to-event outcomes, and we continually expand its capabilities to accommodate other outcome types as new methods emerge. We demonstrate the utility of CSTEapp using real-world examples and simulation datasets. By making advanced statistical methods more accessible, CSTEapp empowers researchers and practitioners across various fields to advance precision medicine and improve patient outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15265v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zhou, Yuhao Deng, Yu-Shi Tian, Peng Wu, Wenjie Hu, Haoxiang Wang, Ewout Steyerberg, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Efficient sampling from a multivariate normal distribution subject to linear equality and inequality constraints</title>
      <link>https://arxiv.org/abs/2508.15292</link>
      <description>arXiv:2508.15292v1 Announce Type: cross 
Abstract: Sampling from multivariate normal distributions, subjected to a variety of restrictions, is a problem that is recurrent in statistics and computing. In the present work, we demonstrate a general framework to efficiently sample a multivariate normal distribution subject to any set of linear inequality constraints and/or linear equality constraints simultaneously. In the approach we detail, sampling a multivariate random variable from the domain formed by the intersection of linear constraints proceeds via a combination of elliptical slice sampling to address the inequality constraints, and linear mapping to address the equality constraints. We also detail a linear programming method for finding an initial sample on the linearly constrained domain; such a method is critical for sampling problems where the domain has small probability.
  We demonstrate the validity of our methods on an arbitrarily chosen four-dimensional multivariate normal distribution subject to five inequality constraints and/or two equality constraints. Our approach compares favourably to direct sampling and/or accept-reject sampling methods; the latter methods vary widely in their efficiency, whereas the methods in the present work are rejection-free. Where practical we compare predictions of probability density functions between our sampling methods and analytical computation. For all simulations we demonstrate that our methods yield accurate computation of the mean and covariance of the multivariate normal distributions restricted by the imposed linear constraints. MATLAB codes to implement our methods are readily available at https://dx.doi.org/10.6084/m9.figshare.29956304 .</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15292v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew P. Adams, Gloria M. Monsalve-Bravo, Lucy G. Dowdell, Scott A. Sisson, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Swap Regression Methodology for Predicting Relationship with Historical Bivariate Data</title>
      <link>https://arxiv.org/abs/2508.15479</link>
      <description>arXiv:2508.15479v1 Announce Type: cross 
Abstract: This study revisits regression for samples with alternating predictors (SWAP) proposed in Chow et al.[2015] with the purpose of finding the best fit model when the role of the response and the explanatory variables was established. In the current work, we explore the directional relationship between the two variables at a given point of time, by a novel approach which draws direct inspiration from the concept of SWAP regression. Our method, based on the Gaussian Mixture Model (GMM) and the beta distribution, while estimating the probability of a latent variable, predicts the suitable model, i.e., earmarks if a variable can take the role of an explanatory or response, at any point of time. To make this switch-over role between variables, a valid consideration, we have established the existence of a bi-directional (Granger) causality between the two variables. A detailed real data analysis of the methodology is carried out using the historical quarterly data on probably the two most intertwined macroeconomic indicators explaining the health of an economy, viz., the Gross Domestic Product (GDP) and Public Debt, thereby making the application, in real data, more challenging. In particular, we use data of the US economy during the sample period 1966-2023.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15479v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Viral Chitlangia, Mosuk Chow, Sharmishtha Mitra</dc:creator>
    </item>
    <item>
      <title>Subgroup comparisons within and across studies in meta-analysis</title>
      <link>https://arxiv.org/abs/2508.15531</link>
      <description>arXiv:2508.15531v1 Announce Type: cross 
Abstract: Subgroup-specific meta-analysis synthesizes treatment effects for patient subgroups across randomized trials. Methods include joint or separate modeling of subgroup effects and treatment-by-subgroup interactions, but inconsistencies arise when subgroup prevalence differs between studies (e.g., proportion of non-smokers). A key distinction is between study-generated evidence within trials and synthesis-generated evidence obtained by contrasting results across trials. This distinction matters for identifying which subgroups benefit or are harmed most. Failing to separate these evidence types can bias estimates and obscure true subgroup-specific effects, leading to misleading conclusions about relative efficacy. Standard approaches often suffer from such inconsistencies, motivating alternatives. We investigate standard and novel estimators of subgroup and interaction effects in random-effects meta-analysis and study their properties. We show that using the same weights across different analyses (SWADA) resolves inconsistencies from unbalanced subgroup distributions and improves subgroup and interaction estimates. Analytical and simulation studies demonstrate that SWADA reduces bias and improves coverage, especially under pronounced imbalance. To illustrate, we revisit recent meta-analyses of randomized trials of COVID-19 therapies. Beyond COVID-19, the findings outline a general strategy for correcting compositional bias in evidence synthesis, with implications for decision-making and statistical modeling. We recommend the Interaction RE-weights SWADA as a practical default when aggregation bias is plausible: it ensures collapsibility, maintains nominal coverage with modest width penalty, and yields BLUE properties for the interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15531v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renato Panaro, Christian R\"over, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Beyond Traditional Surveillance: Harnessing Expert Knowledge for Public Health Forecasting</title>
      <link>https://arxiv.org/abs/2508.15623</link>
      <description>arXiv:2508.15623v1 Announce Type: cross 
Abstract: Downsizing the US public health workforce throughout 2025 amplifies potential risks during public health crises. Expert judgment from public health officials represents a vital information source, distinct from traditional surveillance infrastructure, that should be valued -- not discarded. Understanding how expert knowledge functions under constraints is essential for understanding the potential impact of reduced capacity. To explore expert forecasting capabilities, 114 public health officials at the 2024 CSTE workshop generated 103 predictions plus 102 rationales of peak hospitalizations and 114 predictions of influenza H3 versus H1 dominance in Pennsylvania for the 2024/25 season. We compared expert predictions to computational models and used rationales to analyze reasoning patterns using Latent Dirichlet Allocation. Experts better predicted H3 dominance and assigned lower probability to implausible scenarios than models. Expert rationales drew on historical patterns, pathogen interactions, vaccine data, and cumulative experience. Expert public health knowledge constitutes a critical data source that should be valued equally with traditional datasets. We recommend developing a national toolkit to systematically collect and analyze expert predictions and rationales, treating human judgment as quantifiable data alongside surveillance systems to enhance crisis response capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15623v1</guid>
      <category>q-bio.PE</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Garrik Hoyt, Eleanor Bergren, Gabrielle String, Thomas McAndrew</dc:creator>
    </item>
    <item>
      <title>Classification errors distort findings in automated speech processing: examples and solutions from child-development research</title>
      <link>https://arxiv.org/abs/2508.15637</link>
      <description>arXiv:2508.15637v1 Announce Type: cross 
Abstract: With the advent of wearable recorders, scientists are increasingly turning to automated methods of analysis of audio and video data in order to measure children's experience, behavior, and outcomes, with a sizable literature employing long-form audio-recordings to study language acquisition. While numerous articles report on the accuracy and reliability of the most popular automated classifiers, less has been written on the downstream effects of classification errors on measurements and statistical inferences (e.g., the estimate of correlations and effect sizes in regressions). This paper proposes a Bayesian approach to study the effects of algorithmic errors on key scientific questions, including the effect of siblings on children's language experience and the association between children's production and their input. In both the most commonly used \gls{lena}, and an open-source alternative (the Voice Type Classifier from the ACLEW system), we find that classification errors can significantly distort estimates. For instance, automated annotations underestimated the negative effect of siblings on adult input by 20--80\%, potentially placing it below statistical significance thresholds. We further show that a Bayesian calibration approach for recovering unbiased estimates of effect sizes can be effective and insightful, but does not provide a fool-proof solution. Both the issue reported and our solution may apply to any classifier involving event detection and classification with non-zero error rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15637v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Gautheron, Evan Kidd, Anton Malko, Marvin Lavechin, Alejandrina Cristia</dc:creator>
    </item>
    <item>
      <title>Fast approximate Bayesian inference of HIV indicators using PCA adaptive Gauss-Hermite quadrature</title>
      <link>https://arxiv.org/abs/2508.15665</link>
      <description>arXiv:2508.15665v1 Announce Type: cross 
Abstract: Naomi is a spatial evidence synthesis model used to produce district-level HIV epidemic indicators in sub-Saharan Africa. Multiple outcomes of policy interest, including HIV prevalence, HIV incidence, and antiretroviral therapy treatment coverage are jointly modelled using both household survey data and routinely reported health system data. The model is provided as a tool for countries to input their data to and generate estimates with during a yearly process supported by UNAIDS. Previously, inference has been conducted using empirical Bayes and a Gaussian approximation, implemented via the TMB R package. We propose a new inference method based on an extension of adaptive Gauss-Hermite quadrature to deal with more than 20 hyperparameters. Using data from Malawi, our method improves the accuracy of inferences for model parameters, while being substantially faster to run than Hamiltonian Monte Carlo with the No-U-Turn sampler. Our implementation leverages the existing TMB C++ template for the model's log-posterior, and is compatible with any model with such a template.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15665v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Howes, Alex Stringer, Seth R. Flaxman, Jeffrey W. Imai-Eaton</dc:creator>
    </item>
    <item>
      <title>Tree-like Pairwise Interaction Networks</title>
      <link>https://arxiv.org/abs/2508.15678</link>
      <description>arXiv:2508.15678v1 Announce Type: cross 
Abstract: Modeling feature interactions in tabular data remains a key challenge in predictive modeling, for example, as used for insurance pricing. This paper proposes the Tree-like Pairwise Interaction Network (PIN), a novel neural network architecture that explicitly captures pairwise feature interactions through a shared feed-forward neural network architecture that mimics the structure of decision trees. PIN enables intrinsic interpretability by design, allowing for direct inspection of interaction effects. Moreover, it allows for efficient SHapley's Additive exPlanation (SHAP) computations because it only involves pairwise interactions. We highlight connections between PIN and established models such as GA2Ms, gradient boosting machines, and graph neural networks. Empirical results on the popular French motor insurance dataset show that PIN outperforms both traditional and modern neural networks benchmarks in predictive accuracy, while also providing insight into how features interact with each another and how they contribute to the predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15678v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Richman, Salvatore Scognamiglio, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Numerical models outperform AI weather forecasts of record-breaking extremes</title>
      <link>https://arxiv.org/abs/2508.15724</link>
      <description>arXiv:2508.15724v1 Announce Type: cross 
Abstract: Artificial intelligence (AI)-based models are revolutionizing weather forecasting and have surpassed leading numerical weather prediction systems on various benchmark tasks. However, their ability to extrapolate and reliably forecast unprecedented extreme events remains unclear. Here, we show that for record-breaking weather extremes, the numerical model High RESolution forecast (HRES) from the European Centre for Medium-Range Weather Forecasts still consistently outperforms state-of-the-art AI models GraphCast, GraphCast operational, Pangu-Weather, Pangu-Weather operational, and Fuxi. We demonstrate that forecast errors in AI models are consistently larger for record-breaking heat, cold, and wind than in HRES across nearly all lead times. We further find that the examined AI models tend to underestimate both the frequency and intensity of record-breaking events, and they underpredict hot records and overestimate cold records with growing errors for larger record exceedance. Our findings underscore the current limitations of AI weather models in extrapolating beyond their training domain and in forecasting the potentially most impactful record-breaking weather events that are particularly frequent in a rapidly warming climate. Further rigorous verification and model development is needed before these models can be solely relied upon for high-stakes applications such as early warning systems and disaster management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15724v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhongwei Zhang, Erich Fischer, Jakob Zscheischler, Sebastian Engelke</dc:creator>
    </item>
    <item>
      <title>Dynamic clustering for heterophilic stochastic block models with time-varying node memberships</title>
      <link>https://arxiv.org/abs/2403.05654</link>
      <description>arXiv:2403.05654v2 Announce Type: replace-cross 
Abstract: We consider a time-ordered sequence of networks stemming from stochastic block models where nodes gradually change their memberships over time, and no network at any single time point contains sufficient signal strength to recover its community structure. To estimate the time-varying community structure, we develop KD-SoS (kernel debiased sum-of-squares), a method that performs spectral clustering after a debiased sum-of-squared aggregation of adjacency matrices. Our theory demonstrates, via a novel bias-variance decomposition, that KD-SoS achieves consistent community detection in each network, even when heterophilic networks do not require smoothness in the time-varying dynamics of between-community connectivities. We also prove the identifiability of aligning community structures across time based on how rapidly nodes change communities, and develop a data-adaptive bandwidth tuning procedure for KD-SoS. We demonstrate the utility and advantages of KD-SoS through simulations and a novel analysis of the time-varying dynamics in gene coordination in the human developing brain system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05654v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Z Lin, Jing Lei</dc:creator>
    </item>
    <item>
      <title>Copas-Heckman-type sensitivity analysis for publication bias in rare-event meta-analysis under generalized linear mixed models</title>
      <link>https://arxiv.org/abs/2405.03603</link>
      <description>arXiv:2405.03603v2 Announce Type: replace-cross 
Abstract: In systematic reviews and meta-analyses, publication bias (PB) is one of the serious concerns and mainly induced by selective publication of academic literatures. Although many methods have been proposed to deal with PB, almost all the methods are based on the normal-normal (NN) random-effects model assuming that data are normally distributed in both the within-study and the between-study levels. For rare-event meta-analysis where data contain rare occurrences of events, the standard NN random-effects model may perform poorly. Instead, some generalized linear mixed models (GLMMs) which employ the exact distribution for the number of events in within-study level provide alternatives and have been widely used in practice. However, limited methods can be applied to deal with PB in the GLMMs. To address this limitation, we propose a framework of sensitivity analysis for evaluating the impact of PB in various GLMMs. The proposed framework is developed based on the famous Copas-Heckman-type sensitivity analysis methods and can be easily implemented with the standard software with small computational cost. In this paper, we conduct simulation studies to assess the performance of proposed methods in adjusting PB and compare the results with related existing methods. Several real-world examples are also analyzed to show the broad applicability of our proposal in evaluating the potential impact of PB in meta-analysis of odds ratios and proportions with rare-event outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03603v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zhou, Taojun Hu, Yuji Sakamoto, Ao Huang, Xiao-Hua Zhou, Satoshi Hattori</dc:creator>
    </item>
    <item>
      <title>Graph topology estimation of power grids using pairwise mutual information of time series data</title>
      <link>https://arxiv.org/abs/2505.11517</link>
      <description>arXiv:2505.11517v2 Announce Type: replace-cross 
Abstract: The topology of a power grid is estimated using an information theoretic approach. By modeling the grid as a graph and using voltage magnitude data of individual nodes in the grid, the mutual information between pairs of nodes is computed using different approximation methods. Using the well-known Chow-Liu algorithm, a maximum spanning tree based on mutual information is computed to estimate the power grid topology. This manuscript explores the application of this method to different datasets and explores the domain of applicability. The data quality, precision, time windows, frequency and the method for calculating the mutual information are varied to see the effect on the successful reconstruction of the graph and it's leaf nodes. Success shown for IEEE networks generated with MATPOWER and data generated using GridLAB-D. The algorithm is then cross-validated on IEEE networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11517v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.CE</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel T. Speckhard</dc:creator>
    </item>
    <item>
      <title>The ECME Algorithm Using Factor Analysis for DOA Estimation in Nonuniform Noise</title>
      <link>https://arxiv.org/abs/2508.02223</link>
      <description>arXiv:2508.02223v2 Announce Type: replace-cross 
Abstract: Maximum likelihood factor analysis has been applied to direction of arrival (DOA) estimation in unknown nonuniform noise and a variety of iterative approaches have been developed. In particular, the Factor Analysis for Anisotropic Noise (FAAN) method proposed by Stoica and Babu has excellent convergence properties. In this article, the Expectation/Conditional Maximization Either (ECME) algorithm, an extension of the expectation-maximization algorithm, is designed, which has almost the same computational complexity at each iteration as the FAAN method. However, numerical results show that the ECME algorithm yields faster stable convergence and is computationally more efficient. Importantly, the signal subspace estimated by the ECME algorithm can be employed for the subspace based DOA estimation in unknown nonuniform noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02223v2</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyan Gong</dc:creator>
    </item>
  </channel>
</rss>
