<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Jul 2025 01:23:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An empirical comparison of some outlier detection methods with longitudinal data</title>
      <link>https://arxiv.org/abs/2507.21203</link>
      <description>arXiv:2507.21203v1 Announce Type: new 
Abstract: This note investigates the problem of detecting outliers in longitudinal data. It compares well-known methods used in official statistics with proposals from the fields of data mining and machine learning that are based on the distance between observations or binary partitioning trees. This is achieved by applying the methods to panel survey data related to different types of statistical units. Traditional methods are quite simple, enabling the direct identification of potential outliers, but they require specific assumptions. In contrast, recent methods provide only a score whose magnitude is directly related to the likelihood of an outlier being present. All the methods require the user to set a number of tuning parameters. However, the most recent methods are more flexible and sometimes more effective than traditional methods. In addition, these methods can be applied to multidimensional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21203v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marcello D'Orazio</dc:creator>
    </item>
    <item>
      <title>Online Prediction For Streaming Observational Data</title>
      <link>https://arxiv.org/abs/2507.21308</link>
      <description>arXiv:2507.21308v1 Announce Type: new 
Abstract: The automated collection of streaming observational data has become standard and defies most traditional analytic techniques. It is not just that models are hard to identify, there may not be any model that can be safely and usefully assumed. Indeed, frequently it is only predictions that can be made and assessed. Problems for this kind of data are often called {\cal{M}}-Open and have motivated new approaches and philosophies.
  This paper will review some of the most successful recent predictive methods for the {\cal{M}}-Open problem class. Techniques include predictors using expert advice such as the Shtarkov solution, Bayesian nonparametrics such as Gaussian process priors, hash function based predictors such as the {\sf Count-Min} sketch, and conformal prediction. Throughout, the properties of the predictors are presented and compared from a principled standpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21308v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bertrand Clarke, Aleena Chanda</dc:creator>
    </item>
    <item>
      <title>Detection of a Sparse Change in High-Dimensional Time Series</title>
      <link>https://arxiv.org/abs/2507.21442</link>
      <description>arXiv:2507.21442v1 Announce Type: new 
Abstract: Consider the detection of a sparse change in high-dimensional time-series. We introduce Sparsity Likelihood-based (SL-based) score and the change-points detection procedure in multivariate normal model with general covariance structure. SL-based algorithm is proved to achieve that supremum of error probabilities converges to 0. We run the simulation studies for SL-based algorithm and also illustrate its applications to a S&amp;P500 dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21442v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jingyan Huang</dc:creator>
    </item>
    <item>
      <title>Regression Analysis of Reciprocity in Directed Networks</title>
      <link>https://arxiv.org/abs/2507.21469</link>
      <description>arXiv:2507.21469v1 Announce Type: new 
Abstract: Reciprocity--the tendency of individuals to form mutual ties--is a fundamental structural feature of many directed networks. Despite its ubiquity, reciprocity remains insufficiently integrated into statistical network models, particularly in relation to covariate information. In this paper, we introduce the $R^{2}$-Model, a novel and flexible framework that explicitly models reciprocity while incorporating covariate effects. Built upon a generalized $p_1$ model, our framework accommodates both network sparsity and node heterogeneity, offering the most comprehensive parametrization of reciprocity to date--capturing not only its baseline level but also how it systematically varies with observed covariates. To address the challenges posed by high dimensionality and nuisance parameters, we develop a conditional likelihood estimator that isolates and consistently estimates the reciprocity effects. We establish its theoretical guarantees, including consistency, asymptotic normality, and minimax optimality under broad sparsity regimes. Extensive simulations and real-world applications demonstrate the $R^{2}$-Model's flexibility, interpretability, and strong finite-sample performance, highlighting its practical utility for uncovering covariate-driven patterns of reciprocity in directed networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21469v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rui Feng, Chenlei Leng</dc:creator>
    </item>
    <item>
      <title>Evaluating the performance of Bayesian cumulative logistic models in randomised controlled trials: a simulation study</title>
      <link>https://arxiv.org/abs/2507.21473</link>
      <description>arXiv:2507.21473v1 Announce Type: new 
Abstract: Background: The proportional odds (PO) model is the most common analytic method for ordinal outcomes in randomised controlled trials. While parameter estimates obtained under departures from PO can be interpreted as an average odds ratio, they can obscure differing treatment effects across the distribution of the ordinal categories. Extensions to the PO model exist and this work evaluates their performance under deviations to the PO assumption.
  Methods: We evaluated the bias, coverage and mean square error of four modeling approaches for ordinal outcomes via Monte Carlo simulation. Specifically, independent logistic regression models, the PO model, and constrained and unconstrained partial proportional odds (PPO) models were fit to simulated ordinal outcome data. The simulated data were designed to represent a hypothetical two-arm randomised trial under a range of scenarios. Additionally, we report on a case study; an Australasian COVID-19 Trial that adopted multiple secondary ordinal endpoints.
  Results: The PO model performed best when the data are generated under PO, as expected, but can result in bias and poor coverage in the presence of non-PO, particularly with increasing effect size and number of categories. The odds ratios (ORs) estimated using the unconstrained PPO and separate logistic regression models in the presence of non-PO had negligible bias and good coverage across most scenarios. The unconstrained PPO model under-performed when there was sparse data within some categories.
  Conclusions: While the PO model is effective when PO holds, the unconstrained and constrained PPO and logistic regression models provide unbiased and efficient estimates under non-PO conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21473v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris J. Selman, Katherine J. Lee, Steven Y. C. Tong, Mark Jones, Robert K. Mahar</dc:creator>
    </item>
    <item>
      <title>Evaluating the effect of different non-informative prior specifications on the Bayesian proportional odds model in randomised controlled trials: a simulation study</title>
      <link>https://arxiv.org/abs/2507.21491</link>
      <description>arXiv:2507.21491v1 Announce Type: new 
Abstract: Background: Ordinal outcomes combine multiple distinct ordered patient states into a single endpoint and are commonly analysed using proportional odds (PO) models in clinical trials. When using a Bayesian approach, it is not obvious what the influence of a 'non-informative' prior is in the analysis of a fixed design or on early stopping decisions in adaptive designs.
  Methods: This study compares different non-informative prior specifications for the Bayesian PO model in the context of both a two-arm trial with a fixed design and an adaptive design with an early stopping rule. We conducted an extensive simulation study, varying the effect size, sample size, number of categories and distribution of the control arm probabilities.
  Results: Our findings indicate that the choice of prior specification can introduce bias in the estimation of the treatment effect, particularly when control arm probabilities are right-skewed. The R-square prior specification resulted in the smallest bias and increased the likelihood of appropriately stopping early when there was a treatment effect. However, this specification exhibited larger biases when control arm probabilities were U-shaped when there was an early stopping rule. Dirichlet priors with concentration parameters close to zero resulted in the smallest bias when probabilities were right-skewed in the control arm, but were more likely to inappropriately stop early for superiority when there was no treatment effect and an early stopping rule.
  Conclusions: The specification of non-informative priors in Bayesian adaptive trials with ordinal outcomes has implications for treatment effect estimation and early stopping decisions. We recommend the careful selection of priors that consider the possible distribution of control arm probabilities and that sensitivity analyses to the prior be conducted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21491v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris J. Selman, Katherine J. Lee, Michael Dymock, Ian C. Marschner, Steven Y. C. Tong, Mark Jones, Robert K. Mahar</dc:creator>
    </item>
    <item>
      <title>Signal Detection under Composite Hypotheses with Identical Distributions for Signals and for Noises</title>
      <link>https://arxiv.org/abs/2507.21692</link>
      <description>arXiv:2507.21692v1 Announce Type: new 
Abstract: In this paper, we consider the problem of detecting signals in multiple, sequentially observed data streams. For each stream, the exact distribution is unknown, but characterized by a parameter that takes values in either of two disjoint composite spaces depending on whether it is a signal or noise. Furthermore, we consider a practical yet underexplored setting where all signals share the same parameter, as do all noises. Compared to the unconstrained case where the parameters in all streams are allowed to vary, this assumption facilitates faster decisionmaking thanks to the smaller parameter space. However, it introduces additional challenges in the analysis of the problem and designing of testing procedures since the local parameters are now coupled. In this paper, we establish a universal lower bound on the minimum expected sample size that characterizes the inherent difficulty of the problem. Moreover, we propose a novel testing procedure which not only controls the familywise error probabilities below arbitrary, user-specified levels, but also achieves the minimum expected sample size under every possible distribution asymptotically, as the levels go to zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21692v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiming Xing, Anamitra Chaudhuri, Yifan Chen</dc:creator>
    </item>
    <item>
      <title>Multivariate Spatio-temporal Modelling for Completing Cancer Registries and Forecasting Incidence</title>
      <link>https://arxiv.org/abs/2507.21714</link>
      <description>arXiv:2507.21714v1 Announce Type: new 
Abstract: Cancer data, particularly cancer incidence and mortality, are fundamental to understand the cancer burden, to set targets for cancer control and to evaluate the evolution of the implementation of a cancer control policy. However, the complexity of data collection, classification, validation and processing result in cancer incidence figures often lagging two to three years behind the calendar year. In response, national or regional population-based cancer registries (PBCRs) are increasingly interested in methods for forecasting cancer incidence. However, in many countries there is an additional difficulty in projecting cancer incidence as regional registries are usually not established in the same year and therefore cancer incidence data series between different regions of a country are not harmonised over time. This study addresses the challenge of forecasting cancer incidence with incomplete data at both regional and national levels. To achieve our objective, we propose the use of multivariate spatio-temporal shared component models that jointly model mortality data and available cancer incidence data. The performance of these multivariate models are analyzed using lung cancer incidence data, together with the number of deaths reported in England in the period 2001-2019. Different model predictive measures have been calculated to select the best model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21714v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garazi Retegui, Jaione Etxeberria, Mar\'ia Dolores Ugarte</dc:creator>
    </item>
    <item>
      <title>Properties and approximations of a Bessel distribution for data science applications</title>
      <link>https://arxiv.org/abs/2507.21812</link>
      <description>arXiv:2507.21812v1 Announce Type: new 
Abstract: This paper presents properties and approximations of a random variable based on the zero-order modified Bessel function that results from the compounding of a zero-mean Gaussian with a $\chi^2_1$-distributed variance. This family of distributions is a special case of the McKay family of Bessel distributions and of a family of generalized Laplace distributions. It is found that the Bessel distribution can be approximated with a null-location Laplace distribution, which corresponds to the compounding of a zero-mean Gaussian with a $\chi^2_2$-distributed variance. Other useful properties and representations of the Bessel distribution are discussed, including a closed form for the cumulative distribution function that makes use of the modified Struve functions. Another approximation of the Bessel distribution that is based on an empirical power-series approximation is also presented. The approximations are tested with the application to the typical problem of statistical hypothesis testing. It is found that a Laplace distribution of suitable scale parameter can approximate quantiles of the Bessel distribution with better than 10% accuracy, with the computational advantage associated with the use of simple elementary functions instead of special functions. It is expected that the approximations proposed in this paper be useful for a variety of data science applications where analytic simplicity and computational efficiency are of paramount importance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21812v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.IM</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Massimiliano Bonamente</dc:creator>
    </item>
    <item>
      <title>Marginal and conditional summary measures: transportability and compatibility across studies</title>
      <link>https://arxiv.org/abs/2507.21925</link>
      <description>arXiv:2507.21925v1 Announce Type: new 
Abstract: Marginal and conditional summary measures do not generally coincide, have different interpretations and correspond to different decision questions. While these aspects have primarily been recognized for non-collapsible summary measures, they are also problematic for some collapsible measures in the presence of effect modification. We clarify the interpretation and properties of different marginal and conditional summary measures, considering different types of outcomes and hypothetical outcome-generating mechanisms. We describe implications of the choice of summary measure for transportability, highlighting that covariates not conventionally described as effect modifiers can modify population-level treatment effects. Finally, we illustrate existing summary measure incompatibility issues in the context of evidence synthesis, using the case of covariate adjustment methods for indirect treatment comparisons. Because marginal and conditional summary measures do not generally coincide, their na\"ive pooling in evidence synthesis can produce bias. Almost invariably, care is needed to ensure that evidence synthesis methods are combining compatible summary measures, and this may be easier to ensure with full access to individual patient data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21925v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Remiro-Az\'ocar, David M. Phillippo, Nicky J. Welton, Sofia Dias, A. E. Ades, Anna Heath, Gianluca Baio</dc:creator>
    </item>
    <item>
      <title>Preconditioned Discrete-HAMS: A Second-order Irreversible Discrete Sampler</title>
      <link>https://arxiv.org/abs/2507.21982</link>
      <description>arXiv:2507.21982v1 Announce Type: new 
Abstract: Gradient-based Markov Chain Monte Carlo methods have recently received much attention for sampling discrete distributions, with notable examples such as Norm Constrained Gradient (NCG), Auxiliary Variable Gradient (AVG), and Discrete Hamiltonian Assisted Metropolis Sampling (DHAMS). In this work, we propose the Preconditioned Discrete-HAMS (PDHAMS) algorithm, which extends DHAMS by incorporating a second-order, quadratic approximation of the potential function, and uses Gaussian integral trick to avoid directly sampling a pairwise Markov random field. The PDHAMS sampler not only satisfies generalized detailed balance, hence enabling irreversible sampling, but also is a rejection-free property for a target distribution with a quadratic potential function. In various numerical experiments, PDHAMS algorithms consistently yield superior performance compared with other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21982v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuze Zhou, Zhiqiang Tan</dc:creator>
    </item>
    <item>
      <title>Misspecifications in structural equation modeling: The choice of latent variables, causal-formative constructs or composites</title>
      <link>https://arxiv.org/abs/2507.21998</link>
      <description>arXiv:2507.21998v1 Announce Type: new 
Abstract: Empirical research in many social disciplines involves constructs that are not directly observable, such as behaviors. To model them, constructs must be operationalized using their relations with indicators. Structural equation modeling (SEM) is the primary approach for this purpose. In SEM, three types of constructs are distinguished: latent variables, causal-formative constructs, and composites. To estimate the parameters of the different models, various estimators have been developed. Many Monte Carlo studies have examined the estimation performances of different estimators for the construct types. One aspect evaluated is the consequences of construct misspecification - when the true construct type differs from the modeling choice - on parameter estimates and model fit. For example, parameter bias in models that misspecify latent variables as composites is often attributed to the chosen estimator, although model parameters depend on different estimators, making it impossible to examine the factors individually. This article aims to disentangle the issues of construct misspecification and parameter estimation by a comprehensive Monte Carlo study of all combinations between true and assumed construct types. To focus on misspecification, we used the same estimator for all models, namely the maximum likelihood (ML) estimator. To generalize beyond ML, we replicated the simulation using another estimator. We aim to examine the role of construct misspecification, not estimator choice, on the estimation performance and show that misspecification leads indeed to biased path coefficient estimates. Further, we evaluate whether fit measures can distinguish models with correct from those with misspecified constructs. We find that none of the criteria considered is suited for this. These findings stress the importance of thoughtful construct specification and the need for further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21998v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jonas Bauer, Axel Mayer, Christiane Fuchs, Tamara Schamberger</dc:creator>
    </item>
    <item>
      <title>Horseshoe Forests for High-Dimensional Causal Survival Analysis</title>
      <link>https://arxiv.org/abs/2507.22004</link>
      <description>arXiv:2507.22004v2 Announce Type: new 
Abstract: We develop a Bayesian tree ensemble model to estimate heterogeneous treatment effects in censored survival data with high-dimensional covariates. Instead of imposing sparsity through the tree structure, we place a horseshoe prior directly on the step heights to achieve adaptive global-local shrinkage. This strategy allows flexible regularisation and reduces noise. We develop a reversible jump Gibbs sampler to accommodate the non-conjugate horseshoe prior within the tree ensemble framework. We show through extensive simulations that the method accurately estimates treatment effects in high-dimensional covariate spaces, at various sparsity levels, and under non-linear treatment effect functions. We further illustrate the practical utility of the proposed approach by a re-analysis of pancreatic ductal adenocarcinoma (PDAC) survival data from The Cancer Genome Atlas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22004v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tijn Jacobs, Wessel N. van Wieringen, St\'ephanie L. van der Pas</dc:creator>
    </item>
    <item>
      <title>False discovery rate control with compound p-values</title>
      <link>https://arxiv.org/abs/2507.21465</link>
      <description>arXiv:2507.21465v1 Announce Type: cross 
Abstract: In the setting of multiple testing, compound p-values generalize p-values by asking for superuniformity to hold only \emph{on average} across all true nulls. We study the properties of the Benjamini--Hochberg procedure applied to compound p-values. Under independence, we show that the false discovery rate (FDR) is at most $1.93\alpha$, where $\alpha$ is the nominal level, and exhibit a distribution for which the FDR is $\frac{7}{6}\alpha$. If additionally all nulls are true, then the upper bound can be improved to $\alpha + 2\alpha^2$, with a corresponding worst-case lower bound of $\alpha + \alpha^2/4$. Under positive dependence, on the other hand, we demonstrate that FDR can be inflated by a factor of $O(\log m)$, where~$m$ is the number of hypotheses. We provide numerous examples of settings where compound p-values arise in practice, either because we lack sufficient information to compute non-trivial p-values, or to facilitate a more powerful analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21465v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rina Foygel Barber, Richard J Samworth</dc:creator>
    </item>
    <item>
      <title>An Equal-Probability Partition of the Sample Space: A Non-parametric Inference from Finite Samples</title>
      <link>https://arxiv.org/abs/2507.21712</link>
      <description>arXiv:2507.21712v1 Announce Type: cross 
Abstract: This paper investigates what can be inferred about an arbitrary continuous probability distribution from a finite sample of $N$ observations drawn from it. The central finding is that the $N$ sorted sample points partition the real line into $N+1$ segments, each carrying an expected probability mass of exactly $1/(N+1)$. This non-parametric result, which follows from fundamental properties of order statistics, holds regardless of the underlying distribution's shape. This equal-probability partition yields a discrete entropy of $\log_2(N+1)$ bits, which quantifies the information gained from the sample and contrasts with Shannon's results for continuous variables. I compare this partition-based framework to the conventional ECDF and discuss its implications for robust non-parametric inference, particularly in density and tail estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21712v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Urban Eriksson</dc:creator>
    </item>
    <item>
      <title>Domain Generalization and Adaptation in Intensive Care with Anchor Regression</title>
      <link>https://arxiv.org/abs/2507.21783</link>
      <description>arXiv:2507.21783v1 Announce Type: cross 
Abstract: The performance of predictive models in clinical settings often degrades when deployed in new hospitals due to distribution shifts. This paper presents a large-scale study of causality-inspired domain generalization on heterogeneous multi-center intensive care unit (ICU) data. We apply anchor regression and introduce anchor boosting, a novel, tree-based nonlinear extension, to a large dataset comprising 400,000 patients from nine distinct ICU databases. The anchor regularization consistently improves out-of-distribution performance, particularly for the most dissimilar target domains. The methods appear robust to violations of theoretical assumptions, such as anchor exogeneity. Furthermore, we propose a novel conceptual framework to quantify the utility of large external data datasets. By evaluating performance as a function of available target-domain data, we identify three regimes: (i) a domain generalization regime, where only the external model should be used, (ii) a domain adaptation regime, where refitting the external model is optimal, and (iii) a data-rich regime, where external data provides no additional value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21783v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malte Londschien, Manuel Burger, Gunnar R\"atsch, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>New (and old) predictive schemes with a.c.i.d. sequences</title>
      <link>https://arxiv.org/abs/2507.21874</link>
      <description>arXiv:2507.21874v1 Announce Type: cross 
Abstract: There is a growing interest in procedures for Bayesian inference that bypass the need to specify a model and prior but simply rely on a predictive rule that describes how we learn on future observations given the available ones. At the heart of the idea is a bootstrap-type scheme that allows us to move from the realm of prediction to that of inference. Which conditions the predictive rule needs to satisfy to produce valid inference is a key question. In this work, we substantially relax previous assumptions building on a generalization of martingales, opening up the possibility of employing a much wider range of predictive rules that were previously ruled out. These include ``old" ideas in Statistics and Learning Theory, such as kernel estimators, and more novel ones, such as the parametric Bayesian bootstrap or copula-based algorithms. Our aim is not to advocate in favor of one predictive rule versus the other ones, but rather to showcase the benefits of working with this larger class of predictive rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21874v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Battiston, Lorenzo Cappello</dc:creator>
    </item>
    <item>
      <title>Backward Joint Model for the Joint Dynamic Prediction of Time-to-Event and Longitudinal Data: Basic Formulation and New Developments</title>
      <link>https://arxiv.org/abs/2311.00878</link>
      <description>arXiv:2311.00878v3 Announce Type: replace 
Abstract: Dynamic prediction of future clinical outcomes based on longitudinally measured predictors plays a crucial role in disease management and patient counseling, particularly when conventional static models are inadequate. Joint modeling of longitudinal and time-to-event data provides a useful framework for addressing this challenge. In this paper, we present a comprehensive development of the recently proposed backward joint model (BJM; Shen and Li 2021}, which factorizes the likelihood into the distribution of time-to-event data and the conditional distribution of longitudinal data given the event time. This structure facilitates computation and is well-suited for multivariate longitudinal data. We introduce several novel developments to the BJM, including the extrapolation and two-part specifications, as well as the incorporation of competing risks. We also address an important yet underexplored problem in the literature: predicting future longitudinal trajectories conditional on predicted event times. Additionally, we explore the connection between BJM and existing joint modeling approaches. All these extensions preserve the computational advantages of the basic BJM formulation, including one-dimensional numerical integration, convex optimization via the EM algorithm, and a quick procedure for consistent estimation using standard software. We evaluate the method's performance through simulation studies and illustrate its utility in a chronic kidney disease application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00878v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhao Li, Shikun Wang, Zhe Yin, Brad C. Astor, Wei Yang, Tom H. Greene, Liang Li</dc:creator>
    </item>
    <item>
      <title>A dimension reduction approach to edge weight estimation for use in spatial models</title>
      <link>https://arxiv.org/abs/2407.02684</link>
      <description>arXiv:2407.02684v2 Announce Type: replace 
Abstract: Models for areal data are traditionally defined using the neighborhood structure of the regions on which data are observed. The unweighted adjacency matrix of a graph is commonly used to characterize the relationships between locations, resulting in the implicit assumption that all pairs of neighboring regions interact similarly, an assumption which may not be true in practice. It has been shown that more complex spatial relationships between graph nodes may be represented when edge weights are allowed to vary. Christensen and Hoff (2023) introduced a covariance model for data observed on graphs which is more flexible than traditional alternatives, parameterizing covariance as a function of an unknown edge weights matrix. A potential issue with their approach is that each edge weight is treated as a unique parameter, resulting in increasingly challenging parameter estimation as graph size increases. Within this article we propose a framework for estimating edge weight matrices that reduces their effective dimension via a basis function representation of of the edge weights. We show that this method may be used to enhance the performance and flexibility of covariance models parameterized by such matrices in a series of illustrations, simulations and data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02684v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael F. Christensen, Jo Eidsvik</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Polychoric Correlation</title>
      <link>https://arxiv.org/abs/2407.18835</link>
      <description>arXiv:2407.18835v4 Announce Type: replace 
Abstract: Polychoric correlation is often an important building block in the analysis of rating data, particularly for structural equation models. However, the commonly employed maximum likelihood (ML) estimator is highly susceptible to misspecification of the polychoric correlation model, for instance through violations of latent normality assumptions. We propose a novel estimator that is designed to be robust against partial misspecification of the polychoric model, that is, when the model is misspecified for an unknown fraction of observations, such as careless respondents. To this end, the estimator minimizes a robust loss function based on the divergence between observed frequencies and theoretical frequencies implied by the polychoric model. In contrast to existing literature, our estimator makes no assumption on the type or degree of model misspecification. It furthermore generalizes ML estimation, is consistent as well as asymptotically normally distributed, and comes at no additional computational cost. We demonstrate the robustness and practical usefulness of our estimator in simulation studies and an empirical application on a Big Five administration. In the latter, the polychoric correlation estimates of our estimator and ML differ substantially, which, after further inspection, is likely due to the presence of careless respondents that the estimator helps identify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18835v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Welz, Patrick Mair, Andreas Alfons</dc:creator>
    </item>
    <item>
      <title>A Framework for Covariate-Adjusted Bivariate Causal Discovery</title>
      <link>https://arxiv.org/abs/2502.10317</link>
      <description>arXiv:2502.10317v2 Announce Type: replace 
Abstract: Ascertaining causal direction from observational data is a fundamental challenge in scientific inquiry. Of particular interest is the problem of covariate-adjusted bivariate causal discovery, i.e., determining the causal direction between X and Y in the presence of Z. While unadjusted bivariate causal discovery has seen significant advances (Hoyer et al., 2008; Ni, 2022), there is a lack of methodology dealing with real-world bivariate relationships, which are often modulated by a set of covariate(s), Z. Building on previous work in Purkayastha and Song (2025), we introduce a novel, nonparametric framework for the covariate-adjusted bivariate causal discovery problem and propose a conditional asymmetry coefficient to track said direction of causation. We develop a robust estimation procedure using kernel-based conditional density estimation with cross-fitting and also provide rigorous uncertainty quantification using a nonparametric kernel smoothing technique, addressing a key limitation of many existing algorithms. As a key application, we demonstrate how this framework can be applied to the problem of collider detection, a persistent challenge in causal structure learning. Simulation studies show our method's superior performance in identifying causal structures. We apply our approach to an epigenetic study (Perng et al., 2019), investigating the role of blood pressure in regulating the effects of DNA methylation. In summary, our work offers methodological advancement by providing a robust, inferential toolkit for dissecting complex, moderated bivariate causal relationships in observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10317v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soumik Purkayastha, Peter X. -K. Song</dc:creator>
    </item>
    <item>
      <title>On prior smoothing with discrete spatial data in the context of disease mapping</title>
      <link>https://arxiv.org/abs/2503.16151</link>
      <description>arXiv:2503.16151v2 Announce Type: replace 
Abstract: Disease mapping attempts to explain observed health event counts across areal units, typically using Markov random field models. These models rely on spatial priors to account for variation in raw relative risk or rate estimates. Spatial priors introduce some degree of smoothing, wherein, for any particular unit, empirical risk or incidence estimates are either adjusted towards a suitable mean or incorporate neighbor-based smoothing. While model explanation may be the primary focus, the literature lacks a comparison of the amount of smoothing introduced by different spatial priors. Additionally, there has been no investigation into how varying the parameters of these priors influences the resulting smoothing. This study examines seven commonly used spatial priors through both simulations and real data analyses. Using areal maps of peninsular Spain and England, we analyze smoothing effects using two datasets with associated populations at risk. We propose empirical metrics to quantify the smoothing achieved by each model and theoretical metrics to calibrate the expected extent of smoothing as a function of model parameters. We employ areal maps in order to quantitatively characterize the extent of smoothing within and across the models as well as to link the theoretical metrics to the empirical metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16151v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garazi Retegui, Alan E. Gelfand, Jaione Etxeberria, Mar\'ia Dolores Ugarte</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for High-dimensional Time Series with a Stationary Directed Acyclic Graphical Structure</title>
      <link>https://arxiv.org/abs/2503.23563</link>
      <description>arXiv:2503.23563v5 Announce Type: replace 
Abstract: In multivariate time series analysis, understanding the underlying causal relationships among variables is often of interest for various applications. Directed acyclic graphs (DAGs) provide a powerful framework for representing causal dependencies. This paper proposes a novel Bayesian approach for modeling multivariate time series where conditional independencies and causal structure are encoded by a DAG. The proposed model allows structural properties such as stationarity to be easily accommodated. Given the application, we further extend the model for matrix-variate time series. We take a Bayesian approach to inference, and a ``immersion-posterior'' based efficient computational algorithm is developed. The posterior convergence properties of the proposed method are established along with two identifiability results for the unrestricted structural equation models. The utility of the proposed method is demonstrated through simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23563v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Loss Functions for Measuring the Accuracy of Nonnegative Cross-Sectional Predictions</title>
      <link>https://arxiv.org/abs/2505.18130</link>
      <description>arXiv:2505.18130v3 Announce Type: replace 
Abstract: Measuring the accuracy of cross-sectional predictions is a subjective problem. Generally, this problem is avoided. In contrast, this paper confronts subjectivity up front by eliciting an impartial decision-maker's preferences. These preferences are embedded into an axiomatically-derived loss function, one of the simplest version of which is described. The parameters of the loss function can be estimated by linear regression. Specification tests for this function are described. This framework is extended to weighted averages of estimates to find the optimal weightings. A special case occurs when the predictions represent resource allocations: the apportionment literature is used to construct the Webster-Saint Lag\"ue Rule, a particular parametrization of the loss function. These loss functions are compared to those existing in the literature. Finally, a family of bias measures are created using signed versions of these loss functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18130v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman</dc:creator>
    </item>
    <item>
      <title>Geometric medians on product manifolds</title>
      <link>https://arxiv.org/abs/2505.18844</link>
      <description>arXiv:2505.18844v2 Announce Type: replace 
Abstract: Product manifolds arise when heterogeneous geometric variables are recorded jointly. While the Fr\'{e}chet mean on Riemannian manifolds separates cleanly across factors, the canonical geometric median couples them, and its behavior in product spaces has remained largely unexplored. In this paper, we give the first systematic treatment of this problem. After formulating the coupled objective, we establish general existence and uniqueness results: the median is unique on any Hadamard product, and remains locally unique under sharp conditions on curvature and injectivity radius even when one or more factors have positive curvature. We then prove that the estimator enjoys Lipschitz stability to perturbations and the optimal breakdown point, extending classical robustness guarantees to the product-manifold setting. Two practical solvers are proposed, including a Riemannian subgradient method with global sublinear convergence and a product-aware Weiszfeld iteration that achieves local linear convergence when safely away from data singularities. Both algorithms update the factors independently while respecting the latent coupling term, enabling implementation with standard manifold primitives. Simulations on parameter spaces of univariate and multivariate Gaussian distributions endowed with the Bures-Wasserstein geometry show that the median is more resilient to contamination than the Fr\'{e}chet mean. The results provide both theoretical foundations and computational tools for robust location inference with heterogeneous manifold-valued data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18844v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kisung You, Jiewon Park</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Circular Data</title>
      <link>https://arxiv.org/abs/2507.19889</link>
      <description>arXiv:2507.19889v2 Announce Type: replace 
Abstract: In causal inference, a fundamental task is to estimate the effect resulting from a specific treatment, which is often handled with inverse probability weighting. Despite an abundance of attention to the advancement of this task, most articles have focused on linear data rather than circular data, which are measured in angles. In this article, we extend the causal inference framework to accommodate circular data. Specifically, two new treatment effects, average direction treatment effect (ADTE) and average length treatment effect (ALTE), are introduced to offer a proper causal explanation for these data. As the average direction and average length describe the location and concentration of a random sample of circular data, the ADTE and ALTE measure the change in direction and length between two counterfactual outcomes. With inverse probability weighting, we propose estimators that exhibit ideal theoretical properties, which are validated by a simulation study. To illustrate the practical utility of our estimator, we analyze the effect of different job types on dispatchers' sleep patterns using data from Federal Railroad Administration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19889v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuan-Hsun Wu</dc:creator>
    </item>
    <item>
      <title>Robust Matrix Completion for Discrete Rating-Scale Data: Coping with Fake Profiles in Recommender Systems</title>
      <link>https://arxiv.org/abs/2412.20802</link>
      <description>arXiv:2412.20802v2 Announce Type: replace-cross 
Abstract: Recommender systems are essential tools in the digital landscape for connecting users with content that more closely aligns with their preferences. Matrix completion is a widely used statistical framework for such systems, aiming to predict a user's preferences for items they have not yet rated by leveraging the observed ratings in a partially filled user-item rating matrix. Realistic applications of matrix completion in recommender systems must address several challenges that are too often neglected: (i) the discrete nature of rating-scale data, (ii) the presence of malicious users who manipulate the system to their advantage through the creation of fake profiles, and (iii) missing-not-at-random patterns, where users are more likely to rate items they expect to enjoy. Our goal in this paper is twofold. First, we propose a novel matrix completion method, robust discrete matrix completion (RDMC), designed specifically to handle the discrete nature of sparse rating-scale data and to remain reliable in the presence of adversarial manipulation. We evaluate RDMC through carefully designed experiments and realistic case studies. Our work therefore, secondly, offers a statistically-sound blueprint for future studies on how to evaluate matrix completion methods for recommender systems under realistic scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20802v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aurore Archimbaud, Andreas Alfons, Ines Wilms</dc:creator>
    </item>
  </channel>
</rss>
