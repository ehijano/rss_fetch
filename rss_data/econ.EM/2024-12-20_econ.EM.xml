<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Dec 2024 05:01:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Good Controls Gone Bad: Difference-in-Differences with Covariates</title>
      <link>https://arxiv.org/abs/2412.14447</link>
      <description>arXiv:2412.14447v1 Announce Type: new 
Abstract: This paper introduces the two-way common causal covariates (CCC) assumption, which is necessary to get an unbiased estimate of the ATT when using time-varying covariates in existing Difference-in-Differences methods. The two-way CCC assumption implies that the effect of the covariates remain the same between groups and across time periods. This assumption has been implied in previous literature, but has not been explicitly addressed. Through theoretical proofs and a Monte Carlo simulation study, we show that the standard TWFE and the CS-DID estimators are biased when the two-way CCC assumption is violated. We propose a new estimator called the Intersection Difference-in-differences (DID-INT) which can provide an unbiased estimate of the ATT under two-way CCC violations. DID-INT can also identify the ATT under heterogeneous treatment effects and with staggered treatment rollout. The estimator relies on parallel trends of the residuals of the outcome variable, after appropriately adjusting for covariates. This covariate residualization can recover parallel trends that are hidden with conventional estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14447v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sunny Karim, Matthew D. Webb</dc:creator>
    </item>
    <item>
      <title>Testing linearity of spatial interaction functions \`a la Ramsey</title>
      <link>https://arxiv.org/abs/2412.14778</link>
      <description>arXiv:2412.14778v1 Announce Type: new 
Abstract: We propose a computationally straightforward test for the linearity of a spatial interaction function. Such functions arise commonly, either as practitioner imposed specifications or due to optimizing behaviour by agents. Our test is nonparametric, but based on the Lagrange Multiplier principle and reminiscent of the Ramsey RESET approach. This entails estimation only under the null hypothesis, which yields an easy to estimate linear spatial autoregressive model. Monte Carlo simulations show excellent size control and power. An empirical study with Finnish data illustrates the test's practical usefulness, shedding light on debates on the presence of tax competition among neighbouring municipalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14778v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhimanyu Gupta, Jungyoon Lee, Francesca Rossi</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Average Causal Effects in Fixed Effects Logit Models</title>
      <link>https://arxiv.org/abs/2105.00879</link>
      <description>arXiv:2105.00879v5 Announce Type: replace 
Abstract: This paper studies identification and estimation of average causal effects, such as average marginal or treatment effects, in fixed effects logit models with short panels. Relating the identified set of these effects to an extremal moment problem, we first show how to obtain sharp bounds on such effects simply, without any optimization. We also consider even simpler outer bounds, which, contrary to the sharp bounds, do not require any first-step nonparametric estimators. We build confidence intervals based on these two approaches and show their asymptotic validity. Monte Carlo simulations suggest that both approaches work well in practice, the second being typically competitive in terms of interval length. Finally, we show that our method is also useful to measure treatment effect heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.00879v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laurent Davezies, Xavier D'Haultf{\oe}uille, Louise Laage</dc:creator>
    </item>
    <item>
      <title>A Supervised Machine Learning Approach for Assessing Grant Peer Review Reports</title>
      <link>https://arxiv.org/abs/2411.16662</link>
      <description>arXiv:2411.16662v2 Announce Type: replace 
Abstract: Peer review in grant evaluation informs funding decisions, but the contents of peer review reports are rarely analyzed. In this work, we develop a thoroughly tested pipeline to analyze the texts of grant peer review reports using methods from applied Natural Language Processing (NLP) and machine learning. We start by developing twelve categories reflecting content of grant peer review reports that are of interest to research funders. This is followed by multiple human annotators' iterative annotation of these categories in a novel text corpus of grant peer review reports submitted to the Swiss National Science Foundation. After validating the human annotation, we use the annotated texts to fine-tune pre-trained transformer models to classify these categories at scale, while conducting several robustness and validation checks. Our results show that many categories can be reliably identified by human annotators and machine learning approaches. However, the choice of text classification approach considerably influences the classification performance. We also find a high correspondence between out-of-sample classification performance and human annotators' perceived difficulty in identifying categories. Our results and publicly available fine-tuned transformer models will allow researchers and research funders and anybody interested in peer review to examine and report on the contents of these reports in a structured manner. Ultimately, we hope our approach can contribute to ensuring the quality and trustworthiness of grant peer review.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16662v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Okasa, Alberto de Le\'on, Michaela Strinzel, Anne Jorstad, Katrin Milzow, Matthias Egger, Stefan M\"uller</dc:creator>
    </item>
  </channel>
</rss>
