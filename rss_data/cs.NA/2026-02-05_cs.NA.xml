<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Feb 2026 02:53:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Efficient Explicit Taylor ODE Integrators with Symbolic-Numeric Computing</title>
      <link>https://arxiv.org/abs/2602.04086</link>
      <description>arXiv:2602.04086v1 Announce Type: new 
Abstract: Taylor series methods show a newfound promise for the solution of non-stiff ordinary differential equations (ODEs) given the rise of new compiler-enhanced techniques for calculating high order derivatives. In this paper we detail a new Julia-based implementation that
  has two important techniques: (1) a general purpose higher-order automatic differentiation engine for derivative evaluation with low overhead; (2) a combined symbolic-numeric approach to generate code for recursively computing the Taylor polynomial of the ODE solution. We demonstrate that the resulting software's compiler-based tooling is transparent to the user, requiring no changes from interfaces required to use standard explicit Runge-Kutta methods, while achieving better run time performance. In addition, we also developed a comprehensive adaptive time and order algorithm that uses different step size and polynomial degree across the integration period, which makes this implementation more efficient and versatile in a broad range of dynamics. We show that for codes compatible with compiler transformations, these integrators are more efficient and robust than the traditionally used explicit Runge-Kutta methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04086v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Songchen Tan, Oscar Smith, Christopher Rackauckas</dc:creator>
    </item>
    <item>
      <title>A frequency-domain method to inverse moving source problem with unknown radiating moment</title>
      <link>https://arxiv.org/abs/2602.04207</link>
      <description>arXiv:2602.04207v1 Announce Type: new 
Abstract: This paper introduces a multi-frequency factorization method for imaging a time-dependent source, specifically to recover its spatial support and the associated excitation instants. Using far-field data from two opposite directions, we establish a computational criterion that characterizes both the unknown pulse moments and the narrowest strip (perpendicular to the direction) enclosing the source support. Central to our inversion scheme is the construction of indicator functions, defined pointwise over the spatial and temporal sampling variables. The proposed inversion scheme permits the recovery of the $\Theta$-convex support domain from far-field data at sparse observation directions. Uniqueness in determining the convex hull of the support and the excitation instants-using all observation directions-is also established as a direct consequence of the factorization method. The effectiveness and feasibility of the approach are examined through comprehensive numerical simulations in two and three dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04207v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guanqiu Ma, Hongxia Guo, Guanghui Hu</dc:creator>
    </item>
    <item>
      <title>Towards $C^0$ finite element methods for fourth-order elliptic equation. Part I: general boundary conditions</title>
      <link>https://arxiv.org/abs/2602.04235</link>
      <description>arXiv:2602.04235v1 Announce Type: new 
Abstract: This paper is part of a series developing $C^0$ finite element methods for fourth-order elliptic equations on polygonal domains. Here, we investigate how boundary conditions influence the design of effective $C^0$ schemes, specifically focusing on equations without lower-order terms, namely the biharmonic equation. We propose a modified mixed formulation that decomposes the problem into a system of Poisson equations, where the number of equations depends on both the largest interior angle and the boundary conditions on its two adjacent sides. In contrast to the naive mixed formulation, which involves only two Poisson problems, the proposed approach guarantees convergence to the true solution for arbitrary polygonal domains and general boundary conditions, including Navier, Neumann, and mixed boundary conditions. $C^0$ finite element algorithms are developed, rigorous error estimates are established, and numerical experiments are presented to demonstrate the well-posedness and effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04235v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xihao Zhang, Hengguang Li, Nianyu Yi, Peimeng Yin</dc:creator>
    </item>
    <item>
      <title>Balancing Inexactness in Mixed Precision Matrix Computations</title>
      <link>https://arxiv.org/abs/2602.04348</link>
      <description>arXiv:2602.04348v1 Announce Type: new 
Abstract: Support for arithmetic in multiple precisions and number formats is becoming increasingly common in emerging high-performance architectures. From a computational scientist's perspective, our goal is to determine how and where we can safely exploit mixed precision computation in our codes to improve performance. One case where the use of low precision is natural, common in computational science, is when there are already other significant sources of ``inexactness'' present, e.g., discretization error, measurement error, or algorithmic approximation error. In such instances, analyzing the interaction of these different sources of inexactness can give insight into how the precisions of various computations should be chosen in order to ``balance'' the errors, potentially improving performance without a noticeable decrease in accuracy. We present a few recent examples of this approach which demonstrate the potential for the use of mixed precision in numerical linear algebra and matrix computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04348v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erin Claire Carson</dc:creator>
    </item>
    <item>
      <title>On the pure traction problem of linear elasticity: a regularized formulation and its robust approximation</title>
      <link>https://arxiv.org/abs/2602.04359</link>
      <description>arXiv:2602.04359v1 Announce Type: new 
Abstract: The pure traction problem of elasticity appears frequently in engineering applications, and its complexity stems from the fact that its solution is unique only up to (infinitesimal) rigid body motions. When finite elements are employed to approximate this problem, one solution is typically singled out by applying carefully selected boundary conditions on the discrete model or by imposing global constraints on the deformation. However, neither of these strategies is both simple and computationally efficient. In this work, we propose a new approach to solving the pure traction problem that overcomes existing limitations. Our method builds on a regularized form of the problem whose solution is shown to be unique, converges to the original solution of minimal norm, and can be approximated with finite elements in a straightforward way, without additional degrees of freedom. Additionally, we analyze the situation in which the approximation of the solution domain renders the loading of the discretized problem non-equilibrated, making the problem ill-posed. In this case, we propose a regularized predictor--corrector finite element formulation that handles the incompatibilities of the loading, providing a solution that converges to that of the original Neumann problem as the mesh size and the regularizing parameter tend to zero. Numerical examples illustrate the effectiveness of the proposed approach for representative problems in mechanics where pure traction boundary conditions appear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04359v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ahsan Kaleem, Cristian Gebhardt, Ignacio Romero</dc:creator>
    </item>
    <item>
      <title>A Priori and A Posteriori Error Identities for Vectorial Problems via Convex Duality</title>
      <link>https://arxiv.org/abs/2602.04368</link>
      <description>arXiv:2602.04368v1 Announce Type: new 
Abstract: Convex duality has been leveraged in recent years to derive a posteriori error estimates and identities for a wide range of non-linear and non-smooth scalar problems. By employing remarkable compatibility properties of the Crouzeix-Raviart and Raviart-Thomas elements, optimal convergence of non-conforming discretisations and flux reconstruction formulas have also been established. This paper aims to extend these results to the vectorial setting, focusing on the archetypical problems of incompressible Stokes and Navier-Lam\'e. Moreover, unlike most previous results, we consider inhomogeneous mixed boundary conditions and loads in the topological dual of the energy space. At the discrete level, we derive error identities and estimates that enable to prove quasi-optimal error estimates for a Crouzeix-Raviart discretisation with minimal regularity assumptions and no data oscillation terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04368v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. A. Gazca-Orozco, A. Kaltenbach</dc:creator>
    </item>
    <item>
      <title>Randomized Projection Operators onto Piecewise Polynomial Spaces</title>
      <link>https://arxiv.org/abs/2602.04490</link>
      <description>arXiv:2602.04490v1 Announce Type: new 
Abstract: We introduce computable projection operators onto piecewise polynomial spaces, defined via sampling and discrete least-squares polynomial approximations. The resulting mappings exhibit (almost) optimal approximation properties in $L^2$ and $H^{-1}$. As smoothers for incomplete or rough data, they yield computable finite element discretizations with optimal convergence rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04490v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Storn</dc:creator>
    </item>
    <item>
      <title>Domain decomposition methods and preconditioning strategies using generalized locally Toepltiz tools: proposals, analysis, and numerical validation</title>
      <link>https://arxiv.org/abs/2602.04603</link>
      <description>arXiv:2602.04603v1 Announce Type: new 
Abstract: In the current work we present a spectral analysis of the additive and multiplicative Schwarz methods within the framework of domain decomposition techniques, by investigating the spectral properties of these classical Schwarz preconditioning matrix-sequences, with emphasis on their convergence behavior and on the effect of transmission operators. In particular, after a general presentation of various options, we focus on restricted variants of the Schwarz methods aimed at improving parallel efficiency, while preserving their convergence features. In order to rigorously describe and analyze the convergence behavior, we employ the theory of generalized locally Toeplitz (GLT) sequences, which provides a robust framework for studying the asymptotic spectral distribution of the discretized operators arising from Schwarz iterations. By associating each operator sequence with the appropriate GLT symbol, we derive explicit expressions for the GLT symbols of the convergence factors, for both additive and multiplicative Schwarz methods. The GLT-based spectral approach offers a unified and systematic understanding of how the spectrum evolves with mesh refinement and overlap size (in the algebraic case). Our analysis not only deepens the theoretical understanding of classical Schwarz methods, but also establishes a foundation for examining future restricted or hybrid Schwarz variants using symbolic spectral tools. These results enable the prediction of the remarkable efficiency of block Jacobi/Gauss--Seidel and block additive/multiplicative Schwarz preconditioners for GLT sequences, as further illustrated through a wide choice of numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04603v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdessadek Rifqui, Ahmed Ratnani, Stefano Serra-Capizzano</dc:creator>
    </item>
    <item>
      <title>Numerical study of loss of hyperbolicity using a cold plasma model</title>
      <link>https://arxiv.org/abs/2602.03859</link>
      <description>arXiv:2602.03859v1 Announce Type: cross 
Abstract: We study a one-dimensional system of cold plasma equations taking into account electron-ion collisions in both relativistic and nonrelativistic cases. It is known that for a constant collision coefficient $\nu$, the solution to the Cauchy problem for such a system can lose smoothness. However, if the dependence of $\nu$ on the electron density $N$ is more than linear, then the solution remains globally smooth for any initial data. However, the appearance of the dependence $\nu(N)$ leads to a change in the type of the system, it loses hyperbolicity, which leads to computational problems. In this paper, we propose a new implicit solution method in Euler variables that overcomes these difficulties. It can be used in both nonrelativistic and relativistic cases and is tested for the threshold case of a linear dependence $\nu(N)=\nu_1+\nu_0 N$, when smoothness can still be lost. The computational experiments carried out are in full agreement with the available theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03859v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.plasm-ph</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evgeniy V. Chizhonkov, Olga S. Rozanova</dc:creator>
    </item>
    <item>
      <title>A Probabilistic Framework for Solving High-Frequency Helmholtz Equations via Diffusion Models</title>
      <link>https://arxiv.org/abs/2602.04082</link>
      <description>arXiv:2602.04082v1 Announce Type: cross 
Abstract: Deterministic neural operators perform well on many PDEs but can struggle with the approximation of high-frequency wave phenomena, where strong input-to-output sensitivity makes operator learning challenging, and spectral bias blurs oscillations. We argue for adopting a probabilistic approach for approximating waves in high-frequency regime, and develop our probabilistic framework using a score-based conditional diffusion operator. After demonstrating a stability analysis of the Helmholtz operator, we present our numerical experiments across a wide range of frequencies, benchmarked against other popular data-driven and machine learning approaches for waves. We show that our probabilistic neural operator consistently produces robust predictions with the lowest errors in $L^2$, $H^1$, and energy norms. Moreover, unlike all the other tested deterministic approaches, our framework remarkably captures uncertainties in the input sound speed map propagated to the solution field. We envision that our results position probabilistic operator learning as a principled and effective approach for solving complex PDEs such as Helmholtz in the challenging high-frequency regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04082v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yicheng Zou, Samuel Lanthaler, Hossein Salahshoor</dc:creator>
    </item>
    <item>
      <title>From Dead Neurons to Deep Approximators: Deep Bernstein Networks as a Provable Alternative to Residual Layers</title>
      <link>https://arxiv.org/abs/2602.04264</link>
      <description>arXiv:2602.04264v1 Announce Type: cross 
Abstract: Residual connections are the de facto standard for mitigating vanishing gradients, yet they impose structural constraints and fail to address the inherent inefficiencies of piecewise linear activations. We show that Deep Bernstein Networks (which utilizes Bernstein polynomials as activation functions) can act as residual-free architecture while simultaneously optimize trainability and representation power. We provide a two-fold theoretical foundation for our approach. First, we derive a theoretical lower bound on the local derivative, proving it remains strictly bounded away from zero. This directly addresses the root cause of gradient stagnation; empirically, our architecture reduces ``dead'' neurons from 90\% in standard deep networks to less than 5\%, outperforming ReLU, Leaky ReLU, SeLU, and GeLU. Second, we establish that the approximation error for Bernstein-based networks decays exponentially with depth, a significant improvement over the polynomial rates of ReLU-based architectures. By unifying these results, we demonstrate that Bernstein activations provide a superior mechanism for function approximation and signal flow. Our experiments on HIGGS and MNIST confirm that Deep Bernstein Networks achieve high-performance training without skip-connections, offering a principled path toward deep, residual-free architectures with enhanced expressive capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04264v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ibrahim Albool, Malak Gamal El-Din, Salma Elmalaki, Yasser Shoukry</dc:creator>
    </item>
    <item>
      <title>Maximum-Volume Nonnegative Matrix Factorization</title>
      <link>https://arxiv.org/abs/2602.04795</link>
      <description>arXiv:2602.04795v2 Announce Type: cross 
Abstract: Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04795v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>eess.SP</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivier Vu Thanh, Nicolas Gillis</dc:creator>
    </item>
    <item>
      <title>The matrix-vector complexity of $Ax=b$</title>
      <link>https://arxiv.org/abs/2602.04842</link>
      <description>arXiv:2602.04842v1 Announce Type: cross 
Abstract: Matrix-vector algorithms, particularly Krylov subspace methods, are widely viewed as the most effective algorithms for solving large systems of linear equations. This paper establishes lower bounds on the worst-case number of matrix-vector products needed by such an algorithm to approximately solve a general linear system. The first main result is that, for a matrix-vector algorithm which can perform products with both a matrix and its transpose, $\Omega(\kappa \log(1/\varepsilon))$ matrix-vector products are necessary to solve a linear system with condition number $\kappa$ to accuracy $\varepsilon$, matching an upper bound for conjugate gradient on the normal equations. The second main result is that one-sided algorithms, which lack access to the transpose, must use $n$ matrix-vector products to solve an $n \times n$ linear system, even when the problem is perfectly conditioned. Both main results include explicit constants that match known upper bounds up to a factor of four. These results rigorously demonstrate the limitations of matrix-vector algorithms and confirm the optimality of widely used Krylov subspace algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04842v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Micha{\l} Derezi\'nski, Ethan N. Epperly, Raphael A. Meyer</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of the Upwind Difference Methods for Hamilton-Jacobi-Bellman Equations</title>
      <link>https://arxiv.org/abs/2301.06415</link>
      <description>arXiv:2301.06415v2 Announce Type: replace 
Abstract: This paper investigates the convergence properties of the upwind difference scheme for the Hamilton--Jacobi--Bellman (HJB) equation, a central partial differential equation in optimal control theory. First, assuming the existence of a classical solution, we show that the numerical solution converges to the true solution with a first-order rate with respect to the time step. This result complements the square-root rate established in previous studies for viscosity solutions. Second, by exploiting the correspondence between HJB equations and conservation laws, we prove the convergence of the optimal control input. This analysis is crucial for practical applications where the control input is the primary quantity of interest, yet it has rarely been addressed in previous studies. Finally, we confirm the validity of our theoretical results through numerical experiments on typical control problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06415v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daisuke Inoue, Yuji Ito, Takahito Kashiwabara, Norikazu Saito, Hiroaki Yoshida</dc:creator>
    </item>
    <item>
      <title>Greedy randomized block Kaczmarz method for matrix equation AXB=C and its applications in color image restoration</title>
      <link>https://arxiv.org/abs/2408.05444</link>
      <description>arXiv:2408.05444v2 Announce Type: replace 
Abstract: In view of the advantages of simplicity and effectiveness of the Kaczmarz method, which was originally employed to solve the large-scale system of linear equations $Ax=b$, we study the greedy randomized block Kaczmarz method (ME-GRBK) and its relaxation and deterministic versions to solve the matrix equation $AXB=C$, which is commonly encountered in the applications of engineering sciences. It is demonstrated that our algorithms converge to the unique least-norm solution of the matrix equation when it is consistent and their convergence rate is faster than that of the randomized block Kaczmarz method (ME-RBK). Moreover, the block Kaczmarz method (ME-BK) for solving the matrix equation $AXB=C$ is investigated and it is found that the ME-BK method converges to the solution $A^{+}CB^{+}+X^{0}-A^{+}AX^{0}BB^{+}$ when it is consistent. The numerical tests verify the theoretical results and the methods presented in this paper are applied to the color image restoration problem to obtain satisfactory restored images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05444v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenli Wang, Duo Liu, Gangrong Qu</dc:creator>
    </item>
    <item>
      <title>Weak error on the densities for the Euler scheme of stable additive SDEs with H{\"o}lder drift</title>
      <link>https://arxiv.org/abs/2410.10250</link>
      <description>arXiv:2410.10250v2 Announce Type: replace 
Abstract: We are interested in the Euler-Maruyama dicretization of the SDE dXt =b(t,Xt)dt+ dZt, X0 =x$\in$Rd, where Zt is a symmetric isotropic d-dimensional $\alpha$-stable process, $\alpha$ $\in$ (1, 2] and the drift b $\in$ L$\infty$ ([0,T],C$\beta$(Rd,Rd)), $\beta$ $\in$ (0,1), is bounded and H{\"o}lder regular in space. Using an Euler scheme with a randomization of the time variable, we show that, denoting $\gamma$\,:= $\alpha$ + $\beta$ -- 1, the weak error on densities related to this discretization converges at the rate $\gamma$/$\alpha$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10250v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Stochastic Processes and their Applications, 2025, 190, Paper 104736</arxiv:journal_reference>
      <dc:creator>Mathis Fitoussi (LaMME), Stephane Menozzi (LaMME)</dc:creator>
    </item>
    <item>
      <title>Adaptive hyper-reduction of non-sparse operators: application to parametric particle-based kinetic plasma models</title>
      <link>https://arxiv.org/abs/2504.00604</link>
      <description>arXiv:2504.00604v2 Announce Type: replace 
Abstract: This paper proposes an adaptive hyper-reduction method to reduce the computational cost associated with the simulation of parametric particle-based kinetic plasma models, specifically focusing on the Vlasov-Poisson equation. Conventional model order reduction and hyper-reduction techniques are often ineffective for such models due to the non-sparse nature of the nonlinear operators arising from the interactions between particles. To tackle this issue, we propose an adaptive, structure-preserving hyper-reduction method that leverages a decomposition of the discrete reduced Hamiltonian into a linear combination of terms, each depending on a few components of the state. The proposed approximation strategy allows to: (i) preserve the Hamiltonian structure of the problem; (ii) evaluate nonlinear non-sparse operators in a computationally efficient way; (iii) overcome the Kolmogorov barrier of transport-dominated problems via evolution of the approximation space and adaptivity of the rank of the solution. The proposed method is validated on numerical benchmark simulations, demonstrating stable and accurate performance with substantial runtime reductions compared to the full order model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00604v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jcp.2025.114609</arxiv:DOI>
      <dc:creator>Cecilia Pagliantini, Federico Vismara</dc:creator>
    </item>
    <item>
      <title>A filtered finite difference method for a highly oscillatory nonlinear Klein--Gordon equation</title>
      <link>https://arxiv.org/abs/2504.19359</link>
      <description>arXiv:2504.19359v2 Announce Type: replace 
Abstract: We consider a nonlinear Klein--Gordon equation in the nonrelativistic limit regime with highly oscillatory initial data in the form of a modulated plane wave. In this regime, the solution exhibits rapid oscillations in both time and space, posing challenges for numerical approximation. We propose a filtered finite difference method that achieves second-order accuracy with time steps and mesh sizes that are not restricted in magnitude by the small parameter. Moreover, the method is uniformly convergent in the range from arbitrarily small to moderately bounded scaling parameters. Numerical experiments illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19359v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanyan Shi, Christian Lubich</dc:creator>
    </item>
    <item>
      <title>A Complete-Electrode-Model-Based Forward Approach for Transcranial Temporal Interference Stimulation with Linearization: A Numerical Simulation Study</title>
      <link>https://arxiv.org/abs/2506.18436</link>
      <description>arXiv:2506.18436v2 Announce Type: replace 
Abstract: Background and Objective: Transcranial temporal interference stimulation (tTIS) is a promising non-invasive brain stimulation technique in which interference between electrical current fields extends the possibilities of electrical brain stimulation. The objective of this study is to develop an efficient mathematical tTIS forward modelling scheme that allows for realistic and adaptable simulation and can be updated accurately when the contact resistance is modified in one or more electrodes. Such a model is vital, for example, in optimization processes that seek the best possible stimulation currents to exhibit or inhibit a given brain region. This study aims to establish and evaluate the complete electrode model (CEM), i.e., a set of boundary conditions incorporating electrode impedance and contact patch, as a forward finite-element-method-based simulation technique for tTIS and investigate linearized CEM as a surrogate.
  Results: The CEM-based forward simulation successfully reproduced the volumetric stimulating fields induced by tTIS. Sensitivity analysis showed that variations in electrode resistance affects the field distribution, especially in regions where the interfering currents have nearly equal amplitudes. The linearized CEM model closely matched the full nonlinear model within a predefined peak signal-to-noise ratio (PSNR) threshold for relative error. Both models exhibited the highest sensitivity near the focal region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18436v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santtu S\"oderholm, Maryam Samavaki, Sampsa Pursiainen</dc:creator>
    </item>
    <item>
      <title>Accelerating Conjugate Gradient Solvers for Homogenization Problems with Unitary Neural Operators</title>
      <link>https://arxiv.org/abs/2508.02681</link>
      <description>arXiv:2508.02681v2 Announce Type: replace 
Abstract: Rapid and reliable solvers for parametric partial differential equations (PDEs) are needed in many scientific and engineering disciplines. For example, there is a growing demand for composites and architected materials with heterogeneous microstructures. Designing such materials and predicting their behavior in practical applications requires solving homogenization problems for a wide range of material parameters and microstructures. While classical numerical solvers offer reliable and accurate solutions supported by a solid theoretical foundation, their high computational costs and slow convergence remain limiting factors. As a result, scientific machine learning is emerging as a promising alternative. However, such approaches often lack guaranteed accuracy and physical consistency. This raises the question of whether it is possible to develop hybrid approaches that combine the advantages of both data-driven methods and classical solvers. To address this, we introduce UNO-CG, a hybrid solver that accelerates conjugate gradient (CG) solvers using specially designed machine-learned preconditioners, while ensuring convergence by construction. As a preconditioner, we propose Unitary Neural Operators as a modification of Fourier Neural Operators. Our method can be interpreted as a data-driven discovery of Green's functions, which are then used to accelerate iterative solvers. We evaluate UNO-CG on various homogenization problems involving heterogeneous microstructures and millions of degrees of freedom. Our results demonstrate that UNO-CG enables a substantial reduction in the number of iterations and is competitive with handcrafted preconditioners for homogenization problems that involve expert knowledge. Moreover, UNO-CG maintains strong performance across a variety of boundary conditions, where many specialized solvers are not applicable, highlighting its versatility and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02681v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julius Herb, Felix Fritzen</dc:creator>
    </item>
    <item>
      <title>Computation of stresses in jammed packings modeled with Tresca friction</title>
      <link>https://arxiv.org/abs/2510.13021</link>
      <description>arXiv:2510.13021v2 Announce Type: replace 
Abstract: This paper is interested in the computation of stresses within jammed packings of rigid polygonal cells. The cells are considered to follow a Tresca friction law. First, a constrained minimization problem is introduced where the friction energy is minimized while enforcing the non-interpenetration of neighboring cells as inequality constraints. The corresponding dual maximization problem is then deduced and its solutions provide normal stresses at the interface between cells. Finally, lowest order Raviart-Thomas finite elements are used to reconstruct a consistent stress field by solving local problems. Numerical results are presented to showcase the consistency and robustness of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13021v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Marazzato, Shankar Venkataramani</dc:creator>
    </item>
    <item>
      <title>Numerical methods for solving PIDEs arising in swing option pricing under a two-factor mean-reverting model with jumps</title>
      <link>https://arxiv.org/abs/2511.01587</link>
      <description>arXiv:2511.01587v2 Announce Type: replace 
Abstract: This paper concerns the numerical valuation of swing options with discrete action times under a linear two-factor mean-reverting model with jumps. The resulting sequence of two-dimensional partial integro-differential equations (PIDEs) are convection-dominated and possess a nonlocal integral term due to the presence of jumps. Further, the initial function is nonsmooth. We propose various second-order numerical methods that can adequately handle these challenging features. The stability and convergence of these numerical methods are analysed theoretically. By ample numerical experiments, we confirm their second-order convergence behaviour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01587v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>q-fin.CP</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mustapha Regragui, Karel J. in 't Hout, Mich\`ele Vanmaele, Fred Espen Benth</dc:creator>
    </item>
    <item>
      <title>Sparse Source Identification in Transient Advection-Diffusion Problems with a Primal-Dual-Active-Point Strategy</title>
      <link>https://arxiv.org/abs/2511.02552</link>
      <description>arXiv:2511.02552v2 Announce Type: replace 
Abstract: This work presents a mathematical model to enable rapid prediction of airborne contaminant transport based on scarce sensor measurements. The method is designed for applications in critical infrastructure protection (CIP), such as evacuation planning following contaminant release. In such scenarios, timely and reliable decision-making is essential, despite limited observation data. To identify contaminant sources, we formulate an inverse problem governed by an advection-diffusion equation. Given the problem's underdetermined nature, we further employ a variational regularization ansatz and model the unknown contaminant sources as distribution over the spatial domain. To efficiently solve the arising inverse problem, we employ a problem-specific variant of the Primal-Dual-Active-Point (PDAP) algorithm which efficiently approximates sparse minimizers of the inverse problem by alternating between greedy location updates and source intensity optimization. The approach is demonstrated on two- and three-dimensional test cases involving both instantaneous and continuous contaminant sources and outperforms state-of-the-art techniques with $L^2$-regularization. Its effectiveness is further illustrated in complex domains with real-world building geometries imported from OpenStreetMap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02552v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Mattuschka, Daniel Walter, Max von Danwitz, Alexander Popp</dc:creator>
    </item>
    <item>
      <title>Error Analysis on a Novel Class of Exponential Integrators with Local Linear Extension Techniques for Highly Oscillatory ODEs</title>
      <link>https://arxiv.org/abs/2511.15104</link>
      <description>arXiv:2511.15104v2 Announce Type: replace 
Abstract: This paper investigates a class of non-autonomous highly oscillatory ordinary differential equations characterized by a linear component inversely proportional to a small parameter $\varepsilon$, with purely imaginary eigenvalues, and an $\varepsilon$-independent nonlinear part. When $0&lt;\varepsilon\ll 1$, the rapidly oscillatory nature of the solution imposes severe constraints on step size selection and numerical accuracy, leading to considerable computational difficulties. Inspired by a linearization technique that introduces auxiliary polynomial variables, a new family of explicit exponential integrators has recently been proposed. These methods do not require the linear part to be diagonal or to have eigenvalues that are integer multiples of a fixed value - a common assumption in multiscale approaches - and they achieve arbitrarily high orders of convergence without imposing order conditions. The main contribution of this work is to provide a rigorous error analysis for this new class of methods under a bounded oscillatory energy condition. To this end, we first establish the equivalence between the high-dimensional system and the original problem using algebraic techniques. Building on these foundational results, we prove that the numerical schemes, when employing auxiliary polynomial variables of degree $k$, achieve a uniform convergence order of $O(h^{k+1})$. In particular, an improved order of $O(\varepsilon h^k)$ is attained when $h$ is larger than the scale of $\varepsilon$. These theoretical findings are further applied to second-order oscillatory systems, leading to improved uniform accuracy with respect to $\varepsilon$. Finally, numerical experiments confirm the optimality of the derived error estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15104v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihao Qi, Weibing Deng, Fuhai Zhu</dc:creator>
    </item>
    <item>
      <title>Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation</title>
      <link>https://arxiv.org/abs/2511.15445</link>
      <description>arXiv:2511.15445v2 Announce Type: replace 
Abstract: Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15445v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Victorita Dolean, Daria Hrebenshchykova, St\'ephane Lanteri, Victor Michel-Dansac</dc:creator>
    </item>
    <item>
      <title>Multiword matrix multiplication over large finite fields in floating-point arithmetic</title>
      <link>https://arxiv.org/abs/2601.07508</link>
      <description>arXiv:2601.07508v2 Announce Type: replace 
Abstract: This article is concerned with the efficient computation of modular matrix multiplication C=AB mod p, a key kernel in computer algebra. We focus on floating-point arithmetic, which allows for using efficient matrix multiplication libraries. However, the existing approach is limited to primes p with bitsize at most half the mantissa size (e.g., 26 bits with double precision arithmetic), and becomes quite inefficient when p approaches this limit. We present a new approach that overcomes this limitation and can efficiently handle primes with larger bitsizes. The key idea is to use multiword decompositions, which represent A and B as scaled sums of u and v matrices (words) with smaller coefficients. We provide a rigorous analysis that proves the correctness of this approach for suitably chosen scaling parameters. Our analysis determines the maximum bitsize of p that can be handled for a given number of words; in particular, we show that decomposing in two words each input suffices to handle bitsizes almost equal to the full mantissa size (e.g., the 26 bits limit is raised to 52 bits in double precision arithmetic). Moreover, we show that (1,v) decompositions with v&gt;1 are also of interest to handle intermediate bitsizes. We perform an extensive experimental analysis for various matrix shapes and prime bitsizes. Our performance benchmarks on both CPU and GPU architectures confirm the efficiency of the proposed approach, which can outperform the existing single word approach for bitsizes as low as 23, and can handle bitsizes as high as 52 while retaining high performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07508v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'er\'emy Berthomieu (PolSys), Stef Graillat (PEQUAN), Dimitri Lesnoff (PEQUAN, PolSys), Theo Mary (PEQUAN)</dc:creator>
    </item>
    <item>
      <title>Reliable eigenspace error estimation using source error estimators</title>
      <link>https://arxiv.org/abs/2601.08051</link>
      <description>arXiv:2601.08051v2 Announce Type: replace 
Abstract: We introduce a framework for repurposing error estimators for source problems to compute an estimator for the gap between eigenspaces and their discretizations. Of interest are eigenspaces of finite clusters of eigenvalues of unbounded nonselfadjoint linear operators with compact resolvent. Eigenspaces and eigenvalues of rational functions of such operators are studied as a first step. Under an assumption of convergence of resolvent approximations in the operator norm and an assumption on global reliability of source problem error estimators, we show that the gap in eigenspace approximations can be bounded by a globally reliable and computable error estimator. Also included are applications of the theoretical framework to first-order system least squares (FOSLS) discretizations and discontinuous Petrov-Galerkin (DPG) discretizations, both yielding new estimators for the error gap. Numerical experiments with a selfadjoint model problem and with a leaky nonselfadjoint waveguide eigenproblem show that adaptive algorithms using the new estimators give refinement patterns that target the cluster as a whole instead of individual eigenfunctions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08051v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jay Gopalakrishnan, Gabriel Pinochet-Soto</dc:creator>
    </item>
    <item>
      <title>Numerical methods for diffusion coefficient recovery</title>
      <link>https://arxiv.org/abs/2602.01656</link>
      <description>arXiv:2602.01656v2 Announce Type: replace 
Abstract: We revisit the inverse problem of reconstructing a spatially varying diffusion coefficient in stationary elliptic equations from boundary Cauchy data. From a theoretical perspective, we introduce a gradient-weighted modification of the coupled complex-boundary method (CCBM) incorporating an \(H^1\)-type term, and formulate the reconstruction as a regularized optimization problem over bounded admissible coefficients. We establish continuity and differentiability of the forward map, Lipschitz continuity of the modified cost functional, existence of minimizers, stability with respect to noisy data, and convergence under vanishing noise. From a numerical perspective, reconstructions are computed using a Sobolev-gradient descent scheme and evaluated through extensive numerical experiments across a range of noise levels, boundary inputs, and coefficient structures. In the reported tests, for sufficiently large but not excessive $H^1$-weights, the modified CCBM is observed to yield more stable reconstructions and to reduce certain high-frequency artifacts. Across the numerical scenarios considered in this study, the method often demonstrates favorable stability and robustness properties relative to several classical boundary-based formulations, although performance remains problem- and parameter-dependent. A projection-based extension further supports stable recovery of piecewise-constant diffusion coefficients in multi-subregion test cases. Our results indicate that, as long as all subdomains share a portion of the boundary, the proposed CCBM-based Tikhonov regularization approach with a pick-a-point strategy enables stable and reliable reconstruction of diffusion parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01656v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahat Pandapotan Nainggolan, Julius Fergy Tiongson Rabago, Hirofumi Notsu</dc:creator>
    </item>
    <item>
      <title>Quantum Wave Atom Transforms</title>
      <link>https://arxiv.org/abs/2507.10739</link>
      <description>arXiv:2507.10739v2 Announce Type: replace-cross 
Abstract: This paper constructs the first quantum algorithm for wavelet packet transforms with a "parabolic scaling" tree structure, sometimes called wave atom transforms. Classically, wave atoms are used to construct sparse representations of differential operators, which enable fast numerical algorithms for partial differential equations. Compared to previous work, our quantum algorithm can implement a larger class of wavelet and wave atom transforms, by using an efficient representation for a larger class of possible tree structures. Our quantum implementation has O(poly(n)) gate complexity for applying a transform of dimension 2^n, while classical implementations use O(n*2^n) floating point operations. The result can be used to improve existing quantum algorithms for solving hyperbolic partial differential equations, such as wave equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10739v2</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marianna Podzorova, Yi-Kai Liu</dc:creator>
    </item>
    <item>
      <title>Inference-Time Alignment for Diffusion Models via Variationally Stable Doob's Matching</title>
      <link>https://arxiv.org/abs/2601.06514</link>
      <description>arXiv:2601.06514v2 Announce Type: replace-cross 
Abstract: Inference-time alignment for diffusion models aims to adapt a pre-trained reference diffusion model toward a target distribution without retraining the reference score network, thereby preserving the generative capacity of the reference model while enforcing desired properties at the inference time. A central mechanism for achieving such alignment is guidance, which modifies the sampling dynamics through an additional drift term. In this work, we introduce variationally stable Doob's matching, a novel framework for provable guidance estimation grounded in Doob's $h$-transform. Our approach formulates guidance as the gradient of logarithm of an underlying Doob's $h$-function and employs gradient-regularized regression to simultaneously estimate both the $h$-function and its gradient, resulting in a consistent estimator of the guidance. Theoretically, we establish non-asymptotic convergence rates for the estimated guidance. Moreover, we analyze the resulting controllable diffusion processes and prove non-asymptotic convergence guarantees for the generated distributions in the 2-Wasserstein distance. Finally, we show that variationally stable guidance estimators are adaptive to unknown low dimensionality, effectively mitigating the curse of dimensionality under low-dimensional subspace assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06514v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Chenguang Duan, Yuling Jiao, Yi Xu, Jerry Zhijian Yang</dc:creator>
    </item>
    <item>
      <title>Reducing acquisition time and radiation damage: data-driven subsampling for spectro-microscopy</title>
      <link>https://arxiv.org/abs/2602.03744</link>
      <description>arXiv:2602.03744v2 Announce Type: replace-cross 
Abstract: Spectro-microscopy is an experimental technique which can be used to observe spatial variations in chemical state and changes in chemical state over time or under experimental conditions. As a result it has broad applications across areas such as energy materials, catalysis, environmental science and biological samples. However, the technique is often limited by factors such as long acquisition times and radiation damage. We present two measurement strategies that allow for significantly shorter experiment times and total doses applied. The strategies are based on taking only a small subset of all the measurements (e.g. sparse acquisition or subsampling), and then computationally reconstructing all unobserved measurements using mathematical techniques. The methods are data-driven, using spectral and spatial importance subsampling distributions to identify important measurements. As a result, taking as little as 4-6\% of the measurements is sufficient to capture the same information as in a conventional scan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03744v2</guid>
      <category>physics.med-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.optics</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maike Meier, Lorenzo Lazzarino, Boris Shustin, Hussam Al Daas, Paul Quinn</dc:creator>
    </item>
  </channel>
</rss>
