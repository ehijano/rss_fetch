<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Aug 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>PAL: A Variability-Aware Policy for Scheduling ML Workloads in GPU Clusters</title>
      <link>https://arxiv.org/abs/2408.11919</link>
      <description>arXiv:2408.11919v1 Announce Type: new 
Abstract: Large-scale computing systems are increasingly using accelerators such as GPUs to enable peta- and exa-scale levels of compute to meet the needs of Machine Learning (ML) and scientific computing applications. Given the widespread and growing use of ML, including in some scientific applications, optimizing these clusters for ML workloads is particularly important. However, recent work has demonstrated that accelerators in these clusters can suffer from performance variability and this variability can lead to resource under-utilization and load imbalance. In this work we focus on how clusters schedulers, which are used to share accelerator-rich clusters across many concurrent ML jobs, can embrace performance variability to mitigate its effects. Our key insight to address this challenge is to characterize which applications are more likely to suffer from performance variability and take that into account while placing jobs on the cluster. We design a novel cluster scheduler, PAL, which uses performance variability measurements and application-specific profiles to improve job performance and resource utilization. PAL also balances performance variability with locality to ensure jobs are spread across as few nodes as possible. Overall, PAL significantly improves GPU-rich cluster scheduling: across traces for six ML workload applications spanning image, language, and vision models with a variety of variability profiles, PAL improves geomean job completion time by 42%, cluster utilization by 28%, and makespan by 47% over existing state-of-the-art schedulers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11919v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rutwik Jain, Brandon Tran, Keting Chen, Matthew D. Sinclair, Shivaram Venkataraman</dc:creator>
    </item>
    <item>
      <title>HoSZp: An Efficient Homomorphic Error-bounded Lossy Compressor for Scientific Data</title>
      <link>https://arxiv.org/abs/2408.11971</link>
      <description>arXiv:2408.11971v1 Announce Type: new 
Abstract: Error-bounded lossy compression has been a critical technique to significantly reduce the sheer amounts of simulation datasets for high-performance computing (HPC) scientific applications while effectively controlling the data distortion based on user-specified error bound. In many real-world use cases, users must perform computational operations on the compressed data (a.k.a. homomorphic compression). However, none of the existing error-bounded lossy compressors support the homomorphism, inevitably resulting in undesired decompression costs. In this paper, we propose a novel homomorphic error-bounded lossy compressor (called HoSZp), which supports not only error-bounding features but efficient computations (including negation, addition, multiplication, mean, variance, etc.) on the compressed data without the complete decompression step, which is the first attempt to the best of our knowledge. We develop several optimization strategies to maximize the overall compression ratio and execution performance. We evaluate HoSZp compared to other state-of-the-art lossy compressors based on multiple real-world scientific application datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11971v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tripti Agarwal, Sheng Di, Jiajun Huang, Yafan Huang, Ganesh Gopalakrishnan, Robert Underwood, Kai Zhao, Xin Liang, Guanpeng Li, Franck Cappello</dc:creator>
    </item>
    <item>
      <title>Distributed-Memory Parallel Algorithms for Sparse Matrix and Sparse Tall-and-Skinny Matrix Multiplication</title>
      <link>https://arxiv.org/abs/2408.11988</link>
      <description>arXiv:2408.11988v1 Announce Type: new 
Abstract: We consider a sparse matrix-matrix multiplication (SpGEMM) setting where one matrix is square and the other is tall and skinny. This special variant, called TS-SpGEMM, has important applications in multi-source breadth-first search, influence maximization, sparse graph embedding, and algebraic multigrid solvers. Unfortunately, popular distributed algorithms like sparse SUMMA deliver suboptimal performance for TS-SpGEMM. To address this limitation, we develop a novel distributed-memory algorithm tailored for TS-SpGEMM. Our approach employs customized 1D partitioning for all matrices involved and leverages sparsity-aware tiling for efficient data transfers. In addition, it minimizes communication overhead by incorporating both local and remote computations. On average, our TS-SpGEMM algorithm attains 5x performance gains over 2D and 3D SUMMA. Furthermore, we use our algorithm to implement multi-source breadth-first search and sparse graph embedding algorithms and demonstrate their scalability up to 512 Nodes (or 65,536 cores) on NERSC Perlmutter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11988v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isuru Ranawaka, Md Taufique Hussain, Charles Block, Gerasimos Gerogiannis, Josep Torrellas, Ariful Azad</dc:creator>
    </item>
    <item>
      <title>Time Optimal Distance-$k$-Dispersion on Dynamic Ring</title>
      <link>https://arxiv.org/abs/2408.12220</link>
      <description>arXiv:2408.12220v1 Announce Type: new 
Abstract: Dispersion by mobile agents is a well studied problem in the literature on computing by mobile robots. In this problem, $l$ robots placed arbitrarily on nodes of a network having $n$ nodes are asked to relocate themselves autonomously so that each node contains at most $\lfloor \frac{l}{n}\rfloor$ robots. When $l\le n$, then each node of the network contains at most one robot. Recently, in NETYS'23, Kaur et al. introduced a variant of dispersion called \emph{Distance-2-Dispersion}. In this problem, $l$ robots have to solve dispersion with an extra condition that no two adjacent nodes contain robots.
  In this work, we generalize the problem of Dispersion and Distance-2-Dispersion by introducing another variant called \emph{Distance-$k$-Dispersion (D-$k$-D)}. In this problem, the robots have to disperse on a network in such a way that shortest distance between any two pair of robots is at least $k$ and there exist at least one pair of robots for which the shortest distance is exactly $k$. Note that, when $k=1$ we have normal dispersion and when $k=2$ we have D-$2$-D. Here, we studied this variant for a dynamic ring (1-interval connected ring) for rooted initial configuration. We have proved the necessity of fully synchronous scheduler to solve this problem and provided an algorithm that solves D-$k$-D in $\Theta(n)$ rounds under a fully synchronous scheduler. So, the presented algorithm is time optimal too. To the best of our knowledge, this is the first work that considers this specific variant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12220v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brati Mondal, Pritam Goswami, Buddhadeb Sau</dc:creator>
    </item>
    <item>
      <title>KS+: Predicting Workflow Task Memory Usage Over Time</title>
      <link>https://arxiv.org/abs/2408.12290</link>
      <description>arXiv:2408.12290v1 Announce Type: new 
Abstract: Scientific workflow management systems enable the reproducible execution of data analysis pipelines on cluster infrastructures managed by resource managers such as Kubernetes, Slurm, or HTCondor. These resource managers require resource estimates for each workflow task to be executed on one of the cluster nodes. However, task resource consumption varies significantly between different tasks and for the same task with different inputs. Furthermore, resource consumption also fluctuates during a task's execution. As a result, manually configuring static memory allocations is error-prone, often leading users to overestimate memory usage to avoid costly failures from under-provisioning, which results in significant memory wastage.
  We propose KS+, a method that predicts a task's memory consumption over time depending on its inputs. For this, KS+ dynamically segments the task execution and predicts the memory required for each segment. Our experimental evaluation shows an average reduction in memory wastage of 38% compared to the best-performing state-of-the-art baseline for two real-world workflows from the popular nf-core repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12290v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Bader, Ansgar L\"o{\ss}er, Lauritz Thamsen, Bj\"orn Scheuermann, Odej Kao</dc:creator>
    </item>
    <item>
      <title>Stream parallel skeleton optimization</title>
      <link>https://arxiv.org/abs/2408.12394</link>
      <description>arXiv:2408.12394v1 Announce Type: new 
Abstract: We discuss the properties of the composition of stream parallel skeletons such as pipelines and farms. By looking at the ideal performance figures assumed to hold for these skeletons, we show that any stream parallel skeleton composition can always be rewritten into an equivalent "normal form" skeleton composition, delivering a service time which is equal or even better to the service time of the original skeleton composition, and achieving a better utilization of the processors used. The normal form is defined as a single farm built around a sequential worker code. Experimental results are discussed that validate this normal form.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12394v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Proc. of PDCS: Intl. Conference on Parallel and Distributed Computing and Systems, pages 955-962, Cambridge, Massachusetts, USA, Nov. 1999. IASTED, ACTA press</arxiv:journal_reference>
      <dc:creator>Marco Aldinucci, Marco Danelutto</dc:creator>
    </item>
    <item>
      <title>Poplar: Efficient Scaling of Distributed DNN Training on Heterogeneous GPU Clusters</title>
      <link>https://arxiv.org/abs/2408.12596</link>
      <description>arXiv:2408.12596v1 Announce Type: new 
Abstract: Scaling Deep Neural Networks (DNNs) requires significant computational resources in terms of GPU quantity and compute capacity. In practice, there usually exists a large number of heterogeneous GPU devices due to the rapid release cycle of GPU products. It is highly needed to efficiently and economically harness the power of heterogeneous GPUs, so that it can meet the requirements of DNN research and development. The paper introduces Poplar, a distributed training system that extends Zero Redundancy Optimizer (ZeRO) with heterogeneous-aware capabilities. We explore a broader spectrum of GPU heterogeneity, including compute capability, memory capacity, quantity and a combination of them. In order to achieve high computational efficiency across all heterogeneous conditions, Poplar conducts fine-grained measurements of GPUs in each ZeRO stage. We propose a novel batch allocation method and a search algorithm to optimize the utilization of heterogeneous GPUs clusters. Furthermore, Poplar implements fully automated parallelism, eliminating the need for deploying heterogeneous hardware and finding suitable batch size. Extensive experiments on three heterogeneous clusters, comprising six different types of GPUs, demonstrate that Poplar achieves a training throughput improvement of 1.02-3.92x over current state-of-the-art heterogeneous training systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12596v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>WenZheng Zhang, Yang Hu, Jing Shi, Xiaoying Bai</dc:creator>
    </item>
    <item>
      <title>Combinatorial Auctions without a Numeraire: The Case of Blockchain Trade-Intent Auctions</title>
      <link>https://arxiv.org/abs/2408.12225</link>
      <description>arXiv:2408.12225v1 Announce Type: cross 
Abstract: Blockchain trade intent auctions currently intermediate approximately USD 5 billion monthly. Due to production complementarities, the auction is combinatorial: when multiple trade intents from different traders are auctioned off simultaneously, a bidder (here called solver) can generate additional efficiencies by winning a batch of multiple trade intents. However, unlike other combinatorial auctions studied in the literature, the auction has no numeraire. Fairness is a concern as the efficiencies from batching cannot be easily shared between traders. We formalize this problem and study the most commonly used auction formats: batch auctions and multiple simultaneous auctions. We also propose a novel fair combinatorial auction that combines batch auction and multiple simultaneous auctions: solvers submit individual-trade bids and batched bids, but batched bids are considered only if they are better for all traders relative to the outcome of multiple simultaneous auctions (constructed using the individual-trade bids). We find a trade-off between the fairness guarantees provided by the auction (i.e., the minimum each trader can expect to receive) and the expected value of the assets returned to the traders. Also, the amount that each trader receives in the equilibrium of the fair combinatorial auction may be higher or lower than what they receive in the equilibrium of the simultaneous auctions used as a benchmark for fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12225v1</guid>
      <category>econ.TH</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrea Canidio, Felix Henneke</dc:creator>
    </item>
    <item>
      <title>Verifiable Homomorphic Linear Combinations in Multi-Instance Time-Lock Puzzles</title>
      <link>https://arxiv.org/abs/2408.12444</link>
      <description>arXiv:2408.12444v1 Announce Type: cross 
Abstract: Time-Lock Puzzles (TLPs) have been developed to securely transmit sensitive information into the future without relying on a trusted third party. Multi-instance TLP is a scalable variant of TLP that enables a server to efficiently find solutions to different puzzles provided by a client at once. Nevertheless, existing multi-instance TLPs lack support for (verifiable) homomorphic computation. To address this limitation, we introduce the "Multi-Instance partially Homomorphic TLP" (MH-TLP), a multi-instance TLP supporting efficient verifiable homomorphic linear combinations of puzzles belonging to a client. It ensures anyone can verify the correctness of computations and solutions. Building on MH-TLP, we further propose the "Multi-instance Multi-client verifiable partially Homomorphic TLP" (MMH-TLP). It not only supports all the features of MH-TLP but also allows for verifiable homomorphic linear combinations of puzzles from different clients. Our schemes refrain from using asymmetric-key cryptography for verification and, unlike most homomorphic TLPs, do not require a trusted third party. A comprehensive cost analysis demonstrates that our schemes scale linearly with the number of clients and puzzles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12444v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aydin Abadi</dc:creator>
    </item>
    <item>
      <title>Real-Time Video Generation with Pyramid Attention Broadcast</title>
      <link>https://arxiv.org/abs/2408.12588</link>
      <description>arXiv:2408.12588v1 Announce Type: cross 
Abstract: We present Pyramid Attention Broadcast (PAB), a real-time, high quality and training-free approach for DiT-based video generation. Our method is founded on the observation that attention difference in the diffusion process exhibits a U-shaped pattern, indicating significant redundancy. We mitigate this by broadcasting attention outputs to subsequent steps in a pyramid style. It applies different broadcast strategies to each attention based on their variance for best efficiency. We further introduce broadcast sequence parallel for more efficient distributed inference. PAB demonstrates superior results across three models compared to baselines, achieving real-time generation for up to 720p videos. We anticipate that our simple yet effective method will serve as a robust baseline and facilitate future research and application for video generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12588v1</guid>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanlei Zhao, Xiaolong Jin, Kai Wang, Yang You</dc:creator>
    </item>
    <item>
      <title>FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things</title>
      <link>https://arxiv.org/abs/2310.00109</link>
      <description>arXiv:2310.00109v3 Announce Type: replace-cross 
Abstract: There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most existing FL works do not use datasets collected from authentic IoT devices and thus do not capture unique modalities and inherent challenges of IoT data. To fill this critical gap, in this work, we introduce FedAIoT, an FL benchmark for AIoT. FedAIoT includes eight datasets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. FedAIoT also includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope FedAIoT could serve as an invaluable resource to foster advancements in the important field of FL for AIoT. The repository of FedAIoT is maintained at https://github.com/AIoT-MLSys-Lab/FedAIoT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00109v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.DL</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Samiul Alam, Tuo Zhang, Tiantian Feng, Hui Shen, Zhichao Cao, Dong Zhao, JeongGil Ko, Kiran Somasundaram, Shrikanth S. Narayanan, Salman Avestimehr, Mi Zhang</dc:creator>
    </item>
    <item>
      <title>Multi-Relational Algebra and Its Applications to Data Insights</title>
      <link>https://arxiv.org/abs/2311.04824</link>
      <description>arXiv:2311.04824v2 Announce Type: replace-cross 
Abstract: A range of data insight analytical tasks involves analyzing a large set of tables of different schemas, possibly induced by various groupings, to find salient patterns. In particular, such analyses are about many-to-many transformations of tables, while the classic relational algebra is about one-to-one or many-to-one transformations. This paper presents Multi-Relational Algebra, which extends relational algebra for such transformations and their compositions. Multi-Relational Algebra introduces MultiRelation to model of a set of tables of different schemas. Importantly, while the information unit in Relational Algebra is a tuple, the information unit in Multi-Relational Algebra is a slice, which formally is a pair $(r, X)$ where $r$ is a (region) tuple, and $X$ is a (feature) table. Multi-Relational Algebra introduces three new fundamental algebraic operators, MultiSelect, MultiProject, and MultiJoin, which lift their counterparts Select, Project, and Join to transform MultiRelation to MultiRelation. Through various examples, we show that multi-relational algebra can effortlessly express many complex analytic problems, some of which are traditionally considered out of scope for relational analytics. We have implemented and deployed a service for multi-relational analytics. Due to a unified logical design, we are able to conduct systematic optimization for a variety of seemingly different tasks. Our service has garnered interest from over a hundred internal teams who have developed data-insight applications using it, and serves millions of operators daily.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04824v2</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Wu, Zichen Zhu, Xiangyao Yu, Shaleen Deep, Stratis Viglas, John Cieslewicz, Somesh Jha, Jeffrey F. Naughton</dc:creator>
    </item>
    <item>
      <title>Personalized Federated Learning via ADMM with Moreau Envelope</title>
      <link>https://arxiv.org/abs/2311.06756</link>
      <description>arXiv:2311.06756v2 Announce Type: replace-cross 
Abstract: Personalized federated learning (PFL) is an approach proposed to address the issue of poor convergence on heterogeneous data. However, most existing PFL frameworks require strong assumptions for convergence. In this paper, we propose an alternating direction method of multipliers (ADMM) for training PFL models with Moreau envelope (FLAME), which achieves a sublinear convergence rate, relying on the relatively weak assumption of gradient Lipschitz continuity. Moreover, due to the gradient-free nature of ADMM, FLAME alleviates the need for hyperparameter tuning, particularly in avoiding the adjustment of the learning rate when training the global model. In addition, we propose a biased client selection strategy to expedite the convergence of training of PFL models. Our theoretical analysis establishes the global convergence under both unbiased and biased client selection strategies. Our experiments validate that FLAME, when trained on heterogeneous data, outperforms state-of-the-art methods in terms of model performance. Regarding communication efficiency, it exhibits an average speedup of 3.75x compared to the baselines. Furthermore, experimental results validate that the biased client selection strategy speeds up the convergence of both personalized and global models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06756v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shengkun Zhu, Jinshan Zeng, Sheng Wang, Yuan Sun, Zhiyong Peng</dc:creator>
    </item>
  </channel>
</rss>
