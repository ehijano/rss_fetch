<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.GN updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.GN</link>
    <description>econ.GN updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.GN" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Sep 2025 04:02:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Characterizing and Minimizing Divergent Delivery in Meta Advertising Experiments</title>
      <link>https://arxiv.org/abs/2508.21251</link>
      <description>arXiv:2508.21251v1 Announce Type: new 
Abstract: Many digital platforms offer advertisers experimentation tools like Meta's Lift and A/B tests to optimize their ad campaigns. Lift tests compare outcomes between users eligible to see ads versus users in a no-ad control group. In contrast, A/B tests compare users exposed to alternative ad configurations, absent any control group. The latter setup raises the prospect of divergent delivery: ad delivery algorithms may target different ad variants to different audience segments. This complicates causal interpretation because results may reflect both ad content effectiveness and changes to audience composition. We offer three key contributions. First, we make clear that divergent delivery is specific to A/B tests and intentional, informing advertisers about ad performance in practice. Second, we measure divergent delivery at scale, considering 3,204 Lift tests and 181,890 A/B tests. Lift tests show no meaningful audience imbalance, confirming their causal validity, while A/B tests show clear imbalance, as expected. Third, we demonstrate that campaign configuration choices can reduce divergent delivery in A/B tests, lessening algorithmic influence on results. While no configuration guarantees eliminating divergent delivery entirely, we offer evidence-based guidance for those seeking more generalizable insights about ad content in A/B tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21251v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gordon Burtch, Robert Moakler, Brett R. Gordon, Poppy Zhang, Shawndra Hill</dc:creator>
    </item>
    <item>
      <title>The Impact of a Raw Material Import Ban on Vertical Outward FDI: Theoretical Insights and Quasi-Experimental Evidence</title>
      <link>https://arxiv.org/abs/2508.21291</link>
      <description>arXiv:2508.21291v1 Announce Type: new 
Abstract: This paper examines how adverse supply-side shocks in domestic input markets influence firms' vertical outward foreign direct investment (OFDI) decisions. While the theoretical basis for cost-driven OFDI is well established, empirical evidence on the causal mechanisms remains limited. We develop a framework in which input cost shocks raise unit production costs, but firms undertake vertical OFDI only when shocks are sufficiently severe or when baseline costs are already high. Firm heterogeneity leads to a sorting pattern, whereby more productive firms are more likely to invest abroad. To test this mechanism, we exploit China's 2017 waste paper import ban as an exogenous shock and leverage a distinctive feature of the paper product industry's supply chain. Using a difference-in-differences strategy and firm-level data from 2000 to 2023, we find that the policy shock increased the probability of vertical OFDI by approximately 16% in the post-policy period relative to a control group. These results provide robust evidence that firms respond to domestic input shocks by reallocating production across borders, highlighting vertical OFDI as a strategic response to supply-side disruptions. The findings contribute to understanding the micro-foundations of global production decisions in the face of input market volatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21291v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sajid Anwar, Sizhong Sun</dc:creator>
    </item>
    <item>
      <title>EconAgentic in DePIN Markets: A Large Language Model Approach to the Sharing Economy of Decentralized Physical Infrastructure</title>
      <link>https://arxiv.org/abs/2508.21368</link>
      <description>arXiv:2508.21368v1 Announce Type: new 
Abstract: The Decentralized Physical Infrastructure (DePIN) market is revolutionizing the sharing economy through token-based economics and smart contracts that govern decentralized operations. By 2024, DePIN projects have exceeded \$10 billion in market capitalization, underscoring their rapid growth. However, the unregulated nature of these markets, coupled with the autonomous deployment of AI agents in smart contracts, introduces risks such as inefficiencies and potential misalignment with human values. To address these concerns, we introduce EconAgentic, a Large Language Model (LLM)-powered framework designed to mitigate these challenges. Our research focuses on three key areas: 1) modeling the dynamic evolution of DePIN markets, 2) evaluating stakeholders' actions and their economic impacts, and 3) analyzing macroeconomic indicators to align market outcomes with societal goals. Through EconAgentic, we simulate how AI agents respond to token incentives, invest in infrastructure, and adapt to market conditions, comparing AI-driven decisions with human heuristic benchmarks. Our results show that EconAgentic provides valuable insights into the efficiency, inclusion, and stability of DePIN markets, contributing to both academic understanding and practical improvements in the design and governance of decentralized, tokenized economies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21368v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulin Liu, Mocca Schweitzer</dc:creator>
    </item>
    <item>
      <title>Non-Take-Up of Unemployment Benefit II in Germany: A Longitudinal Perspective Using Administrative Data</title>
      <link>https://arxiv.org/abs/2508.21535</link>
      <description>arXiv:2508.21535v1 Announce Type: new 
Abstract: Extensive research demonstrates that many households eligible for means-tested benefits do not claim them, a phenomenon known as non-take-up. Empirical studies frequently conceptualise non-take-up as a rational decision, occurring when the perceived net utility of claiming is negative. Theoretically, long-term factors can substantially impact this decision. Despite the potential relevance of longitudinal aspects, evidence on their influence remains limited.
  This study addresses this gap by incorporating long-term factors in the analysis of non-take-up behaviour relating to Unemployment Benefit II (UB II), Germany's basic means-tested welfare programme. Using data from the German Panel Study Labour Market and Social Security (PASS) from 2008 to 2020, linked with administrative data from Germany's Federal Employment Agency (PASS-ADIAB), this study reconstructs households' benefit receipt and income histories, even during non-survey periods. This allows modelling benefit non-take-up for eligible households using the duration and frequency of past benefit receipt. In addition, the use of administrative data mitigates bias from self-reported benefit receipt. Household eligibility for UB II is simulated using GETTSIM, an open-source microsimulation model, applied to the PASS dataset for the first time.
  Findings indicate that long-term factors significantly influence the probability of claiming UB II. Specifically, a longer history of benefit receipt increases this probability, whereas higher income potential and positive income shocks reduce it. Including long-term factors substantially affects the estimated impact of traditionally used determinants of non-take-up, indicating a potential misspecification in existing models that neglect them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21535v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J\"urgen Wiemers</dc:creator>
    </item>
    <item>
      <title>Across Time and (Product) Space: A Capability-Centric Model of Relatedness and Economic Complexity</title>
      <link>https://arxiv.org/abs/2508.21616</link>
      <description>arXiv:2508.21616v1 Announce Type: new 
Abstract: Economic complexity - a group of dimensionality-reduction methods that apply network science to trade data - represented a paradigm shift in development economics towards materializing the once-intangible concept of capabilities as inferrable and quantifiable. Measures such as the Economic Complexity Index (ECI) and the Product Space have proven their worth as robust estimators of an economy's subsequent growth; less obvious, however, is how they have come to be so. Despite ECI drawing its micro-foundations from a combinatorial model of capabilities, where a set of homogeneous capabilities combine to form products and the economies which can produce them, such a model is consistent with neither the fact that distinct product classes draw on distinct capabilities, nor the interrelations between different products in the Product Space which so much of economic complexity is based upon.
  In this paper, we extend the combinatorial model of economic complexity through two innovations: an underlying network which governs the relatedness between capabilities, and a production function which trades the original binary specialization function for a fine-grained, product-level output function. Using country-product trade data across 216 countries, 5000 products and two decades, we show that this model is able to accurately replicate both the characteristic topology of the Product Space and the complexity distribution of countries' export baskets. In particular, the model bridges the gap between the ECI and capabilities by transforming measures of economic complexity into direct measures of the capabilities held by an economy - a transformation shown to both improve the informativeness of the Economic Complexity Index in predicting economic growth and enable an interpretation of economic complexity as a proxy for productive structure in the form of capability substitutability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21616v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Huang, Huashan Chen</dc:creator>
    </item>
    <item>
      <title>Evaluating Ethnic Income Gap in China: The Case of Han, Mongol, and Manchu in Liaoning and Inner Mongolia</title>
      <link>https://arxiv.org/abs/2508.21625</link>
      <description>arXiv:2508.21625v1 Announce Type: new 
Abstract: This study analyzes the 2018 Chinese Household Income Project survey data to evaluate the income gaps between an "outsider" ethnic minority group, the Mongols, an "insider" ethnic minority group, the Manchus, and the majority Han group in urban and rural areas of Liaoning province and Inner Mongolia in China. Three statistical methods, a simple first-order OLS linear regression, linear regressions with interaction terms, and the Blinder-Oaxaca Decomposition, are used to investigate the income disparity amongst the three groups. The results indicate that Mongols suffer a significant ethnic wage penalty attributable to possible discrimination in the rural areas of these two provinces, while the urban income gaps between the three groups can mostly be explained by participation in public sector occupations or affiliation with the Chinese Communist Party. In rural settings, Mongols also have higher returns to public sector jobs and CCP membership compared to the other two ethnic groups. The findings suggest that Chinese affirmative actions regarding ethnic policy are effective in accelerating the integration of ethnic minorities with Han in the outcomes of the labor market. This conclusion is consistent with previous studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21625v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyan Deng</dc:creator>
    </item>
    <item>
      <title>The properties of a Leontief production technology for Health System Modeling: the Thanzi la Onse model for Malawi</title>
      <link>https://arxiv.org/abs/2508.21699</link>
      <description>arXiv:2508.21699v1 Announce Type: new 
Abstract: As health system modeling (HSM) advances to include more complete descriptions of the production of healthcare, it is important to establish a robust conceptual characterisation of the production process. For the Thanzi La Onse model in Malawi we have incorporated an approach to production that is based on a form of Leontief technology -- fixed input proportions. At first sight, this form of technology appears restrictive relative to the general conception of a production function employed in economics. In particular, the Leontief technology is associated with constant returns to scale, and level sets that are piecewise linear, both of which are highly restrictive properties. In this article we demonstrate that once incorporated into an all disease, agent-based model these properties are no longer present and the Leontief framework becomes a rich structure for describing healthcare production, and hence for examining the returns to health systems investments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21699v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Chalkley, Sakshi Mohan, Margherita Molaro, Bingling She, Wiktoria Tafesse</dc:creator>
    </item>
    <item>
      <title>A Financial Brain Scan of the LLM</title>
      <link>https://arxiv.org/abs/2508.21285</link>
      <description>arXiv:2508.21285v1 Announce Type: cross 
Abstract: Emerging techniques in computer science make it possible to "brain scan" large language models (LLMs), identify the plain-English concepts that guide their reasoning, and steer them while holding other factors constant. We show that this approach can map LLM-generated economic forecasts to concepts such as sentiment, technical analysis, and timing, and compute their relative importance without reducing performance. We also show that models can be steered to be more or less risk-averse, optimistic, or pessimistic, which allows researchers to correct or simulate biases. The method is transparent, lightweight, and replicable for empirical research in the social sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21285v1</guid>
      <category>q-fin.GN</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hui Chen, Antoine Didisheim, Luciano Somoza, Hanqing Tian</dc:creator>
    </item>
    <item>
      <title>General Social Agents</title>
      <link>https://arxiv.org/abs/2508.17407</link>
      <description>arXiv:2508.17407v2 Announce Type: replace 
Abstract: Useful social science theories predict behavior across settings. However, applying a theory to make predictions in new settings is challenging: rarely can it be done without ad hoc modifications to account for setting-specific factors. We argue that AI agents put in simulations of those novel settings offer an alternative for applying theory, requiring minimal or no modifications. We present an approach for building such "general" agents that use theory-grounded natural language instructions, existing empirical data, and knowledge acquired by the underlying AI during training. To demonstrate the approach in settings where no data from that data-generating process exists--as is often the case in applied prediction problems--we design a highly heterogeneous population of 883,320 novel games. AI agents are constructed using human data from a small set of conceptually related, but structurally distinct "seed" games. In preregistered experiments, on average, agents predict human play better than (i) game-theoretic equilibria and (ii) out-of-the-box agents in a random sample of 1,500 games from the population. For a small set of separate novel games, these simulations predict responses from a new sample of human subjects better even than the most plausibly relevant published human data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17407v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin S. Manning, John J. Horton</dc:creator>
    </item>
    <item>
      <title>How Big Data Dilutes Cognitive Resources, Interferes with Rational Decision-making and Affects Wealth Distribution ?</title>
      <link>https://arxiv.org/abs/2508.20435</link>
      <description>arXiv:2508.20435v2 Announce Type: replace 
Abstract: Big data has exponentially dilated consumption demand and speed, but can they all be converted to utility? We argue about the measures of consumption and utility acquisition in CRRA utility function under the condition of big data interaction, we indicate its weakness, i.e., irrational consumption does not lead to the acquisition of utility. We consider that big data, which is different from macro and micro economic signals, formed by general information entropy, affects agents' rational cognition, which makes a part of their consumption ineffective. We preliminarily propose the theory that how dilution mechanism driven by big data will affect agents' cognitive resources. Based on theoretical and empirical analysis, we construct the Consumption Adjustment Weight Function (CAWF) of agents interacting with big data and further apply it to a model of firm wealth distribution with financial frictions, we get analytical solutions according to the Mean Field Game (MFG) and find: Lower financial friction increases the average wealth of firms but also leads to greater wealth inequality. When agents convert effective consumption into utility, which is a weight of total consumption, the average wealth of firms increases with the weight increasing. Meanwhile, wealth inequality follows a U-shaped trend, and it will be the lowest level when the weight approaches to 0.5. In conclusion, we try to provide a new complementary hypothesis to refine the 'Lucas Critique' according to the cognitive resources as endowments involved in the decision-making of agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20435v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongheng Hu</dc:creator>
    </item>
    <item>
      <title>Do more citations mean better patents?</title>
      <link>https://arxiv.org/abs/2508.20503</link>
      <description>arXiv:2508.20503v2 Announce Type: replace 
Abstract: This article reviews the empirical evidence on the use of patent citations as a proxy for invention importance. It distinguishes between technical merit, private economic value, and social value, and surveys validation studies using expert ratings, market data, renewal records, and compensation reports. The findings confirm that while the count of citations is positively associated with various dimensions of value, it remains a noisy indicator -- correlated but far from definitive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20503v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gaetan de Rassenfosse</dc:creator>
    </item>
  </channel>
</rss>
