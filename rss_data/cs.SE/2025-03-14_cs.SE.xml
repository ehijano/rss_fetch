<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Mar 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Prototype VS Code Extension to Improve Web Accessible Development</title>
      <link>https://arxiv.org/abs/2503.09673</link>
      <description>arXiv:2503.09673v1 Announce Type: new 
Abstract: Achieving web accessibility is essential to building inclusive digital experiences. However, accessibility issues are often identified only after a website has been fully developed, making them difficult to address. This paper introduces a Visual Studio Code plugin that integrates calls to a Large Language Model (LLM) to assist developers in identifying and resolving accessibility issues within the IDE, reducing accessibility defects that might otherwise reach the production environment. Our evaluation shows promising results: the plugin effectively generates functioning fixes for accessibility issues when the errors are correctly detected. However, detecting errors using a generic prompt-designed for broad applicability across various code structures-remains challenging and limited in accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09673v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Elisa Cal\`i, Tommaso Fulcini, Riccardo Coppola, Lorenzo Laudadio, Marco Torchiano</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Multi-Vocal Empirical Study of ML Cloud Service Misuses</title>
      <link>https://arxiv.org/abs/2503.09815</link>
      <description>arXiv:2503.09815v1 Announce Type: new 
Abstract: Machine Learning (ML) models are widely used across various domains, including medical diagnostics and autonomous driving. To support this growth, cloud providers offer ML services to ease the integration of ML components in software systems. The evolving business requirements and the popularity of ML services have led practitioners of all skill levels to implement, and maintain ML service-based systems. However, they may not always adhere to optimal design and usage practices for ML cloud services, resulting in common misuse which could significantly degrade the quality of ML service-based systems and adversely affect their maintenance and evolution. Though much research has been conducted on ML service misuse, a consistent terminology and specification for these misuses remain absent. We therefore conduct in this paper a comprehensive, multi-vocal empirical study exploring the prevalence of ML cloud service misuses in practice. We propose a catalog of 20 ML cloud service misuses, most of which have not been studied in prior research. To achieve this, we conducted a) a systematic literature review of studies on ML misuses, b) a gray literature review of the official documentation provided by major cloud providers, c) an empirical analysis of a curated set of 377 ML service-based systems on GitHub, and d) a survey with 50 ML practitioners. Our results show that ML service misuses are common in both open-source projects and industry, often stemming from a lack of understanding of service capabilities, and insufficient documentation. This emphasizes the importance of ongoing education in best practices for ML services, which is the focus of this paper, while also highlighting the need for tools to automatically detect and refactor ML misuses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09815v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hadil Ben Amor, Manel Abdellatif, Taher Ghaleb</dc:creator>
    </item>
    <item>
      <title>NumScout: Unveiling Numerical Defects in Smart Contracts using LLM-Pruning Symbolic Execution</title>
      <link>https://arxiv.org/abs/2503.10041</link>
      <description>arXiv:2503.10041v1 Announce Type: new 
Abstract: In recent years, the Ethereum platform has witnessed a proliferation of smart contracts, accompanied by exponential growth in total value locked (TVL). High-TVL smart contracts often require complex numerical computations, particularly in mathematical financial models used by many decentralized applications (DApps). Improper calculations can introduce numerical defects, posing potential security risks. Existing research primarily focuses on traditional numerical defects like integer overflow, and there is currently a lack of systematic research and effective detection methods targeting new types of numerical defects. In this paper, we identify five new types of numerical defects through the analysis of 1,199 audit reports by utilizing the open card method. Each defect is defined and illustrated with a code example to highlight its features and potential consequences. We also propose NumScout, a symbolic execution-based tool designed to detect these five defects. Specifically, the tool combines information from source code and bytecode, analyzing key operations such as comparisons and transfers, to effectively locate defects and report them based on predefined detection patterns. Furthermore, NumScout uses a large language model (LLM) to prune functions which are unrelated to numerical operations. This step allows symbolic execution to quickly enter the target function and improve runtime speed by 28.4%. We run NumScout on 6,617 real-world contracts and evaluated its performance based on manually labeled results. We find that 1,774 contracts contained at least one of the five defects, and the tool achieved an overall precision of 89.7%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10041v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiachi Chen, Zhenzhe Shao, Shuo Yang, Yiming Shen, Yanlin Wang, Ting Chen, Zhenyu Shan, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>"Silent Is Not Actually Silent": An Investigation of Toxicity on Bug Report Discussion</title>
      <link>https://arxiv.org/abs/2503.10072</link>
      <description>arXiv:2503.10072v1 Announce Type: new 
Abstract: Toxicity in bug report discussions poses significant challenges to the collaborative dynamics of open-source software development. Bug reports are crucial for identifying and resolving defects, yet their inherently problem-focused nature and emotionally charged context make them susceptible to toxic interactions. This study explores toxicity in GitHub bug reports through a qualitative analysis of 203 bug threads, including 81 toxic ones. Our findings reveal that toxicity frequently arises from misaligned perceptions of bug severity and priority, unresolved frustrations with tools, and lapses in professional communication. These toxic interactions not only derail productive discussions but also reduce the likelihood of actionable outcomes, such as linking issues with pull requests. Our preliminary findings offer actionable recommendations to improve bug resolution by mitigating toxicity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10072v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Conference on the Foundations of Software Engineering: Ideas, Visions and Reflections track - 2025</arxiv:journal_reference>
      <dc:creator>Mia Mohammad Imran, Jaydeb Sarker</dc:creator>
    </item>
    <item>
      <title>AgentDAO: Synthesis of Proposal Transactions Via Abstract DAO Semantics</title>
      <link>https://arxiv.org/abs/2503.10099</link>
      <description>arXiv:2503.10099v1 Announce Type: new 
Abstract: While the trend of decentralized governance is obvious (cryptocurrencies and blockchains are widely adopted by multiple sovereign countries), initiating governance proposals within Decentralized Autonomous Organizations (DAOs) is still challenging, i.e., it requires providing a low-level transaction payload, therefore posing significant barriers to broad community participation. To address these challenges, we propose a multi-agent system powered by Large Language Models with a novel Label-Centric Retrieval algorithm to automate the translation from natural language inputs into executable proposal transactions. The system incorporates DAOLang, a Domain-Specific Language to simplify the specification of various governance proposals. The key optimization achieved by DAOLang is a semantic-aware abstraction of user input that reliably secures proposal generation with a low level of token demand. A preliminary evaluation on real-world applications reflects the potential of DAOLang in terms of generating complicated types of proposals with existing foundation models, e.g. GPT-4o.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10099v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Ao, Han Liu, Huafeng Zhang</dc:creator>
    </item>
    <item>
      <title>Red Teaming Contemporary AI Models: Insights from Spanish and Basque Perspectives</title>
      <link>https://arxiv.org/abs/2503.10192</link>
      <description>arXiv:2503.10192v1 Announce Type: new 
Abstract: The battle for AI leadership is on, with OpenAI in the United States and DeepSeek in China as key contenders. In response to these global trends, the Spanish government has proposed ALIA, a public and transparent AI infrastructure incorporating small language models designed to support Spanish and co-official languages such as Basque. This paper presents the results of Red Teaming sessions, where ten participants applied their expertise and creativity to manually test three of the latest models from these initiatives$\unicode{x2013}$OpenAI o3-mini, DeepSeek R1, and ALIA Salamandra$\unicode{x2013}$focusing on biases and safety concerns. The results, based on 670 conversations, revealed vulnerabilities in all the models under test, with biased or unsafe responses ranging from 29.5% in o3-mini to 50.6% in Salamandra. These findings underscore the persistent challenges in developing reliable and trustworthy AI systems, particularly those intended to support Spanish and Basque languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10192v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miguel Romero-Arjona, Pablo Valle, Juan C. Alonso, Ana B. S\'anchez, Miriam Ugarte, Antonia Cazalilla, Vicente Cambr\'on, Jos\'e A. Parejo, Aitor Arrieta, Sergio Segura</dc:creator>
    </item>
    <item>
      <title>HyperSeq: A Hyper-Adaptive Representation for Predictive Sequencing of States</title>
      <link>https://arxiv.org/abs/2503.10254</link>
      <description>arXiv:2503.10254v1 Announce Type: new 
Abstract: In the rapidly evolving world of software development, the surge in developers' reliance on AI-driven tools has transformed Integrated Development Environments into powerhouses of advanced features. This transformation, while boosting developers' productivity to unprecedented levels, comes with a catch: increased hardware demands for software development. Moreover, the significant economic and environmental toll of using these sophisticated models necessitates mechanisms that reduce unnecessary computational burdens. We propose HyperSeq - Hyper-Adaptive Representation for Predictive Sequencing of States - a novel, resource-efficient approach designed to model developers' cognitive states. HyperSeq facilitates precise action sequencing and enables real-time learning of user behavior. Our preliminary results show how HyperSeq excels in forecasting action sequences and achieves remarkable prediction accuracies that go beyond 70%. Notably, the model's online-learning capability allows it to substantially enhance its predictive accuracy in a majority of cases and increases its capability in forecasting next user actions with sufficient iterations for adaptation. Ultimately, our objective is to harness these predictions to refine and elevate the user experience dynamically within the IDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10254v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Roham Koohestani, Maliheh Izadi</dc:creator>
    </item>
    <item>
      <title>Test Amplification for REST APIs Using "Out-of-the-box" Large Language Models</title>
      <link>https://arxiv.org/abs/2503.10306</link>
      <description>arXiv:2503.10306v1 Announce Type: new 
Abstract: REST APIs are an indispensable building block in today's cloud-native applications, so testing them is critically important. However, writing automated tests for such REST APIs is challenging because one needs strong and readable tests that exercise the boundary values of the protocol embedded in the REST API. In this paper, we report our experience with using "out of the box" large language models (ChatGPT and GitHub's Copilot) to amplify REST API test suites. We compare the resulting tests based on coverage and understandability, and we derive a series of guidelines and lessons learned concerning the prompts that result in the strongest test suite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10306v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tolgahan Bardakci, Serge Demeyer, Mutlu Beyazit</dc:creator>
    </item>
    <item>
      <title>Capturing Semantic Flow of ML-based Systems</title>
      <link>https://arxiv.org/abs/2503.10310</link>
      <description>arXiv:2503.10310v1 Announce Type: new 
Abstract: ML-based systems are software systems that incorporates machine learning components such as Deep Neural Networks (DNNs) or Large Language Models (LLMs). While such systems enable advanced features such as high performance computer vision, natural language processing, and code generation, their internal behaviour remain largely opaque to traditional dynamic analysis such as testing: existing analysis typically concern only what is observable from the outside, such as input similarity or class label changes. We propose semantic flow, a concept designed to capture the internal behaviour of ML-based system and to provide a platform for traditional dynamic analysis techniques to be adapted to. Semantic flow combines the idea of control flow with internal states taken from executions of ML-based systems, such as activation values of a specific layer in a DNN, or embeddings of LLM responses at a specific inference step of LLM agents. The resulting representation, summarised as semantic flow graphs, can capture internal decisions that are not explicitly represented in the traditional control flow of ML-based systems. We propose the idea of semantic flow, introduce two examples using a DNN and an LLM agent, and finally sketch its properties and how it can be used to adapt existing dynamic analysis techniques for use in ML-based software systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10310v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shin Yoo, Robert Feldt, Somin Kim, Naryeong Kim</dc:creator>
    </item>
    <item>
      <title>An Architectural View Type for Elasticity Modeling and Simulation -- The Slingshot Approach</title>
      <link>https://arxiv.org/abs/2503.10407</link>
      <description>arXiv:2503.10407v1 Announce Type: new 
Abstract: The cloud computing model enables the on-demand provisioning of computing resources, reducing manual management, increasing efficiency, and improving environmental impact. Software architects now play a strategic role in designing and deploying elasticity policies for automated resource management. However, creating policies that meet performance and cost objectives is complex. Existing approaches, often relying on formal models like Queueing Theory, require advanced skills and lack specific methods for representing elasticity within architectural models. This paper introduces an architectural view type for modeling and simulating elasticity, supported by the Scaling Policy Definition (SPD) modeling language, a visual notation, and precise simulation semantics. The view type is integrated into the Palladio ecosystem, providing both conceptual and tool-based support. We evaluate the approach through two single-case experiments and a user study. In the first experiment, simulations of elasticity policies demonstrate sufficient accuracy when compared to load tests, showing the utility of simulations for evaluating elasticity. The second experiment confirms feasibility for larger applications, though with increased simulation times. The user study shows that participants completed 90% of tasks, rated the usability at 71%, and achieved an average score of 76% in nearly half the allocated time. However, the empirical evidence suggests that modeling with this architectural view requires more time than modeling control flow, resource environments, or usage profiles, despite its benefits for elasticity policy design and evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10407v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Floriment Klinaku, Sarah Sophie Stie{\ss}, Alireza Hakamian, Steffen Becker</dc:creator>
    </item>
    <item>
      <title>BYOS: Knowledge-driven Large Language Models Bring Your Own Operating System More Excellent</title>
      <link>https://arxiv.org/abs/2503.09663</link>
      <description>arXiv:2503.09663v1 Announce Type: cross 
Abstract: Kernel configurations play an important role in the performance of Operating System (OS). However, with the rapid iteration of OS, finding the proper configurations that meet specific requirements can be challenging, which can be primarily attributed to the default kernel provided by vendors does not take the requirements of specific workloads into account, and the heavyweight tuning process cannot catch up with the rapid evolving pace of the kernel. To address these challenges, we propose BYOS, a novel framework powered by Large Language Models (LLMs) to customize kernel configurations for diverse user requirements. By integrating OS-oriented Dual-layer Knowledge Graph (OD-KG) and corresponding reasoning strategy, BYOS enhanced the LLM's understanding of the characteristics and capabilities of OS, thus enabling customized, cost-effective, and convenient generation of kernel configurations. Experiments show that the kernels configured by BYOS outperform the default vendor-configured kernels by 7.1% to 155.4%, demonstrating the effectiveness and efficiency of BYOS in customizing kernel configurations. Our code is available at https://github.com/LHY-24/BYOS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09663v1</guid>
      <category>cs.OS</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyu Lin, Yuchen Li, Haoran Luo, Kaichun Yao, Libo Zhang, Mingjie Xing, Yanjun Wu</dc:creator>
    </item>
    <item>
      <title>CITADEL: Context Similarity Based Deep Learning Framework Bug Finding</title>
      <link>https://arxiv.org/abs/2406.12196</link>
      <description>arXiv:2406.12196v4 Announce Type: replace 
Abstract: With deep learning (DL) technology becoming an integral part of the new intelligent software, tools of DL framework testing and bug-finding are in high demand. Existing DL framework testing tools have limited coverage on bug types. For example, they lack the capability of finding performance bugs, which are critical for DL model training and inference regarding performance, economics, and the environment. This problem is challenging due to the difficulty of getting test oracles of performance bugs. Moreover, existing tools are inefficient, generating hundreds of test cases with few trigger bugs. In this paper, we propose Citadel, a method that accelerates the finding of bugs in terms of efficiency and effectiveness. We observe that many DL framework bugs are similar due to the similarity of operators and algorithms belonging to the same family (e.g., Conv2D and Conv3D). Orthogonal to existing bug-finding tools, Citadel aims to find new bugs that are similar to reported ones that have known test oracles. It works by first collecting existing bug reports and identifying problematic APIs. Citadel defines context similarity to measure the similarity of DL framework API pairs and automatically generates test cases with oracles for APIs that are similar to the problematic APIs in existing bug reports. Citadel respectively covers 1,436 PyTorch and 5,380 TensorFlow APIs and effectively detects 77 and 74 API bugs, many of which, e.g., 11 performance bugs, cannot be detected by existing tools. Moreover, a remarkable 35.40% of the test cases generated by Citadel can trigger bugs, which significantly transcends the state-of-the-art method (3.90%).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12196v4</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Shiwei Wang, Chao Shen</dc:creator>
    </item>
    <item>
      <title>CleanVul: Automatic Function-Level Vulnerability Detection in Code Commits Using LLM Heuristics</title>
      <link>https://arxiv.org/abs/2411.17274</link>
      <description>arXiv:2411.17274v4 Announce Type: replace 
Abstract: Accurate identification of software vulnerabilities is crucial for system integrity. Vulnerability datasets, often derived from the National Vulnerability Database (NVD) or directly from GitHub, are essential for training machine learning models to detect these security flaws. However, these datasets frequently suffer from significant noise, typically 40% to 75%, due primarily to the automatic and indiscriminate labeling of all changes in vulnerability-fixing commits (VFCs) as vulnerability-related. This misclassification occurs because not all changes in a commit aimed at fixing vulnerabilities pertain to security threats; many are routine updates like bug fixes or test improvements.
  This paper introduces the first methodology that uses the Large Language Model (LLM) with a heuristic enhancement to automatically identify vulnerability-fixing changes from VFCs, achieving an F1-score of 0.82. VulSifter was applied to a large-scale study, where we conducted a crawl of 127,063 repositories on GitHub, resulting in the acquisition of 5,352,105 commits. VulSifter involves utilizing an LLM to comprehend code semantics and contextual information, while applying heuristics to filter out unrelated changes. We then developed CleanVul, a high-quality dataset comprising 8,203 functions using our LLM heuristic enhancement approach, demonstrating Correctness (90.6%) comparable to established datasets such as SVEN and PrimeVul.
  To evaluate the CleanVul dataset, we conducted experiments focusing on fine-tuning various LLMs on CleanVul and other high-quality datasets. Evaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit enhanced accuracy but also superior generalization capabilities compared to those trained on uncleaned datasets. Specifically, models trained on CleanVul and tested on PrimeVul achieve accuracy higher than those trained and tested exclusively on PrimeVul.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17274v4</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yikun Li, Ting Zhang, Ratnadira Widyasari, Yan Naing Tun, Huu Hung Nguyen, Tan Bui, Ivana Clairine Irsan, Yiran Cheng, Xiang Lan, Han Wei Ang, Frank Liauw, Martin Weyssow, Hong Jin Kang, Eng Lieh Ouh, Lwin Khin Shar, David Lo</dc:creator>
    </item>
    <item>
      <title>Predictive Prompt Analysis</title>
      <link>https://arxiv.org/abs/2501.18883</link>
      <description>arXiv:2501.18883v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are machine learning models that have seen widespread adoption due to their capability of handling previously difficult tasks. LLMs, due to their training, are sensitive to how exactly a question is presented, also known as prompting. However, prompting well is challenging, as it has been difficult to uncover principles behind prompting -- generally, trial-and-error is the most common way of improving prompts, despite its significant computational cost. In this context, we argue it would be useful to perform `predictive prompt analysis', in which an automated technique would perform a quick analysis of a prompt and predict how the LLM would react to it, relative to a goal provided by the user. As a demonstration of the concept, we present Syntactic Prevalence Analyzer (SPA), a predictive prompt analysis approach based on sparse autoencoders (SAEs). SPA accurately predicted how often an LLM would generate target syntactic structures during code synthesis, with up to 0.994 Pearson correlation between the predicted and actual prevalence of the target structure. At the same time, SPA requires only 0.4\% of the time it takes to run the LLM on a benchmark. As LLMs are increasingly used during and integrated into modern software development, our proposed predictive prompt analysis concept has the potential to significantly ease the use of LLMs for both practitioners and researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18883v2</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jae Yong Lee, Sungmin Kang, Shin Yoo</dc:creator>
    </item>
    <item>
      <title>Understanding and Predicting Derailment in Toxic Conversations on GitHub</title>
      <link>https://arxiv.org/abs/2503.02191</link>
      <description>arXiv:2503.02191v2 Announce Type: replace 
Abstract: Software projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub.
  To facilitate this research, we curate a novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as a baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants.
  Leveraging these empirical observations, we propose a proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop a conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 70% F1-Score in predicting conversational derailment, strongly improving over a set of baseline approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02191v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mia Mohammad Imran, Robert Zita, Rebekah Copeland, Preetha Chatterjee, Rahat Rizvi Rahman, Kostadin Damevski</dc:creator>
    </item>
    <item>
      <title>A Framework for Supporting the Reproducibility of Computational Experiments in Multiple Scientific Domains</title>
      <link>https://arxiv.org/abs/2503.07080</link>
      <description>arXiv:2503.07080v3 Announce Type: replace 
Abstract: In recent years, the research community, but also the general public, has raised serious questions about the reproducibility and replicability of scientific work. Since many studies include some kind of computational work, these issues are also a technological challenge, not only in computer science, but also in most research domains. Computational replicability and reproducibility are not easy to achieve due to the variety of computational environments that can be used. Indeed, it is challenging to recreate the same environment via the same frameworks, code, programming languages, dependencies, and so on. We propose a framework, known as SciRep, that supports the configuration, execution, and packaging of computational experiments by defining their code, data, programming languages, dependencies, databases, and commands to be executed. After the initial configuration, the experiments can be executed any number of times, always producing exactly the same results. Our approach allows the creation of a reproducibility package for experiments from multiple scientific fields, from medicine to computer science, which can be re-executed on any computer. The produced package acts as a capsule, holding absolutely everything necessary to re-execute the experiment. To evaluate our framework, we compare it with three state-of-the-art tools and use it to reproduce 18 experiments extracted from published scientific articles. With our approach, we were able to execute 16 (89%) of those experiments, while the others reached only 61%, thus showing that our approach is effective. Moreover, all the experiments that were executed produced the results presented in the original publication. Thus, SciRep was able to reproduce 100% of the experiments it could run.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07080v3</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'azaro Costa, Susana Barbosa, J\'acome Cunha</dc:creator>
    </item>
    <item>
      <title>PCLA: A Framework for Testing Autonomous Agents in the CARLA Simulator</title>
      <link>https://arxiv.org/abs/2503.09385</link>
      <description>arXiv:2503.09385v2 Announce Type: replace 
Abstract: Recent research on testing autonomous driving agents has grown significantly, especially in simulation environments. The CARLA simulator is often the preferred choice, and the autonomous agents from the CARLA Leaderboard challenge are regarded as the best-performing agents within this environment. However, researchers who test these agents, rather than training their own ones from scratch, often face challenges in utilizing them within customized test environments and scenarios. To address these challenges, we introduce PCLA (Pretrained CARLA Leaderboard Agents), an open-source Python testing framework that includes nine high-performing pre-trained autonomous agents from the Leaderboard challenges. PCLA is the first infrastructure specifically designed for testing various autonomous agents in arbitrary CARLA environments/scenarios. PCLA provides a simple way to deploy Leaderboard agents onto a vehicle without relying on the Leaderboard codebase, it allows researchers to easily switch between agents without requiring modifications to CARLA versions or programming environments, and it is fully compatible with the latest version of CARLA while remaining independent of the Leaderboard's specific CARLA version. PCLA is publicly accessible at https://github.com/MasoudJTehrani/PCLA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09385v2</guid>
      <category>cs.SE</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masoud Jamshidiyan Tehrani, Jinhan Kim, Paolo Tonella</dc:creator>
    </item>
    <item>
      <title>MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks</title>
      <link>https://arxiv.org/abs/2312.15960</link>
      <description>arXiv:2312.15960v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems. We observe that conventional models often generate solutions as monolithic code blocks, restricting their effectiveness in tackling intricate questions. To overcome this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a pioneering framework for MoT instruction tuning, designed to promote the decomposition of tasks into logical sub-tasks and sub-modules. Our investigations reveal that, through the cultivation and utilization of sub-modules, MoTCoder significantly improves both the modularity and correctness of the generated solutions, leading to substantial relative pass@1 improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are available at https://github.com/dvlab-research/MoTCoder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15960v4</guid>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyao Li, Pengguang Chen, Bin Xia, Hong Xu, Jiaya Jia</dc:creator>
    </item>
    <item>
      <title>VITAMIN: A Compositional Framework for Model Checking of Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2403.02170</link>
      <description>arXiv:2403.02170v3 Announce Type: replace-cross 
Abstract: The verification of Multi-Agent Systems (MAS) poses a significant challenge. Various approaches and methodologies exist to address this challenge; however, tools that support them are not always readily available. Even when such tools are accessible, they tend to be hard-coded, lacking in compositionality, and challenging to use due to a steep learning curve. In this paper, we introduce a methodology designed for the formal verification of MAS in a modular and versatile manner, along with an initial prototype, that we named VITAMIN. Unlike existing verification methodologies and frameworks for MAS, VITAMIN is constructed for easy extension to accommodate various logics (for specifying the properties to verify) and models (for determining on what to verify such properties).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02170v3</guid>
      <category>cs.MA</category>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angelo Ferrando, Vadim Malvone</dc:creator>
    </item>
    <item>
      <title>Assessing Vulnerability in Smart Contracts: The Role of Code Complexity Metrics in Security Analysis</title>
      <link>https://arxiv.org/abs/2411.17343</link>
      <description>arXiv:2411.17343v3 Announce Type: replace-cross 
Abstract: Codes with specific characteristics are more exposed to security vulnerabilities. Studies have revealed that codes that do not adhere to best practices are more challenging to verify and maintain, increasing the likelihood of unnoticed or unintentionally introduced vulnerabilities. Given the crucial role of smart contracts in blockchain systems, ensuring their security and conducting thorough vulnerability analysis is critical. This study investigates the use of code complexity metrics as indicators of vulnerable code in Solidity smart contracts. We highlight the significance of complexity metrics as valuable complementary features for vulnerability assessment and provide insights into the individual power of each metric. By analyzing 21 complexity metrics, we explored their interrelation, association with vulnerability, discriminative power, and mean values in vulnerable versus neutral codes. The results revealed some high correlations and potential redundancies among certain metrics, but weak correlations between each independent metric and vulnerability. Nevertheless, we found that all metrics can effectively discriminate between vulnerable and neutral codes, and most complexity metrics, except for three, exhibited higher values in vulnerable codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17343v3</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masoud Jamshidiyan Tehrani, Sattar Hashemi</dc:creator>
    </item>
    <item>
      <title>Can LLMs Reason About Program Semantics? A Comprehensive Evaluation of LLMs on Formal Specification Inference</title>
      <link>https://arxiv.org/abs/2503.04779</link>
      <description>arXiv:2503.04779v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly being used to automate programming tasks. Yet, LLMs' capabilities in reasoning about program semantics are still inadequately studied, leaving significant potential for further exploration. This paper introduces FormalBench, a comprehensive benchmark designed to evaluate LLMs' reasoning abilities on program semantics, particularly via the task of synthesizing formal program specifications to assist verifying program correctness. This task requires both comprehensive reasoning over all possible program executions and the generation of precise, syntactically correct expressions that adhere to formal syntax and semantics. Using this benchmark, we evaluated the ability of LLMs in synthesizing consistent and complete specifications. Our findings show that LLMs perform well with simple control flows but struggle with more complex structures, especially loops, even with advanced prompting. Additionally, LLMs exhibit limited robustness against semantic-preserving transformations. We also highlight common failure patterns and design self-repair prompts, improving success rates by 25%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04779v2</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thanh Le-Cong, Bach Le, Toby Murray</dc:creator>
    </item>
  </channel>
</rss>
