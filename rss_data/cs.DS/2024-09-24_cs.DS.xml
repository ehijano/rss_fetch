<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Polynomial Kernel for Deletion to the Scattered Class of Cliques and Trees</title>
      <link>https://arxiv.org/abs/2409.14209</link>
      <description>arXiv:2409.14209v1 Announce Type: new 
Abstract: The class of graph deletion problems has been extensively studied in theoretical computer science, particularly in the field of parameterized complexity. Recently, a new notion of graph deletion problems was introduced, called deletion to scattered graph classes, where after deletion, each connected component of the graph should belong to at least one of the given graph classes. While fixed-parameter algorithms were given for a wide variety of problems, little progress has been made on the kernelization complexity of any of them. In this paper, we present the first non-trivial polynomial kernel for one such deletion problem, where, after deletion, each connected component should be a clique or a tree - that is, as densest as possible or as sparsest as possible (while being connected). We develop a kernel consisting of O(k^5) vertices for this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14209v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashwin Jacob, Diptapriyo Majumdar, Meirav Zehavi</dc:creator>
    </item>
    <item>
      <title>Extending the Extension: Deterministic Algorithm for Non-monotone Submodular Maximization</title>
      <link>https://arxiv.org/abs/2409.14325</link>
      <description>arXiv:2409.14325v1 Announce Type: new 
Abstract: Maximization of submodular functions under various constraints is a fundamental problem that has been studied extensively. A powerful technique that has emerged and has been shown to be extremely effective for such problems is the following. First, a continues relaxation of the problem is obtained by relaxing the (discrete) set of feasible solutions to a convex body, and extending the discrete submodular function $f$ to a continuous function $F$ known as the multilinear extension. Then, two algorithmic steps are implemented. The first step approximately solves the relaxation by finding a fractional solution within the convex body that approximately maximizes $F$; and the second step rounds this fractional solution to a feasible integral solution. While this ``fractionally solve and then round'' approach has been a key technique for resolving many questions in the field, the main drawback of algorithms based on it is that evaluating the multilinear extension may require a number of value oracle queries to $f$ that is exponential in the size of $f$'s ground set. The only known way to tackle this issue is to approximate the value of $F$ via sampling, which makes all algorithms based on this approach inherently randomized and quite slow.
  In this work, we introduce a new tool, that we refer to as the extended multilinear extension, designed to derandomize submodular maximization algorithms that are based on the successful ``solve fractionally and then round'' approach. We demonstrate the effectiveness of this new tool on the fundamental problem of maximizing a submodular function subject to a matroid constraint, and show that it allows for a deterministic implementation of both the fractionally solving step and the rounding step of the above approach. As a bonus, we also get a randomized algorithm for the problem with an improved query complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14325v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niv Buchbinder, Moran Feldman</dc:creator>
    </item>
    <item>
      <title>A High-Performance External Validity Index for Clustering with a Large Number of Clusters</title>
      <link>https://arxiv.org/abs/2409.14455</link>
      <description>arXiv:2409.14455v1 Announce Type: new 
Abstract: This paper introduces the Stable Matching Based Pairing (SMBP) algorithm, a high-performance external validity index for clustering evaluation in large-scale datasets with a large number of clusters. SMBP leverages the stable matching framework to pair clusters across different clustering methods, significantly reducing computational complexity to $O(N^2)$, compared to traditional Maximum Weighted Matching (MWM) with $O(N^3)$ complexity. Through comprehensive evaluations on real-world and synthetic datasets, SMBP demonstrates comparable accuracy to MWM and superior computational efficiency. It is particularly effective for balanced, unbalanced, and large-scale datasets with a large number of clusters, making it a scalable and practical solution for modern clustering tasks. Additionally, SMBP is easily implementable within machine learning frameworks like PyTorch and TensorFlow, offering a robust tool for big data applications. The algorithm is validated through extensive experiments, showcasing its potential as a powerful alternative to existing methods such as Maximum Match Measure (MMM) and Centroid Ratio (CR).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14455v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Yasin Karbasian, Ramin Javadi</dc:creator>
    </item>
    <item>
      <title>Computing String Covers in Sublinear Time</title>
      <link>https://arxiv.org/abs/2409.14559</link>
      <description>arXiv:2409.14559v1 Announce Type: new 
Abstract: Let $T$ be a string of length $n$ over an integer alphabet of size $\sigma$. In the word RAM model, $T$ can be represented in $O(n /\log_\sigma n)$ space. We show that a representation of all covers of $T$ can be computed in the optimal $O(n/\log_\sigma n)$ time; in particular, the shortest cover can be computed within this time. We also design an $O(n(\log\sigma + \log \log n)/\log n)$-sized data structure that computes in $O(1)$ time any element of the so-called (shortest) cover array of $T$, that is, the length of the shortest cover of any given prefix of $T$. As a by-product, we describe the structure of cover arrays of Fibonacci strings. On the negative side, we show that the shortest cover of a length-$n$ string cannot be computed using $o(n/\log n)$ operations in the PILLAR model of Charalampopoulos, Kociumaka, and Wellnitz (FOCS 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14559v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-72200-4</arxiv:DOI>
      <arxiv:journal_reference>String Processing and Information Retrieval. SPIRE 2024. Lecture Notes in Computer Science, vol. 14899. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Jakub Radoszewski, Wiktor Zuba</dc:creator>
    </item>
    <item>
      <title>Substring Compression Variations and LZ78-Derivates</title>
      <link>https://arxiv.org/abs/2409.14649</link>
      <description>arXiv:2409.14649v1 Announce Type: new 
Abstract: We propose algorithms computing the semi-greedy Lempel-Ziv 78 (LZ78), the Lempel-Ziv Double (LZD), and the Lempel-Ziv-Miller-Wegman (LZMW) factorizations in linear time for integer alphabets. For LZD and LZMW, we additionally propose data structures that can be constructed in linear time, which can solve the substring compression problems for these factorizations in time linear in the output size. For substring compression, we give results for lexparse and closed factorizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14649v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/DCC58796.2024.00021</arxiv:DOI>
      <arxiv:journal_reference>full version of conference paper published at DCC 2024</arxiv:journal_reference>
      <dc:creator>Dominik K\"oppl</dc:creator>
    </item>
    <item>
      <title>Fast and Small Subsampled R-indexes</title>
      <link>https://arxiv.org/abs/2409.14654</link>
      <description>arXiv:2409.14654v1 Announce Type: new 
Abstract: The $r$-index represented a breakthrough in compressed indexing of repetitive text collections, outperforming its alternatives by orders of magnitude in query time. Its space usage, $O(r)$ where $r$ is the number of runs in the Burrows--Wheeler Transform of the text, is however higher than Lempel--Ziv (LZ) and grammar-based indexes, and makes it uninteresting in various real-life scenarios of milder repetitiveness. We introduce the $sr$-index, a variant that limits the space to $O(\min(r,n/s))$ for a text of length $n$ and a given parameter $s$, at the expense of multiplying by $s$ the time per occurrence reported. The $sr$-index is obtained subsampling the text positions indexed by the $r$-index, being still able to support pattern matching with guaranteed performance. Our experiments show that the theoretical analysis falls short in describing the practical advantages of the $sr$-index, because it performs much better on real texts than on synthetic ones: the $sr$-index retains the performance of the $r$-index while using 1.5--4.0 times less space, sharply outperforming {\em virtually every other} compressed index on repetitive texts in both time and space. Only a particular LZ-based index uses less space than the $sr$-index, but it is an order of magnitude slower.
  Our second contribution are the $r$-csa and $sr$-csa indexes. Just like the $r$-index adapts the well-known FM-Index to repetitive texts, the $r$-csa adapts Sadakane's Compressed Suffix Array (CSA) to this case. We show that the principles used on the $r$-index turn out to fit naturally and efficiently in the CSA framework. The $sr$-csa is the corresponding subsampled version of the $r$-csa. While the CSA performs better than the FM-Index on classic texts with alphabets larger than DNA, we show that the $sr$-csa outperforms the $sr$-index on repetitive texts over those larger alphabets and some DNA texts as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14654v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dustin Cobas, Travis Gagie, Gonzalo Navarro</dc:creator>
    </item>
    <item>
      <title>Gabow's Cardinality Matching Algorithm in General Graphs: Implementation and Experiments</title>
      <link>https://arxiv.org/abs/2409.14849</link>
      <description>arXiv:2409.14849v1 Announce Type: new 
Abstract: It is known since 1975 (\cite{HK75}) that maximum cardinality matchings in bipartite graphs with $n$ nodes and $m$ edges can be computed in time $O(\sqrt{n} m)$. Asymptotically faster algorithms were found in the last decade and maximum cardinality bipartite matchings can now be computed in near-linear time~\cite{NearlyLinearTimeBipartiteMatching, AlmostLinearTimeMaxFlow,AlmostLinearTimeMinCostFlow}. For general graphs, the problem seems harder. Algorithms with running time $O(\sqrt{n} m)$ were given in~\cite{MV80,Vazirani94,Vazirani12,Vazirani20,Vazirani23,Goldberg-Karzanov,GT91,Gabow:GeneralMatching}. Mattingly and Ritchey~\cite{Mattingly-Ritchey} and Huang and Stein~\cite{Huang-Stein} discuss implementations of the Micali-Vazirani Algorithm. We describe an implementation of Gabow's algorithm~\cite{Gabow:GeneralMatching} in C++ based on LEDA~\cite{LEDAsystem,LEDAbook} and report on running time experiments. On worst-case graphs, the asymptotic improvement pays off dramatically. On random graphs, there is no improvement with respect to algorithms that have a worst-case running time of $O(n m)$. The performance seems to be near-linear. The implementation is available open-source.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14849v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matin Ansaripour, Alireza Danaei, Kurt Mehlhorn</dc:creator>
    </item>
    <item>
      <title>Bounded indegree $k$-forests problem and a faster algorithm for directed graph augmentation</title>
      <link>https://arxiv.org/abs/2409.14881</link>
      <description>arXiv:2409.14881v1 Announce Type: new 
Abstract: We consider two problems for a directed graph $G$, which we show to be closely related. The first one is to find $k$ edge-disjoint forests in $G$ of maximal size such that the indegree of each vertex in these forests is at most $k$. We describe a min-max characterization for this problem and show that it can be solved in $O(k \delta m \log n)$ time, where $(n,m)$ is the size of $G$ and $\delta$ is the difference between $k$ and the edge connectivity of the graph. The second problem is the directed edge-connectivity augmentation problem, which has been extensively studied before: find a smallest set of directed edges whose addition to the graph makes it strongly $k$-connected. We improve the complexity for this problem from $O(k \delta (m+\delta n)\log n)$ [Gabow, STOC 1994] to $O(k \delta m \log n)$, by exploiting our solution for the first problem. A similar approach with the same complexity also works for the undirected version of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14881v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavel Arkhipov, Vladimir Kolmogorov</dc:creator>
    </item>
    <item>
      <title>Reducing concept lattices by means of a weaker notion of congruence</title>
      <link>https://arxiv.org/abs/2409.14915</link>
      <description>arXiv:2409.14915v1 Announce Type: new 
Abstract: Attribute and size reductions are key issues in formal concept analysis. In this paper, we consider a special kind of equivalence relation to reduce concept lattices, which will be called local congruence. This equivalence relation is based on the notion of congruence on lattices, with the goal of losing as less information as possible and being suitable for the reduction of concept lattices. We analyze how the equivalence classes obtained from a local congruence can be ordered. Moreover, different properties related to the algebraic structure of the whole set of local congruences are also presented. Finally, a procedure to reduce concept lattices by the new weaker notion of congruence is introduced. This procedure can be applied to the classical and fuzzy formal concept analysis frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14915v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.fss.2020.09.013</arxiv:DOI>
      <arxiv:journal_reference>Fuzzy Sets and Systems, 418 (2021) 153-169</arxiv:journal_reference>
      <dc:creator>Roberto G. Arag\'on, Jes\'us Medina, Elo\'isa Ram\'irez-Poussa</dc:creator>
    </item>
    <item>
      <title>Impact of local congruences in variable selection from datasets</title>
      <link>https://arxiv.org/abs/2409.14931</link>
      <description>arXiv:2409.14931v1 Announce Type: new 
Abstract: Formal concept analysis (FCA) is a useful mathematical tool for obtaining information from relational datasets. One of the most interesting research goals in FCA is the selection of the most representative variables of the dataset, which is called attribute reduction. Recently, the attribute reduction mechanism has been complemented with the use of local congruences in order to obtain robust clusters of concepts, which form convex sublattices of the original concept lattice. Since the application of such local congruences modifies the quotient set associated with the attribute reduction, it is fundamental to know how the original context (attributes, objects and relationship) has been modified in order to understand the impact of the application of the local congruence in the attribute reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14931v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cam.2021.113416</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational and Applied Mathematics, 404(113416) (2022)</arxiv:journal_reference>
      <dc:creator>Roberto G. Arag\'on, Jes\'us Medina, Elo\'isa Ram\'irez-Poussa</dc:creator>
    </item>
    <item>
      <title>Dynamic Pricing Algorithms for Online Set Cover</title>
      <link>https://arxiv.org/abs/2409.15094</link>
      <description>arXiv:2409.15094v1 Announce Type: new 
Abstract: We consider dynamic pricing algorithms as applied to the online set cover problem. In the dynamic pricing framework, we assume the standard client server model with the additional constraint that the server can only place prices over the resources they maintain, rather than authoritatively assign them. In response, incoming clients choose the resource which minimizes their disutility when taking into account these additional prices. Our main contributions are the categorization of online algorithms which can be mimicked via dynamic pricing algorithms and the identification of a strongly competitive deterministic algorithm with respect to the frequency parameter of the online set cover input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15094v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Bender, Aum Desai, Jialin He, Oliver Thompson, Pramithas Upreti</dc:creator>
    </item>
    <item>
      <title>Fast and Accurate Triangle Counting in Graph Streams Using Predictions</title>
      <link>https://arxiv.org/abs/2409.15205</link>
      <description>arXiv:2409.15205v1 Announce Type: new 
Abstract: In this work, we present the first efficient and practical algorithm for estimating the number of triangles in a graph stream using predictions. Our algorithm combines waiting room sampling and reservoir sampling with a predictor for the heaviness of edges, that is, the number of triangles in which an edge is involved. As a result, our algorithm is fast, provides guarantees on the amount of memory used, and exploits the additional information provided by the predictor to produce highly accurate estimates. We also propose a simple and domain-independent predictor, based on the degree of nodes, that can be easily computed with one pass on a stream of edges when the stream is available beforehand.
  Our analytical results show that, when the predictor provides useful information on the heaviness of edges, it leads to estimates with reduced variance compared to the state-of-the-art, even when the predictions are far from perfect. Our experimental results show that, when analyzing a single graph stream, our algorithm is faster than the state-of-the-art for a given memory budget, while providing significantly more accurate estimates. Even more interestingly, when sequences of hundreds of graph streams are analyzed, our algorithm significantly outperforms the state-of-the-art using our simple degree-based predictor built by analyzing only the first graph of the sequence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15205v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cristian Boldrin, Fabio Vandin</dc:creator>
    </item>
    <item>
      <title>An average case efficient algorithm for solving two variable linear diophantine equations</title>
      <link>https://arxiv.org/abs/2409.14052</link>
      <description>arXiv:2409.14052v1 Announce Type: cross 
Abstract: Solving two variable linear diophantine equations has applications in many cryptographic protocols such as RSA and Elliptic curve cryptography. Extended euclid's algorithm is the most widely used algorithm to solve these equations. We revisit two algorithms to solve two variable linear diophantine equations. For one of them, we do fine-grained analysis of the number of recursive calls and find a periodic function, which represents the number of recursive calls. We find the period and use it to derive an accurate closed form expression for the average number of recursive calls incurred by that algorithm. In the process of this derivation we get an upper bound on the average number of recursive calls, which depends on the intermediate values observed during the execution of algorithm. We propose an iterative version of the algorithm. While implementation of our algorithm, we verify a well known result from number theory about the probability of two random integers being coprime. Due to that result, our algorithm encounters an additional constraint for approximately 40% times. On almost all of these constrained inputs i.e. on nearly 100 % of them the algorithm outperforms two existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14052v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>math.NT</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mayank Deora, Pinakpani Pal</dc:creator>
    </item>
    <item>
      <title>Fast Local Search Strategies for Large-Scale General Quadratic Integer Programming</title>
      <link>https://arxiv.org/abs/2409.14176</link>
      <description>arXiv:2409.14176v1 Announce Type: cross 
Abstract: This study investigates the area of general quadratic integer programming (QIP), encompassing both unconstrained (UQIP) and constrained (CQIP) variants. These NP-hard problems have far-reaching applications, yet the non-convex cases have received limited attention in the literature. To address this gap, we introduce a closed-form formula for single-variable changes, establishing novel necessary and sufficient conditions for 1-Opt local improvement in UQIP and CQIP. We develop a simple local and sophisticated tabu search with an oscillation strategy tailored for large-scale problems. Experimental results on instances with up to 8000 variables demonstrate the efficiency of these strategies, producing high-quality solutions within a short time. Our approaches significantly outperform the Gurobi 11.0.2 solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14176v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haibo Wang, Bahram Alidaee</dc:creator>
    </item>
    <item>
      <title>Structural Results for High-Multiplicity Scheduling on Uniform Machines</title>
      <link>https://arxiv.org/abs/2203.01741</link>
      <description>arXiv:2203.01741v2 Announce Type: replace 
Abstract: Parameterizing by the largest processing time $p_{max}$ and the number of different job processing times $d$, we propose a proximity technique for High-Multiplicity Scheduling on Uniform Machines for the objectives Makespan Minimization ($C_{max}$) and Santa Claus ($C_{min}$) to obtain new structural results for these problems. The novelty in our approach is that we deal with a fractional solution for only a sub-instance, where the sub-instance itself is not known a priori. While the construction and computation of the fractional solution -- in contrast to usual proximity techniques -- is not done in polynomial time, this also allows us to formulate a comparably strong and general proximity statement. Eventually, this allows us to reduce the number of jobs that need to be distributed to a polynomial in $p_{max}$ for each machine and job type, by preassigning jobs according to the fractional solution, essentially returning a bounded number (at most $O(p_{max}^{O(d^2)})$) of kernels, one for each (guessed) sub-instance.
  We can use our structural results to obtain an algorithm with running time is $p_{max}^{O(d^2)}poly|I|$, matching the best-known so far by Knop et al. (Oper. Res. Lett. '21).
  Moreover, we propose an $p_{max}^{O(d^2)} poly |I|$ time algorithm for Envy Minimization $C_{envy}$ in the High-Multiplicity Setting on Uniform Machines, showing that this problem is \textsc{fpt} in $p_{max}$.
  Eventually, we also propose a general mechanism to bound the largest coefficient in the Configuration ILP for so called \emph{Load Balancing Problems} by $(dp_{max})^{O(d)}$, which we hope to be of interest for the development of algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.01741v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hauke Brinkop, David Fischer, Klaus Jansen</dc:creator>
    </item>
    <item>
      <title>Balancing Notions of Equity: Trade-offs Between Fair Portfolio Sizes and Achievable Guarantees</title>
      <link>https://arxiv.org/abs/2311.03230</link>
      <description>arXiv:2311.03230v2 Announce Type: replace 
Abstract: Motivated by fairness concerns, we study the `portfolio problem': given an optimization problem with set $D$ of feasible solutions, a class $\mathbf{C}$ of fairness objective functions on $D$, and an approximation factor $\alpha \ge 1$, a set $X \subseteq D$ of feasible solutions is an $\alpha$-approximate portfolio if for each objective $f \in \mathbf{C}$, there is an $\alpha$-approximation for $f$ in $X$. Choosing the classes of top-$k$ norms, ordered norms, and symmetric monotonic norms as our equity objectives, we study the trade-off between the size $|X|$ of the portfolio and its approximation factor $\alpha$ for various combinatorial problems. For the problem of scheduling identical jobs on unidentical machines, we characterize this trade-off for ordered norms and give an exponential improvement in size for symmetric monotonic norms over the general upper bound. We generalize this result as the OrderAndCount framework that obtains an exponential improvement in portfolio sizes for covering polyhedra with a constant number of constraints. Our framework is based on a novel primal-dual counting technique that may be of independent interest. We also introduce a general IterativeOrdering framework for simultaneous approximations or portfolios of size $1$ for symmetric monotonic norms, which generalizes and extends existing results for problems such as scheduling, $k$-clustering, set cover, and routing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03230v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Swati Gupta, Jai Moondra, Mohit Singh</dc:creator>
    </item>
    <item>
      <title>Connectivity Oracles for Predictable Vertex Failures</title>
      <link>https://arxiv.org/abs/2312.08489</link>
      <description>arXiv:2312.08489v4 Announce Type: replace 
Abstract: The problem of designing connectivity oracles supporting vertex failures is one of the basic data structures problems for undirected graphs. It is already well understood: previous works [Duan--Pettie STOC'10; Long--Saranurak FOCS'22] achieve query time linear in the number of failed vertices, and it is conditionally optimal as long as we require preprocessing time polynomial in the size of the graph and update time polynomial in the number of failed vertices.
  We revisit this problem in the paradigm of algorithms with predictions: we ask if the query time can be improved if the set of failed vertices can be predicted beforehand up to a small number of errors. More specifically, we design a data structure that, given a graph $G=(V,E)$ and a set of vertices predicted to fail $\widehat{D} \subseteq V$ of size $d=|\widehat{D}|$, preprocesses it in time $\tilde{O}(d|E|)$ and then can receive an update given as the symmetric difference between the predicted and the actual set of failed vertices $\widehat{D} \triangle D = (\widehat{D} \setminus D) \cup (D \setminus \widehat{D})$ of size $\eta = |\widehat{D} \triangle D|$, process it in time $\tilde{O}(\eta^4)$, and after that answer connectivity queries in $G \setminus D$ in time $O(\eta)$. Viewed from another perspective, our data structure provides an improvement over the state of the art for the \emph{fully dynamic subgraph connectivity problem} in the \emph{sensitivity setting} [Henzinger--Neumann ESA'16].
  We argue that the preprocessing time and query time of our data structure are conditionally optimal under standard fine-grained complexity assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08489v4</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingbing Hu, Evangelos Kosinas, Adam Polak</dc:creator>
    </item>
    <item>
      <title>Practical algorithms for Hierarchical overlap graphs</title>
      <link>https://arxiv.org/abs/2402.13920</link>
      <description>arXiv:2402.13920v3 Announce Type: replace 
Abstract: Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings. Classically, genome assembly uses assembly graphs built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss. The scalable de Bruijn graphs come at the price of losing crucial overlap information. The complete overlap information is stored in overlap graphs using quadratic space. Hierarchical overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space. After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms [CPM2021], where only the former was seemingly practical.
  We empirically analyze all the algorithms for computing Hierarchical overlap graphs, where the optimal algorithm~[CPM2021] outperforms the previous algorithms as expected. However, it is still based on relatively complex arguments for its formal proof and uses relatively complex data structures for its implementation. We present an intuitive algorithm, using only elementary arrays, which is also optimal. Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice.
  We further explore the applications of hierarchical overlap graphs to solve variants of suffix-prefix queries on a set of strings, recently studied by Loukides et al.~[CPM2023]. They presented state-of-the-art algorithms requiring complex black-box data structures, making them seemingly impractical. Our algorithms, despite failing to match their theoretical bounds, answer queries in $0.002$-$100~ms$ for datasets having around a billion characters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13920v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saumya Talera, Parth Bansal, Shabnam Khan, Shahbaz Khan</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic Matching and Ordered Ruzsa-Szemer\'edi Graphs</title>
      <link>https://arxiv.org/abs/2404.06069</link>
      <description>arXiv:2404.06069v3 Announce Type: replace 
Abstract: We study the fully dynamic maximum matching problem. In this problem, the goal is to efficiently maintain an approximate maximum matching of a graph that is subject to edge insertions and deletions. Our focus is on algorithms that maintain the edges of a $(1-\epsilon)$-approximate maximum matching for an arbitrarily small constant $\epsilon &gt; 0$. Until recently, the fastest known algorithm for this problem required $\Theta(n)$ time per update where $n$ is the number of vertices. This bound was slightly improved to $n/(\log^* n)^{\Omega(1)}$ by Assadi, Behnezhad, Khanna, and Li [STOC'23] and very recently to $n/2^{\Omega(\sqrt{\log n})}$ by Liu [FOCS'24]. Whether this can be improved to $n^{1-\Omega(1)}$ remains a major open problem. In this paper, we introduce {\em Ordered Ruzsa-Szemer\'edi (ORS)} graphs (a generalization of Ruzsa-Szemer\'edi graphs) and show that the complexity of dynamic matching is closely tied to them. For $\delta &gt; 0$, define $ORS(\delta n)$ to be the maximum number of matchings $M_1, \ldots, M_t$, each of size $\delta n$, that one can pack in an $n$-vertex graph such that each matching $M_i$ is an {\em induced matching} in subgraph $M_1 \cup \ldots \cup M_{i}$. We show that there is a randomized algorithm that maintains a $(1-\epsilon)$-approximate maximum matching of a fully dynamic graph in $$
  \widetilde{O}\left( \sqrt{n^{1+\epsilon} \cdot ORS(\Theta_\epsilon(n))} \right) $$ amortized update-time. While the value of $ORS(\Theta(n))$ remains unknown and is only upper bounded by $n^{1-o(1)}$, the densest construction known from more than two decades ago only achieves $ORS(\Theta(n)) \geq n^{1/\Theta(\log \log n)} = n^{o(1)}$ [Fischer et al. STOC'02]. If this is close to the right bound, then our algorithm achieves an update-time of $\sqrt{n^{1+O(\epsilon)}}$, resolving the aforementioned longstanding open problem in dynamic algorithms in a strong sense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06069v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soheil Behnezhad, Alma Ghafari</dc:creator>
    </item>
    <item>
      <title>Naively Sorting Evolving Data is Optimal and Robust</title>
      <link>https://arxiv.org/abs/2404.08162</link>
      <description>arXiv:2404.08162v3 Announce Type: replace 
Abstract: We study sorting in the evolving data model, introduced by [AKMU11], where the true total order changes while the sorting algorithm is processing the input. More precisely, each comparison operation of the algorithm is followed by a sequence of evolution steps, where an evolution step perturbs the rank of a random item by a "small" random value. The goal is to maintain an ordering that remains close to the true order over time. Previous works have analyzed adaptations of classic sorting algorithms, assuming that an evolution step changes the rank of an item by just one, and that a fixed constant number $b$ of evolution steps take place between two comparisons. In fact, the only previous result achieving optimal linear total deviation, by [BvDEGJ18a], applies just for $b=1$.
  We analyze a very simple sorting algorithm suggested by [M14], which samples a random pair of adjacent items in each step and swaps them if they are out of order. We show that the algorithm achieves and maintains, with high probability, optimal total deviation, $O(n)$, and optimal maximum deviation, $O(\log n)$, under very general model settings. Namely, the perturbation introduced by each evolution step is sampled from a general distribution of bounded moment generating function, and we just require that the average number of evolution steps between two sorting steps be bounded by an (arbitrary) constant, where the average is over a linear number of steps.
  The key ingredients of our proof are a novel potential function argument that inserts "gaps" in the list of items, and a general analysis framework which separates the analysis of sorting from that of the evolution steps, and is applicable to a variety of settings for which previous approaches do not apply. Our results settle conjectures and open problems in the aforementioned works, and provide theoretical support for empirical observations in [BvDEGJ18b].</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08162v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>In Proceedings of the 65th IEEE Annual Symposium on Foundations of Computer Science (FOCS 2024)</arxiv:journal_reference>
      <dc:creator>George Giakkoupis, Marcos Kiwi, Dimitrios Los</dc:creator>
    </item>
    <item>
      <title>Improved linearly ordered colorings of hypergraphs via SDP rounding</title>
      <link>https://arxiv.org/abs/2405.00427</link>
      <description>arXiv:2405.00427v2 Announce Type: replace 
Abstract: We consider the problem of linearly ordered (LO) coloring of hypergraphs. A hypergraph has an LO coloring if there is a vertex coloring, using a set of ordered colors, so that (i) no edge is monochromatic, and (ii) each edge has a unique maximum color. It is an open question as to whether or not a 2-LO colorable 3-uniform hypergraph can be LO colored with 3 colors in polynomial time. Nakajima and \v{Z}ivn\'{y} recently gave a polynomial-time algorithm to color such hypergraphs with $\widetilde{O}(n^{1/3})$ colors and asked if SDP methods can be used directly to obtain improved bounds. Our main result is to show how to use SDP-based rounding methods to produce an LO coloring with $\widetilde{O}(n^{1/5})$ colors for such hypergraphs. We show how to reduce the problem to cases with highly structured SDP solutions, which we call balanced hypergraphs. Then, we discuss how to apply classic SDP-rounding tools to obtain improved bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00427v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anand Louis, Alantha Newman, Arka Ray</dc:creator>
    </item>
    <item>
      <title>Structured Downsampling for Fast, Memory-efficient Curation of Online Data Streams</title>
      <link>https://arxiv.org/abs/2409.06199</link>
      <description>arXiv:2409.06199v3 Announce Type: replace 
Abstract: Operations over data streams typically hinge on efficient mechanisms to aggregate or summarize history on a rolling basis. For high-volume data steams, it is critical to manage state in a manner that is fast and memory efficient -- particularly in resource-constrained or real-time contexts. Here, we address the problem of extracting a fixed-capacity, rolling subsample from a data stream. Specifically, we explore ``data stream curation'' strategies to fulfill requirements on the composition of sample time points retained. Our ``DStream'' suite of algorithms targets three temporal coverage criteria: (1) steady coverage, where retained samples should spread evenly across elapsed data stream history; (2) stretched coverage, where early data items should be proportionally favored; and (3) tilted coverage, where recent data items should be proportionally favored. For each algorithm, we prove worst-case bounds on rolling coverage quality. We focus on the more practical, application-driven case of maximizing coverage quality given a fixed memory capacity. As a core simplifying assumption, we restrict algorithm design to a single update operation: writing from the data stream to a calculated buffer site -- with data never being read back, no metadata stored (e.g., sample timestamps), and data eviction occurring only implicitly via overwrite. Drawing only on primitive, low-level operations and ensuring full, overhead-free use of available memory, this ``DStream'' framework ideally suits domains that are resource-constrained, performance-critical, and fine-grained (e.g., individual data items as small as single bits or bytes). The proposed approach supports $\mathcal{O}(1)$ data ingestion via concise bit-level operations. To further practical applications, we provide plug-and-play open-source implementations targeting both scripted and compiled application domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06199v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Andres Moreno, Luis Zaman, Emily Dolson</dc:creator>
    </item>
    <item>
      <title>Noise Stability Optimization for Finding Flat Minima: A Hessian-based Regularization Approach</title>
      <link>https://arxiv.org/abs/2306.08553</link>
      <description>arXiv:2306.08553v4 Announce Type: replace-cross 
Abstract: The training of over-parameterized neural networks has received much study in recent literature. An important consideration is the regularization of over-parameterized networks due to their highly nonconvex and nonlinear geometry. In this paper, we study noise injection algorithms, which can regularize the Hessian of the loss, leading to regions with flat loss surfaces. Specifically, by injecting isotropic Gaussian noise into the weight matrices of a neural network, we can obtain an approximately unbiased estimate of the trace of the Hessian. However, naively implementing the noise injection via adding noise to the weight matrices before backpropagation presents limited empirical improvements. To address this limitation, we design a two-point estimate of the Hessian penalty, which injects noise into the weight matrices along both positive and negative directions of the random noise. In particular, this two-point estimate eliminates the variance of the first-order Taylor's expansion term on the Hessian. We show a PAC-Bayes generalization bound that depends on the trace of the Hessian (and the radius of the weight space), which can be measured from data.
  We conduct a detailed experimental study to validate our approach and show that it can effectively regularize the Hessian and improve generalization. First, our algorithm can outperform prior approaches on sharpness-reduced training, delivering up to a 2.4% test accuracy increase for fine-tuning ResNets on six image classification datasets. Moreover, the trace of the Hessian reduces by 15.8%, and the largest eigenvalue is reduced by 9.7% with our approach. We also find that the regularization of the Hessian can be combined with weight decay and data augmentation, leading to stronger regularization. Second, our approach remains effective for improving generalization in pretraining multimodal CLIP models and chain-of-thought fine-tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08553v4</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Trans. Mach. Learn. Res. 2024</arxiv:journal_reference>
      <dc:creator>Hongyang R. Zhang, Dongyue Li, Haotian Ju</dc:creator>
    </item>
    <item>
      <title>Calibration Error for Decision Making</title>
      <link>https://arxiv.org/abs/2404.13503</link>
      <description>arXiv:2404.13503v4 Announce Type: replace-cross 
Abstract: Calibration allows predictions to be reliably interpreted as probabilities by decision makers. We propose a decision-theoretic calibration error, the Calibration Decision Loss (CDL), defined as the maximum improvement in decision payoff obtained by calibrating the predictions, where the maximum is over all payoff-bounded decision tasks. Vanishing CDL guarantees the payoff loss from miscalibration vanishes simultaneously for all downstream decision tasks. We show separations between CDL and existing calibration error metrics, including the most well-studied metric Expected Calibration Error (ECE). Our main technical contribution is a new efficient algorithm for online calibration that achieves near-optimal $O(\frac{\log T}{\sqrt{T}})$ expected CDL, bypassing the $\Omega(T^{-0.472})$ lower bound for ECE by Qiao and Valiant (2021).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13503v4</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lunjia Hu, Yifan Wu</dc:creator>
    </item>
    <item>
      <title>A Persistent Hierarchical Bloom Filter-based Framework for Authentication and Tracking of ICs</title>
      <link>https://arxiv.org/abs/2408.16950</link>
      <description>arXiv:2408.16950v2 Announce Type: replace-cross 
Abstract: Detecting counterfeit integrated circuits (ICs) in unreliable supply chains demands robust tracking and authentication. Physical Unclonable Functions (PUFs) offer unique IC identifiers, but noise undermines their utility. This study introduces the Persistent Hierarchical Bloom Filter (PHBF) framework, ensuring swift and accurate IC authentication with an accuracy rate of 100% across the supply chain even with noisy PUF-generated signatures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16950v2</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fairuz Shadmani Shishir, Md Mashfiq Rizvee, Tanvir Hossain, Tamzidul Hoque, Domenic Forte, Sumaiya Shomaji</dc:creator>
    </item>
  </channel>
</rss>
