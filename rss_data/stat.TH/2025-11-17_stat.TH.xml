<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Nov 2025 05:02:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Moment estimation in paired comparison models with a growing number of subjects</title>
      <link>https://arxiv.org/abs/2511.10917</link>
      <description>arXiv:2511.10917v1 Announce Type: new 
Abstract: When the number of subjects, $n$, is large, paired comparisons are often sparse. Here, we study statistical inference in a class of paired comparison models parameterized by a set of merit parameters, under an Erd\"{o}s--R\'{e}nyi comparison graph, where the sparsity is measured by a probability $p_n$ tending to zero. We use the moment estimation base on the scores of subjects to infer the merit parameters. We establish a unified theoretical framework in which the uniform consistency and asymptotic normality of the moment estimator hold as the number of subjects goes to infinity. A key idea for the proof of the consistency is that we obtain the convergence rate of the Newton iterative sequence for solving the estimator. We use the Thurstone model to illustrate the unified theoretical results. Further extensions to a fixed sparse comparison graph are also provided. Numerical studies and a real data analysis illustrate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10917v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiuping Wang, Lu Pan, Ting Yan</dc:creator>
    </item>
    <item>
      <title>Learning bounds for doubly-robust covariate shift adaptation</title>
      <link>https://arxiv.org/abs/2511.11003</link>
      <description>arXiv:2511.11003v1 Announce Type: new 
Abstract: Distribution shift between the training domain and the test domain poses a key challenge for modern machine learning. An extensively studied instance is the \emph{covariate shift}, where the marginal distribution of covariates differs across domains, while the conditional distribution of outcome remains the same. The doubly-robust (DR) estimator, recently introduced by \cite{kato2023double}, combines the density ratio estimation with a pilot regression model and demonstrates asymptotic normality and $\sqrt{n}$-consistency, even when the pilot estimates converge slowly. However, the prior arts has focused exclusively on deriving asymptotic results and has left open the question of non-asymptotic guarantees for the DR estimator.
  This paper establishes the first non-asymptotic learning bounds for the DR covariate shift adaptation. Our main contributions are two-fold: (\romannumeral 1) We establish \emph{structure-agnostic} high-probability upper bounds on the excess target risk of the DR estimator that depend only on the $L^2$-errors of the pilot estimates and the Rademacher complexity of the model class, without assuming specific procedures to obtain the pilot estimate, and (\romannumeral 2) under \emph{well-specified parameterized models}, we analyze the DR covariate shift adaptation based on modern techniques for non-asymptotic analysis of MLE, whose key terms governed by the Fisher information mismatch term between the source and target distributions. Together, these findings bridge asymptotic efficiency properties and a finite-sample out-of-distribution generalization bounds, providing a comprehensive theoretical underpinnings for the DR covariate shift adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11003v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeonghwan Lee, Cong Ma</dc:creator>
    </item>
    <item>
      <title>Joint robust estimation</title>
      <link>https://arxiv.org/abs/2511.11054</link>
      <description>arXiv:2511.11054v1 Announce Type: new 
Abstract: We introduce a joint robust estimation method for three parametric statistical models with heavy-tailed data: mean estimation, linear regression, and L2-penalized linear regression, where both the trend parameters and the error variance are unknown. Our approach is based on solving two coupled Catoni-type equations, one for estimating the trend parameters and the other for estimating the error variance. Notably, this joint estimation strategy cannot be obtained by minimizing a single loss function involving both the trend and variance parameters. The method offers four key advantages: (i) the length of the resulting (1 - epsilon) confidence interval scales as (log(1/epsilon))^{1/2}, matching the order achieved by classical estimators for sub-Gaussian data; (ii) it is tuning-free, eliminating the need for separate variance estimation; (iii) it allows flexible selection of Catoni-type functions tailored to the data; and (iv) it delivers strong performance for high-variance data, thanks to the explicit inclusion of the variance term in the denominators of both equations.
  We establish the consistency and asymptotic efficiency of the proposed joint robust estimators using new analytical techniques. The coupled equations are inherently complex, which makes the theoretical analysis of their solutions challenging. To address this, we employ the Poincare-Miranda theorem to show that the solutions lie within geometric regions, such as cylinders or cones, centered around the true parameter values. This methodology is of independent interest and extends to other statistical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11054v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Li, Jun S. Liu, Qiang Sun, Lihu Xu</dc:creator>
    </item>
    <item>
      <title>Consistency of M-estimators for non-identically distributed data: the case of fixed-design distributional regression</title>
      <link>https://arxiv.org/abs/2511.11067</link>
      <description>arXiv:2511.11067v1 Announce Type: new 
Abstract: This paper explores strong and weak consistency of M-estimators for non-identically distributed data, extending prior work. Emphasis is given to scenarios where data is viewed as a triangular array, which encompasses distributional regression models with non-random covariates. Primitive conditions are established for specific applications, such as estimation based on minimizing empirical proper scoring rules or conditional maximum likelihood. A key motivation is addressing challenges in extreme value statistics, where parameter-dependent supports can cause criterion functions to attain the value $-\infty$, hindering the application of existing theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11067v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel B\"ucher, Johan Segers, Torben Staud</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for the fractional Calder\'on problem with a single measurement</title>
      <link>https://arxiv.org/abs/2511.11068</link>
      <description>arXiv:2511.11068v1 Announce Type: new 
Abstract: This paper investigates the consistency of a posterior distribution in the single-measurement fractional Calder\'on problem with additive Gaussian noise. We consider a Bayesian framework with rescaled and Gaussian sieve priors, using a collection of noisy, discrete observations taken from a suitable exterior domain. Our main result shows that the posterior distribution concentrates around the true parameter as the number of measurements increases. Furthermore, we establish tight convergence rates for the reconstruction error of the posterior mean. A central technical challenge is to obtain refined stability estimates for both the forward and inverse problems. In particular, the required forward estimates are delicate to obtain because the fractional elliptic problems do not enjoy as strong regularity theory as their classical counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11068v1</guid>
      <category>math.ST</category>
      <category>math.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pu-Zhao Kow, Janne Nurminen, Jesse Railo</dc:creator>
    </item>
    <item>
      <title>Phase transition for conditional covariance matrices estimated by importance sampling, and implications for cross-entropy schemes in high dimension</title>
      <link>https://arxiv.org/abs/2511.11351</link>
      <description>arXiv:2511.11351v1 Announce Type: new 
Abstract: Motivated by the estimation of covariance matrices by importance sampling arising in the cross-entropy (CE) algorithm, we study a random matrix model $\hat \Sigma = {\bf X} L {\bf X}^\top$ with two distinct features: $\bf X$ and $L$ are dependent, and $L$ is heavy-tailed. In the high-dimensional regime $d \to \infty$, we prove under suitable assumptions that a phase transition occurs in the polynomial regime $n = d^\kappa$, with $n$ the sample size. Namely, we prove that $\lVert \hat \Sigma - E \hat \Sigma \rVert \Rightarrow 0$ if and only if $\kappa &gt; \kappa_*$ for some threshold $\kappa_*$ determined by the behavior of the maximum likelihood ratios. Moreover, we identify general situations where $\kappa_* = 1/\lambda_1$, with $\lambda_1$ the smallest eigenvalue of the covariance matrix of the auxiliary distribution used to estimate $\hat \Sigma$ by importance sampling. This suggests that importance sampling will work better with covariance matrices having a large smallest eigenvalue. We carry this insight into recent CE schemes proposed to estimate the probability of high-dimensional rare events. Through numerical simulations, we demonstrate that better CE schemes are also the ones with larger smallest eigenvalue, even though these algorithms were not designed to smooth the spectrum. This new spectral interpretation raises stimulating questions and opens research directions for the design of efficient high-dimensional algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11351v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Beh, Jerome Morio, Florian Simatos</dc:creator>
    </item>
    <item>
      <title>Exploring the Zipf Distribution Through the Lens of Mixtures</title>
      <link>https://arxiv.org/abs/2511.11530</link>
      <description>arXiv:2511.11530v1 Announce Type: new 
Abstract: The Zipf distribution is a probability distribution widely used by scientists from various disciplines due to its ubiquity. Some of these areas include linguistics, physics, genetics, and sociology, among others. In this paper, it is proved that the Zipf distribution is both a mixture of geometric distributions and a mixture of zero-truncated Poisson distributions. It is also shown that it is not the zero-truncation of a mixed Poisson distribution. These results are important because they provide insights on the data generation mechanism that leads to data from a Zipf distribution. Additionally, it is proved, as a corollary, that the Zipf-Poisson Stopped Sum distribution is a particular case of a mixed Poisson distribution. The results are illustrated analyzing the 135 chapters of the novel Moby Dick.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11530v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta P\'erez-Casany, Ariel Duarte-L\'opez, Jordi Valero</dc:creator>
    </item>
    <item>
      <title>The Maximal Variance of Unilaterally Truncated Gaussian and Chi Distributions</title>
      <link>https://arxiv.org/abs/2511.11566</link>
      <description>arXiv:2511.11566v1 Announce Type: new 
Abstract: This work explores the bounds of the variance of unilaterally truncated Gaussian distributions (UTGDs) and scaled chi distributions (UTSCDs) with fixed means. For any arbitrary Gaussian distribution function, $f(x;\mu,\sigma)$, with a fixed, finite mean $M$ on the truncated domain $x \ge a$, where $a \in \mathbb{R}$, it is proven that the variance is bounded: specifically, $\sup \mathrm{Var}(x)_{|x \ge a}= \sup \mathrm{Var}(x)_{|x \le a} =(M-a)^2$. For a fixed cutoff, $a$, the variance can be considered a function of only $M$, $a$, and the location parameter $\mu$. Examples of such approximating functions, which can be used for model calibration, are developed in addition to other, related calibration methods. For UTSCDs, numerical evidence is presented indicating that for $n \in \mathbb{Z+}$ degrees of freedom, or dimensions, and a fixed, finite mean, the variance, $\mathrm{Var}(R)$, over $R \in [a,\infty)$ reaches its maximum value $M^2(\pi-2)/2$ at $a=0$, $n=1$. For a fixed cutoff value, there is a local maximum in the variance as a function of $n$, and the number of dimensions resulting in the maximal variance, $n_{\mathrm{vmx}}$, increases with cutoff value. However, for $n \in \mathbb{R}$, as the cutoff approaches $0$, $n_{\mathrm{vmx}}$ approaches $-1$, while $\mathrm{Var}(R)$ appears to grow without bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11566v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert J. Petrella</dc:creator>
    </item>
    <item>
      <title>Online Price Competition under Generalized Linear Demands</title>
      <link>https://arxiv.org/abs/2511.10718</link>
      <description>arXiv:2511.10718v1 Announce Type: cross 
Abstract: We study sequential price competition among $N$ sellers, each influenced by the pricing decisions of their rivals. Specifically, the demand function for each seller $i$ follows the single index model $\lambda_i(\mathbf{p}) = \mu_i(\langle \boldsymbol{\theta}_{i,0}, \mathbf{p} \rangle)$, with known increasing link $\mu_i$ and unknown parameter $\boldsymbol{\theta}_{i,0}$, where the vector $\mathbf{p}$ denotes the vector of prices offered by all the sellers simultaneously at a given instant. Each seller observes only their own realized demand -- unobservable to competitors -- and the prices set by rivals. Our framework generalizes existing approaches that focus solely on linear demand models. We propose a novel decentralized policy, PML-GLUCB, that combines penalized MLE with an upper-confidence pricing rule, removing the need for coordinated exploration phases across sellers -- which is integral to previous linear models -- and accommodating both binary and real-valued demand observations. Relative to a dynamic benchmark policy, each seller achieves $O(N^{2}\sqrt{T}\log(T))$ regret, which essentially matches the optimal rate known in the linear setting. A significant technical contribution of our work is the development of a variant of the elliptical potential lemma -- typically applied in single-agent systems -- adapted to our competitive multi-agent environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10718v1</guid>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Bracale, Moulinath Banerjee, Cong Shi, Yuekai Sun</dc:creator>
    </item>
    <item>
      <title>Convergence of the extended Kalman filter with small and state-dependent noise</title>
      <link>https://arxiv.org/abs/2511.10814</link>
      <description>arXiv:2511.10814v1 Announce Type: cross 
Abstract: Nonlinear filtering problems are encountered in many applications, and one solution approach is the extended Kalman filter, which is not always convergent. Therefore, it is crucial to identify conditions under which the extended Kalman filter provides accurate approximations. This paper generalizes two significant results from Picard (1991) on the efficiency of the continuous-time extended Kalman filter to a more general setting where the observation noise may be state-dependent but does not allow signal reconstruction from the quadratic variation of the observation process as in epidemic models. Firstly, we show that when the observation's drift coefficient is strongly injective and the signal's and observation's drift become nearly linear for the diffusion scaling coefficient $\epsilon \to 0$, the estimation error is of order $\sqrt{\epsilon}$. Subsequently, we establish conditions under which the impact of the initial filtering error decays exponentially fast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10814v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ibrahim Mbouandi Njiasse, Florent Ouabo Kamkumo, Ralf Wunderlich</dc:creator>
    </item>
    <item>
      <title>Autocovariance and Optimal Design for Random Walk Metropolis-Hastings Algorithm</title>
      <link>https://arxiv.org/abs/2511.10967</link>
      <description>arXiv:2511.10967v1 Announce Type: cross 
Abstract: The Metropolis-Hastings algorithm has been extensively studied in the estimation and simulation literature, with most prior work focusing on convergence behavior and asymptotic theory. However, its covariance structure-an important statistical property for both theory and implementation-remains less understood. In this work, we provide new theoretical insights into the scalar case, focusing primarily on symmetric unimodal target distributions with symmetric random walk proposals, where we also establish an optimal proposal design. In addition, we derive some more general results beyond this setting. For the high-dimensional case, we relate the covariance matrix to the classical 0.23 average acceptance rate tuning criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10967v1</guid>
      <category>stat.CO</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyi Zhang, James C. Spall</dc:creator>
    </item>
    <item>
      <title>Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths</title>
      <link>https://arxiv.org/abs/2511.11161</link>
      <description>arXiv:2511.11161v1 Announce Type: cross 
Abstract: This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11161v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuzhen Zhao, Yating Liu, Marc Hoffmann</dc:creator>
    </item>
    <item>
      <title>Extreme-PLS with missing data under weak dependence</title>
      <link>https://arxiv.org/abs/2511.11338</link>
      <description>arXiv:2511.11338v1 Announce Type: cross 
Abstract: This paper develops a theoretical framework for Extreme Partial Least Squares (EPLS) dimension reduction in the presence of missing data and weak temporal dependence. Building upon the recent EPLS methodology for modeling extremal dependence between a response variable and high-dimensional covariates, we extend the approach to more realistic data settings where both serial correlation and missing-ness occur. Specifically, we consider a single-index inverse regression model under heavy-tailed conditions and introduce a Missing-at-Random (MAR) mechanism acting on the covariates, whose probability depends on the extremeness of the response. The asymptotic behavior of the proposed estimator is established within an alpha-mixing framework, leading to consistency results under regularly varying tails. Extensive Monte-Carlo experiments covering eleven dependence schemes (including ARMA, GARCH, and nonlinear ESTAR processes) demonstrate that the method performs robustly across a wide range of heavy-tailed and dependent scenarios, even when substantial portions of data are missing. A real-world application to environmental data further confirms the method's capacity to recover meaningful tail directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11338v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>St\'ephane Girard, Cambyse Pakzad</dc:creator>
    </item>
    <item>
      <title>Fuzzy Prediction Sets: Conformal Prediction with E-values</title>
      <link>https://arxiv.org/abs/2509.13130</link>
      <description>arXiv:2509.13130v2 Announce Type: replace 
Abstract: We make three contributions to conformal prediction. First, we propose fuzzy conformal prediction sets that offer a degree of exclusion, generalizing beyond the binary inclusion/exclusion offered by classical prediction sets. We connect fuzzy prediction sets to e-values to show this degree of exclusion is equivalent to an exclusion at different confidence levels, capturing precisely what e-values bring to conformal prediction. We show that a fuzzy prediction set is a predictive distribution with an arguably more appropriate error guarantee. Second, we derive optimal conformal prediction sets by interpreting the minimization of the expected measure of a prediction set as an optimal testing problem against a particular alternative. We use this to characterize exactly in what sense traditional conformal prediction is optimal, and show how this may generally be used to construct optimal (fuzzy) prediction sets. Third, we generalize the inheritance of guarantees by subsequent minimax decisions from prediction sets to fuzzy prediction sets. All results generalize beyond the conformal setting to prediction sets for arbitrary models. In particular, we find that constructing a (fuzzy) prediction set for a model is equivalent to constructing a test (e-value) for that model as a hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13130v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>Tropical Gradient Descent</title>
      <link>https://arxiv.org/abs/2405.19551</link>
      <description>arXiv:2405.19551v3 Announce Type: replace-cross 
Abstract: We propose a gradient descent method for solving optimization problems arising in settings of tropical geometry - a variant of algebraic geometry that has attracted growing interest in applications such as computational biology, economics, and computer science. Our approach takes advantage of the polyhedral and combinatorial structures arising in tropical geometry to propose a versatile method for approximating local minima in tropical statistical optimization problems - a rapidly growing body of work in recent years. Theoretical results establish global solvability for 1-sample problems and a convergence rate matching classical gradient descent. Numerical experiments demonstrate the method's superior performance compared to classical gradient descent for tropical optimization problems which exhibit tropical convexity but not classical convexity. We also demonstrate the seamless integration of tropical descent into advanced optimization methods, such as Adam, offering improved overall accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19551v3</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>J Glob Optim 93 (2025) 413-449</arxiv:journal_reference>
      <dc:creator>Roan Talbut, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>Nonlinear Laplacians: Tunable principal component analysis under directional prior information</title>
      <link>https://arxiv.org/abs/2505.12528</link>
      <description>arXiv:2505.12528v2 Announce Type: replace-cross 
Abstract: We introduce a new family of algorithms for detecting and estimating a rank-one signal from a noisy observation under prior information about that signal's direction, focusing on examples where the signal is known to have entries biased to be positive. Given a matrix observation $\mathbf{Y}$, our algorithms construct a nonlinear Laplacian, another matrix of the form $\mathbf{Y}+\mathrm{diag}(\sigma(\mathbf{Y1}))$ for a nonlinear $\sigma:\mathbb{R}\to\mathbb{R}$, and examine the top eigenvalue and eigenvector of this matrix. When $\mathbf{Y}$ is the (suitably normalized) adjacency matrix of a graph, our approach gives a class of algorithms that search for unusually dense subgraphs by computing a spectrum of the graph "deformed" by the degree profile $\mathbf{Y1}$. We study the performance of such algorithms compared to direct spectral algorithms (the case $\sigma=0$) on models of sparse principal component analysis with biased signals, including the Gaussian planted submatrix problem. For such models, we rigorously characterize the strength of rank-one signal, as a function of $\sigma$, required for an outlier eigenvalue to appear in the spectrum of a nonlinear Laplacian matrix. While identifying the $\sigma$ that minimizes the required signal strength in closed form seems intractable, we explore three approaches to design $\sigma$ numerically: exhaustively searching over simple classes of $\sigma$, learning $\sigma$ from datasets of problem instances, and tuning $\sigma$ using black-box optimization of the critical signal strength. We find both theoretically and empirically that, if $\sigma$ is chosen appropriately, then nonlinear Laplacian spectral algorithms substantially outperform direct spectral algorithms, while retaining the conceptual simplicity of spectral methods compared to broader classes of computations like approximate message passing or general first order methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12528v2</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Ma, Dmitriy Kunisky</dc:creator>
    </item>
    <item>
      <title>Tube formula for spherically contoured random fields with subexponential marginals</title>
      <link>https://arxiv.org/abs/2507.11154</link>
      <description>arXiv:2507.11154v2 Announce Type: replace-cross 
Abstract: It is widely known that the tube method, or equivalently the Euler characteristic heuristic, provides a very accurate approximation for the tail probability that the supremum of a smooth Gaussian random field exceeds a threshold value $c$. The relative approximation error $\Delta(c)$ is exponentially small as a function of $c$ when $c$ tends to infinity. On the other hand, little is known about non-Gaussian random fields.
  In this paper, we obtain the approximation error of the tube method applied to the canonical isotropic random fields on a unit sphere defined by $u\mapsto\langle u,\xi\rangle$, $u\in M\subset\mathbb{S}^{n-1}$, where $\xi$ is a spherically contoured random vector. These random fields have statistical applications in multiple testing and simultaneous regression inference when the unknown variance is estimated. The decay rate of the relative error $\Delta(c)$ depends on the tail of the distribution of $\|\xi\|^2$ and the critical radius of the index set $M$. If this distribution is subexponential but not regularly varying, $\Delta(c)\to 0$ as $c\to\infty$. However, in the regularly varying case, $\Delta(c)$ does not vanish and hence is not negligible. To address this limitation, we provide simple upper and lower bounds for $\Delta(c)$ and for the tube formula itself. Numerical studies are conducted to assess the accuracy of the asymptotic approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11154v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satoshi Kuriki, Evgeny Spodarev</dc:creator>
    </item>
    <item>
      <title>Causal Digital Twins for Cyber-Physical Security: A Framework for Robust Anomaly Detection in Industrial Control Systems</title>
      <link>https://arxiv.org/abs/2510.09616</link>
      <description>arXiv:2510.09616v2 Announce Type: replace-cross 
Abstract: Industrial Control Systems (ICS) in water distribution and treatment face cyber-physical attacks exploiting network and physical vulnerabilities. Current water system anomaly detection methods rely on correlations, yielding high false alarms and poor root cause analysis. We propose a Causal Digital Twin (CDT) framework for water infrastructures, combining causal inference with digital twin modeling. CDT supports association for pattern detection, intervention for system response, and counterfactual analysis for water attack prevention. Evaluated on water-related datasets SWaT, WADI, and HAI, CDT shows 90.8\% compliance with physical constraints and structural Hamming distance 0.133 $\pm$ 0.02. F1-scores are $0.944 \pm 0.014$ (SWaT), $0.902 \pm 0.021$ (WADI), $0.923 \pm 0.018$ (HAI, $p&lt;0.0024$). CDT reduces false positives by 74\%, achieves 78.4\% root cause accuracy, and enables counterfactual defenses reducing attack success by 73.2\%. Real-time performance at 3.2 ms latency ensures safe and interpretable operation for medium-scale water systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09616v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mohammadhossein Homaei, Mehran Tarif, Pablo Garcia Rodriguez, Andres Caro, Mar Avila</dc:creator>
    </item>
  </channel>
</rss>
