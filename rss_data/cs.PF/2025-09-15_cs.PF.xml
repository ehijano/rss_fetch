<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Sep 2025 02:42:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>eHashPipe: Lightweight Top-K and Per-PID Resource Monitoring with eBPF</title>
      <link>https://arxiv.org/abs/2509.09879</link>
      <description>arXiv:2509.09879v1 Announce Type: new 
Abstract: System-level resource monitoring with both precision and efficiency is a continuous challenge. We introduce eHashPipe, a lightweight, real-time resource observability system utilizing eBPF and the HashPipe sketching algorithm. eHashPipe supports two tracking modes: Top-k monitoring to identify the most resource-demanding processes and specific PID tracking to detail the behavior of selected processes. We implement two in-kernel eBPF pipelines for on-CPU time and memory usage. Unlike traditional userspace polling tools, eHashPipe operates in the kernel to reduce latency and context-switch overhead while keeping the runtime footprint small. During our experiments, eHashPipe attains 100 percent Top-k precision for CPU and memory at k = 1, 5, and 10, 95.0/90.0 percent at k = 20, and 93.3/83.3 percent at k = 30 compared to the ground truth. It exposes short-lived bursts with about 14 times finer temporal resolution than top while imposing very low overhead. These results show that eHashPipe delivers accurate, responsive insight with minimal impact, making it well suited for latency-sensitive cloud and edge environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09879v1</guid>
      <category>cs.PF</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanjun Dai (Case Western Reserve University, Cleveland, USA), Qingzhe Guo (Case Western Reserve University, Cleveland, USA), Xiangren Wang (University of Florida, Gainesville, USA)</dc:creator>
    </item>
    <item>
      <title>Setchain Algorithms for Blockchain Scalability</title>
      <link>https://arxiv.org/abs/2509.09795</link>
      <description>arXiv:2509.09795v1 Announce Type: cross 
Abstract: Setchain has been proposed to increase blockchain scalability by relaxing the strict total order requirement among transactions. Setchain organizes elements into a sequence of sets, referred to as epochs, so that elements within each epoch are unordered. In this paper, we propose and evaluate three distinct Setchain algorithms, that leverage an underlying block-based ledger. Vanilla is a basic implementation that serves as a reference point. Compresschain aggregates elements into batches, and compresses these batches before appending them as epochs in the ledger. Hashchain converts batches into fixed-length hashes which are appended as epochs in the ledger. This requires Hashchain to use a distributed service to obtain the batch contents from its hash. To allow light clients to safely interact with only one server, the proposed algorithms maintain, as part of the Setchain, proofs for the epochs. An epoch-proof is the hash of the epoch, cryptographically signed by a server. A client can verify the correctness of an epoch with $f+1$ epoch-proofs (where $f$ is the maximum number of Byzantine servers assumed). All three Setchain algorithms are implemented on top of the CometBFT blockchain application platform. We conducted performance evaluations across various configurations, using clusters of four, seven, and ten servers. Our results show that the Setchain algorithms reach orders of magnitude higher throughput than the underlying blockchain, and achieve finality with latency below 4 seconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09795v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arivarasan Karmegam, Gabina Luz Bianchi, Margarita Capretto, Mart\'in Ceresa, Antonio Fern\'andez Anta, C\'esar S\'anchez</dc:creator>
    </item>
    <item>
      <title>Matrix-Free Evaluation Strategies for Continuous and Discontinuous Galerkin Discretizations on Unstructured Tetrahedral Grids</title>
      <link>https://arxiv.org/abs/2509.10226</link>
      <description>arXiv:2509.10226v1 Announce Type: cross 
Abstract: This study presents novel strategies for improving the node-level performance of matrix-free evaluation of continuous and discontinuous Galerkin spatial discretizations on unstructured tetrahedral grids. In our approach the underlying integrals of a generic finite-element operator are computed cell-by-cell through numerical quadrature using tabulated dense local matrices of shape functions, achieving high throughput for low to moderate-order polynomial degrees. By employing dense matrix-matrix products instead of matrix-vector products for the cell-wise interpolation, the method reaches over $60\%$ of peak performance. The optimization strategies exploit explicit data parallelism to enhance computational efficiency, complemented by a hierarchical mesh reordering algorithm that improves data locality. The matrix-free implementation achieves up to a $6\times$ speedup compared to a global sparse matrix-based approach at a polynomial degree of three. The effectiveness of the method is demonstrated through numerical experiments on the Poisson and Navier--Stokes equations. The Poisson operator is preconditioned by a hybrid multigrid scheme that combines auxiliary continuous finite-element spaces, polynomial and geometric coarsening where possible while employing algebraic multigrid on the coarse mesh. Within the preconditioner, the implementation transitions between the matrix-free and matrix-based strategies for optimal efficiency. Finally, we analyze the strong scaling behavior of the Poisson and Helmholtz operators, demonstrating the method's potential to solve large real-world problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10226v1</guid>
      <category>math.NA</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominik Still, Niklas Fehn, Wolfgang A. Wall, Martin Kronbichler</dc:creator>
    </item>
    <item>
      <title>Stencil-Lifting: Hierarchical Recursive Lifting System for Extracting Summary of Stencil Kernel in Legacy Codes</title>
      <link>https://arxiv.org/abs/2509.10236</link>
      <description>arXiv:2509.10236v2 Announce Type: cross 
Abstract: We introduce Stencil-Lifting, a novel system for automatically converting stencil kernels written in low-level languages in legacy code into semantically equivalent Domain-Specific Language (DSL) implementations. Targeting the efficiency bottlenecks of existing verified lifting systems, Stencil-Lifting achieves scalable stencil kernel abstraction through two key innovations. First, we propose a hierarchical recursive lifting theory that represents stencil kernels, structured as nested loops, using invariant subgraphs, which are customized data dependency graphs that capture loop-carried computation and structural invariants. Each vertex in the invariant subgraph is associated with a predicate-based summary, encoding its computational semantics. By enforcing self-consistency across these summaries, Stencil-Lifting ensures the derivation of correct loop invariants and postconditions for nested loops, eliminating the need for external verification. Second, we develop a hierarchical recursive lifting algorithm that guarantees termination through a convergent recursive process, avoiding the inefficiencies of search-based synthesis. The algorithm efficiently derives the valid summaries of stencil kernels, and its completeness is formally proven. We evaluate Stencil-Lifting on diverse stencil benchmarks from two different suites and on four real-world applications. Experimental results demonstrate that Stencil-Lifting achieves 31.62$\times$ and 5.8$\times$ speedups compared to the state-of-the-art verified lifting systems STNG and Dexter, respectively, while maintaining full semantic equivalence. Our work significantly enhances the translation efficiency of low-level stencil kernels to DSL implementations, effectively bridging the gap between legacy optimization techniques and modern DSL-based paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10236v2</guid>
      <category>cs.SE</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3763159</arxiv:DOI>
      <dc:creator>Mingyi Li, Junmin Xiao, Siyan Chen, Hui Ma, Xi Chen, Peihua Bao, Liang Yuan, Guangming Tan</dc:creator>
    </item>
  </channel>
</rss>
