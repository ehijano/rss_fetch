<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 04:16:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the unconventional Hug integrator</title>
      <link>https://arxiv.org/abs/2502.10199</link>
      <description>arXiv:2502.10199v1 Announce Type: cross 
Abstract: Hug is a recently proposed iterative mapping used to design efficient updates in Markov chain Monte Carlo (MCMC) methods when sampling along manifolds is of interest. In this paper we show that Hug may be interpreted as a consistent discretization of a system of differential equations with a rather complicated structure. The proof of convergence of this discretization includes a number of unusual features we explore fully. We uncover an unexpected and, yet, undocumented property of the solutions of the underlying dynamical system that manifest itself by the existence of Hug trajectories that fail to cover the manifold of interest. This suggests caution when using the Hug update.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10199v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Andrieu, J. M. Sanz-Serna</dc:creator>
    </item>
    <item>
      <title>Tensor-variate Gaussian process regression for efficient emulation of complex systems: comparing regressor and covariance structures in outer product and parallel partial emulators</title>
      <link>https://arxiv.org/abs/2502.10319</link>
      <description>arXiv:2502.10319v1 Announce Type: cross 
Abstract: Multi-output Gaussian process regression has become an important tool in uncertainty quantification, for building emulators of computationally expensive simulators, and other areas such as multi-task machine learning. We present a holistic development of tensor-variate Gaussian process (TvGP) regression, appropriate for arbitrary dimensional outputs where a Kronecker product structure is appropriate for the covariance. We show how two common approaches to problems with two-dimensional output, outer product emulators (OPE) and parallel partial emulators (PPE), are special cases of TvGP regression and hence can be extended to higher output dimensions. Focusing on the important special case of matrix output, we investigate the relative performance of these two approaches. The key distinction is the additional dependence structure assumed by the OPE, and we demonstrate when this is advantageous through two case studies, including application to a spatial-temporal influenza simulator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10319v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daria Semochkina, Samuel E. Jackson, David C. Woods</dc:creator>
    </item>
    <item>
      <title>Misspecification-robust likelihood-free inference in high dimensions</title>
      <link>https://arxiv.org/abs/2002.09377</link>
      <description>arXiv:2002.09377v5 Announce Type: replace 
Abstract: Likelihood-free inference for simulator-based statistical models has developed rapidly from its infancy to a useful tool for practitioners. However, models with more than a handful of parameters still generally remain a challenge for the Approximate Bayesian Computation (ABC) based inference. To advance the possibilities for performing likelihood-free inference in higher dimensional parameter spaces, we introduce an extension of the popular Bayesian optimisation based approach to approximate discrepancy functions in a probabilistic manner which lends itself to an efficient exploration of the parameter space. Our approach achieves computational scalability for higher dimensional parameter spaces by using separate acquisition functions and discrepancies for each parameter. The efficient additive acquisition structure is combined with exponentiated loss -likelihood to provide a misspecification-robust characterisation of the marginal posterior distribution for all model parameters. The method successfully performs computationally efficient inference in a 100-dimensional space on canonical examples and compares favourably to existing modularised ABC methods. We further illustrate the potential of this approach by fitting a bacterial transmission dynamics model to a real data set, which provides biologically coherent results on strain competition in a 30-dimensional parameter space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2002.09377v5</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Owen Thomas, Raquel S\'a-Le\~ao, Herm\'inia de Lencastre, Samuel Kaski, Jukka Corander, Henri Pesonen</dc:creator>
    </item>
    <item>
      <title>Monte Carlo sampling with integrator snippets</title>
      <link>https://arxiv.org/abs/2404.13302</link>
      <description>arXiv:2404.13302v2 Announce Type: replace 
Abstract: Assume interest is in sampling from a probability distribution $\mu$ defined on $(\mathsf{Z},\mathscr{Z})$. We develop a framework for sampling algorithms which takes full advantage of ODE numerical integrators, say $\psi\colon\mathsf{Z}\rightarrow\mathsf{Z}$ for one integration step, to explore $\mu$ efficiently and robustly. The popular Hybrid Monte Carlo (HMC) algorithm \cite{duane1987hybrid,neal2011mcmc} and its derivatives are examples of such a use of numerical integrators. A key idea developed here is that of sampling integrator snippets, that is fragments of the orbit of an ODE numerical integrator $\psi$, and the definition of an associated probability distribution $\bar{\mu}$ such that expectations with respect to $\mu$ can be estimated from integrator snippets distributed according to $\bar{\mu}$. The integrator snippet target distribution $\bar{\mu}$ takes the form of a mixture of pushforward distributions which suggests numerous generalisations beyond mappings arising from numerical integrators, e.g. normalising flows. Very importantly this structure also suggests new principled and robust strategies to tune the parameters of integrators, such as the discretisation stepsize, effective integration time, or number of integration steps, in a Leapfrog integrator.
  We focus here primarily on Sequential Monte Carlo (SMC) algorithms, but the approach can be used in the context of Markov chain Monte Carlo algorithms. We illustrate performance and, in particular, robustness through numerical experiments and provide preliminary theoretical results supporting observed performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13302v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Andrieu, Mauro Camara Escudero, Chang Zhang</dc:creator>
    </item>
    <item>
      <title>Gaussian Processes Regression for Uncertainty Quantification: An Introductory Tutorial</title>
      <link>https://arxiv.org/abs/2502.03090</link>
      <description>arXiv:2502.03090v2 Announce Type: replace 
Abstract: Gaussian Process Regression (GPR) is a powerful nonparametric regression method that is widely used in Uncertainty Quantification (UQ) for constructing surrogate models. This tutorial serves as an introductory guide for beginners, aiming to offer a structured and accessible overview of GPR's applications in UQ. We begin with an introduction to UQ and outline its key tasks, including uncertainty propagation, risk estimation, optimization under uncertainty, parameter estimation, and sensitivity analysis. We then introduce Gaussian Processes (GPs) as a surrogate modeling technique, detailing their formulation, choice of covariance kernels, hyperparameter estimation, and active learning strategies for efficient data acquisition. The tutorial further explores how GPR can be applied to different UQ tasks, including Bayesian quadrature for uncertainty propagation, active learning-based risk estimation, Bayesian optimization for optimization under uncertainty, and surrogate-based sensitivity analysis. Throughout, we emphasize how to leverage the unique formulation of GP for these UQ tasks, rather than simply using it as a standard surrogate model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03090v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinglai Li, Hongqiao Wang</dc:creator>
    </item>
  </channel>
</rss>
