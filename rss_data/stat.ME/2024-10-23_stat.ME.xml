<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Oct 2024 04:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Anatomy of Event Studies: Hypothetical Experiments, Exact Decomposition, and Weighting Diagnostics</title>
      <link>https://arxiv.org/abs/2410.17399</link>
      <description>arXiv:2410.17399v1 Announce Type: new 
Abstract: In recent decades, event studies have emerged as a central methodology in health and social research for evaluating the causal effects of staggered interventions. In this paper, we analyze event studies from the perspective of experimental design and focus on the use of information across units and time periods in the construction of effect estimators. As a particular case of this approach, we offer a novel decomposition of the classical dynamic two-way fixed effects (TWFE) regression estimator for event studies. Our decomposition is expressed in closed form and reveals in finite samples the hypothetical experiment that TWFE regression adjustments approximate. This decomposition offers insights into how standard regression estimators borrow information across different units and times, clarifying and supplementing the notion of forbidden comparison noted in the literature. We propose a robust weighting approach for estimation in event studies, which allows investigators to progressively build larger valid weighted contrasts by leveraging increasingly stronger assumptions on the treatment assignment and potential outcomes mechanisms. This weighting approach also allows for the generalization of treatment effect estimates to a target population. We provide diagnostics and visualization tools and illustrate these methods in a case study of the impact of divorce reforms on female suicide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17399v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ambarish Chattopadhyay, Yuzhou Lin, Zhu Shen, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Flexible Approach for Statistical Disclosure Control in Geospatial Data</title>
      <link>https://arxiv.org/abs/2410.17601</link>
      <description>arXiv:2410.17601v1 Announce Type: new 
Abstract: We develop a flexible approach by combining the Quadtree-based method with suppression to maximize the utility of the grid data and simultaneously to reduce the risk of disclosing private information from individual units. To protect data confidentiality, we produce a high resolution grid from geo-reference data with a minimum size of 1 km nested in grids with increasingly larger resolution on the basis of statistical disclosure control methods (i.e threshold and concentration rule). While our implementation overcomes certain weaknesses of Quadtree-based method by accounting for irregularly distributed and relatively isolated marginal units, it also allows creating joint aggregation of several variables. The method is illustrated by relying on synthetic data of the Danish agricultural census 2020 for a set of key agricultural indicators, such as the number of agricultural holdings, the utilized agricultural area and the number of organic farms. We demonstrate the need to assess the reliability of indicators when using a sub-sample of synthetic data followed by an example that presents the same approach for generating a ratio (i.e., the share of organic farming). The methodology is provided as the open-source \textit{R}-package \textit{MRG} that is adaptable to use with other geo-referenced survey data underlying confidentiality or other privacy restrictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17601v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jon Olav Sk{\o}ien, Nicolas Lampach, Helena Ramos, Rudolf Seljak, Renate Koeble, Linda See, Marijn van der Velde</dc:creator>
    </item>
    <item>
      <title>Ranking of Multi-Response Experiment Treatments</title>
      <link>https://arxiv.org/abs/2410.17604</link>
      <description>arXiv:2410.17604v1 Announce Type: new 
Abstract: We present a probabilistic ranking model to identify the optimal treatment in multiple-response experiments. In contemporary practice, treatments are applied over individuals with the goal of achieving multiple ideal properties on them simultaneously. However, often there are competing properties, and the optimality of one cannot be achieved without compromising the optimality of another. Typically, we still want to know which treatment is the overall best. In our framework, we first formulate overall optimality in terms of treatment ranks. Then we infer the latent ranking that allow us to report treatments from optimal to least optimal, provided ideal desirable properties. We demonstrate through simulations and real data analysis how we can achieve reliability of inferred ranks in practice. We adopt a Bayesian approach and derive an associated Markov Chain Monte Carlo algorithm to fit our model to data. Finally, we discuss the prospects of adoption of our method as a standard tool for experiment evaluation in trials-based research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17604v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Miguel R. Pebes-Trujillo, Itamar Shenhar, Aravind Harikumar, Ittai Herrmann, Menachem Moshelion, Kee Woei Ng, Matan Gavish</dc:creator>
    </item>
    <item>
      <title>Unraveling Residualization: enhancing its application and exposing its relationship with the FWL theorem</title>
      <link>https://arxiv.org/abs/2410.17680</link>
      <description>arXiv:2410.17680v1 Announce Type: new 
Abstract: The residualization procedure has been applied in many different fields to estimate models with multicollinearity. However, there exists a lack of understanding of this methodology and some authors discourage its use. This paper aims to contribute to a better understanding of the residualization procedure to promote an adequate application and interpretation of it among statistics and data sciences. We highlight its interesting potential application, not only to mitigate multicollinearity but also when the study is oriented to the analysis of the isolated effect of independent variables. The relation between the residualization methodology and the Frisch-Waugh-Lovell (FWL) theorem is also analyzed, concluding that, although both provide the same estimations, the interpretation of the estimated coefficients is different. These different interpretations justify the application of the residualization methodology regardless of the FWL theorem. A real data example is presented for a better illustration of the contribution of this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17680v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Catalina Garc\'ia Garc\'ia, Rom\'an Salmer\'on G\'omez, Claudia Garc\'ia Garc\'ia</dc:creator>
    </item>
    <item>
      <title>Longitudinal Causal Inference with Selective Eligibility</title>
      <link>https://arxiv.org/abs/2410.17864</link>
      <description>arXiv:2410.17864v1 Announce Type: new 
Abstract: Dropout often threatens the validity of causal inference in longitudinal studies. While existing studies have focused on the problem of missing outcomes caused by treatment, we study an important but overlooked source of dropout, selective eligibility. For example, patients may become ineligible for subsequent treatments due to severe side effects or complete recovery. Selective eligibility differs from the problem of ``truncation by death'' because dropout occurs after observing the outcome but before receiving the subsequent treatment. This difference makes the standard approach to dropout inapplicable. We propose a general methodological framework for longitudinal causal inference with selective eligibility. By focusing on subgroups of units who would become eligible for treatment given a specific treatment history, we define the time-specific eligible treatment effect (ETE) and expected number of outcome events (EOE) under a treatment sequence of interest. Assuming a generalized version of sequential ignorability, we derive two nonparametric identification formulae, each leveraging different parts of the observed data distribution. We then derive the efficient influence function of each causal estimand, yielding the corresponding doubly robust estimator. Finally, we apply the proposed methodology to an impact evaluation of a pre-trial risk assessment instrument in the criminal justice system, in which selective eligibility arises due to recidivism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17864v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhichao Jiang, Eli Ben-Michael, D. James Greiner, Ryan Halen, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>Deep Nonparametric Inference for Conditional Hazard Function</title>
      <link>https://arxiv.org/abs/2410.18021</link>
      <description>arXiv:2410.18021v1 Announce Type: new 
Abstract: We propose a novel deep learning approach to nonparametric statistical inference for the conditional hazard function of survival time with right-censored data. We use a deep neural network (DNN) to approximate the logarithm of a conditional hazard function given covariates and obtain a DNN likelihood-based estimator of the conditional hazard function. Such an estimation approach renders model flexibility and hence relaxes structural and functional assumptions on conditional hazard or survival functions. We establish the nonasymptotic error bound and functional asymptotic normality of the proposed estimator. Subsequently, we develop new one-sample tests for goodness-of-fit evaluation and two-sample tests for treatment comparison. Both simulation studies and real application analysis show superior performances of the proposed estimators and tests in comparison with existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18021v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wen Su, Kin-Yat Liu, Guosheng Yin, Jian Huang, Xingqiu Zhao</dc:creator>
    </item>
    <item>
      <title>Sacred and Profane: from the Involutive Theory of MCMC to Helpful Hamiltonian Hacks</title>
      <link>https://arxiv.org/abs/2410.17398</link>
      <description>arXiv:2410.17398v1 Announce Type: cross 
Abstract: In the first edition of this Handbook, two remarkable chapters consider seemingly distinct yet deeply connected subjects ...</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17398v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan E. Glatt-Holtz, Andrew J. Holbrook, Justin A. Krometis, Cecilia F. Mondaini, Ami Sheth</dc:creator>
    </item>
    <item>
      <title>Asymptotics for parametric martingale posteriors</title>
      <link>https://arxiv.org/abs/2410.17692</link>
      <description>arXiv:2410.17692v1 Announce Type: cross 
Abstract: The martingale posterior framework is a generalization of Bayesian inference where one elicits a sequence of one-step ahead predictive densities instead of the likelihood and prior. Posterior sampling then involves the imputation of unseen observables, and can then be carried out in an expedient and parallelizable manner using predictive resampling without requiring Markov chain Monte Carlo. Recent work has investigated the use of plug-in parametric predictive densities, combined with stochastic gradient descent, to specify a parametric martingale posterior. This paper investigates the asymptotic properties of this class of parametric martingale posteriors. In particular, two central limit theorems based on martingale limit theory are introduced and applied. The first is a predictive central limit theorem, which enables a significant acceleration of the predictive resampling scheme through a hybrid sampling algorithm based on a normal approximation. The second is a Bernstein-von Mises result, which is novel for martingale posteriors, and provides methodological guidance on attaining desirable frequentist properties. We demonstrate the utility of the theoretical results in simulations and a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17692v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edwin Fong, Andrew Yiu</dc:creator>
    </item>
    <item>
      <title>Semi-supervised Triply Robust Inductive Transfer Learning</title>
      <link>https://arxiv.org/abs/2209.04977</link>
      <description>arXiv:2209.04977v2 Announce Type: replace 
Abstract: In this work, we propose a Semi-supervised Triply Robust Inductive transFer LEarning (STRIFLE) approach, which integrates heterogeneous data from a label-rich source population and a label-scarce target population and utilizes a large amount of unlabeled data simultaneously to improve the learning accuracy in the target population. Specifically, we consider a high dimensional covariate shift setting and employ two nuisance models, a density ratio model and an imputation model, to combine transfer learning and surrogate-assisted semi-supervised learning strategies effectively and achieve triple robustness. While the STRIFLE approach assumes the target and source populations to share the same conditional distribution of outcome Y given both the surrogate features S and predictors X, it allows the true underlying model of Y|X to differ between the two populations due to the potential covariate shift in S and X. Different from double robustness, even if both nuisance models are misspecified or the distribution of Y|(S, X) is not the same between the two populations, the triply robust STRIFLE estimator can still partially use the source population when the shifted source population and the target population share enough similarities. Moreover, it is guaranteed to be no worse than the target-only surrogate-assisted semi-supervised estimator with an additional error term from transferability detection. These desirable properties of our estimator are established theoretically and verified in finite samples via extensive simulation studies. We utilize the STRIFLE estimator to train a Type II diabetes polygenic risk prediction model for the African American target population by transferring knowledge from electronic health records linked genomic data observed in a larger European source population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04977v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianxi Cai, Mengyan Li, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Geodesic slice sampling on the sphere</title>
      <link>https://arxiv.org/abs/2301.08056</link>
      <description>arXiv:2301.08056v3 Announce Type: replace 
Abstract: Probability measures on the sphere form an important class of statistical models and are used, for example, in modeling directional data or shapes. Due to their widespread use, but also as an algorithmic building block, efficient sampling of distributions on the sphere is highly desirable. We propose a shrinkage based and an idealized geodesic slice sampling Markov chain, designed to generate approximate samples from distributions on the sphere. In particular, the shrinkage-based version of the algorithm can be implemented such that it runs efficiently in any dimension and has no tuning parameters. We verify reversibility and prove that under weak regularity conditions geodesic slice sampling is uniformly ergodic. Numerical experiments show that the proposed slice samplers achieve excellent mixing on challenging targets including the Bingham distribution and mixtures of von Mises-Fisher distributions. In these settings our approach outperforms standard samplers such as random-walk Metropolis-Hastings and Hamiltonian Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08056v3</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Habeck, Mareike Hasenpflug, Shantanu Kodgirwar, Daniel Rudolf</dc:creator>
    </item>
    <item>
      <title>Transport map unadjusted Langevin algorithms: learning and discretizing perturbed samplers</title>
      <link>https://arxiv.org/abs/2302.07227</link>
      <description>arXiv:2302.07227v4 Announce Type: replace 
Abstract: Langevin dynamics are widely used in sampling high-dimensional, non-Gaussian distributions whose densities are known up to a normalizing constant. In particular, there is strong interest in unadjusted Langevin algorithms (ULA), which directly discretize Langevin dynamics to estimate expectations over the target distribution. We study the use of transport maps that approximately normalize a target distribution as a way to precondition and accelerate the convergence of Langevin dynamics. We show that in continuous time, when a transport map is applied to Langevin dynamics, the result is a Riemannian manifold Langevin dynamics (RMLD) with metric defined by the transport map. We also show that applying a transport map to an irreversibly-perturbed ULA results in a geometry-informed irreversible perturbation (GiIrr) of the original dynamics. These connections suggest more systematic ways of learning metrics and perturbations, and also yield alternative discretizations of the RMLD described by the map, which we study. Under appropriate conditions, these discretized processes can be endowed with non-asymptotic bounds describing convergence to the target distribution in 2-Wasserstein distance. Illustrative numerical results complement our theoretical claims.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07227v4</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin J. Zhang, Youssef M. Marzouk, Konstantinos Spiliopoulos</dc:creator>
    </item>
    <item>
      <title>Large-Scale Multiple Testing of Composite Null Hypotheses Under Heteroskedasticity</title>
      <link>https://arxiv.org/abs/2306.07362</link>
      <description>arXiv:2306.07362v2 Announce Type: replace 
Abstract: Heteroskedasticity poses several methodological challenges in designing valid and powerful procedures for simultaneous testing of composite null hypotheses. In particular, the conventional practice of standardizing or re-scaling heteroskedastic test statistics in this setting may severely affect the power of the underlying multiple testing procedure. Additionally, when the inferential parameter of interest is correlated with the variance of the test statistic, methods that ignore this dependence may fail to control the type I error at the desired level. We propose a new Heteroskedasticity Adjusted Multiple Testing (HAMT) procedure that avoids data reduction by standardization, and directly incorporates the side information from the variances into the testing procedure. Our approach relies on an improved nonparametric empirical Bayes deconvolution estimator that offers a practical strategy for capturing the dependence between the inferential parameter of interest and the variance of the test statistic. We develop theory to show that HAMT is asymptotically valid and optimal for FDR control. Simulation results demonstrate that HAMT outperforms existing procedures with substantial power gain across many settings at the same FDR level. The method is illustrated on an application involving the detection of engaged users on a mobile game app.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07362v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bowen Gang, Trambak Banerjee</dc:creator>
    </item>
    <item>
      <title>Coefficient Shape Alignment in Multiple Functional Linear Regression</title>
      <link>https://arxiv.org/abs/2312.01925</link>
      <description>arXiv:2312.01925v4 Announce Type: replace 
Abstract: In multivariate functional data analysis, different functional covariates often exhibit homogeneity. The covariates with pronounced homogeneity can be analyzed jointly within the same group, offering a parsimonious approach to modeling multivariate functional data. In this paper, a novel grouped multiple functional regression model with a new regularization approach termed {\it ``coefficient shape alignment"} is developed to tackle functional covariates homogeneity. The modeling procedure includes two steps: first aggregate covariates into disjoint groups using the new regularization approach; then the grouped multiple functional regression model is established based on the detected grouping structure. In this grouped model, the coefficient functions of covariates in the same group share the same shape, invariant to scaling. The new regularization approach works by penalizing differences in the shape of the coefficients. We establish conditions under which the true grouping structure can be accurately identified and derive the asymptotic properties of the model estimates. Extensive simulation studies are conducted to assess the finite-sample performance of the proposed methods. The practical applicability of the model is demonstrated through real data analysis in the context of sugar quality evaluation. This work offers a novel framework for analyzing the homogeneity of functional covariates and constructing parsimonious models for multivariate functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01925v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhao Jiao, Ngai-Hang Chan</dc:creator>
    </item>
    <item>
      <title>MM Algorithms for Statistical Estimation in Quantile Regression</title>
      <link>https://arxiv.org/abs/2407.12348</link>
      <description>arXiv:2407.12348v2 Announce Type: replace 
Abstract: Quantile regression is a robust and practically useful way to efficiently model quantile varying correlation and predict varied response quantiles of interest. This article constructs and tests MM algorithms, which are simple to code and have been suggested superior to some other prominent quantile regression methods in nonregularized problems, in an array of quantile regression settings including linear (modeling different quantile coefficients both separately and simultaneously), nonparametric, regularized, and monotone quantile regression. Applications to various real data sets and two simulation studies comparing MM to existing tested methods have corroborated our algorithms' effectiveness. We have made one key advance by generalizing our MM algorithm to efficiently fit easy-to-predict-and-interpret parametric quantile regression models for data sets exhibiting manifest complicated nonlinear correlation patterns, which has not yet been covered by current literature to the best of our knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12348v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Cheng, Anthony Yung Cheung Kuk</dc:creator>
    </item>
    <item>
      <title>A Causal Transformation Model for Time-to-Event Data Affected by Unobserved Confounding: Revisiting the Illinois Reemployment Bonus Experiment</title>
      <link>https://arxiv.org/abs/2410.15968</link>
      <description>arXiv:2410.15968v2 Announce Type: replace 
Abstract: Motivated by studies investigating causal effects in survival analysis, we propose a transformation model to quantify the impact of a binary treatment on a time-to-event outcome. The approach is based on a flexible linear transformation structural model that links a monotone function of the time-to-event with the propensity for treatment through a bivariate Gaussian distribution. The model equations are specified as functions of additive predictors, allowing the impacts of observed confounders to be accounted for flexibly. Furthermore, the effect of the instrumental variable may be regularized through a ridge penalty, while interactions between the treatment and modifier variables can be incorporated into the model to acknowledge potential variations in treatment effects across different subgroups. The baseline survival function is estimated in a flexible manner using monotonic P-splines, while unobserved confounding is captured through the dependence parameter of the bivariate Gaussian. Parameter estimation is achieved via a computationally efficient and stable penalized maximum likelihood estimation approach and intervals constructed using the related inferential results. We revisit a dataset from the Illinois Reemployment Bonus Experiment to estimate the causal effect of a cash bonus on unemployment duration, unveiling new insights. The modeling framework is incorporated into the R package GJRM, enabling researchers and practitioners to fit the proposed causal survival model and obtain easy-to-interpret numerical and visual summaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15968v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giampiero Marra, Rosalba Radice</dc:creator>
    </item>
    <item>
      <title>Bayesian High-dimensional Linear Regression with Sparse Projection-posterior</title>
      <link>https://arxiv.org/abs/2410.16577</link>
      <description>arXiv:2410.16577v2 Announce Type: replace 
Abstract: We consider a novel Bayesian approach to estimation, uncertainty quantification, and variable selection for a high-dimensional linear regression model under sparsity. The number of predictors can be nearly exponentially large relative to the sample size. We put a conjugate normal prior initially disregarding sparsity, but for making an inference, instead of the original multivariate normal posterior, we use the posterior distribution induced by a map transforming the vector of regression coefficients to a sparse vector obtained by minimizing the sum of squares of deviations plus a suitably scaled $\ell_1$-penalty on the vector. We show that the resulting sparse projection-posterior distribution contracts around the true value of the parameter at the optimal rate adapted to the sparsity of the vector. We show that the true sparsity structure gets a large sparse projection-posterior probability. We further show that an appropriately recentred credible ball has the correct asymptotic frequentist coverage. Finally, we describe how the computational burden can be distributed to many machines, each dealing with only a small fraction of the whole dataset. We conduct a comprehensive simulation study under a variety of settings and found that the proposed method performs well for finite sample sizes. We also apply the method to several real datasets, including the ADNI data, and compare its performance with the state-of-the-art methods. We implemented the method in the \texttt{R} package called \texttt{sparseProj}, and all computations have been carried out using this package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16577v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, Subhashis Ghoshal</dc:creator>
    </item>
    <item>
      <title>Robust Variable Selection for High-dimensional Regression with Missing Data and Measurement Errors</title>
      <link>https://arxiv.org/abs/2410.16722</link>
      <description>arXiv:2410.16722v2 Announce Type: replace 
Abstract: In our paper,we focus on robust variable selection for missing data and measurement error.Missing data and measurement errors can lead to confusing data distribution.We propose an exponential loss function with tuning parameter to apply to Missing and measurement errors data.By adjusting the parameter,the loss functioncan be better and more robust under various different data distributions.We use inverse probability weighting and additivityerrormodels to address missing data and measurement errors.Also,we find that the Atan punishment method works better.We used Monte Carlo simulations to assess the validity of robust variable selection and validated our findings with the breast cancer dataset</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16722v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenhao Zhang</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction for Causal Effects of Continuous Treatments</title>
      <link>https://arxiv.org/abs/2407.03094</link>
      <description>arXiv:2407.03094v2 Announce Type: replace-cross 
Abstract: Uncertainty quantification of causal effects is crucial for safety-critical applications such as personalized medicine. A powerful approach for this is conformal prediction, which has several practical benefits due to model-agnostic finite-sample guarantees. Yet, existing methods for conformal prediction of causal effects are limited to binary/discrete treatments and make highly restrictive assumptions such as known propensity scores. In this work, we provide a novel conformal prediction method for potential outcomes of continuous treatments. We account for the additional uncertainty introduced through propensity estimation so that our conformal prediction intervals are valid even if the propensity score is unknown. Our contributions are three-fold: (1) We derive finite-sample prediction intervals for potential outcomes of continuous treatments. (2) We provide an algorithm for calculating the derived intervals. (3) We demonstrate the effectiveness of the conformal prediction intervals in experiments on synthetic and real-world datasets. To the best of our knowledge, we are the first to propose conformal prediction for continuous treatments when the propensity score is unknown and must be estimated from data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03094v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maresa Schr\"oder, Dennis Frauen, Jonas Schweisthal, Konstantin He{\ss}, Valentyn Melnychuk, Stefan Feuerriegel</dc:creator>
    </item>
  </channel>
</rss>
