<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Nov 2024 02:54:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Novel operational algorithms for ride-pooling as on-demand feeder services</title>
      <link>https://arxiv.org/abs/2411.00787</link>
      <description>arXiv:2411.00787v1 Announce Type: new 
Abstract: Ride-pooling (RP) service, as a form of shared mobility, enables multiple riders with similar itineraries to share the same vehicle and split the fee. This makes RP a promising on-demand feeder service for patrons with a common trip end in urban transportation. We propose the RP as Feeder (RPaF) services with tailored operational algorithms. Specifically, we have developed (i) a batch-based matching algorithm that pools a batch of requests within an optimized buffer distance to each RP vehicle; (ii) a dispatching algorithm that adaptively dispatches vehicles to pick up the matched requests for certain occupancy target; and (iii) a repositioning algorithm that relocates vehicles to unmatched requests based on their level of urgency. An agent-based microscopic simulation platform is designed to execute these operational algorithms (via the Operator module), generate spatially distributed random requests (Patron module), and account for traffic conditions (Vehicle module) in street networks. Extensive numerical experiments are conducted to showcase the effectiveness of RPaF services across various demand scenarios in typical morning rush hours. We compare RFaF with two on-demand feeder counterparts proposed in previous studies: Ride-Sharing as Feeder (RSaF) and Flexible-Route Feeder-Bus Transit (Flex-FBT). Comparisons reveal that given the same fleet size, RPaF generally outperforms RSaF in higher service rates (i.e., the percentage of requests served over all requests) and Flex-FBT in shorter average trip times for patrons. Lastly, we illustrate the implementation of RPaF in a real-world case study of the uptown Manhattan network (USA) using actual taxi trip data. The results demonstrate that RPaF effectively balances the level of service (service rate and patrons' average trip time) with operational costs (fleet size).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00787v1</guid>
      <category>cs.NI</category>
      <category>math.OC</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wenbo Fan, Xiaotian Yan, Zhanbo Sun, Xiaohui Yang</dc:creator>
    </item>
    <item>
      <title>An Iterative Algorithm to Impute Truck Information over Nationwide Traffic Networks</title>
      <link>https://arxiv.org/abs/2411.00789</link>
      <description>arXiv:2411.00789v1 Announce Type: new 
Abstract: Understanding the dynamics of truck volumes and activities across the skeleton traffic network is pivotal for effective traffic planning, traffic management, sustainability analysis, and policy making. Yet, relying solely on average annual daily traffic volume for trucks cannot capture the temporal changes over time. Recently, the Traffic Monitoring Analysis System dataset has emerged as a valuable resource to model the system by providing information on an hourly basis for thousands of detectors across the United States. Combining the average annual daily traffic volume from the Highway Performance Monitoring System and the Traffic Monitoring Analysis System dataset, this study proposes an elegant method of imputing information across the traffic network to generate both truck volumes and vehicle class distributions. A series of experiments evaluated the model's performance on various spatial and temporal scales. The method can be helpful as inputs for emission modeling, network resilience analysis, etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00789v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diyi Liu, Ankur Shiledar, Hyeonsup Lim, Vivek Sujan, Adam Siekmann, Junchuan Fan, Lee D. Han</dc:creator>
    </item>
    <item>
      <title>Erlang Model for Multiple Data Streams (Full Version)</title>
      <link>https://arxiv.org/abs/2411.00792</link>
      <description>arXiv:2411.00792v1 Announce Type: new 
Abstract: With the development of information technology, requirements for data flow have become diverse. When multiple data streams (MDS) are used, the demands of users change over time, which makes traditional teletraffic analysis not directly applicable. This paper proposes probabilistic models for the demand of MDS services, and analyzes in three states: non-tolerance, tolerance and delay. When the requirement random variables are co-distributed with respect to time, we rigorously prove the practicability of the Erlang Multirate Loss Model (EMLM) from a mathematical perspective by discretizing time and error analysis. An algorithm of pre-allocating resources for communication society is given to guild the construction of base resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00792v1</guid>
      <category>cs.NI</category>
      <category>math.PR</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liuquan Yao, Pei Yang, Zhichao Liu, Wenyan Li, Jianghua Liu, Zhi-Ming Ma</dc:creator>
    </item>
    <item>
      <title>Effective ML Model Versioning in Edge Networks</title>
      <link>https://arxiv.org/abs/2411.01078</link>
      <description>arXiv:2411.01078v1 Announce Type: new 
Abstract: Machine learning (ML) models, data and software need to be regularly updated whenever essential version updates are released and feasible for integration. This is a basic but most challenging requirement to satisfy in the edge, due to the various system constraints and the major impact that an update can have on robustness and stability. In this paper, we formulate for the first time the ML model versioning optimization problem, and propose effective solutions, including the automation with reinforcement learning (RL) based algorithm. Without loss of generality, we choose the edge network environment due to the known constraints in performance, response time, security, and reliability. The performance study shows that ML model version updates can be fully and effectively automated with reinforcement learning method as compared to other approaches. We show that with a carefully chosen range of traffic load values, the proper versioning can improve the security, reliability and ML model accuracy, while assuring a comparably lower response time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01078v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fin Gentzen, Mounir Bensalem, Admela Jukan</dc:creator>
    </item>
    <item>
      <title>LumosCore: Highly Scalable LLM Clusters with Optical Interconnect</title>
      <link>https://arxiv.org/abs/2411.01503</link>
      <description>arXiv:2411.01503v1 Announce Type: new 
Abstract: The emergence of Large Language Model(LLM) technologies has led to a rapidly growing demand for compute resources in models. In response, the enterprises are building large-scale multi-tenant GPU clusters with 10k or even ore GPUs. In contrast to the rapidly growing cluster size, the bandwidth of clusters has also been increasing to meet communication demands, with 800 Gbps optical modules already in practical use and 1.6 Tbps modules on the horizon. However, designing clusters that simultaneously meet the requirements of large scale and high bandwidth is challenging due to the limited capacity of electrical switch chips. Unlike electrical switch chips, the single-port bandwidth of MEMS-OCS is solely determined by the optical module, making it straightforward to achieve both bandwidth and scability requirement. In this paper, we propose an opto-electronic hybrid architecture called \textbf{LumosCore}. We address the issues of L2 protocols incompatibility potential network contention and algorithm time complexity through physical topology and logical topology design. Additionally, we design a polynomial-time complexity link reconfiguration algorithm to reconfigure MEMS-OCS with minimal time overhead. We validate the feasibility of the proposed scheme in a cluster consisting of 128 NPUs, and through simulation based on real traces, we demonstrate the superiority of \textbf{LumosCore} over traditional architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01503v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinchi Han, Shizhen Zhao, Yongxi Lv, Peirui Cao, Weihao Jiang, Shengkai Lin, Xinbing Wang</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of Resource Allocation Algorithms for Vehicle Platoons over 5G eV2X Communication</title>
      <link>https://arxiv.org/abs/2411.01525</link>
      <description>arXiv:2411.01525v1 Announce Type: new 
Abstract: Vehicle platooning is a cooperative driving technology that can be supported by 5G enhanced Vehicle-to-Everything (eV2X) communication to improve road safety, traffic efficiency, and reduce fuel consumption. eV2X communication among the platoon vehicles involves the periodic exchange of Cooperative Awareness Messages (CAMs) containing vehicle information under strict latency and reliability requirements. These requirements can be maintained by administering the assignment of resources, in terms of time slots and frequency bands, for CAM exchanges in a platoon, with the help of a resource allocation mechanism. State-of-the-art on control and communication design for vehicle platoons either consider a simplified platoon model with a detailed communication architecture or consider a simplified communication delay model with a detailed platoon control system. Departing from existing works, we have developed a comprehensive vehicle platoon communication and control framework using OMNET++, the benchmarking network simulation tool. We have carried out an inclusive and comparative study of three different platoon Information Flow Topologies (IFTs), namely Car-to-Server, Multi-Hop, and One-Hop over 5G using the Predecessor-leader following platoon control law to arrive at the best-suited IFT for platooning. Secondly, for the best-suited 5G eV2X platooning IFT selected, we have analyzed the performance of three different resource allocation algorithms, namely Maximum of Carrier to Interference Ratio (MaxC/I), Proportional Fair (PF), and Deficit Round Robin (DRR). Exhaustive system-level simulations show that the One-Hop information flow strategy along with the MaxC/I resource allocation yields the best Quality of Service (QoS) performance, in terms of latency, reliability, Age of Information (AoI), and throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01525v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gulabi Mandal, Anik Roy, Basabdatta Palit</dc:creator>
    </item>
    <item>
      <title>Building the Self-Improvement Loop: Error Detection and Correction in Goal-Oriented Semantic Communications</title>
      <link>https://arxiv.org/abs/2411.01544</link>
      <description>arXiv:2411.01544v1 Announce Type: new 
Abstract: Error detection and correction are essential for ensuring robust and reliable operation in modern communication systems, particularly in complex transmission environments. However, discussions on these topics have largely been overlooked in semantic communication (SemCom), which focuses on transmitting meaning rather than symbols, leading to significant improvements in communication efficiency. Despite these advantages, semantic errors -- stemming from discrepancies between transmitted and received meanings -- present a major challenge to system reliability. This paper addresses this gap by proposing a comprehensive framework for detecting and correcting semantic errors in SemCom systems. We formally define semantic error, detection, and correction mechanisms, and identify key sources of semantic errors. To address these challenges, we develop a Gaussian process (GP)-based method for latent space monitoring to detect errors, alongside a human-in-the-loop reinforcement learning (HITL-RL) approach to optimize semantic model configurations using user feedback. Experimental results validate the effectiveness of the proposed methods in mitigating semantic errors under various conditions, including adversarial attacks, input feature changes, physical channel variations, and user preference shifts. This work lays the foundation for more reliable and adaptive SemCom systems with robust semantic error management techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01544v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peizheng Li, Xinyi Lin, Adnan Aijaz</dc:creator>
    </item>
    <item>
      <title>Efficient Conflict Graph Creation for Time-Sensitive Networks with Dynamically Changing Communication Demands</title>
      <link>https://arxiv.org/abs/2411.01902</link>
      <description>arXiv:2411.01902v1 Announce Type: new 
Abstract: Many applications of cyber-physical systems require real-time communication: manufacturing, automotive, etc. Recent Ethernet standards for Time Sensitive Networking (TSN) offer time-triggered scheduling in order to guarantee low latency and jitter bounds. This requires precise frame transmission planning, which becomes especially hard when dealing with many streams, large networks, and dynamically changing communications. A very promising approach uses conflict graphs, modeling conflicting transmission configurations. Since the creation of conflict graphs is the bottleneck in these approaches, we provide an improvement to the conflict graph creation. We present a randomized selection process that reduces the overall size of the graph in half and three heuristics to improve the scheduling success. In our evaluations we show substantial improvements in the graph creation speed and the scheduling success compared to existing work, updating existing schedules in fractions of a second. Additionally, offline planning of 9000 streams was performed successfully within minutes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01902v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heiko Geppert, Frank D\"urr, Kurt Rothermel</dc:creator>
    </item>
    <item>
      <title>Connection Performance Modeling and Analysis of a Radiosonde Network in a Typhoon</title>
      <link>https://arxiv.org/abs/2411.01906</link>
      <description>arXiv:2411.01906v1 Announce Type: new 
Abstract: This paper is concerned with the theoretical modeling and analysis of uplink connection performance of a radiosonde network deployed in a typhoon. Similar to existing works, the stochastic geometry theory is leveraged to derive the expression of the uplink connection probability (CP) of a radiosonde. Nevertheless, existing works assume that network nodes are spherically or uniformly distributed. Different from the existing works, this paper investigates two particular motion patterns of radiosondes in a typhoon, which significantly challenges the theoretical analysis. According to their particular motion patterns, this paper first separately models the distributions of horizontal and vertical distances from a radiosonde to its receiver. Secondly, this paper derives the closed-form expressions of cumulative distribution function (CDF) and probability density function (PDF) of a radiosonde's three-dimensional (3D) propagation distance to its receiver. Thirdly, this paper derives the analytical expression of the uplink CP for any radiosonde in the network. Finally, extensive numerical simulations are conducted to validate the theoretical analysis, and the influence of various network design parameters are comprehensively discussed. Simulation results show that when the signal-to-interference-noise ratio (SINR) threshold is below -35 dB, and the density of radiosondes remains under 0.01/km^3, the uplink CP approaches 26%, 39%, and 50% in three patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01906v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanyi Liu, Xianbin Cao, Peng Yang, Zehui Xiong, Tony Q. S. Quek, Dapeng Oliver Wu</dc:creator>
    </item>
    <item>
      <title>Fairness-Utilization Trade-off in Wireless Networks with Explainable Kolmogorov-Arnold Networks</title>
      <link>https://arxiv.org/abs/2411.01924</link>
      <description>arXiv:2411.01924v1 Announce Type: new 
Abstract: The effective distribution of user transmit powers is essential for the significant advancements that the emergence of 6G wireless networks brings. In recent studies, Deep Neural Networks (DNNs) have been employed to address this challenge. However, these methods frequently encounter issues regarding fairness and computational inefficiency when making decisions, rendering them unsuitable for future dynamic services that depend heavily on the participation of each individual user. To address this gap, this paper focuses on the challenge of transmit power allocation in wireless networks, aiming to optimize $\alpha$-fairness to balance network utilization and user equity. We introduce a novel approach utilizing Kolmogorov-Arnold Networks (KANs), a class of machine learning models that offer low inference costs compared to traditional DNNs through superior explainability. The study provides a comprehensive problem formulation, establishing the NP-hardness of the power allocation problem. Then, two algorithms are proposed for dataset generation and decentralized KAN training, offering a flexible framework for achieving various fairness objectives in dynamic 6G environments. Extensive numerical simulations demonstrate the effectiveness of our approach in terms of fairness and inference cost. The results underscore the potential of KANs to overcome the limitations of existing DNN-based methods, particularly in scenarios that demand rapid adaptation and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01924v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masoud Shokrnezhad, Hamidreza Mazandarani, Tarik Taleb</dc:creator>
    </item>
    <item>
      <title>A new control- and management architecture for SDN-enabled quantum key distribution networks</title>
      <link>https://arxiv.org/abs/2411.01970</link>
      <description>arXiv:2411.01970v1 Announce Type: new 
Abstract: This paper aims to address the challenge of designing secure and high performance Quantum Key Distribution Networks (QKDN), which are essential for encrypted communication in the era of quantum computing. Focusing on the control and management (CM) layer essential for monitoring and routing, the study emphasizes centrally managed software defined networks (SDN). We begin by analyzing QKDN routing characteristics needed for evaluating two existed architectures and the proposed, new CM layer implementation. Following the theoretical analysis, we conduct a discrete-event based simulation in which the proposed architecture is compared to an existent serving as performance-baseline. The results provide recommendations based on use cases for which different architectures show superiority and offer valuable insights into the development and evaluation of CM architectures for QKDNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01970v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Horoschenkoff, Jasper R\"odiger, Martin Wilske</dc:creator>
    </item>
    <item>
      <title>Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism</title>
      <link>https://arxiv.org/abs/2411.02086</link>
      <description>arXiv:2411.02086v1 Announce Type: new 
Abstract: Railway Turnout Machines (RTMs) are mission-critical components of the railway transportation infrastructure, responsible for directing trains onto desired tracks. For safety assurance applications, especially in early-warning scenarios, RTM faults are expected to be detected as early as possible on a continuous 7x24 basis. However, limited emphasis has been placed on distributed model inference frameworks that can meet the inference latency and reliability requirements of such mission critical fault diagnosis systems. In this paper, an edge-cloud collaborative early-warning system is proposed to enable real-time and downtime-tolerant fault diagnosis of RTMs, providing a new paradigm for the deployment of models in safety-critical scenarios. Firstly, a modular fault diagnosis model is designed specifically for distributed deployment, which utilizes a hierarchical architecture consisting of the prior knowledge module, subordinate classifiers, and a fusion layer for enhanced accuracy and parallelism. Then, a cloud-edge collaborative framework leveraging pipeline parallelism, namely CEC-PA, is developed to minimize the overhead resulting from distributed task execution and context exchange by strategically partitioning and offloading model components across cloud and edge. Additionally, an election consensus mechanism is implemented within CEC-PA to ensure system robustness during coordinator node downtime. Comparative experiments and ablation studies are conducted to validate the effectiveness of the proposed distributed fault diagnosis approach. Our ensemble-based fault diagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset collected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA demonstrates superior recovery proficiency during node disruptions and speed-up ranging from 1.98x to 7.93x in total inference time compared to its counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02086v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Wu, Muhammad Bilal, Haolong Xiang, Heng Wang, Jinjun Yu, Xiaolong Xu</dc:creator>
    </item>
    <item>
      <title>A Survey on AI-driven Energy Optimisation in Terrestrial Next Generation Radio Access Networks</title>
      <link>https://arxiv.org/abs/2411.02164</link>
      <description>arXiv:2411.02164v1 Announce Type: new 
Abstract: This survey uncovers the tension between AI techniques designed for energy saving in mobile networks and the energy demands those same techniques create. We compare modeling approaches that estimate power usage cost of current commercial terrestrial next-generation radio access network deployments. We then categorize emerging methods for reducing power usage by domain: time, frequency, power, and spatial. Next, we conduct a timely review of studies that attempt to estimate the power usage of the AI techniques themselves. We identify several gaps in the literature. Notably, real-world data for the power consumption is difficult to source due to commercial sensitivity. Comparing methods to reduce energy consumption is beyond challenging because of the diversity of system models and metrics. Crucially, the energy cost of AI techniques is often overlooked, though some studies provide estimates of algorithmic complexity or run-time. We find that extracting even rough estimates of the operational energy cost of AI models and data processing pipelines is complex. Overall, we find the current literature hinders a meaningful comparison between the energy savings from AI techniques and their associated energy costs. Finally, we discuss future research opportunities to uncover the utility of AI for energy saving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02164v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2024.3482561</arxiv:DOI>
      <arxiv:journal_reference>IEEE Access, vol. 12, pp. 157540-157555, 2024</arxiv:journal_reference>
      <dc:creator>Kishan Sthankiya (Queen Mary University of London, London, U.K), Nagham Saeed (University of West London, London, U.K), Greg McSorley (Applied Research BT, Suffolk, U.K), Mona Jaber (Queen Mary University of London, London, U.K), Richard G. Clegg (Queen Mary University of London, London, U.K)</dc:creator>
    </item>
    <item>
      <title>Technical Report: Performance Comparison of Service Mesh Frameworks: the MTLS Test Case</title>
      <link>https://arxiv.org/abs/2411.02267</link>
      <description>arXiv:2411.02267v1 Announce Type: new 
Abstract: Service Mesh has become essential for modern cloud-native applications by abstracting communication between microservices and providing zero-trust security, observability, and advanced traffic control without requiring code changes. This allows developers to leverage new network capabilities and focus on application logic without managing network complexities. However, the additional layer can significantly impact system performance, latency, and resource consumption, posing challenges for cloud managers and operators.
  In this work, we investigate the impact of the mTLS protocol - a common security and authentication mechanism - on application performance within service meshes. Recognizing that security is a primary motivation for deploying a service mesh, we evaluated the performance overhead introduced by leading service meshes: Istio, Istio Ambient, Linkerd, and Cilium. Our experiments were conducted by testing their performance in service-to-service communications within a Kubernetes cluster.
  Our experiments reveal significant performance differences (in terms of latency and memory consumption) among the service meshes, rooting from the different architecture of the service mesh, sidecar versus sidecareless, and default extra features hidden in the mTLS implementation. Our results highlight the understanding of the service mesh architecture and its impact on performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02267v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anat Bremler Barr, Ofek Lavi, Yaniv Naor, Sanjeev Rampal, Jhonatan Tavori</dc:creator>
    </item>
    <item>
      <title>Profiling AI Models: Towards Efficient Computation Offloading in Heterogeneous Edge AI Systems</title>
      <link>https://arxiv.org/abs/2411.00859</link>
      <description>arXiv:2411.00859v1 Announce Type: cross 
Abstract: The rapid growth of end-user AI applications, such as computer vision and generative AI, has led to immense data and processing demands often exceeding user devices' capabilities. Edge AI addresses this by offloading computation to the network edge, crucial for future services in 6G networks. However, it faces challenges such as limited resources during simultaneous offloads and the unrealistic assumption of homogeneous system architecture. To address these, we propose a research roadmap focused on profiling AI models, capturing data about model types, hyperparameters, and underlying hardware to predict resource utilisation and task completion time. Initial experiments with over 3,000 runs show promise in optimising resource allocation and enhancing Edge AI performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00859v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan Marcelo Parra-Ullauri, Oscar Dilley, Hari Madhukumar, Dimitra Simeonidou</dc:creator>
    </item>
    <item>
      <title>Adaptive Optimization of TLS Overhead for Wireless Communication in Critical Infrastructure</title>
      <link>https://arxiv.org/abs/2411.01971</link>
      <description>arXiv:2411.01971v1 Announce Type: cross 
Abstract: With critical infrastructure increasingly relying on wireless communication, using end-to-end security such as TLS becomes imperative. However, TLS introduces significant overhead for resource-constrained devices and networks prevalent in critical infrastructure. In this paper, we propose to leverage the degrees of freedom in configuring TLS to dynamically adapt algorithms, parameters, and other settings to best meet the currently occurring resource and security constraints in a wireless communication scenario. Consequently, we can make the best use of scarce resources to provide tightened security for wireless networks in critical infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01971v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\"orn Bodenhausen, Laurenz Grote, Michael Rademacher, Martin Henze</dc:creator>
    </item>
    <item>
      <title>Optimizing AoI at Query in Multiuser Wireless Uplink Networks: A Whittle Index Approach</title>
      <link>https://arxiv.org/abs/2411.02108</link>
      <description>arXiv:2411.02108v2 Announce Type: cross 
Abstract: In this paper, we explore how to schedule multiple users to optimize information freshness in a pull-based wireless network, where the status updates from users are requested by randomly arriving queries at the destination. We use the age of information at query (QAoI) to characterize the performance of information freshness. Such a decision-making problem is naturally modeled as a Markov decision process (MDP), which, however, is prohibitively high to be solved optimally by the standard method due to the curse of dimensionality. To address this issue, we employ Whittle index approach, which allows us to decouple the original MDP into multiple sub-MDPs by relaxing the scheduling constraints. However, the binary Markovian query arrival process results in a bi-dimensional state and complex state transitions within each sub-MDP, making it challenging to verify Whittle indexability using conventional methods. After a thorough analysis of the sub-MDP's structure, we show that it is unichain and its optimal policy follows a threshold-type structure. This facilitates the verification of Whittle indexability of the sub-MDP by employing an easy-to-verify condition. Subsequently, the steady-state probability distributions of the sub-MDP under different threshold-type policies are analyzed, constituting the analytical expressions of different Whittle indices in terms of the expected average QAoI and scheduling time of the sub-MDP. Building on these, we devise an efficient algorithm to calculate Whittle indices for the formulated sub-MDPs. The simulation results validate our analyses and show the proposed Whittle index policy outperforms baseline policies and achieves near-optimal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02108v2</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingwei Liu (Henry),  He (Henry),  Chen</dc:creator>
    </item>
    <item>
      <title>DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks</title>
      <link>https://arxiv.org/abs/2401.10158</link>
      <description>arXiv:2401.10158v3 Announce Type: replace 
Abstract: Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a novel multi-headed input privacy-aware distributed learning framework for QoS prediction. Our framework supports multiple heterogeneous nodes, in terms of data types and model architectures, by sharing computations across them. This enables the incorporation of diverse knowledge into a sole learning process that will enhance the robustness and generalization capabilities of the final QoS prediction model. DISTINQT also contributes to data privacy preservation by encoding any raw input data into highly complex, compressed, and irreversible latent representations before any transmission. Evaluation results showcase that DISTINQT achieves a statistically identical performance compared to its centralized version, while also proving the validity of the privacy preserving claims. DISTINQT manages to achieve a reduction in prediction error of up to 65% on average against six state-of-the-art centralized baseline solutions presented in the Tele-Operated Driving use case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10158v3</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaos Koursioumpas, Lina Magoula, Ioannis Stavrakakis, Nancy Alonistioti, M. A. Gutierrez-Estevez, Ramin Khalili</dc:creator>
    </item>
    <item>
      <title>Socialized Learning: A Survey of the Paradigm Shift for Edge Intelligence in Networked Systems</title>
      <link>https://arxiv.org/abs/2404.13348</link>
      <description>arXiv:2404.13348v4 Announce Type: replace 
Abstract: Amidst the robust impetus from artificial intelligence (AI) and big data, edge intelligence (EI) has emerged as a nascent computing paradigm, synthesizing AI with edge computing (EC) to become an exemplary solution for unleashing the full potential of AI services. Nonetheless, challenges in communication costs, resource allocation, privacy, and security continue to constrain its proficiency in supporting services with diverse requirements. In response to these issues, this paper introduces socialized learning (SL) as a promising solution, further propelling the advancement of EI. SL is a learning paradigm predicated on social principles and behaviors, aimed at amplifying the collaborative capacity and collective intelligence of agents within the EI system. SL not only enhances the system's adaptability but also optimizes communication, and networking processes, essential for distributed intelligence across diverse devices and platforms. Therefore, a combination of SL and EI may greatly facilitate the development of collaborative intelligence in the future network. This paper presents the findings of a literature review on the integration of EI and SL, summarizing the latest achievements in existing research on EI and SL. Subsequently, we delve comprehensively into the limitations of EI and how it could benefit from SL. Special emphasis is placed on the communication challenges and networking strategies and other aspects within these systems, underlining the role of optimized network solutions in improving system efficiency. Based on these discussions, we elaborate in detail on three integrated components: socialized architecture, socialized training, and socialized inference, analyzing their strengths and weaknesses. Finally, we identify some possible future applications of combining SL and EI, discuss open problems and suggest some future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13348v4</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaofei Wang, Yunfeng Zhao, Chao Qiu, Qinghua Hu, Victor C. M. Leung</dc:creator>
    </item>
    <item>
      <title>Reducing Communication Overhead in the IoT-Edge-Cloud Continuum: A Survey on Protocols and Data Reduction Strategies</title>
      <link>https://arxiv.org/abs/2404.19492</link>
      <description>arXiv:2404.19492v2 Announce Type: replace 
Abstract: The adoption of the Internet of Things (IoT) deployments has led to a sharp increase in network traffic as a vast number of IoT devices communicate with each other and IoT services through the IoT-edge-cloud continuum. This network traffic increase poses a major challenge to the global communications infrastructure since it hinders communication performance and also puts significant strain on the energy consumption of IoT devices. To address these issues, efficient and collaborative IoT solutions which enable information exchange while reducing the transmitted data and associated network traffic are crucial. This survey provides a comprehensive overview of the communication technologies and protocols as well as data reduction strategies that contribute to this goal. First, we present a comparative analysis of prevalent communication technologies in the IoT domain, highlighting their unique characteristics and exploring the potential for protocol composition and joint usage to enhance overall communication efficiency within the IoT-edge-cloud continuum. Next, we investigate various data traffic reduction techniques tailored to the IoT-edge-cloud context and evaluate their applicability and effectiveness on resource-constrained and devices. Finally, we investigate the emerging concepts that have the potential to further reduce the communication overhead in the IoT-edge-cloud continuum, including cross-layer optimization strategies and Edge AI techniques for IoT data reduction. The paper offers a comprehensive roadmap for developing efficient and scalable solutions across the layers of the IoT-edge-cloud continuum that are beneficial for real-time processing to alleviate network congestion in complex IoT environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19492v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dora Krekovi\'c, Petar Krivi\'c, Ivana Podnar \v{Z}arko, Mario Ku\v{s}ek, Danh Le-Phuoc</dc:creator>
    </item>
    <item>
      <title>AI-Enabled System for Efficient and Effective Cyber Incident Detection and Response in Cloud Environments</title>
      <link>https://arxiv.org/abs/2404.05602</link>
      <description>arXiv:2404.05602v3 Announce Type: replace-cross 
Abstract: The escalating sophistication and volume of cyber threats in cloud environments necessitate a paradigm shift in strategies. Recognising the need for an automated and precise response to cyber threats, this research explores the application of AI and ML and proposes an AI-powered cyber incident response system for cloud environments. This system, encompassing Network Traffic Classification, Web Intrusion Detection, and post-incident Malware Analysis (built as a Flask application), achieves seamless integration across platforms like Google Cloud and Microsoft Azure. The findings from this research highlight the effectiveness of the Random Forest model, achieving an accuracy of 90% for the Network Traffic Classifier and 96% for the Malware Analysis Dual Model application. Our research highlights the strengths of AI-powered cyber security. The Random Forest model excels at classifying cyber threats, offering an efficient and robust solution. Deep learning models significantly improve accuracy, and their resource demands can be managed using cloud-based TPUs and GPUs. Cloud environments themselves provide a perfect platform for hosting these AI/ML systems, while container technology ensures both efficiency and scalability. These findings demonstrate the contribution of the AI-led system in guaranteeing a robust and scalable cyber incident response solution in the cloud.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05602v3</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Ashfaaq M. Farzaan, Mohamed Chahine Ghanem, Ayman El-Hajjar, Deepthi N. Ratnayake</dc:creator>
    </item>
    <item>
      <title>Decentralized Social Networks and the Future of Free Speech Online</title>
      <link>https://arxiv.org/abs/2406.06934</link>
      <description>arXiv:2406.06934v2 Announce Type: replace-cross 
Abstract: Decentralized social networks like Mastodon and BlueSky are trending topics that have drawn much attention and discussion in recent years. By devolving powers from the central node to the end users, decentralized social networks aim to cure existing pathologies on the centralized platforms and have been viewed by many as the future of the Internet. This article critically and systematically assesses the decentralization project's prospect for communications online. It uses normative theories of free speech to examine whether and how the decentralization design could facilitate users' freedom of expression online. The analysis shows that both promises and pitfalls exist, highlighting the importance of value-based design in this area. Two most salient issues for the design of the decentralized networks are: how to balance the decentralization ideal with constant needs of centralization on the network, and how to empower users to make them truly capable of exercising their control. The article then uses some design examples, such as the shared blocklist and the opt-in search function, to illustrate the value considerations underlying the design choices. Some tentative proposals for law and policy interventions are offered to better facilitate the design of the new network. Rather than providing clear answers, the article seeks to map the value implications of the design choices, highlight the stakes, and point directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06934v2</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.clsr.2024.106059</arxiv:DOI>
      <arxiv:journal_reference>Computer Law &amp; Security Review 55, 106059 (2024)</arxiv:journal_reference>
      <dc:creator>Tao Huang</dc:creator>
    </item>
    <item>
      <title>Streaming Technologies and Serialization Protocols: Empirical Performance Analysis</title>
      <link>https://arxiv.org/abs/2407.13494</link>
      <description>arXiv:2407.13494v2 Announce Type: replace-cross 
Abstract: Efficient data streaming is essential for real-time data analytics, visualization, and machine learning model training, particularly when dealing with high-volume datasets. Various streaming technologies and serialization protocols have been developed to cater to different streaming requirements, each performing differently depending on specific tasks and datasets involved. This variety poses challenges in selecting the most appropriate combination, as encountered during the implementation of streaming system for the MAST fusion device data or SKA's radio astronomy data. To address this challenge, we conducted an empirical study on widely used data streaming technologies and serialization protocols. We also developed an extensible, open-source software framework to benchmark their efficiency across various performance metrics. Our study uncovers significant performance differences and trade-offs between these technologies, providing valuable insights that can guide the selection of optimal streaming and serialization solutions for modern data-intensive applications. Our goal is to equip the scientific community and industry professionals with the knowledge needed to enhance data streaming efficiency for improved data utilization and real-time analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13494v2</guid>
      <category>cs.SE</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Samuel Jackson, Nathan Cummings, Saiful Khan</dc:creator>
    </item>
    <item>
      <title>Simulation of fidelity in entanglement-based networks with repeater chains</title>
      <link>https://arxiv.org/abs/2410.09779</link>
      <description>arXiv:2410.09779v2 Announce Type: replace-cross 
Abstract: We implement a simulation environment on top of NetSquid that is specifically designed for estimating the end-to-end fidelity across a path of quantum repeaters or quantum switches. The switch model includes several generalizations which are not currently available in other tools, and are useful for gaining insight into practical and realistic quantum network engineering problems: an arbitrary number of memory registers at the switches, simplicity in including entanglement distillation mechanisms, arbitrary switching topologies, and more accurate models for the depolarization noise. An illustrative case study is presented, namely a comparison in terms of performance between a repeater chain where repeaters can only swap sequentially, and a single switch equipped with multiple memory registers, able to handle multiple swapping requests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09779v2</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David P\'erez Castro, Ana Fern\'andez Vilas, Manuel Fern\'andez-Veiga, Mateo Blanco Rodr\'iguez, Rebeca P. D\'iaz Redondo</dc:creator>
    </item>
    <item>
      <title>ReDAN: An Empirical Study on Remote DoS Attacks against NAT Networks</title>
      <link>https://arxiv.org/abs/2410.21984</link>
      <description>arXiv:2410.21984v2 Announce Type: replace-cross 
Abstract: In this paper, we conduct an empirical study on remote DoS attacks targeting NAT networks. We show that Internet attackers operating outside local NAT networks can remotely identify a NAT device and subsequently terminate TCP connections initiated from the identified NAT device to external servers. Our attack involves two steps. First, we identify NAT devices on the Internet by exploiting inadequacies in the PMTUD mechanism within NAT specifications. This deficiency creates a fundamental side channel that allows Internet attackers to distinguish if a public IPv4 address serves a NAT device or a separate IP host, aiding in the identification of target NAT devices. Second, we launch a remote DoS attack to terminate TCP connections on the identified NAT devices. While recent NAT implementations may include protective measures, such as packet legitimacy validation to prevent malicious manipulations on NAT mappings, we discover that these safeguards are not widely adopted in real world. Consequently, attackers can send crafted packets to deceive NAT devices into erroneously removing innocent TCP connection mappings, thereby disrupting the NATed clients to access remote TCP servers. Our experimental results reveal widespread security vulnerabilities in existing NAT devices. After testing 8 types of router firmware and 30 commercial NAT devices from 14 vendors, we identify vulnerabilities in 6 firmware types and 29 NAT devices. Moreover, our measurements reveal a stark reality: 166 out of 180 (over 92%) tested real-world NAT networks, comprising 90 4G LTE/5G networks, 60 public Wi-Fi networks, and 30 cloud VPS networks, are susceptible to exploitation. We responsibly disclosed the vulnerabilities to affected vendors and received a significant number of acknowledgments. Finally, we propose our countermeasures against the identified DoS attack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21984v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuewei Feng, Yuxiang Yang, Qi Li, Xingxiang Zhan, Kun Sun, Ziqiang Wang, Ao Wang, Ganqiu Du, Ke Xu</dc:creator>
    </item>
    <item>
      <title>Diffusion Models as Network Optimizers: Explorations and Analysis</title>
      <link>https://arxiv.org/abs/2411.00453</link>
      <description>arXiv:2411.00453v2 Announce Type: replace-cross 
Abstract: Network optimization is a fundamental challenge in the Internet of Things (IoT) network, often characterized by complex features that make it difficult to solve these problems. Recently, generative diffusion models (GDMs) have emerged as a promising new approach to network optimization, with the potential to directly address these optimization problems. However, the application of GDMs in this field is still in its early stages, and there is a noticeable lack of theoretical research and empirical findings. In this study, we first explore the intrinsic characteristics of generative models. Next, we provide a concise theoretical proof and intuitive demonstration of the advantages of generative models over discriminative models in network optimization. Based on this exploration, we implement GDMs as optimizers aimed at learning high-quality solution distributions for given inputs, sampling from these distributions during inference to approximate or achieve optimal solutions. Specifically, we utilize denoising diffusion probabilistic models (DDPMs) and employ a classifier-free guidance mechanism to manage conditional guidance based on input parameters. We conduct extensive experiments across three challenging network optimization problems. By investigating various model configurations and the principles of GDMs as optimizers, we demonstrate the ability to overcome prediction errors and validate the convergence of generated solutions to optimal solutions.We provide code and data at https://github.com/qiyu3816/DiffSG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00453v2</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruihuai Liang, Bo Yang, Pengyu Chen, Xianjin Li, Yifan Xue, Zhiwen Yu, Xuelin Cao, Yan Zhang, M\'erouane Debbah, H. Vincent Poor, Chau Yuen</dc:creator>
    </item>
  </channel>
</rss>
