<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 02:40:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Streaming Batch Model for Efficient and Fault-Tolerant Heterogeneous Execution</title>
      <link>https://arxiv.org/abs/2501.12407</link>
      <description>arXiv:2501.12407v2 Announce Type: new 
Abstract: While ML model training and inference are both GPU-intensive, CPU-based data processing is often the bottleneck. Distributed data processing systems based on the batch or stream processing models assume homogeneous resource requirements. They excel at CPU-based computation but either under-utilize heterogeneous resources or impose high overheads on failure and reconfiguration. We introduce the streaming batch model, a hybrid of the two models that enables efficient and fault-tolerant heterogeneous execution. The key idea is to execute one partition at a time to allow lineage-based recovery with dynamic resource allocation. This enables memory-efficient pipelining across heterogeneous resources, similar to stream processing, but also offers the elasticity and fault tolerance properties of batch processing. We present Ray Data, an implementation of the streaming batch model that improves throughput on heterogeneous batch inference pipelines by 3--8$\times$ compared to traditional batch and stream processing systems. When training Stable Diffusion, Ray Data matches the throughput of single-node ML data loaders while additionally leveraging distributed heterogeneous clusters to further improve training throughput by 31%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12407v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Frank Sifei Luan, Ziming Mao, Ron Yifeng Wang, Charlotte Lin, Amog Kamsetty, Hao Chen, Cheng Su, Balaji Veeramani, Scott Lee, SangBin Cho, Clark Zinzow, Eric Liang, Ion Stoica, Stephanie Wang</dc:creator>
    </item>
    <item>
      <title>More for Less: Integrating Capability-Predominant and Capacity-Predominant Computing</title>
      <link>https://arxiv.org/abs/2501.12464</link>
      <description>arXiv:2501.12464v1 Announce Type: new 
Abstract: Capability jobs (e.g., large, long-running tasks) and capacity jobs (e.g., small, short-running tasks) are two common types of workloads in high-performance computing (HPC). Different HPC systems are typically deployed to handle distinct computing workloads. For example, Theta at the Argonne Leadership Computing Facility (ALCF) primarily serves capability jobs, while Cori at the National Energy Research Scientific Computing Center (NERSC) predominantly handles capacity workloads. However, this segregation often leads to inefficient resource utilization and higher costs due to the need for operating separate computing platforms. This work examines what-if scenarios for integrating siloed platforms. Specifically, we collect and characterize two real workloads from production systems at DOE laboratories, representing capabilitypredominant and capacity-predominant computing, respectively. We investigate two approaches to unification. Workload fusion explores how efficiently resources are utilized when a unified system accommodates diverse workloads, whereas workload injection identifies opportunities to enhance resource utilization on capability computing systems by leveraging capacity jobs. Finally, through extensive trace-based, event-driven simulations, we explore the potential benefits of co-scheduling both types of jobs on a unified system to enhance resource utilization and reduce costs, offering new insights for future research in unified computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12464v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhong Zheng, Michael E. Papka, Zhiling Lan</dc:creator>
    </item>
    <item>
      <title>Workflow as a Service Broker in Cloud Environment: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2501.12672</link>
      <description>arXiv:2501.12672v1 Announce Type: new 
Abstract: Cloud computing has emerged as a promising platform for running scientific workflows across various domains. Scientists can take advantage of different cloud service models, such as serverful or serverless, to execute workflows based on their specific requirements, along with diverse pricing models like on-demand, reserved, or spot instances to reduce execution costs. However, the challenge of selecting appropriate resources and pricing models, coupled with the orchestration and scheduling of workflow tasks, creates significant complexity for users. To mitigate this burden, Workflow as a Service (WaaS) brokers have been introduced to facilitate workflow execution. In recent years, numerous studies have been published, either directly or indirectly related to this research area, highlighting the need for a comprehensive and systematic review of WaaS brokers to identify key trends and challenges in this field. In this paper, we conduct a Systematic Literature Review (SLR) on WaaS brokers within cloud environments. The SLR employs a thorough 3-tier strategy (database search, backward snowballing, and forward snowballing) to answer five research questions. A total of 74 high-quality articles, published in 43 prestigious venues, are analyzed to derive a taxonomy based on the architecture of WaaS brokers. The articles are classified and surveyed according to this taxonomy, and future research directions for the design and implementation of WaaS brokers are explored. This study provides valuable insights for researchers and developers, helping them identify major trends and issues in the field of WaaS brokers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12672v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeid Abrishami, Faridreza Momtaz Zandi, Alireza Nourbakhsh</dc:creator>
    </item>
    <item>
      <title>An Empirical Characterization of Outages and Incidents in Public Services for Large Language Models</title>
      <link>https://arxiv.org/abs/2501.12469</link>
      <description>arXiv:2501.12469v1 Announce Type: cross 
Abstract: People and businesses increasingly rely on public LLM services, such as ChatGPT, DALLE, and Claude. Understanding their outages, and particularly measuring their failure-recovery processes, is becoming a stringent problem. However, only limited studies exist in this emerging area. Addressing this problem, in this work we conduct an empirical characterization of outages and failure-recovery in public LLM services. We collect and prepare datasets for 8 commonly used LLM services across 3 major LLM providers, including market-leads OpenAI and Anthropic. We conduct a detailed analysis of failure recovery statistical properties, temporal patterns, co-occurrence, and the impact range of outage-causing incidents. We make over 10 observations, among which: (1) Failures in OpenAI's ChatGPT take longer to resolve but occur less frequently than those in Anthropic's Claude;(2) OpenAI and Anthropic service failures exhibit strong weekly and monthly periodicity; and (3) OpenAI services offer better failure-isolation than Anthropic services. Our research explains LLM failure characteristics and thus enables optimization in building and using LLM systems. FAIR data and code are publicly available on https://zenodo.org/records/14018219 and https://github.com/atlarge-research/llm-service-analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12469v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>16th ACM/SPEC International Conference on Performance Engineering (ICPE 2025)</arxiv:journal_reference>
      <dc:creator>Xiaoyu Chu, Sacheendra Talluri, Qingxian Lu, Alexandru Iosup</dc:creator>
    </item>
    <item>
      <title>D-LoRa: a Distributed Parameter Adaptation Scheme for LoRa Network</title>
      <link>https://arxiv.org/abs/2501.12589</link>
      <description>arXiv:2501.12589v1 Announce Type: cross 
Abstract: The deployment of LoRa networks necessitates joint performance optimization, including packet delivery rate, energy efficiency, and throughput. Additionally, multiple LoRa parameters for packet transmission must be dynamically configured to tailor the performance metrics prioritization across varying channel environments. Because of the coupling relationship between LoRa parameters and metrics, existing works have opted to focus on certain parameters or specific metrics to circumvent the intricate coupling relationship, leading to limited adaptability. Therefore, we propose D-LoRa, a distributed parameter adaptation scheme, based on reinforcement learning towards network performance. We decompose the joint performance optimization problem into multiple independent Multi-Armed Bandit (MAB) problems with different reward functions. We have also built a comprehensive analytical model for the LoRa network that considers path loss, quasi-orthogonality of spreading factor, and packet collision. Experimental results show that our scheme can increase packet delivery rate by up to 28.8% and demonstrates superior adaptability across different performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12589v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruiqi Wang, Tongyu Song, Jing Ren, Xiong Wang, Shizhong Xu, Sheng Wang</dc:creator>
    </item>
    <item>
      <title>FedGrAINS: Personalized SubGraph Federated Learning with Adaptive Neighbor Sampling</title>
      <link>https://arxiv.org/abs/2501.12592</link>
      <description>arXiv:2501.12592v2 Announce Type: cross 
Abstract: Graphs are crucial for modeling relational and biological data. As datasets grow larger in real-world scenarios, the risk of exposing sensitive information increases, making privacy-preserving training methods like federated learning (FL) essential to ensure data security and compliance with privacy regulations. Recently proposed personalized subgraph FL methods have become the de-facto standard for training personalized Graph Neural Networks (GNNs) in a federated manner while dealing with the missing links across clients' subgraphs due to privacy restrictions. However, personalized subgraph FL faces significant challenges due to the heterogeneity in client subgraphs, such as degree distributions among the nodes, which complicate federated training of graph models. To address these challenges, we propose \textit{FedGrAINS}, a novel data-adaptive and sampling-based regularization method for subgraph FL. FedGrAINS leverages generative flow networks (GFlowNets) to evaluate node importance concerning clients' tasks, dynamically adjusting the message-passing step in clients' GNNs. This adaptation reflects task-optimized sampling aligned with a trajectory balance objective. Experimental results demonstrate that the inclusion of \textit{FedGrAINS} as a regularizer consistently improves the FL performance compared to baselines that do not leverage such regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12592v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.IR</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emir Ceyani, Han Xie, Baturalp Buyukates, Carl Yang, Salman Avestimehr</dc:creator>
    </item>
    <item>
      <title>Fray: An Efficient General-Purpose Concurrency Testing Platform for the JVM</title>
      <link>https://arxiv.org/abs/2501.12618</link>
      <description>arXiv:2501.12618v1 Announce Type: cross 
Abstract: Concurrency bugs are hard to discover and reproduce. Prior work has developed sophisticated algorithms to search for concurrency bugs, such as partial order sampling (POS); however, fundamental limitations with existing platforms for concurrency control hinder effective testing of real-world software. We observe that the design space for concurrency control on managed code involves complex trade-offs between expressibility, applicability, and maintainability on the one hand, and bug-finding efficiency on the other hand.
  This paper presents Fray, a platform for performing push-button concurrency testing of data-race-free JVM programs. The key insight behind Fray is that effective controlled concurrency testing requires orchestrating thread interleavings without replacing existing concurrency primitives, while encoding their semantics for faithfully expressing the set of all possible program behaviors. Fray incorporates a novel concurrency control mechanism called shadow locking, designed to make controlled concurrency testing practical and efficient for JVM programs. In an empirical evaluation on 53 benchmark programs with known bugs (SCTBench and JaConTeBe), Fray with random search finds 70% more bugs than JPF and 77% more bugs than RR's chaos mode. We also demonstrate Fray's push-button applicability on 2,655 tests from Apache Kafka, Lucene, and Google Guava. In these mature projects, Fray successfully discovered 18 real-world concurrency bugs that can cause 363 tests to fail reproducibly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12618v1</guid>
      <category>cs.PL</category>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ao Li, Byeongjee Kang, Vasudev Vikram, Isabella Laybourn, Samvid Dharanikota, Shrey Tiwari, Rohan Padhye</dc:creator>
    </item>
    <item>
      <title>Toward Model-centric Heterogeneous Federated Graph Learning: A Knowledge-driven Approach</title>
      <link>https://arxiv.org/abs/2501.12624</link>
      <description>arXiv:2501.12624v1 Announce Type: cross 
Abstract: Federated graph learning (FGL) has emerged as a promising paradigm for collaborative machine learning, enabling multiple parties to jointly train models while preserving the privacy of raw graph data. However, existing FGL methods often overlook the model-centric heterogeneous FGL (MHtFGL) problem, which arises in real-world applications, such as the aggregation of models from different companies with varying scales and architectures. MHtFGL presents an additional challenge: the diversity of client model architectures hampers common learning and integration of graph representations. To address this issue, we propose the Federated Graph Knowledge Collaboration (FedGKC) framework, comprising two key components: Client-side Self-Mutual Knowledge Distillation, which fosters effective knowledge sharing among clients through copilot models; and Server-side Knowledge-Aware Model Aggregation, which enhances model integration by accounting for the knowledge acquired by clients. Experiments on eight benchmark datasets demonstrate that FedGKC achieves an average accuracy improvement of 3.74% over baseline models in MHtFGL scenarios, while also maintaining excellent performance in homogeneous settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12624v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huilin lai, Guang Zeng, Xunkai Li, Xudong Shen, Yinlin Zhu, Ye Luo, Jianwei Lu, Lei Zhu</dc:creator>
    </item>
    <item>
      <title>Practical quantum federated learning and its experimental demonstration</title>
      <link>https://arxiv.org/abs/2501.12709</link>
      <description>arXiv:2501.12709v1 Announce Type: cross 
Abstract: Federated learning is essential for decentralized, privacy-preserving model training in the data-driven era. Quantum-enhanced federated learning leverages quantum resources to address privacy and scalability challenges, offering security and efficiency advantages beyond classical methods. However, practical and scalable frameworks addressing privacy concerns in the quantum computing era remain undeveloped. Here, we propose a practical quantum federated learning framework on quantum networks, utilizing distributed quantum secret keys to protect local model updates and enable secure aggregation with information-theoretic security. We experimentally validate our framework on a 4-client quantum network with a scalable structure. Extensive numerical experiments on both quantum and classical datasets show that adding a quantum client significantly enhances the trained global model's ability to classify multipartite entangled and non-stabilizer quantum datasets. Simulations further demonstrate scalability to 200 clients with classical models trained on the MNIST dataset, reducing communication costs by $75\%$ through advanced model compression techniques and achieving rapid training convergence. Our work provides critical insights for building scalable, efficient, and quantum-secure machine learning systems for the coming quantum internet era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12709v1</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi-Ping Liu, Xiao-Yu Cao, Hao-Wen Liu, Xiao-Ran Sun, Yu Bao, Yu-Shuo Lu, Hua-Lei Yin, Zeng-Bing Chen</dc:creator>
    </item>
    <item>
      <title>$\mu$OpTime: Statically Reducing the Execution Time of Microbenchmark Suites Using Stability Metrics</title>
      <link>https://arxiv.org/abs/2501.12878</link>
      <description>arXiv:2501.12878v1 Announce Type: cross 
Abstract: Performance regressions have a tremendous impact on the quality of software. One way to catch regressions before they reach production is executing performance tests before deployment, e.g., using microbenchmarks, which measure performance at subroutine level. In projects with many microbenchmarks, this may take several hours due to repeated execution to get accurate results, disqualifying them from frequent use in CI/CD pipelines.
  We propose $\mu$OpTime, a static approach to reduce the execution time of microbenchmark suites by configuring the number of repetitions for each microbenchmark. Based on the results of a full, previous microbenchmark suite run, $\mu$OpTime determines the minimal number of (measurement) repetitions with statistical stability metrics that still lead to accurate results.
  We evaluate $\mu$OpTime with an experimental study on 14 open-source projects written in two programming languages and five stability metrics. Our results show that (i) $\mu$OpTime reduces the total suite execution time (measurement phase) by up to 95.83% (Go) and 94.17% (Java), (ii) the choice of stability metric depends on the project and programming language, (iii) microbenchmark warmup phases have to be considered for Java projects (potentially leading to higher reductions), and (iv) $\mu$OpTime can be used to reliably detect performance regressions in CI/CD pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12878v1</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Japke, Martin Grambow, Christoph Laaber, David Bermbach</dc:creator>
    </item>
    <item>
      <title>A Selective Homomorphic Encryption Approach for Faster Privacy-Preserving Federated Learning</title>
      <link>https://arxiv.org/abs/2501.12911</link>
      <description>arXiv:2501.12911v1 Announce Type: cross 
Abstract: Federated learning is a machine learning method that supports training models on decentralized devices or servers, where each holds its local data, removing the need for data exchange. This approach is especially useful in healthcare, as it enables training on sensitive data without needing to share them. The nature of federated learning necessitates robust security precautions due to data leakage concerns during communication. To address this issue, we propose a new approach that employs selective encryption, homomorphic encryption, differential privacy, and bit-wise scrambling to minimize data leakage while achieving good execution performance. Our technique , FAS (fast and secure federated learning) is used to train deep learning models on medical imaging data. We implemented our technique using the Flower framework and compared with a state-of-the-art federated learning approach that also uses selective homomorphic encryption. Our experiments were run in a cluster of eleven physical machines to create a real-world federated learning scenario on different datasets. We observed that our approach is up to 90\% faster than applying fully homomorphic encryption on the model weights. In addition, we can avoid the pretraining step that is required by our competitor and can save up to 20\% in terms of total execution time. While our approach was faster, it obtained similar security results as the competitor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12911v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdulkadir Korkmaz, Praveen Rao</dc:creator>
    </item>
    <item>
      <title>BANG: Billion-Scale Approximate Nearest Neighbor Search using a Single GPU</title>
      <link>https://arxiv.org/abs/2401.11324</link>
      <description>arXiv:2401.11324v3 Announce Type: replace 
Abstract: Approximate Nearest Neighbour Search (ANNS) is a subroutine in algorithms routinely employed in information retrieval, data mining, image processing, and beyond. Recent works have established that graph-based ANNS algorithms are practically more efficient than the other methods proposed in the literature. The growing volume and dimensionality of data necessitates designing scalable techniques for ANNS. To this end, the prior art has explored parallelizing graph-based ANNS on GPU leveraging its massive parallelism. The current state-of-the-art GPU-based ANNS algorithms either (i) require both the dataset and the generated graph index to reside entirely in the GPU memory, or (ii) they partition the dataset into small independent shards, each of which can fit in GPU memory, and perform the search on these shards on the GPU. While the first approach fails to handle large datasets due to the limited memory available on the GPU, the latter delivers poor performance on large datasets due to high data traffic over the low-bandwidth PCIe bus.
  We introduce BANG, a first-of-its-kind technique for graph-based ANNS on GPU for billion-scale datasets, that cannot entirely fit in the GPU memory. BANG stands out by harnessing a compressed form of the dataset on a single GPU to perform distance computations while efficiently accessing the graph index kept on the host memory, enabling efficient ANNS on large graphs within the limited GPU memory. BANG incorporates highly-optimized GPU kernels and proceeds in phases that run concurrently on the GPU and CPU, taking advantage of their architectural specificities. We evaluate BANG using a single NVIDIA Ampere A100 GPU on three popular ANN benchmark datasets. BANG outperforms the state-of-the-art comprehensively. Notably, on the billion-size datasets, we achieve throughputs 30x-200x more the competing methods for a high recall value of 0.9.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11324v3</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karthik V., Saim Khan, Somesh Singh, Harsha Vardhan Simhadri, Jyothi Vedurada</dc:creator>
    </item>
    <item>
      <title>The Blocklace: A Byzantine-repelling and Universal Conflict-free Replicated Data Type</title>
      <link>https://arxiv.org/abs/2402.08068</link>
      <description>arXiv:2402.08068v4 Announce Type: replace 
Abstract: Conflict-free Replicated Data Types (CRDTs) are designed for replica convergence without global coordination or consensus. Recent work has achieved the same in a Byzantine environment, through DAG-like structures based on cryptographic hashes of content. The blocklace is a partially-ordered generalization of the blockchain in which each block has any finite number of signed hash pointers to preceding blocks. We show that the blocklace datatype, with the sole operation of adding a single block, is a CRDT: it is both a pure operation-based CRDT, with self-tagging; and a delta-state CRDT, under a slight generalization of the delta framework. Allowing arbitrary values as payload, the blocklace can also be seen as a universal Byzantine fault-tolerant implementation for arbitrary CRDTs, under the operation-based approach. Current approaches only care about CRDT convergence, being equivocation-tolerant (they do not detect or prevent equivocations), allowing a Byzantine node to cause an arbitrary amount of harm by polluting the CRDT state with an unbounded number of equivocations. We show that the blocklace can be used not only in an equivocation-tolerant way, but also so as to detect and eventually exclude Byzantine nodes, including equivocators, even under the presence of undetectable colluders. The blocklace CRDT protocol ensures that a Byzantine node may harm only a finite prefix of the computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08068v4</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paulo S\'ergio Almeida, Ehud Shapiro</dc:creator>
    </item>
    <item>
      <title>How to Evaluate Distributed Coordination Systems? -- A Survey and Analysis</title>
      <link>https://arxiv.org/abs/2403.09445</link>
      <description>arXiv:2403.09445v2 Announce Type: replace 
Abstract: Coordination services and protocols are critical components of distributed systems and are essential for providing consistency, fault tolerance, and scalability. However, due to the lack of a standard benchmarking tool for distributed coordination services, coordination service developers/researchers either use a NoSQL standard benchmark and omit evaluating consistency, distribution, and fault tolerance; or create their own ad-hoc microbenchmarks and skip comparability with other services. In this study, we analyze and compare known and widely used distributed coordination services, their evaluations, and the tools used to benchmark those systems. We identify important requirements of distributed coordination service benchmarking, like the metrics and parameters that need to be evaluated and their evaluation setups and tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09445v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bekir Turkkan, Elvis Rodrigues, Tevfik Kosar, Aleksey Charapko, Ailidani Ailijiang, Murat Demirbas</dc:creator>
    </item>
    <item>
      <title>MicroPython Testbed for Federated Learning Algorithms</title>
      <link>https://arxiv.org/abs/2405.09423</link>
      <description>arXiv:2405.09423v2 Announce Type: replace 
Abstract: Recently, Python Testbed for Federated Learning Algorithms emerged as a low code and generative large language models amenable framework for developing decentralized and distributed applications, primarily targeting edge systems, by nonprofessional programmers with the help of emerging artificial intelligence tools. This light framework is written in pure Python to be easy to install and to fit into a small IoT memory. It supports formally verified generic centralized and decentralized federated learning algorithms, as well as the peer-to-peer data exchange used in time division multiplexing communication, and its current main limitation is that all the application instances can run only on a single PC. This paper presents the MicroPyton Testbed for Federated Learning Algorithms, the new framework that overcomes its predecessor's limitation such that individual application instances may run on different network nodes like PCs and IoTs, primarily in edge systems. The new framework carries on the pure Python ideal, is based on asynchronous I/O abstractions, and runs on MicroPython, and therefore is a great match for IoTs and devices in edge systems. The new framework was experimentally validated on a wireless network comprising PCs and Raspberry Pi Pico W boards, by using application examples originally developed for the predecessor framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09423v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TELFOR63250.2024.10819071</arxiv:DOI>
      <dc:creator>Miroslav Popovic, Marko Popovic, Ivan Kastelan, Miodrag Djukic, Ilija Basicevic</dc:creator>
    </item>
    <item>
      <title>Towards Formal Verification of Federated Learning Orchestration Protocols on Satellites</title>
      <link>https://arxiv.org/abs/2410.13429</link>
      <description>arXiv:2410.13429v2 Announce Type: replace 
Abstract: Python Testbed for Federated Learning Algorithms (PTB-FLA) is a simple FL framework targeting smart Internet of Things in edge systems that provides both generic centralized and decentralized FL algorithms, which implement the corresponding FL orchestration protocols that were formally verified using the process algebra CSP. This approach is appropriate for systems with stationary nodes but cannot be applied to systems with moving nodes. In this paper, we use celestial mechanics to model spacecraft movement, and timed automata (TA) to formalize and verify the centralized FL orchestration protocol, in two phases. In the first phase, we created a conventional TA model to prove traditional properties, namely deadlock freeness and termination. In the second phase, we created a stochastic TA model to prove timing correctness and to estimate termination probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13429v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TELFOR63250.2024.10819039</arxiv:DOI>
      <dc:creator>Miroslav Popovic, Marko Popovic, Miodrag Djukic, Ilija Basicevic</dc:creator>
    </item>
    <item>
      <title>On the Impact of White-box Deployment Strategies for Edge AI on Latency and Model Performance</title>
      <link>https://arxiv.org/abs/2411.00907</link>
      <description>arXiv:2411.00907v3 Announce Type: replace 
Abstract: To help MLOps engineers decide which operator to use in which deployment scenario, this study aims to empirically assess the accuracy vs latency trade-off of white-box (training-based) and black-box operators (non-training-based) and their combinations in an Edge AI setup. We perform inference experiments including 3 white-box (i.e., QAT, Pruning, Knowledge Distillation), 2 black-box (i.e., Partition, SPTQ), and their combined operators (i.e., Distilled SPTQ, SPTQ Partition) across 3 tiers (i.e., Mobile, Edge, Cloud) on 4 commonly-used Computer Vision and Natural Language Processing models to identify the effective strategies, considering the perspective of MLOps Engineers. Our Results indicate that the combination of Distillation and SPTQ operators (i.e., DSPTQ) should be preferred over non-hybrid operators when lower latency is required in the edge at small to medium accuracy drop. Among the non-hybrid operators, the Distilled operator is a better alternative in both mobile and edge tiers for lower latency performance at the cost of small to medium accuracy loss. Moreover, the operators involving distillation show lower latency in resource-constrained tiers (Mobile, Edge) compared to the operators involving Partitioning across Mobile and Edge tiers. For textual subject models, which have low input data size requirements, the Cloud tier is a better alternative for the deployment of operators than the Mobile, Edge, or Mobile-Edge tier (the latter being used for operators involving partitioning). In contrast, for image-based subject models, which have high input data size requirements, the Edge tier is a better alternative for operators than Mobile, Edge, or their combination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00907v3</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaskirat Singh, Bram Adams, Ahmed E. Hassan</dc:creator>
    </item>
    <item>
      <title>Beyond Optimal Fault Tolerance</title>
      <link>https://arxiv.org/abs/2501.06044</link>
      <description>arXiv:2501.06044v3 Announce Type: replace 
Abstract: The optimal fault-tolerance achievable by any protocol has been characterized in a wide range of settings. For example, for state machine replication (SMR) protocols operating in the partially synchronous setting, it is possible to simultaneously guarantee consistency against $\alpha$-bounded adversaries (i.e., adversaries that control less than an $\alpha$ fraction of the participants) and liveness against $\beta$-bounded adversaries if and only if $\alpha + 2\beta \leq 1$.
  This paper characterizes to what extent "better-than-optimal" fault-tolerance guarantees are possible for SMR protocols when the standard consistency requirement is relaxed to allow a bounded number $r$ of consistency violations. We prove that bounding rollback is impossible without additional timing assumptions and investigate protocols that tolerate and recover from consistency violations whenever message delays around the time of an attack are bounded by a parameter $\Delta^*$ (which may be arbitrarily larger than the parameter $\Delta$ that bounds post-GST message delays in the partially synchronous model). Here, a protocol's fault-tolerance can be a non-constant function of $r$, and we prove, for each $r$, matching upper and lower bounds on the optimal "recoverable fault-tolerance" achievable by any SMR protocol. For example, for protocols that guarantee liveness against 1/3-bounded adversaries in the partially synchronous setting, a 5/9-bounded adversary can always cause one consistency violation but not two, and a 2/3-bounded adversary can always cause two consistency violations but not three. Our positive results are achieved through a generic "recovery procedure" that can be grafted on to any accountable SMR protocol and restores consistency following a violation while rolling back only transactions that were finalized in the previous $2\Delta^*$ timesteps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06044v3</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Lewis-Pye, Tim Roughgarden</dc:creator>
    </item>
    <item>
      <title>A Survey on Inference Optimization Techniques for Mixture of Experts Models</title>
      <link>https://arxiv.org/abs/2412.14219</link>
      <description>arXiv:2412.14219v2 Announce Type: replace-cross 
Abstract: The emergence of large-scale Mixture of Experts (MoE) models represents a significant advancement in artificial intelligence, offering enhanced model capacity and computational efficiency through conditional computation. However, deploying and running inference on these models presents significant challenges in computational resources, latency, and energy efficiency. This comprehensive survey analyzes optimization techniques for MoE models across the entire system stack. We first establish a taxonomical framework that categorizes optimization approaches into model-level, system-level, and hardware-level optimizations. At the model level, we examine architectural innovations including efficient expert design, attention mechanisms, various compression techniques such as pruning, quantization, and knowledge distillation, as well as algorithm improvement including dynamic routing strategies and expert merging methods. At the system level, we investigate distributed computing approaches, load balancing mechanisms, and efficient scheduling algorithms that enable scalable deployment. Furthermore, we delve into hardware-specific optimizations and co-design strategies that maximize throughput and energy efficiency. This survey provides both a structured overview of existing solutions and identifies key challenges and promising research directions in MoE inference optimization. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE inference optimization research, we have established a repository accessible at https://github.com/MoE-Inf/awesome-moe-inference/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14219v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li</dc:creator>
    </item>
    <item>
      <title>Track reconstruction as a service for collider physics</title>
      <link>https://arxiv.org/abs/2501.05520</link>
      <description>arXiv:2501.05520v2 Announce Type: replace-cross 
Abstract: Optimizing charged-particle track reconstruction algorithms is crucial for efficient event reconstruction in Large Hadron Collider (LHC) experiments due to their significant computational demands. Existing track reconstruction algorithms have been adapted to run on massively parallel coprocessors, such as graphics processing units (GPUs), to reduce processing time. Nevertheless, challenges remain in fully harnessing the computational capacity of coprocessors in a scalable and non-disruptive manner. This paper proposes an inference-as-a-service approach for particle tracking in high energy physics experiments. To evaluate the efficacy of this approach, two distinct tracking algorithms are tested: Patatrack, a rule-based algorithm, and Exa$.$TrkX, a machine learning-based algorithm. The as-a-service implementations show enhanced GPU utilization and can process requests from multiple CPU cores concurrently without increasing per-request latency. The impact of data transfer is minimal and insignificant compared to running on local coprocessors. This approach greatly improves the computational efficiency of charged particle tracking, providing a solution to the computing challenges anticipated in the High-Luminosity LHC era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05520v2</guid>
      <category>physics.ins-det</category>
      <category>cs.DC</category>
      <category>hep-ex</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoran Zhao, Yuan-Tang Chou, Yao Yao, Xiangyang Ju, Yongbin Feng, William Patrick McCormack, Miles Cochran-Branson, Jan-Frederik Schulte, Miaoyuan Liu, Javier Duarte, Philip Harris, Shih-Chieh Hsu, Kevin Pedro, Nhan Tran</dc:creator>
    </item>
  </channel>
</rss>
