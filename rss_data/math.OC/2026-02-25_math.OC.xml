<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Feb 2026 02:54:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A unified solution framework for truck-and-drone routing problems</title>
      <link>https://arxiv.org/abs/2602.20310</link>
      <description>arXiv:2602.20310v1 Announce Type: new 
Abstract: Coordinated truck-and-drone routing integrates the high capacity and range of ground vehicles with the flexible routing and speed of drones, enabling simultaneous service. Increasingly applied in last-mile delivery, this synchronization helps reduce completion time and operational costs. To improve its efficiency, various coordination modes between trucks and drones have been proposed. Each mode accommodates diverse operational constraints tailored to particular delivery requirements. In existing work, a slight change in the structural framework or operational characteristics could generate a totally different problem variant, which often requires the design of specialized algorithms. Consequently, under the requirement of maintaining structural validation and adapting to multiple operational features, this paper presents a unified three-phase solution framework based on the Lin-Kernighan-Helsgaun algorithm to solve a wide family of truck-and-drone routing problems. To validate its flexibility and effectiveness, we carry out numerical experiments on three problem variants: the Flying Sidekick Traveling Salesman Problem (FSTSP), the Traveling Salesman Problem with Multiple Drones (TSP-mD), and the Vehicle Routing Problem with Drones (VRP-D), benchmarking each against an effective algorithm. Computational results show that the framework can closely match optimal solutions on small-size instances and even improve the best-known solutions for several medium-size instances. Moreover, additional extensions are discussed to further highlight its versatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20310v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ke Xu, John Gunnar Carlsson</dc:creator>
    </item>
    <item>
      <title>Poisson Hamiltonian Pontryagin Dynamics and Optimal Control of Mechanical Systems on Lie Groupoids</title>
      <link>https://arxiv.org/abs/2602.20326</link>
      <description>arXiv:2602.20326v1 Announce Type: new 
Abstract: We develop a Poisson Hamiltonian formulation of Pontryagin dynamics for optimal control of mechanical systems on Lie groupoids. The reduced dynamics is formulated intrinsically on the dual Lie algebroid endowed with its canonical linear Poisson structure and evolves on its symplectic leaves. The main result of this work shows that symplectic leaves, rather than coadjoint orbits, provide the natural reduced phase spaces for Pontryagin dynamics on Lie groupoids. Under suitable regularity assumptions, we prove the equivalence between the variational formulation of the optimal control problem and the associated Poisson Hamiltonian Pontryagin system, and we show that groupoid invariant Lagrangians lead to reduced optimality conditions of Euler Poincare type. Several mechanical examples, including systems with configuration dependent inertia and local symmetries, are presented to illustrate the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20326v1</guid>
      <category>math.OC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ghorbanali Haghighatdoost</dc:creator>
    </item>
    <item>
      <title>A variance reduced framework for (non)smooth nonconvex-nonconcave stochastic minimax problems with extended Kurdyka-Lojasiewicz property</title>
      <link>https://arxiv.org/abs/2602.20357</link>
      <description>arXiv:2602.20357v1 Announce Type: new 
Abstract: In this paper, we study stochastic constrained minimax optimization problems with nonconvex-nonconcave structure, a central problem in modern machine learning, for which reliable and efficient algorithms remain largely unexplored due to its inherent challenges. Prior approaches for nonconvex minimax optimization often require (strong) concavity on the maximization part, or certain restrictive geometric assumptions on the joint objective to have guaranteed convergence. In contrast, our method only assumes weak convexity in the primal variable and the extended Kurdyka-Lojasiewicz (KL) property, with exponent $\theta \in [0,1]$, in the dual variable, significantly broadening the class of tractable problems. To this end, we propose a variance reduced algorithm that provably handles this general setting and achieves an $\varepsilon$-stationary solution with state-of-the-art sample complexity: in the smooth finite-sum setting, the sample complexity is $\mathcal{O}\left(\sqrt{N}\,\varepsilon^{-\max\{4\theta,2\}}\right)$, where $N$ is the number of total samples, and in the online smooth setting, it is $\mathcal{O}\Big(\varepsilon^{-\max\{6\theta,3\}}\Big)$. For the structured nonsmooth problem, the sample complexity is $\mathcal{O}\left(\sqrt{N}\,\max\Big\{\varepsilon^{-3}, \varepsilon^{-5\theta}, \varepsilon^{-\frac{11\theta-3}{2\theta}}\Big\}\right)$ and $\mathcal{O}\left(\max\left\{\varepsilon^{-4}, \varepsilon^{-\frac{15\theta-1}{2}}, \varepsilon^{-\frac{31\theta-9}{4\theta}}\right\}\right)$ respectively for the two settings. To the best of our knowledge, this is the first unified framework that jointly accommodates weak convexity, the extended KL property, and variance-reduced stochastic updates, making it highly suitable for large-scale applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20357v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Muhammad Khan, Yangyang Xu</dc:creator>
    </item>
    <item>
      <title>Stochastic Control Problems with Infinite Horizon and Regime Switching Arising in Optimal Liquidation with Semimartingale Strategies</title>
      <link>https://arxiv.org/abs/2602.20552</link>
      <description>arXiv:2602.20552v1 Announce Type: new 
Abstract: We study an optimal control problem on infinite time horizon with semimartingale strategies, random coefficients and regime switching. The value function and the optimal strategy can be characterized in terms of three systems of backward stochastic differential equations (BSDEs) with infinite horizon. One of them is a system of linear BSDEs with unbounded coefficients and infinite horizon, which seems to be new in literature. We establish the existence of the solutions to these BSDEs by BMO analysis and comparison theorem for multi-dimensional BSDEs. Next, we establish that the optimal control problem is well posed, in the sense that the value function is finite and the optimal strategy-when it exists-is unique. This is achieved by reformulating the cost functional as the sum of a quadratic functional and the candidate value function. The reformulation crucially relies on the well-established well-posedness results for systems of BSDEs. Finally, under additional assumptions, we obtain the unique optimal strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20552v1</guid>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinman Cheng, Guanxing Fu, Xiaonyu Xia</dc:creator>
    </item>
    <item>
      <title>On the Convergence of Stochastic Gradient Descent with Perturbed Forward-Backward Passes</title>
      <link>https://arxiv.org/abs/2602.20646</link>
      <description>arXiv:2602.20646v1 Announce Type: new 
Abstract: We study stochastic gradient descent (SGD) for composite optimization problems with $N$ sequential operators subject to perturbations in both the forward and backward passes. Unlike classical analyses that treat gradient noise as additive and localized, perturbations to intermediate outputs and gradients cascade through the computational graph, compounding geometrically with the number of operators. We present the first comprehensive theoretical analysis of this setting. Specifically, we characterize how forward and backward perturbations propagate and amplify within a single gradient step, derive convergence guarantees for both general non-convex objectives and functions satisfying the Polyak--\L{}ojasiewicz condition, and identify conditions under which perturbations do not deteriorate the asymptotic convergence order. As a byproduct, our analysis furnishes a theoretical explanation for the gradient spiking phenomenon widely observed in deep learning, precisely characterizing the conditions under which training recovers from spikes or diverges. Experiments on logistic regression with convex and non-convex regularization validate our theories, illustrating the predicted spike behavior and the asymmetric sensitivity to forward versus backward perturbations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20646v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boao Kong, Hengrui Zhang, Kun Yuan</dc:creator>
    </item>
    <item>
      <title>Convergent Lifted Lasserre Hierarchy of SDPs for Minimizing Expectation of Piecewise Polynomial Loss over Wasserstein Balls</title>
      <link>https://arxiv.org/abs/2602.20660</link>
      <description>arXiv:2602.20660v1 Announce Type: new 
Abstract: This paper investigates the minimization of the expectation of piecewise polynomial loss functions over Wasserstein balls. This optimization problem often appears as a key sub-problem of distributionally robust optimization problems. We establish the asymptotic convergence of a hierarchy of semi-definite programming (SDP) relaxations, providing a framework for approximating the optimal values of these inherently infinite-dimensional optimization problems. A central foundational contribution is the development of a new lifted positivity certificate: we demonstrate that piecewise polynomials positive over Archimedean basic semi-algebraic sets admit a structured system of sum-of-squares (SOS) representations. Furthermore, we prove that the proposed hierarchy achieves finite convergence under suitable conditions when the defining polynomials are convex. The practical utility and versatility of this approach are demonstrated via numerical experiments in revenue estimation and portfolio optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20660v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>N. D. Dizon, Q. Y. Huang, T. D. Chuong, G. Li, V. Jeyakumar</dc:creator>
    </item>
    <item>
      <title>Implicit Decision Diagrams</title>
      <link>https://arxiv.org/abs/2602.20793</link>
      <description>arXiv:2602.20793v2 Announce Type: new 
Abstract: Decision Diagrams (DDs) have emerged as a powerful tool for discrete optimization, with rapidly growing adoption. DDs are directed acyclic layered graphs; restricted DDs are a generalized greedy heuristic for finding feasible solutions, and relaxed DDs compute combinatorial relaxed bounds. There is substantial theory that leverages DD-based bounding, yet the complexity of constructing the DDs themselves has received little attention. Standard restricted DD construction requires $O(w \log(w))$ per layer; standard relaxed DD construction requires $O(w^2)$, where $w$ is the width of the DD. Increasing $w$ improves bound quality at the cost of more time and memory.
  We introduce implicit Decision Diagrams, storing arcs implicitly rather than explicitly, and reducing per-layer complexity to $O(w)$ for restricted and relaxed DDs. We prove this is optimal: any framework treating state-update and merge operations as black boxes cannot do better.
  Optimal complexity shifts the challenge from algorithmic overhead to low-level engineering. We show how implicit DDs can drive a MIP solver, and release ImplicitDDs.jl (https://https://github.com/IsaacRudich/ImplicitDDs.jl), an open-source Julia solver exploiting the implementation refinements our theory enables. Experiments demonstrate the solver outperforms Gurobi on Subset Sum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20793v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Rudich, Louis-Martin Rousseau</dc:creator>
    </item>
    <item>
      <title>A subdifferential characterization via Busemann functions and applications to DC optimization on Hadamard manifolds</title>
      <link>https://arxiv.org/abs/2602.20931</link>
      <description>arXiv:2602.20931v1 Announce Type: new 
Abstract: This paper investigates the properties of Busemann functions on Hadamard manifolds and their use in optimization algorithms in Riemannian settings. We present a new Busemann-based characterization of the subdifferential, which is particularly well suited to Riemannian optimization. In the classical Hadamard manifold framework, a subgradient provides a global lower model of a convex function expressed through the inverse exponential map. However, this model may fail to exhibit a useful convexity or concavity structure. By contrast, our characterization yields a concave bounding function by exploiting key properties of Busemann functions. We use this concavity to design and analyze difference-of-convex (DC) optimization methods on Hadamard manifolds. In particular, we reformulate the classical DC algorithm (DCA) for Riemannian contexts and study its convergence properties. We also report preliminary numerical experiments comparing the proposed Busemann DCA, which leads to geodesically convex subproblems, with the classical Riemannian DCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20931v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>O. P. Ferreira, D. S. Gon\c{c}alves, M. S. Louzeiro, S. Z. N\'emeth, J. Zhu</dc:creator>
    </item>
    <item>
      <title>Asymptotics of solutions to the linear search problem</title>
      <link>https://arxiv.org/abs/2602.20991</link>
      <description>arXiv:2602.20991v1 Announce Type: new 
Abstract: The exact leading asymptotics of solutions to the symmetric linear search problem are obtained for any positive probability density on the real line with a monotonic, sufficiently regular tail. A similar result holds for densities on a compact interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20991v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robin A. Heinonen</dc:creator>
    </item>
    <item>
      <title>Robustness certificates in data-driven non-convex optimization with additively-uncertain constraints</title>
      <link>https://arxiv.org/abs/2602.21090</link>
      <description>arXiv:2602.21090v1 Announce Type: new 
Abstract: We consider decision-making problems that are formulated as non-convex optimization programs where uncertainty enters the constraints through an additive term, independent of the decision variables, and robustness is imposed using a finite data-set, according to the scenario robust optimization paradigm. By exploiting the structure of the constraints, we show that both a priori and a posteriori distribution-free probabilistic robustness certificates for a possibly sub-optimal solution to the resulting data-driven optimization problem can be obtained with minimal computational effort. Building on these results, we also discuss a one-shot and an incremental procedure to determine the size of the data-set so as to guarantee a user-chosen robustness level. Notably, both the a posteriori robustness assessment and incremental data-set sizing do not require to solve the non-convex scenario program. A comparative analysis performed on the unit commitment problem using real data reveals a limited increase in conservativeness with a significant computational saving with respect to the application of scenario theory results for general, non necessarily structured, non-convex problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21090v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander J Gallo, Massimiliano Zoggia, Alessandro Falsone, Maria Prandini, Simone Garatti</dc:creator>
    </item>
    <item>
      <title>Complexity of Classical Acceleration for $\ell_1$-Regularized PageRank</title>
      <link>https://arxiv.org/abs/2602.21138</link>
      <description>arXiv:2602.21138v1 Announce Type: new 
Abstract: We study the degree-weighted work required to compute $\ell_1$-regularized PageRank using the standard one-gradient-per-iteration accelerated proximal-gradient method (FISTA). For non-accelerated local methods, the best known worst-case work scales as $\widetilde{O} ((\alpha\rho)^{-1})$, where $\alpha$ is the teleportation parameter and $\rho$ is the $\ell_1$-regularization parameter. A natural question is whether FISTA can improve the dependence on $\alpha$ from $1/\alpha$ to $1/\sqrt{\alpha}$ while preserving the $1/\rho$ locality scaling. The challenge is that acceleration can break locality by transiently activating nodes that are zero at optimality, thereby increasing the cost of gradient evaluations. We analyze FISTA on a slightly over-regularized objective and show that, under a checkable confinement condition, all spurious activations remain inside a boundary set $\mathcal{B}$. This yields a bound consisting of an accelerated $(\rho\sqrt{\alpha})^{-1}\log(\alpha/\varepsilon)$ term plus a boundary overhead $\sqrt{vol(\mathcal{B})}/(\rho\alpha^{3/2})$. We provide graph-structural conditions that imply such confinement. Experiments on synthetic and real graphs show the resulting speedup and slowdown regimes under the degree-weighted work model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21138v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kimon Fountoulakis, David Mart\'inez-Rubio</dc:creator>
    </item>
    <item>
      <title>Learning Invariant Visual Representations for Planning with Joint-Embedding Predictive World Models</title>
      <link>https://arxiv.org/abs/2602.18639</link>
      <description>arXiv:2602.18639v1 Announce Type: cross 
Abstract: World models learned from high-dimensional visual observations allow agents to make decisions and plan directly in latent space, avoiding pixel-level reconstruction. However, recent latent predictive architectures (JEPAs), including the DINO world model (DINO-WM), display a degradation in test time robustness due to their sensitivity to "slow features". These include visual variations such as background changes and distractors that are irrelevant to the task being solved. We address this limitation by augmenting the predictive objective with a bisimulation encoder that enforces control-relevant state equivalence, mapping states with similar transition dynamics to nearby latent states while limiting contributions from slow features. We evaluate our model on a simple navigation task under different test-time background changes and visual distractors. Across all benchmarks, our model consistently improves robustness to slow features while operating in a reduced latent space, up to 10x smaller than that of DINO-WM. Moreover, our model is agnostic to the choice of pretrained visual encoder and maintains robustness when paired with DINOv2, SimDINOv2, and iBOT features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18639v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo F. Toso, Davit Shadunts, Yunyang Lu, Nihal Sharma, Donglin Zhan, Nam H. Nguyen, James Anderson</dc:creator>
    </item>
    <item>
      <title>Tensor Network Generator-Enhanced Optimization for Traveling Salesman Problem</title>
      <link>https://arxiv.org/abs/2602.20175</link>
      <description>arXiv:2602.20175v1 Announce Type: cross 
Abstract: We present an application of the tensor network generator-enhanced optimization (TN-GEO) framework to address the traveling salesman problem (TSP), a fundamental combinatorial optimization challenge. Our approach employs a tensor network Born machine based on automatically differentiable matrix product states (MPS) as the generative model, using the Born rule to define probability distributions over candidate solutions. Unlike approaches based on binary encoding, which require $N^2$ variables and penalty terms to enforce valid tour constraints, we adopt a permutation-based formulation with integer variables and use autoregressive sampling with masking to guarantee that every generated sample is a valid tour by construction. We also introduce a $k$-site MPS variant that learns distributions over $k$-grams (consecutive city subsequences) using a sliding window approach, enabling parameter-efficient modeling for larger instances. Experimental validation on TSPLIB benchmark instances with up to 52 cities demonstrates that TN-GEO can outperform classical heuristics including swap and 2-opt hill-climbing. The $k$-site variants, which put more focus on local correlations, show better results compared to the full-MPS case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20175v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>quant-ph</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryo Sakai, Chen-Yu Liu</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware Delivery Delay Duration Prediction via Multi-Task Deep Learning</title>
      <link>https://arxiv.org/abs/2602.20271</link>
      <description>arXiv:2602.20271v1 Announce Type: cross 
Abstract: Accurate delivery delay prediction is critical for maintaining operational efficiency and customer satisfaction across modern supply chains. Yet the increasing complexity of logistics networks, spanning multimodal transportation, cross-country routing, and pronounced regional variability, makes this prediction task inherently challenging. This paper introduces a multi-task deep learning model for delivery delay duration prediction in the presence of significant imbalanced data, where delayed shipments are rare but operationally consequential. The model embeds high-dimensional shipment features with dedicated embedding layers for tabular data, and then uses a classification-then-regression strategy to predict the delivery delay duration for on-time and delayed shipments. Unlike sequential pipelines, this approach enables end-to-end training, improves the detection of delayed cases, and supports probabilistic forecasting for uncertainty-aware decision making. The proposed approach is evaluated on a large-scale real-world dataset from an industrial partner, comprising more than 10 million historical shipment records across four major source locations with distinct regional characteristics. The proposed model is compared with traditional machine learning methods. Experimental results show that the proposed method achieves a mean absolute error of 0.67-0.91 days for delayed-shipment predictions, outperforming single-step tree-based regression baselines by 41-64% and two-step classify-then-regress tree-based models by 15-35%. These gains demonstrate the effectiveness of the proposed model in operational delivery delay forecasting under highly imbalanced and heterogeneous conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20271v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefan Faulkner, Reza Zandehshahvar, Vahid Eghbal Akhlaghi, Sebastien Ouellet, Carsten Jordan, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>GSNR: Graph Smooth Null-Space Representation for Inverse Problems</title>
      <link>https://arxiv.org/abs/2602.20328</link>
      <description>arXiv:2602.20328v1 Announce Type: cross 
Abstract: Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20328v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romario Gualdr\'on-Hurtado, Roman Jacome, Rafael S. Suarez, Henry Arguello</dc:creator>
    </item>
    <item>
      <title>StochasticBarrier.jl: A Toolbox for Stochastic Barrier Function Synthesis</title>
      <link>https://arxiv.org/abs/2602.20359</link>
      <description>arXiv:2602.20359v1 Announce Type: cross 
Abstract: We present StochasticBarrier.jl, an open-source Julia-based toolbox for generating Stochastic Barrier Functions (SBFs) for safety verification of discrete-time stochastic systems with additive Gaussian noise. StochasticBarrier.jl certifies linear, polynomial, and piecewise affine (PWA) systems. The latter enables verification for a wide range of system dynamics, including general nonlinear types. The toolbox implements a Sum-of-Squares (SOS) optimization approach, as well as methods based on piecewise constant (PWC) functions. For SOS-based SBFs, StochasticBarrier.jl leverages semi-definite programming solvers, while for PWC SBFs, it offers three engines: two using linear programming (LP) and one based on gradient descent (GD). Benchmarking StochasticBarrier.jl against the state-of-the-art shows that the tool outperforms existing tools in computation time, safety probability bounds, and scalability across over 30 case studies. Compared to its closest competitor, StochasticBarrier.jl is up to four orders of magnitude faster, achieves significant safety probability improvements, and supports higher-dimensional systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20359v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rayan Mazouz, Frederik Baymler Mathiesen, Luca Laurenti, Morteza Lahijanian</dc:creator>
    </item>
    <item>
      <title>Competition Versus Complexity in Multiple-Selection Prophet Inequalities</title>
      <link>https://arxiv.org/abs/2602.20398</link>
      <description>arXiv:2602.20398v1 Announce Type: cross 
Abstract: Competition complexity formalizes a compelling intuition: rather than refining the mechanism, how much additional competition is sufficient for a simple mechanism to compete with an optimal one? We begin the study of this question in multi-unit pricing for welfare maximization using prophet inequalities. An online decision-maker observes $m \geq k$ nonnegative values drawn independently from a known distribution, may select up to $k$ of them, and aims to maximize the expected sum of selected values. The benchmark is a prophet who observes a sequence of length $n \geq k$ and selects the $k$ largest values. We focus on the widely adopted class of single-threshold algorithms and fully characterize their $(1-\varepsilon)$-competition complexity. Notably, our results reveal a sharp competition-induced phase transition: in the absence of competition, single-threshold algorithms are fundamentally limited to a $1-1/\sqrt{2k\pi}$ fraction of the prophet value, whereas even a $1\%$ multiplicative increase beyond $n$ observations suffices to achieve a $1-\exp(-\Theta(k))$ fraction. Another notable result happens when $k=1$: we show that the $(1-\varepsilon)$-competition complexity is exactly $\ln(1/\varepsilon)$, fully resolving an open question by Brustle et al. [Math. Oper. Res. 2024]. Our analysis is based on infinite-dimensional linear programming and duality arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20398v1</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugenio Cruz-Ossa, Sebastian Perez-Salazar, Victor Verdugo</dc:creator>
    </item>
    <item>
      <title>Wasserstein Distributionally Robust Online Learning</title>
      <link>https://arxiv.org/abs/2602.20403</link>
      <description>arXiv:2602.20403v1 Announce Type: cross 
Abstract: We study distributionally robust online learning, where a risk-averse learner updates decisions sequentially to guard against worst-case distributions drawn from a Wasserstein ambiguity set centered at past observations. While this paradigm is well understood in the offline setting through Wasserstein Distributionally Robust Optimization (DRO), its online extension poses significant challenges in both convergence and computation. In this paper, we address these challenges. First, we formulate the problem as an online saddle-point stochastic game between a decision maker and an adversary selecting worst-case distributions, and propose a general framework that converges to a robust Nash equilibrium coinciding with the solution of the corresponding offline Wasserstein DRO problem. Second, we address the main computational bottleneck, which is the repeated solution of worst-case expectation problems. For the important class of piecewise concave loss functions, we propose a tailored algorithm that exploits problem geometry to achieve substantial speedups over state-of-the-art solvers such as Gurobi. The key insight is a novel connection between the worst-case expectation problem, an inherently infinite-dimensional optimization problem, and a classical and tractable budget allocation problem, which is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20403v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guixian Chen, Salar Fattahi, Soroosh Shafiee</dc:creator>
    </item>
    <item>
      <title>Stability and Generalization of Push-Sum Based Decentralized Optimization over Directed Graphs</title>
      <link>https://arxiv.org/abs/2602.20567</link>
      <description>arXiv:2602.20567v1 Announce Type: cross 
Abstract: Push-Sum-based decentralized learning enables optimization over directed communication networks, where information exchange may be asymmetric. While convergence properties of such methods are well understood, their finite-iteration stability and generalization behavior remain unclear due to structural bias induced by column-stochastic mixing and asymmetric error propagation. In this work, we develop a unified uniform-stability framework for the Stochastic Gradient Push (SGP) algorithm that captures the effect of directed topology. A key technical ingredient is an imbalance-aware consistency bound for Push-Sum, which controls consensus deviation through two quantities: the stationary distribution imbalance parameter $\delta$ and the spectral gap $(1-\lambda)$ governing mixing speed. This decomposition enables us to disentangle statistical effects from topology-induced bias. We establish finite-iteration stability and optimization guarantees for both convex objectives and non-convex objectives satisfying the Polyak--\L{}ojasiewicz condition. For convex problems, SGP attains excess generalization error of order $\tilde{\mathcal{O}}\!\left(\frac{1}{\sqrt{mn}}+\frac{\gamma}{\delta(1-\lambda)}+\gamma\right)$ under step-size schedules, and we characterize the corresponding optimal early stopping time that minimizes this bound. For P\L{} objectives, we obtain convex-like optimization and generalization rates with dominant dependence proportional to $\kappa\!\left(1+\frac{1}{\delta(1-\lambda)}\right)$, revealing a multiplicative coupling between problem conditioning and directed communication topology. Our analysis clarifies when Push-Sum correction is necessary compared with standard decentralized SGD and quantifies how imbalance and mixing jointly shape the best attainable learning performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20567v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifei Liang, Yan Sun, Xiaochun Cao, Li Shen</dc:creator>
    </item>
    <item>
      <title>Upper-Linearizability of Online Non-Monotone DR-Submodular Maximization over Down-Closed Convex Sets</title>
      <link>https://arxiv.org/abs/2602.20578</link>
      <description>arXiv:2602.20578v1 Announce Type: cross 
Abstract: We study online maximization of non-monotone Diminishing-Return(DR)-submodular functions over down-closed convex sets, a regime where existing projection-free online methods suffer from suboptimal regret and limited feedback guarantees. Our main contribution is a new structural result showing that this class is $1/e$-linearizable under carefully designed exponential reparametrization, scaling parameter, and surrogate potential, enabling a reduction to online linear optimization. As a result, we obtain $O(T^{1/2})$ static regret with a single gradient query per round and unlock adaptive and dynamic regret guarantees, together with improved rates under semi-bandit, bandit, and zeroth-order feedback. Across all feedback models, our bounds strictly improve the state of the art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20578v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiyang Lu, Haresh Jadav, Mohammad Pedramfar, Ranveer Singh, Vaneet Aggarwal</dc:creator>
    </item>
    <item>
      <title>Quantum circuit design from a retraction-based Riemannian optimization framework</title>
      <link>https://arxiv.org/abs/2602.20605</link>
      <description>arXiv:2602.20605v1 Announce Type: cross 
Abstract: Designing quantum circuits for ground state preparation is a fundamental task in quantum information science. However, standard Variational Quantum Algorithms (VQAs) are often constrained by limited ansatz expressivity and difficult optimization landscapes. To address these issues, we adopt a geometric perspective, formulating the problem as the minimization of an energy cost function directly over the unitary group. We establish a retraction-based Riemannian optimization framework for this setting, ensuring that all algorithmic procedures are implementable on quantum hardware. Within this framework, we unify existing randomized gradient approaches under a Riemannian Random Subspace Gradient Projection (RRSGP) method. While recent geometric approaches have predominantly focused on such first-order gradient descent techniques, efficient second-order methods remain unexplored. To bridge this gap, we derive explicit expressions for the Riemannian Hessian and show that it can be estimated directly on quantum hardware via parameter-shift rules. Building on this, we propose the Riemannian Random Subspace Newton (RRSN) method, a scalable second-order algorithm that constructs a Newton system from measurement data. Numerical simulations indicate that RRSN achieves quadratic convergence, yielding high-precision ground states in significantly fewer iterations compared to both existing first-order approaches and standard VQA baselines. Ultimately, this work provides a systematic foundation for applying a broader class of efficient Riemannian algorithms to quantum circuit design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20605v1</guid>
      <category>quant-ph</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhijian Lai, Hantao Nie, Jiayuan Wu, Dong An</dc:creator>
    </item>
    <item>
      <title>Efficient Online Learning in Interacting Particle Systems</title>
      <link>https://arxiv.org/abs/2602.20875</link>
      <description>arXiv:2602.20875v1 Announce Type: cross 
Abstract: We introduce a new method for online parameter estimation in stochastic interacting particle systems, based on continuous observation of a small number of particles from the system. Our method recursively updates the model parameters using a stochastic approximation of the gradient of the asymptotic log likelihood, which is computed using the continuous stream of observations. Under suitable assumptions, we rigorously establish convergence of our method to the stationary points of the asymptotic log-likelihood of the interacting particle system. We consider asymptotics both in the limit as the time horizon $t\rightarrow\infty$, for a fixed and finite number of particles, and in the joint limit as the number of particles $N\rightarrow\infty$ and the time horizon $t\rightarrow\infty$. Under additional assumptions on the asymptotic log-likelihood, we also establish an $\mathrm{L}^2$ convergence rate and a central limit theorem. Finally, we present several numerical examples of practical interest, including a model for systemic risk, a model of interacting FitzHugh--Nagumo neurons, and a Cucker--Smale flocking model. Our numerical results corroborate our theoretical results, and also suggest that our estimator is effective even in cases where the assumptions required for our theoretical analysis do not hold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20875v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Sharrock, Nikolas Kantas, Grigorios A. Pavliotis</dc:creator>
    </item>
    <item>
      <title>On the non-uniformity of the 2026 FIFA World Cup draw</title>
      <link>https://arxiv.org/abs/2602.21029</link>
      <description>arXiv:2602.21029v1 Announce Type: cross 
Abstract: The group stage of a sports tournament is often made more appealing by introducing additional constraints in the group draw that promote an attractive and balanced group composition. For example, the number of intra-regional group matches is minimised in several World Cups. However, under such constraints, the traditional draw procedure may become non-uniform, meaning that the feasible allocations of the teams into groups are not equally likely to occur. Our paper quantifies this non-uniformity of the 2026 FIFA World Cup draw for the official draw procedure, as well as for 47 reasonable alternatives implied by all permutations of the four pots and two group labelling policies. We show why simulating with a recursive backtracking algorithm is intractable, and propose a workable implementation using integer programming. The official draw mechanism is found to be optimal based on four measures of non-uniformity. Nonetheless, non-uniformity can be more than halved if the organiser aims to treat the best teams drawn from the first pot equally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21029v1</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Martin Becker, Karel Devriesere, Dries Goossens</dc:creator>
    </item>
    <item>
      <title>Optimizing Representation in Redistricting: Dual Bounds for Partitioning Problems with Non-Convex Objectives</title>
      <link>https://arxiv.org/abs/2305.17298</link>
      <description>arXiv:2305.17298v3 Announce Type: replace 
Abstract: We investigate optimization models for the purpose of computational redistricting. Our focus is on nonconvex objectives for estimating expected Black Representatives and Political Representation. The objectives are a composition of a ratio of variables and a normal distribution's cumulative distribution function (or ``probit curve"). We extend the work of Validi et al.~\cite{validi2022imposing}, which presented a robust implementation of contiguity constraints. By developing mixed integer linear programming models that closely approximate the parent nonlinear model, our approaches yield tight bounds on these optimization problems. We exhibit the effectiveness of these approaches on county-level data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17298v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jamie Fravel, Robert Hildebrand, Nicholas Goedert, Laurel Travis, Matthew Pierson</dc:creator>
    </item>
    <item>
      <title>Long-Time Behaviors of Stochastic Linear-Quadratic Optimal Control Problems</title>
      <link>https://arxiv.org/abs/2409.11633</link>
      <description>arXiv:2409.11633v2 Announce Type: replace 
Abstract: This paper investigates the asymptotic behavior of the solution to a linear-quadratic stochastic optimal control problems. The so-called probability cell problem is introduced the first time. It serves as the probability interpretation of the well-known cell problem in the homogenization of Hamilton-Jacobi equations. By establishing a connection between this problem and the ergodic cost problem, we reveal the turnpike properties of the linear-quadratic stochastic optimal control problems from various perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11633v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiamin Jian, Sixian Jin, Qingshuo Song, Jiongmin Yong</dc:creator>
    </item>
    <item>
      <title>Adaptive Fidelity-Based Density Tracking for Open Quantum Systems</title>
      <link>https://arxiv.org/abs/2410.02882</link>
      <description>arXiv:2410.02882v2 Announce Type: replace 
Abstract: This paper presents an online learning-based adaptive control framework for density-matrix tracking in a two-level Lindblad-Gorini-Kossakowski-Sudarshan (LGKS) quantum system, in which the feedback control law does not require prior knowledge of the system Hamiltonian or dissipative operators. The adaptive controller is based on a continuous-time formulation of retrospective cost adaptive control (RCAC). To preserve the geometric structure of the quantum-state evolution, an adaptive PID controller driven by Uhlmann's fidelity is employed. The proposed approach is validated in numerical simulations for both low-entropy and high-entropy density-tracking tasks, and robustness to measurement noise in the feedback path is investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02882v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jhon Manuel Portella Delgado, Ankit Goel</dc:creator>
    </item>
    <item>
      <title>Sparsity of Quadratically Regularized Optimal Transport: Scalar Case</title>
      <link>https://arxiv.org/abs/2410.03353</link>
      <description>arXiv:2410.03353v2 Announce Type: replace 
Abstract: The quadratically regularized optimal transport problem is empirically known to have sparse solutions: its optimal coupling $\pi_{\varepsilon}$ has sparse support for small regularization parameter $\varepsilon$, in contrast to entropic regularization whose solutions have full support for any $\varepsilon&gt;0$. Focusing on continuous and scalar marginals, we provide the first precise description of this sparsity. Namely, we show that the support of $\pi_{\varepsilon}$ shrinks to the Monge graph at the sharp rate $\varepsilon^{1/3}$. This result is based on a detailed analysis of the dual potential $f_{\varepsilon}$ for small $\varepsilon$. In particular, we prove that $f_{\varepsilon}$ is twice differentiable a.s. and bound the second derivative uniformly in $\varepsilon$, showing that $f_{\varepsilon}$ is uniformly strongly convex. Convergence rates for $f_{\varepsilon}$ and its derivative are also obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03353v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Gonz\'alez-Sanz, Marcel Nutz</dc:creator>
    </item>
    <item>
      <title>A Riemannian approach for PDE-constrained shape optimization over the diffeomorphism group using outer metrics</title>
      <link>https://arxiv.org/abs/2503.22872</link>
      <description>arXiv:2503.22872v3 Announce Type: replace 
Abstract: In this paper, we study the use of outer metrics, in particular Sobolev-type metrics on the diffeomorphism group in the context of PDE-constrained shape optimization. Leveraging the structure of the diffeomorphism group we analyze the connection between the push-forward of a smooth function defined on the diffeomorphism group and the classical shape derivative as an Eulerian semi-derivative. We consider in particular, two predominant examples on PDE-constrained shape optimization. An electric impedance tomography inspired problem, and the optimization of a two-dimensional bridge. These problems are numerically solved using the Riemannian steepest descent method where the descent directions are taken to be the Riemannian gradients associated to various outer metrics. For comparison reasons, we also solve the problem using other previously proposed Riemannian metrics in particular the Steklov-Poincar\'e metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22872v3</guid>
      <category>math.OC</category>
      <category>math.DG</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Estefania Loayza-Romero, Lidiya Pryymak, Kathrin Welker</dc:creator>
    </item>
    <item>
      <title>A potential-theoretic approach to optimal stopping in a spectrally L\'evy Model</title>
      <link>https://arxiv.org/abs/2506.10538</link>
      <description>arXiv:2506.10538v3 Announce Type: replace 
Abstract: We establish a systematic solution method for optimal stopping problems of spectrally negative L\'evy processes. Our approach relies essentially on the potential theory, in particular the Riesz decomposition and the maximum principle. Using these mathematical results, we not only derive necessary and sufficient conditions of optimality for a broad class of reward functions, but also develop a method to tackle general problems in a direct and constructive way (without pre-specifying the solution form). To reinforce the latter point, we provide a step-by-step solution procedure applicable to complex solution structures, including continuation regions with multiple connected components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10538v3</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiko Egami, Tomohiro Koike</dc:creator>
    </item>
    <item>
      <title>A class of singular control problems with tipping points</title>
      <link>https://arxiv.org/abs/2510.16599</link>
      <description>arXiv:2510.16599v2 Announce Type: replace 
Abstract: Tipping points characterize situations where a regulated system may experience a sudden and irreversible change and are generally associated with a random state of the system below which the change materializes. In this paper, we study a singular stochastic control problem in which the performance criterion depends on the hitting time of a random state that is not a stopping time for the reference filtration. We establish a connection between the value of this problem and that of a singular control problem involving a diffusion and its running minimum. We provide a verification lemma that we apply to explicitly solve a resource-extraction problem with an ex-ante unknown tipping point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16599v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Paul D\'ecamps, Fabien Gensbittel, Thomas Mariotti, St\'ephane Villeneuve</dc:creator>
    </item>
    <item>
      <title>Adaptive Multilevel Newton: A Quadratically Convergent Optimization Method</title>
      <link>https://arxiv.org/abs/2510.24967</link>
      <description>arXiv:2510.24967v2 Announce Type: replace 
Abstract: Newton's method may exhibit slower convergence than vanilla Gradient Descent in its initial phase on strongly convex problems. Classical Newton-type multilevel methods mitigate this but, like Gradient Descent, achieve only linear convergence near the minimizer. We introduce an adaptive multilevel Newton-type method with a principled automatic switch to full Newton once its quadratic phase is reached. The local quadratic convergence for strongly convex functions with Lipschitz continuous Hessians and for self-concordant functions is established and confirmed empirically. Although per-iteration cost can exceed that of classical multilevel schemes, the method is efficient and consistently outperforms Newton's method, Gradient Descent, and the multilevel Newton method, indicating that second-order methods can outperform first-order methods even when Newton's method is initially slow. The promising empirical results open new avenues for designing reduced-cost second- and high-order methods with extremely fast convergence rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24967v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick Tsipinakis, Panos Parpas, Matthias Voigt</dc:creator>
    </item>
    <item>
      <title>Structure versus regularity of set-valued maps in convex generalized Nash equilibrium problems in Banach spaces</title>
      <link>https://arxiv.org/abs/2512.12831</link>
      <description>arXiv:2512.12831v2 Announce Type: replace 
Abstract: A generalized Nash equilibrium problem (GNEP) in Banach space consists of $N&gt;1$ optimal control problems with couplings in both the objective functions and, most importantly, in the feasible sets. We address the existence of equilibria for convex GNEPs in Banach space. We show that the standard assumption of lower semicontinuity of the set-valued constraint maps - foundational in the current literature on GNEPs - can be replaced by graph convexity or the so-called Knaster-Kuratowski-Mazurkiewicz (KKM) property. Lower semicontinuity is often essential for obtaining upper semicontinuity of best response maps, crucial for the existence theory based on Kakutani-Fan fixed-point arguments. However, in function spaces or in settings with partial differential equation (PDE) constraints, verifying lower semicontinuity becomes much more challenging (even in convex cases), whereas graph convexity, for example, is often straightforward to check. Our results unify several existence theorems in the literature and clarify the structural role of constraint maps. We also extend Rosen's uniqueness condition to Banach spaces using a multiplier bias framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12831v2</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marcelo Bongarti, Michael Hinterm\"uller</dc:creator>
    </item>
    <item>
      <title>Long-Term Average Impulse and Singular Control of a Growth Model with Two Revenue Sources</title>
      <link>https://arxiv.org/abs/2601.09646</link>
      <description>arXiv:2601.09646v2 Announce Type: replace 
Abstract: This paper analyzes and explicitly solves a class of long-term average impulse control problems and a related class of singular control problems. The underlying process is a general one-dimensional diffusion with appropriate boundary behavior. The model is motivated by applications such as the optimal long-term management of renewable resources and financial portfolio management. A large class of admissible policies is identified over which the agent seeks to maximize her long-term average reward, consisting of a running reward and income from either discrete impulses or singular actions. The long-term expected total reward and its relation to overtaking optimality is also considered. Sensitivity analysis with regard to the parameters of the impulse control model are performed. Key connections between the impulse and singular control problems are displayed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09646v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K. L. Helmes, R. H. Stockbridge, C. Zhu</dc:creator>
    </item>
    <item>
      <title>A Priori Error Estimation of Physics-Informed Neural Networks Solving Allen--Cahn and Cahn--Hilliard Equations</title>
      <link>https://arxiv.org/abs/2402.02667</link>
      <description>arXiv:2402.02667v2 Announce Type: replace-cross 
Abstract: Physics-Informed Neural Networks (PINNs) encounter accuracy limitations when solving the Allen--Cahn (AC) and Cahn--Hilliard (CH) partial differential equations (PDEs). To overcome this, we employ a novel loss function, Residuals-weighted Region Activation Evaluation (Residuals-RAE), featuring a { pre-training weight update scheme}. { Unlike conventional self-adaptive PINNs where weights evolve simultaneously with network parameters, Residuals-RAE-PINNs computes weights from current residuals before each training step and holds them constant during gradient updates. We establish weight convergence under standard neural network optimization assumptions, which justifies analyzing the converged network with constant weights.} Based on this theoretical framework, we derive the error estimation for PINNs with Residuals-RAE when solving AC and CH equations. {The analysis is aligned with Monte-Carlo sampling for the discretization of integrals, consistent with the numerical experiments.} Numerical experiments on one- and two-dimensional AC and CH systems confirm our theoretical results. Additionally, our analysis reveals that feedforward neural networks with two hidden layers and the tanh activation function bound the approximation errors of the solution, its temporal derivative, and the nonlinear term, constrained by the training loss and the number of collocation points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02667v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangtao Zhang, Jiani Lin, Qijia Zhai, Huiyu Yang, Xujun Chen, Ieng Tak Leong, Fang Zhu</dc:creator>
    </item>
    <item>
      <title>A Branch-Price-Cut-And-Switch Approach for Optimizing Team Formation and Routing for Airport Baggage Handling Tasks with Stochastic Travel Times</title>
      <link>https://arxiv.org/abs/2405.20912</link>
      <description>arXiv:2405.20912v4 Announce Type: replace-cross 
Abstract: In airport operations, optimally using dedicated personnel for baggage handling tasks plays a crucial role in the design of resource-efficient processes. Teams of workers with different qualifications must be formed, and loading or unloading tasks must be assigned to them. Each task has a time window within which it can be started and should be finished. Violating these temporal restrictions incurs severe financial penalties for the operator. In practice, various components of this process are subject to uncertainties. We consider the aforementioned problem under the assumption of time-dependent stochastic travel times across the apron. We present two binary program formulations to model the problem at hand and propose a novel solution approach that we call Branch-Price-Cut-and-Switch, in which we dynamically switch between two master problem formulations. Furthermore, we use an exact separation method to identify violated rank-1 Chv\'atal-Gomory cuts and utilize an efficient branching rule relying on task finish times. We test the algorithm on instances generated based on real-world data from a major European hub airport with a planning horizon of up to two hours, 30 flights per hour, and three available task execution modes to choose from. Our results indicate that our algorithm is able to significantly outperform existing solution approaches. Moreover, an explicit consideration of stochastic travel times allows for solutions that utilize the available workforce more efficiently, while simultaneously guaranteeing a stable service level for the baggage handling operator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20912v4</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Hagn, Rainer Kolisch, Giacomo Dall'Olio, Stefan Weltge</dc:creator>
    </item>
    <item>
      <title>Armijo Line-search Can Make (Stochastic) Gradient Descent Provably Faster</title>
      <link>https://arxiv.org/abs/2503.00229</link>
      <description>arXiv:2503.00229v4 Announce Type: replace-cross 
Abstract: Armijo line-search (Armijo-LS) is a standard method to set the step-size for gradient descent (GD). For smooth functions, Armijo-LS alleviates the need to know the global smoothness constant L and adapts to the ``local'' smoothness, enabling GD to converge faster. Existing theoretical analyses show that GD with Armijo-LS (GD-LS) can result in constant factor improvements over GD with a 1/L step-size (denoted as GD(1/L)). We strengthen these results and show that if the objective function satisfies a certain non-uniform smoothness condition, GD-LS can result in a faster convergence rate than GD(1/L). In particular, we prove that for convex objectives corresponding to logistic regression and multi-class classification, GD-LS can converge to the optimum at a linear rate, and hence improves over the sublinear convergence of GD(1/L). Furthermore, for non-convex objectives satisfying gradient domination (e.g., those corresponding to the softmax policy gradient in RL or generalized linear models with a logistic link function), GD-LS can match the fast convergence of algorithms tailored for these specific settings. Finally, we analyze the convergence of stochastic GD with a stochastic line-search on convex losses under the interpolation assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00229v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sharan Vaswani, Reza Babanezhad</dc:creator>
    </item>
    <item>
      <title>Regularity and Stability Properties of Selective SSMs with Discontinuous Gating</title>
      <link>https://arxiv.org/abs/2505.11602</link>
      <description>arXiv:2505.11602v2 Announce Type: replace-cross 
Abstract: Deep selective State-Space Models (SSMs), whose state-space parameters are modulated online by a selection signal, offer significant expressive power but pose challenges for stability analysis, especially under discontinuous gating. We study continuous-time selective SSMs through the lenses of passivity and Input-to-State Stability (ISS), explicitly distinguishing the selection schedule $x(\cdot)$ from the driving (port) input $u(\cdot)$. First, we show that state-strict dissipativity ($\beta&gt;0$) together with quadratic bounds on a storage functional implies exponential decay of homogeneous trajectories ($u\equiv 0$), yielding exponential forgetting. Second, by freezing the selection ($x(t)\equiv 0$) we obtain a passive LTV input-output subsystem and prove that its minimal available storage is necessarily quadratic, $V_{a,0}(t,h)=\tfrac{1}{2}h^H Q_0(t)h,$ with $Q_0 \in \mathrm{AUC}_{\mathrm{loc}}$, accommodating discontinuities induced by gating. Third, under the strong hypothesis that a single quadratic storage certifies passivity uniformly over all admissible selection schedules, we derive a parametric LMI and universal kernel constraints on gating, formalizing an "irreversible forgetting" structure. Finally, we give sufficient conditions for global ISS with respect to the port input $u(\cdot)$, uniformly over admissible selection schedules, and we validate the main predictions in targeted simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11602v2</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikola Zubi\'c, Davide Scaramuzza</dc:creator>
    </item>
    <item>
      <title>Cautious Weight Decay</title>
      <link>https://arxiv.org/abs/2510.12402</link>
      <description>arXiv:2510.12402v2 Announce Type: replace-cross 
Abstract: We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic modification that applies weight decay only to parameter coordinates whose signs align with the optimizer update. Unlike standard decoupled decay, which implicitly optimizes a regularized or constrained objective, CWD preserves the original loss and admits a bilevel interpretation: it induces sliding-mode behavior upon reaching the stationary manifold, allowing it to search for locally Pareto-optimal stationary points of the unmodified objective. In practice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon, requiring no new hyperparameters or additional tuning. For language model pre-training and ImageNet classification, CWD consistently improves final loss and accuracy at million- to billion-parameter scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12402v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lizhang Chen, Jonathan Li, Kaizhao Liang, Baiyu Su, Cong Xie, Nuo Wang Pierse, Chen Liang, Ni Lao, Qiang Liu</dc:creator>
    </item>
  </channel>
</rss>
