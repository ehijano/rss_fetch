<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Feb 2025 03:04:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Policy Learning with Confidence</title>
      <link>https://arxiv.org/abs/2502.10653</link>
      <description>arXiv:2502.10653v1 Announce Type: new 
Abstract: This paper proposes a framework for selecting policies that maximize expected benefit in the presence of estimation uncertainty, by controlling for estimation risk and incorporating risk aversion. The proposed method explicitly balances the size of the estimated benefit against the uncertainty inherent in its estimation, ensuring that chosen policies meet a reporting guarantee, namely that the actual benefit of the implemented policy is guaranteed not to fall below the reported estimate with a pre-specified confidence level. This approach applies to a variety of settings, including the selection of policy rules that allocate individuals to treatments based on observed characteristics, using both experimental and non-experimental data; and the allocation of limited budgets among competing social programs; as well as many others. Across these applications, the framework offers a principled and robust method for making data-driven policy choices under uncertainty. In broader terms, it focuses on policies that are on the efficient decision frontier, describing policies that offer maximum estimated benefit for a given acceptable level of estimation risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10653v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Sokbae Lee, Adam M. Rosen, Liyang Sun</dc:creator>
    </item>
    <item>
      <title>Maximal Inequalities for Separately Exchangeable Empirical Processes</title>
      <link>https://arxiv.org/abs/2502.11432</link>
      <description>arXiv:2502.11432v1 Announce Type: new 
Abstract: This paper derives new maximal inequalities for empirical processes associated with separately exchangeable (SE) random arrays. For any fixed index dimension \(K\ge 1\), we establish a global maximal inequality that bounds the \(q\)-th moment, for any \(q\in[1,\infty)\), of the supremum of these processes. In addition, we obtain a refined local maximal inequality that controls the first absolute moment of the supremum. Both results are proved for a general pointwise measurable class of functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11432v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harold D. Chiang</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Qualitative Outcomes</title>
      <link>https://arxiv.org/abs/2502.11691</link>
      <description>arXiv:2502.11691v1 Announce Type: new 
Abstract: Causal inference methods such as instrumental variables, regression discontinuity, and difference-in-differences are widely used to estimate treatment effects. However, their application to qualitative outcomes poses fundamental challenges, as standard causal estimands are ill-defined in this context. This paper highlights these issues and introduces an alternative framework that focuses on well-defined and interpretable estimands that quantify how treatment affects the probability distribution over outcome categories. We establish that standard identification assumptions are sufficient for identification and propose simple, intuitive estimation strategies that remain fully compatible with conventional econometric methods. To facilitate implementation, we provide an open-source R package, $\texttt{causalQual}$, which is publicly available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11691v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Di Francesco, Giovanni Mellace</dc:creator>
    </item>
    <item>
      <title>Regression Modeling of the Count Relational Data with Exchangeable Dependencies</title>
      <link>https://arxiv.org/abs/2502.11255</link>
      <description>arXiv:2502.11255v1 Announce Type: cross 
Abstract: Relational data characterized by directed edges with count measurements are common in social science. Most existing methods either assume the count edges are derived from continuous random variables or model the edge dependency by parametric distributions. In this paper, we develop a latent multiplicative Poisson model for relational data with count edges. Our approach directly models the edge dependency of count data by the pairwise dependence of latent errors, which are assumed to be weakly exchangeable. This assumption not only covers a variety of common network effects, but also leads to a concise representation of the error covariance. In addition, the identification and inference of the mean structure, as well as the regression coefficients, depend on the errors only through their covariance. Such a formulation provides substantial flexibility for our model. Based on this, we propose a pseudo-likelihood based estimator for the regression coefficients, demonstrating its consistency and asymptotic normality. The newly suggested method is applied to a food-sharing network, revealing interesting network effects in gift exchange behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11255v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqin Du, Bailey K. Fosdick, Wen Zhou</dc:creator>
    </item>
    <item>
      <title>Network regression and supervised centrality estimation</title>
      <link>https://arxiv.org/abs/2111.12921</link>
      <description>arXiv:2111.12921v2 Announce Type: replace 
Abstract: The centrality in a network is often used to measure nodes' importance and model network effects on a certain outcome. Empirical studies widely adopt a two-stage procedure, which first estimates the centrality from the observed noisy network and then infers the network effect from the estimated centrality, even though it lacks theoretical understanding. We propose a unified modeling framework to study the properties of centrality estimation and inference and the subsequent network regression analysis with noisy network observations. Furthermore, we propose a supervised centrality estimation methodology, which aims to simultaneously estimate both centrality and network effect. We showcase the advantages of our method compared with the two-stage method both theoretically and numerically via extensive simulations and a case study in predicting currency risk premiums from the global trade network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.12921v2</guid>
      <category>econ.EM</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junhui Cai, Ran Chen, Dan Yang, Wu Zhu, Haipeng Shen, Linda Zhao</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Fisher Market Equilibrium</title>
      <link>https://arxiv.org/abs/2209.15422</link>
      <description>arXiv:2209.15422v3 Announce Type: replace 
Abstract: Statistical inference under market equilibrium effects has attracted increasing attention recently. In this paper we focus on the specific case of linear Fisher markets. They have been widely use in fair resource allocation of food/blood donations and budget management in large-scale Internet ad auctions. In resource allocation, it is crucial to quantify the variability of the resource received by the agents (such as blood banks and food banks) in addition to fairness and efficiency properties of the systems. For ad auction markets, it is important to establish statistical properties of the platform's revenues in addition to their expected values. To this end, we propose a statistical framework based on the concept of infinite-dimensional Fisher markets. In our framework, we observe a market formed by a finite number of items sampled from an underlying distribution (the "observed market") and aim to infer several important equilibrium quantities of the underlying long-run market. These equilibrium quantities include individual utilities, social welfare, and pacing multipliers. Through the lens of sample average approximation (SSA), we derive a collection of statistical results and show that the observed market provides useful statistical information of the long-run market. In other words, the equilibrium quantities of the observed market converge to the true ones of the long-run market with strong statistical guarantees. These include consistency, finite sample bounds, asymptotics, and confidence. As an extension, we discuss revenue inference in quasilinear Fisher markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.15422v3</guid>
      <category>econ.EM</category>
      <category>cs.GT</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Yuan Gao, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>Just Ask Them Twice: Choice Probabilities and Identification of Ex ante returns and Willingness-To-Pay</title>
      <link>https://arxiv.org/abs/2303.03009</link>
      <description>arXiv:2303.03009v3 Announce Type: replace 
Abstract: One of the exciting developments in the stated preference literature is the use of probabilistic stated preference experiments to estimate semi-parametric population distributions of ex ante returns and WTP for a choice attribute. This relies on eliciting several choices per individual, and estimating separate demand functions, at the cost of possibly long survey instruments. This paper shows that the distributions of interest can be recovered from at most two stated choices, without requiring ad-hoc parametric assumptions. Hence, it allows for significantly shorter survey instruments. The paper also shows that eliciting probabilistic stated choices allows identifying much richer objects than we have done so far, and therefore, provides better tools for ex ante policy evaluation. Finally, it showcases the feasibility and relevance of the results by studying the preference of high ability students in Cote d'Ivoire for public sector jobs exploiting a unique survey on this population. Our analysis supports the claim that public sector jobs might significantly increase the cost of hiring elite students for the private sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03009v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romuald Meango, Esther Mirjam Girsberger</dc:creator>
    </item>
    <item>
      <title>Don't (fully) exclude me, it's not necessary! Causal inference with semi-IVs</title>
      <link>https://arxiv.org/abs/2303.12667</link>
      <description>arXiv:2303.12667v3 Announce Type: replace 
Abstract: This paper proposes semi-instrumental variables (semi-IVs) as a practical alternative to instrumental variables (IVs) to identify the causal effect of a binary (or discrete) endogenous treatment. A semi-IV is a less restrictive form of instrument: it affects the selection into treatment but is excluded only from one, not necessarily both, potential outcomes. Having two semi-IVs, one excluded from the potential outcome under treatment and the other from the potential outcome under control, is sufficient to nonparametrically point identify local average treatment effect (LATE) and marginal treatment effect (MTE) parameters. In practice, semi-IVs provide a solution to the challenge of finding valid IVs because they are easier to find: most selection-specific shocks, policies, costs, or benefits are valid semi-IVs. As an application, I estimate the returns to working in the manufacturing sector using sector-specific semi-IVs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.12667v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bruneel-Zupanc</dc:creator>
    </item>
    <item>
      <title>On the Efficiency of Finely Stratified Experiments</title>
      <link>https://arxiv.org/abs/2307.15181</link>
      <description>arXiv:2307.15181v5 Announce Type: replace 
Abstract: This paper studies the use of finely stratified designs for the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments. By a "finely stratified" design, we mean experiments in which units are divided into groups of a fixed size and a proportion within each group is assigned to a binary treatment uniformly at random. The class of parameters considered are those that can be expressed as the solution to a set of moment conditions constructed using a known function of the observed data. They include, among other things, average treatment effects, quantile treatment effects, and local average treatment effects as well as the counterparts to these quantities in experiments in which the unit is itself a cluster. In this setting, we establish three results. First, we show that under a finely stratified design, the na\"ive method of moments estimator achieves the same asymptotic variance as what could typically be attained under alternative treatment assignment mechanisms only through ex post covariate adjustment. Second, we argue that the na\"ive method of moments estimator under a finely stratified design is asymptotically efficient by deriving a lower bound on the asymptotic variance of regular estimators of the parameter of interest in the form of a convolution theorem. Finally, we establish conditions under which a "fast-balancing" property of finely stratified designs is in fact necessary for the na\"ive method of moments estimator to attain the efficiency bound. In this sense, finely stratified experiments are attractive because they lead to efficient estimators of treatment effect parameters "by design."</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15181v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Jizhou Liu, Azeem M. Shaikh, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>The modified conditional sum-of-squares estimator for fractionally integrated models</title>
      <link>https://arxiv.org/abs/2404.12882</link>
      <description>arXiv:2404.12882v2 Announce Type: replace 
Abstract: In this paper, we analyse the influence of estimating a constant term on the bias of the conditional sum-of-squares (CSS) estimator in a stationary or non-stationary type-II ARFIMA ($p_1$,$d$,$p_2$) model. We derive expressions for the estimator's bias and show that the leading term can be easily removed by a simple modification of the CSS objective function. We call this new estimator the modified conditional sum-of-squares (MCSS) estimator. We show theoretically and by means of Monte Carlo simulations that its performance relative to that of the CSS estimator is markedly improved even for small sample sizes. Finally, we revisit three classical short datasets that have in the past been described by ARFIMA($p_1$,$d$,$p_2$) models with constant term, namely the post-second World War real GNP data, the extended Nelson-Plosser data, and the Nile data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12882v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mustafa R. K{\i}l{\i}n\c{c}, Michael Massmann</dc:creator>
    </item>
    <item>
      <title>Estimation of Integrated Volatility Functionals with Kernel Spot Volatility Estimators</title>
      <link>https://arxiv.org/abs/2407.09759</link>
      <description>arXiv:2407.09759v2 Announce Type: replace 
Abstract: For a multidimensional It\^o semimartingale, we consider the problem of estimating integrated volatility functionals. Jacod and Rosenbaum (2013) studied a plug-in type of estimator based on a Riemann sum approximation of the integrated functional and a spot volatility estimator with a forward uniform kernel. Motivated by recent results that show that spot volatility estimators with general two-side kernels of unbounded support are more accurate, in this paper, an estimator using a general kernel spot volatility estimator as the plug-in is considered. A biased central limit theorem for estimating the integrated functional is established with an optimal convergence rate. Unbiased central limit theorems for estimators with proper de-biasing terms are also obtained both at the optimal convergence regime for the bandwidth and when applying undersmoothing. Our results show that one can significantly reduce the estimator's bias by adopting a general kernel instead of the standard uniform kernel. Our proposed bias-corrected estimators are found to maintain remarkable robustness against bandwidth selection in a variety of sampling frequencies and functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09759v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e E. Figueroa-L\'opez, Jincheng Pang, Bei Wu</dc:creator>
    </item>
  </channel>
</rss>
