<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Jan 2025 05:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the reconstruction limits of complex networks</title>
      <link>https://arxiv.org/abs/2501.01437</link>
      <description>arXiv:2501.01437v1 Announce Type: new 
Abstract: Network reconstruction consists in retrieving the -- hidden -- interaction structure of a system from empirical observations such as time series. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. To this end, we adopt an information-theoretical point of view and define the reconstructability -- the fraction of structural information recoverable from data. The reconstructability depends on the true data generating model which is shown to set the reconstruction limit, i.e., the performance upper bound for all algorithms. We show that the reconstructability is related to various performance measures, such as the probability of error and the Jaccard similarity. In an empirical context where the true data generating model is unknown, we introduce the reconstruction index as an approximation of the reconstructability. We find that performing model selection is crucial for the validity of the reconstruction index as a proxy of the reconstructability, and illustrate how it assesses the reconstruction limit of empirical time series and networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01437v1</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Murphy, Simon Lizotte, Fran\c{c}ois Thibault, Vincent Thibeault, Patrick Desrosiers, Antoine Allard</dc:creator>
    </item>
    <item>
      <title>Multi-view Bayesian optimisation in reduced dimension for engineering design</title>
      <link>https://arxiv.org/abs/2501.01552</link>
      <description>arXiv:2501.01552v1 Announce Type: new 
Abstract: Bayesian optimisation is an adaptive sampling strategy for constructing a Gaussian process surrogate to emulate a black-box computational model with the aim of efficiently searching for the global minimum. However, Gaussian processes have limited applicability for engineering problems with many design variables. Their scalability can be significantly improved by identifying a low-dimensional vector of latent variables that serve as inputs to the Gaussian process. In this paper, we introduce a multi-view learning strategy that considers both the input design variables and output data representing the objective or constraint functions, to identify a low-dimensional space of latent variables. Adopting a fully probabilistic viewpoint, we use probabilistic partial least squares (PPLS) to learn an orthogonal mapping from the design variables to the latent variables using training data consisting of inputs and outputs of the black-box computational model. The latent variables and posterior probability densities of the probabilistic partial least squares and Gaussian process models are determined sequentially and iteratively, with retraining occurring at each adaptive sampling iteration. We compare the proposed probabilistic partial least squares Bayesian optimisation (PPLS-BO) strategy to its deterministic counterpart, partial least squares Bayesian optimisation (PLS-BO), and classical Bayesian optimisation, demonstrating significant improvements in convergence to the global minimum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01552v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas A. Archbold, Ieva Kazlauskaite, Fehmi Cirak</dc:creator>
    </item>
    <item>
      <title>Grid-level impacts of renewable energy on thermal generation: efficiency, emissions and flexibility</title>
      <link>https://arxiv.org/abs/2501.01954</link>
      <description>arXiv:2501.01954v1 Announce Type: new 
Abstract: Wind and solar generation constitute an increasing share of electricity supply globally. We find that this leas to shifts in the operational dynamics of thermal power plants. Using fixed effects panel regression across seven major U.S. balancing authorities, we analyze the impact of renewable generation on coal, natural gas combined cycle plants, and natural gas combustion turbines. Wind generation consistently displaces thermal output, while effects from solar vary significantly by region, achieving substantial displacement in areas with high solar penetration such as the California Independent System Operator but limited impacts in coal reliant grids such as the Midcontinent Independent System Operator. Renewable energy sources effectively reduce carbon dioxide emissions in regions with flexible thermal plants, achieving displacement effectiveness as high as one hundred and two percent in the California Independent System Operator and the Electric Reliability Council of Texas. However, in coal heavy areas such as the Midcontinent Independent System Operator and the Pennsylvania New Jersey Maryland Interconnection, inefficiencies from ramping and cycling reduce carbon dioxide displacement to as low as seventeen percent and often lead to elevated nitrogen oxides and sulfur dioxide emissions. These findings underscore the critical role of grid design, fuel mix, and operational flexibility in shaping the emissions benefits of renewables. Targeted interventions, including retrofitting high emitting plants and deploying energy storage, are essential to maximize emissions reductions and support the decarbonization of electricity systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01954v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruv Suri, Jacques de Chalendar, Ines Azevedo</dc:creator>
    </item>
    <item>
      <title>Integrative Learning of Intensity Fluctuations of Quantum Dots under Excitation via a Tailored Mixture Hidden Markov Model</title>
      <link>https://arxiv.org/abs/2501.01292</link>
      <description>arXiv:2501.01292v1 Announce Type: cross 
Abstract: Semiconductor nano-crystals, known as quantum dots (QDs), have garnered significant interest in various scientific fields due to their unique fluorescence properties. One captivating characteristic of QDs is their ability to emit photons under continuous excitation. The intensity of photon emission fluctuates during the excitation, and such a fluctuation pattern can vary across different dots even under the same experimental conditions. What adding to the complication is that the processed intensity series are non-Gaussian and truncated due to necessary thresholding and normalization. As such, conventional approaches in the chemistry literature, typified by single-dot analysis of raw intensity data with Gaussian hidden Markov models (HMM), cannot meet the many analytical challenges and may fail to capture any novel yet rare fluctuation patterns among QDs. Collaborating with scientists in the chemistry field, we have developed an integrative learning approach to simultaneously analyzing intensity series of multiple QDs. Our approach still inherits the HMM as the skeleton to model the intensity fluctuations of each dot, and based on the data structure and the hypothesized collective behaviors of the QDs, our approach asserts that (i) under each hidden state, the normalized intensity follows a 0/1 inflated Beta distribution, (ii) the state distributions are shared across all the QDs, and (iii) the patterns of transitions can vary across QDs. These unique features allow for a precise characterization of the intensity fluctuation patterns and facilitate the clustering of the QDs. With experimental data collected on 128 QDs, our methods reveal several QD clusters characterized by unique transition patterns across three intensity states. The results provide deeper insight into QD behaviors and their design/application potentials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01292v1</guid>
      <category>physics.app-ph</category>
      <category>cond-mat.mes-hall</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Yang, Hawi Nyiera, Yonglei Sun, Jing Zhao, Kun Chen</dc:creator>
    </item>
    <item>
      <title>Analyzing Country-Level Vaccination Rates and Determinants of Practical Capacity to Administer COVID-19 Vaccines</title>
      <link>https://arxiv.org/abs/2501.01447</link>
      <description>arXiv:2501.01447v1 Announce Type: cross 
Abstract: The COVID-19 vaccine development, manufacturing, transportation, and administration proved an extreme logistics operation of global magnitude. Global vaccination levels, however, remain a key concern in preventing the emergence of new strains and minimizing the impact of the pandemic's disruption of daily life. In this paper, country-level vaccination rates are analyzed through a queuing framework to extract service rates that represent the practical capacity of a country to administer vaccines. These rates are further characterized through regression and interpretable machine learning methods with country-level demographic, governmental, and socio-economic variates. Model results show that participation in multi-governmental collaborations such as COVAX may improve the ability to vaccinate. Similarly, improved transportation and accessibility variates such as roads per area for low-income countries and rail lines per area for high-income countries can improve rates. It was also found that for low-income countries specifically, improvements in basic and health infrastructure (as measured through spending on healthcare, number of doctors and hospital beds per 100k, population percent with access to electricity, life expectancy, and vehicles per 1000 people) resulted in higher vaccination rates. Of the high-income countries, those with larger 65-plus populations struggled to vaccinate at high rates, indicating potential accessibility issues for the elderly. This study finds that improving basic and health infrastructure, focusing on accessibility in the last mile, particularly for the elderly, and fostering global partnerships can improve logistical operations of such a scale. Such structural impediments and inequities in global health care must be addressed in preparation for future global public health crises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01447v1</guid>
      <category>econ.GN</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sharika J. Hegde, Max T. M. Ng, Marcos Rios, Hani S. Mahmassani, Ying Chen, Karen Smilowitz</dc:creator>
    </item>
    <item>
      <title>Bangladesh's Amplified Coastal Storm Tide Hazard from Tropical Cyclones and Rising Sea Levels in a Warming Climate</title>
      <link>https://arxiv.org/abs/2312.06051</link>
      <description>arXiv:2312.06051v2 Announce Type: replace-cross 
Abstract: The risk of extreme storm tides to Bangladesh's low-lying and densely populated coastal regions, already vulnerable to tropical cyclones, remains poorly quantified under a warming climate. Here, using a statistical-physical downscaling approach, our multimodel large-ensemble projections under the IPCC6 SSP2-4.5, SSP3-7.0, and SSP5-8.5 scenarios show that Bangladesh's 100-year storm tide will likely intensify from 3.5 m to between 4.9 m and 5.4 m by the end of the 21st century. The Meghna-North Chattogram region is the most vulnerable, and the storm tide season will broaden significantly, amplifying the strongest during the late monsoon and late post-monsoon seasons. We project substantial increases in seasonal storm tide frequencies, with a four-fold increase in back-to-back extremes in the post-monsoon season. Across the SSP2-4.5, SSP3-7.0, and SSP5-8.5 scenarios assessed using multiple climate models, the frequency of storm tide from destructive cyclones like Bhola and Gorky will significantly increase by 7-18 times and 6-23 times, respectively. Our study indicates a need to re-examine the ongoing coastal improvement and heighten the urgency to enhance coastal resilience in Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06051v2</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangchao Qiu, Sai Ravela, Kerry Emanuel</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Heterogeneity and Importance Measures for Multivariate Continuous Treatments</title>
      <link>https://arxiv.org/abs/2404.09126</link>
      <description>arXiv:2404.09126v2 Announce Type: replace-cross 
Abstract: Estimating the joint effect of a multivariate, continuous exposure is crucial, particularly in environmental health where interest lies in simultaneously evaluating the impact of multiple environmental pollutants on health. We develop novel methodology that addresses two key issues for estimation of treatment effects of multivariate, continuous exposures. We use nonparametric Bayesian methodology that is flexible to ensure our approach can capture a wide range of data generating processes. Additionally, we allow the effect of the exposures to be heterogeneous with respect to covariates. Treatment effect heterogeneity has not been well explored in the causal inference literature for multivariate, continuous exposures, and therefore we introduce novel estimands that summarize the nature and extent of the heterogeneity, and propose estimation procedures for new estimands related to treatment effect heterogeneity. We provide theoretical support for the proposed models in the form of posterior contraction rates and show that it works well in simulated examples both with and without heterogeneity. Our approach is motivated by a study of the health effects of simultaneous exposure to the components of PM$_{2.5}$, where we find that the negative health effects of exposure to environmental pollutants are exacerbated by low socioeconomic status, race and age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09126v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejun Shin, Antonio Linero, Michelle Audirac, Kezia Irene, Danielle Braun, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>The VIX as Stochastic Volatility for Corporate Bonds</title>
      <link>https://arxiv.org/abs/2410.22498</link>
      <description>arXiv:2410.22498v5 Announce Type: replace-cross 
Abstract: Classic stochastic volatility models assume volatility is unobservable. We use the Volatility Index: S&amp;P 500 VIX to observe it, to easier fit the model. We apply it to corporate bonds. We fit autoregression for corporate rates and for risk spreads between these rates and Treasury rates. Next, we divide residuals by VIX. Our main idea is such division makes residuals closer to the ideal case of a Gaussian white noise. This is remarkable, since these residuals and VIX come from separate market segments. Similarly, we model corporate bond returns as a linear function of rates and rate changes. Our article has two main parts: Moody's AAA and BAA spreads; Bank of America investment-grade and high-yield rates, spreads, and returns. We analyze long-term stability of these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22498v5</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihyun Park, Andrey Sarantsev</dc:creator>
    </item>
    <item>
      <title>Robust Market Interventions</title>
      <link>https://arxiv.org/abs/2411.03026</link>
      <description>arXiv:2411.03026v2 Announce Type: replace-cross 
Abstract: When can interventions in markets be designed to increase surplus robustly -- i.e., with high probability -- accounting for uncertainty due to imprecise information about economic primitives? In a setting with many strategic firms, each possessing some market power, we present conditions for such interventions to exist. The key condition, recoverable structure, requires large-scale complementarities among families of products. The analysis works by decomposing the incidence of interventions in terms of principal components of a Slutsky matrix. Under recoverable structure, a noisy signal of this matrix reveals enough about these principal components to design robust interventions. Our results demonstrate the usefulness of spectral methods for analyzing imperfectly observed strategic interactions with many agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03026v2</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Galeotti, Benjamin Golub, Sanjeev Goyal, Eduard Talam\`as, Omer Tamuz</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v4 Announce Type: replace-cross 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
  </channel>
</rss>
