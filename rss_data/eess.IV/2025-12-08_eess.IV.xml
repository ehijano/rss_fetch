<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Dec 2025 03:41:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Two-Stage Camera Calibration Method for Multi-Camera Systems Using Scene Geometry</title>
      <link>https://arxiv.org/abs/2512.05171</link>
      <description>arXiv:2512.05171v1 Announce Type: new 
Abstract: Calibration of multi-camera systems is a key task for accurate object tracking. However, it remains a challenging problem in real-world conditions, where traditional methods are not applicable due to the lack of accurate floor plans, physical access to place calibration patterns, or synchronized video streams. This paper presents a novel two-stage calibration method that overcomes these limitations. In the first stage, partial calibration of individual cameras is performed based on an operator's annotation of natural geometric primitives (parallel, perpendicular, and vertical lines, or line segments of equal length). This allows estimating key parameters (roll, pitch, focal length) and projecting the camera's Effective Field of View (EFOV) onto the horizontal plane in a base 3D coordinate system. In the second stage, precise system calibration is achieved through interactive manipulation of the projected EFOV polygons. The operator adjusts their position, scale, and rotation to align them with the floor plan or, in its absence, using virtual calibration elements projected onto all cameras in the system. This determines the remaining extrinsic parameters (camera position and yaw). Calibration requires only a static image from each camera, eliminating the need for physical access or synchronized video. The method is implemented as a practical web service. Comparative analysis and demonstration videos confirm the method's applicability, accuracy, and flexibility, enabling the deployment of precise multi-camera tracking systems in scenarios previously considered infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05171v1</guid>
      <category>eess.IV</category>
      <category>cs.RO</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksandr Abramov</dc:creator>
    </item>
    <item>
      <title>CATNUS: Coordinate-Aware Thalamic Nuclei Segmentation Using T1-Weighted MRI</title>
      <link>https://arxiv.org/abs/2512.05329</link>
      <description>arXiv:2512.05329v1 Announce Type: new 
Abstract: Accurate segmentation of thalamic nuclei from magnetic resonance images is important due to the distinct roles of these nuclei in overall brain function and to their differential involvement in neurological and psychiatric disorders. However, segmentation remains challenging given the small size of many nuclei, limited intrathalamic contrast and image resolution, and inter-subject anatomical variability. In this work, we present CATNUS (Coordinate-Aware Thalamic Nuclei Segmentation), segmenting 13 thalamic nuclei (or nuclear groups) using a 3D U-Net architecture enhanced with coordinate convolution layers, which provide more precise localization of both large and small nuclei. To support broad clinical applicability, we provide pre-trained model variants that can operate on quantitative T1 maps as well as on widely used magnetization-prepared rapid gradient echo (MPRAGE) and fast gray matter acquisition T1 inversion recovery (FGATIR) sequences. We benchmarked CATNUS against established methods, including FreeSurfer, THOMAS and HIPS-THOMAS, demonstrating improved segmentation accuracy and robust test-retest reliability across multiple nuclei. Furthermore, CATNUS demonstrated strong out-of-distribution generalization on traveling-subject datasets spanning multiple scanners, field strengths, and vendors, producing reliable and anatomically coherent segmentations across diverse acquisition conditions. Overall, CATNUS provides an accurate and generalizable solution for thalamic nuclei segmentation, with strong potential to facilitate large-scale neuroimaging studies and support real-world clinical assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05329v1</guid>
      <category>eess.IV</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anqi Feng, Zhangxing Bian, Samuel W. Remedios, Savannah P. Hays, Blake E. Dewey, Alexa Colinco, Jiachen Zhuo, Dan Benjamini, Jerry L. Prince</dc:creator>
    </item>
    <item>
      <title>Image Semantic Communication with Quadtree Partition-based Coding</title>
      <link>https://arxiv.org/abs/2512.05395</link>
      <description>arXiv:2512.05395v1 Announce Type: new 
Abstract: Deep learning based semantic communication (DeepSC) system has emerged as a promising paradigm for efficient wireless transmission. However, existing image DeepSC methods, frequently encounter challenges in balancing rate-distortion performance and computational complexity, and often exhibit inferior performance compared to traditional schemes, especially on high-resolution datasets. To address these limitations, we propose a novel image DeepSC system, using quadtree partition-based joint semantic-channel coding, named Quad-DeepSC, which maintains low complexity while achieving state-of-the-art transmission performance. Based on maturing learned image compression technologies, we establish a unified DeepSC system design and training pipeline. The proposed Quad-DeepSC integrates quadtree partition-based entropy estimation and feature coding modules with lightweight feature extraction and reconstruction networks to form an end-to-end architecture. During training, all components except the feature coding modules are jointly optimized as a compact learned image codec, Quad-LIC, for source compression tasks. The pretrained Quad-LIC is then embedded into Quad-DeepSC and fine-tuned end-to-end over wireless channels. Extensive experimental results demonstrate that Quad-DeepSC is the first DeepSC system to surpass conventional communication systems, which employ VTM for source coding and adopt the optimal MCS index under 3GPP standards for channel coding and digital modulation, in performance across datasets of varying resolutions. Notably, both Quad-DeepSC and Quad-LIC exhibit minimal latency, rendering them well-suited for deployment in real-time wireless communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05395v1</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinhuan Huang, Zhijin Qin</dc:creator>
    </item>
    <item>
      <title>General and Domain-Specific Zero-shot Detection of Generated Images via Conditional Likelihood</title>
      <link>https://arxiv.org/abs/2512.05590</link>
      <description>arXiv:2512.05590v2 Announce Type: new 
Abstract: The rapid advancement of generative models, particularly diffusion-based methods, has significantly improved the realism of synthetic images. As new generative models continuously emerge, detecting generated images remains a critical challenge. While fully supervised, and few-shot methods have been proposed, maintaining an updated dataset is time-consuming and challenging. Consequently, zero-shot methods have gained increasing attention in recent years. We find that existing zero-shot methods often struggle to adapt to specific image domains, such as artistic images, limiting their real-world applicability. In this work, we introduce CLIDE, a novel zero-shot detection method based on conditional likelihood approximation. Our approach computes likelihoods conditioned on real images, enabling adaptation across diverse image domains. We extensively evaluate CLIDE, demonstrating SOTA performance on a large-scale general dataset and significantly outperform existing methods in domain-specific cases. These results demonstrate the robustness of our method and underscore the need of broad, domain-aware generalization for the AI-generated image detection task. Code is available at https://tinyurl.com/clide-detector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05590v2</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Roy Betser, Omer Hofman, Roman Vainshtein, Guy Gilboa</dc:creator>
    </item>
    <item>
      <title>ARCAS: An Augmented Reality Collision Avoidance System with SLAM-Based Tracking for Enhancing VRU Safety</title>
      <link>https://arxiv.org/abs/2512.05299</link>
      <description>arXiv:2512.05299v1 Announce Type: cross 
Abstract: Vulnerable road users (VRUs) face high collision risks in mixed traffic, yet most existing safety systems prioritize driver or vehicle assistance over direct VRU support. This paper presents ARCAS, a real-time augmented reality collision avoidance system that provides personalized spatial alerts to VRUs via wearable AR headsets. By fusing roadside 360-degree 3D LiDAR with SLAM-based headset tracking and an automatic 3D calibration procedure, ARCAS accurately overlays world-locked 3D bounding boxes and directional arrows onto approaching hazards in the user's passthrough view. The system also enables multi-headset coordination through shared world anchoring. Evaluated in real-world pedestrian interactions with e-scooters and vehicles (180 trials), ARCAS nearly doubled pedestrians' time-to-collision and increased counterparts' reaction margins by up to 4x compared to unaided-eye conditions. Results validate the feasibility and effectiveness of LiDAR-driven AR guidance and highlight the potential of wearable AR as a promising next-generation safety tool for urban mobility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05299v1</guid>
      <category>eess.SY</category>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.IV</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmad Yehia, Jiseop Byeon, Tianyi Wang, Huihai Wang, Yiming Xu, Junfeng Jiao, Christian Claudel</dc:creator>
    </item>
    <item>
      <title>Convergent Primal-Dual Plug-and-Play Image Restoration: A General Algorithm and Applications</title>
      <link>https://arxiv.org/abs/2501.03780</link>
      <description>arXiv:2501.03780v4 Announce Type: replace 
Abstract: We propose a general deep plug-and-play (PnP) algorithm with a theoretical convergence guarantee. PnP strategies have demonstrated outstanding performance in various image restoration tasks by exploiting the powerful priors underlying Gaussian denoisers. However, existing PnP methods often lack theoretical convergence guarantees under realistic assumptions due to their ad-hoc nature, resulting in inconsistent behavior. Moreover, even when convergence guarantees are provided, they are typically designed for specific settings or require a considerable computational cost in handling non-quadratic data-fidelity terms and additional constraints, which are key components in many image restoration scenarios. To tackle these challenges, we integrate the PnP paradigm with primal-dual splitting (PDS), an efficient proximal splitting methodology for solving a wide range of convex optimization problems, and develop a general convergent PnP framework. Specifically, we establish theoretical conditions for the convergence of the proposed PnP algorithm under a reasonable assumption. Furthermore, we show that the problem solved by the proposed PnP algorithm is not a standard convex optimization problem but a more general monotone inclusion problem, where we provide a mathematical representation of the solution set. Our approach efficiently handles a broad class of image restoration problems with guaranteed theoretical convergence. Numerical experiments on specific image restoration tasks validate the practicality and effectiveness of our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03780v4</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yodai Suzuki, Ryosuke Isono, Shunsuke Ono</dc:creator>
    </item>
    <item>
      <title>A robot-assisted pipeline to rapidly scan 1.7 million historical aerial photographs</title>
      <link>https://arxiv.org/abs/2503.24063</link>
      <description>arXiv:2503.24063v3 Announce Type: replace 
Abstract: During the 20th Century, aerial surveys captured hundreds of millions of high-resolution photographs of the earth's surface. These images, the precursors to modern satellite imagery, represent an extraordinary visual record of the environmental and social upheavals of the 20th Century. However, most of these images currently languish in physical archives where retrieval is difficult and costly. Digitization could revolutionize access, but manual scanning is slow and expensive. Automated scanning could make at-scale digitization feasible, unlocking this visual record of the 20th Century for the digital era. Here, we describe and validate a novel robot-assisted pipeline that increases worker productivity in scanning 30-fold, applied at scale to digitize an archive of 1.7 million historical aerial photographs from 65 countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24063v3</guid>
      <category>eess.IV</category>
      <category>cs.SY</category>
      <category>econ.GN</category>
      <category>eess.SY</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheila Masson, Alan Potts, Allan Williams, Steve Berggreen, Kevin McLaren, Sam Martin, Eugenio Noda, Nicklas Nordfors, Nic Ruecroft, Hannah Druckenmiller, Solomon Hsiang, Andreas Madestam, Anna Tompsett</dc:creator>
    </item>
    <item>
      <title>LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight</title>
      <link>https://arxiv.org/abs/2504.20454</link>
      <description>arXiv:2504.20454v2 Announce Type: replace 
Abstract: This study integrates PET metabolic information with CT anatomical structures to establish a 3D multimodal segmentation dataset for lymphoma based on whole-body FDG PET/CT examinations, which bridges the gap of the lack of standardised multimodal segmentation datasets in the field of haematological malignancies. We retrospectively collected 483 examination datasets acquired between March 2011 and May 2024, involving 220 patients (106 non-Hodgkin lymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were rigorously de-identified. Complete 3D structural information was preserved during data acquisition, preprocessing and annotation, and a high-quality dataset was constructed based on the nnUNet format. By systematic technical validation and evaluation of the preprocessing process, annotation quality and automatic segmentation algorithm, the deep learning model trained based on this dataset is verified to achieve accurate segmentation of lymphoma lesions in PET/CT images with high accuracy, good robustness and reproducibility, which proves the applicability and stability of this dataset in accurate segmentation and quantitative analysis. The deep fusion of PET/CT images achieved with this dataset not only significantly improves the accurate portrayal of the morphology, location and metabolic features of tumour lesions, but also provides solid data support for early diagnosis, clinical staging and personalized treatment, and promotes the development of automated image segmentation and precision medicine based on deep learning. The dataset and related resources are available at https://github.com/SuperD0122/LymphAtlas-.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20454v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiajun Ding, Beiyao Zhu, Xiaosheng Liu, Lishen Zhang, Zhao Liu</dc:creator>
    </item>
    <item>
      <title>Stochastic Orthogonal Regularization for deep projective priors</title>
      <link>https://arxiv.org/abs/2505.13078</link>
      <description>arXiv:2505.13078v3 Announce Type: replace 
Abstract: Many crucial tasks of image processing and computer vision are formulated as inverse problems. Thus, it is of great importance to design fast and robust algorithms to solve these problems. In this paper, we focus on generalized projected gradient descent (GPGD) algorithms where generalized projections are realized with learned neural networks and provide state-of-the-art results for imaging inverse problems. Indeed, neural networks allow for projections onto unknown low-dimensional sets that model complex data, such as images. We call these projections deep projective priors. In generic settings, when the orthogonal projection onto a lowdimensional model set is used, it has been shown, under a restricted isometry assumption, that the corresponding orthogonal PGD converges with a linear rate, yielding near-optimal convergence (within the class of GPGD methods) in the classical case of sparse recovery. However, for deep projective priors trained with classical mean squared error losses, there is little guarantee that the hypotheses for linear convergence are satisfied. In this paper, we propose a stochastic orthogonal regularization of the training loss for deep projective priors. This regularization is motivated by our theoretical results: a sufficiently good approximation of the orthogonal projection guarantees linear stable recovery with performance close to orthogonal PGD. We show experimentally, using two different deep projective priors (based on autoencoders and on denoising networks), that our stochastic orthogonal regularization yields projections that improve convergence speed and robustness of GPGD in challenging inverse problem settings, in accordance with our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13078v3</guid>
      <category>eess.IV</category>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Joundi (UB), Yann Traonmilin (UB), Alasdair Newson (ISIR)</dc:creator>
    </item>
    <item>
      <title>A multi-dynamic low-rank deep image prior (ML-DIP) for 3D real-time cardiovascular MRI</title>
      <link>https://arxiv.org/abs/2507.19404</link>
      <description>arXiv:2507.19404v4 Announce Type: replace 
Abstract: Purpose: To develop a reconstruction framework for 3D real-time cine cardiovascular magnetic resonance (CMR) from highly undersampled data without requiring fully sampled training datasets.
  Methods: We developed a multi-dynamic low-rank deep image prior (ML-DIP) framework that models spatial image content and deformation fields using separate neural networks. These sub-networks are jointly trained per scan to reconstruct the dynamic image series directly from undersampled k-space data. ML-DIP was evaluated on (i) a 3D cine digital phantom with simulated premature ventricular contractions (PVCs), (ii) ten healthy subjects (including two scanned during both rest and exercise), and (iii) 12 patients with a history of PVCs. Phantom results were assessed using peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). In vivo performance was evaluated by comparing left-ventricular function quantification (against 2D real-time cine) and image quality (against 2D real-time cine and binning-based 5D-Cine).
  Results: In the phantom study, ML-DIP achieved PSNR &gt; 29 dB and SSIM &gt; 0.90 for scan times as short as two minutes, while recovering cardiac motion, respiratory motion, and PVC events. In healthy subjects, ML-DIP yielded functional measurements comparable to 2D cine and higher image quality than 5D-Cine, including during exercise with high heart rates and bulk motion. In PVC patients, ML-DIP preserved beat-to-beat variability and reconstructed irregular beats, whereas 5D-Cine showed motion artifacts and information loss due to binning.
  Conclusion: ML-DIP enables high-quality 3D real-time CMR with acceleration factors exceeding 1,000 by learning low-rank spatial and motion representations from undersampled data, without relying on external fully sampled training datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19404v4</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chong Chen, Marc Vornehm, Zhenyu Bu, Preethi Chandrasekaran, Muhammad A. Sultan, Syed M. Arshad, Yingmin Liu, Yuchi Han, Rizwan Ahmad</dc:creator>
    </item>
    <item>
      <title>Generative MR Multitasking with complex-harmonic cardiac encoding: Bridging the gap between gated imaging and real-time imaging</title>
      <link>https://arxiv.org/abs/2511.17847</link>
      <description>arXiv:2511.17847v2 Announce Type: replace 
Abstract: Purpose: To develop a unified image reconstruction framework that bridges real-time and gated cardiac MRI, including quantitative MRI. Methods: We introduce Generative Multitasking, which learns an implicit neural temporal basis from sequence timings and an interpretable latent space for cardiac and respiratory motion. Cardiac motion is modeled as a complex harmonic, with phase encoding timing and a latent amplitude capturing beat-to-beat functional variability, linking cardiac phase-resolved ("gated-like") and time-resolved ("real-time-like") views. We implemented the framework using a conditional variational autoencoder (CVAE) and evaluated it for free-breathing, non-ECG-gated radial GRE in three settings: steady-state cine imaging, multicontrast T2prep/IR imaging, and dual-flip-angle T1/T2 mapping, compared with conventional Multitasking. Results: Generative Multitasking provided flexible cardiac motion representation, enabling reconstruction of archetypal cardiac phase-resolved cines (like gating) as well as time-resolved series that reveal beat-to-beat variability (like real-time imaging). Conditioning on the previous k-space angle and modifying this term at inference removed eddy-current artifacts without globally smoothing high temporal frequencies. For quantitative mapping, Generative Multitasking reduced intraseptal T1 and T2 coefficients of variation compared with conventional Multitasking (T1: 0.13 vs. 0.31; T2: 0.12 vs. 0.32; p&lt;0.001), indicating higher SNR. Conclusion: Generative Multitasking uses a CVAE with complex harmonic cardiac coordinates to unify gated and real-time CMR within a single free-breathing, non-ECG-gated acquisition. It allows flexible cardiac motion representation, suppresses trajectory-dependent artifacts, and improves T1 and T2 mapping, suggesting a path toward cine, multicontrast, and quantitative imaging without separate gated and real-time scans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17847v2</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinguo Fang, Anthony G. Christodoulou</dc:creator>
    </item>
    <item>
      <title>A Fractional Variational Approach to Spectral Filtering Using the Fourier Transform</title>
      <link>https://arxiv.org/abs/2511.20675</link>
      <description>arXiv:2511.20675v2 Announce Type: replace 
Abstract: The interference of fluorescence signals and noise remains a significant challenge in Raman spectrum analysis, often obscuring subtle spectral features that are critical for accurate analysis. Inspired by variational methods similar to those used in image denoising, our approach minimizes a functional involving fractional derivatives to balance noise suppression with the preservation of essential chemical features of the signal, such as peak position, intensity, and area. The original problem is reformulated in the frequency domain through the Fourier transform, making the implementation simple and fast. In this work, we discuss the theoretical framework, practical implementation, and the advantages and limitations of this method in the context of {simulated} Raman data, as well as in image processing. The main contribution of this article is the combination of a variational approach in the frequency domain, the use of fractional derivatives, and the optimization of the {regularization parameter and} derivative order through the concept of Shannon entropy. This work explores how the fractional order, combined with the regularization parameter, affects noise removal and preserves the essential features of the spectrum {and image}. Finally, the study shows that the combination of the proposed strategies produces an efficient, robust, and easily implementable filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20675v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nelson H. T. Lemes, Jos\'e Claudinei Ferreira, Higor V. M. Ferreira</dc:creator>
    </item>
    <item>
      <title>A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion</title>
      <link>https://arxiv.org/abs/2506.15747</link>
      <description>arXiv:2506.15747v2 Announce Type: replace-cross 
Abstract: The single-view image guided point cloud completion (SVIPC) task aims to reconstruct a complete point cloud from a partial input with the help of a single-view image. While previous works have demonstrated the effectiveness of this multimodal approach, the fundamental necessity of image guidance remains largely unexamined. To explore this, we propose a strong baseline approach for SVIPC based on an attention-based multi-branch encoder-decoder network that only takes partial point clouds as input, view-free. Our hierarchical self-fusion mechanism, driven by cross-attention and self-attention layers, effectively integrates information across multiple streams, enriching feature representations and strengthening the networks ability to capture geometric structures. Extensive experiments and ablation studies on the ShapeNet-ViPC dataset demonstrate that our view-free framework performs superiorly to state-of-the-art SVIPC methods. We hope our findings provide new insights into the development of multimodal learning in SVIPC. Our demo code will be available at https://github.com/Zhang-VISLab.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15747v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangzhou Lin, Zilin Dai, Rigved Sanku, Songlin Hou, Kazunori D Yamada, Haichong K. Zhang, Ziming Zhang</dc:creator>
    </item>
    <item>
      <title>Bayesian Insights into Exchange and Restriction in Gray Matter Diffusion MRI</title>
      <link>https://arxiv.org/abs/2508.19478</link>
      <description>arXiv:2508.19478v2 Announce Type: replace-cross 
Abstract: Biophysical models in diffusion MRI (dMRI) hold promise for characterizing gray matter tissue microstructure. Yet, the reliability of their parameter estimates remains largely under-studied, especially in models that incorporate water exchange. In this study, we investigate the accuracy, precision, and presence of degeneracy of two recently proposed gray matter models, NEXI and SANDIX, using established acquisition protocols, on both simulated and \textit{in vivo} data. We employ $\mu$GUIDE, a Bayesian inference framework based on deep learning, to quantify parameter uncertainty and detect degeneracies, enabling a more interpretable assessment of model fits. Our results show that while some microstructural parameters, such as extra-cellular diffusivity and neurite signal fraction, are robustly estimated, others, including exchange time and soma radius, are often associated with high uncertainty and estimation bias, particularly under realistic noise conditions and reduced acquisition protocols. Comparison with non-linear least squares fitting highlights the critical advantage of uncertainty-aware methods: the ability to flag and filter out unreliable estimates. Together, these findings emphasize the need to report uncertainty and account for model degeneracies when interpreting model-based estimates. Our study advocates for the integration of probabilistic fitting approaches into imaging pipelines to improve reproducibility and biological interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19478v2</guid>
      <category>physics.med-ph</category>
      <category>eess.IV</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ma\"eliss Jallais, Quentin Uhl, Tommaso Pavan, Malwina Molendowska, Derek K. Jones, Ileana Jelescu, Marco Palombo</dc:creator>
    </item>
  </channel>
</rss>
