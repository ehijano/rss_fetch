<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Mar 2025 02:50:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Energy efficiency of cache eviction algorithms for Zipf distributed objects</title>
      <link>https://arxiv.org/abs/2503.02504</link>
      <description>arXiv:2503.02504v1 Announce Type: new 
Abstract: This paper presents a summary analysis of the Least Frequently Used (LFU) and Perfect Least Frequently Used (PLFU) cache eviction algorithms on real data, transferred on Content Delivery Nettworks (CDNs), as well as on Zipf distributed samples. In light of the growing emphasis on energy efficiency in CDNs in recent years due to rising energy costs, this paper considers and discusses the total CPU time required to run a cache algorithm. The total CPU time represents a novel metric for evaluating cache performance, and it is contrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a new algorithm with an admission policy and the eviction strategy that of PLFU is presented. The results demonstrate that it is a simple and straightforward algorithm to implement and offers high CHR and low CPU time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02504v1</guid>
      <category>cs.PF</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emese Sziklay, Tam\'as Jursonovics</dc:creator>
    </item>
    <item>
      <title>Online Pseudo-average Shifting Attention(PASA) for Robust Low-precision LLM Inference: Algorithms and Numerical Analysis</title>
      <link>https://arxiv.org/abs/2503.01873</link>
      <description>arXiv:2503.01873v1 Announce Type: cross 
Abstract: Attention calculation is extremely time-consuming for long-sequence inference tasks, such as text or image/video generation, in large models. To accelerate this process, we developed a low-precision, mathematically-equivalent algorithm called PASA, based on Flash Attention. PASA introduces two novel techniques: online pseudo-average shifting and global recovering. These techniques enable the use of half-precision computation throughout the Flash Attention process without incurring overflow instability or unacceptable numerical accuracy loss. This algorithm enhances performance on memory-restricted AI hardware architectures, such as the Ascend Neural-network Processing Unit(NPU), by reducing data movement and increasing computational FLOPs. The algorithm is validated using both designed random benchmarks and real large models. We find that the large bias and amplitude of attention input data are critical factors contributing to numerical overflow ($&gt;65504$ for half precision) in two different categories of large models (Qwen2-7B language models and Stable-Video-Diffusion multi-modal models). Specifically, overflow arises due to the large bias in the sequence dimension and the resonance mechanism between the query and key in the head dimension of the Stable-Video-Diffusion models. The resonance mechanism is defined as phase coincidence or 180-degree phase shift between query and key matrices. It will remarkably amplify the element values of attention score matrix. This issue also applies to the Qwen models. Additionally, numerical accuracy is assessed through root mean square error (RMSE) and by comparing the final generated texts and videos to those produced using high-precision attention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01873v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>math.NA</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Long Cheng, Qichen Liao, Fan Wu, Junlin Mu, Tengfei Han, Zhe Qiu, Lianqiang Li, Tianyi Liu, Fangzheng Miao, Keming Gao, Liang Wang, Zhen Zhang, Qiande Yin</dc:creator>
    </item>
    <item>
      <title>CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with Limited Memory</title>
      <link>https://arxiv.org/abs/2503.02354</link>
      <description>arXiv:2503.02354v1 Announce Type: cross 
Abstract: Large language models like GPT-4 are resource-intensive, but recent advancements suggest that smaller, specialized experts can outperform the monolithic models on specific tasks. The Collaboration-of-Experts (CoE) approach integrates multiple expert models, improving the accuracy of generated results and offering great potential for precision-critical applications, such as automatic circuit board quality inspection. However, deploying CoE serving systems presents challenges to memory capacity due to the large number of experts required, which can lead to significant performance overhead from frequent expert switching across different memory and storage tiers.
  We propose CoServe, an efficient CoE model serving system on heterogeneous CPU and GPU with limited memory. CoServe reduces unnecessary expert switching by leveraging expert dependency, a key property of CoE inference. CoServe introduces a dependency-aware request scheduler and dependency-aware expert management for efficient inference. It also introduces an offline profiler to automatically find optimal resource allocation on various processors and devices. In real-world intelligent manufacturing workloads, CoServe achieves 4.5$\times$ to 12$\times$ higher throughput compared to state-of-the-art systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02354v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3676641.3715986</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2. 2025</arxiv:journal_reference>
      <dc:creator>Jiashun Suo, Xiaojian Liao, Limin Xiao, Li Ruan, Jinquan Wang, Xiao Su, Zhisheng Huo</dc:creator>
    </item>
    <item>
      <title>Scalable Binary CUR Low-Rank Approximation Algorithm</title>
      <link>https://arxiv.org/abs/2502.11017</link>
      <description>arXiv:2502.11017v2 Announce Type: replace-cross 
Abstract: This paper proposes a scalable binary CUR low-rank approximation algorithm that leverages parallel selection of representative rows and columns within a deterministic framework. By employing a blockwise adaptive cross approximation strategy, the algorithm efficiently identifies dominant components in large-scale matrices, thereby reducing computational costs. Numerical experiments on $16,384 \times 16,384$ matrices demonstrate a good speed-up, with execution time decreasing from $12.37$ seconds using $2$ processes to $1.02$ seconds using $64$ processes. The tests on Hilbert matrices and synthetic low-rank matrices of different size across various sizes demonstrate an near-optimal reconstruction accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11017v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bowen Su</dc:creator>
    </item>
  </channel>
</rss>
