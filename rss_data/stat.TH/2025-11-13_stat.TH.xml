<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 02:36:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Gaussian Approximation for High-Dimensional Second-Order $U$ and $V$-statistics with Size-Dependent Kernels under i.n.i.d. Sampling</title>
      <link>https://arxiv.org/abs/2511.08870</link>
      <description>arXiv:2511.08870v1 Announce Type: new 
Abstract: We develop Gaussian approximations for high-dimensional vectors formed by second-order $U$ and $V$-statistics whose kernels depend on sample size under independent but not identically distributed (i.n.i.d.) sampling. Our results hold irrespective of which component of the Hoeffding decomposition is dominant, thereby covering both non-degenerate and degenerate regimes as special cases. By allowing i.n.i.d. sampling, the class of statistics we analyze includes weighted $U$- and $V$-statistics and two-sample $U$- and $V$-statistics as special cases, which cover estimators of parameters in regression models with many covariates, many-weak instruments as well as a broad class of smoothed two-sample tests, to name but a few. In addition, we extend sharp maximal inequalities for $U$-statistics with size-dependent kernels from the i.i.d. to the i.n.i.d. setting, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08870v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunsuke Imai</dc:creator>
    </item>
    <item>
      <title>Zero-Order Sharpness-Aware Minimization</title>
      <link>https://arxiv.org/abs/2511.09156</link>
      <description>arXiv:2511.09156v1 Announce Type: new 
Abstract: Prompt learning has become a key method for adapting large language models to specific tasks with limited data. However, traditional gradient-based optimization methods for tuning prompts are computationally intensive, posing challenges for efficiency. We introduce ZOSA (Zero-Order Sharpness-Aware Minimization), a novel optimization framework that integrates zero-order optimization with sharpness-aware minimization to enhance prompt tuning. ZOSA employs Rademacher perturbation vectors to estimate gradients without requiring backpropagation. By incorporating sharpness-aware principles, it targets flat minima in the loss landscape, improving generalization. An adaptive learning rate, guided by loss variability, further ensures stable convergence. Experiments on few-shot learning tasks, such as text classification and natural language inference, show that ZOSA significantly outperforms existing methods. With its theoretical foundation and computational efficiency, ZOSA offers a practical solution for prompt-based learning in resource-limited settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09156v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Fu, Yihang Jin, Chunxia Zhang, Junmin Liu, Haishan Ye</dc:creator>
    </item>
    <item>
      <title>Pseudo-Differential Operators and Generalized Random Fields over Tori</title>
      <link>https://arxiv.org/abs/2511.09423</link>
      <description>arXiv:2511.09423v1 Announce Type: new 
Abstract: Mat\'ern covariance functions are ubiquitous in spatial statistics, valued for their interpretable parameters and well-understood sample path properties in Euclidean settings. This paper examines whether these desirable properties transfer to manifold domains through rigorous analysis of Mat\'ern processes on tori using pseudo-differential operator theory. We establish that processes on $d$-dimensional tori require smoothness parameter $\nu &gt; 3d/2$ to achieve regularity $C^{(\nu-3d/2)^-}_{\text{loc}}$, revealing a dimension-dependent threshold that contrasts with the Euclidean requirement of merely $\nu &gt; 0$. Our proof employs the Cardona-Mart\'inez theory of pseudo-differential operators, providing new analytical tools to the study of random fields over manifolds. We also introduce the canonical-Mat\'ern process, a parameter family that achieves regularity $C^{(\nu-3d/2+2)^-}_{\text{loc}}$, gaining two orders of smoothness over standard Mat\'ern processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09423v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Escobar-Velasquez</dc:creator>
    </item>
    <item>
      <title>A Novel Testing Approach for Differences Among Brain Connectomes</title>
      <link>https://arxiv.org/abs/2511.09431</link>
      <description>arXiv:2511.09431v1 Announce Type: new 
Abstract: Statistical analysis on non-Euclidean spaces typically relies on distances as the primary tool for constructing likelihoods. However, manifold-valued data admits richer structures in addition to Riemannian distances. We demonstrate that simple, tractable models that do not rely exclusively on distances can be constructed on the manifold of symmetric positive definite (SPD) matrices, which naturally arises in brain connectivity analysis. Specifically, we highlight the manifold-valued Mahalanobis distribution, a parametric family that extends classical multivariate concepts to the SPD manifold. We develop estimators for this distribution and establish their asymptotic properties. Building on this framework, we propose a novel ANOVA test that leverages the manifold structure to obtain a test statistic that better captures the dimensionality of the data. We theoretically demonstrate that our test achieves superior statistical power compared to distance-based Fr\'echet ANOVA methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09431v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Escobar-Velasquez, Jaroslaw Harezlak</dc:creator>
    </item>
    <item>
      <title>Deep neural expected shortfall regression with tail-robustness</title>
      <link>https://arxiv.org/abs/2511.08772</link>
      <description>arXiv:2511.08772v1 Announce Type: cross 
Abstract: Expected shortfall (ES), also known as conditional value-at-risk, is a widely recognized risk measure that complements value-at-risk by capturing tail-related risks more effectively. Compared with quantile regression, which has been extensively developed and applied across disciplines, ES regression remains in its early stage, partly because the traditional empirical risk minimization framework is not directly applicable. In this paper, we develop a nonparametric framework for expected shortfall regression based on a two-step approach that treats the conditional quantile function as a nuisance parameter. Leveraging the representational power of deep neural networks, we construct a two-step ES estimator using feedforward ReLU networks, which can alleviate the curse of dimensionality when the underlying functions possess hierarchical composition structures. However, ES estimation is inherently sensitive to heavy-tailed response or error distributions. To address this challenge, we integrate a properly tuned Huber loss into the neural network training, yielding a robust deep ES estimator that is provably resistant to heavy-tailedness in a non-asymptotic sense and first-order insensitive to quantile estimation errors in the first stage. Comprehensive simulation studies and an empirical analysis of the effect of El Ni\~no on extreme precipitation illustrate the accuracy and robustness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08772v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myeonghun Yu, Kean Ming Tan, Huixia Judy Wang, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>Enhanced Rank-Based Correlation Estimation Using Smoothed Wilcoxon Rank Scores</title>
      <link>https://arxiv.org/abs/2511.08979</link>
      <description>arXiv:2511.08979v1 Announce Type: cross 
Abstract: This article proposes an improved version of the Spearman rank correlation based on using Wilcoxon rank score function. A smoothed empirical cumulative distribution function (ecdf)computes the smoothed ranks and replaces the regular ranks in the Wilcoxon rank score function. The smoothed Wilcoxon rank scores are then used for estimation of the Spearman's correlation. The proposed approach is similar to the Spearman's rho estimator which uses ranks of the random samples of X and Y but the proposed method improves Spearman's approach such as handling ties and gaining higher efficiency under monotone associations. A Wald type hypothesis test has been proposed for the new estimator and the asymptotic properties are shown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08979v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.58830/ozgur.pub862.c3491</arxiv:DOI>
      <dc:creator>Feridun Tasdan, Rukiye Dagalp</dc:creator>
    </item>
    <item>
      <title>Robust Cauchy-Based Methods for Predictive Regressions</title>
      <link>https://arxiv.org/abs/2511.09249</link>
      <description>arXiv:2511.09249v1 Announce Type: cross 
Abstract: This paper develops robust inference methods for predictive regressions that address key challenges posed by endogenously persistent or heavy-tailed regressors, as well as persistent volatility in errors. Building on the Cauchy estimation framework, we propose two novel tests: one based on $t$-statistic group inference and the other employing a hybrid approach that combines Cauchy and OLS estimation. These methods effectively mitigate size distortions that commonly arise in standard inference procedures under endogeneity, near nonstationarity, heavy tails, and persistent volatility. The proposed tests are simple to implement and applicable to both continuous- and discrete-time models. Extensive simulation experiments demonstrate favorable finite-sample performance across a range of realistic settings. An empirical application examines the predictability of excess stock returns using the dividend-price and earnings-price ratios as predictors. The results suggest that the dividend-price ratio possesses predictive power, whereas the earnings-price ratio does not significantly forecast returns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09249v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rustam Ibragimov, Jihyun Kim, Anton Skrobotov</dc:creator>
    </item>
    <item>
      <title>Valid and efficient possibilistic structure learning in Gaussian linear regression</title>
      <link>https://arxiv.org/abs/2511.09305</link>
      <description>arXiv:2511.09305v1 Announce Type: cross 
Abstract: A crucial step in fitting a regression model to data is determining the model's structure, i.e., the subset of explanatory variables to be included. However, the uncertainty in this step is often overlooked due to a lack of satisfactory methods. Frequentists have no broadly applicable confidence set constructions for a model's structure, and Bayesian posterior credible sets do not achieve the desired finite-sample coverage. In this paper, we propose an extension of the possibility-theoretic inferential model (IM) framework that offers reliable, data-driven uncertainty quantification about the unknown model structure. This particular extension allows for the inclusion of incomplete prior information about the unknown structure that facilitates regularization. We prove that this new, regularized, possibilistic IM's uncertainty quantification is suitably calibrated relative to the set of joint distributions compatible with the data-generating process and assumed partial prior knowledge about the structure. This implies, among other things, that the derived confidence sets for the unknown model structure attain the nominal coverage probability in finite samples. We provide background and guidance on quantifying prior knowledge in this new context and analyze two benchmark data sets, comparing our results to those obtained by existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09305v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin, Naomi Singer, Jonathan Williams</dc:creator>
    </item>
    <item>
      <title>Distributional Shrinkage I: Universal Denoisers in Multi-Dimensions</title>
      <link>https://arxiv.org/abs/2511.09500</link>
      <description>arXiv:2511.09500v1 Announce Type: cross 
Abstract: We revisit the problem of denoising from noisy measurements where only the noise level is known, not the noise distribution. In multi-dimensions, independent noise $Z$ corrupts the signal $X$, resulting in the noisy measurement $Y = X + \sigma Z$, where $\sigma \in (0, 1)$ is a known noise level. Our goal is to recover the underlying signal distribution $P_X$ from denoising $P_Y$. We propose and analyze universal denoisers that are agnostic to a wide range of signal and noise distributions. Our distributional denoisers offer order-of-magnitude improvements over the Bayes-optimal denoiser derived from Tweedie's formula, if the focus is on the entire distribution $P_X$ rather than on individual realizations of $X$. Our denoisers shrink $P_Y$ toward $P_X$ optimally, achieving $O(\sigma^4)$ and $O(\sigma^6)$ accuracy in matching generalized moments and density functions. Inspired by optimal transport theory, the proposed denoisers are optimal in approximating the Monge-Amp\`ere equation with higher-order accuracy, and can be implemented efficiently via score matching.
  Let $q$ represent the density of $P_Y$; for optimal distributional denoising, we recommend replacing the Bayes-optimal denoiser, \[ \mathbf{T}^*(y) = y + \sigma^2 \nabla \log q(y), \] with denoisers exhibiting less aggressive distributional shrinkage, \[ \mathbf{T}_1(y) = y + \frac{\sigma^2}{2} \nabla \log q(y), \] \[ \mathbf{T}_2(y) = y + \frac{\sigma^2}{2} \nabla \log q(y) - \frac{\sigma^4}{8} \nabla \left( \frac{1}{2} \| \nabla \log q(y) \|^2 + \nabla \cdot \nabla \log q(y) \right) . \]</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09500v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tengyuan Liang</dc:creator>
    </item>
    <item>
      <title>Criterion for the resemblance between the mother and the model distribution</title>
      <link>https://arxiv.org/abs/2212.03397</link>
      <description>arXiv:2212.03397v3 Announce Type: replace 
Abstract: If the probability distribution model aims to approximate the hidden mother distribution, it is imperative to establish a useful criterion for the resemblance between the mother and the model distributions.
  This study proposes a criterion that measures the Hellinger distance between discretized (quantized) samples from both distributions. Unlike information criteria such as AIC, this criterion does not require the probability density function of the model distribution, which cannot be explicitly obtained for a complicated model such as a deep learning machine. Second, it can draw a positive conclusion (i.e., both distributions are sufficiently close) under a given threshold, whereas a statistical hypothesis test, such as the Kolmogorov-Smirnov test, cannot genuinely lead to a positive conclusion when the hypothesis is accepted.
  In this study, we establish a reasonable threshold for the criterion deduced from the Bayes error rate and also present the asymptotic bias of the estimator of the criterion. From these results, a reasonable and easy-to-use criterion is established that can be directly calculated from the two sets of samples from both distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.03397v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yo Sheena</dc:creator>
    </item>
    <item>
      <title>The Categorical Instrumental Variable Model: Characterization, Partial Identification, and Statistical Inference</title>
      <link>https://arxiv.org/abs/2405.09510</link>
      <description>arXiv:2405.09510v5 Announce Type: replace 
Abstract: We study categorical instrumental variable (IV) models with instrument, treatment, and outcome taking finitely many values. We derive a simple closed-form characterization of the set of joint distributions of potential outcomes that are compatible with a given observed data distribution in terms of a set of inequalities. These inequalities unify several different IV models defined by versions of the independence and exclusion restriction assumptions and are shown to be non-redundant. Finally, given a set of linear functionals of the joint counterfactual distribution, such as pairwise average treatment effects, we construct confidence intervals with simultaneous finite-sample coverage, using a tail bound on the Kullback--Leibler divergence. We illustrate our method using data from the Minneapolis Domestic Violence Experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09510v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilin Song, F. Richard Guo, K. C. Gary Chan, Thomas S. Richardson</dc:creator>
    </item>
    <item>
      <title>Fixed-strength spherical designs</title>
      <link>https://arxiv.org/abs/2502.06002</link>
      <description>arXiv:2502.06002v2 Announce Type: replace 
Abstract: A spherical $t$-design is a finite subset $X$ of the unit sphere such that every polynomial of degree at most $t$ has the same average over $X$ as it does over the entire sphere. Determining the minimum possible size of spherical designs, especially in a fixed dimension as $t \to \infty$, has been an important research topic for several decades. This paper presents results on the complementary asymptotic regime, where $t$ is fixed and the dimension tends to infinity. The main results in this paper are (1) a construction of smaller spherical designs via an explicit connection to Gaussian designs and (2) the exact order of magnitude of minimal-size signed $t$-designs, which is far less than predicted by a typical degrees-of-freedom heuristic. We also establish a method to ``project'' spherical designs between dimensions, prove a variety of results on approximate designs, and construct new $t$-wise independent subsets of $\{1,2,\dots,q\}^d$ which may be of independent interest. To achieve these results, we combine techniques from algebra, geometry, probability, representation theory, and optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06002v2</guid>
      <category>math.ST</category>
      <category>math.CO</category>
      <category>math.MG</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Travis Dillon</dc:creator>
    </item>
    <item>
      <title>Possibilistic inferential models: a review</title>
      <link>https://arxiv.org/abs/2507.09007</link>
      <description>arXiv:2507.09007v2 Announce Type: replace 
Abstract: An inferential model (IM) is a model describing the construction of provably reliable, data-driven uncertainty quantification and inference about relevant unknowns. IMs and Fisher's fiducial argument have similar objectives, but a fundamental distinction between the two is that the former doesn't require that uncertainty quantification be probabilistic, offering greater flexibility and allowing for a proof of its reliability. Important recent developments have been made thanks in part to newfound connections with the imprecise probability literature, in particular, possibility theory. The brand of possibilistic IMs studied here are straightforward to construct, have very strong frequentist-like reliability properties, and offer fully conditional, Bayesian-like (imprecise) probabilistic reasoning. This paper reviews these key recent developments, describing the new theory, methods, and computational tools. A generalization of the basic possibilistic IM is also presented, making new and unexpected connections with ideas in modern statistics and machine learning, e.g., bootstrap and conformal prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09007v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Spacing Test for Fused Lasso</title>
      <link>https://arxiv.org/abs/2509.14229</link>
      <description>arXiv:2509.14229v3 Announce Type: replace 
Abstract: Detecting changepoints in a one-dimensional signal is a classical yet fundamental problem. The fused lasso provides an elegant convex formulation that produces a stepwise estimate of the mean, but quantifying the uncertainty of the detected changepoints remains difficult. Post-selection inference (PSI) offers a principled way to compute valid $p$-values after a data-driven selection, but its application to the fused lasso has been considered computationally cumbersome, requiring the tracking of many ``hit'' and ``leave'' events along the regularization path. In this paper, we show that the one-dimensional fused lasso has a surprisingly simple geometry: each changepoint enters in a strictly one-sided fashion, and there are no leave events. This structure implies that the so-called \emph{conservative spacing test} of Tibshirani et al.\ (2016), previously regarded as an approximation, is in fact \emph{exact}. The truncation region in the selective law reduces to a single lower bound given by the next knot on the LARS path. As a result, the exact selective $p$-value takes a closed form identical to the simple spacing statistic used in the LARS/lasso setting, with no additional computation. This finding establishes one of the rare cases in which an exact PSI procedure for the generalized lasso admits a closed-form pivot. We further validate the result by simulations and real data, confirming both exact calibration and high power.
  Keywords: fused lasso; changepoint detection; post-selection inference; spacing test; monotone LASSO</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14229v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rieko Tasaka, Tatsuya Kimura, Joe Suzuki</dc:creator>
    </item>
    <item>
      <title>Efficient Multiple-Robust Estimation for Nonresponse Data Under Informative Sampling</title>
      <link>https://arxiv.org/abs/2311.06719</link>
      <description>arXiv:2311.06719v2 Announce Type: replace-cross 
Abstract: Nonresponse after probability sampling is a universal challenge in survey sampling, often necessitating adjustments to mitigate sampling and selection bias simultaneously. This study explored the removal of bias and effective utilization of available information, not just in nonresponse but also in the scenario of data integration, where summary statistics from other data sources are accessible. We reformulate these settings within a two-step monotone missing data framework, where the first step of missingness arises from sampling and the second originates from nonresponse. Subsequently, we derive the semiparametric efficiency bound for the target parameter. We also propose adaptive estimators utilizing methods of moments and empirical likelihood approaches to attain the lower bound. The proposed estimator exhibits both efficiency and double robustness. However, attaining efficiency with an adaptive estimator requires the correct specification of certain working models. To reinforce robustness against the misspecification of working models, we extend the property of double robustness to multiple robustness by proposing a two-step empirical likelihood method that effectively leverages empirical weights. A numerical study is undertaken to investigate the finite-sample performance of the proposed methods. We further applied our methods to a dataset from the National Health and Nutrition Examination Survey data by efficiently incorporating summary statistics from the National Health Interview Survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06719v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosuke Morikawa, Kenji Beppu, Wataru Aida</dc:creator>
    </item>
    <item>
      <title>Waveform Design for Over-the-Air Computing</title>
      <link>https://arxiv.org/abs/2405.20877</link>
      <description>arXiv:2405.20877v2 Announce Type: replace-cross 
Abstract: In response to the increasing number of devices expected in next-generation networks, a shift to over-the-air (OTA) computing has been proposed. By leveraging the superposition of multiple access channels, OTA computing enables efficient resource management by supporting simultaneous uncoded transmission in the time and frequency domains. To advance the integration of OTA computing, our study presents a theoretical analysis that addresses practical issues encountered in current digital communication transceivers, such as transmitter synchronization (sync) errors and intersymbol interference (ISI). To this end, we investigate the theoretical mean squared error (MSE) for OTA transmission under sync errors and ISI, while also exploring methods for minimizing the MSE in OTA transmission. Using alternating optimization, we also derive optimal power policies for both the devices and the base station. In addition, we propose a novel deep neural network (DNN)-based approach to design waveforms that improve OTA transmission performance under sync errors and ISI. To ensure a fair comparison with existing waveforms such as raised cosine (RC) and better-than-raised-cosine (BTRC), we incorporate a custom loss function that integrates energy and bandwidth constraints along with practical design considerations such as waveform symmetry. Simulation results validate our theoretical analysis and demonstrate performance gains of the designed pulse over RC and BTRC waveforms. To facilitate testing of our results without the need to rebuild the DNN structure, we also provide curve-fitting parameters for the selected DNN-based waveforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20877v2</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TWC.2025.3613838</arxiv:DOI>
      <dc:creator>Nikos G. Evgenidis, Nikos A. Mitsiou, Sotiris A. Tegos, Panagiotis D. Diamantoulakis, Panagiotis Sarigiannidis, Ioannis T. Rekanos, George K. Karagiannidis</dc:creator>
    </item>
    <item>
      <title>Matrix Moment and Concentration Inequalities for Martingales and Ergodic Markov Chains with Applications in Statistical Learning</title>
      <link>https://arxiv.org/abs/2508.04327</link>
      <description>arXiv:2508.04327v2 Announce Type: replace-cross 
Abstract: In this paper, we study moment and concentration inequalities for the spectral norm of sums of dependent random matrices.
  We establish novel Rosenthal-Burkholder inequalities for discrete-time matrix local martingales, Burkholder-Davis-Gundy inequality for continuous matrix local martingales, as well as matrix Rosenthal, Hoeffding, and Bernstein inequalities for ergodic Markov chains.
  Compared with previous work on matrix concentration inequalities for Markov chains, which assume a non-zero absolute $L^2$-spectral gap or the stronger $\psi$-mixing condition, our results assume geometric ergodicity, a condition commonly used in statistical applications.
  Furthermore, our results have leading terms that match the Markov chain central limit theorem, rather than relying on suboptimal variance proxies.
  We also give dimension-free versions of the inequalities, which are independent of the ambient dimension $d$ and relies on the effective rank instead.
  This enables the generalization of our results to linear operators in infinite-dimensional Hilbert spaces.
  Our results have extensive applications in statistics and machine learning; in particular, we obtain improved bounds in covariance estimation and principal component analysis on Markovian data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04327v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Peng, Yuchen Xin, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Topics in Probability, Parametric Estimation and Stochastic Calculus</title>
      <link>https://arxiv.org/abs/2510.20163</link>
      <description>arXiv:2510.20163v2 Announce Type: replace-cross 
Abstract: We begin our journey by recalling the fundamentals of Probability Theory that underlie one of its most significant applications to real-world problems: Parametric Estimation. Throughout the text, we systematically develop this theme by presenting and discussing the main tools it encompasses (concentration inequalities, limit theorems, confidence intervals, maximum likelihood, least squares, and hypothesis testing) always with an eye toward both their theoretical underpinnings and practical relevance. While our approach follows the broad contours of conventional expositions, we depart from tradition by consistently exploring the geometric aspects of probability, particularly the invariance properties of normally distributed random vectors. This geometric perspective is taken further in an extended appendix, where we introduce the rudiments of Brownian motion and the corresponding stochastic calculus, culminating in It\^o's celebrated change-of-variables formula. To highlight its scope and elegance, we present some of its most striking applications: the sharp Gaussian concentration inequality (a central example of the "concentration of measure phenomenon"), the Feynman-Kac formula (used to derive a path integral representation for the Laplacian heat kernel), and, as a concluding delicacy, the Black-Scholes strategy in Finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20163v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Levi Lopes de Lima</dc:creator>
    </item>
  </channel>
</rss>
