<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Jul 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical inference of heterogeneous treatment effects using semiparametric single-index model</title>
      <link>https://arxiv.org/abs/2507.13594</link>
      <description>arXiv:2507.13594v1 Announce Type: new 
Abstract: In recent years, with the rapid development of science and technology, heterogeneous treatment effects have emerged as a focal research topic in statistics, econometrics, and sociology. This paper investigates HTE through semiparametric single-index models based on doubly robust estimation. Departing from conventional approaches, we neither impose boundedness constraints on the link function in single-index models nor restrict its support range. By employing the sieve method to approximate the link function, we achieve simultaneous estimation of both the link function and index parameters. Our study not only establishes the asymptotic properties of the proposed estimator but also systematically evaluates its finite-sample performance through comprehensive simulation studies. Numerical results demonstrate that our method significantly outperforms other commonly used competing estimators. Furthermore, we apply the proposed approach to the National Health and Nutrition Examination Survey dataset to assess the impact of participation in school lunch programs on body mass index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13594v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jichang Yu, Wenjing Chang, Peichao Yu, Lijun Chen, Yuanshan Wu</dc:creator>
    </item>
    <item>
      <title>A mixture distribution approach for assessing genetic impact from twin study</title>
      <link>https://arxiv.org/abs/2507.13605</link>
      <description>arXiv:2507.13605v1 Announce Type: new 
Abstract: This work was motivated by a twin study with the goal of assessing the genetic control of immune traits. We propose a mixture bivariate distribution to model twin data where the underlying order within a pair is unclear. Though estimation from mixture distribution is usually subject to low convergence rate, the combined likelihood, which is constructed over monozygotic and dizygotic twins combined, reaches root-n consistency and allows effective statistical inference on the genetic impact. The method is applicable to general unordered pairs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13605v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.9367</arxiv:DOI>
      <arxiv:journal_reference>Statistics in Medicine, 41, 2513-2522 (2022)</arxiv:journal_reference>
      <dc:creator>Zonghui Hu, Pengfei Li, Dean Follmann, Jing Qin</dc:creator>
    </item>
    <item>
      <title>Empirical likelihood meta analysis with publication bias correction under Copas-like selection model</title>
      <link>https://arxiv.org/abs/2507.13615</link>
      <description>arXiv:2507.13615v1 Announce Type: new 
Abstract: Meta analysis is commonly-used to synthesize multiple results from individual studies. However, its validation is usually threatened by publication bias and between-study heterogeneity, which can be captured by the Copas selection model. Existing inference methods under this model are all based on conditional likelihood and may not be fully efficient. In this paper, we propose a full likelihood approach to meta analysis by integrating the conditional likelihood and a marginal semi-parametric empirical likelihood under a Copas-like selection model. We show that the maximum likelihood estimators (MLE) of all the underlying parameters have a jointly normal limiting distribution, and the full likelihood ratio follows an asymptotic central chisquare distribution. Our simulation results indicate that compared with the conditional likelihood method, the proposed MLEs have smaller mean squared errors and the full likelihood ratio confidence intervals have more accurate coverage probabilities. A real data example is analyzed to show the advantages of the full likelihood method over the conditional likelihood method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13615v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10463-021-00793-4</arxiv:DOI>
      <arxiv:journal_reference>Annals of the Institute of Statistical Mathematics, 74, 93-112 (2022)</arxiv:journal_reference>
      <dc:creator>Mengke Li, Yukun Liu, Pengfei Li, Jing Qin</dc:creator>
    </item>
    <item>
      <title>Controlling IER and EER in replicated regular two-level factorial experiments</title>
      <link>https://arxiv.org/abs/2507.13621</link>
      <description>arXiv:2507.13621v1 Announce Type: new 
Abstract: Replicated regular two-level factorial experiments are very useful for industry. The goal of these experiments is to identify active effects that affect the mean and variance of the response. Hypothesis testing procedures are widely used for this purpose. However, the existing methods give results that are either too anticonservative or conservative in controlling the individual and experimentwise error rates (IER and EER). In this paper, we propose {a Monte Carlo method} and an exact-variance method to identify active effects for the mean and variance, respectively, of the response. Simulation studies show that our methods control the IER and EER extremely well. Real data are used to illustrate the performance of the methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13621v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/03610918.2019.1588311</arxiv:DOI>
      <arxiv:journal_reference>Communications in Statistics - Simulation and Computation, 50, 1770-1790 (2021)</arxiv:journal_reference>
      <dc:creator>Pengfei Li, Oludotun J. Akinlawon, Shengli Zhao</dc:creator>
    </item>
    <item>
      <title>Intellectual Up-streams of Percentage Scale ($ps$) and Percentage Coefficient ($b_p$) -- Effect Size Analysis (Theory Paper 2)</title>
      <link>https://arxiv.org/abs/2507.13695</link>
      <description>arXiv:2507.13695v1 Announce Type: new 
Abstract: Percentage thinking, i.e., assessing quantities as parts per hundred, has traveled widely from Roman tax ledgers to modern algorithms. Building on early decimalization by Simon Stevin in La Thiende (1585) and the 19th-century metrication movement that institutionalized base-10 measurement worldwide (Cajori, 1925), this article traces the intellectual trails through which base-10 normalization, especially 0~1 percentage scale. We discuss commonalities between those Wisconsin-Carolina experiments and classic indices, especially the plus minus 1 Pearson (1895) correlation (r) and 0~1 coefficient of determination, aka r squared (Wright, 1920). We pay tribute to the influential percent of maximum possible (POMP) coefficient by Cohen et al. (1999). The history of the 0~100 or 0~1 scales goes back far and wide. Roman fiscal records, early American grading experiments at Yale and Harvard, and contemporary analysis of percent scales (0~100) and percentage scales (0~1, or -1~1) show the tendency to rediscover the scales and the indices based on the scales (Durm, 1993; Schneider &amp; and Hutt, 2014). Data mining and machine learning since the last century adopted the same logic: min-max normalization, which maps any feature to [0, 1] (i.e., 0-100%), equalizing the scale ranges. Because 0~1 percentage scale assigns the entire scale to be the unit, equalizing the scales also equalizes the units of all percentized scales. Equitable units are necessary and sufficient for comparability of two indices, according to the percentage theory of measurement indices (Cohen et al., 1999; Zhao et al., 2024; Zhao &amp; Zhang, 2014). Thus, the success of modern AI serves as a large scale test confirming the comparability of percentage-based indices, foremost among them the percentage coefficient ($b_p$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13695v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinshu Zhao, Qinru Ruby Ju, Piper Liping Liu, Dianshi Moses Li, Luxi Zhang, Jizhou Francis Ye, Song Harris Ao, Ming Milano Li</dc:creator>
    </item>
    <item>
      <title>Confidence Intervals for Random Forest Permutation Importance with Missing Data</title>
      <link>https://arxiv.org/abs/2507.13918</link>
      <description>arXiv:2507.13918v1 Announce Type: new 
Abstract: Random Forests are renowned for their predictive accuracy, but valid inference, particularly about permutation-based feature importances, remains challenging. Existing methods, such as the confidence intervals (CIs) from Ishwaran et al. (2019), are promising but assume complete feature observation. However, real-world data often contains missing values. In this paper, we investigate how common imputation techniques affect the validity of Random Forest permutation-importance CIs when data are incomplete. Through an extensive simulation and real-world benchmark study, we compare state-of-the-art imputation methods across various missing-data mechanisms and missing rates. Our results show that single-imputation strategies lead to low CI coverage. As a remedy, we adapt Rubin's rule to aggregate feature-importance estimates and their variances over several imputed datasets and account for imputation uncertainty. Our numerical results indicate that the adjusted CIs achieve better nominal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13918v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nico F\"oge, Markus Pauly</dc:creator>
    </item>
    <item>
      <title>A regression-based approach for bidirectional proximal causal inference in the presence of unmeasured confounding</title>
      <link>https://arxiv.org/abs/2507.13965</link>
      <description>arXiv:2507.13965v1 Announce Type: new 
Abstract: Proxy variables are commonly used in causal inference when unmeasured confounding exists. While most existing proximal methods assume a unidirectional causal relationship between two primary variables, many social and biological systems exhibit complex feedback mechanisms that imply bidirectional causality. In this paper, using regression-based models, we extend the proximal framework to identify bidirectional causal effects in the presence of unmeasured confounding. We establish the identification of bidirectional causal effects and develop a sensitivity analysis method for violations of the proxy structural conditions. Building on this identification result, we derive bidirectional two-stage least squares estimators that are consistent and asymptotically normal under standard regularity conditions. Simulation studies demonstrate that our approach delivers unbiased causal effect estimates and outperforms some standard methods. The simulation results also confirm the reliability of the sensitivity analysis procedure. Applying our methodology to a state-level panel dataset from 1985 to 2014 in the United States, we examine the bidirectional causal effects between abortion rates and murder rates. The analysis reveals a consistent negative effect of abortion rates on murder rates, while also detecting a potential reciprocal effect from murder rates to abortion rates that conventional unidirectional analyses have not considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13965v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Min, Xueyue Zhang, Shanshan Luo</dc:creator>
    </item>
    <item>
      <title>Using off-treatment sequential multiple imputation for binary outcomes to address intercurrent events handled by a treatment policy strategy</title>
      <link>https://arxiv.org/abs/2507.14006</link>
      <description>arXiv:2507.14006v1 Announce Type: new 
Abstract: The estimand framework proposes different strategies to address intercurrent events. The treatment policy strategy seems to be the most favoured as it is closely aligned with the pre-addendum intention-to-treat principle. All data for all patients should ideally be collected, however, in reality patients may withdraw from a study leading to missing data. This needs to be dealt with as part of the estimation. Several areas of research have been conducted exploring models to estimate the estimand when intercurrent events are handled using a treatment policy strategy, however the research is limited for binary endpoints. We explore different retrieved dropout models, where post-intercurrent event, the observed data can be used to multiply impute the missing post-intercurrent event data. We compare our proposed models to a simple imputation model that makes no distinction between the pre- and post-intercurrent event data, and assess varying statistical properties through a simulation study. We then provide an example how retrieved dropout models were used in practice for Phase 3 clinical trials in rheumatoid arthritis. From the models explored, we conclude that a simple retrieved dropout model including an indicator for whether or not the intercurrent event occurred is the most pragmatic choice. However, at least 50% of observed post-intercurrent event data is required for these models to work well. Therefore, the suitability of implementing this model in practice will depend on the amount of observed post-intercurrent event data available and missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14006v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunita Rehal, Nicky Best, Sarah Watts, Thomas Drury</dc:creator>
    </item>
    <item>
      <title>A Bayesian Dirichlet Auto-Regressive Conditional Heteroskedasticity Model for Compositional Time Series</title>
      <link>https://arxiv.org/abs/2507.14132</link>
      <description>arXiv:2507.14132v1 Announce Type: new 
Abstract: We analyze daily Airbnb service-fee shares across eleven settlement currencies, a compositional series that shows bursts of volatility after shocks such as the COVID-19 pandemic. Standard Dirichlet time series models assume constant precision and therefore miss these episodes. We introduce B-DARMA-DARCH, a Bayesian Dirichlet autoregressive moving average model with a Dirichlet ARCH component, which lets the precision parameter follow an ARMA recursion. The specification preserves the Dirichlet likelihood so forecasts remain valid compositions while capturing clustered volatility. Simulations and out-of-sample tests show that B-DARMA-DARCH lowers forecast error and improves interval calibration relative to Dirichlet ARMA and log-ratio VARMA benchmarks, providing a concise framework for settings where both the level and the volatility of proportions matter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14132v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Robert E. Weiss</dc:creator>
    </item>
    <item>
      <title>Methodological considerations for semialgebraic hypothesis testing with incomplete U-statistics</title>
      <link>https://arxiv.org/abs/2507.13531</link>
      <description>arXiv:2507.13531v1 Announce Type: cross 
Abstract: Recently, Sturma, Drton, and Leung proposed a general-purpose stochastic method for hypothesis testing in models defined by polynomial equality and inequality constraints. Notably, the method remains theoretically valid even near irregular points, such as singularities and boundaries, where traditional testing approaches often break down. In this paper, we evaluate its practical performance on a collection of biologically motivated models from phylogenetics. While the method performs remarkably well across different settings, we catalogue a number of issues that should be considered for effective application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13531v1</guid>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Barnhill, Marina Garrote-L\'opez, Elizabeth Gross, Max Hill, Bryson Kagy, John A. Rhodes, Joy Z. Zhang</dc:creator>
    </item>
    <item>
      <title>Conformalized Regression for Continuous Bounded Outcomes</title>
      <link>https://arxiv.org/abs/2507.14023</link>
      <description>arXiv:2507.14023v1 Announce Type: cross 
Abstract: Regression problems with bounded continuous outcomes frequently arise in real-world statistical and machine learning applications, such as the analysis of rates and proportions. A central challenge in this setting is predicting a response associated with a new covariate value. Most of the existing statistical and machine learning literature has focused either on point prediction of bounded outcomes or on interval prediction based on asymptotic approximations. We develop conformal prediction intervals for bounded outcomes based on transformation models and beta regression. We introduce tailored non-conformity measures based on residuals that are aligned with the underlying models, and account for the inherent heteroscedasticity in regression settings with bounded outcomes. We present a theoretical result on asymptotic marginal and conditional validity in the context of full conformal prediction, which remains valid under model misspecification. For split conformal prediction, we provide an empirical coverage analysis based on a comprehensive simulation study. The simulation study demonstrates that both methods provide valid finite-sample predictive coverage, including settings with model misspecification. Finally, we demonstrate the practical performance of the proposed conformal prediction intervals on real data and compare them with bootstrap-based alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14023v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhanli Wu, Fabrizio Leisen, F. Javier Rubio</dc:creator>
    </item>
    <item>
      <title>Composite empirical likelihood for multisample clustered data</title>
      <link>https://arxiv.org/abs/1610.05809</link>
      <description>arXiv:1610.05809v2 Announce Type: replace 
Abstract: In many applications, data cluster. Failing to take the cluster structure into consideration generally leads to underestimated variances of point estimators and inflated type I errors in hypothesis tests. Many circumstance-dependent approaches have been developed to handle clustered data. A working covariance matrix may be used in generalized estimating equations. One may throw out the cluster structure and use only the cluster means, or explicitly model the cluster structure. Our interest is the case where multiple samples of clustered data are collected, and the population quantiles are particularly important. We develop a composite empirical likelihood for clustered data under a density ratio model. This approach avoids parametric assumptions on the population distributions or the cluster structure. It efficiently utilizes the common features of the multiple populations and the exchangeability of the cluster members. We also develop a cluster-based bootstrap method to provide valid variance estimation and to control the type I errors. We examine the performance of the proposed method through simulation experiments and illustrate its usage via a real-world example.</description>
      <guid isPermaLink="false">oai:arXiv.org:1610.05809v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10485252.2021.1914337</arxiv:DOI>
      <arxiv:journal_reference>Journal of Nonparametric Statistics, 33, 60-81 (2021)</arxiv:journal_reference>
      <dc:creator>Jiahua Chen, Pengfei Li, Yukun Liu, James V. Zidek</dc:creator>
    </item>
    <item>
      <title>Pseudo-variance quasi-maximum likelihood estimation of semi-parametric time series models</title>
      <link>https://arxiv.org/abs/2309.06100</link>
      <description>arXiv:2309.06100v2 Announce Type: replace 
Abstract: We propose a novel estimation approach for a general class of semi-parametric time series models where the conditional expectation is modeled through a parametric function. The proposed class of estimators is based on a Gaussian quasi-likelihood function and it relies on the specification of a parametric pseudo-variance that can contain parametric restrictions with respect to the conditional expectation. The specification of the pseudo-variance and the parametric restrictions follow naturally in observation-driven models with bounds in the support of the observable process, such as count processes and double-bounded time series. We derive the asymptotic properties of the estimators and a validity test for the parameter restrictions. We show that the results remain valid irrespective of the correct specification of the pseudo-variance. The key advantage of the restricted estimators is that they can achieve higher efficiency compared to alternative quasi-likelihood methods that are available in the literature. Furthermore, the testing approach can be used to build specification tests for parametric time series models. We illustrate the practical use of the methodology in a simulation study and two empirical applications featuring integer-valued autoregressive processes, where assumptions on the dispersion of the thinning operator are formally tested, and autoregressions for double-bounded data with application to a realized correlation time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06100v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jeconom.2024.105894</arxiv:DOI>
      <arxiv:journal_reference>Journal of Econometrics, 246: 105894, 2024</arxiv:journal_reference>
      <dc:creator>Mirko Armillotta, Paolo Gorgi</dc:creator>
    </item>
    <item>
      <title>Two-stage weighted least squares estimator of multivariate non-negative observation-driven models</title>
      <link>https://arxiv.org/abs/2310.13487</link>
      <description>arXiv:2310.13487v2 Announce Type: replace 
Abstract: A novel estimation approach for a general class of semi-parametric multivariate time series models is introduced where the conditional mean is modeled through parametric functions. The focus of the estimation is the conditional mean parameter vector for non-negative time series. Quasi-Maximum Likelihood Estimators (QMLEs) based on the linear exponential family are typically employed for such estimation problems when the true multivariate conditional probability distribution is unknown or too complex. Although QMLEs provide consistent estimates they may be inefficient. Novel two-stage Multivariate Weighted Least Square Estimators (MWLSEs) are introduced which enjoy the same consistency property as the QMLEs but provide improved efficiency with a suitable choice of the weighting sequence of matrices in the second stage. The proposed method enables a more accurate estimation of model parameters, particularly for data where maximum likelihood estimation is infeasible. Moreover, consistency and asymptotic normality of MWLSEs are derived, and their efficiency is proved under the correct specification of the weighting sequence. The estimation performance of QMLE and MWLSE is also compared through simulation experiments and a real data application, showing the superior accuracy of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13487v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mirko Armillotta</dc:creator>
    </item>
    <item>
      <title>Structuring, Sequencing, Staging, Selecting: the 4S method for the longitudinal analysis of multidimensional questionnaires in chronic diseases</title>
      <link>https://arxiv.org/abs/2407.08278</link>
      <description>arXiv:2407.08278v2 Announce Type: replace 
Abstract: In clinical studies, questionnaires are often used to report disease-related manifestations from clinician and/or patient perspectives. Their analysis can help identify relevant manifestations throughout the disease course, enhancing knowledge of disease progression and guiding clinicians in appropriate care provision. However, the analysis of questionnaires in health studies is not straightforward as made of repeated, ordinal, and potentially multidimensional item data. Sum-score summaries may considerably reduce information and hamper interpretation; item changes over time occur along clinical progression; and as many other longitudinal processes, observations may be truncated by events. This work establishes a comprehensive strategy in four consecutive steps to leverage repeated ordinal data from multidimensional questionnaires. The 4S method successively (1) identifies the questionnaire structure into dimensions satisfying three calibration assumptions (unidimensionality, conditional independence, increasing monotonicity), (2) describes each dimension progression using a joint latent process model which includes a continuous-time item response theory model for the longitudinal subpart, (3) aligns each dimension progression with disease stages through a projection approach, and (4) identifies the most informative items across disease stages using the Fisher information. The method is applied to multiple system atrophy (MSA), a rare neurodegenerative disease, with the analysis of daily activity and motor impairments over disease progression. The 4S method provides an effective and complete analytical strategy for questionnaires repeatedly collected in health studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08278v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiphaine Saulnier, Wassilios G. Meissner, Margherita Fabbri, Alexandra Foubert-Samier, C\'ecile Proust-Lima</dc:creator>
    </item>
    <item>
      <title>BRcal: An R Package to Boldness-Recalibrate Probability Predictions</title>
      <link>https://arxiv.org/abs/2409.13858</link>
      <description>arXiv:2409.13858v4 Announce Type: replace 
Abstract: When probability predictions are too cautious for decision making, boldness-recalibration enables responsible emboldening while maintaining the probability of calibration required by the user. We formulate boldness-recalibration as a nonlinear optimization of boldness with a nonlinear inequality constraint on calibration. We further show that recalibration based on the maximized linear log odds likelihood also maximizes the posterior probability of calibration. We introduce BRcal, an R package implementing boldness-recalibration and supporting methodology as recently proposed. The BRcal package provides direct control of the calibration-boldness tradeoff and visualizes how different calibration levels change individual predictions. We present a new real world case study involving housing foreclosure predictions. The BRcal package is available on the Comprehensive R Archive Network (CRAN) (https://cran.r-project.org/web/packages/BRcal/index.html) and on Github (https://github.com/apguthrie/BRcal).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13858v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s42519-025-00466-y</arxiv:DOI>
      <dc:creator>Adeline P. Guthrie, Christopher T. Franck</dc:creator>
    </item>
    <item>
      <title>Random irregular histograms</title>
      <link>https://arxiv.org/abs/2505.22034</link>
      <description>arXiv:2505.22034v2 Announce Type: replace 
Abstract: We propose a new method of histogram construction, providing a fully Bayesian approach to irregular histograms. Our procedure applies Bayesian model selection to a piecewise constant model of the underlying distribution, resulting in a method that selects both the number of bins as well as their location based on the data in a fully automatic fashion. We show that the histogram estimate is consistent with respect to the Hellinger metric under mild regularity conditions, and that it attains a convergence rate equal to the minimax rate (up to a logarithmic factor) for H\"{o}lder continuous densities. Simulation studies indicate that the new method performs comparably to other histogram procedures, both for minimizing the estimation error and for identifying modes. A software implementation is included as supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22034v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oskar H{\o}gberg Simensen, Dennis Christensen, Nils Lid Hjort</dc:creator>
    </item>
    <item>
      <title>Adapting to Misspecification</title>
      <link>https://arxiv.org/abs/2305.14265</link>
      <description>arXiv:2305.14265v5 Announce Type: replace-cross 
Abstract: Empirical research typically involves a robustness-efficiency tradeoff. A researcher seeking to estimate a scalar parameter can invoke strong assumptions to motivate a restricted estimator that is precise but may be heavily biased, or they can relax some of these assumptions to motivate a more robust, but variable, unrestricted estimator. When a bound on the bias of the restricted estimator is available, it is optimal to shrink the unrestricted estimator towards the restricted estimator. For settings where a bound on the bias of the restricted estimator is unknown, we propose adaptive estimators that minimize the percentage increase in worst case risk relative to an oracle that knows the bound. We show that adaptive estimators solve a weighted convex minimax problem and provide lookup tables facilitating their rapid computation. Revisiting some well known empirical studies where questions of model specification arise, we examine the advantages of adapting to -- rather than testing for -- misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14265v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy B. Armstrong, Patrick Kline, Liyang Sun</dc:creator>
    </item>
    <item>
      <title>Fitting Dynamically Misspecified Models: An Optimal Transportation Approach</title>
      <link>https://arxiv.org/abs/2412.20204</link>
      <description>arXiv:2412.20204v2 Announce Type: replace-cross 
Abstract: This paper considers filtering, parameter estimation, and testing for potentially dynamically misspecified state-space models. When dynamics are misspecified, filtered values of state variables often do not satisfy model restrictions, making them hard to interpret, and parameter estimates may fail to characterize the dynamics of filtered variables. To address this, a sequential optimal transportation approach is used to generate a model-consistent sample by mapping observations from a flexible reduced-form to the structural conditional distribution iteratively. Filtered series from the generated sample are model-consistent. Specializing to linear processes, a closed-form Optimal Transport Filtering algorithm is derived. Minimizing the discrepancy between generated and actual observations defines an Optimal Transport Estimator. Its large sample properties are derived. A specification test determines if the model can reproduce the sample path, or if the discrepancy is statistically significant. Empirical applications to trend-cycle decomposition, DSGE models, and affine term structure models illustrate the methodology and the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20204v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Jacques Forneron, Zhongjun Qu</dc:creator>
    </item>
  </channel>
</rss>
