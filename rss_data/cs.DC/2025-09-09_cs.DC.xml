<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Sep 2025 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats</title>
      <link>https://arxiv.org/abs/2509.05303</link>
      <description>arXiv:2509.05303v1 Announce Type: new 
Abstract: Infrastructure as Code (IaC) is fundamental to modern cloud computing, enabling teams to define and manage infrastructure through machine-readable configuration files. However, different cloud service providers utilize diverse IaC formats. The lack of a standardized format requires cloud architects to be proficient in multiple IaC languages, adding complexity to cloud deployment. While Large Language Models (LLMs) show promise in automating IaC creation and maintenance, progress has been limited by the lack of comprehensive benchmarks across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark dataset for evaluating LLM-based IaC generation and mutation across AWS CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset consists of triplets containing initial IaC templates, natural language modification requests, and corresponding updated templates, created through a synthetic data generation pipeline with rigorous validation. We evaluate several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while modern LLMs can achieve high success rates (&gt;95%) in generating syntactically valid IaC across formats, significant challenges remain in semantic alignment and handling complex infrastructure patterns. Our ablation studies highlight the importance of prompt engineering and retry mechanisms in successful IaC generation. We release Multi-IaC-Bench to facilitate further research in AI-assisted infrastructure management and establish standardized evaluation metrics for this crucial domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05303v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sam Davidson, Li Sun, Bhavana Bhasker, Laurent Callot, Anoop Deoras</dc:creator>
    </item>
    <item>
      <title>A Simple and Robust Protocol for Distributed Counting</title>
      <link>https://arxiv.org/abs/2509.05870</link>
      <description>arXiv:2509.05870v1 Announce Type: new 
Abstract: We revisit the distributed counting problem, where a server must continuously approximate the total number of events occurring across $k$ sites while minimizing communication. The communication complexity of this problem is known to be $\Theta(\frac{k}{\epsilon}\log N)$ for deterministic protocols. Huang, Yi, and Zhang (2012) showed that randomization can reduce this to $\Theta(\frac{\sqrt{k}}{\epsilon}\log N)$, but their analysis is restricted to the {\em oblivious setting}, where the stream of events is independent of the protocol's outputs.
  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed counting that removes the oblivious assumption. However, their communication complexity is suboptimal by a $polylog(k)$ factor and their protocol is substantially more complex than the oblivious protocol of Huang et al. (2012). This left open a natural question: could it be that the simple protocol of Huang et al. (2012) is already robust?
  We resolve this question with two main contributions. First, we show that the protocol of Huang et al. (2012) is itself not robust by constructing an explicit adaptive attack that forces it to lose its accuracy. Second, we present a new, surprisingly simple, robust protocol for distributed counting that achieves the optimal communication complexity of $O(\frac{\sqrt{k}}{\epsilon} \log N)$. Our protocol is simpler than that of Xiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and is the first to match the optimal oblivious complexity in the adaptive setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05870v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edith Cohen, Moshe Shechner, Uri Stemmer</dc:creator>
    </item>
    <item>
      <title>DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers</title>
      <link>https://arxiv.org/abs/2509.06046</link>
      <description>arXiv:2509.06046v1 Announce Type: new 
Abstract: We present DISTRIBUTEDANN, a distributed vector search service that makes it possible to search over a single 50 billion vector graph index spread across over a thousand machines that offers 26ms median query latency and processes over 100,000 queries per second. This is 6x more efficient than existing partitioning and routing strategies that route the vector query to a subset of partitions in a scale out vector search system. DISTRIBUTEDANN is built using two well-understood components: a distributed key-value store and an in-memory ANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for serving the Bing search engine, and we share our experience from making this transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06046v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Philip Adams, Menghao Li, Shi Zhang, Li Tan, Qi Chen, Mingqin Li, Zengzhong Li, Knut Risvik, Harsha Vardhan Simhadri</dc:creator>
    </item>
    <item>
      <title>Gathering in Non-Vertex-Transitive Graphs Under Round Robin</title>
      <link>https://arxiv.org/abs/2509.06064</link>
      <description>arXiv:2509.06064v1 Announce Type: new 
Abstract: The Gathering problem for a swarm of robots asks for a distributed algorithm that brings such entities to a common place, not known in advance. We consider the well-known OBLOT model with robots constrained to move along the edges of a graph, hence gathering in one vertex, eventually. Despite the classical setting under which the problem has been usually approached, we consider the `hostile' case where: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities. As a scheduler for robot activation, we consider the "favorable" round-robin case, where robots are activated one at a time.
  Our objective is to achieve a complete characterization of the problem in the broad context of non-vertex-transitive graphs, i.e., graphs where the vertices are partitioned into at least two different classes of equivalence. We provide a resolution algorithm for any configuration of robots moving on such graphs, along with its correctness. Furthermore, we analyze its time complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06064v1</guid>
      <category>cs.DC</category>
      <category>math.CO</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Serafino Cicerone, Alessia Di Fonso, Gabriele Di Stefano, Alfredo Navarra</dc:creator>
    </item>
    <item>
      <title>20 Years in Life of a Smart Building: A retrospective</title>
      <link>https://arxiv.org/abs/2509.06229</link>
      <description>arXiv:2509.06229v1 Announce Type: new 
Abstract: Operating an intelligent smart building automation system in 2025 is met with many challenges: hardware failures, vendor obsolescence, evolving security threats and more. None of these have been comprehensibly addressed by the industrial building nor home automation industries, limiting feasibility of operating large, truly smart automation deployments. This paper introduces KaOS, a distributed control platform for constructing robust and evolvable smart building automation systems using affordable, off-the-shelf IoT hardware. Supporting control applications and distributed system operations by leveraging containerisation and managed resource access, KaOS seeks to achieve flexibility, security, and fault tolerance without sacrificing cost-effectiveness. Initial evaluation confirms the practical feasibility of our approach, highlighting its potential to sustainably maintain and incrementally evolve building control functionalities over extended timeframes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06229v1</guid>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Karolina Skrivankova, Mark Handley, Stephen Hailes</dc:creator>
    </item>
    <item>
      <title>FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving</title>
      <link>https://arxiv.org/abs/2509.06261</link>
      <description>arXiv:2509.06261v1 Announce Type: new 
Abstract: Recent advances in Post-Training Quantization (PTQ) techniques have significantly increased demand for serving quantized large language models (LLMs), enabling higher throughput and substantially reduced memory usage with minimal accuracy loss. Quantized models address memory constraints in LLMs and enhance GPU resource utilization through efficient GPU sharing. However, quantized models have smaller KV block sizes than non-quantized models, causing limited memory efficiency due to memory fragmentation. Also, distinct resource usage patterns between quantized and non-quantized models require efficient scheduling to maximize throughput. To address these challenges, we propose FineServe, an inference serving framework for mixed-precision LLMs. FineServe's key contributions include: (1) KV Slab, a precision-aware adaptive memory management technique dynamically allocating KV cache based on model quantization characteristics, significantly reducing GPU memory fragmentation, and (2) a two-level scheduling framework comprising a global scheduler that places models to GPUs based on request rates, latency SLOs, and memory constraints and efficiency, and a local scheduler that adaptively adjusts batch sizes according to real-time request fluctuations. Experimental results demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x higher token generation throughput compared to the state-of-the-art GPU sharing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06261v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kyungmin Bin, Seungbeom Choi, Jimyoung Son, Jieun Choi, Daseul Bae, Daehyeon Baek, Kihyo Moon, Minsung Jang, Hyojung Lee</dc:creator>
    </item>
    <item>
      <title>MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS</title>
      <link>https://arxiv.org/abs/2509.06362</link>
      <description>arXiv:2509.06362v1 Announce Type: new 
Abstract: Model-as-a-Service (MaaS) platforms face diverse Service Level Objective (SLO) requirements stemming from various large language model (LLM) applications, manifested in contextual complexity, first-token latency, and between-token latency. On the other hand, an LLM instance, when configured with different parallelism strategies and inference batch sizes, exhibits distinct performance characteristics and can thus be used to serve different SLO requirements. However, current LLM inference systems typically deploy instances of the same model with identical configurations, lacking mechanisms to leverage such heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS Orchestrator, which comprises three modules: (1) a profiler characterizing instance performance under diverse parallelism strategies and inference batch sizes; (2) a placer optimizing heterogeneous instance configurations; (3) a distributor enabling SLO-aware request distribution and preventing cascaded timeouts in continuous batching. Experiments show that MaaSO improves the SLO satisfaction ratio by 15 to 30% and reduces response latency by 40 to 60% compared to existing approaches, and significantly lowers overall orchestration overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06362v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Xuan, Zhang yue, Wu Weigang</dc:creator>
    </item>
    <item>
      <title>IM-PIR: In-Memory Private Information Retrieval</title>
      <link>https://arxiv.org/abs/2509.06514</link>
      <description>arXiv:2509.06514v1 Announce Type: new 
Abstract: Private information retrieval (PIR) is a cryptographic primitive that allows a client to securely query one or multiple servers without revealing their specific interests. In spite of their strong security guarantees, current PIR constructions are computationally costly. Specifically, most PIR implementations are memory-bound due to the need to scan extensive databases (in the order of GB), making them inherently constrained by the limited memory bandwidth in traditional processor-centric computing architectures.Processing-in-memory (PIM) is an emerging computing paradigm that augments memory with compute capabilities, addressing the memory bandwidth bottleneck while simultaneously providing extensive parallelism.Recent research has demonstrated PIM's potential to significantly improve performance across a range of data-intensive workloads, including graph processing, genome analysis, and machine learning.
  In this work, we propose the first PIM-based architecture for multi-server PIR. We discuss the algorithmic foundations of the latter and show how its operations align with the core strengths of PIM architectures: extensive parallelism and high memory bandwidth. Based on this observation, we design and implement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM, the first openly commercialized PIM architecture. Our evaluation demonstrates that a PIM-based multi-server PIR implementation significantly improves query throughput by more than 3.7x when compared to a standard CPU-based PIR approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06514v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mpoki Mwaisela, Peterson Yuhala, Pascal Felber, Valerio Schiavoni</dc:creator>
    </item>
    <item>
      <title>Mangrove: Fast and Parallelizable State Replication for Blockchains</title>
      <link>https://arxiv.org/abs/2509.06616</link>
      <description>arXiv:2509.06616v1 Announce Type: new 
Abstract: Mangrove is a novel scaling approach to building blockchains with parallel smart contract support. Unlike in monolithic blockchains, where a single consensus mechanism determines a strict total order over all transactions, Mangrove uses separate consensus instances per smart contract, without a global order. To allow multiple instances to run in parallel while ensuring that no conflicting transactions are committed, we propose a mechanism called Parallel Optimistic Agreement. Additionally, for simple transactions, we leverage a lightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove is optimized for performance under optimistic conditions, where there is no misbehavior and the network is synchronous. Under these conditions, our protocol can achieve a latency of 2 communication steps between creating and executing a transaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06616v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Paramonov, Yann Vonlanthen, Quentin Kniep, Jakub Sliwinski, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>Efficient Fault Localization in a Cloud Stack Using End-to-End Application Service Topology</title>
      <link>https://arxiv.org/abs/2509.05511</link>
      <description>arXiv:2509.05511v1 Announce Type: cross 
Abstract: Cloud application services are distributed in nature and have components across the stack working together to deliver the experience to end users. The wide adoption of microservice architecture exacerbates failure management due to increased service components. To be effective, the strategies to enhance the application service resilience need to be autonomous and developed at the service's granularity, considering its end-to-end components. However, the massive amount of observability data generated by all these components across the service stack poses a significant challenge in reacting to anomalies and restoring the service quality in real time. Identifying the most informative observability data from across the cloud service stack and timely localization of root causes of anomalies thus becomes crucial to ensure service resilience. This article presents a novel approach that considers the application service topology to select the most informative metrics across the cloud stack to support efficient, explainable, and accurate root cause identifications in case of performance anomalies. The usefulness of the selected metrics is then evaluated using the state-of-the-art Root Cause Detection (RCD) algorithm for localizing the root cause of performance anomalies. As a step towards improving the accuracy and efficiency of RCD, this article then proposes the Topology-Aware-RCD (TA-RCD) that incorporates the end-to-end application service topology in RCD. The evaluation of the failure injection studies shows that the proposed approach performs at least 2X times better on average than the state-of-the-art RCD algorithm regarding Top-3 and Top-5 recall.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05511v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dhanya R Mathews, Mudit Verma, Pooja Aggarwal, J. Lakshmi</dc:creator>
    </item>
    <item>
      <title>Workflow for High-Fidelity Dynamic Analysis of Structures with Pile Foundation</title>
      <link>https://arxiv.org/abs/2509.05675</link>
      <description>arXiv:2509.05675v1 Announce Type: cross 
Abstract: The demand for high-fidelity numerical simulations in soil-structure interaction analysis is on the rise, yet a standardized workflow to guide the creation of such simulations remains elusive. This paper aims to bridge this gap by presenting a step-by-step guideline proposing a workflow for dynamic analysis of structures with pile foundations. The proposed workflow encompasses instructions on how to use Domain Reduction Method for loading, Perfectly Matched Layer elements for wave absorption, soil-structure interaction modeling using Embedded interface elements, and domain decomposition for efficient use of processing units. Through a series of numerical simulations, we showcase the practical application of this workflow. Our results reveal the efficacy of the Domain Reduction Method in reducing simulation size without compromising model fidelity, show the precision of Perfectly Matched Layer elements in modeling infinite domains, highlight the efficiency of Embedded Interface elements in establishing connections between structures and the soil domain, and demonstrate the overall effectiveness of the proposed workflow in conducting high-fidelity simulations. While our study focuses on simplified geometries and loading scenarios, it serves as a foundational framework for future research endeavors aimed at exploring more intricate structural configurations and dynamic loading conditions</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05675v1</guid>
      <category>math.NA</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 17th Pan-American Conference on Soil Mechanics and Geotechnical Engineering, 2024, 1733-1740</arxiv:journal_reference>
      <dc:creator>Amin Pakzad, Pedro Arduino, Wenyang Zhang, Ertugrul Tacirouglu</dc:creator>
    </item>
    <item>
      <title>Distributed Deep Learning using Stochastic Gradient Staleness</title>
      <link>https://arxiv.org/abs/2509.05679</link>
      <description>arXiv:2509.05679v1 Announce Type: cross 
Abstract: Despite the notable success of deep neural networks (DNNs) in solving complex tasks, the training process still remains considerable challenges. A primary obstacle is the substantial time required for training, particularly as high performing DNNs tend to become increasingly deep (characterized by a larger number of hidden layers) and require extensive training datasets. To address these challenges, this paper introduces a distributed training method that integrates two prominent strategies for accelerating deep learning: data parallelism and fully decoupled parallel backpropagation algorithm. By utilizing multiple computational units operating in parallel, the proposed approach enhances the amount of training data processed in each iteration while mitigating locking issues commonly associated with the backpropagation algorithm. These features collectively contribute to significant improvements in training efficiency. The proposed distributed training method is rigorously proven to converge to critical points under certain conditions. Its effectiveness is further demonstrated through empirical evaluations, wherein an DNN is trained to perform classification tasks on the CIFAR-10 dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05679v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Viet Hoang Pham, Hyo-Sung Ahn</dc:creator>
    </item>
    <item>
      <title>Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]</title>
      <link>https://arxiv.org/abs/2509.05759</link>
      <description>arXiv:2509.05759v1 Announce Type: cross 
Abstract: This paper presents Tiga, a new design for geo-replicated and scalable transactional databases such as Google Spanner. Tiga aims to commit transactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of scenarios, while maintaining high throughput with minimal computational overhead. Tiga consolidates concurrency control and consensus, completing both strictly serializable execution and consistent replication in a single round. It uses synchronized clocks to proactively order transactions by assigning each a future timestamp at submission. In most cases, transactions arrive at servers before their future timestamps and are serialized according to the designated timestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed and proactive ordering fails, in which case Tiga falls back to a slow path, committing in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can commit more transactions at 1-WRTT latency, and incurs much less throughput overhead. Evaluation results show that Tiga outperforms all baselines, achieving 1.3--7.2$\times$ higher throughput and 1.4--4.6$\times$ lower latency. Tiga is open-sourced at https://github.com/New-Consensus-Concurrency-Control/Tiga.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05759v1</guid>
      <category>cs.NI</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731569.3764854</arxiv:DOI>
      <dc:creator>Jinkun Geng, Shuai Mu, Anirudh Sivaraman, Balaji Prabhakar</dc:creator>
    </item>
    <item>
      <title>Introduction to Number Theoretic Transform</title>
      <link>https://arxiv.org/abs/2509.05884</link>
      <description>arXiv:2509.05884v1 Announce Type: cross 
Abstract: The Number Theoretic Transform (NTT) can be regarded as a variant of the Discrete Fourier Transform. NTT has been quite a powerful mathematical tool in developing Post-Quantum Cryptography and Homomorphic Encryption. The Fourier Transform essentially decomposes a signal into its frequencies. They are traditionally sine or cosine waves. NTT works more over groups or finite fields rather than on a continuous signal and polynomials work as the analog of sine waves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has been proven to be useful in lattice-based cryptography due to its ability to reduce the complexity of polynomial multiplication from quadratic to quasilinear. We have introduced the concepts of cyclic, negacyclic convolutions along with NTT and its inverse and their fast versions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05884v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Banhirup Sengupta, Peenal Gupta, Souvik Sengupta</dc:creator>
    </item>
    <item>
      <title>VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles</title>
      <link>https://arxiv.org/abs/2509.06133</link>
      <description>arXiv:2509.06133v1 Announce Type: cross 
Abstract: Modern vehicles accumulate fragmented lifecycle records across OEMs, owners, and service centers that are difficult to verify and prone to fraud. We propose VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with zero-knowledge proofs (ZKPs) for privacy-preserving verification. VehiclePassport immutably commits to manufacturing, telemetry, and service events while enabling selective disclosure via short-lived JWTs and Groth16 proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at &lt;$0.02 per event, validates proofs in &lt;10 ms, and scales to millions of vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant traceability, and establishes a trustless foundation for insurance, resale, and regulatory applications in global mobility data markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06133v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pradyumna Kaushal</dc:creator>
    </item>
    <item>
      <title>Social Dynamics of DAOs: Power, Onboarding, and Inclusivity</title>
      <link>https://arxiv.org/abs/2509.06163</link>
      <description>arXiv:2509.06163v1 Announce Type: cross 
Abstract: This report explores the often-overlooked cultural and social dynamics shaping participation and power in DAOs. Drawing on qualitative interviews and ethnographic observations, it shows how factors such as financial privilege, informal gatekeeping, visibility bias, and onboarding structures create barriers to meaningful inclusion. While DAOs are frequently framed as permissionless and egalitarian, the lived experiences of contributors reveal a more complex reality, one in which soft power and implicit norms determine people's position within DAOs. Instead of offering solutionist prescriptions, this report argues for a deeper cultural reflection within the DAO ecosystem. It highlights that decentralisation is not solely a protocol-level feature, but an ongoing social process that requires intentional cultivation of trust, belonging, and epistemic plurality. With this report, we want to sharpen the collective awareness of structural blind spots and call for building more inclusive and culturally conscious decentralised systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06163v1</guid>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Victoria Kozlova, Ben Biedermann</dc:creator>
    </item>
    <item>
      <title>Several Performance Bounds on Decentralized Online Optimization are Highly Conservative and Potentially Misleading</title>
      <link>https://arxiv.org/abs/2509.06466</link>
      <description>arXiv:2509.06466v1 Announce Type: cross 
Abstract: We analyze Decentralized Online Optimization algorithms using the Performance Estimation Problem approach which allows, to automatically compute exact worst-case performance of optimization algorithms. Our analysis shows that several available performance guarantees are very conservative, sometimes by multiple orders of magnitude, and can lead to misguided choices of algorithm. Moreover, at least in terms of worst-case performance, some algorithms appear not to benefit from inter-agent communications for a significant period of time. We show how to improve classical methods by tuning their step-sizes, and find that we can save up to 20% on their actual worst-case performance regret.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06466v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Erwan Meunier, Julien M. Hendrickx</dc:creator>
    </item>
    <item>
      <title>Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing</title>
      <link>https://arxiv.org/abs/2509.06552</link>
      <description>arXiv:2509.06552v1 Announce Type: cross 
Abstract: The on-device real-time data distribution shift on devices challenges the generalization of lightweight on-device models. This critical issue is often overlooked in current research, which predominantly relies on data-intensive and computationally expensive fine-tuning approaches. To tackle this, we introduce Persona, a novel personalized method using a prototype-based, backpropagation-free parameter editing framework to enhance model generalization without post-deployment retraining. Persona employs a neural adapter in the cloud to generate a parameter editing matrix based on real-time device data. This matrix adeptly adapts on-device models to the prevailing data distributions, efficiently clustering them into prototype models. The prototypes are dynamically refined via the parameter editing matrix, facilitating efficient evolution. Furthermore, the integration of cross-layer knowledge transfer ensures consistent and context-aware multi-layer parameter changes and prototype assignment. Extensive experiments on vision task and recommendation task on multiple datasets confirm Persona's effectiveness and generality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06552v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.IR</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheqi Lv, Wenqiao Zhang, Kairui Fu, Qi Tian, Shengyu Zhang, Jiajie Su, Jingyuan Chen, Kun Kuang, Fei Wu</dc:creator>
    </item>
    <item>
      <title>Distributed Automatic Generation Control subject to Ramp-Rate-Limits: Anytime Feasibility and Uniform Network-Connectivity</title>
      <link>https://arxiv.org/abs/2509.06588</link>
      <description>arXiv:2509.06588v1 Announce Type: cross 
Abstract: This paper considers automatic generation control over an information-sharing network of communicating generators as a multi-agent system. The optimization solution is distributed among the agents based on information consensus algorithms, while addressing the generators' ramp-rate-limits (RRL). This is typically ignored in the existing linear/nonlinear optimization solutions but they exist in real-time power generation scenarios. Without addressing the RRL, the generators cannot follow the assigned rate of generating power by the optimization algorithm; therefore, the existing solutions may not necessarily converge to the exact optimal cost or may lose feasibility in practice. The proposed solution in this work addresses the ramp-rate-limit constraint along with the box constraint (limits on the generated powers) and the coupling-constraint (generation-demand balance) at all iteration times of the algorithm. The latter is referred to as the anytime feasibility and implies that at every termination point of the algorithm, the balance between the demand and generated power holds. To improve the convergence rate of the algorithm we further consider internal signum-based nonlinearity. We also show that our solution can tolerate communication link removal. This follows from the uniform-connectivity assumption on the communication network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06588v1</guid>
      <category>eess.SY</category>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Doostmohammadian, Hamid R. Rabiee</dc:creator>
    </item>
    <item>
      <title>Flexible Coded Distributed Convolution Computing for Enhanced Straggler Resilience and Numerical Stability in Distributed CNNs</title>
      <link>https://arxiv.org/abs/2411.01579</link>
      <description>arXiv:2411.01579v3 Announce Type: replace 
Abstract: Deploying Convolutional Neural Networks (CNNs) on resource-constrained devices necessitates efficient management of computational resources, often via distributed environments susceptible to latency from straggler nodes. This paper introduces the Flexible Coded Distributed Convolution Computing (FCDCC) framework to enhance straggler resilience and numerical stability in distributed CNNs. We extend Coded Distributed Computing (CDC) with Circulant and Rotation Matrix Embedding (CRME) which was originally proposed for matrix multiplication to high-dimensional tensor convolution. For the proposed scheme, referred to as the Numerically Stable Coded Tensor Convolution (NSCTC) scheme, we also propose two new coded partitioning schemes: Adaptive-Padding Coded Partitioning (APCP) for the input tensor and Kernel-Channel Coded Partitioning (KCCP) for the filter tensor. These strategies enable linear decomposition of tensor convolutions and encoding them into CDC subtasks, combining model parallelism with coded redundancy for robust and efficient execution. Theoretical analysis identifies an optimal trade-off between communication and storage costs. Empirical results validate the framework's effectiveness in computational efficiency, straggler resilience, and scalability across various CNN architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01579v3</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuo Tan, Rui Liu, Xuesong Han, XianLei Long, Kai Wan, Linqi Song, Yong Li</dc:creator>
    </item>
    <item>
      <title>Strong Linearizability without Compare&amp;Swap: The Case of Bags</title>
      <link>https://arxiv.org/abs/2411.19365</link>
      <description>arXiv:2411.19365v2 Announce Type: replace 
Abstract: Because strongly-linearizable objects provide stronger guarantees than linearizability, they serve as valuable building blocks for the design of concurrent data structures. Yet, many objects that have linearizable implementations from base objects weaker than compare&amp;swap objects do not have strongly-linearizable implementations from the same base objects. We focus on one such object: the bag, a multiset from which processes can take unspecified elements.
  We present the first lock-free, strongly-linearizable implementation of a bag from interfering objects (specifically, registers, and test&amp;set objects). This may be surprising, since there are provably no such implementations of stacks or queues.
  Since a bag can contain arbitrarily many elements, an unbounded amount of space must be used to implement it. Hence, it makes sense to also consider a bag with a bound on its capacity. However, like stacks and queues, a bag with capacity $b$ shared by more than $2b$ processes has no lock-free, strongly-linearizable implementation from interfering objects. If we further restrict a bounded bag so that only one process can insert into it, we are able to obtain a lock-free, strongly-linearizable implementation from $O(b + n)$ interfering objects, where $n$ is the number of processes.
  Our goal is to understand the circumstances under which strongly-linearizable implementations of bags exist and, more generally, to understand the power of interfering objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19365v2</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.DISC.2025.22</arxiv:DOI>
      <dc:creator>Faith Ellen, Gal Sela</dc:creator>
    </item>
    <item>
      <title>Dynamic Approximate Maximum Matching in the Distributed Vertex Partition Model</title>
      <link>https://arxiv.org/abs/2504.17338</link>
      <description>arXiv:2504.17338v2 Announce Type: replace 
Abstract: We initiate the study of approximate maximum matching in the vertex partition model, for graphs subject to dynamic changes. We assume that the $n$ vertices of the graph are partitioned among $k$ players, who execute a distributed algorithm and communicate via message passing. An adaptive adversary may perform dynamic updates to the graph topology by inserting or removing edges between the nodes, and the algorithm needs to respond to these changes by adapting the output of the players, with the goal of maintaining an approximate maximum matching. The main performance metric in this setting is the algorithm's update time, which corresponds to the number of rounds required for updating the solution upon an adversarial change. For the standard setting of single-edge insertions and deletions, we obtain the following results:
  We give a randomized Las Vegas algorithm with an expected update time of $O( \frac{\sqrt{m}}{\beta k} )$ rounds that maintains a $\frac{2}{3}$-approximate maximum matching that is also maximal, where $m$ is the number of edges of the graph. We also show that any algorithm has a worst case update time of $\Omega( \frac{n}{\beta k^2\log n} )$, assuming a link bandwidth of $O(\beta\log n)$ bits per round, if it maintains a matching that is maximal and does not have any 3-augmenting paths. For batch-dynamic updates, where the adversary may modify up to $\ell\ge 1$ edges at once, we prove the following: There is a randomized algorithm that succeeds with high probability in maintaining a $\frac{2}{3}$-approximate maximum matching and has a worst case update time of $\Omega( \frac{\ell\log n}{\sqrt{\beta k}} )$ rounds. We show that $\Omega( \frac{\ell}{\beta k \log n} )$ poses a lower bound for maintaining a maximal matching without 3-augmenting paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17338v2</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Robinson, Xianbin Zhu</dc:creator>
    </item>
    <item>
      <title>Grassroots Consensus</title>
      <link>https://arxiv.org/abs/2505.19216</link>
      <description>arXiv:2505.19216v3 Announce Type: replace 
Abstract: Consider people with smartphones operating without external authorities or global resources other than the network itself. In this setting, high-end applications supporting sovereign democratic digital communities, community banks, and digital cooperatives require consensus executed by community members, which must be reconfigurable to support community dynamics.
  The Constitutional Consensus protocol aims to address this need by introducing constitutional self-governance to consensus: participants dynamically amend the participant set, supermajority threshold, and timeout parameter through the consensus protocol itself. We achieve this by enhancing a DAG-based protocol (like Cordial Miners) with participant-controlled reconfiguration, while also supporting both high- and low-throughput operation (like Morpheus), remaining quiescent when idle. This three-way synthesis uniquely combines: (1) constitutional amendments for self-governance, (2) a cryptographic DAG structure for simplicity, parallelism, and throughput, and (3) both high- and low-throughput operation. The protocol achieves consensus in $3\delta$, maintains O(n) amortized communication complexity during high throughput, and seamlessly transitions between modes. The basic protocol (without constitutional amendments) realizes these features in 25 lines of pseudocode, making it one of the most concise consensus protocols for eventual synchrony.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19216v3</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Idit Keidar, Andrew Lewis-Pye, Ehud Shapiro</dc:creator>
    </item>
    <item>
      <title>The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU Libraries</title>
      <link>https://arxiv.org/abs/2508.07071</link>
      <description>arXiv:2508.07071v2 Announce Type: replace 
Abstract: Existing GPU libraries often struggle to fully exploit the parallel resources and on-chip memory (SRAM) of GPUs when chaining multiple GPU functions as individual kernels. While Kernel Fusion (KF) techniques like Horizontal Fusion (HF) and Vertical Fusion (VF) can mitigate this, current library implementations often require library developers to manually create fused kernels. Hence, library users rely on limited sets of pre-compiled or template-based fused kernels. This limits the use cases that can benefit from HF and VF and increases development costs. In order to solve these issues, we present a novel methodology for building GPU libraries that enables automatic on-demand HF and VF for arbitrary combinations of GPU library functions. Our methodology defines reusable, fusionable components that users combine via high-level programming interfaces. Leveraging C++17 metaprogramming features available in compilers like nvcc, our methodology generates a single and optimized fused kernel tailored to the user's specific sequence of operations at compile time, without needing a custom compiler or manual development and pre-compilation of kernel combinations. This approach abstracts low-level GPU complexities while maximizing GPU resource utilization and keeping intermediate data in SRAM. We provide an open-source implementation demonstrating significant speedups compared to traditional libraries in various benchmarks, validating the effectiveness of this methodology for improving GPU performance in the range of 2x to more than 1000x, while preserving high-level programmability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07071v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Amoros (Universitat Politecnica de Catalunya), Albert Andaluz (Independent researcher), Johnny Nunez (NVIDIA), Antonio J. Pena (Barcelona Supercomputing Center)</dc:creator>
    </item>
    <item>
      <title>Formal Modeling and Verification of the Algorand Consensus Protocol in CADP</title>
      <link>https://arxiv.org/abs/2508.19452</link>
      <description>arXiv:2508.19452v3 Announce Type: replace 
Abstract: Algorand is a scalable and secure permissionless blockchain that achieves proof-of-stake consensus via cryptographic self-sortition and binary Byzantine agreement. In this paper, we present a process algebraic model of the Algorand consensus protocol with the aim of enabling rigorous formal verification. Our model captures the behavior of participants with respect to the structured alternation of consensus steps toward a committee-based agreement by means of a probabilistic process calculus. We validate the correctness of the protocol in the absence of adversaries and then extend our model to capture the influence of coordinated malicious nodes that can force the commit of an empty block instead of the proposed one. The adversarial scenario is analyzed through an equivalence-checking-based noninterference framework that we have implemented in the CADP verification toolkit. In addition to highlighting both the robustness and the limitations of the Algorand protocol under adversarial assumptions, this work illustrates the added value of using formal methods for the analysis of blockchain consensus algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19452v3</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Esposito, Francesco P. Rossi, Marco Bernardo, Francesco Fabris, Hubert Garavel</dc:creator>
    </item>
    <item>
      <title>Beyond Pairwise Comparisons: Unveiling Structural Landscape of Mobile Robot Models</title>
      <link>https://arxiv.org/abs/2508.19805</link>
      <description>arXiv:2508.19805v2 Announce Type: replace 
Abstract: Understanding the computational power of mobile robot systems is a fundamental challenge in distributed computing. While prior work has focused on pairwise separations between models, we explore how robot capabilities, light observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable only in the strongest model -- fully-synchronous robots with full mutual lights ($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and TAR(d)* problems to demonstrate how internal memory and lights interact with synchrony: under weak synchrony, internal memory alone is insufficient, while full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and ZCC to show fine-grained separations between $\mathcal{FSTA}$ and $\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and Leave Place Convergence (LP-Cv), illustrating the limitations of internal memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models, revealing structural phenomena only visible through higher-order comparisons. Our work provides new impossibility criteria and deepens the understanding of how observability, memory, and synchrony collectively shape the computational power of mobile robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19805v2</guid>
      <category>cs.DC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shota Naito, Tsukasa Ninomiya, Koichi Wada</dc:creator>
    </item>
    <item>
      <title>DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving</title>
      <link>https://arxiv.org/abs/2509.01083</link>
      <description>arXiv:2509.01083v2 Announce Type: replace 
Abstract: Speculative decoding accelerates large language model inference, but its reliance on a fixed speculation length is suboptimal in large-batch serving environments with diverse requests. This paper explores a new direction for dynamic adaptation by investigating a novel class of post-hoc, diagnostic signals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free framework built on two primary components: (1) a predictive signal based on the variance of the Kullback-Leibler (KLD) divergence, which diagnoses the generation's regional stability, and (2) an adaptive speculation length cap to mitigate the straggler problem in per-sequence decoding. Experiments demonstrate the potential of using KLD-based stability signals for dynamic adaptation. An algorithm guided by these signals achieves end-to-end latency competitive with leading baselines and exhibits superior robustness across diverse workloads. This robustness is particularly valuable in challenging low-acceptance-rate regimes, where the proposed signal maintains its diagnostic utility. Collectively, these findings validate post-hoc signals as a valuable component for building more robust and intelligent LLM inference systems, and highlight a promising direction for future research on dynamic speculation length adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01083v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyu Yang, Jae-Young Choi, Kihyo Moon, Minsung Jang, Eunjoo Jeon</dc:creator>
    </item>
    <item>
      <title>XChainWatcher: Monitoring and Identifying Attacks in Cross-Chain Bridges</title>
      <link>https://arxiv.org/abs/2410.02029</link>
      <description>arXiv:2410.02029v3 Announce Type: replace-cross 
Abstract: Cross-chain bridges are a type of middleware for blockchain interoperability that supports the transfer of assets and data across blockchains. However, several of these bridges have vulnerabilities that have caused 3.2 billion dollars in losses since May 2021. Some studies have revealed the existence of these vulnerabilities, but there is little quantitative research available, and there are no safeguard mechanisms to protect bridges from such attacks. Furthermore, no studies are available on the practices of cross-chain bridges that can cause financial losses. We propose \toolName~(Cross-Chain Watcher), a modular and extensible logic-driven anomaly detector for cross-chain bridges. It operates in three main phases: (1) decoding events and transactions from multiple blockchains, (2) building logic relations from the extracted data, and (3) evaluating these relations against a set of detection rules. Using \toolName, we analyze data from two previously attacked bridges: the Ronin and Nomad bridges. \toolName~was able to successfully identify the transactions that led to losses of \$611M and \$190M (USD) and surpassed the results obtained by a reputable security firm in the latter. We not only uncover successful attacks, but also reveal other anomalies, such as 37 cross-chain transactions (\CCTX) that these bridges should not have accepted, failed attempts to exploit Nomad, over \$7.8M worth of tokens locked on one chain but never released on Ethereum, and \$200K lost by users due to inadequate interaction with bridges. We provide the first open dataset of 81,000 \CCTXS~across three blockchains, capturing more than \$4.2B in token transfers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02029v3</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andr\'e Augusto, Rafael Belchior, Jonas Pfannschmidt, Andr\'e Vasconcelos, Miguel Correia</dc:creator>
    </item>
    <item>
      <title>Sense4FL: Vehicular Crowdsensing Enhanced Federated Learning for Object Detection in Autonomous Driving</title>
      <link>https://arxiv.org/abs/2503.17697</link>
      <description>arXiv:2503.17697v2 Announce Type: replace-cross 
Abstract: To accommodate constantly changing road conditions, real-time vision model training is essential for autonomous driving (AD). Federated learning (FL) serves as a promising paradigm to enable autonomous vehicles to train models collaboratively with their onboard computing resources. However, existing vehicle selection schemes for FL all assume predetermined and location-independent vehicles' datasets, neglecting the fact that vehicles collect training data along their routes, thereby resulting in suboptimal vehicle selection. In this paper, we focus on the fundamental perception problem and propose Sense4FL, a vehicular crowdsensing-enhanced FL framework featuring \textit{trajectory-dependent} vehicular \textit{training data collection} to \rev{improve the object detection quality} in AD for a region. To this end, we first derive the convergence bound of FL by considering the impact of both vehicles' uncertain trajectories and uploading probabilities, from which we discover that minimizing the training loss is equivalent to minimizing a weighted sum of local and global earth mover's distance (EMD) between vehicles' collected data distribution and global data distribution. Based on this observation, we formulate the trajectory-dependent vehicle selection and data collection problem for FL in AD. Given that the problem is NP-hard, we develop an efficient algorithm to find the solution with an approximation guarantee. Extensive simulation results have demonstrated the effectiveness of our approach in improving object detection performance compared with existing benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17697v2</guid>
      <category>cs.RO</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanan Ma, Senkang Hu, Zhengru Fang, Yun Ji, Yiqin Deng, Yuguang Fang</dc:creator>
    </item>
    <item>
      <title>Byzantine-Robust Federated Learning Using Generative Adversarial Networks</title>
      <link>https://arxiv.org/abs/2503.20884</link>
      <description>arXiv:2503.20884v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) enables collaborative model training across distributed clients without sharing raw data, but its robustness is threatened by Byzantine behaviors such as data and model poisoning. Existing defenses face fundamental limitations: robust aggregation rules incur error lower bounds that grow with client heterogeneity, while detection-based methods often rely on heuristics (e.g., a fixed number of malicious clients) or require trusted external datasets for validation. We present a defense framework that addresses these challenges by leveraging a conditional generative adversarial network (cGAN) at the server to synthesize representative data for validating client updates. This approach eliminates reliance on external datasets, adapts to diverse attack strategies, and integrates seamlessly into standard FL workflows. Extensive experiments on benchmark datasets demonstrate that our framework accurately distinguishes malicious from benign clients while maintaining overall model accuracy. Beyond Byzantine robustness, we also examine the representativeness of synthesized data, computational costs of cGAN training, and the transparency and scalability of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20884v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Usama Zafar, Andr\'e Teixeira, Salman Toor</dc:creator>
    </item>
    <item>
      <title>Scaling Intelligence: Designing Data Centers for Next-Gen Language Models</title>
      <link>https://arxiv.org/abs/2506.15006</link>
      <description>arXiv:2506.15006v3 Announce Type: replace-cross 
Abstract: The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8 trillion parameters, demands a fundamental rethinking of data center architecture to ensure scalability, efficiency, and cost-effectiveness. Our work provides a comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat optical), the size of the scale-out domain, and popular parallelism/optimization strategies used in LLMs. We introduce and evaluate FullFlat network architectures, which provide uniform high-bandwidth, low-latency connectivity between all nodes, and demonstrate their transformative impact on performance and scalability. Through detailed sensitivity analyses, we quantify the benefits of overlapping compute and communication, leveraging hardware-accelerated collectives, widening the scale-out domain, and increasing memory capacity. Our study spans both sparse (mixture of experts) and dense transformer-based LLMs, revealing how system design choices affect Model FLOPS Utilization (MFU = Model FLOPS per token * Observed tokens per second / Peak FLOPS of the hardware) and overall throughput. For the co-design study, we utilized an analytical performance modeling tool capable of predicting LLM runtime within 10% of real-world measurements. Our findings offer actionable insights and a practical roadmap for designing AI data centers that can efficiently support trillion-parameter models, reduce optimization complexity, and sustain the rapid evolution of AI capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15006v3</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesmin Jahan Tithi, Hanjiang Wu, Avishaii Abuhatzera, Fabrizio Petrini</dc:creator>
    </item>
    <item>
      <title>ELK: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques</title>
      <link>https://arxiv.org/abs/2507.11506</link>
      <description>arXiv:2507.11506v2 Announce Type: replace-cross 
Abstract: To meet the increasing demand of deep learning (DL) models, AI chips are employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency interconnect for direct inter-core data exchange. However, it is not easy to explore the efficiency of these inter-core connected AI (ICCA) chips, due to a fundamental tussle among compute (per-core execution), communication (inter-core data exchange), and I/O (off-chip data access).
  In this paper, we develop Elk, a DL compiler framework to maximize the efficiency of ICCA chips by jointly trading off all the three performance factors discussed above. Elk structures these performance factors into configurable parameters and forms a global trade-off space in the DL compiler. To systematically explore this space and maximize overall efficiency, Elk employs a new inductive operator scheduling policy and a cost-aware on-chip memory allocation algorithm. It generates globally optimized execution plans that best overlap off-chip data loading and on-chip execution. To examine the efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different interconnect network topologies. Elk achieves 94% of the ideal roofline performance of ICCA chips on average, showing the benefits of supporting large DL models on ICCA chips. We also show Elk's capability of enabling architecture design space exploration for new ICCA chip development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11506v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>In Proceedings of the 58th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO'25), Seoul, Korea, October, 2025</arxiv:journal_reference>
      <dc:creator>Yiqi Liu, Yuqi Xue, Noelle Crawford, Jilong Xue, Jian Huang</dc:creator>
    </item>
    <item>
      <title>Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures</title>
      <link>https://arxiv.org/abs/2508.07423</link>
      <description>arXiv:2508.07423v3 Announce Type: replace-cross 
Abstract: As the particle physics community needs higher and higher precisions in order to test our current model of the subatomic world, larger and larger datasets are necessary. With upgrades scheduled for the detectors of colliding-beam experiments around the world, and specifically at the Large Hadron Collider at CERN, more collisions and more complex interactions are expected. This directly implies an increase in data produced and consequently in the computational resources needed to process them. At CERN, the amount of data produced is gargantuan. This is why the data have to be heavily filtered and selected in real time before being permanently stored. This data can then be used to perform physics analyses, in order to expand our current understanding of the universe and improve the Standard Model of physics. This real-time filtering, known as triggering, involves complex processing happening often at frequencies as high as 40 MHz. This thesis contributes to understanding how machine learning models can be efficiently deployed in such environments, in order to maximize throughput and minimize energy consumption. Inevitably, modern hardware designed for such tasks and contemporary algorithms are needed in order to meet the challenges posed by the stringent, high-frequency data rates. In this work, I present our graph neural network-based pipeline, developed for charged particle track reconstruction at the LHCb experiment at CERN. The pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely on GPUs. Its performance was compared against the classical tracking algorithms currently in production at LHCb. The pipeline was also accelerated on the FPGA architecture, and its performance in terms of power consumption and processing speed was compared against the GPU implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07423v3</guid>
      <category>physics.data-an</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fotis I. Giasemis</dc:creator>
    </item>
    <item>
      <title>Online Identification of IT Systems through Active Causal Learning</title>
      <link>https://arxiv.org/abs/2509.02130</link>
      <description>arXiv:2509.02130v2 Announce Type: replace-cross 
Abstract: Identifying a causal model of an IT system is fundamental to many branches of systems engineering and operation. Such a model can be used to predict the effects of control actions, optimize operations, diagnose failures, detect intrusions, etc., which is central to achieving the longstanding goal of automating network and system management tasks. Traditionally, causal models have been designed and maintained by domain experts. This, however, proves increasingly challenging with the growing complexity and dynamism of modern IT systems. In this paper, we present the first principled method for online, data-driven identification of an IT system in the form of a causal model. The method, which we call active causal learning, estimates causal functions that capture the dependencies among system variables in an iterative fashion using Gaussian process regression based on system measurements, which are collected through a rollout-based intervention policy. We prove that this method is optimal in the Bayesian sense and that it produces effective interventions. Experimental validation on a testbed shows that our method enables accurate identification of a causal system model while inducing low interference with system operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02130v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kim Hammar, Rolf Stadler</dc:creator>
    </item>
  </channel>
</rss>
