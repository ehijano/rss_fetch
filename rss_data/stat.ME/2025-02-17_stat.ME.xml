<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 04:16:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Flexible Empirical Bayesian Approaches to Pharmacovigilance for Simultaneous Signal Detection and Signal Strength Estimation in Spontaneous Reporting Systems Data</title>
      <link>https://arxiv.org/abs/2502.09816</link>
      <description>arXiv:2502.09816v1 Announce Type: new 
Abstract: Inferring adverse events (AEs) of medical products from Spontaneous Reporting Systems (SRS) databases is a core challenge in contemporary pharmacovigilance. Bayesian methods for pharmacovigilance are attractive for their rigorous ability to simultaneously detect potential AE signals and estimate their strengths/degrees of relevance. However, existing Bayesian and empirical Bayesian methods impose restrictive parametric assumptions and/or demand substantial computational resources, limiting their practical utility. This paper introduces a suite of novel, scalable empirical Bayes methods for pharmacovigilance that utilize flexible non-parametric priors and custom, efficient data-driven estimation techniques to enhance signal detection and signal strength estimation at a low computational cost. Our highly flexible methods accommodate a broader range of data and achieve signal detection performance comparable to or better than existing Bayesian and empirical Bayesian approaches. More importantly, they provide coherent and high-fidelity estimation and uncertainty quantification for potential AE signal strengths, offering deeper insights into the comparative importance and relevance of AEs. Extensive simulation experiments across diverse data-generating scenarios demonstrate the superiority of our methods in terms of accurate signal strength estimation, as measured by replication root mean squared errors. Additionally, our methods maintain or exceed the signal detection performance of state-of-the-art techniques, as evaluated by frequentist false discovery rates and sensitivity metrics. Applications on FDA FAERS data for the statin group of drugs reveal interesting insights through Bayesian posterior probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09816v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihao Tan, Marianthi Markatou, Saptarshi Chakraborty</dc:creator>
    </item>
    <item>
      <title>Pseudo-spectra of multivariate inhomogeneous spatial point processes</title>
      <link>https://arxiv.org/abs/2502.09948</link>
      <description>arXiv:2502.09948v1 Announce Type: new 
Abstract: In this article, we propose a spectral method for multivariate inhomogeneous spatial point processes. A key ingredient is utilizing the asymptotic behavior of the periodogram. The periodogram is an asymptotically unbiased estimator of the spectrum of a second-order stationary point process. By extending this property, we show that under inhomogeneity, the expectation of the periodogram also converges to a matrix-valued function, which we refer to as the pseudo-spectrum. The pseudo-spectrum shares similar properties with the spectrum of stationary processes and can be interpreted using local parameters. We derive a consistent estimator of the pseudo-spectrum through kernel smoothing and propose two bandwidth selection methods. The performance and utility of our frequency domain methods are illustrated through simulation studies and a real data analysis of rainforest data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09948v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi-Wen Ding, Junho Yang, Joonho Shin</dc:creator>
    </item>
    <item>
      <title>Thinning a Wishart Random Matrix</title>
      <link>https://arxiv.org/abs/2502.09957</link>
      <description>arXiv:2502.09957v1 Announce Type: new 
Abstract: Recent work has explored data thinning, a generalization of sample splitting that involves decomposing a (possibly matrix-valued) random variable into independent components. In the special case of a $n \times p$ random matrix with independent and identically distributed $N_p(\mu, \Sigma)$ rows, Dharamshi et al. (2024a) provides a comprehensive analysis of the settings in which thinning is or is not possible: briefly, if $\Sigma$ is unknown, then one can thin provided that $n&gt;1$. However, in some situations a data analyst may not have direct access to the data itself. For example, to preserve individuals' privacy, a data bank may provide only summary statistics such as the sample mean and sample covariance matrix. While the sample mean follows a Gaussian distribution, the sample covariance follows (up to scaling) a Wishart distribution, for which no thinning strategies have yet been proposed. In this note, we fill this gap: we show that it is possible to generate two independent data matrices with independent $N_p(\mu, \Sigma)$ rows, based only on the sample mean and sample covariance matrix. These independent data matrices can either be used directly within a train-test paradigm, or can be used to derive independent summary statistics. Furthermore, they can be recombined to yield the original sample mean and sample covariance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09957v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ameer Dharamshi, Anna Neufeld, Lucy L. Gao, Daniela Witten, Jacob Bien</dc:creator>
    </item>
    <item>
      <title>Statistical modeling of categorical trajectories with multivariate functional principal components</title>
      <link>https://arxiv.org/abs/2502.09986</link>
      <description>arXiv:2502.09986v1 Announce Type: new 
Abstract: There are many examples in which the statistical units of interest are samples of a continuous time categorical random process, that is to say a continuous time stochastic process taking values in a finite state space. Without loosing any information, we associate to each state a binary random function, taking values in $\{0,1\}$, and turn the problem of statistical modeling of a categorical process into a multivariate functional data analysis issue. The (multivariate) covariance operator has nice interpretations in terms of departure from independence of the joint probabilities and the multivariate functional principal components are simple to interpret. Under the weak hypothesis assuming only continuity in probability of the $0-1$ trajectories, it is simple to build consistent estimators of the covariance kernel and perform multivariate functional principal components analysis. The sample paths being piecewise constant, with a finite number of jumps, this a rare case in functional data analysis in which the trajectories can be observed exhaustively. The approach is illustrated on a data set of sensory perceptions, considering different gustometer-controlled stimuli experiments. We show how it can be easily extended to analyze experiments, such as temporal check-all-that-apply, in which two states or more can be observed at the same time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09986v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Herv\'e Cardot, Caroline Peltier</dc:creator>
    </item>
    <item>
      <title>Using Subspace Algorithms for the Estimation of Linear State Space Models for Over-Differenced Processes</title>
      <link>https://arxiv.org/abs/2502.09987</link>
      <description>arXiv:2502.09987v1 Announce Type: new 
Abstract: Subspace methods like canonical variate analysis (CVA) are regression based methods for the estimation of linear dynamic state space models. They have been shown to deliver accurate (consistent and asymptotically equivalent to quasi maximum likelihood estimation using the Gaussian likelihood) estimators for invertible stationary autoregressive moving average (ARMA) processes.
  These results use the assumption that the spectral density of the stationary process does not have zeros on the unit circle. This assumption is violated, for example, for over-differenced series that may arise in the setting of co-integrated processes made stationary by differencing. A second source of spectral zeros is inappropriate seasonal differencing to obtain seasonally adjusted data. This occurs, for example, by investigating yearly differences of processes that do not contain unit roots at all seasonal frequencies.
  In this paper we show consistency for the CVA estimators for vector processes containing spectral zeros. The derived rates of convergence demonstrate that over-differencing can severely harm the asymptotic properties of the estimators making a case for working with unadjusted data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09987v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dietmar Bauer</dc:creator>
    </item>
    <item>
      <title>Principal Decomposition with Nested Submanifolds</title>
      <link>https://arxiv.org/abs/2502.10010</link>
      <description>arXiv:2502.10010v1 Announce Type: new 
Abstract: Over the past decades, the increasing dimensionality of data has increased the need for effective data decomposition methods. Existing approaches, however, often rely on linear models or lack sufficient interpretability or flexibility. To address this issue, we introduce a novel nonlinear decomposition technique called the principal nested submanifolds, which builds on the foundational concepts of principal component analysis. This method exploits the local geometric information of data sets by projecting samples onto a series of nested principal submanifolds with progressively decreasing dimensions. It effectively isolates complex information within the data in a backward stepwise manner by targeting variations associated with smaller eigenvalues in local covariance matrices. Unlike previous methods, the resulting subspaces are smooth manifolds, not merely linear spaces or special shape spaces. Validated through extensive simulation studies and applied to real-world RNA sequencing data, our approach surpasses existing models in delineating intricate nonlinear structures. It provides more flexible subspace constraints that improve the extraction of significant data components and facilitate noise reduction. This innovative approach not only advances the non-Euclidean statistical analysis of data with low-dimensional intrinsic structure within Euclidean spaces, but also offers new perspectives for dealing with high-dimensional noisy data sets in fields such as bioinformatics and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10010v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaji Su, Zhigang Yao</dc:creator>
    </item>
    <item>
      <title>The Probability of Tiered Benefit: Partial Identification with Robust and Stable Inference</title>
      <link>https://arxiv.org/abs/2502.10049</link>
      <description>arXiv:2502.10049v1 Announce Type: new 
Abstract: We define the probability of tiered benefit in scenarios with a binary exposure and an outcome that is either ordered categorical with $K \geq 2$ tiers or continuous partitioned by $K-1$ fixed thresholds into disjoint intervals. Similar to other pure counterfactual queries, this parameter is not $g$-identifiable without additional assumptions. We demonstrate that strong monotonicity does not suffice for point identification when $K \geq 3$ and provide sharp bounds both with and without this constraint. Inference and uncertainty quantification for these bounds are challenging due to potential nonregularity induced by ambiguities in the individualized optimization problems underlying the bounds. Such ambiguities can arise from immunities or null treatment effects in subpopulations with positive probability, affecting the lower bound estimate and hindering conservative inference. To address these issues, we extend the available stabilized one-step correction (S1S) procedure by incorporating stratum-specific stabilizing matrices. Through simulations, we illustrate the benefits of this approach over existing alternatives. We apply our method to estimate bounds on the probability of tiered benefit and harm from ADHD pharmacological treatment upon academic achievement, employing observational data from Norwegian schoolchildren with ADHD. Our findings suggest that although girls and children with previously low numeracy scores experience moderate probabilities of both treatment benefit and harm, in no group does the minimum benefit surpass the maximum harm, complicating a clear-cut treatment recommendation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10049v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Johan de Aguas, Sebastian Krumscheid, Johan Pensar, Guido Biele</dc:creator>
    </item>
    <item>
      <title>Revisiting the Berkeley Admissions data: Statistical Tests for Causal Hypotheses</title>
      <link>https://arxiv.org/abs/2502.10161</link>
      <description>arXiv:2502.10161v1 Announce Type: new 
Abstract: Reasoning about fairness through correlation-based notions is rife with pitfalls. The 1973 University of California, Berkeley graduate school admissions case from Bickel et. al. (1975) is a classic example of one such pitfall, namely Simpson's paradox. The discrepancy in admission rates among males and female applicants, in the aggregate data over all departments, vanishes when admission rates per department are examined. We reason about the Berkeley graduate school admissions case through a causal lens. In the process, we introduce a statistical test for causal hypothesis testing based on Pearl's instrumental-variable inequalities (Pearl 1995). We compare different causal notions of fairness that are based on graphical, counterfactual and interventional queries on the causal model, and develop statistical tests for these notions that use only observational data. We study the logical relations between notions, and show that while notions may not be equivalent, their corresponding statistical tests coincide for the case at hand. We believe that a thorough case-based causal analysis helps develop a more principled understanding of both causal hypothesis testing and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10161v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourbh Bhadane, Joris M. Mooij, Philip Boeken, Onno Zoeter</dc:creator>
    </item>
    <item>
      <title>Identifying Key Influencers using an Egocentric Network-based Randomized Design</title>
      <link>https://arxiv.org/abs/2502.10170</link>
      <description>arXiv:2502.10170v1 Announce Type: new 
Abstract: Behavioral health interventions, such as trainings or incentives, are implemented in settings where individuals are interconnected, and the intervention assigned to some individuals may also affect others within their network. Evaluating such interventions requires assessing both the effect of the intervention on those who receive it and the spillover effect on those connected to the treated individuals. With behavioral interventions, spillover effects can be heterogeneous in that certain individuals, due to their social connectedness and individual characteristics, are more likely to respond to the intervention and influence their peers' behaviors. Targeting these individuals can enhance the effectiveness of interventions in the population. In this paper, we focus on an Egocentric Network-based Randomized Trial (ENRT) design, wherein a set of index participants is recruited from the population and randomly assigned to the treatment group, while concurrently collecting outcome data on their nominated network members, who remina untreated. In such design, spillover effects on network members may vary depending on the characteristics of the index participant. Here, we develop a testing method, the Multiple Comparison with Best (MCB), to identify subgroups of index participants whose treatment exhibits the largest spillover effect on their network members. Power and sample size calculations are then provided to design ENRTs that can detect key influencers. The proposed methods are demonstrated in a study on network-based peer HIV prevention education program, providing insights into strategies for selecting peer educators in peer education interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10170v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhibing He, Junhan Fan, Ashley Buchanan, Donna Spiegelman, Laura Forastiere</dc:creator>
    </item>
    <item>
      <title>Assessing the Accuracy of Multisource Register-based Official Statistics for Multinomial Outcomes</title>
      <link>https://arxiv.org/abs/2502.10182</link>
      <description>arXiv:2502.10182v1 Announce Type: new 
Abstract: The emergence of new data sources and statistical methods is driving an update in the traditional official statistics paradigm. As an example, the Italian National Institute of Statistics (ISTAT) is undergoing a significant modernisation of the data production process, transitioning from a statistical paradigm based on single sources (census, sample surveys, or administrative data) to an integrated system of statistical registers. The latter results from an integration process of administrative and survey data based on different statistical methods, and, as such, prone to different sources of error. This work discusses and validates a global measure of error assessment for such multisource register-based statistics. Focusing on two important sources of uncertainty (sampling and modelling), we provide an analytical solution that well approximates the global error of mass-imputation procedures for multi-category type of outcomes, assuming a multinomial logistic model. Among other advantages, the proposed measure results in an interpretable, computationally feasible, and flexible approach, while allowing for unplanned on-the-fly statistics on totals to be supported by accuracy estimates. An application to education data from the Base Register of Individuals from ISTAT's integrated system of statistical registers is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10182v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nina Deliu, Piero Demetrio Falorsi, Stefano Falorsi, Diego Chianella, Giorgio Alleva</dc:creator>
    </item>
    <item>
      <title>Robust variance estimators in application to segmentation of measurement data distorted by impulsive and non-Gaussian noise</title>
      <link>https://arxiv.org/abs/2502.10275</link>
      <description>arXiv:2502.10275v1 Announce Type: new 
Abstract: The paper algorithmizes the problem of regime change point identification for data measured in a system exhibiting impulsive behaviors. This is a fundamental challenge for annotation of measurement data relevant, e.g., for designing data-driven autonomous systems. The contribution consists in the formulation of an offline robust methodology based on the classical approach for structural break detection. The problem of data segmentation is considered in the context of scale change, which physically can be translated into the occurrence of a critical event that reorganizes the system structure. The main advantage of our approach is that it does not require the existence of a variance of the data distribution. The efficiency has been evaluated for simulated data from two distributions and for real-world datasets measured in financial, mechanical, and medical systems. Simulation studies show that in the most challenging case, the error in estimating regime change is 20 times smaller for robust approach compared to the classical one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10275v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justyna Witulska, Anna Zaleska, Natalia Kremzer-Osiadacz, Agnieszka Wy{\l}oma\'nska, Ireneusz Jab{\l}o\'nski</dc:creator>
    </item>
    <item>
      <title>A Latent Causal Inference Framework for Ordinal Variables</title>
      <link>https://arxiv.org/abs/2502.10276</link>
      <description>arXiv:2502.10276v1 Announce Type: new 
Abstract: Ordinal variables, such as on the Likert scale, are common in applied research. Yet, existing methods for causal inference tend to target nominal or continuous data. When applied to ordinal data, this fails to account for the inherent ordering or imposes well-defined relative magnitudes. Hence, there is a need for specialised methods to compute interventional effects between ordinal variables while accounting for their ordinality. One potential framework is to presume a latent Gaussian Directed Acyclic Graph (DAG) model: that the ordinal variables originate from marginally discretizing a set of Gaussian variables whose latent covariance matrix is constrained to satisfy the conditional independencies inherent in a DAG. Conditioned on a given latent covariance matrix and discretisation thresholds, we derive a closed-form function for ordinal causal effects in terms of interventional distributions in the latent space. Our causal estimation combines naturally with algorithms to learn the latent DAG and its parameters, like the Ordinal Structural EM algorithm. Simulations demonstrate the applicability of the proposed approach in estimating ordinal causal effects both for known and unknown structures of the latent graph. As an illustration of a real-world use case, the method is applied to survey data of 408 patients from a study on the functional relationships between symptoms of obsessive-compulsive disorder and depression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10276v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Martina Scauda, Jack Kuipers, Giusi Moffa</dc:creator>
    </item>
    <item>
      <title>A Mechanistic Framework for Collider Detection in Observational Data</title>
      <link>https://arxiv.org/abs/2502.10317</link>
      <description>arXiv:2502.10317v1 Announce Type: new 
Abstract: Understanding directionality is crucial for identifying causal structures from observational data. A key challenge lies in detecting collider structures, where a $V$--structure is formed between a child node $Z$ receiving directed edges from parents $X$ and $Y$, denoted by $X \rightarrow Z \leftarrow Y$. Traditional causal discovery approaches, such as constraint-based and score-based structure learning algorithms, do not provide statistical inference on estimated pathways and are often sensitive to latent confounding. To overcome these issues, we introduce methodology to quantify directionality in collider structures using a pair of conditional asymmetry coefficients to simultaneously examine validity of the pathways $Y \rightarrow Z$ and $X \rightarrow Z$ in the collider structure. These coefficients are based on Shannon's differential entropy. Leveraging kernel-based conditional density estimation and a nonparametric smoothing technique, we utilise our proposed method to estimate collider structures and provide uncertainty quantification.
  Simulation studies demonstrate that our method outperforms existing structure learning algorithms in accurately identifying collider structures. We further apply our approach to investigate the role of blood pressure as a collider in epigenetic DNA methylation, uncovering novel insights into the genetic regulation of blood pressure. This framework represents a significant advancement in causal structure learning, offering a robust, nonparametric method for collider detection with practical applications in biostatistics and epidemiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10317v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soumik Purkayastha, Peter X. -K. Song</dc:creator>
    </item>
    <item>
      <title>Tensor-variate Gaussian process regression for efficient emulation of complex systems: comparing regressor and covariance structures in outer product and parallel partial emulators</title>
      <link>https://arxiv.org/abs/2502.10319</link>
      <description>arXiv:2502.10319v1 Announce Type: new 
Abstract: Multi-output Gaussian process regression has become an important tool in uncertainty quantification, for building emulators of computationally expensive simulators, and other areas such as multi-task machine learning. We present a holistic development of tensor-variate Gaussian process (TvGP) regression, appropriate for arbitrary dimensional outputs where a Kronecker product structure is appropriate for the covariance. We show how two common approaches to problems with two-dimensional output, outer product emulators (OPE) and parallel partial emulators (PPE), are special cases of TvGP regression and hence can be extended to higher output dimensions. Focusing on the important special case of matrix output, we investigate the relative performance of these two approaches. The key distinction is the additional dependence structure assumed by the OPE, and we demonstrate when this is advantageous through two case studies, including application to a spatial-temporal influenza simulator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10319v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daria Semochkina, Samuel E. Jackson, David C. Woods</dc:creator>
    </item>
    <item>
      <title>A Novel Hybrid Approach to Contraceptive Demand Forecasting: Integrating Point Predictions with Probabilistic Distributions</title>
      <link>https://arxiv.org/abs/2502.09685</link>
      <description>arXiv:2502.09685v1 Announce Type: cross 
Abstract: Accurate demand forecasting is vital for ensuring reliable access to contraceptive products, supporting key processes like procurement, inventory, and distribution. However, forecasting contraceptive demand in developing countries presents challenges, including incomplete data, poor data quality, and the need to account for multiple geographical and product factors. Current methods often rely on simple forecasting techniques, which fail to capture demand uncertainties arising from these factors, warranting expert involvement. Our study aims to improve contraceptive demand forecasting by combining probabilistic forecasting methods with expert knowledge. We developed a hybrid model that combines point forecasts from domain-specific model with probabilistic distributions from statistical and machine learning approaches, enabling human input to fine-tune and enhance the system-generated forecasts. This approach helps address the uncertainties in demand and is particularly useful in resource-limited settings. We evaluate different forecasting methods, including time series, Bayesian, machine learning, and foundational time series methods alongside our new hybrid approach. By comparing these methods, we provide insights into their strengths, weaknesses, and computational requirements. Our research fills a gap in forecasting contraceptive demand and offers a practical framework that combines algorithmic and human expertise. Our proposed model can also be generalized to other humanitarian contexts with similar data patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09685v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harsha Chamara Hewage, Bahman Rostami-Tabar, Aris Syntetos, Federico Liberatore, Glenn Milano</dc:creator>
    </item>
    <item>
      <title>Prioritized Ranking Experimental Design Using Recommender Systems in Two-Sided Platforms</title>
      <link>https://arxiv.org/abs/2502.09806</link>
      <description>arXiv:2502.09806v1 Announce Type: cross 
Abstract: Interdependencies between units in online two-sided marketplaces complicate estimating causal effects in experimental settings. We propose a novel experimental design to mitigate the interference bias in estimating the total average treatment effect (TATE) of item-side interventions in online two-sided marketplaces. Our Two-Sided Prioritized Ranking (TSPR) design uses the recommender system as an instrument for experimentation. TSPR strategically prioritizes items based on their treatment status in the listings displayed to users. We designed TSPR to provide users with a coherent platform experience by ensuring access to all items and a consistent realization of their treatment by all users. We evaluate our experimental design through simulations using a search impression dataset from an online travel agency. Our methodology closely estimates the true simulated TATE, while a baseline item-side estimator significantly overestimates TATE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09806v1</guid>
      <category>econ.EM</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahyar Habibi, Zahra Khanalizadeh, Negar Ziaeian</dc:creator>
    </item>
    <item>
      <title>Bayesian calculus and predictive characterizations of extended feature allocation models</title>
      <link>https://arxiv.org/abs/2502.10257</link>
      <description>arXiv:2502.10257v1 Announce Type: cross 
Abstract: We introduce and study a unified Bayesian framework for extended feature allocations which flexibly captures interactions -- such as repulsion or attraction -- among features and their associated weights. We provide a complete Bayesian analysis of the proposed model and specialize our general theory to noteworthy classes of priors. This includes a novel prior based on determinantal point processes, for which we show promising results in a spatial statistics application. Within the general class of extended feature allocations, we further characterize those priors that yield predictive probabilities of discovering new features depending either solely on the sample size or on both the sample size and the distinct number of observed features. These predictive characterizations, known as "sufficientness" postulates, have been extensively studied in the literature on species sampling models starting from the seminal contribution of the English philosopher W.E. Johnson for the Dirichlet distribution. Within the feature allocation setting, existing predictive characterizations are limited to very specific examples; in contrast, our results are general, providing practical guidance for prior selection. Additionally, our approach, based on Palm calculus, is analytical in nature and yields a novel characterization of the Poisson point process through its reduced Palm kernel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10257v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Federico Camerlenghi, Lorenzo Ghilotti</dc:creator>
    </item>
    <item>
      <title>A new and flexible class of sharp asymptotic time-uniform confidence sequences</title>
      <link>https://arxiv.org/abs/2502.10380</link>
      <description>arXiv:2502.10380v1 Announce Type: cross 
Abstract: Confidence sequences are anytime-valid analogues of classical confidence intervals that do not suffer from multiplicity issues under optional continuation of the data collection. As in classical statistics, asymptotic confidence sequences are a nonparametric tool showing under which high-level assumptions asymptotic coverage is achieved so that they also give a certain robustness guarantee against distributional deviations. In this paper, we propose a new flexible class of confidence sequences yielding sharp asymptotic time-uniform confidence sequences under mild assumptions. Furthermore, we highlight the connection to corresponding sequential testing problems and detail the underlying limit theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10380v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Gnettner, Claudia Kirch</dc:creator>
    </item>
    <item>
      <title>Variable selection and basis learning for ordinal classification</title>
      <link>https://arxiv.org/abs/2208.10690</link>
      <description>arXiv:2208.10690v2 Announce Type: replace 
Abstract: We propose a method for variable selection and basis learning for high-dimensional classification with ordinal responses. The proposed method extends sparse multiclass linear discriminant analysis, with the aim of identifying not only the variables relevant to discrimination but also the variables that are order-concordant with the responses. For this purpose, we compute for each variable an ordinal weight, where larger weights are given to variables with ordered group-means, and penalize the variables with smaller weights more severely. A two-step construction for ordinal weights is developed, and we show that the ordinal weights correctly separate ordinal variables from non-ordinal variables with high probability. The resulting sparse ordinal basis learning method is shown to consistently select either the discriminant variables or the ordinal and discriminant variables, depending on the choice of a tunable parameter. Such asymptotic guarantees are given under a high-dimensional asymptotic regime where the dimension grows much faster than the sample size. We also discuss a two-step procedure of post-screening ordinal variables among the selected discriminant variables. Simulated and real data analyses confirm that the proposed basis learning provides sparse and interpretable basis, as it mostly consists of ordinal variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.10690v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2025.2451680</arxiv:DOI>
      <dc:creator>Minwoo Kim, Sangil Han, Jeongyoun Ahn, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>Personalized Two-sided Dose Interval</title>
      <link>https://arxiv.org/abs/2302.12479</link>
      <description>arXiv:2302.12479v3 Announce Type: replace 
Abstract: In fields such as medicine and social sciences, the goal of treatment is often to maintain the outcome of interest within a desirable range rather than to optimize its value. To achieve this, it may be more practical to recommend a treatment dose interval rather than a single fixed level for a study unit. Since individuals may respond differently to the same treatment level, the recommended dose interval should be personalized based on their unique characteristics. Iterative procedures have been proposed to jointly learn the lower and upper bounds of personalized dose intervals, but they lack theoretical justification. To fill this gap, we propose a method to learn personalized two-sided dose intervals based on empirical risk minimization using a novel loss function. The proposed loss function is designed to be well-defined over a tensor product function space, eliminating the need for iterative procedures. In addition, the loss function is doubly-robust to the misspecification of nuisance functions. We establish statistical properties of the estimated dose interval in terms of excess risk by leveraging the reproducing kernel Hilbert space theory. Our simulation study and real-world applications in warfarin dosing and the Job Corps program show that our proposed direct estimation method outperforms competing methods, including indirect regression-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12479v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chan Park, Guanhua Chen, Menggang Yu</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes Estimation with Side Information: A Nonparametric Integrative Tweedie Approach</title>
      <link>https://arxiv.org/abs/2308.05883</link>
      <description>arXiv:2308.05883v2 Announce Type: replace 
Abstract: We investigate the problem of compound estimation of normal means while accounting for the presence of side information. Leveraging the empirical Bayes framework, we develop a nonparametric integrative Tweedie (NIT) approach that incorporates structural knowledge encoded in multivariate auxiliary data to enhance the precision of compound estimation. Our approach employs convex optimization tools to estimate the gradient of the log-density directly, enabling the incorporation of structural constraints. We conduct theoretical analyses of the asymptotic risk of NIT and establish the rate at which NIT converges to the oracle estimator. As the dimension of the auxiliary data increases, we accurately quantify the improvements in estimation risk and the associated deterioration in convergence rate. The numerical performance of NIT is illustrated through the analysis of both simulated and real data, demonstrating its superiority over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05883v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiajun Luo, Trambak Banerjee, Gourab Mukherjee, Wenguang Sun</dc:creator>
    </item>
    <item>
      <title>Parameter identifiability, parameter estimation and model prediction for differential equation models</title>
      <link>https://arxiv.org/abs/2405.08177</link>
      <description>arXiv:2405.08177v4 Announce Type: replace 
Abstract: Interpreting data with mathematical models is an important aspect of real-world industrial and applied mathematical modeling. Often we are interested to understand the extent to which a particular set of data informs and constrains model parameters. This question is closely related to the concept of parameter identifiability, and in this article we present a series of computational exercises to introduce tools that can be used to assess parameter identifiability, estimate parameters and generate model predictions. Taking a likelihood-based approach, we show that very similar ideas and algorithms can be used to deal with a range of different mathematical modeling frameworks. The exercises and results presented in this article are supported by a suite of open access codes that can be accessed on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08177v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew J Simpson, Ruth E Baker</dc:creator>
    </item>
    <item>
      <title>Climate change analysis from LRD manifold functional regression</title>
      <link>https://arxiv.org/abs/2407.00381</link>
      <description>arXiv:2407.00381v2 Announce Type: replace 
Abstract: This work is motivated by the problem of predicting downward solar radiation flux spherical maps from the observation of atmospheric pressure at high cloud bottom. To this aim nonlinear functional regression is implemented under strong-correlated functional data. The link operator reflects the heat transfer in the atmosphere. A latent parametric linear functional regression model reduces uncertainty in the support of this operator. An additive long-memory manifold-supported functional time series error models persistence in time of random fluctuations observed in the response. Time is incorporated via the scalar covariates in the latent linear functional regression model. The functional regression parameters in this model are supported on a connected and compact two point homogeneous space. Its Generalized Least--Squares (GLS) parameter estimation is achieved. When the second-order structure of the functional error term is unknown, its minimum contrast estimation is obtained in the spectral domain. The performance of the theoretical and plug-in nonlinear functional regression predictors is illustrated in the simulation study undertaken in the sphere. The Supplementary Material provides a detailed empirical analysis in the one way ANOVA context. The real-data application extends the purely spatial statistical analysis of atmospheric pressure at high cloud bottom, and downward solar radiation flux in Alegria et al. (2021) to the spatiotemporal context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00381v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diana P. Ovalle-Mu\~noz, M. Dolores Ruiz-Medina</dc:creator>
    </item>
    <item>
      <title>A non-parametric U-statistic testing approach for multi-arm clinical trials with multivariate longitudinal data</title>
      <link>https://arxiv.org/abs/2408.10149</link>
      <description>arXiv:2408.10149v2 Announce Type: replace 
Abstract: Randomized clinical trials (RCTs) often involve multiple longitudinal primary outcomes to comprehensively assess treatment efficacy. The Longitudinal Rank-Sum Test (LRST), a robust U-statistics-based, non-parametric, rank-based method, effectively controls Type I error and enhances statistical power by leveraging the temporal structure of the data without relying on distributional assumptions. However, the LRST is limited to two-arm comparisons. To address the need for comparing multiple doses against a control group in many RCTs, we extend the LRST to a multi-arm setting. This novel multi-arm LRST provides a flexible and powerful approach for evaluating treatment efficacy across multiple arms and outcomes, with a strong capability for detecting the most effective dose in multi-arm trials. Extensive simulations demonstrate that this method maintains excellent Type I error control while providing greater power compared to the two-arm LRST with multiplicity adjustments. Application to the Bapineuzumab (Bapi) 301 trial further validates the multi-arm LRST's practical utility and robustness, confirming its efficacy in complex clinical trial analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10149v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>A note on promotion time cure models with a new biological consideration</title>
      <link>https://arxiv.org/abs/2408.17188</link>
      <description>arXiv:2408.17188v2 Announce Type: replace 
Abstract: We introduce a generalized promotion time cure model motivated by a new biological consideration. The new approach is flexible to model heterogeneous survival data, in particular for addressing intra-sample heterogeneity. We also indicate that the new approach is suited to model a series or parallel system consisting of multiple subsystems in reliability analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17188v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.QM</category>
      <category>q-bio.SC</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Zhao, Fatih K{\i}z{\i}laslan</dc:creator>
    </item>
    <item>
      <title>Partial membership models for soft clustering of multivariate football player performance data</title>
      <link>https://arxiv.org/abs/2409.01874</link>
      <description>arXiv:2409.01874v2 Announce Type: replace 
Abstract: The standard mixture modeling framework has been widely used to study heterogeneous populations, by modeling them as being composed of a finite number of homogeneous sub-populations. However, the standard mixture model assumes that each data point belongs to one and only one mixture component, or cluster, but when data points have fractional membership in multiple clusters this assumption is unrealistic. It is in fact conceptually very different to represent an observation as partly belonging to multiple groups instead of belonging to one group with uncertainty. For this purpose, various soft clustering approaches, or individual-level mixture models, have been developed. In this context, Heller et al (2008) formulated the Bayesian partial membership model (PM) as an alternative structure for individual-level mixtures, which also captures partial membership in the form of attribute-specific mixtures. Our work proposes using the PM for soft clustering of count data arising in football performance analysis and compares the results with those achieved with the mixed membership model and finite mixture model. Learning and inference are carried out using Markov chain Monte Carlo methods. The method is applied on Serie A football player data from the 2022/2023 football season, to estimate the positions on the field where the players tend to play, in addition to their primary position, based on their playing style. The application of partial membership model to football data could have practical implications for coaches, talent scouts, team managers and analysts. These stakeholders can utilize the findings to make informed decisions related to team strategy, talent acquisition, and statistical research, ultimately enhancing performance and understanding in the field of football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01874v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiliano Seri, Roberto Rocci, Thomas Brendan Murphy</dc:creator>
    </item>
    <item>
      <title>Sequential stratified inference for the mean</title>
      <link>https://arxiv.org/abs/2409.06680</link>
      <description>arXiv:2409.06680v2 Announce Type: replace 
Abstract: We develop conservative tests for the mean of a bounded population under stratified sampling and apply them to risk-limiting post-election audits. The tests are "anytime valid" under sequential sampling, allowing optional stopping in each stratum. Our core method expresses a global hypothesis about the population mean as a union of intersection hypotheses describing within-stratum means. It tests each intersection hypothesis using independent test supermartingales (TSMs) combined across strata by multiplication. A P-value for each intersection hypothesis is the reciprocal of that test statistic, and the largest P-value in the union is a P-value for the global hypothesis. This approach has two primary moving parts: the rule selecting which stratum to draw from next given the sample so far, and the form of the TSM within each stratum. These rules may vary over intersection hypotheses. We construct the test with the smallest expected stopping time, and present a few strategies for approximating that optimum. Approximately optimal methods are challenging to compute when there are more than two strata, while some simple rules that scale well can be inconsistent -- the resulting test will never reject for some alternatives, no matter how large the sample. We present a set of rules that leads to a computationally tractable test for arbitrarily many strata. In instances that arise in auditing and other applications, its expected sample size is nearly optimal and substantially smaller than that of previous methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06680v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacob V. Spertus, Mayuri Sridhar, Philip B. Stark</dc:creator>
    </item>
    <item>
      <title>Asymptotic and compound e-values: multiple testing and empirical Bayes</title>
      <link>https://arxiv.org/abs/2409.19812</link>
      <description>arXiv:2409.19812v2 Announce Type: replace 
Abstract: We explicitly define the notions of (exact, approximate or asymptotic) compound p-values and e-values, which have been implicitly presented and extensively used in the recent multiple testing literature. While it is known that the e-BH procedure with compound e-values controls the FDR, we show the converse: every FDR controlling procedure can be recovered by instantiating the e-BH procedure with certain compound e-values. Since compound e-values are closed under averaging, this allows for combination and derandomization of FDR procedures. We then connect compound e-values to empirical Bayes. In particular, we use the fundamental theorem of compound decision theory to derive the log-optimal simple separable compound e-value for testing a set of point nulls against point alternatives: it is a ratio of mixture likelihoods. We extend universal inference to the compound setting. As one example, we construct approximate compound e-values for multiple t-tests, where the (nuisance) variances may be different across hypotheses. Finally, we provide connections to related notions in the literature stated in terms of p-values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19812v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaos Ignatiadis, Ruodu Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Spherical Double K-Means: a co-clustering approach for text data analysis</title>
      <link>https://arxiv.org/abs/2501.04562</link>
      <description>arXiv:2501.04562v2 Announce Type: replace 
Abstract: In text analysis, Spherical K-means (SKM) is a specialized k-means clustering algorithm widely utilized for grouping documents represented in high-dimensional, sparse term-document matrices, often normalized using techniques like TF-IDF. Researchers frequently seek to cluster not only documents but also the terms associated with them into coherent groups. To address this dual clustering requirement, we introduce Spherical Double K-Means (SDKM), a novel methodology that simultaneously clusters documents and terms. This approach offers several advantages: first, by integrating the clustering of documents and terms, SDKM provides deeper insights into the relationships between content and vocabulary, enabling more effective topic identification and keyword extraction. Additionally, the two-level clustering assists in understanding both overarching themes and specific terminologies within document clusters, enhancing interpretability. SDKM effectively handles the high dimensionality and sparsity inherent in text data by utilizing cosine similarity, leading to improved computational efficiency. Moreover, the method captures dynamic changes in thematic content over time, making it well-suited for applications in rapidly evolving fields. Ultimately, SDKM presents a comprehensive framework for advancing text mining efforts, facilitating the uncovering of nuanced patterns and structures that are critical for robust data analysis. We apply SDKM to the corpus of US presidential inaugural addresses, spanning from George Washington in 1789 to Joe Biden in 2021. Our analysis reveals distinct clusters of words and documents that correspond to significant historical themes and periods, showcasing the method's ability to facilitate a deeper understanding of the data. Our findings demonstrate the efficacy of SDKM in uncovering underlying patterns in textual data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04562v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilaria Bombelli, Domenica Fioredistella Iezzi, Emiliano Seri, Maurizio Vichi</dc:creator>
    </item>
    <item>
      <title>Bias-Corrected and Variance-Corrected MLE for the New Median Based Unit Weibull Distribution (MBUW)</title>
      <link>https://arxiv.org/abs/2501.16853</link>
      <description>arXiv:2501.16853v2 Announce Type: replace 
Abstract: As the maximum likelihood method is the most commonly used method for parameters estimation being unbiased, consistent, efficient, and asymptotically normal, MLE is used to fit the new distribution (MBUW). But in small to moderate sample size, this MLE estimator is biased unlike the MLE estimators obtained from large sample sizes. In this paper, the Bias-corrected approach for this distribution is discussed and applied to real data analysis. The MLE estimators of MBUW obtained from some optimization techniques like derivative free Nelder Mead algorithm suffers from significant high correlation that is reflected on high covariance between the parameters. Also this association between the parameters affects the variances which may be inflated enough to approach infinity hampering construction of confidence intervals for each parameter. This problem may arise with any optimization technique which necessitates remedies trying to fix it. The author also elaborates a variance correction approach heavily relaying on re-parameterizing the negative log likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16853v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>Misspecification-robust likelihood-free inference in high dimensions</title>
      <link>https://arxiv.org/abs/2002.09377</link>
      <description>arXiv:2002.09377v5 Announce Type: replace-cross 
Abstract: Likelihood-free inference for simulator-based statistical models has developed rapidly from its infancy to a useful tool for practitioners. However, models with more than a handful of parameters still generally remain a challenge for the Approximate Bayesian Computation (ABC) based inference. To advance the possibilities for performing likelihood-free inference in higher dimensional parameter spaces, we introduce an extension of the popular Bayesian optimisation based approach to approximate discrepancy functions in a probabilistic manner which lends itself to an efficient exploration of the parameter space. Our approach achieves computational scalability for higher dimensional parameter spaces by using separate acquisition functions and discrepancies for each parameter. The efficient additive acquisition structure is combined with exponentiated loss -likelihood to provide a misspecification-robust characterisation of the marginal posterior distribution for all model parameters. The method successfully performs computationally efficient inference in a 100-dimensional space on canonical examples and compares favourably to existing modularised ABC methods. We further illustrate the potential of this approach by fitting a bacterial transmission dynamics model to a real data set, which provides biologically coherent results on strain competition in a 30-dimensional parameter space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2002.09377v5</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Owen Thomas, Raquel S\'a-Le\~ao, Herm\'inia de Lencastre, Samuel Kaski, Jukka Corander, Henri Pesonen</dc:creator>
    </item>
    <item>
      <title>Monte Carlo sampling with integrator snippets</title>
      <link>https://arxiv.org/abs/2404.13302</link>
      <description>arXiv:2404.13302v2 Announce Type: replace-cross 
Abstract: Assume interest is in sampling from a probability distribution $\mu$ defined on $(\mathsf{Z},\mathscr{Z})$. We develop a framework for sampling algorithms which takes full advantage of ODE numerical integrators, say $\psi\colon\mathsf{Z}\rightarrow\mathsf{Z}$ for one integration step, to explore $\mu$ efficiently and robustly. The popular Hybrid Monte Carlo (HMC) algorithm \cite{duane1987hybrid,neal2011mcmc} and its derivatives are examples of such a use of numerical integrators. A key idea developed here is that of sampling integrator snippets, that is fragments of the orbit of an ODE numerical integrator $\psi$, and the definition of an associated probability distribution $\bar{\mu}$ such that expectations with respect to $\mu$ can be estimated from integrator snippets distributed according to $\bar{\mu}$. The integrator snippet target distribution $\bar{\mu}$ takes the form of a mixture of pushforward distributions which suggests numerous generalisations beyond mappings arising from numerical integrators, e.g. normalising flows. Very importantly this structure also suggests new principled and robust strategies to tune the parameters of integrators, such as the discretisation stepsize, effective integration time, or number of integration steps, in a Leapfrog integrator.
  We focus here primarily on Sequential Monte Carlo (SMC) algorithms, but the approach can be used in the context of Markov chain Monte Carlo algorithms. We illustrate performance and, in particular, robustness through numerical experiments and provide preliminary theoretical results supporting observed performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13302v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Andrieu, Mauro Camara Escudero, Chang Zhang</dc:creator>
    </item>
    <item>
      <title>A Note on the Conversion of Nonnegative Integers to the Canonical Signed-digit Representation</title>
      <link>https://arxiv.org/abs/2501.10908</link>
      <description>arXiv:2501.10908v2 Announce Type: replace-cross 
Abstract: This note addresses the signed-digit representation of nonnegative binary integers. Popular literature methods for the conversion into the canonical signed-digit representation are reviewed and revisited. A method based on string substitution is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10908v2</guid>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R. J. Cintra</dc:creator>
    </item>
  </channel>
</rss>
