<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.BM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.BM</link>
    <description>q-bio.BM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.BM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Oct 2025 04:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Speak to a Protein: An Interactive Multimodal Co-Scientist for Protein Analysis</title>
      <link>https://arxiv.org/abs/2510.17826</link>
      <description>arXiv:2510.17826v1 Announce Type: new 
Abstract: Building a working mental model of a protein typically requires weeks of reading, cross-referencing crystal and predicted structures, and inspecting ligand complexes, an effort that is slow, unevenly accessible, and often requires specialized computational skills. We introduce \emph{Speak to a Protein}, a new capability that turns protein analysis into an interactive, multimodal dialogue with an expert co-scientist. The AI system retrieves and synthesizes relevant literature, structures, and ligand data; grounds answers in a live 3D scene; and can highlight, annotate, manipulate and see the visualization. It also generates and runs code when needed, explaining results in both text and graphics. We demonstrate these capabilities on relevant proteins, posing questions about binding pockets, conformational changes, or structure-activity relationships to test ideas in real-time. \emph{Speak to a Protein} reduces the time from question to evidence, lowers the barrier to advanced structural analysis, and enables hypothesis generation by tightly coupling language, code, and 3D structures. \emph{Speak to a Protein} is freely accessible at https://open.playmolecule.org.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17826v1</guid>
      <category>q-bio.BM</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carles Navarro, Mariona Torrens, Philipp Th\"olke, Stefan Doerr, Gianni De Fabritiis</dc:creator>
    </item>
    <item>
      <title>Molecular Fingerprints Are Strong Models for Peptide Function Prediction</title>
      <link>https://arxiv.org/abs/2501.17901</link>
      <description>arXiv:2501.17901v2 Announce Type: replace 
Abstract: Understanding peptide properties is often assumed to require modeling long-range molecular interactions, motivating the use of complex graph neural networks and pretrained transformers. Yet, whether such long-range dependencies are essential remains unclear. We investigate if simple, domain-specific molecular fingerprints can capture peptide function without these assumptions. Atomic-level representation aims to provide richer information than purely sequence-based models and better efficiency than structural ones. Across 132 datasets, including LRGB and five other peptide benchmarks, models using count-based ECFP, Topological Torsion, and RDKit fingerprints with LightGBM achieve state-of-the-art accuracy. Despite encoding only short-range molecular features, these models outperform GNNs and transformer-based approaches. Control experiments with sequence shuffling and amino acid counts confirm that fingerprints, though inherently local, suffice for robust peptide property prediction. Our results challenge the presumed necessity of long-range interaction modeling and highlight molecular fingerprints as efficient, interpretable, and computationally lightweight alternatives for peptide prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17901v2</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Adamczyk, Piotr Ludynia, Wojciech Czech</dc:creator>
    </item>
    <item>
      <title>From Possibility to Precision in Macromolecular Ensemble Prediction</title>
      <link>https://arxiv.org/abs/2505.01919</link>
      <description>arXiv:2505.01919v2 Announce Type: replace 
Abstract: Proteins and other macromolecules exist not in a single state but as dynamic ensembles of interconverting conformations, which are essential for catalysis, allosteric regulation, and molecular recognition. While AI-based structure predictors like AlphaFold have revolutionized static structure prediction, they are not yet capable of capturing conformational ensembles. Progress towards the next generation of AI models capable of ensemble prediction is currently limited by the lack of accurate, high-resolution ground truth ensembles at the scale required for training and validation. This is due to the fact that no single experimental technique can fully resolve the atomistic complexity of conformational landscapes, and fundamental challenges remain in defining, representing, comparing, and validating structural ensembles. Here, we outline the infrastructure and methodological advances needed to overcome these barriers. We highlight emerging strategies for integrating heterogeneous experimental data into unified ensemble encoding representations and how to leverage these new methodologies to build benchmarks and establish ensemble-specific validation protocols. Finally, we discuss how ensemble predictions will be an interactive cycle of experimental and computational innovation. Establishing this ecosystem will allow structural biology to move beyond static snapshots toward a dynamic understanding of molecular behavior that captures the full complexity of biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01919v2</guid>
      <category>q-bio.BM</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephanie A. Wankowicz, Massimiliano Bonomi</dc:creator>
    </item>
    <item>
      <title>Steering Generative Models with Experimental Data for Protein Fitness Optimization</title>
      <link>https://arxiv.org/abs/2505.15093</link>
      <description>arXiv:2505.15093v2 Announce Type: replace 
Abstract: Protein fitness optimization involves finding a protein sequence that maximizes desired quantitative properties in a combinatorially large design space of possible sequences. Recent advances in steering protein generative models (e.g., diffusion models and language models) with labeled data offer a promising approach. However, most previous studies have optimized surrogate rewards and/or utilized large amounts of labeled data for steering, making it unclear how well existing methods perform and compare to each other in real-world optimization campaigns where fitness is measured through low-throughput wet-lab assays. In this study, we explore fitness optimization using small amounts (hundreds) of labeled sequence-fitness pairs and comprehensively evaluate strategies such as classifier guidance and posterior sampling for guiding generation from different discrete diffusion models of protein sequences. We also demonstrate how guidance can be integrated into adaptive sequence selection akin to Thompson sampling in Bayesian optimization, showing that plug-and-play guidance strategies offer advantages over alternatives such as reinforcement learning with protein language models. Overall, we provide practical insights into how to effectively steer modern generative models for next-generation protein fitness optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15093v2</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Yang, Wenda Chu, Daniel Khalil, Raul Astudillo, Bruce J. Wittmann, Frances H. Arnold, Yisong Yue</dc:creator>
    </item>
    <item>
      <title>One protein is all you need</title>
      <link>https://arxiv.org/abs/2411.02109</link>
      <description>arXiv:2411.02109v2 Announce Type: replace-cross 
Abstract: Generalization beyond training data remains a central challenge in machine learning for biology. A common way to enhance generalization is self-supervised pre-training on large datasets. However, aiming to perform well on all possible proteins can limit a model's capacity to excel on any specific one, whereas experimentalists typically need accurate predictions for individual proteins they study, often not covered in training data. To address this limitation, we propose a method that enables self-supervised customization of protein language models to one target protein at a time, on the fly, and without assuming any additional data. We show that our Protein Test-Time Training (ProteinTTT) method consistently enhances generalization across different models, their sizes, and datasets. ProteinTTT improves structure prediction for challenging targets, achieves new state-of-the-art results on protein fitness prediction, and enhances function prediction on two tasks. Through two challenging case studies, we also show that customization via ProteinTTT achieves more accurate antibody-antigen loop modeling and enhances 19% of structures in the Big Fantastic Virus Database, delivering improved predictions where general-purpose AlphaFold2 and ESMFold struggle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02109v2</guid>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Bushuiev, Roman Bushuiev, Olga Pimenova, Nikola Zadorozhny, Raman Samusevich, Elisabet Manaskova, Rachel Seongeun Kim, Hannes St\"ark, Jiri Sedlar, Martin Steinegger, Tom\'a\v{s} Pluskal, Josef Sivic</dc:creator>
    </item>
  </channel>
</rss>
