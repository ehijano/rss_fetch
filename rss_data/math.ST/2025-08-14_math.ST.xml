<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Aug 2025 04:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Better bootstrap t confidence intervals for the mean</title>
      <link>https://arxiv.org/abs/2508.10083</link>
      <description>arXiv:2508.10083v1 Announce Type: new 
Abstract: This article explores combinations of weighted bootstraps, like the Bayesian bootstrap, with the bootstrap $t$ method for setting approximate confidence intervals for the mean of a random variable in small samples. For this problem the usual bootstrap $t$ has good coverage but provides intervals with long and highly variable lengths. Those intervals can have infinite length not just for tiny $n$, when the data have a discrete distribution. The BC$_a$ bootstrap produces shorter intervals but tends to severely under-cover the mean. Bootstrapping the studentized mean with weights from a Beta$(1/2,3/2)$ distribution is shown to attain second order accuracy. It never yields infinite length intervals and the mean square bootstrap $t$ statistic is finite when there are at least three distinct values in the data, or two distinct values appearing at least three times each. In a range of small sample settings, the beta bootstrap $t$ intervals have closer to nominal coverage than the BC$_a$ and shorter length than the multinomial ootstrap $t$. The paper includes a lengthy discussion of the difficulties in constructing a utility function to evaluate nonparametric approximate confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10083v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Art B. Owen</dc:creator>
    </item>
    <item>
      <title>Online selective conformal inference: adaptive scores, convergence rate and optimality</title>
      <link>https://arxiv.org/abs/2508.10336</link>
      <description>arXiv:2508.10336v1 Announce Type: new 
Abstract: In a supervised online setting, quantifying uncertainty has been proposed in the seminal work of \cite{gibbs2021adaptive}. For any given point-prediction algorithm, their method (ACI) produces a conformal prediction set with an average missed coverage getting close to a pre-specified level $\alpha$ for a long time horizon. We introduce an extended version of this algorithm, called OnlineSCI, allowing the user to additionally select times where such an inference should be made. OnlineSCI encompasses several prominent online selective tasks, such as building prediction intervals for extreme outcomes, classification with abstention, and online testing. While OnlineSCI controls the average missed coverage on the selected in an adversarial setting, our theoretical results also show that it controls the instantaneous error rate (IER) at the selected times, up to a non-asymptotical remainder term. Importantly, our theory covers the case where OnlineSCI updates the point-prediction algorithm at each time step, a property which we refer to as {\it adaptive} capability. We show that the adaptive versions of OnlineSCI can convergence to an optimal solution and provide an explicit convergence rate in each of the aforementioned application cases, under specific mild conditions. Finally, the favorable behavior of OnlineSCI in practice is illustrated by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10336v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Humbert, Ulysse Gazin, Ruth Heller, Etienne Roquain</dc:creator>
    </item>
    <item>
      <title>On Random Fields Associated with Analytic Wavelet Transform</title>
      <link>https://arxiv.org/abs/2508.10495</link>
      <description>arXiv:2508.10495v1 Announce Type: new 
Abstract: Despite the broad application of the analytic wavelet transform (AWT), a systematic statistical characterization of its magnitude and phase as inhomogeneous random fields on the time-frequency domain when the input is a random process remains underexplored. In this work, we study the magnitude and phase of the AWT as random fields on the time-frequency domain when the observed signal is a deterministic function plus additive stationary Gaussian noise. We derive their marginal and joint distributions, establish concentration inequalities that depend on the signal-to-noise ratio (SNR), and analyze their covariance structures. Based on these results, we derive an upper bound on the probability of incorrectly identifying the time-scale ridge of the clean signal, explore the regularity of scalogram contours, and study the relationship between AWT magnitude and phase. Our findings lay the groundwork for developing rigorous AWT-based algorithms in noisy environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10495v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gi-Ren Liu, Yuan-Chung Sheu, Hau-Tieng Wu</dc:creator>
    </item>
    <item>
      <title>Approximation rates for finite mixtures of location-scale models</title>
      <link>https://arxiv.org/abs/2508.10612</link>
      <description>arXiv:2508.10612v1 Announce Type: new 
Abstract: Finite mixture models provide useful methods for approximation and estimation of probability density functions. In the context of location-scale mixture models, we improve upon previous results to show that it is possible to quantitatively bound the ${\cal L}_{p}$ approximation error in terms of the number of mixture components $m$ for any $p\in\left(1,\infty\right)$. In particular, for approximation on $\mathbb{R}^{d}$ of targets $f_{0}\in{\cal W}^{s,p}$, the (fractional) Sobolev spaces with smoothness order $s\in\left(0,1\right]$, if $p&lt;2$ then the error has size $O\left(m^{-\frac{s}{sq+d}}\right)$, while if $p\ge2$, the error has size $O\left(m^{-\frac{2s}{2\left(sq+d\right)}}\right)$, where $q=1/\left(p-1\right)$. We demonstrate that these results can be used to derive estimation rates for a location-scale mixture-based adaptive least-squares estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10612v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hien Duy Nguyen, TrungTin Nguyen, Jacob Westerhout, Xin Guo</dc:creator>
    </item>
    <item>
      <title>Bayesian Models for Joint Selection of Features and Auto-Regressive Lags: Theory and Applications in Environmental and Financial Forecasting</title>
      <link>https://arxiv.org/abs/2508.10055</link>
      <description>arXiv:2508.10055v1 Announce Type: cross 
Abstract: We develop a Bayesian framework for variable selection in linear regression with autocorrelated errors, accommodating lagged covariates and autoregressive structures. This setting occurs in time series applications where responses depend on contemporaneous or past explanatory variables and persistent stochastic shocks, including financial modeling, hydrological forecasting, and meteorological applications requiring temporal dependency capture. Our methodology uses hierarchical Bayesian models with spike-and-slab priors to simultaneously select relevant covariates and lagged error terms. We propose an efficient two-stage MCMC algorithm separating sampling of variable inclusion indicators and model parameters to address high-dimensional computational challenges. Theoretical analysis establishes posterior selection consistency under mild conditions, even when candidate predictors grow exponentially with sample size, common in modern time series with many potential lagged variables. Through simulations and real applications (groundwater depth prediction, S&amp;P 500 log returns modeling), we demonstrate substantial gains in variable selection accuracy and predictive performance. Compared to existing methods, our framework achieves lower MSPE, improved true model component identification, and greater robustness with autocorrelated noise, underscoring practical utility for model interpretation and forecasting in autoregressive settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10055v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Alokesh Manna, Sujit K. Ghosh</dc:creator>
    </item>
    <item>
      <title>Regression adjustment in covariate-adaptive randomized experiments with missing covariates</title>
      <link>https://arxiv.org/abs/2508.10061</link>
      <description>arXiv:2508.10061v1 Announce Type: cross 
Abstract: Covariate-adaptive randomization is widely used in clinical trials to balance prognostic factors, and regression adjustments are often adopted to further enhance the estimation and inference efficiency. In practice, the covariates may contain missing values. Various methods have been proposed to handle the covariate missing problem under simple randomization. However, the statistical properties of the resulting average treatment effect estimators under stratified randomization, or more generally, covariate-adaptive randomization, remain unclear. To address this issue, we investigate the asymptotic properties of several average treatment effect estimators obtained by combining commonly used missingness processing procedures and regression adjustment methods. Moreover, we derive consistent variance estimators to enable valid inferences. Finally, we conduct a numerical study to evaluate the finite-sample performance of the considered estimators under various sample sizes and numbers of covariates and provide recommendations accordingly. Our analysis is model-free, meaning that the conclusions remain asymptotically valid even in cases of misspecification of the regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10061v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wanjia Fu, Yingying Ma, Hanzhong Liu</dc:creator>
    </item>
    <item>
      <title>Statistical methods: Basic concepts, interpretations, and cautions</title>
      <link>https://arxiv.org/abs/2508.10168</link>
      <description>arXiv:2508.10168v1 Announce Type: cross 
Abstract: The study of associations and their causal explanations is a central research activity whose methodology varies tremendously across fields. Even within specialized subfields, comparisons across textbooks and journals reveals that the basics are subject to considerable variation and controversy. This variation is often obscured by the singular viewpoints presented within textbooks and journal guidelines, which may be deceptively written as if the norms they adopt are unchallenged. Furthermore, human limitations and the vastness within fields imply that no one can have expertise across all subfields and that interpretations will be severely constrained by the limitations of studies of human populations.
  The present chapter outlines an approach to statistical methods that attempts to recognize these problems from the start, rather than assume they are absent as in the claims of 'statistical significance' and 'confidence' ordinarily attached to statistical tests and interval estimates. It does so by grounding models and statistics in data description, and treating inferences from them as speculations based on assumptions that cannot be fully validated or checked using the analysis data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10168v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sander Greenland</dc:creator>
    </item>
    <item>
      <title>Incorporating Taxonomies of Cyber Incidents Into Detection Networks for Improved Detection Performance</title>
      <link>https://arxiv.org/abs/2508.10187</link>
      <description>arXiv:2508.10187v1 Announce Type: cross 
Abstract: Many taxonomies exist to organize cybercrime incidents into ontological categories. We examine some of the taxonomies introduced in the literature; providing a framework, and analysis, of how best to leverage different taxonomy structures to optimize performance of detections targeting various types of threat-actor behaviors under the umbrella of precision and recall. Networks of detections are studied, and results are outlined showing properties of networks of interconnected detections. Some illustrations are provided to show how the construction of sets of detections to prevent broader types of attacks is limited by trade-offs in precision and recall under constraints. An equilibrium result is proven and validated on simulations, illustrating the existence of an optimal detection design strategy in this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10187v1</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryan Warnick</dc:creator>
    </item>
    <item>
      <title>Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2508.10608</link>
      <description>arXiv:2508.10608v1 Announce Type: cross 
Abstract: Multi-Objective Reinforcement Learning (MORL) is a generalization of traditional Reinforcement Learning (RL) that aims to optimize multiple, often conflicting objectives simultaneously rather than focusing on a single reward. This approach is crucial in complex decision-making scenarios where agents must balance trade-offs between various goals, such as maximizing performance while minimizing costs. We consider the problem of MORL where the objectives are combined using a non-linear scalarization function. Just like in standard RL, policy gradient methods (PGMs) are amongst the most effective for handling large and continuous state-action spaces in MORL. However, existing PGMs for MORL suffer from high sample inefficiency, requiring large amounts of data to be effective. Previous attempts to solve this problem rely on overly strict assumptions, losing PGMs' benefits in scalability to large state-action spaces. In this work, we address the issue of sample efficiency by implementing variance-reduction techniques to reduce the sample complexity of policy gradients while maintaining general assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10608v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Guidobene, Lorenzo Benedetti, Diego Arapovic</dc:creator>
    </item>
    <item>
      <title>Higher-order Gini indices: An axiomatic approach</title>
      <link>https://arxiv.org/abs/2508.10663</link>
      <description>arXiv:2508.10663v1 Announce Type: cross 
Abstract: Via an axiomatic approach, we characterize the family of n-th order Gini deviation, defined as the expected range over n independent draws from a distribution, to quantify joint dispersion across multiple observations. This extends the classical Gini deviation, which relies solely on pairwise comparisons. Our generalization grows increasingly sensitive to tail inequality as n increases, offering a more nuanced view of distributional extremes. We show that these higher-order Gini deviations admit a Choquet integral representation, inheriting the desirable properties of coherent deviation measures. Furthermore, we prove that both the n-th order Gini deviation and its normalized version, the n-th order Gini coefficient, are n-observation elicitable, facilitating rigorous backtesting. Empirical analysis using World Inequality Database data reveals that higher-order Gini coefficients detect disparities obscured by the classical Gini coefficient, particularly in cases of extreme income or wealth concentration. Our results establish higher-order Gini indices as valuable complementary tools for robust inequality assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10663v1</guid>
      <category>q-fin.MF</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xia Han, Ruodu Wang, Qinyu Wu</dc:creator>
    </item>
    <item>
      <title>Dimension-Free Bounds for Generalized First-Order Methods via Gaussian Coupling</title>
      <link>https://arxiv.org/abs/2508.10782</link>
      <description>arXiv:2508.10782v1 Announce Type: cross 
Abstract: We establish non-asymptotic bounds on the finite-sample behavior of generalized first-order iterative algorithms -- including gradient-based optimization methods and approximate message passing (AMP) -- with Gaussian data matrices and full-memory, non-separable nonlinearities. The central result constructs an explicit coupling between the iterates of a generalized first-order method and a conditionally Gaussian process whose covariance evolves deterministically via a finite-dimensional state evolution recursion. This coupling yields tight, dimension-free bounds under mild Lipschitz and moment-matching conditions. Our analysis departs from classical inductive AMP proofs by employing a direct comparison between the generalized first-order method and the conditionally Gaussian comparison process. This approach provides a unified derivation of AMP theory for Gaussian matrices without relying on separability or asymptotics. A complementary lower bound on the Wasserstein distance demonstrates the sharpness of our upper bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10782v1</guid>
      <category>stat.ML</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Galen Reeves</dc:creator>
    </item>
    <item>
      <title>An Iterative Algorithm for Differentially Private $k$-PCA with Adaptive Noise</title>
      <link>https://arxiv.org/abs/2508.10879</link>
      <description>arXiv:2508.10879v1 Announce Type: cross 
Abstract: Given $n$ i.i.d. random matrices $A_i \in \mathbb{R}^{d \times d}$ that share a common expectation $\Sigma$, the objective of Differentially Private Stochastic PCA is to identify a subspace of dimension $k$ that captures the largest variance directions of $\Sigma$, while preserving differential privacy (DP) of each individual $A_i$. Existing methods either (i) require the sample size $n$ to scale super-linearly with dimension $d$, even under Gaussian assumptions on the $A_i$, or (ii) introduce excessive noise for DP even when the intrinsic randomness within $A_i$ is small. Liu et al. (2022a) addressed these issues for sub-Gaussian data but only for estimating the top eigenvector ($k=1$) using their algorithm DP-PCA. We propose the first algorithm capable of estimating the top $k$ eigenvectors for arbitrary $k \leq d$, whilst overcoming both limitations above. For $k=1$ our algorithm matches the utility guarantees of DP-PCA, achieving near-optimal statistical error even when $n = \tilde{\!O}(d)$. We further provide a lower bound for general $k &gt; 1$, matching our upper bound up to a factor of $k$, and experimentally demonstrate the advantages of our algorithm over comparable baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10879v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johanna D\"ungler, Amartya Sanyal</dc:creator>
    </item>
    <item>
      <title>Statistical Inference on Latent Space Models for Network Data</title>
      <link>https://arxiv.org/abs/2312.06605</link>
      <description>arXiv:2312.06605v3 Announce Type: replace 
Abstract: Latent space models are powerful statistical tools for modeling and understanding network data. While the importance of accounting for uncertainty in network analysis has been well recognized, the current literature predominantly focuses on point estimation and prediction, leaving the statistical inference of latent space models an open question. This work aims to fill this gap by providing a general framework to analyze the theoretical properties of the maximum likelihood estimators. In particular, we establish the uniform consistency and asymptotic distribution results for the latent space models under different edge types and link functions. Furthermore, the proposed framework enables us to generalize our results to the dependent-edge and sparse scenarios. Our theories are supported by simulation studies and have the potential to be applied in downstream inferences, such as link prediction and network testing problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06605v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinming Li, Shihao Wu, Chengyu Cui, Gongjun Xu, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>M-estimation for Gaussian processes with time-inhomogeneous drifts from high-frequency data</title>
      <link>https://arxiv.org/abs/2508.01164</link>
      <description>arXiv:2508.01164v2 Announce Type: replace 
Abstract: We propose a contrast-based estimation method for Gaussian processes with time-inhomogeneous drifts, observed under high-frequency sampling. The process is modeled as the sum of a deterministic drift function and a stationary Gaussian component with a parametric kernel. Our method constructs a local contrast function from adjacent increments, which avoids inversion of large covariance matrices and allows for efficient computation. We prove consistency and asymptotic normality of the resulting estimators under general ergodicity conditions. A distinctive feature of our approach is that the drift estimator attains a nonstandard convergence rate, stemming from the direct Riemann integrability of the drift density. This highlights a fundamental difference from standard estimation regimes. Furthermore, when the local contrast fails to identify all parameters in the covariance kernel, moment-based corrections can be incorporated to recover identifiability. The proposed framework is simple, flexible, and particularly well suited for high-frequency inference with time-inhomogeneous structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01164v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasutaka Shimizu</dc:creator>
    </item>
    <item>
      <title>Distribution-free data-driven smooth tests without $\chi^2$</title>
      <link>https://arxiv.org/abs/2508.01973</link>
      <description>arXiv:2508.01973v2 Announce Type: replace 
Abstract: This article demonstrates how recent developments in the theory of empirical processes allow us to construct a new family of asymptotically distribution-free smooth test statistics. Their distribution-free property is preserved even when the parameters are estimated, model selection is performed, and the sample size is only moderately large. A computationally efficient alternative to the classical parametric bootstrap is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01973v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangyu Zhang, Sara Algeri</dc:creator>
    </item>
    <item>
      <title>A General Design-Based Framework and Estimator for Randomized Experiments</title>
      <link>https://arxiv.org/abs/2210.08698</link>
      <description>arXiv:2210.08698v3 Announce Type: replace-cross 
Abstract: We describe a design-based framework for drawing causal inference in general randomized experiments. Causal effects are defined as linear functionals evaluated at unit-level potential outcome functions. Assumptions about the potential outcome functions are encoded as function spaces. This makes the framework expressive, allowing experimenters to formulate and investigate a wide range of causal questions, including about interference, that previously could not be investigated with design-based methods. We describe a class of estimators for estimands defined using the framework and investigate their properties. We provide necessary and sufficient conditions for unbiasedness and consistency. We also describe a class of conservative variance estimators, which facilitate the construction of confidence intervals. Finally, we provide several examples of empirical settings that previously could not be examined with design-based methods to illustrate the use of our approach in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08698v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Harshaw, Fredrik S\"avje, Yitan Wang</dc:creator>
    </item>
    <item>
      <title>Classification of small-ball modes and maximum a posteriori estimators in metric spaces</title>
      <link>https://arxiv.org/abs/2306.16278</link>
      <description>arXiv:2306.16278v4 Announce Type: replace-cross 
Abstract: A mode, or `most likely point', for a probability measure $\mu$ can be defined in various ways via the asymptotic behaviour of the $\mu$-mass of balls as their radius tends to zero. Such points are of intrinsic interest in the local theory of measures on metric spaces and also arise naturally in the study of Bayesian inverse problems and diffusion processes. Building upon special cases already proposed in the literature, this paper develops a systematic framework for defining modes through small-ball probabilities. We propose `common-sense' axioms that such definitions should obey, including appropriate treatment of discrete and absolutely continuous measures, as well as symmetry and invariance properties. We show that there are exactly ten such definitions consistent with these axioms, and that they are partially but not totally ordered in strength, forming a complete, distributive lattice. We also show how this classification simplifies for well-behaved $\mu$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16278v4</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilja Klebanov, Hefin Lambley, T. J. Sullivan</dc:creator>
    </item>
    <item>
      <title>Post-clustering Inference under Dependence</title>
      <link>https://arxiv.org/abs/2310.11822</link>
      <description>arXiv:2310.11822v2 Announce Type: replace-cross 
Abstract: Recent work by Gao et al. (JASA 2022) has laid the foundations for post-clustering inference, establishing a theoretical framework allowing to test for differences between means of estimated clusters. Additionally, they studied the estimation of unknown parameters while controlling the selective type I error. However, their theory was developed for independent observations identically distributed as $p$-dimensional Gaussian variables, where the parameter estimation could only be performed for spherical covariance matrices. Here, we aim at extending this framework to a more convenient scenario for practical applications, where arbitrary dependence structures between observations and features are allowed. We establish sufficient conditions for extending the setting presented by Gao et al. to the general dependence framework. Moreover, we assess theoretical conditions allowing the compatible estimation of a covariance matrix. The theory is developed for hierarchical agglomerative clustering algorithms with several types of linkages, and for the $k$-means algorithm. We illustrate our method with synthetic data and real data of protein structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11822v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Javier Gonz\'alez-Delgado, Mathis Deronzier, Juan Cort\'es, Pierre Neuvial</dc:creator>
    </item>
    <item>
      <title>Sharp Generalization for Nonparametric Regression in Interpolation Space by Over-Parameterized Neural Networks Trained with Preconditioned Gradient Descent and Early-Stopping</title>
      <link>https://arxiv.org/abs/2407.11353</link>
      <description>arXiv:2407.11353v2 Announce Type: replace-cross 
Abstract: We study nonparametric regression using an over-parameterized two-layer neural networks trained with algorithmic guarantees in this paper. We consider the setting where the training features are drawn uniformly from the unit sphere in $\RR^d$, and the target function lies in an interpolation space commonly studied in statistical learning theory. We demonstrate that training the neural network with a novel Preconditioned Gradient Descent (PGD) algorithm, equipped with early stopping, achieves a sharp regression rate of $\cO(n^{-\frac{2\alpha s'}{2\alpha s'+1}})$ when the target function is in the interpolation space $\bth{\cH_K}^{s'}$ with $s' \ge 3$. This rate is even sharper than the currently known nearly-optimal rate of $\cO(n^{-\frac{2\alpha s'}{2\alpha s'+1}})\log^2(1/\delta)$, where $n$ is the size of the training data and $\delta \in (0,1)$ is a small probability. This rate is also sharper than the standard kernel regression rate of $\cO(n^{-\frac{2\alpha}{2\alpha+1}})$ obtained under the regular Neural Tangent Kernel (NTK) regime when training the neural network with the vanilla gradient descent (GD), where $2\alpha = d/(d-1)$. Our analysis is based on two key technical contributions. First, we present a principled decomposition of the network output at each PGD step into a function in the reproducing kernel Hilbert space (RKHS) of a newly induced integral kernel, and a residual function with small $L^{\infty}$-norm. Second, leveraging this decomposition, we apply local Rademacher complexity theory to tightly control the complexity of the function class comprising all the neural network functions obtained in the PGD iterates. Our results further suggest that PGD enables the neural network to escape the linear NTK regime and achieve improved generalization, as it effectively induces a new kernel termed the integral kernel, compared to the regular NTK arising from the vanilla GD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11353v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingzhen Yang, Ping Li</dc:creator>
    </item>
    <item>
      <title>Neural Networks Generalize on Low Complexity Data</title>
      <link>https://arxiv.org/abs/2409.12446</link>
      <description>arXiv:2409.12446v5 Announce Type: replace-cross 
Abstract: We show that feedforward neural networks with ReLU activation generalize on low complexity data, suitably defined. Given i.i.d.~data generated from a simple programming language, the minimum description length (MDL) feedforward neural network which interpolates the data generalizes with high probability. We define this simple programming language, along with a notion of description length of such networks. We provide several examples on basic computational tasks, such as checking primality of a natural number. For primality testing, our theorem shows the following and more. Suppose that we draw an i.i.d.~sample of $n$ numbers uniformly at random from $1$ to $N$. For each number $x_i$, let $y_i = 1$ if $x_i$ is a prime and $0$ if it is not. Then, the interpolating MDL network accurately answers, with error probability $1- O((\ln N)/n)$, whether a newly drawn number between $1$ and $N$ is a prime or not. Note that the network is not designed to detect primes; minimum description learning discovers a network which does so. Extensions to noisy data are also discussed, suggesting that MDL neural network interpolators can demonstrate tempered overfitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12446v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Timothy Sudijono</dc:creator>
    </item>
    <item>
      <title>Decision Theory For Large Scale Outlier Detection Using Aleatoric Uncertainty: With a Note on Bayesian FDR</title>
      <link>https://arxiv.org/abs/2508.01988</link>
      <description>arXiv:2508.01988v3 Announce Type: replace-cross 
Abstract: Aleatoric and Epistemic uncertainty have achieved recent attention in the literature as different sources from which uncertainty can emerge in stochastic modeling. Epistemic being intrinsic or model based notions of uncertainty, and aleatoric being the uncertainty inherent in the data. We propose a novel decision theoretic framework for outlier detection in the context of aleatoric uncertainty; in the context of Bayesian modeling. The model incorporates bayesian false discovery rate control for multiplicty adjustment, and a new generalization of Bayesian FDR is introduced. The model is applied to simulations based on temporally fluctuating outlier detection where fixing thresholds often results in poor performance due to nonstationarity, and a case study is outlined on on a novel cybersecurity detection. Cyberthreat signals are highly nonstationary; giving a credible stress test of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01988v3</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryan Warnick</dc:creator>
    </item>
  </channel>
</rss>
