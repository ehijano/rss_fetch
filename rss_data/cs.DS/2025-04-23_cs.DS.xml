<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Apr 2025 01:40:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Determining Implication of Fixed Matrix Prenex Normal Forms Can Be Decided in Linear Time</title>
      <link>https://arxiv.org/abs/2504.15294</link>
      <description>arXiv:2504.15294v1 Announce Type: new 
Abstract: For a fixed arbitrary matrix depending on $n$ variables, one may ask whether a Prenex Normal Form (PNF) implies another. A RAM algorithm running in linear time is presented and shown to be asymptotically optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15294v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Wang</dc:creator>
    </item>
    <item>
      <title>Prize-Collecting Forest with Submodular Penalties: Improved Approximation</title>
      <link>https://arxiv.org/abs/2504.15445</link>
      <description>arXiv:2504.15445v1 Announce Type: new 
Abstract: Constrained forest problems form a class of graph problems where specific connectivity requirements for certain cuts within the graph must be satisfied by selecting the minimum-cost set of edges. The prize-collecting version of these problems introduces flexibility by allowing penalties to be paid to ignore some connectivity requirements.
  Goemans and Williamson introduced a general technique and developed a 2-approximation algorithm for constrained forest problems. Further, Sharma, Swamy, and Williamson extended this work by developing a 2.54-approximation algorithm for the prize-collecting version of these problems. Motivated by the generality of their framework, which includes problems such as Steiner trees, Steiner forests, and their variants, we pursued further exploration.
  We present a significant improvement by achieving a 2-approximation algorithm for this general model, matching the approximation factor of the constrained forest problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15445v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Ahmadi, Iman Gholami, MohammadTaghi Hajiaghayi, Peyman Jabbarzade, Mohammad Mahdavi</dc:creator>
    </item>
    <item>
      <title>Adaptivity Gaps for Stochastic Probing with Subadditive Functions</title>
      <link>https://arxiv.org/abs/2504.15547</link>
      <description>arXiv:2504.15547v1 Announce Type: new 
Abstract: In this paper, we study the stochastic probing problem under a general monotone norm objective. Given a ground set $U = [n]$, each element $i \in U$ has an independent nonnegative random variable $X_i$ with known distribution. Probing an element reveals its value, and the sequence of probed elements must satisfy a prefix-closed feasibility constraint $\mathcal{F}$. A monotone norm $f: \mathbb{R}_{\geq 0}^n \to \mathbb{R}_{\geq 0}$ determines the reward $f(X_P)$, where $P$ is the set of probed elements and $X_P$ is the vector with $X_i$ for $i \in P$ and 0 otherwise. The goal is to design a probing strategy maximizing the expected reward $\mathbb{E}[f(X_P)]$. We focus on the adaptivity gap: the ratio between the expected rewards of optimal adaptive and optimal non-adaptive strategies. We resolve an open question posed in [GNS17, KMS24], showing that for general monotone norms, the adaptivity gap is $O(\log^2 n)$. A refined analysis yields an improved bound of $O(\log r \log n / \log\log n)$, where $r$ is the maximum size of a feasible probing sequence. As a by-product, we derive an asymptotically tight adaptivity gap $\Theta(\log n / \log\log n)$ for Bernoulli probing with binary-XOS objectives, matching the known lower bound. Additionally, we show an $O(\log^3 n)$ upper bound for Bernoulli probing with general subadditive objectives. For monotone symmetric norms, we prove the adaptivity gap is $O(1)$, improving the previous $O(\log n)$ bound from [PRS23].</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15547v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jian Li, Yinchen Liu, Yiran Zhang</dc:creator>
    </item>
    <item>
      <title>On the Price of Differential Privacy for Hierarchical Clustering</title>
      <link>https://arxiv.org/abs/2504.15580</link>
      <description>arXiv:2504.15580v1 Announce Type: new 
Abstract: Hierarchical clustering is a fundamental unsupervised machine learning task with the aim of organizing data into a hierarchy of clusters. Many applications of hierarchical clustering involve sensitive user information, therefore motivating recent studies on differentially private hierarchical clustering under the rigorous framework of Dasgupta's objective. However, it has been shown that any privacy-preserving algorithm under edge-level differential privacy necessarily suffers a large error. To capture practical applications of this problem, we focus on the weight privacy model, where each edge of the input graph is at least unit weight. We present a novel algorithm in the weight privacy model that shows significantly better approximation than known impossibility results in the edge-level DP setting. In particular, our algorithm achieves $O(\log^{1.5}n/\varepsilon)$ multiplicative error for $\varepsilon$-DP and runs in polynomial time, where $n$ is the size of the input graph, and the cost is never worse than the optimal additive error in existing work. We complement our algorithm by showing if the unit-weight constraint does not apply, the lower bound for weight-level DP hierarchical clustering is essentially the same as the edge-level DP, i.e. $\Omega(n^2/\varepsilon)$ additive error. As a result, we also obtain a new lower bound of $\tilde{\Omega}(1/\varepsilon)$ additive error for balanced sparsest cuts in the weight-level DP model, which may be of independent interest. Finally, we evaluate our algorithm on synthetic and real-world datasets. Our experimental results show that our algorithm performs well in terms of extra cost and has good scalability to large graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15580v1</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyuan Deng, Jie Gao, Jalaj Upadhyay, Chen Wang, Samson Zhou</dc:creator>
    </item>
    <item>
      <title>Towards True Work-Efficiency in Parallel Derandomization: MIS, Maximal Matching, and Hitting Set</title>
      <link>https://arxiv.org/abs/2504.15700</link>
      <description>arXiv:2504.15700v1 Announce Type: new 
Abstract: Derandomization is one of the classic topics studied in the theory of parallel computations, dating back to the early 1980s. Despite much work, all known techniques lead to deterministic algorithms that are not work-efficient. For instance, for the well-studied problem of maximal independent set -- e.g., [Karp, Wigderson STOC'84; Luby STOC' 85; Luby FOCS'88] -- state-of-the-art deterministic algorithms require at least $m \cdot poly(\log n)$ work, where $m$ and $n$ denote the number of edges and vertices. Hence, these deterministic algorithms will remain slower than their trivial sequential counterparts unless we have at least $poly(\log n)$ processors.
  In this paper, we present a generic parallel derandomization technique that moves exponentially closer to work-efficiency. The method iteratively rounds fractional solutions representing the randomized assignments to integral solutions that provide deterministic assignments, while maintaining certain linear or quadratic objective functions, and in an \textit{essentially work-efficient} manner. As example end-results, we use this technique to obtain deterministic algorithms with $m \cdot poly(\log \log n)$ work and $poly(\log n)$ depth for problems such as maximal independent set, maximal matching, and hitting set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15700v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Ghaffari, Christoph Grunau</dc:creator>
    </item>
    <item>
      <title>Branch-and-Bound Algorithms as Polynomial-time Approximation Schemes</title>
      <link>https://arxiv.org/abs/2504.15885</link>
      <description>arXiv:2504.15885v1 Announce Type: new 
Abstract: Branch-and-bound algorithms (B&amp;B) and polynomial-time approximation schemes (PTAS) are two seemingly distant areas of combinatorial optimization. We intend to (partially) bridge the gap between them while expanding the boundary of theoretical knowledge on the B&amp;B framework. Branch-and-bound algorithms typically guarantee that an optimal solution is eventually found. However, we show that the standard implementation of branch-and-bound for certain knapsack and scheduling problems also exhibits PTAS-like behavior, yielding increasingly better solutions within polynomial time. Our findings are supported by computational experiments and comparisons with benchmark methods. This paper is an extended version of a paper accepted at ICALP 2025.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15885v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICALP 2025</arxiv:journal_reference>
      <dc:creator>Kopp\'any Istv\'an Encz, Monaldo Mastrolilli, Eleonora Vercesi</dc:creator>
    </item>
    <item>
      <title>A Mysterious Connection Between Tolerant Junta Testing and Agnostically Learning Conjunctions</title>
      <link>https://arxiv.org/abs/2504.16065</link>
      <description>arXiv:2504.16065v1 Announce Type: new 
Abstract: The main conceptual contribution of this paper is identifying a previously unnoticed connection between two central problems in computational learning theory and property testing: agnostically learning conjunctions and tolerantly testing juntas. Inspired by this connection, the main technical contribution is a pair of improved algorithms for these two problems.
  In more detail,
  - We give a distribution-free algorithm for agnostically PAC learning conjunctions over $\{\pm 1\}^n$ that runs in time $2^{\widetilde{O}(n^{1/3})}$, for constant excess error $\varepsilon$. This improves on the fastest previously published algorithm, which runs in time $2^{\widetilde{O}(n^{1/2})}$ [KKMS08].
  - Building on the ideas in our agnostic conjunction learner and using significant additional technical ingredients, we give an adaptive tolerant testing algorithm for $k$-juntas that makes $2^{\widetilde{O}(k^{1/3})}$ queries, for constant "gap parameter" $\varepsilon$ between the "near" and "far" cases. This improves on the best previous results, due to [ITW21, NP24], which make $2^{\widetilde{O}(\sqrt{k})}$ queries. Since there is a known $2^{\widetilde{\Omega}(\sqrt{k})}$ lower bound for non-adaptive tolerant junta testers, our result shows that adaptive tolerant junta testing algorithms provably outperform non-adaptive ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16065v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Chen, Shyamal Patel, Rocco A. Servedio</dc:creator>
    </item>
    <item>
      <title>Range Counting Oracles for Geometric Problems</title>
      <link>https://arxiv.org/abs/2504.15292</link>
      <description>arXiv:2504.15292v1 Announce Type: cross 
Abstract: In this paper, we study estimators for geometric optimization problems in the sublinear geometric model. In this model, we have oracle access to a point set with size $n$ in a discrete space $[\Delta]^d$, where queries can be made to an oracle that responds to orthogonal range counting requests. The query complexity of an optimization problem is measured by the number of oracle queries required to compute an estimator for the problem. We investigate two problems in this framework, the Euclidean Minimum Spanning Tree (MST) and Earth Mover Distance (EMD). For EMD, we show the existence of an estimator that approximates the cost of EMD with $O(\log \Delta)$-relative error and $O(\frac{n\Delta}{s^{1+1/d}})$-additive error using $O(s\polylog \Delta)$ range counting queries for any parameter $s$ with $1\leq s \leq n$. Moreover, we prove that this bound is tight. For MST, we demonstrate that the weight of MST can be estimated within a factor of $(1 \pm \eps)$ using $\tilde{O}(\sqrt{n})$ range counting queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15292v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anne Driemel, Morteza Monemizadeh, Eunjin Oh, Frank Staals, David P. Woodruff</dc:creator>
    </item>
    <item>
      <title>Smooth Calibration and Decision Making</title>
      <link>https://arxiv.org/abs/2504.15582</link>
      <description>arXiv:2504.15582v1 Announce Type: cross 
Abstract: Calibration requires predictor outputs to be consistent with their Bayesian posteriors. For machine learning predictors that do not distinguish between small perturbations, calibration errors are continuous in predictions, e.g., smooth calibration error (Foster and Hart, 2018), Distance to Calibration (Blasiok et al., 2023a). On the contrary, decision-makers who use predictions make optimal decisions discontinuously in probabilistic space, experiencing loss from miscalibration discontinuously. Calibration errors for decision-making are thus discontinuous, e.g., Expected Calibration Error (Foster and Vohra, 1997), and Calibration Decision Loss (Hu and Wu, 2024). Thus, predictors with a low calibration error for machine learning may suffer a high calibration error for decision-making, i.e., they may not be trustworthy for decision-makers optimizing assuming their predictions are correct. It is natural to ask if post-processing a predictor with a low calibration error for machine learning is without loss to achieve a low calibration error for decision-making. In our paper, we show that post-processing an online predictor with $\epsilon$ distance to calibration achieves $O(\sqrt{\epsilon})$ ECE and CDL, which is asymptotically optimal. The post-processing algorithm adds noise to make predictions differentially private. The optimal bound from low distance to calibration predictors from post-processing is non-optimal compared with existing online calibration algorithms that directly optimize for ECE and CDL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15582v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Hartline, Yifan Wu, Yunran Yang</dc:creator>
    </item>
    <item>
      <title>Quantum Speedup for Sampling Random Spanning Trees</title>
      <link>https://arxiv.org/abs/2504.15603</link>
      <description>arXiv:2504.15603v1 Announce Type: cross 
Abstract: We present a quantum algorithm for sampling random spanning trees from a weighted graph in $\widetilde{O}(\sqrt{mn})$ time, where $n$ and $m$ denote the number of vertices and edges, respectively. Our algorithm has sublinear runtime for dense graphs and achieves a quantum speedup over the best-known classical algorithm, which runs in $\widetilde{O}(m)$ time. The approach carefully combines, on one hand, a classical method based on ``large-step'' random walks for reduced mixing time and, on the other hand, quantum algorithmic techniques, including quantum graph sparsification and a sampling-without-replacement variant of Hamoudi's multiple-state preparation. We also establish a matching lower bound, proving the optimality of our algorithm up to polylogarithmic factors. These results highlight the potential of quantum computing in accelerating fundamental graph sampling problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15603v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenghua Liu, Minbo Gao, Zhengfeng Ji, Simon Apers</dc:creator>
    </item>
    <item>
      <title>Tight Approximation and Kernelization Bounds for Vertex-Disjoint Shortest Paths</title>
      <link>https://arxiv.org/abs/2402.15348</link>
      <description>arXiv:2402.15348v2 Announce Type: replace 
Abstract: We examine the possibility of approximating Maximum Vertex-Disjoint Shortest Paths. In this problem, the input is an edge-weighted (directed or undirected) $n$-vertex graph $G$ along with $k$ terminal pairs $(s_1,t_1),(s_2,t_2),\ldots,(s_k,t_k)$. The task is to connect as many terminal pairs as possible by pairwise vertex-disjoint paths such that each path is a shortest path between the respective terminals. Our work is anchored in the recent breakthrough by Lochet [SODA '21], which demonstrates the polynomial-time solvability of the problem for a fixed value of $k$.
  Lochet's result implies the existence of a polynomial-time $ck$-approximation for Maximum Vertex-Disjoint Shortest Paths, where $c \leq 1$ is a constant. Our first result suggests that this approximation algorithm is, in a sense, the best we can hope for. More precisely, assuming the gap-ETH, we exclude the existence of an $o(k)$-approximations within $f(k)$poly($n$) time for any function $f$ that only depends on $k$.
  Our second result demonstrates the infeasibility of achieving an approximation ratio of $n^{\frac{1}{2}-\varepsilon}$ in polynomial time, unless P = NP. It is not difficult to show that a greedy algorithm selecting a path with the minimum number of arcs results in a $\sqrt{\ell}$-approximation, where $\ell$ is the number of edges in all the paths of an optimal solution. Since $\ell \leq n$, this underscores the tightness of the $n^{\frac{1}{2}-\varepsilon}$-inapproximability bound. Additionally, we establish that the problem can be solved in $2^{O(\ell)}$poly($n$) time, but does not admit a polynomial kernel in $\ell$. Moreover, it cannot be solved in $2^{o(\ell)}$poly($n$) time unless the ETH fails. Our hardness results hold for undirected graphs with unit weights, while our positive results extend to scenarios where the input graph is directed and features arbitrary (non-negative) edge weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15348v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, Fedor V. Fomin, Petr A. Golovach</dc:creator>
    </item>
    <item>
      <title>On the accurate computation of expected modularity in probabilistic networks</title>
      <link>https://arxiv.org/abs/2408.07161</link>
      <description>arXiv:2408.07161v3 Announce Type: replace 
Abstract: Modularity is one of the most widely used measures for evaluating communities in networks. In probabilistic networks, where the existence of edges is uncertain and uncertainty is represented by probabilities, the expected value of modularity can be used instead. However, efficiently computing expected modularity is challenging. To address this challenge, we propose a novel and efficient technique (FPWP) for computing the probability distribution of modularity and its expected value. In this paper, we implement and compare our method and various general approaches for expected modularity computation in probabilistic networks. These include: (1) translating probabilistic networks into deterministic ones by removing low-probability edges or treating probabilities as weights, (2) using Monte Carlo sampling to approximate expected modularity, and (3) brute-force computation. We evaluate the accuracy and time efficiency of FPWP through comprehensive experiments on both real-world and synthetic networks with diverse characteristics. Our results demonstrate that removing low-probability edges or treating probabilities as weights produces inaccurate results, while the convergence of the sampling method varies with the parameters of the network. Brute-force computation, though accurate, is prohibitively slow. In contrast, our method is much faster than brute-force computation, but guarantees an accurate result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07161v3</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Shen, Matteo Magnani, Christian Rohner, Fiona Skerman</dc:creator>
    </item>
    <item>
      <title>Sparse induced subgraphs in $P_7$-free graphs of bounded clique number</title>
      <link>https://arxiv.org/abs/2412.14836</link>
      <description>arXiv:2412.14836v3 Announce Type: replace 
Abstract: Many natural computational problems, including e.g. Max Weight Independent Set, Feedback Vertex Set, or Vertex Planarization, can be unified under an umbrella of finding the largest sparse induced subgraph, that satisfies some property definable in CMSO$_2$ logic.
  It is believed that each problem expressible with this formalism can be solved in polynomial time in graphs that exclude a fixed path as an induced subgraph.
  This belief is supported by the existence of a quasipolynomial-time algorithm by Gartland, Lokshtanov, Pilipczuk, Pilipczuk, and Rz\k{a}\.zewski [STOC 2021], and a recent polynomial-time algorithm for $P_6$-free graphs by Chudnovsky, McCarty, Pilipczuk, Pilipczuk, and Rz\k{a}\.zewski [SODA 2024].
  In this work we extend polynomial-time tractability of all such problems to $P_7$-free graphs of bounded clique number.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14836v3</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Chudnovsky, Jadwiga Czy\.zewska, Kacper Kluk, Marcin Pilipczuk, Pawe{\l} Rz\k{a}\.zewski</dc:creator>
    </item>
    <item>
      <title>Faster Semi-streaming Matchings via Alternating Trees</title>
      <link>https://arxiv.org/abs/2412.19057</link>
      <description>arXiv:2412.19057v3 Announce Type: replace 
Abstract: We design a deterministic algorithm for the $(1+\epsilon)$-approximate maximum matching problem. Our primary result demonstrates that this problem can be solved in $O(\epsilon^{-6})$ semi-streaming passes, improving upon the $O(\epsilon^{-19})$ pass-complexity algorithm by [Fischer, Mitrovi\'c, and Uitto, STOC'22]. This contributes substantially toward resolving Open question 2 from [Assadi, SOSA'24]. Leveraging the framework introduced in [FMU'22], our algorithm achieves an analogous round complexity speed-up for computing a $(1+\epsilon)$-approximate maximum matching in both the Massively Parallel Computation (MPC) and CONGEST models.
  The data structures maintained by our algorithm are formulated using blossom notation and represented through alternating trees. This approach enables a simplified correctness analysis by treating specific components as if operating on bipartite graphs, effectively circumventing certain technical intricacies present in prior work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19057v3</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Slobodan Mitrovi\'c, Anish Mukherjee, Piotr Sankowski, Wen-Horng Sheu</dc:creator>
    </item>
    <item>
      <title>Static to Dynamic Correlation Clustering</title>
      <link>https://arxiv.org/abs/2504.12060</link>
      <description>arXiv:2504.12060v2 Announce Type: replace 
Abstract: Correlation clustering is a well-studied problem, first proposed by Bansal, Blum, and Chawla [BBC04]. The input is an unweighted, undirected graph. The problem is to cluster the vertices so as to minimizing the number of edges between vertices in different clusters and missing edges between vertices inside the same cluster. This problem has a wide application in data mining and machine learning. We introduce a general framework that transforms existing static correlation clustering algorithms into fully-dynamic ones that work against an adaptive adversary.
  We show how to apply our framework to known efficient correlation clustering algorithms, starting from the classic $3$-approximate Pivot algorithm from [ACN08]. Applied to the most recent near-linear $1.437$-approximation algorithm from [CCL+25], we get a $1.437$-approximation fully-dynamic algorithm that works with worst-case constant update time. The original static algorithm gets its approximation factor with constant probability, and we get the same against an adaptive adversary in the sense that for any given update step not known to our algorithm, our solution is a $1.437$-approximation with constant probability when we reach this update.
  Previous dynamic algorithms had approximation factors around $3$ in expectation, and they could only handle an oblivious adversary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12060v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nairen Cao, Vincent Cohen-Addad, Euiwoong Lee, Shi Li, David Rasmussen Lolck, Alantha Newman, Mikkel Thorup, Lukas Vogl, Shuyi Yan, Hanwen Zhang</dc:creator>
    </item>
    <item>
      <title>Robust Distributed Arrays: Provably Secure Networking for Data Availability Sampling</title>
      <link>https://arxiv.org/abs/2504.13757</link>
      <description>arXiv:2504.13757v2 Announce Type: replace 
Abstract: Data Availability Sampling (DAS), a central component of Ethereum's roadmap, enables clients to verify data availability without requiring any single client to download the entire dataset. DAS operates by having clients randomly retrieve individual symbols of erasure-encoded data from a peer-to-peer network. While the cryptographic and encoding aspects of DAS have recently undergone formal analysis, the peer-to-peer networking layer remains underexplored, with a lack of security definitions and efficient, provably secure constructions. In this work, we address this gap by introducing a novel distributed data structure that can serve as the networking layer for DAS, which we call \emph{robust distributed arrays}. That is, we rigorously define a robustness property of a distributed data structure in an open permissionless network, that mimics a collection of arrays. Then, we give a simple and efficient construction and formally prove its robustness. Notably, every individual node is required to store only small portions of the data, and accessing array positions incurs minimal latency. The robustness of our construction relies solely on the presence of a minimal \emph{absolute} number of honest nodes in the network. In particular, we avoid any honest majority assumption. Beyond DAS, we anticipate that robust distributed arrays can have wider applications in distributed systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13757v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dankrad Feist, Gottfried Herold, Mark Simkin, Benedikt Wagner</dc:creator>
    </item>
    <item>
      <title>Weakly Approximating Knapsack in Subquadratic Time</title>
      <link>https://arxiv.org/abs/2504.15001</link>
      <description>arXiv:2504.15001v2 Announce Type: replace 
Abstract: We consider the classic Knapsack problem. Let $t$ and $\mathrm{OPT}$ be the capacity and the optimal value, respectively. If one seeks a solution with total profit at least $\mathrm{OPT}/(1 + \varepsilon)$ and total weight at most $t$, then Knapsack can be solved in $\tilde{O}(n + (\frac{1}{\varepsilon})^2)$ time [Chen, Lian, Mao, and Zhang '24][Mao '24]. This running time is the best possible (up to a logarithmic factor), assuming that $(\min,+)$-convolution cannot be solved in truly subquadratic time [K\"unnemann, Paturi, and Schneider '17][Cygan, Mucha, W\k{e}grzycki, and W{\l}odarczyk '19]. The same upper and lower bounds hold if one seeks a solution with total profit at least $\mathrm{OPT}$ and total weight at most $(1 + \varepsilon)t$. Therefore, it is natural to ask the following question.
  If one seeks a solution with total profit at least $\mathrm{OPT}/(1+\varepsilon)$ and total weight at most $(1 + \varepsilon)t$, can Knsapck be solved in $\tilde{O}(n + (\frac{1}{\varepsilon})^{2-\delta})$ time for some constant $\delta &gt; 0$?
  We answer this open question affirmatively by proposing an $\tilde{O}(n + (\frac{1}{\varepsilon})^{7/4})$-time algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15001v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Chen, Jiayi Lian, Yuchen Mao, Guochuan Zhang</dc:creator>
    </item>
    <item>
      <title>Flow-augmentation III: Complexity dichotomy for Boolean CSPs parameterized by the number of unsatisfied constraints</title>
      <link>https://arxiv.org/abs/2207.07422</link>
      <description>arXiv:2207.07422v3 Announce Type: replace-cross 
Abstract: We study the parameterized problem of satisfying ``almost all'' constraints of a given formula $F$ over a fixed, finite Boolean constraint language $\Gamma$, with or without weights. More precisely, for each finite Boolean constraint language $\Gamma$, we consider the following two problems. In Min SAT$(\Gamma)$, the input is a formula $F$ over $\Gamma$ and an integer $k$, and the task is to find an assignment $\alpha \colon V(F) \to \{0,1\}$ that satisfies all but at most $k$ constraints of $F$, or determine that no such assignment exists. In Weighted Min SAT$(\Gamma$), the input additionally contains a weight function $w \colon F \to \mathbb{Z}_+$ and an integer $W$, and the task is to find an assignment $\alpha$ such that (1) $\alpha$ satisfies all but at most $k$ constraints of $F$, and (2) the total weight of the violated constraints is at most $W$. We give a complete dichotomy for the fixed-parameter tractability of these problems: We show that for every Boolean constraint language $\Gamma$, either Weighted Min SAT$(\Gamma)$ is FPT; or Weighted Min SAT$(\Gamma)$ is W[1]-hard but Min SAT$(\Gamma)$ is FPT; or Min SAT$(\Gamma)$ is W[1]-hard. This generalizes recent work of Kim et al. (SODA 2021) which did not consider weighted problems, and only considered languages $\Gamma$ that cannot express implications $(u \to v)$ (as is used to, e.g., model digraph cut problems). Our result generalizes and subsumes multiple previous results, including the FPT algorithms for Weighted Almost 2-SAT, weighted and unweighted $\ell$-Chain SAT, and Coupled Min-Cut, as well as weighted and directed versions of the latter. The main tool used in our algorithms is the recently developed method of directed flow-augmentation (Kim et al., STOC 2022).</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.07422v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eun Jung Kim, Stefan Kratsch, Marcin Pilipczuk, Magnus Wahlstr\"om</dc:creator>
    </item>
    <item>
      <title>Slipstream: Ebb-and-Flow Consensus on a DAG with Fast Confirmation for UTXO Transactions</title>
      <link>https://arxiv.org/abs/2410.14876</link>
      <description>arXiv:2410.14876v2 Announce Type: replace-cross 
Abstract: This paper introduces Slipstream, a Byzantine Fault Tolerance (BFT) protocol where nodes concurrently propose blocks to be added to a Directed Acyclic Graph (DAG) and aim to agree on block ordering. Slipstream offers two types of block orderings: an optimistic ordering, which is live and secure in a sleepy model under up to 50% Byzantine nodes, and a final ordering, which is a prefix of the optimistic ordering and ensures safety and liveness in an eventual lock-step synchronous model under up to 33% Byzantine nodes. Additionally, Slipstream integrates a payment system that allows for fast UTXO transaction confirmation independently of block ordering. Transactions are confirmed in three rounds during synchrony, and unconfirmed double spends are resolved in a novel way using the DAG structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14876v2</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikita Polyanskii, Sebastian Muller, Mayank Raikwar</dc:creator>
    </item>
    <item>
      <title>Learning the structure of any Hamiltonian from minimal assumptions</title>
      <link>https://arxiv.org/abs/2410.21635</link>
      <description>arXiv:2410.21635v2 Announce Type: replace-cross 
Abstract: We study the problem of learning an unknown quantum many-body Hamiltonian $H$ from black-box queries to its time evolution $e^{-\mathrm{i} H t}$. Prior proposals for solving this task either impose some assumptions on $H$, such as its interaction structure or locality, or otherwise use an exponential amount of computational postprocessing. In this paper, we present algorithms to learn any $n$-qubit Hamiltonian, which do not need to know the Hamiltonian terms in advance, nor are they restricted to local interactions. Our algorithms are efficient as long as the number of terms $m$ is polynomially bounded in the system size $n$. We consider two models of control over the time evolution:~the first has access to time reversal ($t &lt; 0$), enabling an algorithm that outputs an $\epsilon$-accurate classical description of $H$ after querying its dynamics for a total of $\widetilde{\mathcal{O}}(m/\epsilon)$ evolution time. The second access model is more conventional, allowing only forward-time evolutions;~our algorithm requires $\widetilde{\mathcal{O}}(\|H\|^3/\epsilon^4)$ evolution time in this setting. Central to our results is the recently introduced concept of a pseudo-Choi state of $H$. We extend the utility of this learning resource by showing how to use it to learn the Fourier spectrum of $H$, how to achieve nearly Heisenberg-limited scaling with it, and how to prepare it even under our more restricted access models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21635v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Zhao</dc:creator>
    </item>
    <item>
      <title>Vertical Federated Learning with Missing Features During Training and Inference</title>
      <link>https://arxiv.org/abs/2410.22564</link>
      <description>arXiv:2410.22564v3 Announce Type: replace-cross 
Abstract: Vertical federated learning trains models from feature-partitioned datasets across multiple clients, who collaborate without sharing their local data. Standard approaches assume that all feature partitions are available during both training and inference. Yet, in practice, this assumption rarely holds, as for many samples only a subset of the clients observe their partition. However, not utilizing incomplete samples during training harms generalization, and not supporting them during inference limits the utility of the model. Moreover, if any client leaves the federation after training, its partition becomes unavailable, rendering the learned model unusable. Missing feature blocks are therefore a key challenge limiting the applicability of vertical federated learning in real-world scenarios. To address this, we propose LASER-VFL, a vertical federated learning method for efficient training and inference of split neural network-based models that is capable of handling arbitrary sets of partitions. Our approach is simple yet effective, relying on the sharing of model parameters and on task-sampling to train a family of predictors. We show that LASER-VFL achieves a $\mathcal{O}({1}/{\sqrt{T}})$ convergence rate for nonconvex objectives and, under the Polyak-{\L}ojasiewicz inequality, it achieves linear convergence to a neighborhood of the optimum. Numerical experiments show improved performance of LASER-VFL over the baselines. Remarkably, this is the case even in the absence of missing features. For example, for CIFAR-100, we see an improvement in accuracy of $19.3\%$ when each of four feature blocks is observed with a probability of 0.5 and of $9.5\%$ when all features are observed. The code for this work is available at https://github.com/Valdeira/LASER-VFL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22564v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Valdeira, Shiqiang Wang, Yuejie Chi</dc:creator>
    </item>
    <item>
      <title>Six Candidates Suffice to Win a Voter Majority</title>
      <link>https://arxiv.org/abs/2411.03390</link>
      <description>arXiv:2411.03390v4 Announce Type: replace-cross 
Abstract: A cornerstone of social choice theory is Condorcet's paradox which says that in an election where $n$ voters rank $m$ candidates it is possible that, no matter which candidate is declared the winner, a majority of voters would have preferred an alternative candidate. Instead, can we always choose a small committee of winning candidates that is preferred to any alternative candidate by a majority of voters?
  Elkind, Lang, and Saffidine raised this question and called such a committee a Condorcet winning set. They showed that winning sets of size $2$ may not exist, but sets of size logarithmic in the number of candidates always do. In this work, we show that Condorcet winning sets of size $6$ always exist, regardless of the number of candidates or the number of voters. More generally, we show that if $\frac{\alpha}{1 - \ln \alpha} \geq \frac{2}{k + 1}$, then there always exists a committee of size $k$ such that less than an $\alpha$ fraction of the voters prefer an alternate candidate. These are the first nontrivial positive results that apply for all $k \geq 2$.
  Our proof uses the probabilistic method and the minimax theorem, inspired by recent work on approximately stable committee selection. We construct a distribution over committees that performs sufficiently well (when compared against any candidate on any small subset of the voters) so that this distribution must contain a committee with the desired property in its support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03390v4</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3717823.3718235</arxiv:DOI>
      <dc:creator>Moses Charikar, Alexandra Lassota, Prasanna Ramakrishnan, Adrian Vetta, Kangning Wang</dc:creator>
    </item>
    <item>
      <title>Stable Hypergraph Matching in Unimodular Hypergraphs</title>
      <link>https://arxiv.org/abs/2502.08827</link>
      <description>arXiv:2502.08827v2 Announce Type: replace-cross 
Abstract: We study the NP-hard Stable Hypergraph Matching (SHM) problem and its generalization allowing capacities, the Stable Hypergraph $b$-Matching (SH$b$M) problem, and investigate their computational properties under various structural constraints. Our study is motivated by the fact that Scarf's Lemma (Scarf, 1967) together with a result of Lov\'asz (1972) guarantees the existence of a stable matching whenever the underlying hypergraph is normal. Furthermore, if the hypergraph is unimodular (i.e., its incidence matrix is totally unimodular), then even a stable $b$-matching is guaranteed to exist. However, no polynomial-time algorithm is known for finding a stable matching or $b$-matching in unimodular hypergraphs.
  We identify subclasses of unimodular hypergraphs where SHM and SH$b$M are tractable such as laminar hypergraphs or so-called subpath hypergraphs with bounded-size hyperedges; for the latter case, even a maximum-weight stable $b$-matching can be found efficiently. We complement our algorithms by showing that optimizing over stable matchings is NP-hard even in laminar hypergraphs. As a practically important special case of SH$b$M for unimodular hypergraphs, we investigate a tripartite stable matching problem with students, schools, and companies as agents, called the University Dual Admission problem, which models real-world scenarios in higher education admissions.
  Finally, we examine a superclass of subpath hypergraphs that are normal but necessarily not unimodular, namely subtree hypergraphs where hyperedges correspond to subtrees of a tree. We establish that for such hypergraphs, stable matchings can be found in polynomial time but, in the setting with capacities, finding a stable $b$-matching is NP-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08827v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P\'eter Bir\'o, Gergely Cs\'aji, Ildik\'o Schlotter</dc:creator>
    </item>
    <item>
      <title>Improving Algorithmic Efficiency using Cryptography</title>
      <link>https://arxiv.org/abs/2502.13065</link>
      <description>arXiv:2502.13065v2 Announce Type: replace-cross 
Abstract: Cryptographic primitives have been used for various non-cryptographic objectives, such as eliminating or reducing randomness and interaction. We show how to use cryptography to improve the time complexity of solving computational problems. Specifically, we show that under standard cryptographic assumptions, we can design algorithms that are asymptotically faster than existing ones while maintaining correctness. As a concrete demonstration, we construct a distribution of trapdoored matrices with the following properties: (a) computationally bounded adversaries cannot distinguish a random matrix from one drawn from this distribution (under computational hardness assumptions), and (b) given a trapdoor, we can multiply such an $n \times n$ matrix with any vector in near-linear (in $n$) time. We provide constructions both over finite fields and over the reals. This enables a broad speedup technique: any algorithm relying on a random matrix -- such as those that use various notions of dimensionality reduction -- can replace it with a matrix from our distribution, achieving computational speedups while preserving correctness. Using these trapdoored matrices, we present the first uniform reduction from worst-case to approximate and average-case matrix multiplication with optimal parameters (improving on Hirahara--Shimizu STOC 2025, albeit under computational assumptions), the first worst-case to average-case reductions for matrix inversion, solving a linear system, and computing a determinant, as well as a speedup of inference time in classification models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13065v2</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vinod Vaikuntanathan, Or Zamir</dc:creator>
    </item>
  </channel>
</rss>
