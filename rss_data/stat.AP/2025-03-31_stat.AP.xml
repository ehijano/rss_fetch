<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Apr 2025 03:14:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>March Madness Tournament Predictions Model: A Mathematical Modeling Approach</title>
      <link>https://arxiv.org/abs/2503.21790</link>
      <description>arXiv:2503.21790v1 Announce Type: new 
Abstract: This paper proposes a model to predict the outcome of the March Madness tournament based on historical NCAA basketball data since 2013. The framework of this project is a simplification of the FiveThrityEight NCAA March Madness prediction model, where the only four predictors of interest are Adjusted Offensive Efficiency (ADJOE), Adjusted Defensive Efficiency (ADJDE), Power Rating, and Two-Point Shooting Percentage Allowed. A logistic regression was utilized with the aforementioned metrics to generate a probability of a particular team winning each game. Then, a tournament simulation is developed and compared to real-world March Madness brackets to determine the accuracy of the model. Accuracies of performance were calculated using a naive approach and a Spearman rank correlation coefficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21790v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian McIver, Karla Avalos, Nikhil Nayak</dc:creator>
    </item>
    <item>
      <title>Binary AddiVortes: (Bayesian) Additive Voronoi Tessellations for Binary Classification with an application to Predicting Home Mortgage Application Outcomes</title>
      <link>https://arxiv.org/abs/2503.21792</link>
      <description>arXiv:2503.21792v1 Announce Type: new 
Abstract: The Additive Voronoi Tessellations (AddiVortes) model is a multivariate regression model that uses multiple Voronoi tessellations to partition the covariate space for an additive ensemble model. In this paper, the AddiVortes framework is extended to binary classification by incorporating a probit model with a latent variable formulation. Specifically, we utilise a data augmentation technique, where a latent variable is introduced and the binary response is determined via thresholding. In most cases, the AddiVortes model outperforms random forests, BART and other leading black-box regression models when compared using a range of metrics. A comprehensive analysis is conducted using AddiVortes to predict an individual's likelihood of being approved for a home mortgage, based on a range of covariates. This evaluation highlights the model's effectiveness in capturing complex relationships within the data and its potential for improving decision-making in mortgage approval processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21792v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam J. Stone, Emmanuel Ogundimu, John Paul Gosling</dc:creator>
    </item>
    <item>
      <title>Structured and sparse partial least squares coherence for multivariate cortico-muscular analysis</title>
      <link>https://arxiv.org/abs/2503.21802</link>
      <description>arXiv:2503.21802v1 Announce Type: new 
Abstract: Multivariate cortico-muscular analysis has recently emerged as a promising approach for evaluating the corticospinal neural pathway. However, current multivariate approaches encounter challenges such as high dimensionality and limited sample sizes, thus restricting their further applications. In this paper, we propose a structured and sparse partial least squares coherence algorithm (ssPLSC) to extract shared latent space representations related to cortico-muscular interactions. Our approach leverages an embedded optimization framework by integrating a partial least squares (PLS)-based objective function, a sparsity constraint and a connectivity-based structured constraint, addressing the generalizability, interpretability and spatial structure. To solve the optimization problem, we develop an efficient alternating iterative algorithm within a unified framework and prove its convergence experimentally. Extensive experimental results from one synthetic and several real-world datasets have demonstrated that ssPLSC can achieve competitive or better performance over some representative multivariate cortico-muscular fusion methods, particularly in scenarios characterized by limited sample sizes and high noise levels. This study provides a novel multivariate fusion method for cortico-muscular analysis, offering a transformative tool for the evaluation of corticospinal pathway integrity in neurological disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21802v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyao Sun, Qilu Zhang, Di Ma, Tianyu Jia, Shijie Jia, Xiaoxue Zhai, Ruimou Xie, Ping-Ju Lin, Zhibin Li, Yu Pan, Linhong Ji, Chong Li</dc:creator>
    </item>
    <item>
      <title>Enhancing Predictive Accuracy in Tennis: Integrating Fuzzy Logic and CV-GRNN for Dynamic Match Outcome and Player Momentum Analysis</title>
      <link>https://arxiv.org/abs/2503.21809</link>
      <description>arXiv:2503.21809v1 Announce Type: new 
Abstract: The predictive analysis of match outcomes and player momentum in professional tennis has long been a subject of scholarly debate. In this paper, we introduce a novel approach to game prediction by combining a multi-level fuzzy evaluation model with a CV-GRNN model. We first identify critical statistical indicators via Principal Component Analysis and then develop a two-tier fuzzy model based on the Wimbledon data. In addition, the results of Pearson Correlation Coefficient indicate that the momentum indicators, such as Player Win Streak and Score Difference, have a strong correlation among them, revealing insightful trends among players transitioning between losing and winning streaks. Subsequently, we refine the CV-GRNN model by incorporating 15 statistically significant indicators, resulting in an increase in accuracy to 86.64% and a decrease in MSE by 49.21%. This consequently strengthens the methodological framework for predicting tennis match outcomes, emphasizing its practical utility and potential for adaptation in various athletic contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21809v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kechen Li, Jiaming Liu, Zhenyu Wu, Jinpeng Li</dc:creator>
    </item>
    <item>
      <title>Hierarchical models for small area estimation using zero-inflated forest inventory variables: comparison and implementation</title>
      <link>https://arxiv.org/abs/2503.22103</link>
      <description>arXiv:2503.22103v1 Announce Type: new 
Abstract: National Forest Inventory (NFI) data are typically limited to sparse networks of sample locations due to cost constraints. While traditional design-based estimators provide reliable forest parameter estimates for large areas, there is increasing interest in model-based small area estimation (SAE) methods to improve precision for smaller spatial, temporal, or biophysical domains. SAE methods can be broadly categorized into area- and unit-level models, with unit-level models offering greater flexibility -- making them the focus of this study. Ensuring valid inference requires satisfying model distributional assumptions, which is particularly challenging for NFI variables that exhibit positive support and zero inflation, such as forest biomass, carbon, and volume. Here, we evaluate a class of two-stage unit-level hierarchical Bayesian models for estimating forest biomass at the county-level in Washington and Nevada, United States. We compare these models to simpler Bayesian single-stage and two-stage frequentist approaches. To assess estimator performance, we employ simulated populations and cross-validation techniques. Results indicate that small area estimators that incorporate a two-stage approach to account for zero inflation, county-specific random intercepts and residual variances, and spatial random effects provide the most reliable county-level estimates. Additionally, findings suggest that unit-level cross-validation within the training dataset is as effective as area-level validation using simulated populations for model selection. We also illustrate the usefulness of simulated populations for better assessing qualities of the various estimators considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22103v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grayson W. White, Andrew O. Finley, Josh K. Yamamoto, Jennifer L. Green, Tracey S. Frescino, Hans-Erik Andersen</dc:creator>
    </item>
    <item>
      <title>Estimation of Building Energy Demand Characteristics using Bayesian Statistics and Energy Signature Models</title>
      <link>https://arxiv.org/abs/2503.22321</link>
      <description>arXiv:2503.22321v1 Announce Type: new 
Abstract: This work presents a scalable Bayesian modeling framework for evaluating building energy performance using smart-meter data from 2,788 Danish single-family homes. The framework leverages Bayesian statistical inference integrated with Energy Signature (ES) models to characterize thermal performance in buildings. This approach quantifies key parameters such as the Heat Loss Coefficient (HLC), solar gain, and wind infiltration, while providing full posterior distributions to reflect parameter uncertainty.
  Three model variants are developed: a baseline ES model, an auto-regressive model (ARX-ES) to account for thermal inertia, and an auto-regressive moving average model (ARMAX-ES) that approximates stochastic gray-box dynamics. Results show that model complexity improves one-step-ahead predictive performance, with the ARMAX-ES model achieving a median Bayesian R^2 of 0.94 across the building stock. At the single-building level, the Bayesian approach yields credible intervals for yearly energy demand within $\pm1\%$, enabling more robust diagnostics than deterministic methods.
  Beyond improved accuracy, the Bayesian framework enhances decision-making by explicitly representing uncertainty in building performance parameters. This provides a more realistic foundation for investment prioritization, demand forecasting, and long-term energy planning. The method is readily applicable to other building typologies or geographies, offering a scalable tool for data-driven energy management under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22321v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justinas Smertinas, Nicolaj Hans Nielsen, Matthias Y. C. Van Hove, Peder Bacher, Henrik Madsen</dc:creator>
    </item>
    <item>
      <title>Comparing methods to assess treatment effect heterogeneity in general parametric regression models</title>
      <link>https://arxiv.org/abs/2503.22548</link>
      <description>arXiv:2503.22548v1 Announce Type: new 
Abstract: This paper reviews and compares methods to assess treatment effect heterogeneity in the context of parametric regression models. These methods include the standard likelihood ratio tests, bootstrap likelihood ratio tests, and Goeman's global test motivated by testing whether the random effect variance is zero. We place particular emphasis on tests based on the score-residual of the treatment effect and explore different variants of tests in this class. All approaches are compared in a simulation study, and the approach based on residual scores is illustrated in a study comparing multiple doses versus placebo. Our findings demonstrate that score-residual based methods provide practical, flexible and reliable tools for identifying treatment effect heterogeneity and treatment effect modifiers, and can provide useful guidance for decision making around treatment effect heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22548v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Chen, Sophie Sun, Konstantinos Sechidis, Cong Zhang, Torsten Hothorn, Bj\"orn Bornkamp</dc:creator>
    </item>
    <item>
      <title>A novel smoothing-based goodness-of-fit test of covariance for multivariate sparse functional data</title>
      <link>https://arxiv.org/abs/2503.21913</link>
      <description>arXiv:2503.21913v1 Announce Type: cross 
Abstract: Accurately specifying covariance structures is critical for valid inference in longitudinal and functional data analysis, particularly when data are sparsely observed. In this study, we develop a global goodness-of-fit test to assess parametric covariance structures in multivariate sparse functional data. Our contribution is twofold. First, we extend the univariate goodness-of-fit test proposed by Chen et al. (2019) to better accommodate sparse data by improving error variance estimation and applying positive semi-definite smoothing to covariance estimation. These corrections ensure appropriate Type I error control under sparse designs. Second, we introduce a multivariate extension of the improved test that jointly evaluates covariance structures across multiple outcomes, employing novel test statistics based on the maximum and $\ell_2$ norms to account for inter-outcome dependencies and enhance statistical power. Through extensive simulation studies, we demonstrate that the proposed methods maintain proper Type I error rates and achieve greater power than univariate tests with multiple testing adjustments. Applications to longitudinal neuroimaging and clinical data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Parkinson's Progression Marker Initiative (PPMI) illustrate the practical utility of the proposed methods for evaluating covariance structures in sparse multivariate longitudinal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21913v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Zhuolin Song, Luo Xiao, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>A Semiparametric Quantile Single-Index Model for Zero-Inflated Outcomes</title>
      <link>https://arxiv.org/abs/2503.21924</link>
      <description>arXiv:2503.21924v1 Announce Type: cross 
Abstract: We consider the complex data modeling problem motivated by the zero-inflated and overdispersed data from microbiome studies. Analyzing how microbiome abundance is associated with human biological features, such as BMI, is of great importance for host health. Methods based on parametric distributional assumptions, such as zero-inflated Poisson and zero-inflated Negative Binomial regression, have been widely used in modeling such data, yet the parametric assumptions are restricted and hard to verify in real-world applications. We relax the parametric assumptions and propose a semiparametric single-index quantile regression model. It is flexible to include a wide range of possible association functions and adaptable to the various zero proportions across subjects, which relaxes the strong parametric distributional assumptions of most existing zero-inflated data modeling approaches. We establish the asymptotic properties for the index coefficients estimator and quantile regression curve estimation. Through extensive simulation studies, we demonstrate the superior performance of the proposed method regarding model fitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21924v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5705/ss.202024.0104</arxiv:DOI>
      <dc:creator>Zirui Wang, Tianying Wang</dc:creator>
    </item>
    <item>
      <title>Parapolitics and Roll-Call Voting in Colombia: A Bayesian Euclidean and Spherical Spatial Analysis</title>
      <link>https://arxiv.org/abs/2503.22045</link>
      <description>arXiv:2503.22045v1 Announce Type: cross 
Abstract: This study presents a Bayesian spatial voting analysis of the Colombian Senate during the 2006-2010 legislative period, leveraging a newly constructed roll-call dataset comprising 147 senators and 136 plenary votes. We estimate legislators' ideal points under two alternative geometric frameworks: A traditional Euclidean model and a circular model that embeds preferences on the unit circle. Both models are implemented using Markov Chain Monte Carlo methods, with the circular specification capturing geodesic distances and von Mises-distributed latent traits. The results reveal a latent structure in voting behavior best characterized not by a conventional left-right ideological continuum but by an opposition-non-opposition alignment. Using Bayesian logistic regression, we further investigate the association between senators' ideal points and their involvement in the para-politics scandal. Findings indicate a significant and robust relationship between political alignment and para-politics implication, suggesting that extralegal influence was systematically related to senators' legislative behavior during this period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22045v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Carolina Luque, Juan Valero</dc:creator>
    </item>
    <item>
      <title>An integrated method for clustering and association network inference</title>
      <link>https://arxiv.org/abs/2503.22467</link>
      <description>arXiv:2503.22467v1 Announce Type: cross 
Abstract: We consider high dimensional Gaussian graphical models inference. These models provide a rigorous framework to describe a network of statistical dependencies between entities, such as genes in genomic regulation studies or species in ecology. Penalized methods, including the standard Graphical-Lasso, are well-known approaches to infer the parameters of these models. As the number of variables in the model (of entities in the network) grow, the network inference and interpretation become more complex. We propose Normal-Block, a new model that clusters variables and consider a network at the cluster level. Normal-Block both adds structure to the network and reduces its size. We build on Graphical-Lasso to add a penalty on the network's edges and limit the detection of spurious dependencies, we also propose a zero-inflated version of the model to account for real-world data properties. For the inference procedure, we propose a direct heuristic method and another more rigorous one that simultaneously infers the clustering of variables and the association network between clusters, using a penalized variational Expectation-Maximization approach. An implementation of the model in R, in a package called normalblockr, is available on github (https://github.com/jeannetous/normalblockr). We present the results in terms of clustering and network inference using both simulated data and various types of real-world data (proteomics, words occurrences on webpages, and microbiota distribution).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22467v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeanne Tous, Julien Chiquet</dc:creator>
    </item>
    <item>
      <title>Optimal treatment regimes for the net benefit of a treatment</title>
      <link>https://arxiv.org/abs/2503.22580</link>
      <description>arXiv:2503.22580v1 Announce Type: cross 
Abstract: We developed a mathematical setup inspired by Buyse's generalized pairwise comparisons to define a notion of optimal individualized treatment rule (ITR) in the presence of prioritized outcomes in a randomized controlled trial, terming such an ITR pairwise optimal. We present two approaches to estimate pairwise optimal ITRs. The first is a variant of the k-nearest neighbors algorithm. The second is a meta-learner based on a randomized bagging scheme, allowing the use of any classification algorithm for constructing an ITR. We study the behavior of these estimation schemes from a theoretical standpoint and through Monte Carlo simulations and illustrate their use on trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22580v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Petit, G\'erard Biau, Rapha\"el Porcher</dc:creator>
    </item>
    <item>
      <title>Ride-sharing Determinants: Spatial and Spatio-temporal Bayesian Analysis for Chicago Service in 2022</title>
      <link>https://arxiv.org/abs/2406.11590</link>
      <description>arXiv:2406.11590v2 Announce Type: replace 
Abstract: The rapid expansion of ride-sharing services has caused significant disruptions in the transpor-tation industry and fundamentally altered the way individuals move from one place to another. Accurate estimation of ride-sharing improves service utilization and reliability and reduces travel time and traffic congestion. In this study, we employ two Bayesian models to estimate ride-sharing demand in the 77 Chicago community areas. We consider demographic, scoio-economic, transportation factors as well as land-use characteristics as explanatory variables. Our models assume conditional autoregression (CAR) prior for the explanatory variables. Moreover, the Bayesian frameworks estimate both the unstructured random error and the struc-tured errors for the spatial and the spatiotemporal correlation. We assessed the performance of the estimated models and the residuals of the spatial regression model have no left-over spatial structure. For the spatiotemporal model, the squared correlation between actual ride-shares and the fitted values is 0.95. Our analysis revealed that the demographic factors (populations size and registered crimes) positively impact the ride-sharing demand. Additionally, the ride-sharing demand increases with higher income and increase in the economically active propor-tion of the population as well as the residents with no cars. Moreover, the transit availability and the walkability indices are crucial determinants for the ridesharing in Chicago.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11590v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Elkhouly, Taqwa Alhadidi</dc:creator>
    </item>
    <item>
      <title>Kairosis: A method for dynamical probability forecast aggregation informed by Bayesian change point detection</title>
      <link>https://arxiv.org/abs/2408.00785</link>
      <description>arXiv:2408.00785v4 Announce Type: replace 
Abstract: We present a new method, "kairosis", for aggregating probability forecasts made over a time period of a single outcome determined at the end of that period. Informed by work on Bayesian change-point detection, we begin by constructing for each time during the period a posterior probability that the forecasts before and after this time are distributed differently. The resulting posterior probability mass function is integrated to give a cumulative mass function, which is used to create a weighted median forecast. The effect is to construct an aggregate in which the most heavily weighted forecasts are those which have been made since the probable most recent change in the forecasts' distribution. Kairosis outperforms standard methods, and is especially suitable for geopolitical forecasting tournaments because it is observed to be robust across disparate questions and forecaster distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00785v4</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zane Hassoun, Ben Powell, Niall MacKay</dc:creator>
    </item>
    <item>
      <title>Gridding and Parameter Expansion for Scalable Latent Gaussian Models of Spatial Multivariate Data</title>
      <link>https://arxiv.org/abs/2101.03579</link>
      <description>arXiv:2101.03579v2 Announce Type: replace-cross 
Abstract: Scalable spatial GPs for massive datasets can be built via sparse Directed Acyclic Graphs (DAGs) where a small number of directed edges is sufficient to flexibly characterize spatial dependence. The DAG can be used to devise fast algorithms for posterior sampling of the latent process, but these may exhibit pathological behavior in estimating covariance parameters. In this article, we introduce gridding and parameter expansion methods to improve the practical performance of MCMC algorithms in terms of effective sample size per unit time (ESS/s). Gridding is a model-based strategy that reduces the number of expensive operations necessary during MCMC on irregularly spaced data. Parameter expansion reduces dependence in posterior samples in spatial regression for high resolution data. These two strategies lead to computational gains in the big data settings on which we focus. We consider popular constructions of univariate spatial processes based on Mat\'ern covariance functions and multivariate coregionalization models for Gaussian outcomes in extensive analyses of synthetic datasets comparing with alternative methods. We demonstrate effectiveness of our proposed methods in a forestry application using remotely sensed data from NASA's Goddard LiDAR, Hyper-Spectral, and Thermal imager (G-LiHT).</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.03579v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-BA1515</arxiv:DOI>
      <dc:creator>Michele Peruzzi, Sudipto Banerjee, David B. Dunson, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>Multiple Imputation of Hierarchical Nonlinear Time Series Data with an Application to School Enrollment Data</title>
      <link>https://arxiv.org/abs/2401.01872</link>
      <description>arXiv:2401.01872v2 Announce Type: replace-cross 
Abstract: International comparisons of hierarchical time series data sets based on survey data, such as annual country-level estimates of school enrollment rates, can suffer from large amounts of missing data due to differing coverage of surveys across countries and across times. A popular approach to handling missing data in these settings is through multiple imputation, which can be especially effective when there is an auxiliary variable that is strongly predictive of and has a smaller amount of missing data than the variable of interest. However, standard methods for multiple imputation of hierarchical time series data can perform poorly when the auxiliary variable and the variable of interest have a nonlinear relationship. Performance can also suffer if the multiple imputations are used to estimate an analysis model that makes different assumptions about the data compared to the imputation model, leading to uncongeniality between analysis and imputation models. We propose a Bayesian method for multiple imputation of hierarchical nonlinear time series data that uses a sequential decomposition of the joint distribution and incorporates smoothing splines to account for nonlinear relationships between variables. We compare the proposed method with existing multiple imputation methods through a simulation study and an application to secondary school enrollment data. We find that the proposed method can lead to substantial performance increases for estimation of parameters in uncongenial analysis models and for prediction of individual missing values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01872v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daphne H. Liu, Adrian E. Raftery</dc:creator>
    </item>
    <item>
      <title>Model-based Clustering of Multi-Dimensional Zero-Inflated Counts via the EM Algorithm</title>
      <link>https://arxiv.org/abs/2406.00245</link>
      <description>arXiv:2406.00245v3 Announce Type: replace-cross 
Abstract: Zero-inflated count data arise in various fields, including health, biology, economics, and the social sciences. These data are often modelled using probabilistic distributions such as zero-inflated Poisson (ZIP), zero-inflated negative binomial (ZINB), or zero-inflated binomial (ZIB). To account for heterogeneity in the data, it is often useful to cluster observations into groups that may explain underlying differences in the data-generating process. This paper focuses on model-based clustering for zero-inflated counts when observations are structured in a matrix form rather than a vector. We propose a clustering framework based on mixtures of ZIP or ZINB distributions, with both the count and zero components depending on cluster assignments. Our approach incorporates covariates through a log-linear structure for the mean parameter and includes a size factor to adjust for differences in total sampling or exposure. Model parameters and cluster assignments are estimated via the Expectation-Maximization (EM) algorithm. We assess the performance of our proposed methodology through simulation studies evaluating clustering accuracy and estimator properties, followed by applications to publicly available datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00245v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zahra AghahosseinaliShirazi, Pedro A. Rangel, Camila P. E. de Souza</dc:creator>
    </item>
    <item>
      <title>Improving probabilistic forecasts of extreme wind speeds by training statistical post-processing models with weighted scoring rules</title>
      <link>https://arxiv.org/abs/2407.15900</link>
      <description>arXiv:2407.15900v4 Announce Type: replace-cross 
Abstract: Accurate forecasts of extreme wind speeds are of high importance for many applications. Such forecasts are usually generated by ensembles of numerical weather prediction (NWP) models, which however can be biased and have errors in dispersion, thus necessitating the application of statistical post-processing techniques. In this work we aim to improve statistical post-processing models for probabilistic predictions of extreme wind speeds. We do this by adjusting the training procedure used to fit ensemble model output statistics (EMOS) models - a commonly applied post-processing technique - and propose estimating parameters using the so-called threshold-weighted continuous ranked probability score (twCRPS), a proper scoring rule that places special emphasis on predictions over a threshold. We show that training using the twCRPS leads to improved extreme event performance of post-processing models for a variety of thresholds. We find a distribution body-tail trade-off where improved performance for probabilistic predictions of extreme events comes with worse performance for predictions of the distribution body. However, we introduce strategies to mitigate this trade-off based on weighted training and linear pooling. Finally, we consider some synthetic experiments to explain the training impact of the twCRPS and derive closed-form expressions of the twCRPS for a number of distributions, giving the first such collection in the literature. The results will enable researchers and practitioners alike to improve the performance of probabilistic forecasting models for extremes and other events of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15900v4</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1175/MWR-D-24-0151.1</arxiv:DOI>
      <dc:creator>Jakob Benjamin Wessel, Christopher A. T. Ferro, Gavin R. Evans, Frank Kwasniok</dc:creator>
    </item>
    <item>
      <title>Enhancing Psychometric Analysis with Interactive ShinyItemAnalysis Modules</title>
      <link>https://arxiv.org/abs/2407.18943</link>
      <description>arXiv:2407.18943v2 Announce Type: replace-cross 
Abstract: ShinyItemAnalysis (SIA) is an R package and shiny application for an interactive presentation of psychometric methods and analysis of multi-item measurements in psychology, education, and social sciences in general. In this article, we present a new feature introduced in the recent version of the package, called "SIA modules", which allows researchers and practitioners to offer new analytical methods for broader use via add-on extensions. We describe how to build the add-on modules with the support of the new SIAtools package and demonstrate the concepts using sample modules from the newly introduced SIAmodules package. SIA modules are designed to integrate with and build upon the SIA interactive application, enabling them to leverage the existing infrastructure for tasks such as data uploading and processing. They can access a range of outputs from various analyses, including item response theory models, exploratory factor analysis, or differential item functioning models. Because SIA modules come in R packages (or extend the existing ones), they may come bundled with their datasets, use object-oriented systems, or even compiled code. We discuss the possibility of broader use of the concept of SIA modules in other areas and the importance of freely available interactive psychometric software for methods dissemination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18943v2</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patr\'icia Martinkov\'a, Jan Net\'ik, Ad\'ela Hladk\'a</dc:creator>
    </item>
    <item>
      <title>Direct measurement of darkness using a standard single-photon avalanche photodiode</title>
      <link>https://arxiv.org/abs/2410.06691</link>
      <description>arXiv:2410.06691v2 Announce Type: replace-cross 
Abstract: In experiments requiring extreme darkness, such as experiments probing the limits of human vision, assessment of the background photon flux is essential. However, direct measurement thereof with standard single photon detectors is challenged by dark counts and their fluctuations. Here we report an experiment and detailed statistical analysis of a direct measurement of darkness in a dedicated dark chamber suitable for human vision experiments, only using a standard single photon detector and a mechanical shutter. From a Bayesian analysis of $616$ h of data, we find substantial to decisive evidence for absolute darkness (depending on choice of prior distribution) based on the Savage-Dickey ratio, and a light level $&lt;0.039$ cnt/s (posterior $0.95$-highest density interval).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06691v2</guid>
      <category>quant-ph</category>
      <category>physics.data-an</category>
      <category>physics.optics</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1364/JOSAA.547686</arxiv:DOI>
      <arxiv:journal_reference>JOSA A 42:506 (2025)</arxiv:journal_reference>
      <dc:creator>T. H. A. van der Reep, D. Molenaar, W. L\"offler</dc:creator>
    </item>
    <item>
      <title>Combining BART and Principal Stratification to estimate the effect of intermediate variables on primary outcomes with application to estimating the effect of family planning on employment in Nigeria and Senegal</title>
      <link>https://arxiv.org/abs/2412.16320</link>
      <description>arXiv:2412.16320v2 Announce Type: replace-cross 
Abstract: There is interest in learning about the causal effects of family planning (FP) on empowerment-related outcomes. Data related to this question are available from studies in which FP programs increase access to FP, but such interventions do not necessarily result in uptake of FP. In addition, women impacted by such programs may differ systematically from target populations of interest in ways that alter the effect of FP. To assess the causal effect of FP on empowerment-related outcomes, we developed a 2-step approach. We use principal stratification and Bayesian Additive Regression Trees (BART) to non-parametrically estimate the effect in the source population among women affected by a FP program. We generalize the results to a broader population by taking the expectation of conditional average treatment effects from the selective sample over the covariate distribution in the target population. To estimate (uncertainty in) the covariate distribution from survey data with a complex sampling design, we use a Bayesian bootstrap (BB). We apply the approach to estimate the causal effect of modern contraceptive use on employment among urban women in Nigeria and Senegal and find strong effects and effect heterogeneity. Sensitivity analyses suggest robustness to violations of assumptions for internal and external validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16320v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Ilene Speizer, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Compositional Outcomes and Environmental Mixtures: the Dirichlet Bayesian Weighted Quantile Sum Regression</title>
      <link>https://arxiv.org/abs/2503.21428</link>
      <description>arXiv:2503.21428v2 Announce Type: replace-cross 
Abstract: Environmental mixture approaches do not accommodate compositional outcomes, consisting of vectors constrained onto the unit simplex. This limitation poses challenges in effectively evaluating the associations between multiple concurrent environmental exposures and their respective impacts on this type of outcomes. As a result, there is a pressing need for the development of analytical methods that can more accurately assess the complexity of these relationships. Here, we extend the Bayesian weighted quantile sum regression (BWQS) framework for jointly modeling compositional outcomes and environmental mixtures using a Dirichlet distribution with a multinomial logit link function. The proposed approach, named Dirichlet-BWQS (DBWQS), allows for the simultaneous estimation of mixture weights associated with each exposure mixture component as well as the association between the overall exposure mixture index and each outcome proportion. We assess the performance of DBWQS regression on extensive simulated data and a real scenario where we investigate the associations between environmental chemical mixtures and DNA methylation-derived placental cell composition, using publicly available data (GSE75248). We also compare our findings with results considering environmental mixtures and each outcome component. Finally, we developed an R package "xbwqs" where we made our proposed method publicly available (https://github.com/hasdk/xbwqs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21428v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hachem Saddiki, Joshua L. Warren, Corina Lesseur, Elena Colicino</dc:creator>
    </item>
  </channel>
</rss>
