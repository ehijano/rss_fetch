<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Aug 2025 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training</title>
      <link>https://arxiv.org/abs/2508.11035</link>
      <description>arXiv:2508.11035v1 Announce Type: new 
Abstract: Large-scale deep learning workloads increasingly suffer from I/O bottlenecks as datasets grow beyond local storage capacities and GPU compute outpaces network and disk latencies. While recent systems optimize data-loading time, they overlook the energy cost of I/O - a critical factor at large scale. We introduce EMLIO, an Efficient Machine Learning I/O service that jointly minimizes end-to-end data-loading latency T and I/O energy consumption E across variable-latency networked storage. EMLIO deploys a lightweight data-serving daemon on storage nodes that serializes and batches raw samples, streams them over TCP with out-of-order prefetching, and integrates seamlessly with GPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive evaluations over local disk, LAN (0.05 ms &amp; 10 ms RTT), and WAN (30 ms RTT) environments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use compared to state-of-the-art loaders, while maintaining constant performance and energy profiles irrespective of network distance. EMLIO's service-based architecture offers a scalable blueprint for energy-aware I/O in next-generation AI clouds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11035v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasibul Jamil, MD S Q Zulkar Nine, Tevfik Kosar</dc:creator>
    </item>
    <item>
      <title>Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets</title>
      <link>https://arxiv.org/abs/2508.11266</link>
      <description>arXiv:2508.11266v1 Announce Type: new 
Abstract: Alternative assets such as mines, power plants, or infrastructure projects are often large, heterogeneous bundles of resources, rights, and outputs whose value is difficult to trade or fractionalize under traditional frameworks. This paper proposes a novel two-tier tokenization architecture to enhance the liquidity and transparency of such complex assets. We introduce the concepts of Element Tokens and Everything Tokens: elemental tokens represent standardized, fully collateralized components of an asset (e.g., outputs, rights, or credits), while an everything token represents the entire asset as a fixed combination of those elements. The architecture enables both fine-grained partial ownership and integrated whole-asset ownership through a system of two-way convertibility. We detail the design and mechanics of this system, including an arbitrage mechanism that keeps the price of the composite token aligned with the net asset value of its constituents. Through illustrative examples in the energy and industrial sectors, we demonstrate that our approach allows previously illiquid, high-value projects to be fractionalized and traded akin to stocks or exchange-traded funds (ETFs). We discuss the benefits for investors and asset owners, such as lower entry barriers, improved price discovery, and flexible financing, as well as the considerations for implementation and regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11266v1</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ailiya Borjigin, Cong He, Charles CC Lee, Wei Zhou</dc:creator>
    </item>
    <item>
      <title>Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive</title>
      <link>https://arxiv.org/abs/2508.11298</link>
      <description>arXiv:2508.11298v1 Announce Type: new 
Abstract: The ever-increasing compute performance of GPU accelerators drives up the need for efficient data movements within HPC applications to sustain performance. Proposed as a solution to alleviate CPU-GPU data movement, AMD MI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth memory (HBM) within a single physical package. Leadership supercomputers, such as El Capitan, group four APUs within a single compute node, using Infinity Fabric Interconnect. In this work, we design specific benchmarks to evaluate direct memory access from the GPU, explicit inter-APU data movement, and collective multi-APU communication. We also compare the efficiency of HIP APIs, MPI routines, and the GPU-specialized RCCL library. Our results highlight key design choices for optimizing inter-APU communication on multi-APU AMD MI300A systems with Infinity Fabric, including programming interfaces, allocators, and data movement. Finally, we optimize two real HPC applications, Quicksilver and CloverLeaf, and evaluate them on a four MI100A APU system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11298v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabin Schieffer, Jacob Wahlgren, Ruimin Shi, Edgar A. Le\'on, Roger Pearce, Maya Gokhale, Ivy Peng</dc:creator>
    </item>
    <item>
      <title>Space-efficient population protocols for exact majority in general graphs</title>
      <link>https://arxiv.org/abs/2508.11384</link>
      <description>arXiv:2508.11384v1 Announce Type: new 
Abstract: We study exact majority consensus in the population protocol model. In this model, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in each time step, a scheduler samples uniformly at random a pair of adjacent nodes to interact. In the exact majority consensus task, each node is given a binary input, and the goal is to design a protocol that almost surely reaches a stable configuration, where all nodes output the majority input value.
  We give improved upper and lower bounds for the exact majority in general graphs. First, we give asymptotically tight time lower bounds for general (unbounded space) protocols. Second, we obtain new upper bounds parameterized by the relaxation time $\tau_{\mathsf{rel}}$ of the random walk on $G$ induced by the scheduler and the degree imbalance $\Delta/\delta$ of $G$. Specifically, we give a protocol that stabilizes in $O\left( \tfrac{\Delta}{\delta} \tau_{\mathsf{rel}} \log^2 n \right)$ steps in expectation and with high probability and uses $O\left( \log n \cdot \left( \log\left(\tfrac{\Delta}{\delta}\right) + \log \left(\tfrac{\tau_{\mathsf{rel}}}{n}\right) \right) \right)$ states in any graph with minimum degree at least $\delta$ and maximum degree at most $\Delta$.
  For regular expander graphs, this matches the optimal space complexity of $\Theta(\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA 2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of $O(n \log^2 n)$ steps. Finally, we give a new upper bound of $O(\tau_{\mathsf{rel}} \cdot n \log n)$ for the stabilization time of a constant-state protocol.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11384v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joel Rybicki, Jakob Solnerzik, Olivier Stietel, Robin Vacus</dc:creator>
    </item>
    <item>
      <title>Time, Fences and the Ordering of Events in TSO</title>
      <link>https://arxiv.org/abs/2508.11415</link>
      <description>arXiv:2508.11415v1 Announce Type: new 
Abstract: The Total Store Order (TSO) is arguably the most widely used relaxed memory model in multiprocessor architectures, widely implemented, for example in Intel's x86 and x64 platforms. It allows processes to delay the visibility of writes through store buffering. While this supports hardware-level optimizations and makes a significant contribution to multiprocessor efficiency, it complicates reasoning about correctness, as executions may violate sequential consistency. Ensuring correct behavior often requires inserting synchronization primitives such as memory fences ($F$) or atomic read-modify-write ($RMW$) operations, but this approach can incur significant performance costs. In this work, we develop a semantic framework that precisely characterizes when such synchronization is necessary under TSO. We introduce a novel TSO-specific occurs-before relation, which adapts Lamport's celebrated happens-before relation from asynchronous message-passing systems to the TSO setting. Our main result is a theorem that proves that the only way to ensure that two events that take place at different sites are temporally ordered is by having the execution create an occurs-before chain between the events. By studying the role of fences and $RMW$s in creating occurs-before chains, we are then able to capture cases in which these costly synchronization operations are unavoidable. Since proper real-time ordering of events is a fundamental aspect of consistency conditions such as Linearizability, our analysis provides a sound theoretical understanding of essential aspects of the TSO model. In particular, we are able to generalize prior lower bounds for linearizable implementations of shared memory objects. Our results capture the structure of information flow and causality in the TSO model by extending the standard communication-based reasoning from asynchronous systems to the TSO memory model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11415v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ra\"issa Nataf, Yoram Moses</dc:creator>
    </item>
    <item>
      <title>Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method</title>
      <link>https://arxiv.org/abs/2508.11467</link>
      <description>arXiv:2508.11467v1 Announce Type: new 
Abstract: Singular Value Decomposition (SVD) is a fundamental matrix factorization technique in linear algebra, widely applied in numerous matrix-related problems. However, traditional SVD approaches are hindered by slow panel factorization and frequent CPU-GPU data transfers in heterogeneous systems, despite advancements in GPU computational capabilities. In this paper, we introduce a GPU-centered SVD algorithm, incorporating a novel GPU-based bidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and data layout of different steps for SVD computation, performing all panel-level computations and trailing matrix updates entirely on GPU to eliminate CPU-GPU data transfers. Furthermore, we integrate related computations to optimize BLAS utilization, thereby increasing arithmetic intensity and fully leveraging the computational capabilities of GPUs. Additionally, we introduce a newly developed GPU-based BDC algorithm that restructures the workflow to eliminate matrix-level CPU-GPU data transfers and enable asynchronous execution between the CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs demonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x and 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11467v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shifang Liu, Huiyuan Li, Hongjiao Sheng, Haoyuan Gui, Xiaoyu Zhang</dc:creator>
    </item>
    <item>
      <title>The Price of Cheaper Data: Measuring the Strategic Inefficiencies in the Post-EIP-4844 Ethereum Market</title>
      <link>https://arxiv.org/abs/2411.03892</link>
      <description>arXiv:2411.03892v3 Announce Type: replace 
Abstract: High transaction fees on Ethereum have long hindered its scalability and user adoption. The recent Ethereum EIP-4844 upgrade aims to alleviate the scalability issue by introducing the blob, a new data structure for Layer-2 rollups. Instead of using expensive blockchain storage, blobs provide a cheaper, separate data layer with its own fee market, which drastically lowers data availability costs. This change, while lowering transaction costs, has created a new high-stakes economic game for block builders and rollups. However, the dynamics of this game remain poorly understood. In this paper, we conduct the first large-scale empirical analysis of the post-EIP-4844 ecosystem, leveraging a dataset of 319.5 million transactions, including 1.3 million blob-carrying type-3 transactions. Our analysis demonstrates clear evidence of a structural shift towards the utilization of cheap blobs over expensive transactions for Layer-2 data posting: while average block size grew 2.5 times, the space consumed by expensive transactions in the public mempool shrank from over 150 KB to just 30 KB. Yet, this scalability success masks widespread economic inefficiencies on both sides of the market. On the builder side, 29.48% of blocks containing blobs are constructed sub-optimally, earning less revenue than that could be achieved otherwise. On the rollup side, we identify that flawed submission strategies have led to $186.92$ ETH in direct losses and average inclusion delays of over 19 seconds. Moving beyond characterization, our work offers actionable solutions. For builders, we develop an optimal pricing model derived from a formal first-price auction framework, allowing builders to make provably profitable inclusion decisions. For rollups, we prove that batching multiple blobs into a single transaction is a dominant, utility-maximizing strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03892v3</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue Huang, Shuzheng Wang, Liang Du, Chuxuan Zeng, Ling Deng, Yuming Huang, Gareth Tyson, Jing Tang</dc:creator>
    </item>
    <item>
      <title>New Improvements in Solving Large LABS Instances Using Massively Parallelizable Memetic Tabu Search</title>
      <link>https://arxiv.org/abs/2504.00987</link>
      <description>arXiv:2504.00987v2 Announce Type: replace 
Abstract: Low Autocorrelation Binary Sequences (LABS) is a particularly challenging binary optimization problem which quickly becomes intractable in finding the global optimum for problem sizes beyond 66. This aspect makes LABS appealing to use as a test-bed for meta-heuristic optimization solvers to target large problem sizes. In this work, we introduce a massively parallelized implementation of the memetic tabu search algorithm to tackle LABS problem for sizes up to 120. By effectively combining the block level and thread level parallelism framework within a single Nvidia-A100 GPU, and creating hyper optimized binary-valued data structures for shared memory among the blocks, we showcase up to 26 fold speedup compared to the analogous 16-core CPU implementation. Our implementation has also enabled us to find new LABS merit factor values for sixteen different problem sizes between 92 and 120. Crucially, we also showcase improved values for five odd-sized problems {99, 107, 109, 113, 119} whose previous best known results coincided with the provably optimal skew-symmetric search sequences. Consequently, our result highlights the importance of a focus on general-purpose solver to tackle LABS, since leveraging its skew-symmetry could lead to sub-optimal solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00987v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiwei Zhang, Jiayu Shen, Niraj Kumar, Marco Pistoia</dc:creator>
    </item>
    <item>
      <title>Fully-Distributed Construction of Byzantine-Resilient Dynamic Peer-to-Peer Networks</title>
      <link>https://arxiv.org/abs/2506.04368</link>
      <description>arXiv:2506.04368v2 Announce Type: replace 
Abstract: We address a fundamental problem in Peer-to-Peer (P2P) networks, namely, constructing and maintaining dynamic P2P overlay network topologies with essential properties such as connectivity, low diameter, and high expansion, that are resilient to continuous high churn and the presence of a large number of malicious (Byzantine) nodes. Our main goal is to construct and maintain a sparse (bounded degree) expander topology despite high churn and a large number of Byzantine nodes. Such an expander topology has logarithmic diameter, high expansion, and is robust to churn and the presence of a large number of bad nodes, and facilitates efficient and robust algorithms for fundamental problems in distributed computing, such as agreement, broadcasting, routing, etc.
  Our main contribution is a randomized, fully-distributed dynamic P2P protocol that works with only local initial knowledge and guarantees, with a high probability, the maintenance of a constant degree graph with high expansion even under continuous churn and in the presence of a large number of Byzantine nodes. Our protocol can tolerate up to $o(n/poly\log(n))$ Byzantine nodes (where $n$ is the stable network size). Our protocol is efficient, lightweight, and scalable, and it incurs only $O(poly\log(n))$ overhead for topology maintenance: only polylogarithmic (in $n$) bits need to be processed and sent by each honest node per round, and any honest node's computation cost per round is also polylogarithmic.
  Our protocol can be used as a building block for solving fundamental distributed computing problems in highly dynamic networks, such as Byzantine agreement and Byzantine leader election, and enables fast and scalable algorithms for these problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04368v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aayush Gupta, Gopal Pandurangan</dc:creator>
    </item>
    <item>
      <title>ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism</title>
      <link>https://arxiv.org/abs/2507.10069</link>
      <description>arXiv:2507.10069v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10069v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zedong Liu, Shenggan Cheng, Guangming Tan, Yang You, Dingwen Tao</dc:creator>
    </item>
    <item>
      <title>Coordinated Power Management on Heterogeneous Systems</title>
      <link>https://arxiv.org/abs/2508.07605</link>
      <description>arXiv:2508.07605v2 Announce Type: replace 
Abstract: Performance prediction is essential for energy-efficient computing in heterogeneous computing systems that integrate CPUs and GPUs. However, traditional performance modeling methods often rely on exhaustive offline profiling, which becomes impractical due to the large setting space and the high cost of profiling large-scale applications. In this paper, we present OPEN, a framework consists of offline and online phases. The offline phase involves building a performance predictor and constructing an initial dense matrix. In the online phase, OPEN performs lightweight online profiling, and leverages the performance predictor with collaborative filtering to make performance prediction. We evaluate OPEN on multiple heterogeneous systems, including those equipped with A100 and A30 GPUs. Results show that OPEN achieves prediction accuracy up to 98.29\%. This demonstrates that OPEN effectively reduces profiling cost while maintaining high accuracy, making it practical for power-aware performance modeling in modern HPC environments. Overall, OPEN provides a lightweight solution for performance prediction under power constraints, enabling better runtime decisions in power-aware computing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07605v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhong Zheng, Zhiling Lan, Xingfu Wu, Valerie E. Taylor, Michael E. Papka</dc:creator>
    </item>
    <item>
      <title>Distributed quantum computing with black-box subroutines</title>
      <link>https://arxiv.org/abs/2505.14519</link>
      <description>arXiv:2505.14519v2 Announce Type: replace-cross 
Abstract: In this work, we propose a general protocol for distributed quantum computing that accommodates arbitrary unknown subroutines. It can be applied to scale up quantum computing through multi-chip interconnection, as well as to tasks such as estimating unknown parameters or processes for circuit depth reduction and constructing secure quantum cryptographic protocols. Our protocol builds upon a few techniques we develop, such as the oblivious quantum teleportation and control, which can circumvent quantum no-go theorems on the manipulation of unknown objects. Furthermore, we demonstrate that this protocol can be physically implemented using currently available quantum computing platforms. These results suggest that our framework could provide a foundation for developing more advanced quantum algorithms and protocols in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14519v2</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1088/2058-9565/adf6d5</arxiv:DOI>
      <arxiv:journal_reference>Quantum Sci. Technol. 10 045014 (2025)</arxiv:journal_reference>
      <dc:creator>X. Xu, Y. -D. Liu, S. Shi, Y. -J. Wang, D. -S. Wang</dc:creator>
    </item>
    <item>
      <title>DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning</title>
      <link>https://arxiv.org/abs/2508.06972</link>
      <description>arXiv:2508.06972v2 Announce Type: replace-cross 
Abstract: DSperse is a modular framework for distributed machine learning inference with strategic cryptographic verification. Operating within the emerging paradigm of distributed zero-knowledge machine learning, DSperse avoids the high cost and rigidity of full-model circuitization by enabling targeted verification of strategically chosen subcomputations. These verifiable segments, or "slices", may cover part or all of the inference pipeline, with global consistency enforced through audit, replication, or economic incentives. This architecture supports a pragmatic form of trust minimization, localizing zero-knowledge proofs to the components where they provide the greatest value. We evaluate DSperse using multiple proving systems and report empirical results on memory usage, runtime, and circuit behavior under sliced and unsliced configurations. By allowing proof boundaries to align flexibly with the model's logical structure, DSperse supports scalable, targeted verification strategies suited to diverse deployment needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06972v2</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dan Ivanov, Tristan Freiberg, Shirin Shahabi, Jonathan Gold, Haruna Isah</dc:creator>
    </item>
  </channel>
</rss>
