<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 May 2025 04:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>CGReplay: Capture and Replay of Cloud Gaming Traffic for QoE/QoS Assessment</title>
      <link>https://arxiv.org/abs/2505.11973</link>
      <description>arXiv:2505.11973v1 Announce Type: new 
Abstract: Cloud Gaming (CG) research faces challenges due to the unpredictability of game engines and restricted access to commercial platforms and their logs. This creates major obstacles to conducting fair experimentation and evaluation. CGReplay captures and replays player commands and the corresponding video frames in an ordered and synchronized action-reaction loop, ensuring reproducibility. It enables Quality of Experience/Service (QoE/QoS) assessment under varying network conditions and serves as a foundation for broader CG research. The code is publicly available for further development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11973v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Shirmarz, Ariel G. de Castro, Fabio L. Verdi, Christian E. Rothenberg</dc:creator>
    </item>
    <item>
      <title>Task Scheduling in Space-Air-Ground Uniformly Integrated Networks with Ripple Effects</title>
      <link>https://arxiv.org/abs/2505.11974</link>
      <description>arXiv:2505.11974v1 Announce Type: new 
Abstract: Space-air-ground uniformly integrated network (SAGUIN), which integrates the satellite, aerial, and terrestrial networks into a unified communication architecture, is a promising candidate technology for the next-generation wireless systems. Transmitting on the same frequency band, higher-layer access points (AP), e.g., satellites, provide extensive coverage; meanwhile, it may introduce significant signal propagation delays due to the relatively long distances to the ground users, which can be multiple times longer than the packet durations in task-oriented communications. This phenomena is modeled as a new ``ripple effect'', which introduces spatiotemporally correlated interferences in SAGUIN. This paper studies the task scheduling problem in SAGUIN with ripple effect, and formulates it as a Markov decision process (MDP) to jointly minimize the age of information (AoI) at users and energy consumption at APs. The obtained MDP is challenging due to high dimensionality, partial observations, and dynamic resource constraints caused by ripple effect. To address the challenges of high dimensionality, we reformulate the original problem as a Markov game, where the complexities are managed through interactive decision-making among APs. Meanwhile, to tackle partial observations and the dynamic resource constraints, we adopt a modified multi-agent proximal policy optimization (MAPPO) algorithm, where the actor network filters out irrelevant input states based on AP coverage and its dimensionality can be reduced by more than an order of magnitude. Simulation results reveal that the proposed approach outperforms the benchmarks, significantly reducing users' AoI and APs' energy consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11974v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuan Huang, Ran Li, Jiachen Wang</dc:creator>
    </item>
    <item>
      <title>LLM-guided DRL for Multi-tier LEO Satellite Networks with Hybrid FSO/RF Links</title>
      <link>https://arxiv.org/abs/2505.11978</link>
      <description>arXiv:2505.11978v1 Announce Type: new 
Abstract: Despite significant advancements in terrestrial networks, inherent limitations persist in providing reliable coverage to remote areas and maintaining resilience during natural disasters. Multi-tier networks with low Earth orbit (LEO) satellites and high-altitude platforms (HAPs) offer promising solutions, but face challenges from high mobility and dynamic channel conditions that cause unstable connections and frequent handovers. In this paper, we design a three-tier network architecture that integrates LEO satellites, HAPs, and ground terminals with hybrid free-space optical (FSO) and radio frequency (RF) links to maximize coverage while maintaining connectivity reliability. This hybrid approach leverages the high bandwidth of FSO for satellite-to-HAP links and the weather resilience of RF for HAP-to-ground links. We formulate a joint optimization problem to simultaneously balance downlink transmission rate and handover frequency by optimizing network configuration and satellite handover decisions. The problem is highly dynamic and non-convex with time-coupled constraints. To address these challenges, we propose a novel large language model (LLM)-guided truncated quantile critics algorithm with dynamic action masking (LTQC-DAM) that utilizes dynamic action masking to eliminate unnecessary exploration and employs LLMs to adaptively tune hyperparameters. Simulation results demonstrate that the proposed LTQC-DAM algorithm outperforms baseline algorithms in terms of convergence, downlink transmission rate, and handover frequency. We also reveal that compared to other state-of-the-art LLMs, DeepSeek delivers the best performance through gradual, contextually-aware parameter adjustments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11978v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahui Li, Geng Sun, Zemin Sun, Jiacheng Wang, Yinqiu Liu, Ruichen Zhang, Dusit Niyato, Shiwen Mao</dc:creator>
    </item>
    <item>
      <title>Discrete Time Credit-Based Shaping for Time-Sensitive Applications in 5G/6G Networks</title>
      <link>https://arxiv.org/abs/2505.12091</link>
      <description>arXiv:2505.12091v1 Announce Type: new 
Abstract: Future wireless networks must deliver deterministic end-to-end delays for workloads such as smart-factory control loops. On Ethernet these guarantees are delivered by the set of tools within IEEE 802.1 time sensitive networking~(TSN) standards. Credit-based shaper (CBS) is one such tool which enforces bounded latency. Directly porting CBS to 5G/6G New Radio (NR) is non-trivial because NR schedules traffic in discrete-time, modulation-dependent resource allocation, whereas CBS assumes a continuous, fixed-rate link. Existing TSN-over-5G translators map Ethernet priorities to 5G quality of service (QoS) identifiers but leave the radio scheduler unchanged, so deterministic delay is lost within the radio access network (RAN). To address this challenge, we propose a novel slot-native approach that adapts CBS to operate natively in discrete NR slots. We first propose a per-slot credit formulation for each user-equipment ({UE}) queue that debits credit by the granted transport block size~(TBS); we call this discrete-time CBS (CBS-DT). Recognizing that debiting the full {TBS} can unduly penalize transmissions that actually use only part of their grant, we then introduce and analyze {CBS} with Partial Usage ({CBS-PU}). {CBS-PU} scales the credit debit in proportion to the actual bytes dequeued from the downlink queue. The resulting CBS-PU algorithm is shown to maintain bounded credit, preserve long-term rate reservations, and guarantees worst-case delay performance no worse than {CBS-DT}. Simulation results show that slot-level credit gating--particularly CBS-PU--enables NR to export TSN class QoS while maximizing resource utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12091v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anudeep Karnam, Kishor C. Joshi, Jobish John, George Exarchakos, Sonia Heemstra de Groot, Ignas Niemegeers</dc:creator>
    </item>
    <item>
      <title>Towards Sustainability in 6G Network Slicing with Energy-Saving and Optimization Methods</title>
      <link>https://arxiv.org/abs/2505.12132</link>
      <description>arXiv:2505.12132v1 Announce Type: new 
Abstract: The 6G mobile network is the next evolutionary step after 5G, with a prediction of an explosive surge in mobile traffic. It provides ultra-low latency, higher data rates, high device density, and ubiquitous coverage, positively impacting services in various areas. Energy saving is a major concern for new systems in the telecommunications sector because all players are expected to reduce their carbon footprints to contribute to mitigating climate change. Network slicing is a fundamental enabler for 6G/5G mobile networks and various other new systems, such as the Internet of Things (IoT), Internet of Vehicles (IoV), and Industrial IoT (IIoT). However, energy-saving methods embedded in network slicing architectures are still a research gap. This paper discusses how to embed energy-saving methods in network-slicing architectures that are a fundamental enabler for nearly all new innovative systems being deployed worldwide. This paper's main contribution is a proposal to save energy in network slicing. That is achieved by deploying ML-native agents in NS architectures to dynamically orchestrate and optimize resources based on user demands. The SFI2 network slicing reference architecture is the concrete use case scenario in which contrastive learning improves energy saving for resource allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12132v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.15449843</arxiv:DOI>
      <dc:creator>Rodrigo Moreira, Tereza C. M. Carvalho, Fl\'avio de Oliveira Silva, Nazim Agoulmine, Joberto S. B. Martins</dc:creator>
    </item>
    <item>
      <title>LAMeTA: Intent-Aware Agentic Network Optimization via a Large AI Model-Empowered Two-Stage Approach</title>
      <link>https://arxiv.org/abs/2505.12247</link>
      <description>arXiv:2505.12247v1 Announce Type: new 
Abstract: Nowadays, Generative AI (GenAI) reshapes numerous domains by enabling machines to create content across modalities. As GenAI evolves into autonomous agents capable of reasoning, collaboration, and interaction, they are increasingly deployed on network infrastructures to serve humans automatically. This emerging paradigm, known as the agentic network, presents new optimization challenges due to the demand to incorporate subjective intents of human users expressed in natural language. Traditional generic Deep Reinforcement Learning (DRL) struggles to capture intent semantics and adjust policies dynamically, thus leading to suboptimality. In this paper, we present LAMeTA, a Large AI Model (LAM)-empowered Two-stage Approach for intent-aware agentic network optimization. First, we propose Intent-oriented Knowledge Distillation (IoKD), which efficiently distills intent-understanding capabilities from resource-intensive LAMs to lightweight edge LAMs (E-LAMs) to serve end users. Second, we develop Symbiotic Reinforcement Learning (SRL), integrating E-LAMs with a policy-based DRL framework. In SRL, E-LAMs translate natural language user intents into structured preference vectors that guide both state representation and reward design. The DRL, in turn, optimizes the generative service function chain composition and E-LAM selection based on real-time network conditions, thus optimizing the subjective Quality-of-Experience (QoE). Extensive experiments conducted in an agentic network with 81 agents demonstrate that IoKD reduces mean squared error in intent prediction by up to 22.5%, while SRL outperforms conventional generic DRL by up to 23.5% in maximizing intent-aware QoE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12247v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinqiu Liu, Guangyuan Liu, Jiacheng Wang, Ruichen Zhang, Dusit Niyato, Geng Sun, Zehui Xiong, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Modeling and Performance Analysis of IoT-over-LEO Satellite Systems under Realistic Operational Constraints: A Stochastic Geometry Approach</title>
      <link>https://arxiv.org/abs/2505.12336</link>
      <description>arXiv:2505.12336v1 Announce Type: new 
Abstract: Current theoretical studies on IoT-over-LEO satellite systems often rely on unrealistic assumptions, such as infinite terrestrial areas and omnidirectional satellite coverage, leaving significant gaps in theoretical analysis for more realistic operational constraints. These constraints involve finite terrestrial area, limited satellite coverage, Earth curvature effect, integral uplink and downlink analysis, and link-dependent interference. To address these gaps, this paper proposes a novel stochastic geometry based model to rigorously analyze the performance of IoT-over-LEO satellite systems. By adopting a binomial point process (BPP) instead of the conventional Poisson point process (PPP), our model accurately characterizes the geographical distribution of a fixed number of IoT devices in a finite terrestrial region. This modeling framework enables the derivation of distance distribution functions for both the links from the terrestrial IoT devices to the satellites (T-S) and from the satellites to the Earth station (S-ES), while also accounting for limited satellite coverage and Earth curvature effects. To realistically represent channel conditions, the Nakagami fading model is employed for the T-S links to characterize diverse small-scale fading environments, while the shadowed-Rician fading model is used for the S-ES links to capture the combined effects of shadowing and dominant line-of-sight paths. Furthermore, the analysis incorporates uplink and downlink interference, ensuring a comprehensive evaluation of system performance. The accuracy and effectiveness of our theoretical framework are validated through extensive Monte Carlo simulations. These results provide insights into key performance metrics, such as coverage probability and average ergodic rate, for both individual links and the overall system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12336v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2025.3570843</arxiv:DOI>
      <dc:creator>Wen-Yu Dong, Shaoshi Yang, Ping Zhang, Sheng Chen</dc:creator>
    </item>
    <item>
      <title>Unleashing Automated Congestion Control Customization in the Wild</title>
      <link>https://arxiv.org/abs/2505.12492</link>
      <description>arXiv:2505.12492v1 Announce Type: new 
Abstract: Congestion control (CC) crucially impacts user experience across Internet services like streaming, gaming, AR/VR, and connected cars. Traditionally, CC algorithm design seeks universal control rules that yield high performance across diverse application domains and networks. However, varying service needs and network conditions challenge this approach. We share operational experience with a system that automatically customizes congestion control logic to service needs and network conditions. We discuss design, deployment challenges, and solutions, highlighting performance benefits through case studies in streaming, gaming, connected cars, and more.
  Our system leverages PCC Vivace, an online-learning based congestion control protocol developed by researchers. Hence, along with insights from customizing congestion control, we also discuss lessons learned and modifications made to adapt PCC Vivace for real-world deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12492v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amit Cohen, Lev Gloukhenki, Ravid Hadar, Eden Itah, Yehuda Shvut, Michael Schapira</dc:creator>
    </item>
    <item>
      <title>Forewarned is Forearmed: A Survey on Large Language Model-based Agents in Autonomous Cyberattacks</title>
      <link>https://arxiv.org/abs/2505.12786</link>
      <description>arXiv:2505.12786v1 Announce Type: new 
Abstract: With the continuous evolution of Large Language Models (LLMs), LLM-based agents have advanced beyond passive chatbots to become autonomous cyber entities capable of performing complex tasks, including web browsing, malicious code and deceptive content generation, and decision-making. By significantly reducing the time, expertise, and resources, AI-assisted cyberattacks orchestrated by LLM-based agents have led to a phenomenon termed Cyber Threat Inflation, characterized by a significant reduction in attack costs and a tremendous increase in attack scale. To provide actionable defensive insights, in this survey, we focus on the potential cyber threats posed by LLM-based agents across diverse network systems. Firstly, we present the capabilities of LLM-based cyberattack agents, which include executing autonomous attack strategies, comprising scouting, memory, reasoning, and action, and facilitating collaborative operations with other agents or human operators. Building on these capabilities, we examine common cyberattacks initiated by LLM-based agents and compare their effectiveness across different types of networks, including static, mobile, and infrastructure-free paradigms. Moreover, we analyze threat bottlenecks of LLM-based agents across different network infrastructures and review their defense methods. Due to operational imbalances, existing defense methods are inadequate against autonomous cyberattacks. Finally, we outline future research directions and potential defensive strategies for legacy network systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12786v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minrui Xu (Sherman), Jiani Fan (Sherman), Xinyu Huang (Sherman), Conghao Zhou (Sherman), Jiawen Kang (Sherman), Dusit Niyato (Sherman), Shiwen Mao (Sherman), Zhu Han (Sherman),  Xuemin (Sherman),  Shen, Kwok-Yan Lam</dc:creator>
    </item>
    <item>
      <title>Learning Driven Elastic Task Multi-Connectivity Immersive Computing Systems</title>
      <link>https://arxiv.org/abs/2505.13331</link>
      <description>arXiv:2505.13331v1 Announce Type: new 
Abstract: In virtual reality (VR) environments, computational tasks exhibit an elastic nature, meaning they can dynamically adjust based on various user and system constraints. This elasticity is essential for maintaining immersive experiences; however, it also introduces challenges for communication and computing in VR systems. In this paper, we investigate elastic task offloading for multi-user edge-computing-enabled VR systems with multi-connectivity, aiming to maximize the computational energy-efficiency (computational throughput per unit of energy consumed). To balance the induced communication, computation, energy consumption, and quality of experience trade-offs due to the elasticity of VR tasks, we formulate a constrained stochastic computational energy-efficiency optimization problem that integrates the multi-connectivity/multi-user action space and the elastic nature of VR computational tasks. We formulate a centralized phasic policy gradient (CPPG) framework to solve the problem of interest online, using only prior elastic task offloading statistics (energy consumption, response time, and transmission time), and task information (i.e., task size and computational intensity), while observing the induced system performance (energy consumption and latency). We further extend our approach to decentralized learning by formulating an independent phasic policy gradient (IPPG) method and a decentralized shared multi-armed bandit (DSMAB) method. We train our methods with real-world 4G, 5G, and WiGig network traces and 360 video datasets to evaluate their performance in terms of response time, energy efficiency, scalability, and delivered quality of experience. We also provide a comprehensive analysis of task size and its effect on offloading policy and system performance. In particular, we show that CPPG reduces latency by 28% and energy consumption by 78% compared to IPPG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13331v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>cs.MM</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Babak Badnava, Jacob Chakareski, Morteza Hashemi</dc:creator>
    </item>
    <item>
      <title>Communication-Efficient Hybrid Language Model via Uncertainty-Aware Opportunistic and Compressed Transmission</title>
      <link>https://arxiv.org/abs/2505.11788</link>
      <description>arXiv:2505.11788v1 Announce Type: cross 
Abstract: To support emerging language-based applications using dispersed and heterogeneous computing resources, the hybrid language model (HLM) offers a promising architecture, where an on-device small language model (SLM) generates draft tokens that are validated and corrected by a remote large language model (LLM). However, the original HLM suffers from substantial communication overhead, as the LLM requires the SLM to upload the full vocabulary distribution for each token. Moreover, both communication and computation resources are wasted when the LLM validates tokens that are highly likely to be accepted. To overcome these limitations, we propose communication-efficient and uncertainty-aware HLM (CU-HLM). In CU-HLM, the SLM transmits truncated vocabulary distributions only when its output uncertainty is high. We validate the feasibility of this opportunistic transmission by discovering a strong correlation between SLM's uncertainty and LLM's rejection probability. Furthermore, we theoretically derive optimal uncertainty thresholds and optimal vocabulary truncation strategies. Simulation results show that, compared to standard HLM, CU-HLM achieves up to 206$\times$ higher token throughput by skipping 74.8% transmissions with 97.4% vocabulary compression, while maintaining 97.4% accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11788v1</guid>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungeun Oh, Jinhyuk Kim, Jihong Park, Seung-Woo Ko, Jinho Choi, Tony Q. S. Quek, Seong-Lyun Kim</dc:creator>
    </item>
    <item>
      <title>Synapse: Virtualizing Match Tables in Programmable Hardware</title>
      <link>https://arxiv.org/abs/2505.12036</link>
      <description>arXiv:2505.12036v1 Announce Type: cross 
Abstract: Efficient network packet processing increasingly demands dynamic, adaptive, and run-time resizable match table allocation to handle the diverse and heterogeneous nature of traffic patterns and rule sets. Achieving this flexibility at high performance in hardware is challenging, as fixed resource constraints and architectural limitations have traditionally restricted such adaptability. In this paper, we introduce Synapse, an extension to programmable data plane architectures that incorporates the Virtual Matching Table (VMT) framework, drawing inspiration from virtual memory systems in Operating Systems (OSs), but specifically tailored to network processing. This abstraction layer allows logical tables to be elastic, enabling dynamic and efficient match table allocation at runtime. Our design features a hybrid memory system, leveraging on-chip associative memories for fast matching of the most popular rules and off-chip addressable memory for scalable and cost-effective storage. Furthermore, by employing a sharding mechanism across physical match tables, Synapse ensures that the power required per key match remains bounded and proportional to the key distribution and the size of the involved shard. To address the challenge of dynamic allocation, we formulate and solve an optimization problem that dynamically allocates physical match tables to logical tables based on pipeline usage and traffic characteristics at the millisecond scale. We prototype our design on FPGA and develop a simulator to evaluate the performance, demonstrating its effectiveness and scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12036v1</guid>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seyyidahmed Lahmer, Angelo Tulumello, Alessandro Rivitti, Giuseppe Bianchi, Andrea Zanella</dc:creator>
    </item>
    <item>
      <title>Scalable Time-Tagged Data Acquisition for Entanglement Distribution in Quantum Networks</title>
      <link>https://arxiv.org/abs/2505.12102</link>
      <description>arXiv:2505.12102v1 Announce Type: cross 
Abstract: In distributed quantum applications such as entanglement distribution, precise time synchronization and efficient time-tagged data handling are essential. Traditional systems often suffer from overflow, synchronization drift, and storage inefficiencies. We propose a modular Time Tagging (TT) agent that uses a 1 pulse per second (PPS) signal from White Rabbit (WR) devices to achieve network-wide synchronization, while applying real-time calibration, overflow mitigation, and compression. A live two-lab entanglement distribution experiment validated the system's performance, achieving synchronized coincidence detection at 25,000 counts/sec.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12102v1</guid>
      <category>cs.SE</category>
      <category>cs.IR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abderrahim Amlou, Thomas Gerrits, Anouar Rahmouni, Amar Abane, Mheni Merzouki, Ya-Shian Li-Baboud, Ahmed Lbath, Abdella Battou, Oliver Slattery</dc:creator>
    </item>
    <item>
      <title>An Automated Blackbox Noncompliance Checker for QUIC Server Implementations</title>
      <link>https://arxiv.org/abs/2505.12690</link>
      <description>arXiv:2505.12690v1 Announce Type: cross 
Abstract: We develop QUICtester, an automated approach for uncovering non-compliant behaviors in the ratified QUIC protocol implementations (RFC 9000/9001). QUICtester leverages active automata learning to abstract the behavior of a QUIC implementation into a finite state machine (FSM) representation. Unlike prior noncompliance checking methods, to help uncover state dependencies on event timing, QUICtester introduces the idea of state learning with event timing variations, adopting both valid and invalid input configurations, and combinations of security and transport layer parameters during learning. We use pairwise differential analysis of learned behaviour models of tested QUIC implementations to identify non-compliance instances as behaviour deviations in a property-agnostic way. This exploits the existence of the many different QUIC implementations, removing the need for validated, formal models. The diverse implementations act as cross-checking test oracles to discover non-compliance. We used QUICtester to analyze analyze 186 learned models from 19 QUIC implementations under the five security settings and discovered 55 implementation errors. Significantly, the tool uncovered a QUIC specification ambiguity resulting in an easily exploitable DoS vulnerability, led to 5 CVE assignments from developers, and two bug bounties thus far.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12690v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kian Kai Ang, Guy Farrelly, Cheryl Pope, Damith C. Ranasinghe</dc:creator>
    </item>
    <item>
      <title>Confidence-Regulated Generative Diffusion Models for Reliable AI Agent Migration in Vehicular Metaverses</title>
      <link>https://arxiv.org/abs/2505.12710</link>
      <description>arXiv:2505.12710v1 Announce Type: cross 
Abstract: Vehicular metaverses are an emerging paradigm that merges intelligent transportation systems with virtual spaces, leveraging advanced digital twin and Artificial Intelligence (AI) technologies to seamlessly integrate vehicles, users, and digital environments. In this paradigm, vehicular AI agents are endowed with environment perception, decision-making, and action execution capabilities, enabling real-time processing and analysis of multi-modal data to provide users with customized interactive services. Since vehicular AI agents require substantial resources for real-time decision-making, given vehicle mobility and network dynamics conditions, the AI agents are deployed in RoadSide Units (RSUs) with sufficient resources and dynamically migrated among them. However, AI agent migration requires frequent data exchanges, which may expose vehicular metaverses to potential cyber attacks. To this end, we propose a reliable vehicular AI agent migration framework, achieving reliable dynamic migration and efficient resource scheduling through cooperation between vehicles and RSUs. Additionally, we design a trust evaluation model based on the theory of planned behavior to dynamically quantify the reputation of RSUs, thereby better accommodating the personalized trust preferences of users. We then model the vehicular AI agent migration process as a partially observable markov decision process and develop a Confidence-regulated Generative Diffusion Model (CGDM) to efficiently generate AI agent migration decisions. Numerical results demonstrate that the CGDM algorithm significantly outperforms baseline methods in reducing system latency and enhancing robustness against cyber attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12710v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingkai Kang, Jiawen Kang, Jinbo Wen, Tao Zhang, Zhaohui Yang, Dusit Niyato, Yan Zhang</dc:creator>
    </item>
    <item>
      <title>AutoBS: Autonomous Base Station Deployment with Reinforcement Learning and Digital Network Twins</title>
      <link>https://arxiv.org/abs/2502.19647</link>
      <description>arXiv:2502.19647v2 Announce Type: replace-cross 
Abstract: This paper introduces AutoBS, a reinforcement learning (RL)-based framework for optimal base station (BS) deployment in 6G radio access networks (RAN). AutoBS leverages the Proximal Policy Optimization (PPO) algorithm and fast, site-specific pathloss predictions from PMNet-a generative model for digital network twins (DNT). By efficiently learning deployment strategies that balance coverage and capacity, AutoBS achieves about 95% of the capacity of exhaustive search in single BS scenarios (and in 90% for multiple BSs), while cutting inference time from hours to milliseconds, making it highly suitable for real-time applications (e.g., ad-hoc deployments). AutoBS therefore provides a scalable, automated solution for large-scale 6G networks, meeting the demands of dynamic environments with minimal computational overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19647v2</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ju-Hyung Lee, Andreas F. Molisch</dc:creator>
    </item>
    <item>
      <title>UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture</title>
      <link>https://arxiv.org/abs/2503.20377</link>
      <description>arXiv:2503.20377v3 Announce Type: replace-cross 
Abstract: As the Large-scale Language Models (LLMs) continue to scale, the requisite computational power and bandwidth escalate. To address this, we introduce UB-Mesh, a novel AI datacenter network architecture designed to enhance scalability, performance, cost-efficiency and availability. Unlike traditional datacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a hierarchically localized nD-FullMesh network topology. This design fully leverages the data locality of LLM training, prioritizing short-range, direct interconnects to minimize data movement distance and reduce switch usage.
  Although UB-Mesh's nD-FullMesh topology offers several theoretical advantages, its concrete architecture design, physical implementation and networking system optimization present new challenges. For the actual construction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is based on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of hardware components that serve as the foundational building blocks, including specifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch (HRS), NICs and others. These components are interconnected via a novel Unified Bus (UB) technique, which enables flexible IO bandwidth allocation and hardware resource pooling. For networking system optimization, we propose advanced routing mechanism named All-Path-Routing (APR) to efficiently manage data traffic. These optimizations, combined with topology-aware performance enhancements and robust reliability measures like 64+1 backup design, result in 2.04x higher cost-efficiency, 7.2% higher network availability compared to traditional Clos architecture and 95%+ linearity in various LLM training tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20377v3</guid>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heng Liao, Bingyang Liu, Xianping Chen, Zhigang Guo, Chuanning Cheng, Jianbing Wang, Xiangyu Chen, Peng Dong, Rui Meng, Wenjie Liu, Zhe Zhou, Ziyang Zhang, Yuhang Gai, Cunle Qian, Yi Xiong, Zhongwu Cheng, Jing Xia, Yuli Ma, Xi Chen, Wenhua Du, Shizhong Xiao, Chungang Li, Yong Qin, Liudong Xiong, Zhou Yu, Lv Chen, Lei Chen, Buyun Wang, Pei Wu, Junen Gao, Xiaochu Li, Jian He, Shizhuan Yan, Bill McColl</dc:creator>
    </item>
  </channel>
</rss>
