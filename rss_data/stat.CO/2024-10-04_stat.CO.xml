<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Oct 2024 04:03:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A fast, flexible simulation framework for Bayesian adaptive designs -- the R package BATSS</title>
      <link>https://arxiv.org/abs/2410.02050</link>
      <description>arXiv:2410.02050v1 Announce Type: new 
Abstract: The use of Bayesian adaptive designs for randomised controlled trials has been hindered by the lack of software readily available to statisticians. We have developed a new software package (Bayesian Adaptive Trials Simulator Software - BATSS for the statistical software R, which provides a flexible structure for the fast simulation of Bayesian adaptive designs for clinical trials. We illustrate how the BATSS package can be used to define and evaluate the operating characteristics of Bayesian adaptive designs for various different types of primary outcomes (e.g., those that follow a normal, binary, Poisson or negative binomial distribution) and can incorporate the most common types of adaptations: stopping treatments (or the entire trial) for efficacy or futility, and Bayesian response adaptive randomisation - based on user-defined adaptation rules. Other important features of this highly modular package include: the use of (Integrated Nested) Laplace approximations to compute posterior distributions, parallel processing on a computer or a cluster, customisability, adjustment for covariates and a wide range of available conditional distributions for the response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02050v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dominique-Laurent Couturier, Elizabeth G Ryan, Rainer Puhr, Thomas Jaki, Stephane Heritier</dc:creator>
    </item>
    <item>
      <title>Polynomial approximation of noisy functions</title>
      <link>https://arxiv.org/abs/2410.02317</link>
      <description>arXiv:2410.02317v1 Announce Type: cross 
Abstract: Approximating a univariate function on the interval $[-1,1]$ with a polynomial is among the most classical problems in numerical analysis. When the function evaluations come with noise, a least-squares fit is known to reduce the effect of noise as more samples are taken. The generic algorithm for the least-squares problem requires $O(Nn^2)$ operations, where $N+1$ is the number of sample points and $n$ is the degree of the polynomial approximant. This algorithm is unstable when $n$ is large, for example $n\gg \sqrt{N}$ for equispaced sample points. In this study, we blend numerical analysis and statistics to introduce a stable and fast $O(N\log N)$ algorithm called NoisyChebtrunc based on the Chebyshev interpolation. It has the same error reduction effect as least-squares and the convergence is spectral until the error reaches $O(\sigma \sqrt{{n}/{N}})$, where $\sigma$ is the noise level, after which the error continues to decrease at the Monte-Carlo $O(1/\sqrt{N})$ rate. To determine the polynomial degree, NoisyChebtrunc employs a statistical criterion, namely Mallows' $C_p$. We analyze NoisyChebtrunc in terms of the variance and concentration in the infinity norm to the underlying noiseless function. These results show that with high probability the infinity-norm error is bounded by a small constant times $\sigma \sqrt{{n}/{N}}$, when the noise {is} independent and follows a subgaussian or subexponential distribution. We illustrate the performance of NoisyChebtrunc with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02317v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takeru Matsuda, Yuji Nakatsukasa</dc:creator>
    </item>
    <item>
      <title>GIST: Gibbs self-tuning for locally adaptive Hamiltonian Monte Carlo</title>
      <link>https://arxiv.org/abs/2404.15253</link>
      <description>arXiv:2404.15253v3 Announce Type: replace 
Abstract: We introduce a novel and flexible framework for constructing locally adaptive Hamiltonian Monte Carlo (HMC) samplers by Gibbs sampling the algorithm's tuning parameters conditionally based on the position and momentum at each step. For adaptively sampling path lengths, this framework -- which we call Gibbs self-tuning (GIST) -- encompasses randomized HMC, multinomial HMC, the No-U-Turn Sampler (NUTS), and the Apogee-to-Apogee Path Sampler as special cases. The GIST framework is illustrated with a novel alternative to NUTS for locally adapting path lengths, evaluated with an exact Hamiltonian for a high-dimensional, ill-conditioned Gaussian measure and with the leapfrog integrator for a suite of diverse models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15253v3</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Bob Carpenter, Milo Marsden</dc:creator>
    </item>
    <item>
      <title>Randomized Runge-Kutta-Nystr\"om Methods for Unadjusted Hamiltonian and Kinetic Langevin Monte Carlo</title>
      <link>https://arxiv.org/abs/2310.07399</link>
      <description>arXiv:2310.07399v2 Announce Type: replace-cross 
Abstract: We introduce $5/2$- and $7/2$-order $L^2$-accurate randomized Runge-Kutta-Nystr\"{o}m methods, tailored for approximating Hamiltonian flows within non-reversible Markov chain Monte Carlo samplers, such as unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin Monte Carlo. We establish quantitative $5/2$-order $L^2$-accuracy upper bounds under gradient and Hessian Lipschitz assumptions on the potential energy function. The numerical experiments demonstrate the superior efficiency of the proposed unadjusted samplers on a variety of well-behaved, high-dimensional target distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07399v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Tore Selland Kleppe</dc:creator>
    </item>
  </channel>
</rss>
