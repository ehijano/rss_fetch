<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Sep 2024 01:44:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Market Reaction to News Flows in Supply Chain Networks</title>
      <link>https://arxiv.org/abs/2409.06255</link>
      <description>arXiv:2409.06255v1 Announce Type: new 
Abstract: This study examines whether positive news about firms increases their stock prices and, moreover, whether it increases stock prices of the firms' suppliers and customers, using a large sample of publicly listed firms across the world and another of Japanese listed firms. The level of positiveness of each news article is determined by FinBERT, a natural language processing model fine-tuned specifically for financial information. Supply chains of firms across the world are identified mostly by financial statements, while those of Japanese firms are taken from large-scale firm-level surveys. We find that positive news increases the change rate of stock prices of firms mentioned in the news before its disclosure, most likely because of diffusion of information through informal channels. Positive news also raises stock prices of the firms' suppliers and customers before its disclosure, confirming propagation of market values through supply chains. In addition, we generally find a larger post-news effect on stock prices of the mentioned firms and their suppliers and customers than the pre-news effect. The positive difference between the post- and pre-news effects can be considered as the net effect of the disclosure of positive news, controlling for informal information diffusion. However, the post-news effect on suppliers and customers in Japan is smaller than the pre-news effect, a result opposite to those from firms across the world. This notable result is possibly because supply chain links of Japanese firms are stronger than global supply chains while such knowledge is restricted to selected investors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06255v1</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroyasu Inoue, Yasuyuki Todo</dc:creator>
    </item>
    <item>
      <title>Fast nonparametric inference of network backbones for graph sparsification</title>
      <link>https://arxiv.org/abs/2409.06417</link>
      <description>arXiv:2409.06417v1 Announce Type: new 
Abstract: A network backbone provides a useful sparse representation of a weighted network by keeping only its most important links, permitting a range of computational speedups and simplifying complex network visualizations. There are many possible criteria for a link to be considered important, and hence many methods have been developed for the task of network backboning for graph sparsification. These methods can be classified as global or local in nature depending on whether they evaluate the importance of an edge in the context of the whole network or an individual node neighborhood. A key limitation of existing network backboning methods is that they either artificially restrict the topology of the backbone to take a specific form (e.g. a tree) or they require the specification of a free parameter (e.g. a significance level) that determines the number of edges to keep in the backbone. Here we develop a completely nonparametric framework for inferring the backbone of a weighted network that overcomes these limitations by automatically selecting the optimal number of edges to retain in the backbone using the Minimum Description Length (MDL) principle from information theory. We develop two encoding schemes that serve as objective functions for global and local network backbones, as well as efficient optimization algorithms to identify the optimal backbones according to these objectives with runtime complexity log-linear in the number of edges. We show that the proposed framework is generalizable to any discrete weight distribution on the edges using a maximum a posteriori (MAP) estimation procedure with an asymptotically equivalent Bayesian generative model of the backbone. We compare the proposed method with existing methods in a range of tasks on real and synthetic networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06417v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec Kirkley</dc:creator>
    </item>
    <item>
      <title>Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity</title>
      <link>https://arxiv.org/abs/2409.06091</link>
      <description>arXiv:2409.06091v1 Announce Type: cross 
Abstract: Multitask learning is a widely used paradigm for training models on diverse tasks, with applications ranging from graph neural networks to language model fine-tuning. Since tasks may interfere with each other, a key notion for modeling their relationships is task affinity. This includes pairwise task affinity, computed among pairs of tasks, and higher-order affinity, computed among subsets of tasks. Naively computing either of them requires repeatedly training on data from various task combinations, which is computationally intensive. We present a new algorithm Grad-TAG that can estimate task affinities without this repeated training.
  The key idea of Grad-TAG is to train a "base" model for all tasks and then use a linearization technique to estimate the loss of the model for a specific task combination. The linearization works by computing a gradient-based approximation of the loss, using low-dimensional projections of gradients as features in a logistic regression to predict labels for the task combination. We show that the linearized model can provably approximate the loss when the gradient-based approximation is accurate, and also empirically verify that on several large models. Then, given the estimated task affinity, we design a semi-definite program for clustering similar tasks by maximizing the average density of clusters.
  We evaluate Grad-TAG's performance across seven datasets, including multi-label classification on graphs, and instruction fine-tuning of language models. Our task affinity estimates are within 2.7% distance to the true affinities while needing only 3% of FLOPs in full training. On our largest graph with 21M edges and 500 labeling tasks, our algorithm delivers estimates within 5% distance to the true affinities, using only 112 GPU hours. Our results show that Grad-TAG achieves excellent performance and runtime tradeoffs compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06091v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671835</arxiv:DOI>
      <dc:creator>Dongyue Li, Aneesh Sharma, Hongyang R. Zhang</dc:creator>
    </item>
    <item>
      <title>INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2307.08131</link>
      <description>arXiv:2307.08131v4 Announce Type: replace 
Abstract: Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the continuous evolution of customer-brand relationships. In this paper, we present INFLECT-DGNN, a new method for profit-driven INFLuencer prEdiCTion with Dynamic Graph Neural Networks that innovatively combines Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) with weighted loss functions, synthetic minority oversampling adapted to graph data, and a carefully crafted rolling-window strategy. We introduce a novel profit-driven framework that supports decision-making based on model predictions. To test the framework, we use a unique corporate dataset with diverse networks, capturing the customer interactions across three cities with different socioeconomic and demographic characteristics. Our results show how using RNNs to encode temporal attributes alongside GNNs significantly improves predictive performance, while the profit-driven framework determines the optimal classification threshold for profit maximization. We compare the results of different models to demonstrate the importance of capturing network representation, temporal dependencies, and using a profit-driven evaluation. Our research has significant implications for the fields of referral and targeted marketing, expanding the technical use of deep graph learning within corporate environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08131v4</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2024.3443533</arxiv:DOI>
      <arxiv:journal_reference>IEEE Access, 12, 115026-115041 (2024)</arxiv:journal_reference>
      <dc:creator>Elena Tiukhova, Emiliano Penaloza, Mar\'ia \'Oskarsd\'ottir, Bart Baesens, Monique Snoeck, Cristi\'an Bravo</dc:creator>
    </item>
    <item>
      <title>An impossibility result for Markov Chain Monte Carlo sampling from micro-canonical bipartite graph ensembles</title>
      <link>https://arxiv.org/abs/2308.10838</link>
      <description>arXiv:2308.10838v3 Announce Type: replace 
Abstract: Markov Chain Monte Carlo (MCMC) algorithms are commonly used to sample from graph ensembles. Two graphs are neighbors in the state space if one can be obtained from the other with only a few modifications, e.g., edge rewirings. For many common ensembles, e.g., those preserving the degree sequences of bipartite graphs, rewiring operations involving two edges are sufficient to create a fully-connected state space, and they can be performed efficiently. We show that, for ensembles of bipartite graphs with fixed degree sequences and number of butterflies (k2,2 bi-cliques), there is no universal constant c such that a rewiring of at most c edges at every step is sufficient for any such ensemble to be fully connected. Our proof relies on an explicit construction of a family of pairs of graphs with the same degree sequences and number of butterflies, with each pair indexed by a natural c, and such that any sequence of rewiring operations transforming one graph into the other must include at least one rewiring operation involving at least c edges. Whether rewiring these many edges is sufficient to guarantee the full connectivity of the state space of any such ensemble remains an open question. Our result implies the impossibility of developing efficient, graph-agnostic, MCMC algorithms for these ensembles, as the necessity to rewire an impractically large number of edges may hinder taking a step on the state space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10838v3</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.109.L053301</arxiv:DOI>
      <dc:creator>Giulia Preti, Gianmarco De Francisci Morales, Matteo Riondato</dc:creator>
    </item>
    <item>
      <title>Relational Prompt-based Pre-trained Language Models for Social Event Detection</title>
      <link>https://arxiv.org/abs/2404.08263</link>
      <description>arXiv:2404.08263v2 Announce Type: replace-cross 
Abstract: Social Event Detection (SED) aims to identify significant events from social streams, and has a wide application ranging from public opinion analysis to risk management. In recent years, Graph Neural Network (GNN) based solutions have achieved state-of-the-art performance. However, GNN-based methods often struggle with missing and noisy edges between messages, affecting the quality of learned message embedding. Moreover, these methods statically initialize node embedding before training, which, in turn, limits the ability to learn from message texts and relations simultaneously. In this paper, we approach social event detection from a new perspective based on Pre-trained Language Models (PLMs), and present RPLM_SED (Relational prompt-based Pre-trained Language Models for Social Event Detection). We first propose a new pairwise message modeling strategy to construct social messages into message pairs with multi-relational sequences. Secondly, a new multi-relational prompt-based pairwise message learning mechanism is proposed to learn more comprehensive message representation from message pairs with multi-relational prompts using PLMs. Thirdly, we design a new clustering constraint to optimize the encoding process by enhancing intra-cluster compactness and inter-cluster dispersion, making the message representation more distinguishable. We evaluate the RPLM_SED on three real-world datasets, demonstrating that the RPLM_SED model achieves state-of-the-art performance in offline, online, low-resource, and long-tail distribution scenarios for social event detection tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08263v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pu Li, Xiaoyan Yu, Hao Peng, Yantuan Xian, Linqin Wang, Li Sun, Jingyun Zhang, Philip S. Yu</dc:creator>
    </item>
  </channel>
</rss>
