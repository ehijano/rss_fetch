<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Feb 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Time-Varying Meta-Analysis via Hierarchical Mean-Variance Random-effects Models</title>
      <link>https://arxiv.org/abs/2502.03809</link>
      <description>arXiv:2502.03809v1 Announce Type: new 
Abstract: Meta-analysis is widely used to integrate results from multiple experiments to obtain generalized insights. Since meta-analysis datasets are often heteroscedastic due to varying subgroups and temporal heterogeneity arising from experiments conducted at different time points, the typical meta-analysis approach, which assumes homoscedasticity, fails to adequately address this heteroscedasticity among experiments. This paper proposes a new Bayesian estimation method that simultaneously shrinks estimates of the means and variances of experiments using a hierarchical Bayesian approach while accounting for time effects through a Gaussian process. This method connects experiments via the hierarchical framework, enabling "borrowing strength" between experiments to achieve high-precision estimates of each experiment's mean. The method can flexibly capture potential time trends in datasets by modeling time effects with the Gaussian process. We demonstrate the effectiveness of the proposed method through simulation studies and illustrate its practical utility using a real marketing promotions dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03809v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kohsuke Kubota, Shonosuke Sugasawa, Keiichi Ochiai, Takahiro Hoshino</dc:creator>
    </item>
    <item>
      <title>Unbiased Parameter Estimation for Bayesian Inverse Problems</title>
      <link>https://arxiv.org/abs/2502.03920</link>
      <description>arXiv:2502.03920v1 Announce Type: new 
Abstract: In this paper we consider the estimation of unknown parameters in Bayesian inverse problems. In most cases of practical interest, there are several barriers to performing such estimation, This includes a numerical approximation of a solution of a differential equation and, even if exact solutions are available, an analytical intractability of the marginal likelihood and its associated gradient, which is used for parameter estimation. The focus of this article is to deliver unbiased estimates of the unknown parameters, that is, stochastic estimators that, in expectation, are equal to the maximize of the marginal likelihood, and possess no numerical approximation error. Based upon the ideas of [4] we develop a new approach for unbiased parameter estimation for Bayesian inverse problems. We prove unbiasedness and establish numerically that the associated estimation procedure is faster than the current state-of-the-art methodology for this problem. We demonstrate the performance of our methodology on a range of problems which include a PDE and ODE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03920v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil K. Chada, Ajay Jasra, Mohamed Maama, Raul Tempone</dc:creator>
    </item>
    <item>
      <title>A retake on the analysis of scores truncated by terminal events</title>
      <link>https://arxiv.org/abs/2502.03942</link>
      <description>arXiv:2502.03942v1 Announce Type: new 
Abstract: Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on disease score and the terminal event. Our proposal is based on a natural data-generating mechanism respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behavior of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03942v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus K\"ahler Holst, Andreas Nordland, Julie Furkj{\ae}r, Lars Holm Damgaard, Christian Bressen Pipper</dc:creator>
    </item>
    <item>
      <title>High-Frequency Market Manipulation Detection with a Markov-modulated Hawkes process</title>
      <link>https://arxiv.org/abs/2502.04027</link>
      <description>arXiv:2502.04027v1 Announce Type: new 
Abstract: This work focuses on a self-exciting point process defined by a Hawkes-like intensity and a switching mechanism based on a hidden Markov chain. Previous works in such a setting assume constant intensities between consecutive events. We extend the model to general Hawkes excitation kernels that are piecewise constant between events. We develop an expectation-maximization algorithm for the statistical inference of the Hawkes intensities parameters as well as the state transition probabilities. The numerical convergence of the estimators is extensively tested on simulated data. Using high-frequency cryptocurrency data on a top centralized exchange, we apply the model to the detection of anomalous bursts of trades. We benchmark the goodness-of-fit of the model with the Markov-modulated Poisson process and demonstrate the relevance of the model in detecting suspicious activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04027v1</guid>
      <category>stat.ME</category>
      <category>q-fin.ST</category>
      <category>q-fin.TR</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timoth\'ee Fabre, Ioane Muni Toke</dc:creator>
    </item>
    <item>
      <title>A method for sparse and robust independent component analysis</title>
      <link>https://arxiv.org/abs/2502.04046</link>
      <description>arXiv:2502.04046v1 Announce Type: new 
Abstract: This work presents sparse invariant coordinate analysis, SICS, a new method for sparse and robust independent component analysis. SICS is based on classical invariant coordinate analysis, which is presented in such a form that a LASSO-type penalty can be applied to promote sparsity. Robustness is achieved by using robust scatter matrices. In the first part of the paper, the background and building blocks: scatter matrices, measures of robustness, ICS and independent component analysis, are carefully introduced. Then the proposed new method and its algorithm are derived and presented. This part also includes a consistency result for a general case of sparse ICS-like methods. The performance of SICS in identifying sparse independent component loadings is investigated with simulations. The method is also illustrated with example in constructing sparse causal graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04046v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lauri Heinonen, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Quasi maximum likelihood estimation of high-dimensional approximate dynamic matrix factor models via the EM algorithm</title>
      <link>https://arxiv.org/abs/2502.04112</link>
      <description>arXiv:2502.04112v1 Announce Type: new 
Abstract: This paper considers an approximate dynamic matrix factor model that accounts for the time series nature of the data by explicitly modelling the time evolution of the factors. We study Quasi Maximum Likelihood estimation of the model parameters based on the Expectation Maximization (EM) algorithm, implemented jointly with the Kalman smoother which gives estimates of the factors. This approach allows to easily handle arbitrary patterns of missing data. We establish the consistency of the estimated loadings and factor matrices as the sample size $T$ and the matrix dimensions $p_1$ and $p_2$ diverge to infinity. The finite sample properties of the estimators are assessed through a large simulation study and an application to a financial dataset of volatility proxies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04112v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Luca Trapin</dc:creator>
    </item>
    <item>
      <title>How many unseen species are in multiple areas?</title>
      <link>https://arxiv.org/abs/2502.04122</link>
      <description>arXiv:2502.04122v1 Announce Type: new 
Abstract: In ecology, the description of species composition and biodiversity calls for statistical methods that involve estimating features of interest in unobserved samples based on an observed one. In the last decade, the Bayesian nonparametrics literature has thoroughly investigated the case where data arise from a homogeneous population. In this work, we propose a novel framework to address heterogeneous populations, specifically dealing with scenarios where data arise from two areas. This setting significantly increases the mathematical complexity of the problem and, as a consequence, it received limited attention in the literature. While early approaches leverage on computational methods, we provide a distributional theory for the in-sample analysis of any observed sample and we enable out-of-sample prediction for the number of unseen distinct and shared species in additional samples of arbitrary sizes. The latter also extends the frequentist estimators which solely deal with the one-step ahead prediction. Furthermore, our results can be applied to address the sample size determination in sampling problems aimed at detecting shared species. Our results are illustrated in a real-world dataset concerning a population of ants in the city of Trieste.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04122v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Colombi, Raffaele Argiento, Federico Camerlenghi, Lucia Paci</dc:creator>
    </item>
    <item>
      <title>Detecting Mild Traumatic Brain Injury with MEG Scan Data: One-vs-K-Sample Tests</title>
      <link>https://arxiv.org/abs/2502.04258</link>
      <description>arXiv:2502.04258v1 Announce Type: new 
Abstract: Magnetoencephalography (MEG) scanner has been shown to be more accurate than other medical devices in detecting mild traumatic brain injury (mTBI). However, MEG scan data in certain spectrum ranges can be skewed, multimodal and heterogeneous which can mislead the conventional case-control analysis that requires the data to be homogeneous and normally distributed within the control group. To meet this challenge, we propose a flexible one-vs-K-sample testing procedure for detecting brain injury for a single-case versus heterogeneous controls. The new procedure begins with source magnitude imaging using MEG scan data in frequency domain, followed by region-wise contrast tests for abnormality between the case and controls. The critical values for these tests are automatically determined by cross-validation. We adjust the testing results for heterogeneity effects by similarity analysis. An asymptotic theory is established for the proposed test statistic. By simulated and real data analyses in the context of neurotrauma, we show that the proposed test outperforms commonly used nonparametric methods in terms of overall accuracy and ability in accommodating data non-normality and subject-heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04258v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Zhang, Gary Green</dc:creator>
    </item>
    <item>
      <title>Capacity Constraints in Ball and Urn Distribution Problems</title>
      <link>https://arxiv.org/abs/2502.03495</link>
      <description>arXiv:2502.03495v1 Announce Type: cross 
Abstract: This paper explores the distribution of indistinguishable balls into distinct urns with varying capacity constraints, a foundational issue in combinatorial mathematics with applications across various disciplines. We present a comprehensive theoretical framework that addresses both upper and lower capacity constraints under different distribution conditions, elaborating on the combinatorial implications of such variations. Through rigorous analysis, we derive analytical solutions that cater to different constrained environments, providing a robust theoretical basis for future empirical and theoretical investigations. These solutions are pivotal for advancing research in fields that rely on precise distribution strategies, such as physics and parallel processing. The paper not only generalizes classical distribution problems but also introduces novel methodologies for tackling capacity variations, thereby broadening the utility and applicability of distribution theory in practical and theoretical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03495v1</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingwei Li, Thomas G. Robertazzi</dc:creator>
    </item>
    <item>
      <title>Distribution learning via neural differential equations: minimal energy regularization and approximation theory</title>
      <link>https://arxiv.org/abs/2502.03795</link>
      <description>arXiv:2502.03795v1 Announce Type: cross 
Abstract: Neural ordinary differential equations (ODEs) provide expressive representations of invertible transport maps that can be used to approximate complex probability distributions, e.g., for generative modeling, density estimation, and Bayesian inference. We show that for a large class of transport maps $T$, there exists a time-dependent ODE velocity field realizing a straight-line interpolation $(1-t)x + tT(x)$, $t \in [0,1]$, of the displacement induced by the map. Moreover, we show that such velocity fields are minimizers of a training objective containing a specific minimum-energy regularization. We then derive explicit upper bounds for the $C^k$ norm of the velocity field that are polynomial in the $C^k$ norm of the corresponding transport map $T$; in the case of triangular (Knothe--Rosenblatt) maps, we also show that these bounds are polynomial in the $C^k$ norms of the associated source and target densities. Combining these results with stability arguments for distribution approximation via ODEs, we show that Wasserstein or Kullback--Leibler approximation of the target distribution to any desired accuracy $\epsilon &gt; 0$ can be achieved by a deep neural network representation of the velocity field whose size is bounded explicitly in terms of $\epsilon$, the dimension, and the smoothness of the source and target densities. The same neural network ansatz yields guarantees on the value of the regularized training objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03795v1</guid>
      <category>cs.LG</category>
      <category>math.CA</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youssef Marzouk, Zhi Ren, Jakob Zech</dc:creator>
    </item>
    <item>
      <title>MXMap: A Multivariate Cross Mapping Framework for Causal Discovery in Dynamical Systems</title>
      <link>https://arxiv.org/abs/2502.03802</link>
      <description>arXiv:2502.03802v1 Announce Type: cross 
Abstract: Convergent Cross Mapping (CCM) is a powerful method for detecting causality in coupled nonlinear dynamical systems, providing a model-free approach to capture dynamic causal interactions. Partial Cross Mapping (PCM) was introduced as an extension of CCM to address indirect causality in three-variable systems by comparing cross-mapping quality between direct cause-effect mapping and indirect mapping through an intermediate conditioning variable. However, PCM remains limited to univariate delay embeddings in its cross-mapping processes. In this work, we extend PCM to the multivariate setting, introducing multiPCM, which leverages multivariate embeddings to more effectively distinguish indirect causal relationships. We further propose a multivariate cross-mapping framework (MXMap) for causal discovery in dynamical systems. This two-phase framework combines (1) pairwise CCM tests to establish an initial causal graph and (2) multiPCM to refine the graph by pruning indirect causal connections. Through experiments on simulated data and the ERA5 Reanalysis weather dataset, we demonstrate the effectiveness of MXMap. Additionally, MXMap is compared against several baseline methods, showing advantages in accuracy and causal graph refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03802v1</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elise Zhang, Fran\c{c}ois Mirall\`es, Rapha\"el Rousseau-Rizzi, Arnaud Zinflou, Di Wu, Benoit Boulet</dc:creator>
    </item>
    <item>
      <title>A fast algorithm to compute a curve of confidence upper bounds for the False Discovery Proportion using a reference family with a forest structure</title>
      <link>https://arxiv.org/abs/2502.03849</link>
      <description>arXiv:2502.03849v1 Announce Type: cross 
Abstract: This paper presents a new algorithm (and an additional trick) that allows to compute fastly an entire curve of post hoc bounds for the False Discovery Proportion when the underlying bound $V^*_{\mathfrak{R}}$ construction is based on a reference family $\mathfrak{R}$ with a forest structure {\`a} la Durand et al. (2020). By an entire curve, we mean the values $V^*_{\mathfrak{R}}(S_1),\dotsc,V^*_{\mathfrak{R}}(S_m)$ computed on a path of increasing selection sets $S_1\subsetneq\dotsb\subsetneq S_m$, $|S_t|=t$. The new algorithm leverages the fact that going from $S_t$ to $S_{t+1}$ is done by adding only one hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03849v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillermo Durand</dc:creator>
    </item>
    <item>
      <title>Dimension estimation in PCA model using high-dimensional data augmentation</title>
      <link>https://arxiv.org/abs/2502.04220</link>
      <description>arXiv:2502.04220v1 Announce Type: cross 
Abstract: We propose a modified, high-dimensional version of a recent dimension estimation procedure that determines the dimension via the introduction of augmented noise variables into the data. Our asymptotic results show that the proposal is consistent in wide high-dimensional scenarios, and further shed light on why the original method breaks down when the dimension of either the data or the augmentation becomes too large. Simulations are used to demonstrate the superiority of the proposal to competitors both under and outside of the theoretical model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04220v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Una Radojicic, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Efficient Randomized Experiments Using Foundation Models</title>
      <link>https://arxiv.org/abs/2502.04262</link>
      <description>arXiv:2502.04262v1 Announce Type: cross 
Abstract: Randomized experiments are the preferred approach for evaluating the effects of interventions, but they are costly and often yield estimates with substantial uncertainty. On the other hand, in silico experiments leveraging foundation models offer a cost-effective alternative that can potentially attain higher statistical precision. However, the benefits of in silico experiments come with a significant risk: statistical inferences are not valid if the models fail to accurately predict experimental responses to interventions. In this paper, we propose a novel approach that integrates the predictions from multiple foundation models with experimental data while preserving valid statistical inference. Our estimator is consistent and asymptotically normal, with asymptotic variance no larger than the standard estimator based on experimental data alone. Importantly, these statistical properties hold even when model predictions are arbitrarily biased. Empirical results across several randomized experiments show that our estimator offers substantial precision gains, equivalent to a reduction of up to 20% in the sample size needed to match the same precision as the standard estimator based on experimental data alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04262v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piersilvio De Bartolomeis, Javier Abad, Guanbo Wang, Konstantin Donhauser, Raymond M. Duch, Fanny Yang, Issa J. Dahabreh</dc:creator>
    </item>
    <item>
      <title>Prediction-Powered E-Values</title>
      <link>https://arxiv.org/abs/2502.04294</link>
      <description>arXiv:2502.04294v1 Announce Type: cross 
Abstract: Quality statistical inference requires a sufficient amount of data, which can be missing or hard to obtain. To this end, prediction-powered inference has risen as a promising methodology, but existing approaches are largely limited to Z-estimation problems such as inference of means and quantiles. In this paper, we apply ideas of prediction-powered inference to e-values. By doing so, we inherit all the usual benefits of e-values -- such as anytime-validity, post-hoc validity and versatile sequential inference -- as well as greatly expand the set of inferences achievable in a prediction-powered manner. In particular, we show that every inference procedure that can be framed in terms of e-values has a prediction-powered counterpart, given by our method. We showcase the effectiveness of our framework across a wide range of inference tasks, from simple hypothesis testing and confidence intervals to more involved procedures for change-point detection and causal discovery, which were out of reach of previous techniques. Our approach is modular and easily integrable into existing algorithms, making it a compelling choice for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04294v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Csillag, Claudio Jos\'e Struchiner, Guilherme Tegoni Goedert</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Copula Models with a Dirichlet-Laplace Prior</title>
      <link>https://arxiv.org/abs/2202.13689</link>
      <description>arXiv:2202.13689v2 Announce Type: replace 
Abstract: We discuss a Bayesian hierarchical copula model for clusters of financial time series. A similar approach has been developed in recent paper. However, the prior distributions proposed there do not always provide a proper posterior. In order to circumvent the problem, we adopt a proper global-local shrinkage prior, which is also able to account for potential dependence structures among different clusters. The performance of the proposed model is presented via simulations and a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.13689v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paolo Onorati, Brunero Liseo</dc:creator>
    </item>
    <item>
      <title>Random forests for binary geospatial data</title>
      <link>https://arxiv.org/abs/2302.13828</link>
      <description>arXiv:2302.13828v2 Announce Type: replace 
Abstract: The manuscript develops new method and theory for non-linear regression for binary dependent data using random forests. Existing implementations of random forests for binary data cannot explicitly account for data correlation common in geospatial and time-series settings. For continuous outcomes, recent work has extended random forests (RF) to RF-GLS that incorporate spatial covariance using the generalized least squares (GLS) loss. However, adoption of this idea for binary data is challenging due to the use of the Gini impurity measure in classification trees, which has no known extension to model dependence. We show that for binary data, the GLS loss is also an extension of the Gini impurity measure, as the latter is exactly equivalent to the ordinary least squares (OLS) loss. This justifies using RF-GLS for non-parametric mean function estimation for binary dependent data. We then consider the special case of generalized mixed effects models, the traditional statistical model for binary geospatial data, which models the spatial random effects as a Gaussian process (GP). We propose a novel link-inversion technique that embeds the RF-GLS estimate of the mean function from the first step within the generalized mixed effects model framework, enabling estimation of non-linear covariate effects and offering spatial predictions. We establish consistency of our method, RF-GP, for both mean function and covariate effect estimation. The theory holds for a general class of stationary absolutely regular dependent processes that includes common choices like Gaussian processes with Mat\'ern or compactly supported covariances and autoregressive processes. The theory relaxes the common assumption of additive mean functions and accounts for the non-linear link. We demonstrate that RF-GP outperforms competing methods for estimation and prediction in both simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13828v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkajyoti Saha, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Discrete-time Competing-Risks Regression with or without Penalization</title>
      <link>https://arxiv.org/abs/2303.01186</link>
      <description>arXiv:2303.01186v3 Announce Type: replace 
Abstract: Many studies employ the analysis of time-to-event data that incorporates competing risks and right censoring. Most methods and software packages are geared towards analyzing data that comes from a continuous failure time distribution. However, failure-time data may sometimes be discrete either because time is inherently discrete or due to imprecise measurement. This paper introduces a new estimation procedure for discrete-time survival analysis with competing events. The proposed approach offers a major key advantage over existing procedures and allows for straightforward integration and application of widely used regularized regression and screening-features methods. We illustrate the benefits of our proposed approach by a comprehensive simulation study. Additionally, we showcase the utility of the proposed procedure by estimating a survival model for the length of stay of patients hospitalized in the intensive care unit, considering three competing events: discharge to home, transfer to another medical facility, and in-hospital death. A Python package, PyDTS, is available for applying the proposed method with additional features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.01186v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomer Meir, Malka Gorfine</dc:creator>
    </item>
    <item>
      <title>Resolving power: A general approach to compare the distinguishing ability of threshold-free evaluation metrics</title>
      <link>https://arxiv.org/abs/2304.00059</link>
      <description>arXiv:2304.00059v3 Announce Type: replace 
Abstract: Selecting an evaluation metric is fundamental to model development, but uncertainty remains about when certain metrics are preferable and why. This paper introduces the concept of *resolving power* to describe the ability of an evaluation metric to distinguish between binary classifiers of similar quality. This ability depends on two attributes: 1. The metric's response to improvements in classifier quality (its signal), and 2. The metric's sampling variability (its noise). The paper defines resolving power generically as a metric's sampling uncertainty scaled by its signal. A simulation study compares the area under the receiver operating characteristic curve (AUROC) and the and the area under the precision-recall curve (AUPRC) in a variety of contexts. It finds that the AUROC generally has greater resolving power, but that the AUPRC is better when searching among high-quality classifiers applied to low prevalence outcomes. The paper also proposes an empirical method to estimate resolving power that can be applied to any dataset and any initial classification model. The AUROC is useful for developing the resolving power concept, but it has been criticized for being misleading. Newer metrics developed to address its interpretative issues can be easily incorporated into the resolving power framework. The best metrics for model search will be both interpretable and high in resolving power. Sometimes these objectives will conflict and how to address this tension remains an open question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00059v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Machine Learning, 114(1), 9 (2025)</arxiv:journal_reference>
      <dc:creator>Colin S. Beam</dc:creator>
    </item>
    <item>
      <title>Conditional variable screening for ultra-high dimensional longitudinal data with time interactions</title>
      <link>https://arxiv.org/abs/2306.09518</link>
      <description>arXiv:2306.09518v2 Announce Type: replace 
Abstract: In recent years we have been able to gather large amounts of genomic data at a fast rate, creating situations where the number of variables greatly exceeds the number of observations. In these situations, most models that can handle a moderately high dimension will now become computationally infeasible or unstable. Hence, there is a need for a pre-screening of variables to reduce the dimension efficiently and accurately to a more moderate scale. There has been much work to develop such screening procedures for independent outcomes. However, much less work has been done for high-dimensional longitudinal data in which the observations can no longer be assumed to be independent. In addition, it is of interest to capture possible interactions between the genomic variable and time in many of these longitudinal studies. In this work, we propose a novel conditional screening procedure that ranks variables according to the likelihood value at the maximum likelihood estimates in a marginal linear mixed model, where the genomic variable and its interaction with time are included in the model. This is to our knowledge the first conditional screening approach for clustered data. We prove that this approach enjoys the sure screening property, and assess the finite sample performance of the method through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09518v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Bratsberg, Abhik Ghosh, Magne Thoresen</dc:creator>
    </item>
    <item>
      <title>Linear classification methods for multivariate repeated measures data -- a simulation study</title>
      <link>https://arxiv.org/abs/2310.00107</link>
      <description>arXiv:2310.00107v2 Announce Type: replace 
Abstract: Researchers in the behavioral and social sciences use linear discriminant analysis (LDA) for predictions of group membership (classification) and for identifying the variables most relevant to group separation among a set of continuous correlated variables (description). \\ In these and other disciplines, longitudinal data are often collected which provide additional temporal information. Linear classification methods for repeated measures data are more sensitive to actual group differences by taking the complex correlations between time points and variables into account, but are rarely discussed in the literature. Moreover, psychometric data rarely fulfill the multivariate normality assumption.\\ In this paper, we compare existing linear classification algorithms for nonnormally distributed multivariate repeated measures data in a simulation study based on psychological questionnaire data comprising Likert scales. The results show that in data without any specific assumed structure and larger sample sizes, the robust alternatives to standard repeated measures LDA may not be needed. To our knowledge, this is one of the few studies discussing repeated measures classification techniques, and the first one comparing multiple alternatives among each other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00107v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricarda Graf, Marina Zeldovich, Sarah Friedrich</dc:creator>
    </item>
    <item>
      <title>A Personalized Predictive Model that Jointly Optimizes Discrimination and Calibration</title>
      <link>https://arxiv.org/abs/2403.17132</link>
      <description>arXiv:2403.17132v2 Announce Type: replace 
Abstract: Precision medicine is accelerating rapidly in the field of health research. This includes fitting predictive models for individual patients based on patient similarity in an attempt to improve model performance. We propose an algorithm which fits a personalized predictive model (PPM) using an optimal size of a similar subpopulation that jointly optimizes model discrimination and calibration, as it is criticized that calibration is not assessed nearly as often as discrimination despite poorly calibrated models being potentially misleading. We define a mixture loss function that considers model discrimination and calibration, and allows for flexibility in emphasizing one performance measure over another. We empirically show that the relationship between the size of subpopulation and calibration is quadratic, which motivates the development of our jointly optimized model. We also investigate the effect of within-population patient weighting on performance and conclude that the size of subpopulation has a larger effect on the predictive performance of the PPM compared to the choice of weight function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17132v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiana Krikella, Joel A. Dubin</dc:creator>
    </item>
    <item>
      <title>On Neighbourhood Cross Validation</title>
      <link>https://arxiv.org/abs/2404.16490</link>
      <description>arXiv:2404.16490v3 Announce Type: replace 
Abstract: Many varieties of cross validation would be statistically appealing for the estimation of smoothing and other penalized regression hyperparameters, were it not for the high cost of evaluating such criteria. Here it is shown how to efficiently and accurately compute and optimize a broad variety of cross validation criteria for a wide range of models estimated by minimizing a quadratically penalized loss. The leading order computational cost of hyperparameter estimation is made comparable to the cost of a single model fit given hyperparameters. In many cases this represents an $O(n)$ computational saving when modelling $n$ data. This development makes if feasible, for the first time, to use leave-out-neighbourhood cross validation to deal with the wide spread problem of un-modelled short range autocorrelation which otherwise leads to underestimation of smoothing parameters. It is also shown how to accurately quantifying uncertainty in this case, despite the un-modelled autocorrelation. Practical examples are provided including smooth quantile regression, generalized additive models for location scale and shape, and focussing particularly on dealing with un-modelled autocorrelation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16490v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon N. Wood</dc:creator>
    </item>
    <item>
      <title>Out-of-distribution generalization under random, dense distributional shifts</title>
      <link>https://arxiv.org/abs/2404.18370</link>
      <description>arXiv:2404.18370v2 Announce Type: replace 
Abstract: Many existing approaches for estimating parameters in settings with distributional shifts operate under an invariance assumption. For example, under covariate shift, it is assumed that $p(y|x)$ remains invariant. We refer to such distribution shifts as sparse, since they may be substantial but affect only a part of the data generating system. In contrast, in various real-world settings, shifts might be dense. More specifically, these dense distributional shifts may arise through numerous small and random changes in the population and environment. First, we discuss empirical evidence for such random dense distributional shifts. Then, we develop tools to infer parameters and make predictions for partially observed, shifted distributions. Finally, we apply the framework to several real-world datasets and discuss diagnostics to evaluate the fit of the distributional uncertainty model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18370v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujin Jeong, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Robust Elicitable Functionals</title>
      <link>https://arxiv.org/abs/2409.04412</link>
      <description>arXiv:2409.04412v2 Announce Type: replace 
Abstract: Elicitable functionals and (strictly) consistent scoring functions are of interest due to their utility of determining (uniquely) optimal forecasts, and thus the ability to effectively backtest predictions. However, in practice, assuming that a distribution is correctly specified is too strong a belief to reliably hold. To remediate this, we incorporate a notion of statistical robustness into the framework of elicitable functionals, meaning that our robust functional accounts for "small" misspecifications of a baseline distribution. Specifically, we propose a robustified version of elicitable functionals by using the Kullback-Leibler divergence to quantify potential misspecifications from a baseline distribution. We show that the robust elicitable functionals admit unique solutions lying at the boundary of the uncertainty region, and provide conditions for existence and uniqueness. Since every elicitable functional possesses infinitely many scoring functions, we propose the class of b-homogeneous strictly consistent scoring functions, for which the robust functionals maintain desirable statistical properties. We show the applicability of the robust elicitable functional in several examples: in a reinsurance setting and in robust regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04412v2</guid>
      <category>stat.ME</category>
      <category>q-fin.MF</category>
      <category>q-fin.RM</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kathleen E. Miao, Silvana M. Pesenti</dc:creator>
    </item>
    <item>
      <title>A novel longitudinal rank-sum test for multiple primary endpoints in clinical trials: Applications to neurodegenerative disorders</title>
      <link>https://arxiv.org/abs/2410.19190</link>
      <description>arXiv:2410.19190v2 Announce Type: replace 
Abstract: Neurodegenerative disorders such as Alzheimer's disease (AD) present a significant global health challenge, characterized by cognitive decline, functional impairment, and other debilitating effects. Current AD clinical trials often assess multiple longitudinal primary endpoints to comprehensively evaluate treatment efficacy. Traditional methods, however, may fail to capture global treatment effects, require larger sample sizes due to multiplicity adjustments, and may not fully exploit multivariate longitudinal data. To address these limitations, we introduce the Longitudinal Rank Sum Test (LRST), a novel nonparametric rank-based omnibus test statistic. The LRST enables a comprehensive assessment of treatment efficacy across multiple endpoints and time points without multiplicity adjustments, effectively controlling Type I error while enhancing statistical power. It offers flexibility against various data distributions encountered in AD research and maximizes the utilization of longitudinal data. Extensive simulations and real-data applications demonstrate the LRST's performance, underscoring its potential as a valuable tool in AD clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19190v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/19466315.2025.2458018</arxiv:DOI>
      <dc:creator>Xiaoming Xu, Dhrubajyoti Ghosh, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>A Beta Cauchy-Cauchy (BECCA) shrinkage prior for Bayesian variable selection</title>
      <link>https://arxiv.org/abs/2501.07061</link>
      <description>arXiv:2501.07061v2 Announce Type: replace 
Abstract: This paper introduces a novel Bayesian approach for variable selection in high-dimensional and potentially sparse regression settings. Our method replaces the indicator variables in the traditional spike and slab prior with continuous, Beta-distributed random variables and places half Cauchy priors over the parameters of the Beta distribution, which significantly improves the predictive and inferential performance of the technique. Similar to shrinkage methods, our continuous parameterization of the spike and slab prior enables us explore the posterior distributions of interest using fast gradient-based methods, such as Hamiltonian Monte Carlo (HMC), while at the same time explicitly allowing for variable selection in a principled framework. We study the frequentist properties of our model via simulation and show that our technique outperforms the latest Bayesian variable selection methods in both linear and logistic regression. The efficacy, applicability and performance of our approach, are further underscored through its implementation on real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07061v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linduni M. Rodrigo, Robert Kohn, Hadi M. Afshar, Sally Cripps</dc:creator>
    </item>
    <item>
      <title>Sequential Methods for Error Correction of Probabilistic Wind Power Forecasts</title>
      <link>https://arxiv.org/abs/2501.14805</link>
      <description>arXiv:2501.14805v2 Announce Type: replace 
Abstract: Reliable probabilistic production forecasts are required to better manage the uncertainty that the rapid build-out of wind power capacity adds to future energy systems. In this article, we consider sequential methods to correct errors in power production forecast ensembles derived from numerical weather predictions. We propose combining neural networks with time-adaptive quantile regression to enhance the accuracy of wind power forecasts. We refer to this approach as Neural Adaptive Basis for (time-adaptive) Quantile Regression or NABQR. First, we use NABQR to correct power production ensembles with neural networks. We find that Long Short-Term Memory networks are the most effective architecture for this purpose. Second, we apply time-adaptive quantile regression to the corrected ensembles to obtain optimal median predictions along with quantiles of the forecast distribution. With the suggested method we achieve accuracy improvements up to 40% in mean absolute terms in an application to day-ahead forecasting of on- and offshore wind power production in Denmark. In addition, we explore the value of our method for applications in energy trading. We have implemented the NABQR method as an open-source Python package to support applications in renewable energy forecasting and future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14805v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastian Schmidt J{\o}rgensen, Jan Kloppenborg M{\o}ller, Peter Nystrup, Henrik Madsen</dc:creator>
    </item>
    <item>
      <title>SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation On Diverse Modalities</title>
      <link>https://arxiv.org/abs/2407.11676</link>
      <description>arXiv:2407.11676v2 Announce Type: replace-cross 
Abstract: Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a labeled source domain to perform well on an unlabeled target domain with some data distribution shift. While many methods have been proposed in the literature, fair and realistic evaluation remains an open question, particularly due to methodological difficulties in selecting hyperparameters in the unsupervised setting. With SKADA-bench, we propose a framework to evaluate DA methods on diverse modalities, beyond computer vision task that have been largely explored in the literature. We present a complete and fair evaluation of existing shallow algorithms, including reweighting, mapping, and subspace alignment. Realistic hyperparameter selection is performed with nested cross-validation and various unsupervised model selection scores, on both simulated datasets with controlled shifts and real-world datasets across diverse modalities, such as images, text, biomedical, and tabular data. Our benchmark highlights the importance of realistic validation and provides practical guidance for real-life applications, with key insights into the choice and impact of model selection approaches. SKADA-bench is open-source, reproducible, and can be easily extended with novel DA methods, datasets, and model selection criteria without requiring re-evaluating competitors. SKADA-bench is available on Github at https://github.com/scikit-adaptation/skada-bench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11676v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 07 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanis Lalou, Th\'eo Gnassounou, Antoine Collas, Antoine de Mathelin, Oleksii Kachaiev, Ambroise Odonnat, Alexandre Gramfort, Thomas Moreau, R\'emi Flamary</dc:creator>
    </item>
  </channel>
</rss>
