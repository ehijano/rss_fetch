<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 07 Apr 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>What People Share With a Robot When Feeling Lonely and Stressed and How It Helps Over Time</title>
      <link>https://arxiv.org/abs/2504.02991</link>
      <description>arXiv:2504.02991v1 Announce Type: new 
Abstract: Loneliness and stress are prevalent among young adults and are linked to significant psychological and health-related consequences. Social robots may offer a promising avenue for emotional support, especially when considering the ongoing advancements in conversational AI. This study investigates how repeated interactions with a social robot influence feelings of loneliness and perceived stress, and how such feelings are reflected in the themes of user disclosures towards the robot. Participants engaged in a five-session robot-led intervention, where a large language model powered QTrobot facilitated structured conversations designed to support cognitive reappraisal. Results from linear mixed-effects models show significant reductions in both loneliness and perceived stress over time. Additionally, semantic clustering of 560 user disclosures towards the robot revealed six distinct conversational themes. Results from a Kruskal-Wallis H-test demonstrate that participants reporting higher loneliness and stress more frequently engaged in socially focused disclosures, such as friendship and connection, whereas lower distress was associated with introspective and goal-oriented themes (e.g., academic ambitions). By exploring both how the intervention affects well-being, as well as how well-being shapes the content of robot-directed conversations, we aim to capture the dynamic nature of emotional support in huma-robot interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02991v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Laban, Sophie Chiang, Hatice Gunes</dc:creator>
    </item>
    <item>
      <title>A Review of Prototyping in XR: Linking Extended Reality to Digital Fabrication</title>
      <link>https://arxiv.org/abs/2504.02998</link>
      <description>arXiv:2504.02998v1 Announce Type: new 
Abstract: Extended Reality (XR) has expanded the horizons of entertainment and social life and shows great potential in the manufacturing industry. Prototyping in XR can help designers make initial proposals and iterations at low cost before manufacturers and investors decide whether to invest in research, development or even production. According to the literature (54 manuscripts in the last 15 years) prototyping in XR in XR is easier to use than three-dimensional (3D) modeling with a personal computer and more capable of displaying 3D structures than paper drawing. In this comprehensive review, we systematically surveyed the literature on prototyping in XR and discussed the possibility of transferring created virtual prototypes from XR to commonly used 3D modeling software and reality. We proposed five research questions regarding prototyping in XR. They are: what the constituent elements and workflow of prototyping are; which display devices can deliver satisfying immersive and interactive experiences; how user control input is obtained and what methods are available for users to interact with virtual elements and create XR prototypes; what approaches can facilitate the connection with fabrication to ensure a smooth transition from the virtual to the physical world; and what the challenges are and what the future holds for this research domain. Based on these questions, we summarized the components and workflows of prototyping in XR. Moreover, we present an overview of the latest trends in display device evolution, control technologies, digital model construction, and manufacturing processes. In view of these latest developments and gaps, we speculated on the challenges and opportunities in the field of prototyping in XR, especially in linking extended reality to digital fabrication, with the aim of guiding researchers towards new research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02998v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bixun Chen, Shaun Macdonald, Moataz Attallah, Paul Chapman, Rami Ghannam</dc:creator>
    </item>
    <item>
      <title>Quantifying Personality in Human-Drone Interactions for Building Heat Loss Inspection with Virtual Reality Training</title>
      <link>https://arxiv.org/abs/2504.03014</link>
      <description>arXiv:2504.03014v1 Announce Type: new 
Abstract: Reliable building energy audits are crucial for efficiency through heat loss detection. While drones assist inspections, they overlook the interplay between personality traits, stress management, and operational strategies expert engineers employ. This gap, combined with workforce shortages, necessitates effective knowledge transfer. This study proposes a VR-based training system for human-drone interaction in building heat loss inspection. Participants piloted a virtual drone with a thermographic monitor to identify defects. By analyzing flight patterns, stress adaptation, and inspection performance across diverse trainees, we found: (1) Flight Trajectories - Extraverts, Intuitives, Feelers, and Perceivers explored larger areas but showed higher misclassification rates, while Introverts, Sensors, Thinkers, and Judgers demonstrated methodical approaches. (2) Stress Adaptation - Heart rate variability revealed broader stress fluctuations among Extraverts, Intuitives, Feelers, and Perceivers, whereas Introverts, Sensors, Thinkers, and Judgers maintained steadier responses. Task complexity magnified these differences. (3) Inspection Performance - Extraverts, Intuitives, and Feelers achieved higher recall but over-identified defects. Introverts, Sensors, Thinkers, and Judgers made fewer random errors but risked overlooking subtle heat losses. These insights highlight the interplay among personality traits, stress management, and operational strategies in VR training for drone-assisted audits. The framework shows potential for addressing workforce shortages by facilitating knowledge transfer and optimizing human-drone collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03014v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pengkun Liu, Pingbo Tang, Jiepeng Liu, Yu Hou</dc:creator>
    </item>
    <item>
      <title>Ontologies in Design: How Imagining a Tree Reveals Possibilites and Assumptions in Large Language Models</title>
      <link>https://arxiv.org/abs/2504.03029</link>
      <description>arXiv:2504.03029v1 Announce Type: new 
Abstract: Amid the recent uptake of Generative AI, sociotechnical scholars and critics have traced a multitude of resulting harms, with analyses largely focused on values and axiology (e.g., bias). While value-based analyses are crucial, we argue that ontologies -- concerning what we allow ourselves to think or talk about -- is a vital but under-recognized dimension in analyzing these systems. Proposing a need for a practice-based engagement with ontologies, we offer four orientations for considering ontologies in design: pluralism, groundedness, liveliness, and enactment. We share examples of potentialities that are opened up through these orientations across the entire LLM development pipeline by conducting two ontological analyses: examining the responses of four LLM-based chatbots in a prompting exercise, and analyzing the architecture of an LLM-based agent simulation. We conclude by sharing opportunities and limitations of working with ontologies in the design and development of sociotechnical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03029v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713633</arxiv:DOI>
      <dc:creator>Nava Haghighi, Sunny Yu, James Landay, Daniela Rosner</dc:creator>
    </item>
    <item>
      <title>Design of AI-Powered Tool for Self-Regulation Support in Programming Education</title>
      <link>https://arxiv.org/abs/2504.03068</link>
      <description>arXiv:2504.03068v1 Announce Type: new 
Abstract: Large Language Model (LLM) tools have demonstrated their potential to deliver high-quality assistance by providing instant, personalized feedback that is crucial for effective programming education. However, many of these tools operate independently from institutional Learning Management Systems, which creates a significant disconnect. This isolation limits the ability to leverage learning materials and exercise context for generating tailored, context-aware feedback. Furthermore, previous research on self-regulated learning and LLM support mainly focused on knowledge acquisition, not the development of important self-regulation skills. To address these challenges, we developed CodeRunner Agent, an LLM-based programming assistant that integrates the CodeRunner, a student-submitted code executing and automated grading plugin in Moodle. CodeRunner Agent empowers educators to customize AI-generated feedback by incorporating detailed context from lecture materials, programming questions, student answers, and execution results. Additionally, it enhances students' self-regulated learning by providing strategy-based AI responses. This integrated, context-aware, and skill-focused approach offers promising avenues for data-driven improvements in programming education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03068v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huiyong Li, Boxuan Ma</dc:creator>
    </item>
    <item>
      <title>Symbiotic AI: Augmenting Human Cognition from PCs to Cars</title>
      <link>https://arxiv.org/abs/2504.03105</link>
      <description>arXiv:2504.03105v1 Announce Type: new 
Abstract: As AI takes on increasingly complex roles in human-computer interaction, fundamental questions arise: how can HCI help maintain the user as the primary agent while augment human cognition and intelligence? This paper suggests questions to guide researchers in considering the implications for agency, autonomy, the augmentation of human intellect, and the future of human-AI synergies. We observe a key paradigm shift behind the transformation of HCI, shifting from explicit command-and-control models to systems where users define high-level goals directly. This shift will be facilitated by XR technologies, whose multi-modal inputs and outputs offer a more seamless way to convey these goals. This paper considers this transformation through the lens of two cultural milestones: the personal computer and the automobile, moving beyond traditional interfaces like keyboards or steering wheels and thinking of them as vessels for everyday XR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03105v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Bovo, Karan Ahuja, Ryo Suzuki, Mustafa Doga Dogan, Mar Gonzalez-Franco</dc:creator>
    </item>
    <item>
      <title>See-Through Face Display for DHH People: Enhancing Gaze Awareness in Remote Sign Language Conversations with Camera-Behind Displays</title>
      <link>https://arxiv.org/abs/2504.03141</link>
      <description>arXiv:2504.03141v1 Announce Type: new 
Abstract: This paper presents a sign language conversation system based on the See-Through Face Display to address the challenge of maintaining eye contact in remote sign language interactions. A camera positioned behind a transparent display allows users to look at the face of their conversation partner while appearing to maintain direct eye contact. Unlike conventional methods that rely on software-based gaze correction or large-scale half-mirror setups, this design reduces visual distortions and simplifies installation. We implemented and evaluated a videoconferencing system that integrates See-Through Face Display, comparing it to traditional videoconferencing methods. We explore its potential applications for Deaf and Hard of Hearing (DHH), including multi-party sign language conversations, corpus collection, remote interpretation, and AI-driven sign language avatars. Collaboration with DHH communities will be key to refining the system for real-world use and ensuring its practical deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03141v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3715669.3726848</arxiv:DOI>
      <dc:creator>Kazuya Izumi, Akihisa Shitara, Yoichi Ochiai</dc:creator>
    </item>
    <item>
      <title>A Human Digital Twin Architecture for Knowledge-based Interactions and Context-Aware Conversations</title>
      <link>https://arxiv.org/abs/2504.03147</link>
      <description>arXiv:2504.03147v1 Announce Type: new 
Abstract: Recent developments in Artificial Intelligence (AI) and Machine Learning (ML) are creating new opportunities for Human-Autonomy Teaming (HAT) in tasks, missions, and continuous coordinated activities. A major challenge is enabling humans to maintain awareness and control over autonomous assets, while also building trust and supporting shared contextual understanding. To address this, we present a real-time Human Digital Twin (HDT) architecture that integrates Large Language Models (LLMs) for knowledge reporting, answering, and recommendation, embodied in a visual interface.
  The system applies a metacognitive approach to enable personalized, context-aware responses aligned with the human teammate's expectations. The HDT acts as a visually and behaviorally realistic team member, integrated throughout the mission lifecycle, from training to deployment to after-action review. Our architecture includes speech recognition, context processing, AI-driven dialogue, emotion modeling, lip-syncing, and multimodal feedback. We describe the system design, performance metrics, and future development directions for more adaptive and realistic HAT systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03147v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.17605/OSF.IO/KEH9T</arxiv:DOI>
      <dc:creator>Abdul Mannan Mohammed, Azhar Ali Mohammad, Jason A. Ortiz, Carsten Neumann, Grace Bochenek, Dirk Reiners, Carolina Cruz-Neira</dc:creator>
    </item>
    <item>
      <title>Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted Decision-Making</title>
      <link>https://arxiv.org/abs/2504.03207</link>
      <description>arXiv:2504.03207v1 Announce Type: new 
Abstract: How can we use generative AI to design tools that augment rather than replace human cognition? In this position paper, we review our own research on AI-assisted decision-making for lessons to learn. We observe that in both AI-assisted decision-making and generative AI, a popular approach is to suggest AI-generated end-to-end solutions to users, which users can then accept, reject, or edit. Alternatively, AI tools could offer more incremental support to help users solve tasks themselves, which we call process-oriented support. We describe findings on the challenges of end-to-end solutions, and how process-oriented support can address them. We also discuss the applicability of these findings to generative AI based on a recent study in which we compared both approaches to assist users in a complex decision-making task with LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03207v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zelun Tony Zhang, Leon Reicherts</dc:creator>
    </item>
    <item>
      <title>Improving Clinical Imaging Systems using Cognition based Approaches</title>
      <link>https://arxiv.org/abs/2504.03251</link>
      <description>arXiv:2504.03251v1 Announce Type: new 
Abstract: Clinical systems operate in safety-critical environments and are not intended to function autonomously; however, they are currently designed to replicate clinicians' diagnoses rather than assist them in the diagnostic process. To enable better supervision of system-generated diagnoses, we replicate radiologists' systematic approach used to analyze chest X-rays. This approach facilitates comprehensive analysis across all regions of clinical images and can reduce errors caused by inattentional blindness and under reading. Our work addresses a critical research gap by identifying difficult-to-diagnose diseases for clinicians using insights from human vision, enabling these systems to serve as an effective "second pair of eyes". These improvements make the clinical imaging systems more complementary and combine the strengths of human and machine vision. Additionally, we leverage effective receptive fields in deep learning models to present machine-generated diagnoses with sufficient context, making it easier for clinicians to evaluate them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03251v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kailas Dayanandan, Brejesh Lall</dc:creator>
    </item>
    <item>
      <title>Ultra-low-power ring-based wireless mouse</title>
      <link>https://arxiv.org/abs/2504.03253</link>
      <description>arXiv:2504.03253v1 Announce Type: new 
Abstract: Wireless mouse rings offer subtle, reliable pointing interactions for wearable computing platforms. However, the small battery below 27 mAh in the miniature rings restricts the ring's continuous lifespan to just 1-10 hours, because current low-powered wireless communication such as BLE is power-consuming for ring's continuous use. The ring's short lifespan frequently disrupts users' mouse use with the need for frequent charging. This paper presents picoRing mouse, enabling a continuous ring-based mouse interaction with ultra-low-powered ring-to-wristband wireless communication. picoRing mouse employs a coil-based impedance sensing named semi-passive inductive telemetry, allowing a wristband coil to capture a unique frequency response of a nearby ring coil via a sensitive inductive coupling between the coils. The ring coil converts the corresponding user's mouse input into the unique frequency response via an up to 449 uW mouse-driven modulation system. Therefore, the continuous use of picoRing mouse can last approximately 600 (8hrs use/day)-1000 (4hrs use/day) hours on a single charge of a 27 mAh battery while supporting subtle thumb-to-index scrolling and pressing interactions in real-world wearable computing situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03253v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Li, Masaaki Fukumoto, Mohamed Kari, Shigemi Ishida, Akihito Noda, Tomoyuki Yokota, Takao Someya, Yoshihiro Kawahara, Ryo Takahashi</dc:creator>
    </item>
    <item>
      <title>How to Test for Compliance with Human Oversight Requirements in AI Regulation?</title>
      <link>https://arxiv.org/abs/2504.03300</link>
      <description>arXiv:2504.03300v1 Announce Type: new 
Abstract: Human oversight requirements are a core component of the European AI Act and in AI governance. In this paper, we highlight key challenges in testing for compliance with these requirements. A key difficulty lies in balancing simple, but potentially ineffective checklist-based approaches with resource-intensive empirical testing in diverse contexts where humans oversee AI systems. Additionally, the absence of easily operationalizable standards and the context-dependent nature of human oversight further complicate compliance testing. We argue that these challenges illustrate broader challenges in the future of sociotechnical AI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03300v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Langer, Veronika Lazar, Kevin Baum</dc:creator>
    </item>
    <item>
      <title>Virtualizing a Collaboration Task as an Interactable Environment and Installing it on Real World</title>
      <link>https://arxiv.org/abs/2504.03375</link>
      <description>arXiv:2504.03375v1 Announce Type: new 
Abstract: This paper proposes a novel approach to scaling distributed collaboration in mixed reality by virtualizing collaborative tasks as independent, installable environments. By mapping group activities into dedicated virtual spaces that adapt to each user's real-world context, the proposed method supports consistent MR interactions, dynamic group engagement, and seamless task transitions. Preliminary studies in individual ideation demonstrate enhanced immersion and productivity, paving the way for future multi-user collaborative systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03375v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Euijun Jung, Youngki Lee</dc:creator>
    </item>
    <item>
      <title>Managing Information Overload in Large-Scale Distributed Mixed-Reality Meetings</title>
      <link>https://arxiv.org/abs/2504.03455</link>
      <description>arXiv:2504.03455v1 Announce Type: new 
Abstract: Large-scale distributed mixed-reality meetings involve many people and their audiovisual representations. These collaborative environments can introduce challenges such as sensory overload, cognitive strain, and social fatigue. In this paper, we discuss how the unique adaptability of Mixed Reality can be leveraged to weaken these stressors by managing information overload.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03455v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katja Krug, Wolfgang B\"uschel, Mats Ole Ellenberg</dc:creator>
    </item>
    <item>
      <title>Survey and synthesis of state of the art in driver monitoring</title>
      <link>https://arxiv.org/abs/2110.00472</link>
      <description>arXiv:2110.00472v1 Announce Type: cross 
Abstract: Road-vehicle accidents are mostly due to human errors, and many such accidents could be avoided by continuously monitoring the driver. Driver monitoring (DM) is a topic of growing interest in the automotive industry, and it will remain relevant for all vehicles that are not fully autonomous, and thus for decades for the average vehicle owner. The present paper focuses on the first step of DM, which consists in characterizing the state of the driver. Since DM will be increasingly linked to driving automation (DA), this paper presents a clear view of the role of DM at each of the six SAE levels of DA. This paper surveys the state of the art of DM, and then synthesizes it, providing a unique, structured, polychotomous view of the many characterization techniques of DM. Informed by the survey, the paper characterizes the driver state along the five main dimensions--called here "(sub)states"--of drowsiness, mental workload, distraction, emotions, and under the influence. The polychotomous view of DM is presented through a pair of interlocked tables that relate these states to their indicators (e.g., the eye-blink rate) and the sensors that can access each of these indicators (e.g., a camera). The tables factor in not only the effects linked directly to the driver, but also those linked to the (driven) vehicle and the (driving) environment. They show, at a glance, to concerned researchers, equipment providers, and vehicle manufacturers (1) most of the options they have to implement various forms of advanced DM systems, and (2) fruitful areas for further research and innovation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.00472v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>eess.IV</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3390/s21165558</arxiv:DOI>
      <arxiv:journal_reference>Sensors 2021, 21(16):5558</arxiv:journal_reference>
      <dc:creator>Ana\"is Halin, Jacques G. Verly, Marc Van Droogenbroeck</dc:creator>
    </item>
    <item>
      <title>Processes Matter: How ML/GAI Approaches Could Support Open Qualitative Coding of Online Discourse Datasets</title>
      <link>https://arxiv.org/abs/2504.02887</link>
      <description>arXiv:2504.02887v1 Announce Type: cross 
Abstract: Open coding, a key inductive step in qualitative research, discovers and constructs concepts from human datasets. However, capturing extensive and nuanced aspects or "coding moments" can be challenging, especially with large discourse datasets. While some studies explore machine learning (ML)/Generative AI (GAI)'s potential for open coding, few evaluation studies exist. We compare open coding results by five recently published ML/GAI approaches and four human coders, using a dataset of online chat messages around a mobile learning software. Our systematic analysis reveals ML/GAI approaches' strengths and weaknesses, uncovering the complementary potential between humans and AI. Line-by-line AI approaches effectively identify content-based codes, while humans excel in interpreting conversational dynamics. We discussed how embedded analytical processes could shape the results of ML/GAI approaches. Instead of replacing humans in open coding, researchers should integrate AI with and according to their analytical processes, e.g., as parallel co-coders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02887v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Chen, Alexandros Lotsos, Grace Wang, Lexie Zhao, Bruce Sherin, Uri Wilensky, Michael Horn</dc:creator>
    </item>
    <item>
      <title>The Use of Gaze-Derived Confidence of Inferred Operator Intent in Adjusting Safety-Conscious Haptic Assistance</title>
      <link>https://arxiv.org/abs/2504.03098</link>
      <description>arXiv:2504.03098v1 Announce Type: cross 
Abstract: Humans directly completing tasks in dangerous or hazardous conditions is not always possible where these tasks are increasingly be performed remotely by teleoperated robots. However, teleoperation is difficult since the operator feels a disconnect with the robot caused by missing feedback from several senses, including touch, and the lack of depth in the video feedback presented to the operator. To overcome this problem, the proposed system actively infers the operator's intent and provides assistance based on the predicted intent. Furthermore, a novel method of calculating confidence in the inferred intent modifies the human-in-the-loop control. The operator's gaze is employed to intuitively indicate the target before the manipulation with the robot begins. A potential field method is used to provide a guiding force towards the intended target, and a safety boundary reduces risk of damage. Modifying these assistances based on the confidence level in the operator's intent makes the control more natural, and gives the robot an intuitive understanding of its human master. Initial validation results show the ability of the system to improve accuracy, execution time, and reduce operator error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03098v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jeremy D. Webb, Michael Bowman, Songpo Li, Xiaoli Zhang</dc:creator>
    </item>
    <item>
      <title>Data Augmentation of Time-Series Data in Human Movement Biomechanics: A Scoping Review</title>
      <link>https://arxiv.org/abs/2504.03334</link>
      <description>arXiv:2504.03334v1 Announce Type: cross 
Abstract: The integration of machine learning and deep learning has transformed data analytics in biomechanics, enabled by extensive wearable sensor data. However, the field faces challenges such as limited large-scale datasets and high data acquisition costs, which hinder the development of robust algorithms. Data augmentation techniques show promise in addressing these issues, but their application to biomechanical time-series data requires comprehensive evaluation.
  This scoping review investigates data augmentation methods for time-series data in the biomechanics domain. It analyzes current approaches for augmenting and generating time-series datasets, evaluates their effectiveness, and offers recommendations for applying these techniques in biomechanics.
  Four databases, PubMed, IEEE Xplore, Scopus, and Web of Science, were searched for studies published between 2013 and 2024. Following PRISMA-ScR guidelines, a two-stage screening identified 21 relevant publications.
  Results show that there is no universally preferred method for augmenting biomechanical time-series data; instead, methods vary based on study objectives. A major issue identified is the absence of soft tissue artifacts in synthetic data, leading to discrepancies referred to as the synthetic gap. Moreover, many studies lack proper evaluation of augmentation methods, making it difficult to assess their effects on model performance and data quality.
  This review highlights the critical role of data augmentation in addressing limited dataset availability and improving model generalization in biomechanics. Tailoring augmentation strategies to the characteristics of biomechanical data is essential for advancing predictive modeling. A better understanding of how different augmentation methods impact data quality and downstream tasks will be key to developing more effective and realistic techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03334v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christina Halmich, Lucas H\"oschler, Christoph Schranz, Christian Borgelt</dc:creator>
    </item>
    <item>
      <title>Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered Chatbots on the Web</title>
      <link>https://arxiv.org/abs/2504.03343</link>
      <description>arXiv:2504.03343v1 Announce Type: cross 
Abstract: Integrated into websites, LLM-powered chatbots offer alternative means of navigation and information retrieval, leading to a shift in how users access information on the web. Yet, predominantly closed-sourced solutions limit proliferation among web hosts and suffer from a lack of transparency with regard to implementation details and energy efficiency. In this work, we propose our openly available agent Talk2X leveraging an adapted retrieval-augmented generation approach (RAG) combined with an automatically generated vector database, benefiting energy efficiency. Talk2X's architecture is generalizable to arbitrary websites offering developers a ready to use tool for integration. Using a mixed-methods approach, we evaluated Talk2X's usability by tasking users to acquire specific assets from an open science repository. Talk2X significantly improved task completion time, correctness, and user experience supporting users in quickly pinpointing specific information as compared to standard user-website interaction. Our findings contribute technical advancements to an ongoing paradigm shift of how we access information on the web.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03343v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars Krupp, Daniel Gei{\ss}ler, Peter Hevesi, Marco Hirsch, Paul Lukowicz, Jakob Karolus</dc:creator>
    </item>
    <item>
      <title>Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings</title>
      <link>https://arxiv.org/abs/2504.03352</link>
      <description>arXiv:2504.03352v1 Announce Type: cross 
Abstract: Stereotypes are known to be highly pernicious, making their detection critically important. However, current research predominantly focuses on detecting and evaluating stereotypical biases in LLMs, leaving the study of stereotypes in its early stages. Many studies have failed to clearly distinguish between stereotypes and stereotypical biases, which has significantly slowed progress in advancing research in this area. Stereotype and anti-stereotype detection is a problem that requires knowledge of society; hence, it is one of the most difficult areas in Responsible AI. This work investigates this task, where we propose a four-tuple definition and provide precise terminology distinguishing stereotype, anti-stereotype, stereotypical bias, and bias, offering valuable insights into their various aspects. In this paper, we propose StereoDetect, a high-quality benchmarking dataset curated for this task by optimally utilizing current datasets such as StereoSet and WinoQueer, involving a manual verification process and the transfer of semantic information. We demonstrate that language models for reasoning with fewer than 10B parameters often get confused when detecting anti-stereotypes. We also demonstrate the critical importance of well-curated datasets by comparing our model with other current models for stereotype detection. The dataset and code is available at https://github.com/KaustubhShejole/StereoDetect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03352v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaustubh Shivshankar Shejole, Pushpak Bhattacharyya</dc:creator>
    </item>
    <item>
      <title>Walk along: An Experiment on Controlling the Mobile Robot 'Spot' with Voice and Gestures</title>
      <link>https://arxiv.org/abs/2407.11218</link>
      <description>arXiv:2407.11218v4 Announce Type: replace 
Abstract: Robots are becoming more capable and can autonomously perform tasks such as navigating between locations. However, human oversight remains crucial. This study compared two touchless methods for directing mobile robots: voice control and gesture control, to investigate the efficiency of the methods and the preference of users. We tested these methods in two conditions: one in which participants remained stationary and one in which they walked freely alongside the robot. We hypothesized that walking alongside the robot would result in higher intuitiveness ratings and improved task performance, based on the idea that walking promotes spatial alignment and reduces the effort required for mental rotation. In a 2x2 within-subject design, 218 participants guided the quadruped robot Spot along a circuitous route with multiple 90-degree turns using rotate left, rotate right, and walk forward commands. After each trial, participants rated the intuitiveness of the command mapping, while post-experiment interviews were used to gather the participants' preferences. Results showed that voice control combined with walking with Spot was the most favored and intuitive, whereas gesture control while standing caused confusion for left/right commands. Nevertheless, 29% of participants preferred gesture control, citing increased task engagement and visual congruence as reasons. An odometry-based analysis revealed that participants often followed behind Spot, particularly in the gesture control condition, when they were allowed to walk. In conclusion, voice control with walking produced the best outcomes. Improving physical ergonomics and adjusting gesture types could make gesture control more effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11218v4</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Renchi Zhang, Jesse van der Linden, Dimitra Dodou, Harleigh Seyffert, Yke Bauke Eisma, Joost C. F. de Winter</dc:creator>
    </item>
    <item>
      <title>How Do Teachers Create Pedagogical Chatbots?: Current Practices and Challenges</title>
      <link>https://arxiv.org/abs/2503.00967</link>
      <description>arXiv:2503.00967v2 Announce Type: replace 
Abstract: AI chatbots have emerged as promising educational tools for personalized learning experiences, with advances in large language models (LLMs) enabling teachers to create and customize these chatbots for their specific classroom needs. However, there is a limited understanding of how teachers create pedagogical chatbots and integrate them into their lessons. Through semi-structured interviews with seven K-12 teachers, we examined their practices and challenges when designing, implementing, and deploying chatbots. Our findings revealed that teachers prioritize developing task-specific chatbots aligned with their lessons. Teachers engaged in various creation practices and had different challenges; novices in chatbot creation struggled mainly with initial design and technical implementation, while experienced teachers faced challenges with technical aspects and analyzing conversational data. Based on these insights, we explore approaches to supporting teachers' chatbot development and opportunities for designing future chatbot creation systems. This work provides foundational insights from teachers that can empower teacher-created chatbots, facilitating AI-augmented teaching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00967v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Minju Yoo, Hyoungwook Jin, Juho Kim</dc:creator>
    </item>
    <item>
      <title>A Survey on Moral Foundation Theory and Pre-Trained Language Models: Current Advances and Challenges</title>
      <link>https://arxiv.org/abs/2409.13521</link>
      <description>arXiv:2409.13521v2 Announce Type: replace-cross 
Abstract: Moral values have deep roots in early civilizations, codified within norms and laws that regulated societal order and the common good. They play a crucial role in understanding the psychological basis of human behavior and cultural orientation. The Moral Foundation Theory (MFT) is a well-established framework that identifies the core moral foundations underlying the manner in which different cultures shape individual and social lives. Recent advancements in natural language processing, particularly Pre-trained Language Models (PLMs), have enabled the extraction and analysis of moral dimensions from textual data. This survey presents a comprehensive review of MFT-informed PLMs, providing an analysis of moral tendencies in PLMs and their application in the context of the MFT. We also review relevant datasets and lexicons and discuss trends, limitations, and future directions. By providing a structured overview of the intersection between PLMs and MFT, this work bridges moral psychology insights within the realm of PLMs, paving the way for further research and development in creating morally aware AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13521v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00146-025-02225-w</arxiv:DOI>
      <arxiv:journal_reference>AI &amp; Society, March 2025</arxiv:journal_reference>
      <dc:creator>Lorenzo Zangari, Candida M. Greco, Davide Picca, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>AI red-teaming is a sociotechnical challenge: on values, labor, and harms</title>
      <link>https://arxiv.org/abs/2412.09751</link>
      <description>arXiv:2412.09751v2 Announce Type: replace-cross 
Abstract: As generative AI technologies find more and more real-world applications, the importance of testing their performance and safety seems paramount. "Red-teaming" has quickly become the primary approach to test AI models--prioritized by AI companies, and enshrined in AI policy and regulation. Members of red teams act as adversaries, probing AI systems to test their safety mechanisms and uncover vulnerabilities. Yet we know far too little about this work or its implications. This essay calls for collaboration between computer scientists and social scientists to study the sociotechnical systems surrounding AI technologies, including the work of red-teaming, to avoid repeating the mistakes of the recent past. We highlight the importance of understanding the values and assumptions behind red-teaming, the labor arrangements involved, and the psychological impacts on red-teamers, drawing insights from the lessons learned around the work of content moderation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09751v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarleton Gillespie, Ryland Shaw, Mary L. Gray, Jina Suh</dc:creator>
    </item>
    <item>
      <title>High-Performance Vision-Based Tactile Sensing Enhanced by Microstructures and Lightweight CNN</title>
      <link>https://arxiv.org/abs/2412.20758</link>
      <description>arXiv:2412.20758v3 Announce Type: replace-cross 
Abstract: Tactile sensing is critical in advanced interactive systems by emulating the human sense of touch to detect stimuli. Vision-based tactile sensors are promising for providing multimodal capabilities and high robustness, yet existing technologies still have limitations in sensitivity, spatial resolution, and high computational demands of deep learning-based image processing. This paper presents a comprehensive approach combining a novel microstructure-based sensor design and efficient image processing, demonstrating that carefully engineered microstructures can significantly enhance performance while reducing computational load. Without traditional tracking markers, our sensor incorporates an surface with micromachined trenches, as an example of microstructures, which modulate light transmission and amplify the response to applied force. The amplified image features can be extracted by a ultra lightweight convolutional neural network to accurately inferring contact location, displacement, and applied force with high precision. Through theoretical analysis, we demonstrated that the micro trenches significantly amplified the visual effects of shape distortion. Using only a commercial webcam, the sensor system effectively detected forces below 5 mN, and achieved a millimetre-level single-point spatial resolution. Using a model with only one convolutional layer, a mean absolute error below 0.05 mm was achieved. Its soft sensor body allows seamless integration with soft robots, while its immunity to electrical crosstalk and interference guarantees reliability in complex human-machine environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20758v3</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mayue Shi, Yongqi Zhang, Xiaotong Guo, Eric M. Yeatman</dc:creator>
    </item>
    <item>
      <title>Streaming Generation of Co-Speech Gestures via Accelerated Rolling Diffusion</title>
      <link>https://arxiv.org/abs/2503.10488</link>
      <description>arXiv:2503.10488v2 Announce Type: replace-cross 
Abstract: Generating co-speech gestures in real time requires both temporal coherence and efficient sampling. We introduce Accelerated Rolling Diffusion, a novel framework for streaming gesture generation that extends rolling diffusion models with structured progressive noise scheduling, enabling seamless long-sequence motion synthesis while preserving realism and diversity. We further propose Rolling Diffusion Ladder Acceleration (RDLA), a new approach that restructures the noise schedule into a stepwise ladder, allowing multiple frames to be denoised simultaneously. This significantly improves sampling efficiency while maintaining motion consistency, achieving up to a 2x speedup with high visual fidelity and temporal coherence. We evaluate our approach on ZEGGS and BEAT, strong benchmarks for real-world applicability. Our framework is universally applicable to any diffusion-based gesture generation model, transforming it into a streaming approach. Applied to three state-of-the-art methods, it consistently outperforms them, demonstrating its effectiveness as a generalizable and efficient solution for real-time, high-fidelity co-speech gesture synthesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10488v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evgeniia Vu, Andrei Boiarov, Dmitry Vetrov</dc:creator>
    </item>
  </channel>
</rss>
