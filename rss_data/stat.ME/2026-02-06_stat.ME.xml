<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Feb 2026 05:01:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Penalized Likelihood Parameter Estimation for Differential Equation Models: A Computational Tutorial</title>
      <link>https://arxiv.org/abs/2602.04891</link>
      <description>arXiv:2602.04891v1 Announce Type: new 
Abstract: Parameter estimation connects mathematical models to real-world data and decision making across many scientific and industrial applications. Standard approaches such as maximum likelihood estimation and Markov chain Monte Carlo estimate parameters by repeatedly solving the model, which often requires numerical solutions of differential equation models. In contrast, generalized profiling (also called parameter cascading) focuses directly on the governing differential equation(s), linking the model and data through a penalized likelihood that explicitly measures both the data fit and model fit. Despite several advantages, generalized profiling is relatively rarely used in practice. This tutorial-style article outlines a set of self-directed computational exercises that facilitate skills development in applying generalized profiling to a range of ordinary differential equation models. All calculations can be repeated using reproducible open-source Jupyter notebooks that are available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04891v1</guid>
      <category>stat.ME</category>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew J Simpson, James S Bennett, Alexander Johnston, Ruth E Baker</dc:creator>
    </item>
    <item>
      <title>Double Variable Importance Matching to Estimate Distinct Causal Effects on Event Probability and Timing</title>
      <link>https://arxiv.org/abs/2602.05022</link>
      <description>arXiv:2602.05022v1 Announce Type: new 
Abstract: In many clinical contexts, estimating effects of treatment in time-to-event data is complicated not only by confounding, censoring, and heterogeneity, but also by the presence of a cured subpopulation in which the event of interest never occurs. In such settings, treatment may have distinct effects on (1) the probability of being cured and (2) the event timing among non-cured individuals. Standard survival analysis and causal inference methods typically do not separate cured from non-cured individuals, obscuring distinct treatment mechanisms on cure probability and event timing. To address these challenges, we propose a matching-based framework that constructs distinct match groups to estimate heterogeneous treatment effects (HTE) on cure probability and event timing, respectively. We use mixture cure models to identify feature importance for both estimands, which in turn informs weighted distance metrics for matching in high-dimensional spaces. Within matched groups, Kaplan-Meier estimators provide estimates of cure probability and expected time to event, from which individual-level treatment effects are derived. We provide theoretical guarantees for estimator consistency and distance metric optimality under an equal-scale constraint. We further decompose estimation error into contributions from censoring, model fitting, and irreducible noise. Simulations and real-world data analyses demonstrate that our approach delivers interpretable and robust HTE estimates in time-to-event settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05022v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuqi Li, Quinn Lanners, Matthew M. Engelhard</dc:creator>
    </item>
    <item>
      <title>Billions-Scale Forecast Reconciliation</title>
      <link>https://arxiv.org/abs/2602.05030</link>
      <description>arXiv:2602.05030v1 Announce Type: new 
Abstract: The problem of combining multiple forecasts of related quantities that obey expected equality and additivity constraints, often referred to a hierarchical forecast reconciliation, is naturally stated as a simple optimization problem. In this paper we explore optimization-based point forecast reconciliation at scales faced by large retailers. We implement and benchmark several algorithms to solve the forecast reconciliation problem, showing efficacy when the dimension of the problem exceeds four billion forecasted values. To the best of our knowledge, this is the largest forecast reconciliation problem, and perhaps on-par with the largest constrained least-squares-problem ever solved. We also make several theoretical contributions. We show that for a restricted class of problems and when the loss function is weighted appropriately, least-squares forecast reconciliation is equivalent to share-based forecast reconciliation. This formalizes how the optimization based approach can be thought of as a generalization of share-based reconciliation, applicable to multiple, overlapping data hierarchies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05030v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Tianyu (Tim),  Wang, Matthew C. Johnson, Steven Klee, Matthew L. Malloy</dc:creator>
    </item>
    <item>
      <title>A Weighting Framework for Clusters as Confounders in Observational Studies</title>
      <link>https://arxiv.org/abs/2602.05041</link>
      <description>arXiv:2602.05041v1 Announce Type: new 
Abstract: When units in observational studies are clustered in groups, such as students in schools or patients in hospitals, researchers often address confounding by adjusting for cluster-level covariates or cluster membership. In this paper, we develop a unified weighting framework that clarifies how different estimation methods control two distinct sources of imbalance: global balance (differences between treated and control units across clusters) and local balance (differences within clusters). We show that inverse propensity score weighting (IPW) with a random effects propensity score model -- the current standard in the literature -- targets only global balance and constant level shifts across clusters, but imposes no constraints on local balance. We then present two approaches that target both forms of balance. First, hierarchical balancing weights directly control global and local balance through a constrained optimization problem. Second, building on the recently proposed Generalized Mundlak approach, we develop a novel Mundlak balancing weights estimator that adjusts for cluster-level sufficient statistics rather than cluster indicators; this approach can accommodate small clusters where all units are treated or untreated. Critically, these approaches rest on different assumptions: hierarchical balancing weights require only that treatment is ignorable given covariates and cluster membership, while Mundlak methods additionally require an exponential family structure. We then compare these methods in a simulation study and in two applications in education and health services research that exhibit very different cluster structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05041v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Ben-Michael, Avi Feller, Luke Keele</dc:creator>
    </item>
    <item>
      <title>Impact Range Assessment (IRA): An Interpretable Sensitivity Measure for Regression Modelling</title>
      <link>https://arxiv.org/abs/2602.05239</link>
      <description>arXiv:2602.05239v1 Announce Type: new 
Abstract: While regression models capture the relationship between predictors and the response variable, they often lack intuitive accompanying methods to understand the influence of predictors on the outcome. To address this, we introduce an interpretability method called Impact Range Assessment (IRA), which quantifies the maximal influence of each predictor by measuring the total potential change in the response variable, across the predictor range. Validation using synthetic linear and nonlinear datasets demonstrates that relevant predictors produced higher IRA values than irrelevant ones. Moreover, repeated evaluations produced results closely aligned with those from the single-execution analysis, confirming the robustness of the method. A case study using a model that predicts pellet quality demonstrated that the IRA provides a simple and intuitive approach to interpret and rank predictor influence, thereby improving model transparency and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05239v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihao You (Department of Animal Biosciences, University of Guelph, Canada), Dan Tulpan (Department of Animal Biosciences, University of Guelph, Canada), Jiaojiao Diao (Department of Integrative Biology, University of Guelph, Canada), Jennifer L. Ellis (Department of Animal Biosciences, University of Guelph, Canada)</dc:creator>
    </item>
    <item>
      <title>Boxplots and quartile plots for grouped and periodic angular data</title>
      <link>https://arxiv.org/abs/2602.05335</link>
      <description>arXiv:2602.05335v1 Announce Type: new 
Abstract: Angular observations, or observations lying on the unit circle, arise in many disciplines and require special care in their description, analysis, interpretation and visualization. We provide methods to construct concentric circular boxplot displays of distributions of groups of angular data. The use of concentric boxplots brings challenges of visual perception, so we set the boxwidths to be inversely proportional to the square root of their distance from the centre. A perception survey supports this scaled boxwidth choice. For a large number of groups, we propose circular quartile plots. A three-dimensional toroidal display is also implemented for periodic angular distributions. We illustrate our methods on datasets in (1) psychology, to display motor resonance under different conditions, (2) genomics, to understand the distribution of peak phases for ancillary clock genes, and (3) meteorology and wind turbine power generation, to study the changing and periodic distribution of wind direction over the course of a year.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05335v1</guid>
      <category>stat.ME</category>
      <category>cs.GR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua D. Berlinski, Fan Dai, Ranjan Maitra</dc:creator>
    </item>
    <item>
      <title>A Flexible Modeling of Extremes in the Presence of Inliers</title>
      <link>https://arxiv.org/abs/2602.05351</link>
      <description>arXiv:2602.05351v1 Announce Type: new 
Abstract: Many random phenomena, including life-testing and environmental data, show positive values and excess zeros, which pose modeling challenges. In life testing, immediate failures result in zero lifetimes, often due to defects or poor quality, especially in electronics and clinical trials. These failures, called inliers at zero, are difficult to model using standard approaches. The presence and proportion of inliers may influence the accuracy of extreme value analysis, bias parameter estimates, or even lead to severe events or extreme effects, such as drought or crop failure. In such scenarios, a key issue in extreme value analysis is determining a suitable threshold to capture tail behaviour accurately. Although some extreme value mixture models address threshold and tail estimation, they often inadequately handle inliers, resulting in suboptimal results. Bulk model misspecification can affect the threshold, extreme value estimates, and, in particular, the tail proportion. There is no unified framework for defining extreme value mixture models, especially the tail proportion. This paper proposes a flexible model that handles extremes, inliers, and the tail proportion. Parameters are estimated using maximum likelihood estimation. Compared the proposed model estimates with the classical mean excess plot, parameter stability plot, and Pickands plot estimates. Theoretical results are established, and the proposed model outperforms traditional methods in both simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05351v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivshankar Nila, Ishapathik Das, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis for contamination in egocentric-network randomized trials with interference</title>
      <link>https://arxiv.org/abs/2602.05553</link>
      <description>arXiv:2602.05553v1 Announce Type: new 
Abstract: Egocentric-Network Randomized Trials (ENRTs) are increasingly used to estimate causal effects under interference when measuring complete sociocentric network data is infeasible. ENRTs rely on egocentric network sampling, where a set of egos is first sampled, and each ego recruits a subset of its neighbors as alters. Treatments are then randomized across egos. While the observed ego-networks are disjoint by design, the underlying population network may contain edges connecting them, leading to contamination. Under a design-based framework, we show that the Horvitz-Thompson estimators of direct and indirect effects are biased whenever contamination is present. To address this, we derive bias-corrected estimators and propose a novel sensitivity analysis framework based on sensitivity parameters representing the probability or expected number of missing edges. This framework is implemented via both grid sensitivity analysis and probabilistic bias analysis, providing researchers with a flexible tool to assess the robustness of the causal estimators to contamination. We apply our methodology to the HIV Prevention Trials Network 037 study, finding that ignoring contamination may lead to underestimation of indirect effects and overestimation of direct effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05553v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bar Weinstein, Daniel Nevo</dc:creator>
    </item>
    <item>
      <title>The stochastic view used in climate sciences: (some) perspectives from (some of) mathematical statistics</title>
      <link>https://arxiv.org/abs/2602.05611</link>
      <description>arXiv:2602.05611v1 Announce Type: new 
Abstract: Climate statistics is of course a very broad field, along with the many connections and impacts for yet other areas, with a history as long as mankind has been recording temperatures, describing drastic weather events, etc. The important work of Klaus Hasselmann, with crucial contributions to the field, along with various other connected strands of work,is being reviewed and discussed in other chapters. The aim of the present chapter is to point to a few statistical methodology themes of relevance for and joint interest with climate statistics. These themes, presented from a statistical methods perspective, include (i) more careful modelling and model selection strategies for meteorological type time series; (ii) methods for prediction, not only for future values of a time series, but for assessing when a trend might be crossing a barrier, along with relevant measures of uncertainty for these; (iii) climatic influence on marine biology; (iv) monitoring processes to assess whether and then to what extent models and their parameters have stayed reasonably constant over time; (v) combination of outputs from different information sources; and (vi) analysing probabilities and their uncertainties related to extreme events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05611v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Lid Hjort</dc:creator>
    </item>
    <item>
      <title>MixMashNet: An R Package for Single and Multilayer Networks</title>
      <link>https://arxiv.org/abs/2602.05716</link>
      <description>arXiv:2602.05716v1 Announce Type: new 
Abstract: The R package MixMashNet provides an integrated framework for estimating and analyzing single and multilayer networks using Mixed Graphical Models (MGMs), accommodating continuous, count, and categorical variables. In the multilayer setting, layers may comprise different types and numbers of variables, and users can explicitly impose a predefined multilayer topology. Bootstrap procedures are implemented to derive confidence intervals for edge weights and node-level centrality indices. In addition, the package includes tools to assess the stability of node community membership and to compute community scores that summarize the latent dimensions identified through network clustering. MixMashNet also offers interactive Shiny applications to support exploration, visualization, and interpretation of the estimated networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05716v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria De Martino, Federico Triolo, Adrien Perigord, Alice Margherita Ornago, Davide Liborio Vetrano, Caterina Gregorio</dc:creator>
    </item>
    <item>
      <title>Copula-based models for spatially dependent cylindrical data</title>
      <link>https://arxiv.org/abs/2602.05778</link>
      <description>arXiv:2602.05778v1 Announce Type: new 
Abstract: Cylindrical data frequently arise across various scientific disciplines, including meteorology (e.g., wind direction and speed), oceanography (e.g., marine current direction and speed or wave heights), ecology (e.g., telemetry), and medicine (e.g., seasonality and intensity in disease onset). Such data often occur as spatially correlated series of intensities and angles, thereby representing dependent bivariate response vectors of linear and circular components. To accommodate both the circular-linear dependence and spatial autocorrelation, while remaining flexible in marginal specifications, copula-based models for cylindrical data have been developed in the literature. However, existing approaches typically treat the copula parameters as constants unrelated to covariates, and regression specifications for marginal distributions are frequently restricted to linear predictors, thereby ignoring spatial correlation. In this work, we propose a structured additive conditional copula regression model for cylindrical data. The circular component is modeled using a wrapped Gaussian process, and the linear component follows a distributional regression model. Both components allow for the inclusion of linear covariate effects. Furthermore, by leveraging the empirical equivalence between Gaussian random fields (GRFs) and Gaussian Markov random fields, our approach avoids the computational burden typically associated with GRFs, while simultaneously allowing for non-stationarity in the covariance structure. Posterior estimation is performed via Markov chain Monte Carlo simulation. We evaluate the proposed model in a simulation study and subsequently in an analysis of wind directions and speed in Germany.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05778v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca Labanca, Anna Gottard, Nadja Klein</dc:creator>
    </item>
    <item>
      <title>Correcting Measurement Error and Zero Inflation in Functional Covariates for Scalar-on-Function Quantile Regression</title>
      <link>https://arxiv.org/abs/2602.05784</link>
      <description>arXiv:2602.05784v1 Announce Type: new 
Abstract: Wearable devices collect time-varying biobehavioral data, offering opportunities to investigate how behaviors influence health outcomes. However, these data often contain measurement error and excess zeros (due to nonwear, sedentary behavior, or connectivity issues), each characterized by subject-specific distributions. Current statistical methods fail to address these issues simultaneously. We introduce a novel modeling framework for zero-inflated and error-prone functional data by incorporating a subject-specific time-varying validity indicator that explicitly distinguishes structural zeros from intrinsic values. We iteratively estimate the latent functional covariates and zero-inflation probabilities via maximum likelihood, using basis expansions and linear mixed models to adjust for measurement error. To assess the effects of the recovered latent covariates, we apply joint quantile regression across multiple quantile levels. Through extensive simulations, we demonstrate that our approach significantly improves estimation accuracy over methods that only address measurement error, and joint estimation yields substantial improvements compared with fitting separate quantile regressions. Applied to a childhood obesity study, our approach effectively corrects for zero inflation and measurement error in step counts, yielding results that closely align with energy expenditure and supporting their use as a proxy for physical activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05784v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caihong Qin, Lan Xue, Ufuk Beyaztas, Roger S. Zoh, Mark Benden, Jeff Goldsmith, Carmen D. Tekwe</dc:creator>
    </item>
    <item>
      <title>Learning False Discovery Rate Control via Model-Based Neural Networks</title>
      <link>https://arxiv.org/abs/2602.05798</link>
      <description>arXiv:2602.05798v1 Announce Type: new 
Abstract: Controlling the false discovery rate (FDR) in high-dimensional variable selection requires balancing rigorous error control with statistical power. Existing methods with provable guarantees are often overly conservative, creating a persistent gap between the realized false discovery proportion (FDP) and the target FDR level. We introduce a learning-augmented enhancement of the T-Rex Selector framework that narrows this gap. Our approach replaces the analytical FDP estimator with a neural network trained solely on diverse synthetic datasets, enabling a substantially tighter and more accurate approximation of the FDP. This refinement allows the procedure to operate much closer to the desired FDR level, thereby increasing discovery power while maintaining effective approximate control. Through extensive simulations and a challenging synthetic genome-wide association study (GWAS), we demonstrate that our method achieves superior detection of true variables compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05798v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnau Vilella, Jasin Machkour, Michael Muma, Daniel P. Palomar</dc:creator>
    </item>
    <item>
      <title>SpARCD: A Spectral Graph Framework for Revealing Differential Functional Connectivity in fMRI Data</title>
      <link>https://arxiv.org/abs/2602.05807</link>
      <description>arXiv:2602.05807v1 Announce Type: new 
Abstract: Identifying brain regions that exhibit altered functional connectivity across cognitive or emotional states is a key problem in neuroscience. Existing methods, such as edge-wise testing, seed-based psychophysiological interaction (PPI) analysis, or correlation network comparison, typically suffer from low statistical power, arbitrary thresholding, and limited ability to capture distributed or nonlinear dependence patterns. We propose SpARCD (Spectral Analysis of Revealing Connectivity Differences), a novel statistical framework for detecting differences in brain connectivity between two experimental conditions. SpARCD leverages distance correlation, a dependence measure sensitive to both linear and nonlinear associations, to construct a weighted graph for each condition. It then constructs a differential operator via spectral filtering and uncovers connectivity changes by computing its leading eigenvectors. Inference is achieved via a permutation-based testing scheme that yields interpretable, region-level significance maps. Extensive simulation studies demonstrate that SpARCD achieves superior power relative to conventional edge-wise or univariate approaches, particularly in the presence of complex dependency structures. Application to fMRI data from 113 early PTSD patients performing an emotional face-matching task reveals distinct networks associated with emotional reactivity and regulatory processes. Overall, SpARCD provides a statistically rigorous and computationally efficient framework for comparing high-dimensional connectivity structures, with broad applicability to neuroimaging and other network-based scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05807v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shira Yoffe, Ziv Ben-Zion, Talma Hendler, Malka Gorfine, Ariel Jaffe</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to differential prevalence analysis with applications in microbiome studies</title>
      <link>https://arxiv.org/abs/2602.05938</link>
      <description>arXiv:2602.05938v1 Announce Type: new 
Abstract: Recent evidence suggests that analyzing the presence/absence of taxonomic features can offer a compelling alternative to differential abundance analysis in microbiome studies. However, standard approaches face challenges with boundary cases and multiple testing. To address these challenges, we developed DiPPER (Differential Prevalence via Probabilistic Estimation in R), a method based on Bayesian hierarchical modeling. We benchmarked our method against existing differential prevalence and abundance methods using data from 67 publicly available human gut microbiome studies. We observed considerable variation in performance across methods, with DiPPER outperforming alternatives by combining high sensitivity with effective error control. DiPPER also demonstrated superior replication of findings across independent studies. Furthermore, DiPPER provides differential prevalence estimates and uncertainty intervals that are inherently adjusted for multiple testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05938v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juho Pelto, Kari Auranen, Janne V. Kujala, Leo Lahti</dc:creator>
    </item>
    <item>
      <title>Physics as the Inductive Bias for Causal Discovery</title>
      <link>https://arxiv.org/abs/2602.04907</link>
      <description>arXiv:2602.04907v1 Announce Type: cross 
Abstract: Causal discovery is often a data-driven paradigm to analyze complex real-world systems. In parallel, physics-based models such as ordinary differential equations (ODEs) provide mechanistic structure for many dynamical processes. Integrating these paradigms potentially allows physical knowledge to act as an inductive bias, improving identifiability, stability, and robustness of causal discovery in dynamical systems. However, such integration remains challenging: real dynamical systems often exhibit feedback, cyclic interactions, and non-stationary data trend, while many widely used causal discovery methods are formulated under acyclicity or equilibrium-based assumptions. In this work, we propose an integrative causal discovery framework for dynamical systems that leverages partial physical knowledge as an inductive bias. Specifically, we model system evolution as a stochastic differential equation (SDE), where the drift term encodes known ODE dynamics and the diffusion term corresponds to unknown causal couplings beyond the prescribed physics. We develop a scalable sparsity-inducing MLE algorithm that exploits causal graph structure for efficient parameter estimation. Under mild conditions, we establish guarantees to recover the causal graph. Experiments on dynamical systems with diverse causal structures show that our approach improves causal graph recovery and produces more stable, physically consistent estimates than purely data-driven state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04907v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jianhong Chen, Naichen Shi, Xubo Yue</dc:creator>
    </item>
    <item>
      <title>Predictive Synthesis under Sporadic Participation: Evidence from Inflation Density Surveys</title>
      <link>https://arxiv.org/abs/2602.05226</link>
      <description>arXiv:2602.05226v1 Announce Type: cross 
Abstract: Central banks rely on density forecasts from professional surveys to assess inflation risks and communicate uncertainty. A central challenge in using these surveys is irregular participation: forecasters enter and exit, skip rounds, and reappear after long gaps. In the European Central Bank's Survey of Professional Forecasters, turnover and missingness vary substantially over time, causing the set of submitted predictions to change from quarter to quarter. Standard aggregation rules -- such as equal-weight pooling, renormalization after dropping missing forecasters, or ad hoc imputation -- can generate artificial jumps in combined predictions driven by panel composition rather than economic information, complicating real-time interpretation and obscuring forecaster performance. We develop coherent Bayesian updating rules for forecast combination under sporadic participation that maintain a well-defined latent predictive state for each forecaster even when their forecast is unobserved. Rather than relying on renormalization or imputation, the combined predictive distribution is updated through the implied conditional structure of the panel. This approach isolates genuine performance differences from mechanical participation effects and yields interpretable dynamics in forecaster influence. In the ECB survey, it improves predictive accuracy relative to equal-weight benchmarks and delivers smoother and better-calibrated inflation density forecasts, particularly during periods of high turnover.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05226v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew C. Johnson, Matteo Luciani, Minzhengxiong Zhang, Kenichiro McAlinn</dc:creator>
    </item>
    <item>
      <title>Radon--Wasserstein Gradient Flows for Interacting-Particle Sampling in High Dimensions</title>
      <link>https://arxiv.org/abs/2602.05227</link>
      <description>arXiv:2602.05227v1 Announce Type: cross 
Abstract: Gradient flows of the Kullback--Leibler (KL) divergence, such as the Fokker--Planck equation and Stein Variational Gradient Descent, evolve a distribution toward a target density known only up to a normalizing constant. We introduce new gradient flows of the KL divergence with a remarkable combination of properties: they admit accurate interacting-particle approximations in high dimensions, and the per-step cost scales linearly in both the number of particles and the dimension. These gradient flows are based on new transportation-based Riemannian geometries on the space of probability measures: the Radon--Wasserstein geometry and the related Regularized Radon--Wasserstein (RRW) geometry. We define these geometries using the Radon transform so that the gradient-flow velocities depend only on one-dimensional projections. This yields interacting-particle-based algorithms whose per-step cost follows from efficient Fast Fourier Transform-based evaluation of the required 1D convolutions. We additionally provide numerical experiments that study the performance of the proposed algorithms and compare convergence behavior and quantization. Finally, we prove some theoretical results including well-posedness of the flows and long-time convergence guarantees for the RRW flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05227v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elias Hess-Childs, Dejan Slep\v{c}ev, Lantian Xu</dc:creator>
    </item>
    <item>
      <title>Classification Under Local Differential Privacy with Model Reversal and Model Averaging</title>
      <link>https://arxiv.org/abs/2602.05797</link>
      <description>arXiv:2602.05797v1 Announce Type: cross 
Abstract: Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05797v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caihong Qin, Yang Bai</dc:creator>
    </item>
    <item>
      <title>Causal Inference on Stopped Random Walks in Online Advertising</title>
      <link>https://arxiv.org/abs/2602.05997</link>
      <description>arXiv:2602.05997v1 Announce Type: cross 
Abstract: We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an ad, but also changes each user's interaction-trajectory, and each advertiser's bidding policy -- as the latter is constrained by a finite budget. In particular, each a treatment may even affect the size of the population, since users interact longer with a tolerable advertising mechanism. We drop the classical i.i.d. assumption and model the experiment measurements (e.g., advertising revenue) as a stopped random walk, and use a budget-splitting experimental design, the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for the long-term treatment effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05997v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jia Yuan Yu</dc:creator>
    </item>
    <item>
      <title>Bridging Binarization: Causal Inference with Dichotomized Continuous Exposures</title>
      <link>https://arxiv.org/abs/2405.07109</link>
      <description>arXiv:2405.07109v5 Announce Type: replace 
Abstract: The average treatment effect (ATE) is a common parameter estimated in causal inference literature, but it is only defined for binary exposures. Thus, despite concerns raised by some researchers, many studies seeking to estimate the causal effect of a continuous exposure create a new binary exposure variable by dichotomizing the continuous values into two categories. In this paper, we affirm binarization as a statistically valid method for answering causal questions about continuous exposures by showing the equivalence between the binarized ATE and the difference in the average outcomes of two specific modified treatment policies. These policies impose cut-offs corresponding to the binarized exposure variable and assume preservation of relative self-selection. Relative self-selection is the ratio of the probability density of an individual having an exposure equal to one value of the continuous exposure variable versus another. The policies assume that, for any two values of the exposure variable with non-zero probability density after the cut-off, this ratio will remain unchanged. Through this equivalence, we clarify the assumptions underlying binarization and discuss how to properly interpret the resulting estimator. Additionally, we introduce a new target parameter that can be computed after binarization that considers the observed world as a benchmark. We argue that this parameter addresses more relevant causal questions than the traditional binarized ATE parameter. We present a simulation study to illustrate the implications of these assumptions when analyzing data and to demonstrate how to correctly implement estimators of the parameters discussed. Finally, we present an application of this method to evaluate the effect of a law in the state of California which seeks to limit exposures to oil and gas wells on birth outcomes to further illustrate the underlying assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07109v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1515/jci-2024-0049</arxiv:DOI>
      <dc:creator>Kaitlyn J. Lee, Alan Hubbard, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>Regularized estimation of Monge-Kantorovich quantiles for spherical data</title>
      <link>https://arxiv.org/abs/2407.02085</link>
      <description>arXiv:2407.02085v4 Announce Type: replace 
Abstract: Tools from optimal transport (OT) theory have recently been used to define a notion of quantile function for directional data. In practice, regularization is mandatory for applications that require out-of-sample estimates. To this end, we introduce a regularized estimator built from entropic optimal transport, by extending the definition of the entropic map to the spherical setting. We propose a stochastic algorithm to directly solve a continuous OT problem between the uniform distribution and a target distribution, by expanding Kantorovich potentials in the basis of spherical harmonics. In addition, we define the directional Monge-Kantorovich depth, a companion concept for OT-based quantiles. We show that it benefits from desirable properties related to Liu-Zuo-Serfling axioms for the statistical analysis of directional data. Building on our regularized estimators, we illustrate the benefits of our methodology for data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02085v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernard Bercu, J\'er\'emie Bigot, Gauthier Thurin</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the total treatment effect with multiple outcomes in the presence of terminal events</title>
      <link>https://arxiv.org/abs/2412.09304</link>
      <description>arXiv:2412.09304v4 Announce Type: replace 
Abstract: As standards of care advance, patients are living longer and once-fatal diseases are becoming manageable. Clinical trials increasingly focus on reducing disease burden, which can be quantified by the timing and occurrence of multiple non-fatal clinical events. Most existing methods for the analysis of multiple event-time data require stringent modeling assumptions that can be difficult to verify empirically, leading to treatment efficacy estimates that forego interpretability when the underlying assumptions are not met. Moreover, most existing methods do not appropriately account for informative terminal events, such as premature treatment discontinuation or death, which prevent the occurrence of subsequent events. To address these limitations, we derive and validate estimation and inference procedures for the area under the mean cumulative function (AUMCF), an extension of the restricted mean survival time to the multiple event-time setting. The AUMCF is nonparametric, clinically interpretable, and properly accounts for terminal competing risks. To enable covariate adjustment, we also develop an augmentation estimator that provides efficiency at least equaling, and often exceeding, the unadjusted estimator. The utility and interpretability of the AUMCF are illustrated with extensive simulation studies and through an analysis of multiple heart-failure-related endpoints using data from the Beta-Blocker Evaluation of Survival Trial (BEST) clinical trial. Our open-source R package MCC makes conducting AUMCF analyses straightforward and accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09304v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jessica Gronsbell, Zachary R. McCaw, Isabelle-Emmanuella Nogues, Xiangshan Kong, Tianxi Cai, Lu Tian, LJ Wei</dc:creator>
    </item>
    <item>
      <title>Moving toward best practice when using propensity score weighting in survey observational studies</title>
      <link>https://arxiv.org/abs/2501.16156</link>
      <description>arXiv:2501.16156v2 Announce Type: replace 
Abstract: Propensity score weighting is a common method for estimating treatment effects with survey data. The method is applied to minimize confounding using measured covariates that are often different between individuals in treatment and control. However, existing literature does not reach a consensus on the optimal use of survey weights for population-level inference in the propensity score weighting analysis. Under the balancing weights framework, we provided a unified solution for incorporating survey weights in both the propensity score of estimation and the outcome regression model. We derived estimators for different target populations, including the combined, treated, controlled, and overlap populations. We provide a unified expression of the sandwich variance estimator and demonstrate that the survey-weighted estimator is asymptotically normal, as established through the theory of M-estimators. Through an extensive series of simulation studies, we examined the performance of our derived estimators and compared the results to those of alternative methods. We further carried out two case studies to illustrate the application of the different methods of propensity score analysis with complex survey data. We concluded with a discussion of our findings and provided practical guidelines for propensity score weighting analysis of observational data from complex surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16156v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukang Zeng, Fan Li, Guangyu Tong</dc:creator>
    </item>
    <item>
      <title>Statistical description and dimension reduction of continuous time categorical trajectories with multivariate functional principal components</title>
      <link>https://arxiv.org/abs/2502.09986</link>
      <description>arXiv:2502.09986v4 Announce Type: replace 
Abstract: Getting tools that allow simple representations and comparisons of a set of categorical trajectories is of major interest for statisticians. Without loosing any information, we associate to each state a binary random indicator function, taking values in $\{0,1\}$, and turn the problem of statistical description of the categorical trajectories into a multivariate functional principal components analysis. This viewpoint encompasses experimental frameworks where two or more states can be observed simultaneously. The sample paths being piecewise constant, with a finite number of jumps, this a rare case in functional data analysis in which the trajectories are not supposed to be continuous and can be observed exhaustively. Under the weak hypothesis assuming only continuity in probability of the $0-1$ trajectories, the means and the (multivariate) covariance function are continuous and have interpretations in terms of departure from independence of the joint probabilities. Considering a functional data point of view, we show that the binary trajectories, which are right-continuous functions with left-hand limits, can be seen as random elements in the Hilbert space of square integrable functions. The multivariate functional principal components are simple to interpret and we show that we can get consistent estimators of the mean trajectories and the covariance functions under weak regularity assumptions. The ability of the approach to represent categorical trajectories in a small dimension space is illustrated on a data set of sensory perceptions, considering different gustometer-controlled stimuli experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09986v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Herv\'e Cardot, Caroline Peltier</dc:creator>
    </item>
    <item>
      <title>Inference for Heterogeneous Treatment Effects with Efficient Instruments and Machine Learning</title>
      <link>https://arxiv.org/abs/2503.03530</link>
      <description>arXiv:2503.03530v3 Announce Type: replace 
Abstract: We introduce a new instrumental variable (IV) estimator for heterogeneous treatment effects in the presence of endogeneity. Our estimator is based on double/debiased machine learning (DML) and uses efficient machine learning instruments (MLIV) and kernel smoothing. We prove consistency and asymptotic normality of our estimator and also construct confidence sets that are more robust towards weak IV. Along the way, we also provide an accessible discussion of the corresponding estimator for the homogeneous treatment effect with efficient machine learning instruments. The methods are evaluated on synthetic and real datasets and an implementation is made available in the R package IVDML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03530v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyrill Scheidegger, Zijian Guo, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>AdapDISCOM: An Adaptive Sparse Regression Method for High-Dimensional Multimodal Data With Block-Wise Missingness and Measurement Errors</title>
      <link>https://arxiv.org/abs/2508.00120</link>
      <description>arXiv:2508.00120v2 Announce Type: replace 
Abstract: Multimodal high-dimensional data are increasingly prevalent in biomedical research, yet they are often compromised by block-wise missingness and measurement errors, posing significant challenges for statistical inference and prediction. We propose AdapDISCOM, a novel adaptive direct sparse regression method that simultaneously addresses these two pervasive issues. Building on the DISCOM framework, AdapDISCOM introduces modality-specific weighting schemes to account for heterogeneity in data structures and error magnitudes across modalities. We establish the theoretical properties of AdapDISCOM, including model selection consistency and convergence rates under sub-Gaussian and heavy-tailed settings, and develop robust and computationally efficient variants (AdapDISCOM-Huber and Fast-AdapDISCOM). Extensive simulations demonstrate that AdapDISCOM consistently outperforms existing methods such as DISCOM, SCOM, and CoCoLasso, particularly under heterogeneous contamination and heavy-tailed distributions. Finally, we apply AdapDISCOM to Alzheimers Disease Neuroimaging Initiative (ADNI) data, demonstrating improved prediction of cognitive scores and reliable selection of established biomarkers, even with substantial missingness and measurement errors. AdapDISCOM provides a flexible, robust, and scalable framework for high-dimensional multimodal data analysis under realistic data imperfections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00120v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maimouna Bald\'e (and for the Alzheimers Disease Neuroimaging Initiative), Abdoul O. Diakit\'e (and for the Alzheimers Disease Neuroimaging Initiative), Claudia Moreau (and for the Alzheimers Disease Neuroimaging Initiative), Gleb Bezgin (and for the Alzheimers Disease Neuroimaging Initiative), Nikhil Bhagwat (and for the Alzheimers Disease Neuroimaging Initiative), Pedro Rosa-Neto (and for the Alzheimers Disease Neuroimaging Initiative), Jean-Baptiste Poline (and for the Alzheimers Disease Neuroimaging Initiative), Simon Girard (and for the Alzheimers Disease Neuroimaging Initiative), Amadou Barry (and for the Alzheimers Disease Neuroimaging Initiative)</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes for Data Integration</title>
      <link>https://arxiv.org/abs/2508.08336</link>
      <description>arXiv:2508.08336v2 Announce Type: replace 
Abstract: We discuss the use of empirical Bayes for data integration, in the sense of transfer learning. Our main interest is in settings where one wishes to learn structure (e.g. feature selection) and one only has access to incomplete data from previous studies, such as summaries, estimates or lists of relevant features. We discuss differences between full Bayes and empirical Bayes, and develop a computational framework for the latter. We discuss how empirical Bayes attains consistent variable selection under weaker conditions (sparsity and betamin assumptions) than full Bayes and other standard criteria do, and how it attains faster convergence rates. Our high-dimensional regression examples show that fully Bayesian inference enjoys excellent properties, and that data integration with empirical Bayes can offer moderate yet meaningful improvements in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08336v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Rognon-Vael, David Rossell</dc:creator>
    </item>
    <item>
      <title>Using Individualized Treatment Effects to Assess Treatment Effect Heterogeneity</title>
      <link>https://arxiv.org/abs/2502.00713</link>
      <description>arXiv:2502.00713v2 Announce Type: replace-cross 
Abstract: Assessing treatment effect heterogeneity (TEH) in clinical trials is crucial, as it provides insights into the variability of treatment responses among patients, influencing important decisions related to drug development. Furthermore, it can lead to personalized medicine by tailoring treatments to individual patient characteristics. This paper introduces novel methodologies for assessing treatment effects using the individual treatment effect as a basis. To estimate this effect, we use a Double Robust (DR) learner to infer a pseudo-outcome that reflects the causal contrast. This pseudo-outcome is then used to perform three objectives: (1) a global test for heterogeneity, (2) ranking covariates based on their influence on effect modification, and (3) providing estimates of the individualized treatment effect. We compare our DR-learner with various alternatives and competing methods in a simulation study, and also use it to assess heterogeneity in a pooled analysis of five Phase III trials in psoriatic arthritis. By integrating these methods with the recently proposed WATCH workflow (Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors), we provide a robust framework for analyzing TEH, offering insights that enable more informed decision-making in this challenging area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00713v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.70324</arxiv:DOI>
      <arxiv:journal_reference>Statistics in Medicine 2025</arxiv:journal_reference>
      <dc:creator>Konstantinos Sechidis, Cong Zhang, Sophie Sun, Yao Chen, Asher Spector, Bj\"orn Bornkamp</dc:creator>
    </item>
    <item>
      <title>Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2512.24139</link>
      <description>arXiv:2512.24139v4 Announce Type: replace-cross 
Abstract: While conformal prediction provides robust marginal coverage guarantees, achieving reliable conditional coverage for specific inputs remains challenging. Although exact distribution-free conditional coverage is impossible with finite samples, recent work has focused on improving the conditional coverage of standard conformal procedures. Distinct from approaches that target relaxed notions of conditional coverage, we directly minimize the mean squared error of conditional coverage by refining the quantile regression components that underpin many conformal methods. Leveraging a Taylor expansion, we derive a sharp surrogate objective for quantile regression: a density-weighted pinball loss, where the weights are given by the conditional density of the conformity score evaluated at the true quantile. We propose a three-headed quantile network that estimates these weights via finite differences using auxiliary quantile levels at \(1-\alpha \pm \delta\), subsequently fine-tuning the central quantile by optimizing the weighted loss. We provide a theoretical analysis with exact non-asymptotic guarantees characterizing the resulting excess risk. Extensive experiments on diverse high-dimensional real-world datasets demonstrate remarkable improvements in conditional coverage performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24139v4</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyi Chen, Bo Li</dc:creator>
    </item>
    <item>
      <title>SC3D: Dynamic and Differentiable Causal Discovery for Temporal and Instantaneous Graphs</title>
      <link>https://arxiv.org/abs/2602.02830</link>
      <description>arXiv:2602.02830v2 Announce Type: replace-cross 
Abstract: Discovering causal structures from multivariate time series is a key problem because interactions span across multiple lags and possibly involve instantaneous dependencies. Additionally, the search space of the dynamic graphs is combinatorial in nature. In this study, we propose \textit{Stable Causal Dynamic Differentiable Discovery (SC3D)}, a two-stage differentiable framework that jointly learns lag-specific adjacency matrices and, if present, an instantaneous directed acyclic graph (DAG). In Stage 1, SC3D performs edge preselection through node-wise prediction to obtain masks for lagged and instantaneous edges, whereas Stage 2 refines these masks by optimizing a likelihood with sparsity along with enforcing acyclicity on the instantaneous block. Numerical results across synthetic and benchmark dynamical systems demonstrate that SC3D achieves improved stability and more accurate recovery of both lagged and instantaneous causal structures compared to existing temporal baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02830v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourajit Das, Dibyajyoti Chakraborty, Romit Maulik</dc:creator>
    </item>
  </channel>
</rss>
