<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 May 2025 04:01:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Simulated Annealing ABC with multiple summary statistics</title>
      <link>https://arxiv.org/abs/2505.23261</link>
      <description>arXiv:2505.23261v1 Announce Type: new 
Abstract: Bayesian inference for stochastic models is often challenging because evaluating the likelihood function typically requires integrating over a large number of latent variables. However, if only few parameters need to be inferred, it can be more efficient to perform the inference based on a comparison of the observations with (a large number of) model simulations, in terms of only few summary statistics. In Machine Learning (ML), Simulation Based Inference (SBI) using neural density estimation is often considered superior to the traditional sampling-based approach known as Approximate Bayesian Computation (ABC). Here, we present a new set of ABC algorithms based on Simulated Annealing and demonstrate that they are competitive with ML approaches, whilst requiring much less hyper-parameter tuning. For the design of these sampling algorithms we draw intuition from non-equilibrium thermodynamics, where we associate each summary statistic with a state variable (energy) quantifying the distance to the observed value as well as a temperature that controls the degree to which the associated statistic contributes to the posterior. We derive an optimal annealing schedule on a Riemannian manifold of state variables based on a minimal entropy production principle. Our new algorithms generalize the established Simulated Annealing based ABC to multiple state variables and temperatures. In situations where the information-content is unevenly distributed among the summary statistics, this can greatly improve performance of the algorithm. Our method also allows monitoring the convergence of individual statistics, which is a great diagnostic tool in out-of-sample situations. We validate our approach on standard benchmark tasks from the SBI literature and a hard inference problem from solar physics and demonstrate that it is highly competitive with the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23261v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlo Albert, Simone Ulzega, Simon Dirmeier, Andreas Scheidegger, Alberto Bassi, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>State Space Model Programming in Turing.jl</title>
      <link>https://arxiv.org/abs/2505.23302</link>
      <description>arXiv:2505.23302v1 Announce Type: new 
Abstract: State space models (SSMs) are a powerful and widely-used class of probabilistic models for analysing time-series data across various fields, from econometrics to robotics. Despite their prevalence, existing software frameworks for SSMs often lack compositionality and scalability, hindering experimentation and making it difficult to leverage advanced inference techniques. This paper introduces SSMProblems.jl and GeneralisedFilters.jl, two Julia packages within the Turing.jl ecosystem, that address this challenge by providing a consistent, composable, and general framework for defining SSMs and performing inference on them. This unified interface allows researchers to easily define a wide range of SSMs and apply various inference algorithms, including Kalman filtering, particle filtering, and combinations thereof. By promoting code reuse and modularity, our packages reduce development time and improve the reliability of SSM implementations. We prioritise scalability through efficient memory management and GPU-acceleration, ensuring that our framework can handle large-scale inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23302v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Hargreaves, Qing Li, Charles Knipp, Frederic Wantiez, Simon J. Godsill, Hong Ge</dc:creator>
    </item>
    <item>
      <title>Particle exchange Monte Carlo methods for eigenfunction and related nonlinear problems</title>
      <link>https://arxiv.org/abs/2505.23456</link>
      <description>arXiv:2505.23456v1 Announce Type: cross 
Abstract: We introduce and develop a novel particle exchange Monte Carlo method. Whereas existing methods apply to eigenfunction problems where the eigenvalue is known (e.g., integrals with respect to a Gibbs measure, which can be interpreted as corresponding to eigenvalue zero), here the focus is on problems where the eigenvalue is not known a priori. To obtain an appropriate particle exchange rule we must consider a pair of processes, with one evolving forward in time and the other backward. Applications to eigenfunction problems corresponding to quasistationary distributions and ergodic stochastic control are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23456v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Dupuis, Benjamin J. Zhang</dc:creator>
    </item>
    <item>
      <title>Estimating MCMC convergence rates using common random number simulation</title>
      <link>https://arxiv.org/abs/2309.15735</link>
      <description>arXiv:2309.15735v3 Announce Type: replace 
Abstract: This paper presents how to use common random number (CRN) simulation to evaluate Markov chain Monte Carlo (MCMC) convergence to stationarity. We provide an upper bound on the Wasserstein distance of a Markov chain to its stationary distribution after $N$ steps in terms of averages over CRN simulations. We apply our bound to Gibbs samplers on a variance component model, a model related to James-Stein estimators, and a Bayesian linear regression model. For the former two examples, we show that the CRN simulated bound converges to zero significantly more quickly compared to available drift and minorization bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15735v3</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabrina Sixta, Jeffrey S. Rosenthal, Austin Brown</dc:creator>
    </item>
    <item>
      <title>Proximal Interacting Particle Langevin Algorithms</title>
      <link>https://arxiv.org/abs/2406.14292</link>
      <description>arXiv:2406.14292v3 Announce Type: replace 
Abstract: We introduce a class of algorithms, termed proximal interacting particle Langevin algorithms (PIPLA), for inference and learning in latent variable models whose joint probability density is non-differentiable. Leveraging proximal Markov chain Monte Carlo techniques and interacting particle Langevin algorithms, we propose three algorithms tailored to the problem of estimating parameters in a non-differentiable statistical model. We prove nonasymptotic bounds for the parameter estimates produced by the different algorithms in the strongly log-concave setting and provide comprehensive numerical experiments on various models to demonstrate the effectiveness of the proposed methods. In particular, we demonstrate the utility of our family of algorithms for sparse Bayesian logistic regression, training of sparse Bayesian neural networks or neural networks with non-differentiable activation functions, image deblurring, and sparse matrix completion. Our theory and experiments together show that PIPLA family can be the de facto choice for parameter estimation problems in non-differentiable latent variable models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14292v3</guid>
      <category>stat.CO</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Conference on Uncertainty in Artificial Intelligence (UAI) 2025. Oral presentation</arxiv:journal_reference>
      <dc:creator>Paula Cordero Encinar, Francesca R. Crucinio, O. Deniz Akyildiz</dc:creator>
    </item>
  </channel>
</rss>
