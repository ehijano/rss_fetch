<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Aug 2025 04:01:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The fundamental limits of minimax risk for high-dimensional speckle noise model</title>
      <link>https://arxiv.org/abs/2508.18503</link>
      <description>arXiv:2508.18503v1 Announce Type: new 
Abstract: Unlike conventional imaging modalities, such as magnetic resonance imaging, which are often well described by a linear regression framework, coherent imaging systems follow a significantly more complex model. In these systems, the task is to recover the unknown signal or image $\mathbf{x}_o \in \mathbb{R}^n$ from observations $\mathbf{y}_1, \ldots, \mathbf{y}_L \in \mathbb{R}^m$ of the form \[ \mathbf{y}_l = A_l X_o \mathbf{w}_l + \mathbf{z}_l, \quad l = 1, \ldots, L, \] where $X_o = \mathrm{diag}(\mathbf{x}_o)$ is an $n \times n$ diagonal matrix, $\mathbf{w}_1, \ldots, \mathbf{w}_L \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,I_n)$ represent speckle noise, and $\mathbf{z}_1, \ldots, \mathbf{z}_L \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,\sigma_z^2 I_m)$ denote additive noise. The matrices $A_1, \ldots, A_L$ are known forward operators determined by the imaging system.
  Our goal is to characterize the minimax risk of estimating $\mathbf{x}_o$, in high-dimensional settings where $m$ could be even less than $n$. Motivated by insights from sparse regression, we note that the structure of $\mathbf{x}_o$ plays a central role in the estimation error. Here, we adopt a general notion of structure better suited to coherent imaging: we assume that $\mathbf{x}_o$ lies in a signal class $\mathcal{C}_k$ whose Minkowski dimension is bounded by $k \ll n$.
  We show that, when $A_1,\ldots,A_L$ are independent $m \times n$ Gaussian matrices, the minimax mean squared error (MSE) scales as \[ \frac{\max\{\sigma_z^4,\, m^2,\, n^2\}\, k \log n}{m^2 n L}. \]</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18503v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Xing, Soham Jana, Arian Maleki</dc:creator>
    </item>
    <item>
      <title>Adaptive estimation for nonparametric circular regression with errors in variables</title>
      <link>https://arxiv.org/abs/2508.18581</link>
      <description>arXiv:2508.18581v1 Announce Type: new 
Abstract: This paper investigates the nonparametric estimation of a circular regression function in an errors-in-variables framework. Two settings are studied, depending on whether the covariates are circular or linear. Adaptive estimators are constructed and their theoretical performance is assessed through convergence rates over Sobolev and H\"older smoothness classes. Numerical experiments on simulated and real datasets illustrate the practical relevance of the methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18581v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tien Dat Nguyen, Thanh Mai Pham Ngoc</dc:creator>
    </item>
    <item>
      <title>Simple and Sharp Generalization Bounds via Lifting</title>
      <link>https://arxiv.org/abs/2508.18682</link>
      <description>arXiv:2508.18682v1 Announce Type: new 
Abstract: We develop an information-theoretic framework for bounding the expected supremum and tail probabilities of stochastic processes, offering a simpler and sharper alternative to classical chaining and slicing arguments for generalization bounds. The key idea is a lifting argument that produces information-theoretic analogues of empirical process bounds, such as Dudley's entropy integral. The lifting introduces symmetry, yielding sharp bounds even when the classical Dudley integral is loose. As a by-product, we obtain a concise proof of the majorizing measure theorem, providing explicit constants. The information-theoretic approach provides a soft version of classical localized complexity bounds in generalization theory, but is more concise and does not require the slicing argument. We apply this approach to empirical risk minimization over Sobolev ellipsoids and weak $\ell_q$ balls, obtaining sharper convergence rates or extensions to settings not covered by classical methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18682v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jingbo Liu</dc:creator>
    </item>
    <item>
      <title>Asymptotic properties of goodness of fit tests based on higher order overlapping spacings</title>
      <link>https://arxiv.org/abs/2508.18965</link>
      <description>arXiv:2508.18965v1 Announce Type: new 
Abstract: The paper is devoted to tests for uniformity based on sum-functions of overlapping spacings, where the order of spacings can diverge to infinity as the sample size increases. In particular, it is shown that the asymptotic local power of these tests depends significantly on the asymptotic properties of the counterpart statistics based on disjoint spacings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18965v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Statistics and Probability Letters 225 (2025) 110461</arxiv:journal_reference>
      <dc:creator>Sherzod M. Mirakhmedov</dc:creator>
    </item>
    <item>
      <title>Unified theory of testing relevant hypothesis in functional time series</title>
      <link>https://arxiv.org/abs/2508.18624</link>
      <description>arXiv:2508.18624v1 Announce Type: cross 
Abstract: In this paper, we present a general framework for testing relevant hypotheses in functional time series. Our unified approach covers one-sample, two-sample, and change point problems under contaminated observations with arbitrary sampling schemes. By employing B-spline estimators and the self-normalization technique, we propose nuisance-parameter-free testing procedures, obviating the need for additional procedures such as estimating long-run covariance or measurement-error variance functions. A key challenge arises from related nonparametric statistics may not be tight, complicating the joint weak convergence for the test statistics and self-normalizers, particularly in sparse scenarios. To address this, we leverage a Gaussian approximation in a diverging-dimension regime to derive a pivotal approximate distribution. Then, we develop consistent decision rules, provide sufficient conditions ensuring non-degeneracy, and establish phase transition boundaries from sparse to dense. We also examine the multiple change point scenario and extend the theory when one obtains consistent estimates of the change points. The choice of self-normalizers is further discussed, including the recently developed range-adjusted self-normalizer. Extensive numerical experiments support the proposed theory, and we illustrate our methodologies using the AU.SHF implied volatility and traffic volume datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18624v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leheng Cai, Qirui Hu</dc:creator>
    </item>
    <item>
      <title>Think before you fit: parameter identifiability, sensitivity and uncertainty in systems biology models</title>
      <link>https://arxiv.org/abs/2508.18853</link>
      <description>arXiv:2508.18853v1 Announce Type: cross 
Abstract: Reliable predictions from systems biology models require knowing whether parameters can be estimated from available data, and with what certainty. Identifiability analysis reveals whether parameters are learnable in principle (structural identifiability) and in practice (practical identifiability). We introduce the core ideas using linear models, highlighting how experimental design and output sensitivity shape identifiability. In nonlinear models, identifiability can vary with parameter values, motivating global and simulation-based approaches. We summarise computational methods for assessing identifiability noting that weakly identifiable parameters can undermine predictions beyond the calibration dataset. Strategies to improve identifiability include measuring different outputs, refining model structure, and adding prior knowledge. Far from a technical afterthought, identifiability determines the limits of inference and prediction. Recognising and addressing identifiability is essential for building models that are not only well-fitted to data, but also capable of delivering predictions with robust, quantifiable uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18853v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon P. Preston, Richard D. Wilkinson, Richard H. Clayton, Mike J. Chappell, Gary R. Mirams</dc:creator>
    </item>
    <item>
      <title>Replicability: Terminology, Measuring Success, and Strategy</title>
      <link>https://arxiv.org/abs/2508.19070</link>
      <description>arXiv:2508.19070v1 Announce Type: cross 
Abstract: Empirical science needs to be based on facts and claims that can be reproduced. This calls for replicating the studies that proclaim the claims, but practice in most fields still fails to implement this idea. When such studies emerged in the past decade, the results were generally disappointing. There have been an overwhelming number of papers addressing the ``reproducibility crisis'' in the last 20 years. Nevertheless, terminology is not yet settled, and there is no consensus about when a replication should be called successful. This paper intends to clarify such issues. A fundamental problem in empirical science is that usual claims only state that effects are non-zero, and such statements are scientifically void. An effect must have a \emph{relevant} size to become a reasonable item of knowledge. Therefore, estimation of an effect, with an indication of precision, forms a substantial scientific task, whereas testing it against zero does not. A relevant effect is one that is shown to exceed a relevance threshold. This paradigm has implications for the judgement on replication success.
  A further issue is the unavoidable variability between studies, called heterogeneity in meta-analysis. Therefore, it is of little value, again, to test for zero difference between an original effect and its replication, but exceedance of a corresponding relevance threshold should be tested. In order to estimate the degree of heterogeneity, more than one replication is needed, and an appropriate indication of the precision of an estimated effect requires such an estimate.
  These insights, which are discussed in the paper, show the complexity of obtaining solid scientific results, implying the need for a strategy to make replication happen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19070v1</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Werner A. Stahel (ETH Zurich, Switzerland)</dc:creator>
    </item>
    <item>
      <title>How many samples are needed to train a deep neural network?</title>
      <link>https://arxiv.org/abs/2405.16696</link>
      <description>arXiv:2405.16696v2 Announce Type: replace 
Abstract: Neural networks have become standard tools in many areas, yet many important statistical questions remain open. This paper studies the question of how much data are needed to train a ReLU feed-forward neural network. Our theoretical and empirical results suggest that the generalization error of ReLU feed-forward neural networks scales at the rate $1/\sqrt{n}$ in the sample size $n$ rather than the usual "parametric rate" $1/n$. Thus, broadly speaking, our results underpin the common belief that neural networks need "many" training samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16696v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pegah Golestaneh, Mahsa Taheri, Johannes Lederer</dc:creator>
    </item>
    <item>
      <title>Granulometric Smoothing on Manifolds</title>
      <link>https://arxiv.org/abs/2407.07559</link>
      <description>arXiv:2407.07559v2 Announce Type: replace 
Abstract: Given a random sample from a density function supported on a manifold $M$, a new method for the estimating highest density regions of the underlying population is introduced. The new proposal is based on the empirical version of the opening operator from mathematical morphology combined with a preliminary estimator of the density function. This results in an estimator that is easy-to-compute since it simply consists of a list of centers and a radius $r$ that are adequately selected from the data. The new estimator is shown to be consistent and its convergence rates in terms of the Hausdorff distance are provided. All consistency results are established uniformly on the level of the set and for any Riemannian manifold $M$ satisfying mild assumptions. The applicability of the procedure is shown by some illustrative examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07559v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Bol\'on, Rosa M. Crujeiras, Alberto Rodr\'iguez-Casal</dc:creator>
    </item>
    <item>
      <title>Distribution free M-estimation</title>
      <link>https://arxiv.org/abs/2505.22807</link>
      <description>arXiv:2505.22807v4 Announce Type: replace 
Abstract: The basic question of delineating those statistical problems that are solvable without making any assumptions on the underlying data distribution has long animated statistics and learning theory. This paper characterizes when a convex M-estimation or stochastic optimization problem is solvable in such an assumption-free setting, providing a precise dividing line between solvable and unsolvable problems. The conditions we identify show, perhaps surprisingly, that Lipschitz continuity of the loss being minimized is not necessary for distribution free minimization, and they are also distinct from classical characterizations of learnability in machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22807v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felipe Areces, John C. Duchi</dc:creator>
    </item>
    <item>
      <title>On MCMC mixing for predictive inference under unidentified transformation models</title>
      <link>https://arxiv.org/abs/2411.01382</link>
      <description>arXiv:2411.01382v3 Announce Type: replace-cross 
Abstract: Reliable Bayesian predictive inference has long been an open problem under unidentified transformation models, since the Markov Chain Monte Carlo (MCMC) chains of posterior predictive distribution (PPD) values are generally poorly mixed. We address the poorly mixed PPD value chains under unidentified transformation models through an adaptive scheme for prior adjustment. Specifically, we originate a conception of sufficient informativeness, which explicitly quantifies the information level provided by nonparametric priors, and assesses MCMC mixing by comparison with the within-chain MCMC variance. We formulate the prior information level by a set of hyperparameters induced from the nonparametric prior elicitation with an analytic expression, which is guaranteed by asymptotic theory for the posterior variance under unidentified transformation models. The analytic prior information level consequently drives a hyperparameter tuning procedure to achieve MCMC mixing. The proposed method is general enough to cover various data domains through a multiplicative error working model. Comprehensive simulations and real-world data analysis demonstrate that our method successfully achieves MCMC mixing and outperforms state-of-the-art competitors in predictive capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01382v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Zhong, Jin Yang, Junshan Shen, Zhaohai Li, Catherine C. Liu</dc:creator>
    </item>
  </channel>
</rss>
