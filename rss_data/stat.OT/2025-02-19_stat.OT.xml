<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.OT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.OT</link>
    <description>stat.OT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.OT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Feb 2025 02:40:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Simplified and Numerically Stable Approach to the BG/NBD Churn Prediction model</title>
      <link>https://arxiv.org/abs/2502.12912</link>
      <description>arXiv:2502.12912v1 Announce Type: new 
Abstract: This study extends the BG/NBD churn probability model, addressing its limitations in industries where customer behaviour is often influenced by seasonal events and possibly high purchase counts. We propose a modified definition of churn, considering a customer to have churned if they make no purchases within M days. Our contribution is twofold: First, we simplify the general equation for the specific case of zero purchases within M days. Second, we derive an alternative expression using numerical techniques to mitigate numerical overflow or underflow issues. This approach provides a more practical and robust method for predicting customer churn in industries with irregular purchase patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12912v1</guid>
      <category>stat.OT</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan Zammit, Christopher Zerafa</dc:creator>
    </item>
    <item>
      <title>Score Matching Riemannian Diffusion Means</title>
      <link>https://arxiv.org/abs/2502.13106</link>
      <description>arXiv:2502.13106v1 Announce Type: new 
Abstract: Estimating means on Riemannian manifolds is generally computationally expensive because the Riemannian distance function is not known in closed-form for most manifolds. To overcome this, we show that Riemannian diffusion means can be efficiently estimated using score matching with the gradient of Brownian motion transition densities using the same principle as in Riemannian diffusion models. Empirically, we show that this is more efficient than Monte Carlo simulation while retaining accuracy and is also applicable to learned manifolds. Our method, furthermore, extends to computing the Fr\'echet mean and the logarithmic map for general Riemannian manifolds. We illustrate the applicability of the estimation of diffusion mean by efficiently extending Euclidean algorithms to general Riemannian manifolds with a Riemannian $k$-means algorithm and maximum likelihood Riemannian regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13106v1</guid>
      <category>stat.OT</category>
      <category>stat.CO</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederik M\"obius Rygaard, Steen Markvorsen, S{\o}ren Hauberg, Stefan Sommer</dc:creator>
    </item>
    <item>
      <title>A Survey: Potential Dimensionality Reduction Methods</title>
      <link>https://arxiv.org/abs/2502.11036</link>
      <description>arXiv:2502.11036v2 Announce Type: replace 
Abstract: Dimensionality reduction is a fundamental technique in machine learning and data analysis, enabling efficient representation and visualization of high-dimensional data. This paper explores five key methods: Principal Component Analysis (PCA), Kernel PCA (KPCA), Sparse Kernel PCA, t-Distributed Stochastic Neighbor Embedding (t-SNE), and Uniform Manifold Approximation and Projection (UMAP). PCA provides a linear approach to capturing variance, whereas KPCA and Sparse KPCA extend this concept to non-linear structures using kernel functions. Meanwhile, t-SNE and UMAP focus on preserving local relationships, making them effective for data visualization. Each method is examined in terms of its mathematical formulation, computational complexity, strengths, and limitations. The trade-offs between global structure preservation, computational efficiency, and interpretability are discussed to guide practitioners in selecting the appropriate technique based on their application needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11036v2</guid>
      <category>stat.OT</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan-chin Ivan Chang</dc:creator>
    </item>
  </channel>
</rss>
