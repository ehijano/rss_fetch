<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.ST</link>
    <description>q-fin.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Mar 2024 04:02:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Nonparametric Bayesian volatility learning under microstructure noise</title>
      <link>https://arxiv.org/abs/1805.05606</link>
      <description>arXiv:1805.05606v2 Announce Type: replace-cross 
Abstract: In this work, we study the problem of learning the volatility under market microstructure noise. Specifically, we consider noisy discrete time observations from a stochastic differential equation and develop a novel computational method to learn the diffusion coefficient of the equation. We take a nonparametric Bayesian approach, where we \emph{a priori} model the volatility function as piecewise constant. Its prior is specified via the inverse Gamma Markov chain. Sampling from the posterior is accomplished by incorporating the Forward Filtering Backward Simulation algorithm in the Gibbs sampler. Good performance of the method is demonstrated on two representative synthetic data examples. We also apply the method on a EUR/USD exchange rate dataset. Finally we present a limit result on the prior distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:1805.05606v2</guid>
      <category>stat.ME</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s42081-022-00185-9</arxiv:DOI>
      <arxiv:journal_reference>Jpn. J. Stat. Data. Sci 6, 551-571 (2023)</arxiv:journal_reference>
      <dc:creator>Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij</dc:creator>
    </item>
    <item>
      <title>Entropy corrected geometric Brownian motion</title>
      <link>https://arxiv.org/abs/2403.06253</link>
      <description>arXiv:2403.06253v2 Announce Type: replace-cross 
Abstract: The geometric Brownian motion (GBM) is widely employed for modeling stochastic processes, yet its solutions are characterized by the log-normal distribution. This comprises predictive capabilities of GBM mainly in terms of forecasting applications. Here, entropy corrections to GBM are proposed to go beyond log-normality restrictions and better account for intricacies of real systems. It is shown that GBM solutions can be effectively refined by arguing that entropy is reduced when deterministic content of considered data increases. Notable improvements over conventional GBM are observed for several cases of non-log-normal distributions, ranging from a dice roll experiment to real world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06253v2</guid>
      <category>physics.data-an</category>
      <category>q-fin.ST</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishabh Gupta, Ewa A. Drzazga-Szcz\c{e}\'sniak, Sabre Kais, Dominik Szcz\c{e}\'sniak</dc:creator>
    </item>
  </channel>
</rss>
