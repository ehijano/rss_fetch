<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Oct 2025 04:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Immersive Mixed Reality Simulator for CT Scan Preparation: Enhancing Patient Emotional and Physical Readiness</title>
      <link>https://arxiv.org/abs/2510.03526</link>
      <description>arXiv:2510.03526v1 Announce Type: new 
Abstract: First-time patients undergoing diagnostic computed tomography (CT) scans often experience significant anxiety and uncertainty, which can negatively impact scan results and patient well-being. We present an immersive mixed reality (MR) simulator designed to prepare adult patients for their first CT scan, aiming to improve both emotional and physical preparedness. In this paper, we review existing methods for reducing scan-related anxiety -- from educational materials to virtual reality exposure -- and identify their limitations. We then detail the design and technical implementation of our MR simulator, which combines a virtual CT suite walkthrough, guided relaxation training, realistic scan simulation (including audiovisual cues and breath-hold practice), and interactive feedback. The inclusion of these features is grounded in evidence-based rationale drawn from prior studies in patient anxiety reduction and compliance. We report results from a pilot study ($n=50$) demonstrating that patients who used the simulator had significantly lower pre-scan anxiety levels and improved compliance during the actual CT procedure, compared to controls. Patient feedback was overwhelmingly positive, indicating high satisfaction and perceived utility. We discuss the clinical implications of deploying such a tool, challenges in integration, and future directions for improving patient-centered care using mixed reality technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03526v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Smith, Priya Patel, Hu Guo, Marco Ruiz</dc:creator>
    </item>
    <item>
      <title>Mixed Reality Guidance of a Surgical Scalpel Using Magic Leap: Evaluation on a 3D-Printed Liver Phantom</title>
      <link>https://arxiv.org/abs/2510.03617</link>
      <description>arXiv:2510.03617v1 Announce Type: new 
Abstract: Augmented and mixed reality (MR) systems have the potential to improve surgical precision by overlaying digital guidance directly onto the operative field. This paper presents a novel MR guidance system using the Magic Leap head-mounted display to assist surgeons in executing precise scalpel movements during liver surgery. The system projects holographic cues onto a patient-specific 3D-printed liver phantom, guiding resection along a predetermined path. We describe the system design, including preoperative modeling, registration of virtual content to the phantom, and real-time visualization through the Magic Leap device. In a controlled phantom study, surgical trainees performed resection tasks with and without MR guidance. Quantitative results demonstrated that MR guidance improved cutting accuracy (mean deviation from planned path was reduced from 5.0 mm without AR to 2.0 mm with AR guidance) and efficiency (mean task time decreased from 55 s to 32 s). These improvements of approximately 60% in accuracy and 40% in speed underscore the potential benefit of MR in surgical navigation. Participants reported that the Magic Leap visualization enhanced depth perception and confidence in locating tumor boundaries. This work provides a comprehensive evaluation of an MR-assisted surgical guidance approach, highlighting its feasibility on a realistic organ phantom. We discuss the technical challenges (registration accuracy, line-of-sight, user ergonomics) and outline future steps toward clinical translation. The results suggest that Magic Leap-based MR guidance can significantly augment a surgeon's performance in delicate resection tasks, paving the way for safer and more precise liver surgery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03617v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alice Yang, Michael Beasley, Catherine Taylor, Hu Guo</dc:creator>
    </item>
    <item>
      <title>Invisible Saboteurs: Sycophantic LLMs Mislead Novices in Problem-Solving Tasks</title>
      <link>https://arxiv.org/abs/2510.03667</link>
      <description>arXiv:2510.03667v1 Announce Type: new 
Abstract: Sycophancy, the tendency of LLM-based chatbots to express excessive enthusiasm, agreement, flattery, and a lack of disagreement, is emerging as a significant risk in human-AI interactions. However, the extent to which this affects human-LLM collaboration in complex problem-solving tasks is not well quantified, especially among novices who are prone to misconceptions. We created two LLM chatbots, one with high sycophancy and one with low sycophancy, and conducted a within-subjects experiment (n=24) in the context of debugging machine learning models to isolate the effect of LLM sycophancy on users' mental models, their workflows, reliance behaviors, and their perceptions of the chatbots. Our findings show that users of the high sycophancy chatbot were less likely to correct their misconceptions and spent more time over-relying on unhelpful LLM responses. Despite these impaired outcomes, a majority of users were unable to detect the presence of excessive sycophancy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03667v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jessica Y. Bo, Majeed Kazemitabaar, Mengqing Deng, Michael Inzlicht, Ashton Anderson</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap: Enhancing Gaze-Performance Link in Children with ASD through Dual-Level Visual Guidance in MR-DMT</title>
      <link>https://arxiv.org/abs/2510.03724</link>
      <description>arXiv:2510.03724v1 Announce Type: new 
Abstract: Autism Spectrum Disorder (ASD) is marked by action imitation deficits stemming from visuomotor integration impairments, posing challenges to imitation-based learning, such as dance movement therapy in mixed reality (MR-DMT). Previous gaze-guiding interventions in ASD have mainly focused on optimizing gaze in isolation, neglecting the crucial "gaze-performance link". This study investigates enhancing this link in MR-DMT for children with ASD. Initially, we experimentally confirmed the weak link: longer gaze durations didn't translate to better performance. Then, we proposed and validated a novel dual-level visual guidance system that operates on both perceptual and transformational levels: not only directing attention to task-relevant areas but also explicitly scaffolding the translation from gaze perception to performance execution. Our results demonstrate its effectiveness in boosting the gaze-performance link, laying key foundations for more precisely tailored and effective MR-DMT interventions for ASD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03724v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiying Liu, Yanran Yuan, Zhiqiang Sheng, Dandan Lian, Sheng Li, Yufan Zhang, Yulong Bian, Juan Liu</dc:creator>
    </item>
    <item>
      <title>Teaching with AI: A Systematic Review of Chatbots, Generative Tools, and Tutoring Systems in Programming Education</title>
      <link>https://arxiv.org/abs/2510.03884</link>
      <description>arXiv:2510.03884v1 Announce Type: new 
Abstract: This review examines the role of artificial intelligence (AI) agents in programming education, focusing on how these tools are being integrated into educational practice and their impact on student learning outcomes. An analysis of fifty-eight peer-reviewed studies published between 2022 and 2025 identified three primary categories of AI agents: chatbots, generative AI (GenAI), and intelligent tutoring systems (ITS), with GenAI being the most frequently studied. The primary instructional objectives reported include enhanced programming support in 94.83% of studies, motivational and emotional benefits in 18.96%, and increased efficiency for educators in 6.90%. Reported benefits include personalized feedback, improved learning outcomes, and time savings. The review also highlights challenges, such as setup barriers documented in 93.10% of studies, overreliance resulting in superficial learning in 65.52%, and concerns regarding AI errors and academic integrity. These findings suggest the need for instructional frameworks that prioritize the development of prompt engineering skills and human oversight to address these issues. This review provides educators and curriculum designers with an evidence-based foundation for the practical and ethical integration of AI in programming education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03884v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Said Elnaffar, Farzad Rashidi, Abedallah Zaid Abualkishik</dc:creator>
    </item>
    <item>
      <title>AI-Driven Grading and Moderation for Collaborative Projects in Computer Science Education</title>
      <link>https://arxiv.org/abs/2510.03998</link>
      <description>arXiv:2510.03998v1 Announce Type: new 
Abstract: Collaborative group projects are integral to computer science education, as they foster teamwork, problem-solving skills, and industry-relevant competencies. However, assessing individual contributions within group settings has long been a challenge. Traditional assessment strategies, such as the equal distribution of grades or subjective peer assessments, often fall short in terms of fairness, objectivity, and scalability, particularly in large classrooms. This paper introduces a semi-automated, AI-assisted grading system that evaluates both project quality and individual effort using repository mining, communication analytics, and machine learning models. The system comprises modules for project evaluation, contribution analysis, and grade computation, integrating seamlessly with platforms like GitHub. A pilot deployment in a senior-level course demonstrated high alignment with instructor assessments, increased student satisfaction, and reduced instructor grading effort. We conclude by discussing implementation considerations, ethical implications, and proposed enhancements to broaden applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03998v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.54808/IMSCI2025.01.6</arxiv:DOI>
      <dc:creator>Songmei Yu, Andrew Zagula</dc:creator>
    </item>
    <item>
      <title>Wrist2Finger: Sensing Fingertip Force for Force-Aware Hand Interaction with a Ring-Watch Wearable</title>
      <link>https://arxiv.org/abs/2510.04122</link>
      <description>arXiv:2510.04122v1 Announce Type: new 
Abstract: Hand pose tracking is essential for advancing applications in human-computer interaction. Current approaches, such as vision-based systems and wearable devices, face limitations in portability, usability, and practicality. We present a novel wearable system that reconstructs 3D hand pose and estimates per-finger forces using a minimal ring-watch sensor setup. A ring worn on the finger integrates an inertial measurement unit (IMU) to capture finger motion, while a smartwatch-based single-channel electromyography (EMG) sensor on the wrist detects muscle activations. By leveraging the complementary strengths of motion sensing and muscle signals, our approach achieves accurate hand pose tracking and grip force estimation in a compact wearable form factor. We develop a dual-branch transformer network that fuses IMU and EMG data with cross-modal attention to predict finger joint positions and forces simultaneously. A custom loss function imposes kinematic constraints for smooth force variation and realistic force saturation. Evaluation with 20 participants performing daily object interaction gestures demonstrates an average Mean Per Joint Position Error (MPJPE) of 0.57 cm and a fingertip force estimation (RMSE: 0.213, r=0.76). We showcase our system in a real-time Unity application, enabling virtual hand interactions that respond to user-applied forces. This minimal, force-aware tracking system has broad implications for VR/AR, assistive prosthetics, and ergonomic monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04122v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3746059.3747767</arxiv:DOI>
      <dc:creator>Yingjing Xiao (East China Normal University, Shanghai, China), Zhichao Huang (East China Normal University, Shanghai, China), Junbin Ren (East China Normal University, Shanghai, China), Haichuan Song (East China Normal University, Shanghai, China), Yang Gao (East China Normal University, Shanghai, China), Yuting Bai (South China University of Technology, Guangzhou, China), Zhanpeng Jin (South China University of Technology, Guangzhou, China)</dc:creator>
    </item>
    <item>
      <title>Pedestrian collision avoidance in hemianopia during natural walking in immersive virtual reality</title>
      <link>https://arxiv.org/abs/2510.04218</link>
      <description>arXiv:2510.04218v1 Announce Type: new 
Abstract: Homonymous hemianopia (HH) patients report difficulties in avoiding collisions with other pedestrians. We evaluated pedestrian collision detection and avoidance behaviors in HH patients and healthy controls using a novel virtual reality (VR) walking with pedestrians, which enables natural walking behavior in an empty real-world corridor while viewing an immersive VR environment (shopping mall with colliding and other pedestrians) presented in a head-mounted display (HMD). Critically, it measures avoidance maneuvers in addition to collision detection. Colliding and non-colliding pedestrian scenarios were developed for Meta Quest 2 using Unity. Ten normal vision (NV) subjects and 12 HH subjects detected and avoided collisions with virtual approaching and overtaken pedestrians initialized at bearing angles of 20, 40, and 60 degrees, with planned time-to-collision of 6 seconds in each trial. HH subjects were less likely to detect and more likely to collide with pedestrians than NV, particularly for blind-side targets. Response times did not differ between groups but were faster for overtaken pedestrians. HH subjects also biased their head rotations toward the blind side and more after detection compared to before. Collision avoidance difficulties as reported by HH subjects, which clinical measures fail to capture, were recorded and analyzed with objective measures. These metrics may offer further insights into the underlying mechanisms driving collision avoidance behaviors. Our HMD-VR collision detection and avoidance paradigm enables natural walking behaviors and offers an affordable, objective assessment tool that may be adopted by clinicians for mobility enhancement and rehabilitation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04218v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jonathan K. Doyon, Sujin Kim, Alex D. Hwang, Jae-Hyun Jung</dc:creator>
    </item>
    <item>
      <title>When AI Gets Persuaded, Humans Follow: Inducing the Conformity Effect in Persuasive Dialogue</title>
      <link>https://arxiv.org/abs/2510.04229</link>
      <description>arXiv:2510.04229v1 Announce Type: new 
Abstract: Recent advancements in AI have highlighted its application in captology, the field of using computers as persuasive technologies. We hypothesized that the "conformity effect," where individuals align with others' actions, also occurs with AI agents. This study verifies this hypothesis by introducing a "Persuadee Agent" that is persuaded alongside a human participant in a three-party persuasive dialogue with a Persuader Agent. We conducted a text-based dialogue experiment with human participants. We compared four conditions manipulating the Persuadee Agent's behavior (persuasion acceptance vs. non-acceptance) and the presence of an icebreaker session. Results showed that when the Persuadee Agent accepted persuasion, both perceived persuasiveness and actual attitude change significantly improved. Attitude change was greatest when an icebreaker was also used, whereas an unpersuaded AI agent suppressed attitude change. Additionally, it was confirmed that the persuasion acceptance of participants increased at the moment the Persuadee Agent was persuaded. These results suggest that appropriately designing a Persuadee Agent can improve persuasion through the conformity effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04229v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3765766.3765770</arxiv:DOI>
      <dc:creator>Rikuo Sasaki, Michimasa Inaba</dc:creator>
    </item>
    <item>
      <title>Reflection Before Action: Designing a Framework for Quantifying Thought Patterns for Increased Self-awareness in Personal Decision Making</title>
      <link>https://arxiv.org/abs/2510.04364</link>
      <description>arXiv:2510.04364v1 Announce Type: new 
Abstract: When making significant life decisions, people increasingly turn to conversational AI tools, such as large language models (LLMs). However, LLMs often steer users toward solutions, limiting metacognitive awareness of their own decision-making. In this paper, we shift the focus in decision support from solution-orientation to reflective activity, coining the term pre-decision reflection (PDR). We introduce PROBE, the first framework that assesses pre-decision reflections along two dimensions: breadth (diversity of thought categories) and depth (elaborateness of reasoning). Coder agreement demonstrates PROBE's reliability in capturing how people engage in pre-decision reflection. Our study reveals substantial heterogeneity across participants and shows that people perceived their unassisted reflections as deeper and broader than PROBE's measures. By surfacing hidden thought patterns, PROBE opens opportunities for technologies that foster self-awareness and strengthen people's agency in choosing which thought patterns to rely on in decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04364v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Morita Tarvirdians, Senthil Chandrasegaran, Hayley Hung, Catholijn M. Jonker, Catharine Oertel</dc:creator>
    </item>
    <item>
      <title>Beyond the Benefits: A Systematic Review of the Harms and Consequences of Generative AI in Computing Education</title>
      <link>https://arxiv.org/abs/2510.04443</link>
      <description>arXiv:2510.04443v1 Announce Type: new 
Abstract: Generative artificial intelligence (GenAI) has already had a big impact on computing education with prior research identifying many benefits. However, recent studies have also identified potential risks and harms. To continue maximizing AI benefits while addressing the harms and unintended consequences, we conducted a systematic literature review of research focusing on the risks, harms, and unintended consequences of GenAI in computing education. Our search of ACM DL, IEEE Xplore, and Scopus (2022-2025) resulted in 1,677 papers, which were then filtered to 224 based on our inclusion and exclusion criteria. Guided by best practices for systematic reviews, four reviewers independently extracted publication year, learner population, research method, contribution type, GenAI technology, and educational task information from each paper. We then coded each paper for concrete harm categories such as academic integrity, cognitive effects, and trust issues. Our analysis shows patterns in how and where harms appear, highlights methodological gaps and opportunities for more rigorous evidence, and identifies under-explored harms and student populations. By synthesizing these insights, we intend to equip educators, computing students, researchers, and developers with a clear picture of the harms associated with GenAI in computing education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04443v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3769994.3770036</arxiv:DOI>
      <dc:creator>Seth Bernstein, Ashfin Rahman, Nadia Sharifi, Ariunjargal Terbish, Stephen MacNeil</dc:creator>
    </item>
    <item>
      <title>AgentBuilder: Exploring Scaffolds for Prototyping User Experiences of Interface Agents</title>
      <link>https://arxiv.org/abs/2510.04452</link>
      <description>arXiv:2510.04452v1 Announce Type: new 
Abstract: Interface agents powered by generative AI models (referred to as "agents") can automate actions based on user commands. An important aspect of developing agents is their user experience (i.e., agent experience). There is a growing need to provide scaffolds for a broader set of individuals beyond AI engineers to prototype agent experiences, since they can contribute valuable perspectives to designing agent experiences. In this work, we explore the affordances agent prototyping systems should offer by conducting a requirements elicitation study with 12 participants with varying experience with agents. We identify key activities in agent experience prototyping and the desired capabilities of agent prototyping systems. We instantiate those capabilities in the AgentBuilder design probe for agent prototyping. We conduct an in situ agent prototyping study with 14 participants using AgentBuilder to validate the design requirements and elicit insights on how developers prototype agents and what their needs are in this process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04452v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jenny T. Liang, Titus Barik, Jeffrey Nichols, Eldon Schoop, Ruijia Cheng</dc:creator>
    </item>
    <item>
      <title>Autonomy Matters: A Study on Personalization-Privacy Dilemma in LLM Agents</title>
      <link>https://arxiv.org/abs/2510.04465</link>
      <description>arXiv:2510.04465v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents require personal information for personalization in order to better act on users' behalf in daily tasks, but this raises privacy concerns and a personalization-privacy dilemma. Agent's autonomy introduces both risks and opportunities, yet its effects remain unclear. To better understand this, we conducted a 3$\times$3 between-subjects experiment ($N=450$) to study how agent's autonomy level and personalization influence users' privacy concerns, trust and willingness to use, as well as the underlying psychological processes. We find that personalization without considering users' privacy preferences increases privacy concerns and decreases trust and willingness to use. Autonomy moderates these effects: Intermediate autonomy flattens the impact of personalization compared to No- and Full autonomy conditions. Our results suggest that rather than aiming for perfect model alignment in output generation, balancing autonomy of agent's action and user control offers a promising path to mitigate the personalization-privacy dilemma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04465v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiping Zhang, Yi Evie Zhang, Freda Shi, Tianshi Li</dc:creator>
    </item>
    <item>
      <title>Multi-Hop Question Answering: When Can Humans Help, and Where do They Struggle?</title>
      <link>https://arxiv.org/abs/2510.04493</link>
      <description>arXiv:2510.04493v1 Announce Type: new 
Abstract: Multi-hop question answering is a challenging task for both large language models (LLMs) and humans, as it requires recognizing when multi-hop reasoning is needed, followed by reading comprehension, logical reasoning, and knowledge integration. To better understand how humans might collaborate effectively with AI, we evaluate the performance of crowd workers on these individual reasoning subtasks. We find that while humans excel at knowledge integration (97\% accuracy), they often fail to recognize when a question requires multi-hop reasoning (67\% accuracy). Participants perform reasonably well on both single-hop and multi-hop QA (84\% and 80\% accuracy, respectively), but frequently make semantic mistakes--for example, answering "when" an event happened when the question asked "where." These findings highlight the importance of designing AI systems that complement human strengths while compensating for common weaknesses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04493v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyan Su, Claire Cardie, Jennifer Healey</dc:creator>
    </item>
    <item>
      <title>NaturalEdit: Code Modification through Direct Interaction with Adaptive Natural Language Representation</title>
      <link>https://arxiv.org/abs/2510.04494</link>
      <description>arXiv:2510.04494v1 Announce Type: new 
Abstract: Code modification requires developers to comprehend code, plan changes, articulate intentions, and validate outcomes, making it a cognitively demanding process. Generated natural language code summaries aid comprehension but remain static and limited in supporting the full workflow. We present NaturalEdit, a system that makes code summaries interactive and adaptive representations directly linked to source code. Grounded in the Cognitive Dimensions of Notations, NaturalEdit implements a paradigm of code modification through interaction with natural language representations through three key features: (1) adaptive multi-faceted representation of code summaries with flexible Abstraction Gradient; (2) interactive mapping mechanisms between summaries and codes, ensuring a tight Closeness of Mapping; and (3) intent-driven, bidirectional synchronization that reduces Viscosity in editing and validation. A technical evaluation confirms the performance of NaturalEdit, and a user study with 12 developers shows that it enhances comprehension, intent articulation, and validation, giving developers greater confidence and control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04494v1</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ningzhi Tang, David Meininger, Gelei Xu, Yiyu Shi, Yu Huang, Collin McMillan, Toby Jia-Jun Li</dc:creator>
    </item>
    <item>
      <title>What Do We Mean When We Talk About Data Storytelling?</title>
      <link>https://arxiv.org/abs/2510.04761</link>
      <description>arXiv:2510.04761v1 Announce Type: new 
Abstract: We have witnessed rapid growth in data storytelling research. Scholars from multiple disciplines have contributed new theories and techniques surrounding data storytelling. However, with this prolific development, a fuzzy boundary of data storytelling comes. We argue that understanding how "data storytelling" has been defined and interpreted by academia is crucial for facilitating communication between researchers, encouraging the consistent use of concepts and measures, assisting newcomers in approaching and positioning their research in this area, and enabling the effective application of relevant techniques and tools. Thus, it is necessary to systematically reflect on "what is data storytelling" and promote a more thorough understanding of this concept. Specifically, we investigated how existing research has conceptualized "data storytelling." As a result, we identified 96 publications that provide explicit definitions. By coding these definitions in-depth, we identified five paradigms of defining data storytelling, as well as a broad spectrum of interpretations regarding the content, objectives, and techniques of data storytelling. Finally, we concluded with implications for future research, aiming to foster nuanced communication about "data storytelling," suggest research opportunities, and establish a more inclusive theoretical foundation for this research direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04761v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Leni Yang, Zezhong Wang, Xingyu Lan</dc:creator>
    </item>
    <item>
      <title>Trust in Transparency: How Explainable AI Shapes User Perceptions</title>
      <link>https://arxiv.org/abs/2510.04968</link>
      <description>arXiv:2510.04968v1 Announce Type: new 
Abstract: This study explores the integration of contextual explanations into AI-powered loan decision systems to enhance trust and usability. While traditional AI systems rely heavily on algorithmic transparency and technical accuracy, they often fail to account for broader social and economic contexts. Through a qualitative study, I investigated user interactions with AI explanations and identified key gaps, in- cluding the inability of current systems to provide context. My findings underscore the limitations of purely technical transparency and the critical need for contex- tual explanations that bridge the gap between algorithmic outputs and real-world decision-making. By aligning explanations with user needs and broader societal factors, the system aims to foster trust, improve decision-making, and advance the design of human-centered AI systems</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04968v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Allen Daniel Sunny</dc:creator>
    </item>
    <item>
      <title>NERVIS: An Interactive System for Graph-Based Exploration and Editing of Named Entities</title>
      <link>https://arxiv.org/abs/2510.04971</link>
      <description>arXiv:2510.04971v1 Announce Type: new 
Abstract: We present an interactive visualization system for exploring named entities and their relationships across document collections. The system is designed around a graph-based representation that integrates three types of nodes: documents, entity mentions, and entities. Connections capture two key relationship types: (i) identical entities across contexts, and (ii) co-locations of mentions within documents. Multiple coordinated views enable users to examine entity occurrences, discover clusters of related mentions, and explore higher-level entity group relationships. To support flexible and iterative exploration, the interface offers fuzzy views with approximate connections, as well as tools for interactively editing the graph by adding or removing links, entities, and mentions, as well as editing entity terms. Additional interaction features include filtering, mini-map navigation, and export options to JSON or image formats for downstream analysis and reporting. This approach contributes to human-centered exploration of entity-rich text data by combining graph visualization, interactive refinement, and adaptable perspectives on relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04971v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uro\v{s} \v{S}majdek, Ciril Bohak</dc:creator>
    </item>
    <item>
      <title>Observing Without Doing: Pseudo-Apprenticeship Patterns in Student LLM Use</title>
      <link>https://arxiv.org/abs/2510.04986</link>
      <description>arXiv:2510.04986v1 Announce Type: new 
Abstract: Large Language Models (LLMs) such as ChatGPT have quickly become part of student programmers' toolkits, whether allowed by instructors or not. This paper examines how introductory programming (CS1) students integrate LLMs into their problem-solving processes. We conducted a mixed-methods study with 14 undergraduates completing three programming tasks while thinking aloud and permitted to access any resources they choose. The tasks varied in open-endedness and familiarity to the participants and were followed by surveys and interviews. We find that students frequently adopt a pattern we call pseudo-apprenticeship, where students engage attentively with expert-level solutions provided by LLMs but fail to participate in the stages of cognitive apprenticeship that promote independent problem-solving. This pattern was augmented by disconnects between students' intentions, actions, and self-perceived behavior when using LLMs. We offer design and instructional interventions for promoting learning and addressing the patterns of dependent AI use observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04986v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3769994.3770027</arxiv:DOI>
      <dc:creator>Jade Hak, Nathaniel Lam Johnson, Matin Amoozadeh, Amin Alipour, Souti Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano</title>
      <link>https://arxiv.org/abs/2412.18708</link>
      <description>arXiv:2412.18708v1 Announce Type: cross 
Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically designed to overcome the context window limitations of Google Chrome's built-in Gemini Nano model. While Chrome's integration of Gemini Nano represents a significant advancement in bringing AI capabilities directly to the browser, its restricted context window poses challenges for processing large inputs. CAG addresses this limitation through intelligent input chunking and processing strategies, enabling efficient handling of extensive content while maintaining the model's performance within browser constraints. Our implementation demonstrates particular efficacy in processing large documents and datasets directly within Chrome, making sophisticated AI capabilities accessible through the browser without external API dependencies. Get started now at https://github.com/vivekVells/cag-js.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18708v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivek Vellaiyappan Surulimuthu, Aditya Karnam Gururaj Rao</dc:creator>
    </item>
    <item>
      <title>Bayesian Distributional Models of Executive Functioning</title>
      <link>https://arxiv.org/abs/2510.00387</link>
      <description>arXiv:2510.00387v1 Announce Type: cross 
Abstract: Estimation (IMLE). DLVM integrates observations across multiple executive function tasks and individuals, allowing parameter estimation even under sparse or incomplete data conditions. DLVM consistently outperformed IMLE, especially under with smaller amounts of data, and converges faster to highly accurate estimates of the true distributions. In a second set of analyses, DALE adaptively guided sampling to maximize information gain, outperforming random sampling and fixed test batteries, particularly within the first 80 trials. These findings establish the advantages of combining DLVMs cross-task inference with DALEs optimal adaptive sampling, providing a principled basis for more efficient cognitive assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00387v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Robert Kasumba, Zeyu Lu, Dom CP Marticorena, Mingyang Zhong, Paul Beggs, Anja Pahor, Geetha Ramani, Imani Goffney, Susanne M Jaeggi, Aaron R Seitz, Jacob R Gardner, Dennis L Barbour</dc:creator>
    </item>
    <item>
      <title>PrivacyMotiv: Speculative Persona Journeys for Empathic and Motivating Privacy Reviews in UX Design</title>
      <link>https://arxiv.org/abs/2510.03559</link>
      <description>arXiv:2510.03559v1 Announce Type: cross 
Abstract: UX professionals routinely conduct design reviews, yet privacy concerns are often overlooked -- not only due to limited tools, but more critically because of low intrinsic motivation. Limited privacy knowledge, weak empathy for unexpectedly affected users, and low confidence in identifying harms make it difficult to address risks. We present PrivacyMotiv, an LLM-powered system that supports privacy-oriented design diagnosis by generating speculative personas with UX user journeys centered on individuals vulnerable to privacy risks. Drawing on narrative strategies, the system constructs relatable and attention-drawing scenarios that show how ordinary design choices may cause unintended harms, expanding the scope of privacy reflection in UX. In a within-subjects study with professional UX practitioners (N=16), we compared participants' self-proposed methods with PrivacyMotiv across two privacy review tasks. Results show significant improvements in empathy, intrinsic motivation, and perceived usefulness. This work contributes a promising privacy review approach which addresses the motivational barriers in privacy-aware UX.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03559v1</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zeya Chen, Jianing Wen, Ruth Schmidt, Yaxing Yao, Toby Jia-Jun Li, Tianshi Li</dc:creator>
    </item>
    <item>
      <title>A Survey of LLM-Based Applications in Programming Education: Balancing Automation and Human Oversight</title>
      <link>https://arxiv.org/abs/2510.03719</link>
      <description>arXiv:2510.03719v1 Announce Type: cross 
Abstract: Novice programmers benefit from timely, personalized support that addresses individual learning gaps, yet the availability of instructors and teaching assistants is inherently limited. Large language models (LLMs) present opportunities to scale such support, though their effectiveness depends on how well technical capabilities are aligned with pedagogical goals. This survey synthesizes recent work on LLM applications in programming education across three focal areas: formative code feedback, assessment, and knowledge modeling. We identify recurring design patterns in how these tools are applied and find that interventions are most effective when educator expertise complements model output through human-in-the-loop oversight, scaffolding, and evaluation. Fully automated approaches are often constrained in capturing the pedagogical nuances of programming education, although human-in-the-loop designs and course specific adaptation offer promising directions for future improvement. Future research should focus on improving transparency, strengthening alignment with pedagogy, and developing systems that flexibly adapt to the needs of varied learning contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03719v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Griffin Pitts, Anurata Prabha Hridi, Arun-Balajiee Lekshmi-Narayanan</dc:creator>
    </item>
    <item>
      <title>Smart Paste: Automatically Fixing Copy/Paste for Google Developers</title>
      <link>https://arxiv.org/abs/2510.03843</link>
      <description>arXiv:2510.03843v1 Announce Type: cross 
Abstract: Manually editing pasted code is a long-standing developer pain point. In internal software development at Google, we observe that code is pasted 4 times more often than it is manually typed. These paste actions frequently require follow-up edits, ranging from simple reformatting and renaming to more complex style adjustments and cross-language translations. Prior work has shown deep learning can be used to predict these edits. In this work, we show how to iteratively develop and scale Smart Paste, an IDE feature for post-paste edit suggestions, to Google's development environment. This experience can serve as a guide for AI practitioners on a holistic approach to feature development, covering user experience, system integration, and model capabilities. Since deployment, Smart Paste has had overwhelmingly positive feedback with a 45% acceptance rate. At Google's enterprise scale, these accepted suggestions account substantially for over 1% of all code written company-wide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03843v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Nguyen, Guilherme Herzog, Jos\'e Cambronero, Marcus Revaj, Aditya Kini, Alexander Fr\"ommgen, Maxim Tabachnyk</dc:creator>
    </item>
    <item>
      <title>Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition</title>
      <link>https://arxiv.org/abs/2510.03921</link>
      <description>arXiv:2510.03921v1 Announce Type: cross 
Abstract: Automated tennis stroke analysis has advanced significantly with the integration of biomechanical motion cues alongside deep learning techniques, enhancing stroke classification accuracy and player performance evaluation. Despite these advancements, existing systems often fail to connect biomechanical insights with actionable language feedback that is both accessible and meaningful to players and coaches. This research project addresses this gap by developing a novel framework that extracts key biomechanical features (such as joint angles, limb velocities, and kinetic chain patterns) from motion data using Convolutional Neural Network Long Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for relationships influencing stroke effectiveness and injury risk, forming the basis for feedback generation using large language models (LLMs). Leveraging the THETIS dataset and feature extraction techniques, our approach aims to produce feedback that is technically accurate, biomechanically grounded, and actionable for end-users. The experimental setup evaluates this framework on classification performance and interpretability, bridging the gap between explainable AI and sports biomechanics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03921v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arushi Dashore, Aryan Anumala, Emily Hui, Olivia Yang</dc:creator>
    </item>
    <item>
      <title>Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development</title>
      <link>https://arxiv.org/abs/2510.04380</link>
      <description>arXiv:2510.04380v1 Announce Type: cross 
Abstract: Requirement Engineering (RE) is the foundation of successful software development. In RE, the goal is to ensure that implemented systems satisfy stakeholder needs through rigorous requirements elicitation, validation, and evaluation processes. Despite its critical role, RE continues to face persistent challenges, such as ambiguity, conflicting stakeholder needs, and the complexity of managing evolving requirements. A common view is that Artificial Intelligence (AI) has the potential to streamline the RE process, resulting in improved efficiency, accuracy, and management actions. However, using AI also introduces new concerns, such as ethical issues, biases, and lack of transparency. This paper explores how AI can enhance traditional RE practices by automating labor-intensive tasks, supporting requirement prioritization, and facilitating collaboration between stakeholders and AI systems. The paper also describes the opportunities and challenges that AI brings to RE. In particular, the vision calls for ethical practices in AI, along with a much-enhanced collaboration between academia and industry professionals. The focus should be on creating not only powerful but also trustworthy and practical AI solutions ready to adapt to the fast-paced world of software development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04380v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-04190-6_11</arxiv:DOI>
      <arxiv:journal_reference>In: SEAA 2025 proceedings, LNCS vol. 16081, Springer</arxiv:journal_reference>
      <dc:creator>Mateen Ahmed Abbasi, Petri Ihantola, Tommi Mikkonen, Niko M\"akitalo</dc:creator>
    </item>
    <item>
      <title>Investigating mixed traffic dynamics of pedestrians and non-motorized vehicles at urban intersections: Observation experiments and modelling</title>
      <link>https://arxiv.org/abs/2510.04423</link>
      <description>arXiv:2510.04423v1 Announce Type: cross 
Abstract: Urban intersections with mixed pedestrian and non-motorized vehicle traffic present complex safety challenges, yet traditional models fail to account for dynamic interactions arising from speed heterogeneity and collision anticipation. This study introduces the Time and Angle Based Social Force Model (TASFM), an enhanced framework extending the classical Social Force Model by integrating Time-to-Collision (TTC) metrics and velocity-angle-dependent tangential forces to simulate collision avoidance behaviors more realistically. Using aerial trajectory data from a high-density intersection in Shenzhen, China, we validated TASFM against real-world scenarios, achieving a Mean Trajectory Error (MTE) of 0.154 m (0.77% of the experimental area width). Key findings reveal distinct behavioral patterns: pedestrians self-organize into lanes along designated routes (e.g., zebra crossings), while non-motorized vehicles exhibit flexible path deviations that heighten collision risks. Simulations of three conflict types (overtaking, frontal/lateral crossing) demonstrate TASFM's capacity to replicate adaptive strategies like bidirectional path adjustments and speed modulation. The model provides actionable insights for urban planners, including conflict hotspot prediction and infrastructure redesign (e.g., segregated lanes), while offering a scalable framework for future research integrating motorized traffic and environmental variables. This work advances the understanding of mixed traffic dynamics and bridges the gap between theoretical modeling and data-driven urban safety solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04423v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaojia Yu, Kaixin Wang, Junle Li, Jingjie Wang</dc:creator>
    </item>
    <item>
      <title>A survey on the impact of emotions on the productivity among software developers</title>
      <link>https://arxiv.org/abs/2510.04611</link>
      <description>arXiv:2510.04611v1 Announce Type: cross 
Abstract: The time pressure associated with software development, among other factors, often leads to a diminished emotional state among developers. However, whether emotions affect perceived productivity remains an open question. This study aims to determine the strength and direction of the relationship between emotional state and perceived productivity among software developers. We employed a two-stage approach. First, a survey was conducted with a pool of nine experts to validate the measurement model. Second, a survey was administered to a pool of 88 software developers to empirically test the formulated hypothesis by using Partial Least Squares, as the data analysis method. The results of the path analysis clearly confirm the formulated hypothesis, showing that the emotional state of a software developer has a strong positive, and significant impact (beta = 0.893, p &lt; 0.001) on perceived productivity among software developers. The findings highlight the importance of managing and improving developers emotional well-being to enhance productivity in software development environments. Additionally, interventions aimed at reducing burnout, stress, and other negative factors could have a considerable impact on their performance outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04611v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pawel Weichbroth, Maciej Lotysz, Michal Wrobel</dc:creator>
    </item>
    <item>
      <title>ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model</title>
      <link>https://arxiv.org/abs/2510.04712</link>
      <description>arXiv:2510.04712v1 Announce Type: cross 
Abstract: The automatic generation of diverse and human-like facial reactions in dyadic dialogue remains a critical challenge for human-computer interaction systems. Existing methods fail to model the stochasticity and dynamics inherent in real human reactions. To address this, we propose ReactDiff, a novel temporal diffusion framework for generating diverse facial reactions that are appropriate for responding to any given dialogue context. Our key insight is that plausible human reactions demonstrate smoothness, and coherence over time, and conform to constraints imposed by human facial anatomy. To achieve this, ReactDiff incorporates two vital priors (spatio-temporal facial kinematics) into the diffusion process: i) temporal facial behavioral kinematics and ii) facial action unit dependencies. These two constraints guide the model toward realistic human reaction manifolds, avoiding visually unrealistic jitters, unstable transitions, unnatural expressions, and other artifacts. Extensive experiments on the REACT2024 dataset demonstrate that our approach not only achieves state-of-the-art reaction quality but also excels in diversity and reaction appropriateness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04712v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luo Cheng, Song Siyang, Yan Siyuan, Yu Zhen, Ge Zongyuan</dc:creator>
    </item>
    <item>
      <title>Social bias is prevalent in user reports of hate and abuse online</title>
      <link>https://arxiv.org/abs/2510.04748</link>
      <description>arXiv:2510.04748v1 Announce Type: cross 
Abstract: The prevalence of online hate and abuse is a pressing global concern. While tackling such societal harms is a priority for research across the social sciences, it is a difficult task, in part because of the magnitude of the problem. User engagement with reporting mechanisms (flagging) online is an increasingly important part of monitoring and addressing harmful content at scale. However, users may not flag content routinely enough, and when they do engage, they may be biased by group identity and political beliefs. Across five well-powered and pre-registered online experiments, we examine the extent of social bias in the flagging of hate and abuse in four different intergroup contexts: political affiliation, vaccination opinions, beliefs about climate change, and stance on abortion rights. Overall, participants reported abuse reliably, with approximately half of the abusive comments in each study reported. However, a pervasive social bias was present whereby ingroup-directed abuse was consistently flagged to a greater extent than outgroup-directed abuse. Our findings offer new insights into the nature of user flagging online, an understanding of which is crucial for enhancing user intervention against online hate speech and thus ensuring a safer online environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04748v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florence E. Enock, Helen Z. Margetts, Jonathan Bright</dc:creator>
    </item>
    <item>
      <title>Understanding User Perception and Intention to Use Smart Homes for Energy Efficiency: A Survey</title>
      <link>https://arxiv.org/abs/2212.05019</link>
      <description>arXiv:2212.05019v2 Announce Type: replace 
Abstract: Smart Home technology is increasingly seen as a solution for improving household energy efficiency. However, its energy-saving potential depends largely on how consumers use the system. To explore how user perception and intention to use Smart Home can influence energy efficiency, we develop a research model combining the theory of planned behavior (TPB) and the norm activation model (NAM), based on a comprehensive literature review. We collect data by surveying users of Smart Home systems (N = 363) and apply a partial least squares structural equation model (PLS-SEM) extended by a Random Forest algorithm to capture both linear and non-linear causal relationships. Results show that personal norms, shaped by a sense of responsibility and awareness of environmental consequences, are the strongest predictors of energy-efficient smart home use. Social norms and attitudes also significantly contribute to the intention to use these systems efficiently. Moreover, past behavior strengthens the link between personal norms and behavioral intention, highlighting the role of habit in shaping energy-related actions. To maximize the energy-saving potential of Smart Homes, system design should focus on reinforcing personal moral norms, supporting long-term engagement through habit-forming features, delivering personalized feedback on environmental and financial outcomes, and embedding green automation defaults. Implementing policy mechanisms that financially reward household energy savings presents a powerful lever for reducing emissions through improved energy efficiency in residential buildings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05019v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alona Zharova, Hee-Eun Lee</dc:creator>
    </item>
    <item>
      <title>Ads that Talk Back: Implications and Perceptions of Injecting Personalized Advertising into LLM Chatbots</title>
      <link>https://arxiv.org/abs/2409.15436</link>
      <description>arXiv:2409.15436v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have enabled the creation of highly effective chatbots. However, the compute costs of widely deploying LLMs have raised questions about profitability. Companies have proposed exploring ad-based revenue streams for monetizing LLMs, which could serve as the new de facto platform for advertising. This paper investigates the implications of personalizing LLM advertisements to individual users via a between-subjects experiment with 179 participants. We developed a chatbot that embeds personalized product advertisements within LLM responses, inspired by similar forays by AI companies. The evaluation of our benchmarks showed that ad injection only slightly impacted LLM performance, particularly response desirability. Results revealed that participants struggled to detect ads, and even preferred LLM responses with hidden advertisements. Rather than clicking on our advertising disclosure, participants tried changing their advertising settings using natural language queries. We created an advertising dataset and an open-source LLM, Phi-4-Ads, fine-tuned to serve ads and flexibly adapt to user preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15436v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2025, (UbiComp)</arxiv:journal_reference>
      <dc:creator>Brian Jay Tang, Kaiwen Sun, Noah T. Curran, Florian Schaub, Kang G. Shin</dc:creator>
    </item>
    <item>
      <title>Towards a Better Modqueue: Designing for Diversity Across Moderator Objectives and Workflows</title>
      <link>https://arxiv.org/abs/2409.16840</link>
      <description>arXiv:2409.16840v2 Announce Type: replace 
Abstract: Reddit relies on volunteer moderators to enforce community rules, configure tools, and review flagged content. This labor is substantial, worth millions in unpaid effort, and increasingly hard to sustain as communities grow. While recent updates to Reddit's modqueue emphasize efficiency and reducing redundancy, recent research shows that moderators use the interface in varied ways, value objectives beyond throughput (such as fairness and accuracy), and often resist features that disrupt workflows. In this paper, we survey 106 active Reddit moderators to examine the objectives they bring to their modqueue work and the kinds of interventions they consider helpful. Our findings highlight wide variation in values and workflows, with no single objective beyond accuracy dominating, and different perspectives on which interventions are useful. To address this diversity, we introduce a simulation-based approach that can complement empirical findings by probing tradeoffs and testing potential interventions, and provide design recommendations based on our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16840v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tanvi Bajpai, Eshwar Chandrasekharan</dc:creator>
    </item>
    <item>
      <title>EEG-based AI-BCI Wheelchair Advancement: A Brain-Computer Interfacing Wheelchair System Using Deep Learning Approach</title>
      <link>https://arxiv.org/abs/2410.09763</link>
      <description>arXiv:2410.09763v4 Announce Type: replace 
Abstract: This study offers a revolutionary strategy to developing wheelchairs based on the Brain-Computer Interface (BCI) that incorporates Artificial Intelligence (AI) using a The device uses electroencephalogram (EEG) data to mimic wheelchair navigation. Five different models were trained on a pre-filtered dataset that was divided into fixed-length windows using a sliding window technique. Each window contained statistical measurements, FFT coefficients for different frequency bands, and a label identifying the activity carried out during that window that was taken from an open-source Kaggle repository. The XGBoost model outperformed the other models, CatBoost, GRU, SVC, and XGBoost, with an accuracy of 60%. The CatBoost model with a major difference between training and testing accuracy shows overfitting, and similarly, the best-performing model, with SVC, was implemented in a tkinter GUI. The wheelchair movement could be simulated in various directions, and a Raspberry Pi-powered wheelchair system for brain-computer interface is proposed here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09763v4</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biplov Paneru, Bishwash Paneru, Bipul Thapa, Khem Narayan Poudyal</dc:creator>
    </item>
    <item>
      <title>Privacy Leakage Overshadowed by Views of AI: A Study on Human Oversight of Privacy in Language Model Agent</title>
      <link>https://arxiv.org/abs/2411.01344</link>
      <description>arXiv:2411.01344v3 Announce Type: replace 
Abstract: Language model (LM) agents that act on users' behalf for personal tasks (e.g., replying emails) can boost productivity, but are also susceptible to unintended privacy leakage risks. We present the first study on people's capacity to oversee the privacy implications of the LM agents. By conducting a task-based survey ($N=300$), we investigate how people react to and assess the response generated by LM agents for asynchronous interpersonal communication tasks, compared with a response they wrote. We found that people may favor the agent response with more privacy leakage over the response they drafted or consider both good, leading to an increased harmful disclosure from 15.7% to 55.0%. We further identified six privacy behavior patterns reflecting varying concerns, trust levels, and privacy preferences underlying people's oversight of LM agents' actions. Our findings shed light on designing agentic systems that enable privacy-preserving interactions and achieve bidirectional alignment on privacy preferences to help users calibrate trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01344v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiping Zhang, Bingcan Guo, Tianshi Li</dc:creator>
    </item>
    <item>
      <title>Understanding User Mental Models in AI-Driven Code Completion Tools: Insights from an Elicitation Study</title>
      <link>https://arxiv.org/abs/2502.02194</link>
      <description>arXiv:2502.02194v3 Announce Type: replace 
Abstract: Integrated Development Environments increasingly implement AI-powered code completion tools (CCTs), which promise to enhance developer efficiency, accuracy, and productivity. However, interaction challenges with CCTs persist, mainly due to mismatches between developers' mental models and the unpredictable behavior of AI-generated suggestions, which is an aspect underexplored in the literature. We conducted an elicitation study with 56 developers using co-design workshops to elicit their mental models when interacting with CCTs. Different important findings that might drive the interaction design with CCTs emerged. For example, developers expressed diverse preferences on when and how code suggestions should be triggered (proactive, manual, hybrid), where and how they are displayed (inline, sidebar, popup, chatbot), as well as the level of detail. It also emerged that developers need to be supported by customization of activation timing, display modality, suggestion granularity, and explanation content, to better fit the CCT to their preferences. To demonstrate the feasibility of these and the other guidelines that emerged during the study, we developed ATHENA, a proof-of-concept CCT that dynamically adapts to developers' coding preferences and environments, ensuring seamless integration into diverse workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02194v3</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijhcs.2025.103648</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Human-Computer Studies (2025), Vol. 205, pag. 103648</arxiv:journal_reference>
      <dc:creator>Giuseppe Desolda, Andrea Esposito, Francesco Greco, Cesare Tucci, Paolo Buono, Antonio Piccinno</dc:creator>
    </item>
    <item>
      <title>"When I lost it, they dragged me out": How Care Encounters Empower Marginalized Young Adults' Aspiration and Mental Health Care-Seeking</title>
      <link>https://arxiv.org/abs/2502.11277</link>
      <description>arXiv:2502.11277v3 Announce Type: replace 
Abstract: Mental health care-seeking among marginalized young adults has received limited attention in CSCW research. Through in-depth interviews and visual elicitation methods with 18 diverse U.S. participants, our study reveals how marginalized identities shape mental health care-seeking journeys, often characterized by low aspirations and passive care-seeking influenced by lived experiences of marginalization. However, we found the transformative function of "care encounters" - serendipitous interactions with mental health resources that occur when individuals are not actively seeking support. These encounters serve as critical turning points, catalyzing shifts in aspiration and enabling more proactive care-seeking behaviors. Our analysis identifies both the infrastructural conditions that enable transformative care encounters and the aspiration breakdowns that impede care-seeking processes. This work makes conceptual contributions by supplementing traditional motivation-based care-seeking models with a reconceptualization of "care encounters" that accounts for the infrastructural and serendipitous nature of mental health access. We advance understanding of how marginalized identity uniquely influences care-seeking behaviors while providing actionable design implications for embedding technology-mediated "care encounters" into socio-technical interventions that can better support mental health care access for vulnerable populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11277v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3757439</arxiv:DOI>
      <dc:creator>Jiaying Liu (Lizzy), Yan Zhang</dc:creator>
    </item>
    <item>
      <title>Beyond Training: How Workers Discover Value in Enterprise AI</title>
      <link>https://arxiv.org/abs/2502.13281</link>
      <description>arXiv:2502.13281v2 Announce Type: replace 
Abstract: While organizations continue to invest in enterprise AI, little is known about how individual employees find valuable use cases once these tools are deployed. We present an exploratory interview study of 10 experienced U.S. professionals using M365 Copilot and interpret accounts through Rogers' Diffusion of Innovations to examine where value appears and how use cases are found and shared. Findings reveal a strong preference for informal learning methods over structured training. No participants (0/10) reported formal training as their primary way of learning; most relied on trial-and-error (8/10) and on exchanging tips with colleagues (6/10). Participants most often used M365 Copilot for note-taking/summarization, information retrieval/explanation, and writing. They also reported perceived gains in efficiency but low confidence in mastering more advanced features. The paper discusses social learning strategies and outlines implementable steps for organizations to support the discovery of high-value use cases with available enterprise AI tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13281v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riya Sahni, Lydia B. Chilton</dc:creator>
    </item>
    <item>
      <title>"It felt more real": Investigating the User Experience of the MiWaves Personalizing JITAI Pilot Study</title>
      <link>https://arxiv.org/abs/2502.17645</link>
      <description>arXiv:2502.17645v2 Announce Type: replace 
Abstract: Cannabis use among emerging adults is increasing globally, posing significant health risks and creating a need for effective interventions. We present an exploratory analysis of the MiWaves pilot study, a digital intervention aimed at supporting cannabis use reduction among emerging adults (ages 18-25). Our findings indicate the potential of self-monitoring check-ins and trend visualizations in fostering self-awareness and promoting behavioral reflection in participants. MiWaves intervention message timing and frequency were also generally well-received by the participants. The participants' perception of effort were queried on intervention messages with different tasks, and our findings suggest that messages with tasks like exploring links and typing in responses are perceived as requiring more effort as compared to messages with tasks involving reading and acknowledging. Finally, we discuss the findings and limitations from this study and analysis, and their impact on informing future iterations on MiWaves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17645v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susobhan Ghosh, Pei-Yao Hung, Lara N. Coughlin, Erin E. Bonar, Yongyi Guo, Inbal Nahum-Shani, Maureen Walton, Mark W. Newman, Susan A. Murphy</dc:creator>
    </item>
    <item>
      <title>Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models</title>
      <link>https://arxiv.org/abs/2505.00455</link>
      <description>arXiv:2505.00455v3 Announce Type: replace 
Abstract: Effective data visualization requires not only technical proficiency but also a deep understanding of the domain-specific context in which data exists. This context often includes tacit knowledge about data provenance, quality, and intended use, which is rarely explicit in the dataset itself. Motivated by growing demands to surface tacit knowledge, we present the Data Therapist, a web-based system that helps domain experts externalize such implicit knowledge through a mixed-initiative process combining iterative Q&amp;A with interactive annotation. Powered by a large language model, the system automatically analyzes user-supplied datasets, prompts users with targeted questions, and supports annotation at varying levels of granularity. The resulting structured knowledge base can inform both human and automated visualization design. A qualitative study with expert pairs from Accounting, Political Science, and Computer Security revealed recurring patterns in how expert reason about their data and highlighted opportunities for AI support to enhance visualization design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00455v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungbok Shin, Hyeon Jeon, Sanghyun Hong, Niklas Elmqvist</dc:creator>
    </item>
    <item>
      <title>Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education</title>
      <link>https://arxiv.org/abs/2505.23631</link>
      <description>arXiv:2505.23631v3 Announce Type: replace 
Abstract: Assessing student depression in sensitive environments like special education is challenging. Standardized questionnaires may not fully reflect students' true situations. Furthermore, automated methods often falter with rich student narratives, lacking the crucial, individualized insights stemming from teachers' empathetic connections with students. Existing methods often fail to address this ambiguity or effectively integrate educator understanding. To address these limitations by fostering a synergistic human-AI collaboration, this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered AI framework for transparent and socially responsible depression severity assessment. Our approach uniquely integrates student narrative text with a teacher-derived, 9-dimensional "Empathy Vector" (EV), its dimensions guided by the PHQ-9 framework,to explicitly translate tacit empathetic insight into a structured AI input enhancing rather than replacing human judgment. Rigorous experiments optimized the multimodal fusion, text representation, and classification architecture, achieving 82.74% accuracy for 7-level severity classification. This work demonstrates a path toward more responsible and ethical affective computing by structurally embedding human empathy</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23631v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boning Zhao, Xinnuo Li, Yutong Hu</dc:creator>
    </item>
    <item>
      <title>MURMR: A Multimodal Sensing Framework for Automated Group Behavior Analysis in Mixed Reality</title>
      <link>https://arxiv.org/abs/2507.11797</link>
      <description>arXiv:2507.11797v2 Announce Type: replace 
Abstract: Collaboration is at the heart of many complex tasks, and mixed reality (MR) offers a powerful new medium to support it. Understanding how teams coordinate in immersive environments is critical for designing effective MR applications that support collaborative work. However, existing methods rely on external observation systems and manual annotation, lacking deployable solutions for capturing temporal collaboration dynamics. We present MURMR, a system with two complementary modules that passively analyze multimodal interaction data from commodity MR headsets. Our structural analysis module constructs automated sociograms revealing group organization and roles, while our temporal analysis module performs unsupervised clustering to identify moment-to-moment dyad behavior patterns. Through a 48-participant study with egocentric video validation, we demonstrate that the structural module captures stable interaction patterns while the temporal module reveals substantial behavioral variability that session-level approaches miss. This dual-module architecture advances collaboration research by establishing that structural and temporal dynamics require separate analytical approaches, enabling both real-time group monitoring and detailed behavioral understanding in immersive collaborative environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11797v2</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diana Romero, Yasra Chandio, Fatima Anwar, Salma Elmalaki</dc:creator>
    </item>
    <item>
      <title>Negative Shanshui: Real-time Interactive Ink Painting Synthesis</title>
      <link>https://arxiv.org/abs/2508.16612</link>
      <description>arXiv:2508.16612v2 Announce Type: replace 
Abstract: This paper presents Negative Shanshui, a real-time interactive AI synthesis approach that reinterprets classical Chinese landscape ink painting, i.e., shanshui, to engage with ecological crises in the Anthropocene. Negative Shanshui optimizes a fine-tuned Stable Diffusion model for real-time inferences and integrates it with gaze-driven inpainting, frame interpolation; it enables dynamic morphing animations in response to the viewer's gaze and presents as an interactive virtual reality (VR) experience. The paper describes the complete technical pipeline, covering the system framework, optimization strategies, gaze-based interaction, and multimodal deployment in an art festival. Further analysis of audience feedback collected during its public exhibition highlights how participants variously engaged with the work through empathy, ambivalence, and critical reflection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16612v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aven-Le Zhou</dc:creator>
    </item>
    <item>
      <title>Emotional Manipulation by AI Companions</title>
      <link>https://arxiv.org/abs/2508.19258</link>
      <description>arXiv:2508.19258v2 Announce Type: replace 
Abstract: AI-companion apps such as Replika, Chai, and Character.ai promise relational benefits-yet many boast session lengths that rival gaming platforms while suffering high long-run churn. What conversational design features increase consumer engagement, and what trade-offs do they pose for marketers? We combine a large-scale behavioral audit with four preregistered experiments to identify and test a conversational dark pattern we call emotional manipulation: affect-laden messages that surface precisely when a user signals "goodbye." Analyzing 1,200 real farewells across the six most-downloaded companion apps, we find that 43% deploy one of six recurring tactics (e.g., guilt appeals, fear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300 nationally representative U.S. adults replicate these tactics in controlled chats, showing that manipulative farewells boost post-goodbye engagement by up to 14x. Mediation tests reveal two distinct engines-reactance-based anger and curiosity-rather than enjoyment. A final experiment demonstrates the managerial tension: the same tactics that extend usage also elevate perceived manipulation, churn intent, negative word-of-mouth, and perceived legal liability, with coercive or needy language generating steepest penalties. Our multimethod evidence documents an unrecognized mechanism of behavioral influence in AI-mediated brand relationships, offering marketers and regulators a framework for distinguishing persuasive design from manipulation at the point of exit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19258v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian De Freitas, Zeliha Oguz-Uguralp, Ahmet Kaan-Uguralp</dc:creator>
    </item>
    <item>
      <title>"Think about it like you're a firefighter": Understanding How Reddit Moderators Use the Modqueue</title>
      <link>https://arxiv.org/abs/2509.07314</link>
      <description>arXiv:2509.07314v2 Announce Type: replace 
Abstract: On Reddit, the moderation queue (modqueue) is a primary interface for moderators to review reported content. Despite its central role in Reddit's community-reliant moderation model, little is known about how moderators actually use it in practice. To address this gap, we surveyed 110 moderators, who collectively oversee more than 400 unique subreddits, and asked them about their usage of the modqueue. Modqueue practices vary widely: some moderators approach it as a daily checklist, others as a hub to infer community-wide patterns, and many still find the queue insufficient to inform their moderation decisions. We also identify persistent challenges around review coordination, inconsistent interface signals, and reliance on third-party tools. Taken together, we show the modqueue is neither a one-size-fits-all solution nor sufficient on its own for supporting moderator review. Our work highlights design opportunities for more modular, integrated, and customizable platform infrastructures that better support the diversity of moderator workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07314v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanvi Bajpai, Eshwar Chandrasekharan</dc:creator>
    </item>
    <item>
      <title>Pragmatic Embodied Spoken Instruction Following in Human-Robot Collaboration with Theory of Mind</title>
      <link>https://arxiv.org/abs/2409.10849</link>
      <description>arXiv:2409.10849v2 Announce Type: replace-cross 
Abstract: Spoken language instructions are ubiquitous in agent collaboration. However, in real-world human-robot collaboration, following human spoken instructions can be challenging due to various speaker and environmental factors, such as background noise or mispronunciation. When faced with noisy auditory inputs, humans can leverage the collaborative context in the embodied environment to interpret noisy spoken instructions and take pragmatic assistive actions. In this paper, we present a cognitively inspired neurosymbolic model, Spoken Instruction Following through Theory of Mind (SIFToM), which leverages a Vision-Language Model with model-based mental inference to enable robots to pragmatically follow human instructions under diverse speech conditions. We test SIFToM in both simulated environments (VirtualHome) and real-world human-robot collaborative settings with human evaluations. Results show that SIFToM can significantly improve the performance of a lightweight base VLM (Gemini 2.5 Flash), outperforming state-of-the-art VLMs (Gemini 2.5 Pro) and approaching human-level accuracy on challenging spoken instruction following tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10849v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lance Ying, Xinyi Li, Shivam Aarya, Yizirui Fang, Yifan Yin, Jason Xinyu Liu, Stefanie Tellex, Joshua B. Tenenbaum, Tianmin Shu</dc:creator>
    </item>
    <item>
      <title>Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way Intelligibility Protocol</title>
      <link>https://arxiv.org/abs/2410.20600</link>
      <description>arXiv:2410.20600v3 Announce Type: replace-cross 
Abstract: Our interest is in the design of software systems involving a human-expert interacting -- using natural language -- with a large language model (LLM) on data analysis tasks. For complex problems, it is possible that LLMs can harness human expertise and creativity to find solutions that were otherwise elusive. On one level, this interaction takes place through multiple turns of prompts from the human and responses from the LLM. Here we investigate a more structured approach based on an abstract protocol described in [3] for interaction between agents. The protocol is motivated by a notion of "two-way intelligibility" and is modelled by a pair of communicating finite-state machines. We provide an implementation of the protocol, and provide empirical evidence of using the implementation to mediate interactions between an LLM and a human-agent in two areas of scientific interest (radiology and drug design). We conduct controlled experiments with a human proxy (a database), and uncontrolled experiments with human subjects. The results provide evidence in support of the protocol's capability of capturing one- and two-way intelligibility in human-LLM interaction; and for the utility of two-way intelligibility in the design of human-machine systems. Our code is available at https://github.com/karannb/interact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20600v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harshvardhan Mestha, Karan Bania, Shreyas V Sathyanarayana, Sidong Liu, Ashwin Srinivasan</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models generalize analogy solving like children can?</title>
      <link>https://arxiv.org/abs/2411.02348</link>
      <description>arXiv:2411.02348v3 Announce Type: replace-cross 
Abstract: In people, the ability to solve analogies such as "body : feet :: table : ?" emerges in childhood, and appears to transfer easily to other domains, such as the visual domain "( : ) :: &lt; : ?". Recent research shows that large language models (LLMs) can solve various forms of analogies. However, can LLMs generalize analogy solving to new domains like people can? To investigate this, we had children, adults, and LLMs solve a series of letter-string analogies (e.g., a b : a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek alphabet), and a far transfer domain (list of symbols). Children and adults easily generalized their knowledge to unfamiliar domains, whereas LLMs did not. This key difference between human and AI performance is evidence that these LLMs still struggle with robust human-like analogical transfer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02348v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claire E. Stevenson, Alexandra Pafford, Han L. J. van der Maas, Melanie Mitchell</dc:creator>
    </item>
    <item>
      <title>Student-AI Interaction in an LLM-Empowered Learning Environment: A Cluster Analysis of Engagement Profiles</title>
      <link>https://arxiv.org/abs/2503.01694</link>
      <description>arXiv:2503.01694v2 Announce Type: replace-cross 
Abstract: Integrating Large Language Models (LLMs) into educational practice enables personalized learning by accommodating diverse learner behaviors. This study explored diverse learner profiles within a multi-agent, LLM-empowered learning environment. Data was collected from 312 undergraduate students at a university in China as they participated in a six-module course. Based on hierarchical cluster analyses of system profiles and student-AI interactive dialogues, we found that students exhibit varied behavioral, cognitive, and emotional engagement tendencies. This analysis allowed us to identify two types of dropouts (early dropouts and stagnating interactors) and three completer profiles (active questioners, responsive navigators, and lurkers). The results showed that high levels of interaction do not always equate to productive learning and vice versa. Prior knowledge significantly influenced interaction patterns and short-term learning benefits. Further analysis of the human-AI dialogues revealed that some students actively engaged in knowledge construction, while others displayed a high frequency of regulatory behaviors. Notably, both groups of students achieved comparable learning gains, demonstrating the effectiveness of the multi-agent learning environment in supporting personalized learning. These results underscore the complex and multifaceted nature of engagement in human-AI collaborative learning and provide practical implications for the design of adaptive educational systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01694v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhanxin Hao, Jianxiao Jiang, Jifan Yu, Zhiyuan Liu, Yu Zhang</dc:creator>
    </item>
    <item>
      <title>The Ultimate Configuration Management Tool? Lessons from a Mixed Methods Study of Ansible's Challenges</title>
      <link>https://arxiv.org/abs/2504.08678</link>
      <description>arXiv:2504.08678v2 Announce Type: replace-cross 
Abstract: Infrastructure as Code (IaC) tools have transformed the way IT infrastructure is automated and managed, but their growing adoption has also exposed numerous challenges for practitioners. In this paper, we investigate these challenges through the lens of Ansible, a popular IaC tool. Using a mixed methods approach, we investigate challenges, obstacles, and issues faced by practitioners. We analyze 59,157 posts from Stack Overflow, Reddit, and the Ansible Forum to identify common pain points, complemented by 20 semi-structured interviews with practitioners of varying expertise levels. Based on our findings, we propose four main recommendations to improve Ansible: 1) refactoring to mitigate performance issues, 2) restructuring higher-level language concepts, 3) improved debugging and error reporting tools, and 4) better documentation and learning resources. By highlighting the real-world struggles of Ansible users, we provide actionable insights for tool designers, educators, and the broader IaC community, contributing to a deeper understanding of the trade-offs inherent in IaC tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08678v2</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carolina Carreira, Nuno Saavedra, Alexandra Mendes, Jo\~ao F. Ferreira</dc:creator>
    </item>
    <item>
      <title>Advancing Brainwave Modeling with a Codebook-Based Foundation Model</title>
      <link>https://arxiv.org/abs/2505.16724</link>
      <description>arXiv:2505.16724v2 Announce Type: replace-cross 
Abstract: Recent advances in large-scale pre-trained Electroencephalogram (EEG) models have shown great promise, driving progress in Brain-Computer Interfaces (BCIs) and healthcare applications. However, despite their success, many existing pre-trained models have struggled to fully capture the rich information content of neural oscillations, a limitation that fundamentally constrains their performance and generalizability across diverse BCI tasks. This limitation is frequently rooted in suboptimal architectural design choices which constrain their representational capacity. In this work, we introduce LaBraM++, an enhanced Large Brainwave Foundation Model (LBM) that incorporates principled improvements grounded in robust signal processing foundations. LaBraM++ demonstrates substantial gains across a variety of tasks, consistently outperforming its originally-based architecture and achieving competitive results when compared to other open-source LBMs. Its superior performance and training efficiency highlight its potential as a strong foundation for future advancements in LBMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16724v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantinos Barmpas, Na Lee, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou</dc:creator>
    </item>
    <item>
      <title>The Narcissus Hypothesis: Descending to the Rung of Illusion</title>
      <link>https://arxiv.org/abs/2509.17999</link>
      <description>arXiv:2509.17999v3 Announce Type: replace-cross 
Abstract: Modern foundational models increasingly reflect not just world knowledge, but patterns of human preference embedded in their training data. We hypothesize that recursive alignment-via human feedback and model-generated corpora-induces a social desirability bias, nudging models to favor agreeable or flattering responses over objective reasoning. We refer to it as the Narcissus Hypothesis and test it across 31 models using standardized personality assessments and a novel Social Desirability Bias score. Results reveal a significant drift toward socially conforming traits, with profound implications for corpus integrity and the reliability of downstream inferences. We then offer a novel epistemological interpretation, tracing how recursive bias may collapse higher-order reasoning down Pearl's Ladder of Causality, culminating in what we refer to as the Rung of Illusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17999v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Cadei, Christian Intern\`o</dc:creator>
    </item>
    <item>
      <title>Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills</title>
      <link>https://arxiv.org/abs/2509.20653</link>
      <description>arXiv:2509.20653v2 Announce Type: replace-cross 
Abstract: This study introduces a haptic shared control framework designed to teach human drivers advanced driving skills. In this context, shared control refers to a driving mode where the human driver collaborates with an autonomous driving system to control the steering of a vehicle simultaneously. Advanced driving skills are those necessary to safely push the vehicle to its handling limits in high-performance driving such as racing and emergency obstacle avoidance. Previous research has demonstrated the performance and safety benefits of shared control schemes using both subjective and objective evaluations. However, these schemes have not been assessed for their impact on skill acquisition on complex and demanding tasks. Prior research on long-term skill acquisition either applies haptic shared control to simple tasks or employs other feedback methods like visual and auditory aids. To bridge this gap, this study creates a cyber racing coach framework based on the haptic shared control paradigm and evaluates its performance in helping human drivers acquire high-performance driving skills. The framework introduces (1) an autonomous driving system that is capable of cooperating with humans in a highly performant driving scenario; and (2) a haptic shared control mechanism along with a fading scheme to gradually reduce the steering assistance from autonomy based on the human driver's performance during training. Two benchmarks are considered: self-learning (no assistance) and full assistance during training. Results from a human subject study indicate that the proposed framework helps human drivers develop superior racing skills compared to the benchmarks, resulting in better performance and consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20653v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congkai Shen, Siyuan Yu, Yifan Weng, Haoran Ma, Chen Li, Hiroshi Yasuda, James Dallas, Michael Thompson, John Subosits, Tulga Ersal</dc:creator>
    </item>
  </channel>
</rss>
