<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Nov 2025 02:49:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Genetic Algorithm for Optimizing Fantasy Football Trades with Playoff Biasing</title>
      <link>https://arxiv.org/abs/2511.17535</link>
      <description>arXiv:2511.17535v1 Announce Type: new 
Abstract: Fantasy football leagues involve strategic player trades to optimize team performance. However, identifying optimal trades is complex due to varying player projections, positional needs, and league-specific scoring. Existing approaches focus on team selection or lineup optimization, but automated trade generation remains underexplored. In this paper, an algorithm that generates optimal trades, biasing toward improved playoff performance while maintaining apparent fairness for negotiation is explored. We introduce a genetic algorithm for fantasy football trade optimization, building on existing frameworks for team selection and lineup generation. The algorithm initializes with single-player trades, evolves through custom mutations (add/remove players, combine trades, exchange players, add from other trades, and spawn new trades), and uses team-specific elitism to preserve diversity. The cost function incorporates a playoff-weighted gain for the user's team (while maintaining apparent fairness), opponent gain, and fairness penalty. Integration with ESPN data sources enables real-time projections for all positions, including kickers and defenses. On a 12-team ESPN league (Week 8, 2025), the algorithm generated trades that upgraded the projected point totals of both the trade initiator and trade partner by nearly 3 fantasy points per week ensuring positive gains for both teams. The algorithm demonstrates effective trade optimization, with potential extensions to other fantasy sports or combinatorial problems requiring temporal biasing. Open-source implementation enables practical use and further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17535v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Parshall, Junaid Ali, Michael Zimmerman</dc:creator>
    </item>
    <item>
      <title>Evo* 2025 -- Late-Breaking Abstracts Volume</title>
      <link>https://arxiv.org/abs/2511.17543</link>
      <description>arXiv:2511.17543v1 Announce Type: new 
Abstract: Volume containing the Late-Breaking Abstracts submitted to the Evo* 2025 Conference, held in Trieste (Italy) from April 23rd to 25th. These extended abstracts showcase ongoing research and preliminary findings exploring the application of various Bioinspired Methods (primarily Evolutionary Computation) to a range of problems, many of which address real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17543v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>A. M. Mora, A. I. Esparcia-Alc\'azar, M. S. Cruz</dc:creator>
    </item>
    <item>
      <title>Gate-level boolean evolutionary geometric attention neural networks</title>
      <link>https://arxiv.org/abs/2511.17550</link>
      <description>arXiv:2511.17550v1 Announce Type: new 
Abstract: This paper presents a gate-level Boolean evolutionary geometric attention neural network that models images as Boolean fields governed by logic gates. Each pixel is a Boolean variable (0 or 1) embedded on a two-dimensional geometric manifold (for example, a discrete toroidal lattice), which defines adjacency and information propagation among pixels. The network updates image states through a Boolean reaction-diffusion mechanism: pixels receive Boolean diffusion from neighboring pixels (diffusion process) and perform local logic updates via trainable gate-level logic kernels (reaction process), forming a reaction-diffusion logic network.
  A Boolean self-attention mechanism is introduced, using XNOR-based Boolean Query-Key (Q-K) attention to modulate neighborhood diffusion pathways and realize logic attention. We also propose Boolean Rotary Position Embedding (RoPE), which encodes relative distances by parity-bit flipping to simulate Boolean ``phase'' offsets.
  The overall structure resembles a Transformer but operates entirely in the Boolean domain. Trainable parameters include Q-K pattern bits and gate-level kernel configurations. Because outputs are discrete, continuous relaxation methods (such as sigmoid approximation or soft-logic operators) ensure differentiable training.
  Theoretical analysis shows that the network achieves universal expressivity, interpretability, and hardware efficiency, capable of reproducing convolutional and attention mechanisms. Applications include high-speed image processing, interpretable artificial intelligence, and digital hardware acceleration, offering promising future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17550v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xianshuai Shi, Jianfeng Zhu, Leibo Liu</dc:creator>
    </item>
    <item>
      <title>Further Commentary on the Sooty Tern Optimization Algorithm and Tunicate Swarm Algorithm</title>
      <link>https://arxiv.org/abs/2511.17556</link>
      <description>arXiv:2511.17556v1 Announce Type: new 
Abstract: In the article (Kudela, 2022), experimental demonstrations indicated that two Bio-/Nature inspired optimization algorithms (BNIOAs), Sooty Tern Optimization Algorithm (STOA) and Tunicate Swarm Algorithm (TSA), exhibit a zero-bias, leading to the conclusion that the claims made in the original papers were overstated. In this work, we extend the analysis by investigating the source of this bias from a probabilistic perspective. Our findings suggest that operations involving exponentiation, trigonometric functions, and divisions between random numbers are the primary causes of design flaws. These operations result in probability density distributions with a noticeable shift toward zero. Therefore, the application of these two algorithms should be approached with due caution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17556v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ngaiming Kwok</dc:creator>
    </item>
    <item>
      <title>On the Structural and Statistical Flaws of the Exponential-Trigonometric Optimizer</title>
      <link>https://arxiv.org/abs/2511.17557</link>
      <description>arXiv:2511.17557v1 Announce Type: new 
Abstract: The proliferation of metaphor-based metaheuristics has often been accompanied by issues of symbolic inflation, benchmarking opacity, and statistical misuse. This study presents a diagnostic critique of the recently proposed Exponential Trigonometric Optimizer (ETO), exposing fundamental flaws in its algorithmic structure and the statistical reporting of its performance. Through a stripped mathematical reconstruction, we identify inert symbolic constructs, ill-defined recurrence schedules, and ineffective update mechanisms that collectively undermine the algorithm's purported balance and effectiveness. A principled benchmarking comparison against nine established metaheuristics on the CEC 2017 and 2021 suites reveals that ETO's performance claims are inflated. While it demonstrates mid-tier competitiveness, it consistently fails against top-tier algorithms, especially under high-dimensional and shift-rotated landscapes. Our statistical framework, employing rank-based non-parametric tests and effect size diagnostics, quantifies these limitations and highlights ETO's structural fragility and lack of scalability. The paper concludes by advocating for a reformist framework in metaheuristic research, emphasizing symbolic hygiene, operator attribution, and statistical transparency to mitigate misleading narratives and foster a more robust and reproducible optimization literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17557v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ngaiming Kwok</dc:creator>
    </item>
    <item>
      <title>Dynamic Weight Adaptation in Spiking Neural Networks Inspired by Biological Homeostasis</title>
      <link>https://arxiv.org/abs/2511.17563</link>
      <description>arXiv:2511.17563v1 Announce Type: new 
Abstract: Homeostatic mechanisms play a crucial role in maintaining optimal functionality within the neural circuits of the brain. By regulating physiological and biochemical processes, these mechanisms ensure the stability of an organism's internal environment, enabling it to better adapt to external changes. Among these mechanisms, the Bienenstock, Cooper, and Munro (BCM) theory has been extensively studied as a key principle for maintaining the balance of synaptic strengths in biological systems. Despite the extensive development of spiking neural networks (SNNs) as a model for bionic neural networks, no prior work in the machine learning community has integrated biologically plausible BCM formulations into SNNs to provide homeostasis. In this study, we propose a Dynamic Weight Adaptation Mechanism (DWAM) for SNNs, inspired by the BCM theory. DWAM can be integrated into the host SNN, dynamically adjusting network weights in real time to regulate neuronal activity, providing homeostasis to the host SNN without any fine-tuning. We validated our method through dynamic obstacle avoidance and continuous control tasks under both normal and specifically designed degraded conditions. Experimental results demonstrate that DWAM not only enhances the performance of SNNs without existing homeostatic mechanisms under various degraded conditions but also further improves the performance of SNNs that already incorporate homeostatic mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17563v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yunduo Zhou, Bo Dong, Chang Li, Yuanchen Wang, Xuefeng Yin, Yang Wang, Xin Yang</dc:creator>
    </item>
    <item>
      <title>Temporal-adaptive Weight Quantization for Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2511.17567</link>
      <description>arXiv:2511.17567v1 Announce Type: new 
Abstract: Weight quantization in spiking neural networks (SNNs) could further reduce energy consumption. However, quantizing weights without sacrificing accuracy remains challenging. In this study, inspired by astrocyte-mediated synaptic modulation in the biological nervous systems, we propose Temporal-adaptive Weight Quantization (TaWQ), which incorporates weight quantization with temporal dynamics to adaptively allocate ultra-low-bit weights along the temporal dimension. Extensive experiments on static (e.g., ImageNet) and neuromorphic (e.g., CIFAR10-DVS) datasets demonstrate that our TaWQ maintains high energy efficiency (4.12M, 0.63mJ) while incurring a negligible quantization loss of only 0.22% on ImageNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17567v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Zhang, Qingyan Meng, Jiaqi Wang, Baiyu Chen, Zhengyu Ma, Xiaopeng Fan</dc:creator>
    </item>
    <item>
      <title>An improved clustering-based multi-swarm PSO using local diversification and topology information</title>
      <link>https://arxiv.org/abs/2511.17571</link>
      <description>arXiv:2511.17571v1 Announce Type: new 
Abstract: Multi-swarm particle optimisation algorithms are gaining popularity due to their ability to locate multiple optimum points concurrently. In this family of algorithms, clustering-based multi-swarm algorithms are among the most effective techniques that join the closest particles together to form independent niche swarms that exploit potential promising regions. However, most clustering-based multi-swarms are Euclidean distance-based and only inquire about the potential of one peak within a cluster and thus can lose multiple peaks due to poor resolution. In a bid to improve the peak detection ratio, the current study proposes two enhancements. First, a preliminary local search across initial particles is proposed to ensure that each local region is sufficiently scouted prior to particle collaboration. Secondly, an investigative clustering approach that performs concavity analysis is proposed to evaluate the potential for several sub-niches within a single cluster. An improved clustering-based multi-swarm PSO (TImPSO) has resulted from these enhancements and has been tested against three competing algorithms in the same family using the IEEE CEC2013 niching datasets, resulting in an improved peak ratio for almost all the test functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17571v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yves Matanga, Yanxia Sun, Zenghui Wang</dc:creator>
    </item>
    <item>
      <title>GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms</title>
      <link>https://arxiv.org/abs/2511.17592</link>
      <description>arXiv:2511.17592v1 Announce Type: new 
Abstract: Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17592v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentin Khrulkov, Andrey Galichin, Denis Bashkirov, Dmitry Vinichenko, Oleg Travkin, Roman Alferov, Andrey Kuznetsov, Ivan Oseledets</dc:creator>
    </item>
    <item>
      <title>Robust Differential Evolution via Nonlinear Population Size Reduction and Adaptive Restart: The ARRDE Algorithm</title>
      <link>https://arxiv.org/abs/2511.18429</link>
      <description>arXiv:2511.18429v1 Announce Type: new 
Abstract: This study is motivated by a robustness issue in numerical optimization of bound-constrained problems: many algorithms that perform well on a particular benchmark suite, such as the IEEE CEC2017 problems, struggle to maintain the same level of performance when applied to other suites that differ in dimensionality, landscape complexity, or the maximum number of function evaluations ($N_{\text{max}}$). To address this, we propose the Adaptive Restart-Refine Differential Evolution (ARRDE) algorithm, a new variant of Differential Evolution (DE). ARRDE builds upon the LSHADE algorithm, incorporates key mechanisms from jSO, and introduces a nonlinear population-size reduction strategy combined with an adaptive restart-refine mechanism.
  We evaluate ARRDE on five benchmark suites (CEC2011, CEC2017, CEC2019, CEC2020, and CEC2022) which, to the best of our knowledge, constitutes the most extensive experimental study to date in the context of algorithmic comparison, as most prior works consider only one or two suites. This broad evaluation enables a rigorous assessment of generalization across markedly different problem characteristics. To further support fair cross-suite comparisons, we also introduce a bounded accuracy-based scoring metric derived from relative error. Using both rank-based and accuracy-based metrics, and comparing against algorithms that perform strongly on CEC2017 (e.g., jSO and LSHADE-cnEpSin) as well as those that excel on CEC2020 (e.g., j2020 and NLSHADE-RSP), ARRDE consistently demonstrates top-tier performance, ranking first across all benchmark suites considered. These results highlight ARRDE's robustness and its superior generalization capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18429v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Khoirul Faiq Muzakka, Ahsani Hafizhu Shali, Haris Suhendar, S\"oren M\"oller, Martin Finsterbusch</dc:creator>
    </item>
    <item>
      <title>Theoretical and Empirical Analysis of Lehmer Codes to Search Permutation Spaces with Evolutionary Algorithms</title>
      <link>https://arxiv.org/abs/2511.19089</link>
      <description>arXiv:2511.19089v1 Announce Type: new 
Abstract: A suitable choice of the representation of candidate solutions is crucial for the efficiency of evolutionary algorithms and related metaheuristics. We focus on problems in permutation spaces, which are at the core of numerous practical applications of such algorithms, e.g. in scheduling and transportation. Inversion vectors (also called Lehmer codes) are an alternative representation of the permutation space $S_n$ compared to the classical encoding as a vector of $n$ unique entries. In particular, they do not require any constraint handling. Using rigorous mathematical runtime analyses, we compare the efficiency of inversion vector encodings to the classical representation and give theory-guided advice on their choice. Moreover, we link the effect of local changes in the inversion code space to classical measures on permutations like the number of inversions. Finally, through experimental studies on linear ordering and quadratic assignment problems, we demonstrate the practical efficiency of inversion vector encodings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19089v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Ma, Valentino Santucci, Carsten Witt</dc:creator>
    </item>
    <item>
      <title>Boosting Brain-inspired Path Integration Efficiency via Learning-based Replication of Continuous Attractor Neurodynamics</title>
      <link>https://arxiv.org/abs/2511.17687</link>
      <description>arXiv:2511.17687v1 Announce Type: cross 
Abstract: The brain's Path Integration (PI) mechanism offers substantial guidance and inspiration for Brain-Inspired Navigation (BIN). However, the PI capability constructed by the Continuous Attractor Neural Networks (CANNs) in most existing BIN studies exhibits significant computational redundancy, and its operational efficiency needs to be improved; otherwise, it will not be conducive to the practicality of BIN technology. To address this, this paper proposes an efficient PI approach using representation learning models to replicate CANN neurodynamic patterns. This method successfully replicates the neurodynamic patterns of CANN-modeled Head Direction Cells (HDCs) and Grid Cells (GCs) using lightweight Artificial Neural Networks (ANNs). These ANN-reconstructed HDC and GC models are then integrated to achieve brain-inspired PI for Dead Reckoning (DR). Benchmark tests in various environments, compared with the well-known NeuroSLAM system, demonstrate that this work not only accurately replicates the neurodynamic patterns of navigation cells but also matches NeuroSLAM in positioning accuracy. Moreover, efficiency improvements of approximately 17.5% on the general-purpose device and 40~50% on the edge device were observed, compared with NeuroSLAM. This work offers a novel implementation strategy to enhance the practicality of BIN technology and holds potential for further extension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17687v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhangyu Ge, Xu He, Lingfei Mo, Xiaolin Meng, Wenxuan Yin, Youdong Zhang, Lansong Jiang, Fengyuan Liu</dc:creator>
    </item>
    <item>
      <title>Neural Architecture Search for Quantum Autoencoders</title>
      <link>https://arxiv.org/abs/2511.19246</link>
      <description>arXiv:2511.19246v1 Announce Type: cross 
Abstract: In recent years, machine learning and deep learning have driven advances in domains such as image classification, speech recognition, and anomaly detection by leveraging multi-layer neural networks to model complex data. Simultaneously, quantum computing (QC) promises to address classically intractable problems via quantum parallelism, motivating research in quantum machine learning (QML). Among QML techniques, quantum autoencoders show promise for compressing high-dimensional quantum and classical data. However, designing effective quantum circuit architectures for quantum autoencoders remains challenging due to the complexity of selecting gates, arranging circuit layers, and tuning parameters.
  This paper proposes a neural architecture search (NAS) framework that automates the design of quantum autoencoders using a genetic algorithm (GA). By systematically evolving variational quantum circuit (VQC) configurations, our method seeks to identify high-performing hybrid quantum-classical autoencoders for data reconstruction without becoming trapped in local minima. We demonstrate effectiveness on image datasets, highlighting the potential of quantum autoencoders for efficient feature extraction within a noise-prone, near-term quantum era. Our approach lays a foundation for broader application of genetic algorithms to quantum architecture search, aiming for a robust, automated method that can adapt to varied data and hardware constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19246v1</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hibah Agha, Samuel Yen-Chi Chen, Huan-Hsin Tseng, Shinjae Yoo</dc:creator>
    </item>
    <item>
      <title>Octopus Inspired Optimization (OIO): A Hierarchical Framework for Navigating Protein Fitness Landscapes</title>
      <link>https://arxiv.org/abs/2410.07968</link>
      <description>arXiv:2410.07968v4 Announce Type: replace 
Abstract: Navigating vast, rugged biological fitness landscapes to discover high-value functional patterns-such as optimal protein sequences-is a central challenge in health informatics. However, conventional algorithms often struggle with the exploration-exploitation dilemma, failing to synergize global search with deep local refinement, which leads to entrapment in suboptimal solutions. To overcome this barrier, we introduce Octopus Inspired Optimization (OIO), a novel hierarchical metaheuristic that mimics the octopus's unique neural architecture to intrinsically unify centralized global exploration and parallelized local exploitation. We validated OIO on a real-world protein engineering benchmark, where it surpassed 15 competing metaheuristics. This success is underpinned by OIO's architectural suitability for protein-like landscapes, confirmed by its top ranking on the NK-Landscape benchmark, and its powerful optimization engine, demonstrated by its first-place performance on the gold-standard CEC2022 benchmark. OIO thus provides a robust, nature-inspired computational tool for complex optimization problems in drug discovery and personalized medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07968v4</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xu Wang, Yiquan Wang, Tin-Yeh Huang, Yuhua Dong, Jia Deng, Longji Xu, Xiang Li, Rui He</dc:creator>
    </item>
    <item>
      <title>Learn from Global Correlations: Enhancing Evolutionary Algorithm via Spectral GNN</title>
      <link>https://arxiv.org/abs/2412.17629</link>
      <description>arXiv:2412.17629v5 Announce Type: replace 
Abstract: Evolutionary algorithms (EAs) simulate natural selection but have two main limitations: (1) they rarely update individuals based on global correlations, limiting comprehensive learning; (2) they struggle with balancing exploration and exploitation, where excessive exploitation causes premature convergence, and excessive exploration slows down the search. Moreover, EAs often depend on manual parameter settings, which can disrupt the exploration-exploitation balance. To address these issues, we propose Graph Neural Evolution (GNE), a novel EA framework. GNE represents the population as a graph, where nodes represent individuals, and edges capture their relationships, enabling global information usage. GNE utilizes spectral graph neural networks (GNNs) to decompose evolutionary signals into frequency components, applying a filtering function to fuse these components. High-frequency components capture diverse global information, while low-frequency ones capture more consistent information. This explicit frequency filtering strategy directly controls global-scale features through frequency components, overcoming the limitations of manual parameter settings and making the exploration-exploitation control more interpretable and manageable. Tests on nine benchmark functions (e.g., Sphere, Rastrigin, Rosenbrock) show that GNE outperforms classical (GA, DE, CMA-ES) and advanced algorithms (SDAES, RL-SHADE) under various conditions, including noise-corrupted and optimal solution deviation scenarios. GNE achieves solutions several orders of magnitude better (e.g., 3.07e-20 mean on Sphere vs. 1.51e-07).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17629v5</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaichen Ouyang, Zong Ke, Shengwei Fu, Lingjie Liu, Puning Zhao, Dayu Hu</dc:creator>
    </item>
    <item>
      <title>Large Neighborhood and Hybrid Genetic Search for Inventory Routing Problems</title>
      <link>https://arxiv.org/abs/2506.03172</link>
      <description>arXiv:2506.03172v2 Announce Type: replace 
Abstract: The inventory routing problem (IRP) focuses on jointly optimizing inventory and distribution operations from a supplier to retailers over multiple days. Compared to other problems from the vehicle routing family, the interrelations between inventory and routing decisions render IRP optimization more challenging and call for advanced solution techniques. A few studies have focused on developing large neighborhood search approaches for this class of problems, but this remains a research area with vast possibilities due to the challenges related to the integration of inventory and routing decisions. In this study, we advance this research area by developing a new large neighborhood search operator tailored for the IRP. Specifically, the operator optimally removes and reinserts all visits to a specific retailer while minimizing routing and inventory costs. We propose an efficient tailored dynamic programming algorithm that exploits preprocessing and acceleration strategies. The operator is used to build an effective local search routine, and included in a state-of-the-art routing algorithm, i.e., Hybrid Genetic Search (HGS). Through extensive computational experiments, we demonstrate that the resulting heuristic algorithm leads to solutions of unmatched quality up to this date, especially on large-scale benchmark instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03172v2</guid>
      <category>cs.NE</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyi Zhao, Claudia Archetti, Tuan Anh Pham, Thibaut Vidal</dc:creator>
    </item>
    <item>
      <title>The Geometry of Cortical Computation: Manifold Disentanglement and Predictive Dynamics in VCNet</title>
      <link>https://arxiv.org/abs/2508.02995</link>
      <description>arXiv:2508.02995v3 Announce Type: replace 
Abstract: Despite their success, modern convolutional neural networks (CNNs) exhibit fundamental limitations, including data inefficiency, poor out-of-distribution generalization, and vulnerability to adversarial perturbations. These shortcomings can be traced to a lack of inductive biases that reflect the inherent geometric structure of the visual world. The primate visual system, in contrast, demonstrates superior efficiency and robustness, suggesting that its architectural and computational principles,which evolved to internalize these structures,may offer a blueprint for more capable artificial vision. This paper introduces Visual Cortex Network (VCNet), a novel neural network architecture whose design is informed by the macro-scale organization of the primate visual cortex. VCNet is framed as a geometric framework that emulates key biological mechanisms, including hierarchical processing across distinct cortical areas, dual-stream information segregation for learning disentangled representations, and top-down predictive feedback for representation refinement. We interpret these mechanisms through the lens of geometry and dynamical systems, positing that they guide the learning of structured, low-dimensional neural manifolds. We evaluate VCNet on two specialized benchmarks: the Spots-10 animal pattern dataset, which probes sensitivity to natural textures, and a light field image classification task, which requires processing higher-dimensional visual data. Our results show that VCNet achieves state-of-the-art accuracy of 92.1\% on Spots-10 and 74.4\% on the light field dataset, surpassing contemporary models of comparable size. This work demonstrates that integrating high-level neuroscientific principles, viewed through a geometric lens, can lead to more efficient and robust models, providing a promising direction for addressing long-standing challenges in machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02995v3</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brennen A. Hill, Zhang Xinyu, Timothy Putra Prasetio</dc:creator>
    </item>
    <item>
      <title>Training Deep Normalization-Free Spiking Neural Networks with Lateral Inhibition</title>
      <link>https://arxiv.org/abs/2509.23253</link>
      <description>arXiv:2509.23253v2 Announce Type: replace 
Abstract: Spiking neural networks (SNNs) have garnered significant attention as a central paradigm in neuromorphic computing, owing to their energy efficiency and biological plausibility. However, training deep SNNs has critically depended on explicit normalization schemes, leading to a trade-off between performance and biological realism. To resolve this conflict, we propose a normalization-free learning framework that incorporates lateral inhibition inspired by cortical circuits. Our framework replaces the traditional feedforward SNN layer with a circuit of distinct excitatory (E) and inhibitory (I) neurons that captures the features of the canonical architecture of cortical E-I circuits. The circuit dynamically regulates neuronal activity through subtractive and divisive inhibition, which respectively control the activity and the gain of excitatory neurons. To enable and stabilize end-to-end training of the biologically constrained SNN, we propose two key techniques: E-I Init and E-I Prop. E-I Init is a dynamic parameter initialization scheme that balances excitatory and inhibitory inputs while performing gain control. E-I Prop decouples the backpropagation of the E-I circuits from the forward pass and regulates gradient flow. Experiments across multiple datasets and network architectures demonstrate that our framework enables stable training of deep normalization-free SNNs with biological realism and achieves competitive performance without resorting to explicit normalization schemes. Therefore, our work not only provides a solution to training deep SNNs but also serves as a computational platform for further exploring the functions of E-I interactions in large-scale cortical computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23253v2</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peiyu Liu, Jianhao Ding, Zhaofei Yu</dc:creator>
    </item>
    <item>
      <title>Node Preservation and its Effect on Crossover in Cartesian Genetic Programming</title>
      <link>https://arxiv.org/abs/2511.00634</link>
      <description>arXiv:2511.00634v2 Announce Type: replace 
Abstract: While crossover is a critical and often indispensable component in other forms of Genetic Programming, such as Linear- and Tree-based, it has consistently been claimed that it deteriorates search performance in CGP. As a result, a mutation-alone $(1+\lambda)$ evolutionary strategy has become the canonical approach for CGP. Although several operators have been developed that demonstrate an increased performance over the canonical method, a general solution to the problem is still lacking. In this paper, we compare basic crossover methods, namely one-point and uniform, to variants in which nodes are ``preserved,'' including the subgraph crossover developed by Roman Kalkreuth, the difference being that when ``node preservation'' is active, crossover is not allowed to break apart instructions. We also compare a node mutation operator to the traditional point mutation; the former simply replaces an entire node with a new one. We find that node preservation in both mutation and crossover improves search using symbolic regression benchmark problems, moving the field towards a general solution to CGP crossover.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00634v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Kocherovsky, Illya Bakurov, Wolfgang Banzhaf</dc:creator>
    </item>
    <item>
      <title>Improvement of Spiking Neural Network with Bit Planes and Color Models</title>
      <link>https://arxiv.org/abs/2410.08229</link>
      <description>arXiv:2410.08229v5 Announce Type: replace-cross 
Abstract: Spiking neural network (SNN) has emerged as a promising paradigm in computational neuroscience and artificial intelligence, offering advantages such as low energy consumption and small memory footprint. However, their practical adoption is constrained by several challenges, prominently among them being performance optimization. In this study, we present a novel approach to enhance the performance of SNN for images through a new coding method that exploits bit plane representation. Our proposed technique is designed to improve the accuracy of SNN without increasing model size. Also, we investigate the impacts of color models of the proposed coding process. Through extensive experimental validation, we demonstrate the effectiveness of our coding strategy in achieving performance gain across multiple datasets. To the best of our knowledge, this is the first research that considers bit planes and color models in the context of SNN. By leveraging the unique characteristics of bit planes, we hope to unlock new potentials in SNNs performance, potentially paving the way for more efficient and effective SNNs models in future researches and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08229v5</guid>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2025.3635297</arxiv:DOI>
      <dc:creator>Nhan T. Luu, Duong T. Luu, Nam N. Pham, Thang C. Truong</dc:creator>
    </item>
    <item>
      <title>Do Spikes Protect Privacy? Investigating Black-Box Model Inversion Attacks in Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2502.05509</link>
      <description>arXiv:2502.05509v2 Announce Type: replace-cross 
Abstract: As machine learning models become integral to security-sensitive applications, concerns over data leakage from adversarial attacks continue to rise. Model Inversion (MI) attacks pose a significant privacy threat by enabling adversaries to reconstruct training data from model outputs. While MI attacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking Neural Networks (SNNs) remain largely unexplored in this context. Due to their event-driven and discrete computations, SNNs introduce fundamental differences in information processing that may offer inherent resistance to such attacks. A critical yet underexplored aspect of this threat lies in black-box settings, where attackers operate through queries without direct access to model parameters or gradients-representing a more realistic adversarial scenario in deployed systems. This work presents the first study of black-box MI attacks on SNNs. We adapt a generative adversarial MI framework to the spiking domain by incorporating rate-based encoding for input transformation and decoding mechanisms for output interpretation. Our results show that SNNs exhibit significantly greater resistance to MI attacks than ANNs, as demonstrated by degraded reconstructions, increased instability in attack convergence, and overall reduced attack effectiveness across multiple evaluation metrics. Further analysis suggests that the discrete and temporally distributed nature of SNN decision boundaries disrupts surrogate modeling, limiting the attacker's ability to approximate the target model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05509v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hamed Poursiami, Ayana Moshruba, Maryam Parsa</dc:creator>
    </item>
    <item>
      <title>Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs</title>
      <link>https://arxiv.org/abs/2502.15938</link>
      <description>arXiv:2502.15938v2 Announce Type: replace-cross 
Abstract: LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10x decay). In a large-scale empirical study, we show that under an optimal peak LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at compute-optimal dataset sizes. D2Z is superior across a range of model sizes, batch sizes, datasets, and vocabularies. Benefits increase as dataset size increases. Leveraging a novel interpretation of AdamW as an exponential moving average of weight updates, we show how linear D2Z optimally balances the demands of early training (moving away from initial conditions) and late training (averaging over more updates in order to mitigate gradient noise). In experiments, a 610M-parameter model trained for 80 tokens-per-parameter (TPP) using D2Z achieves lower loss than when trained for 200 TPP using 10x decay, corresponding to an astonishing 60% compute savings. Models such as Llama2-7B, trained for 286 TPP with 10x decay, could likely have saved a majority of compute by training with D2Z.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15938v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shane Bergsma, Nolan Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, Joel Hestness</dc:creator>
    </item>
    <item>
      <title>Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks</title>
      <link>https://arxiv.org/abs/2505.01218</link>
      <description>arXiv:2505.01218v4 Announce Type: replace-cross 
Abstract: Kernel-based learning methods such as Kernel Logistic Regression (KLR) can substantially increase the storage capacity of Hopfield networks, but the principles governing their performance and stability remain largely uncharacterized. This paper presents a comprehensive quantitative analysis of the attractor landscape in KLR-trained networks to establish a solid foundation for their design and application. Through extensive, statistically validated simulations, we address critical questions of generality, scalability, and robustness. Our comparative analysis shows that KLR and Kernel Ridge Regression (KRR) exhibit similarly high storage capacities and clean attractor landscapes under typical operating conditions, suggesting that this behavior is a general property of kernel regression methods, although KRR is computationally much faster. We identify a non-trivial, scale-dependent law for the kernel width $\gamma$, demonstrating that optimal capacity requires $\gamma$ to be scaled such that $\gamma N$ increases with network size $N$. This finding implies that larger networks require more localized kernels, in which each pattern's influence is more spatially confined, to mitigate inter-pattern interference. Under this optimized scaling, we provide clear evidence that storage capacity scales linearly with network size~($P \propto N$). Furthermore, our sensitivity analysis shows that performance is remarkably robust with respect to the choice of the regularization parameter $\lambda$. Collectively, these findings provide a concise set of empirical principles for designing high-capacity and robust associative memories and clarify the mechanisms that enable kernel methods to overcome the classical limitations of Hopfield-type models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01218v4</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akira Tamamori</dc:creator>
    </item>
    <item>
      <title>Why Consciousness Should Explain Physical Phenomena: Toward a Testable Theory</title>
      <link>https://arxiv.org/abs/2511.04047</link>
      <description>arXiv:2511.04047v3 Announce Type: replace-cross 
Abstract: The reductionist approach commonly employed in scientific methods presupposes that both macro and micro phenomena can be explained by micro-level laws alone. This assumption implies intra-level causal closure, rendering all macro phenomena epiphenomenal. However, the integrative nature of consciousness suggests that it is a macro phenomenon. To ensure scientific testability and reject epiphenomenalism, the reductionist assumption of intra-level causal closure must be rejected. This implies that even neural-level behavior cannot be explained by observable neural-level laws alone. Therefore, a new methodology is necessary to acknowledge the causal efficacy of macro-level phenomena. We model the brain as operating under dual laws at different levels. This model includes hypothetical macro-level psychological laws that are not determined solely by micro-level neural laws, as well as the causal effects from macro to micro levels. In this study, we propose a constructive approach that explains both mental and physical phenomena through the interaction between these two sets of laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04047v3</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiyuki Ohmura, Yasuo Kuniyoshi</dc:creator>
    </item>
  </channel>
</rss>
