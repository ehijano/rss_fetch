<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the Functoriality of Belief Propagation Algorithms on finite Partially Ordered Sets</title>
      <link>https://arxiv.org/abs/2503.15705</link>
      <description>arXiv:2503.15705v1 Announce Type: new 
Abstract: Undirected graphical models are a widely used class of probabilistic models in machine learning that capture prior knowledge or putative pairwise interactions between variables. Those interactions are encoded in a graph for pairwise interactions; however, generalizations such as factor graphs account for higher-degree interactions using hypergraphs. Inference on such models, which is performed by conditioning on some observed variables, is typically done approximately by optimizing a free energy, which is an instance of variational inference. The Belief Propagation algorithm is a dynamic programming algorithm that finds critical points of that free energy. Recent efforts have been made to unify and extend inference on graphical models and factor graphs to more expressive probabilistic models. A synthesis of these works shows that inference on graphical models, factor graphs, and their generalizations relies on the introduction of presheaves and associated invariants (homology and cohomology groups).We propose to study the impact of the transformation of the presheaves onto the associated message passing algorithms. We show that natural transformations between presheaves associated with graphical models and their generalizations, which can be understood as coherent binning of the set of values of the variables, induce morphisms between associated message-passing algorithms. It is, to our knowledge, the first result on functoriality of the Loopy Belief Propagation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15705v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gr\'egoire Sergeant-Perthuis, Toby St Clere Smithe, L\'eo Boitel</dc:creator>
    </item>
    <item>
      <title>The Fundamental Limits of Recovering Planted Subgraphs</title>
      <link>https://arxiv.org/abs/2503.15723</link>
      <description>arXiv:2503.15723v1 Announce Type: new 
Abstract: Given an arbitrary subgraph $H=H_n$ and $p=p_n \in (0,1)$, the planted subgraph model is defined as follows. A statistician observes the union a random copy $H^*$ of $H$, together with random noise in the form of an instance of an Erdos-Renyi graph $G(n,p)$. Their goal is to recover the planted $H^*$ from the observed graph. Our focus in this work is to understand the minimum mean squared error (MMSE) for sufficiently large $n$.
  A recent paper [MNSSZ23] characterizes the graphs for which the limiting MMSE curve undergoes a sharp phase transition from $0$ to $1$ as $p$ increases, a behavior known as the all-or-nothing phenomenon, up to a mild density assumption on $H$. In this paper, we provide a formula for the limiting MMSE curve for any graph $H=H_n$, up to the same mild density assumption. This curve is expressed in terms of a variational formula over pairs of subgraphs of $H$, and is inspired by the celebrated subgraph expectation thresholds from the probabilistic combinatorics literature [KK07]. Furthermore, we give a polynomial-time description of the optimizers of this variational problem. This allows one to efficiently approximately compute the MMSE curve for any dense graph $H$ when $n$ is large enough. The proof relies on a novel graph decomposition of $H$ as well as a new minimax theorem which may be of independent interest.
  Our results generalize to the setting of minimax rates of recovering arbitrary monotone boolean properties planted in random noise, where the statistician observes the union of a planted minimal element $A \subseteq [N]$ of a monotone property and a random $Ber(p)^{\otimes N}$ vector. In this setting, we provide a variational formula inspired by the so-called "fractional" expectation threshold [Tal10], again describing the MMSE curve (in this case up to a multiplicative constant) for large enough $n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15723v1</guid>
      <category>math.ST</category>
      <category>cs.DM</category>
      <category>cs.IT</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Lee, Francisco Pernice, Amit Rajaraman, Ilias Zadik</dc:creator>
    </item>
    <item>
      <title>General reproducing properties in RKHS with application to derivative and integral operators</title>
      <link>https://arxiv.org/abs/2503.15922</link>
      <description>arXiv:2503.15922v1 Announce Type: new 
Abstract: In this paper, we generalize the reproducing property in Reproducing Kernel Hilbert Spaces (RKHS). We establish a reproducing property for the closure of the class of combinations of composition operators under minimal conditions. As an application, we improve the existing sufficient conditions for the reproducing property to hold for the derivative operator, as well as for the existence of the mean embedding function. These results extend the scope of applicability of the representer theorem for regularized learning algorithms that involve data for function values, gradients, or any other operator from the considered class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15922v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatima-Zahrae El-Boukkouri (INSA Toulouse, IMT), Josselin Garnier (CMAP, ASCII), Olivier Roustant (INSA Toulouse, IMT, RT-UQ)</dc:creator>
    </item>
    <item>
      <title>Statistical accuracy of the ensemble Kalman filter in the near-linear setting</title>
      <link>https://arxiv.org/abs/2503.16154</link>
      <description>arXiv:2503.16154v1 Announce Type: new 
Abstract: Estimating the state of a dynamical system from partial and noisy observations is a ubiquitous problem in a large number of applications, such as probabilistic weather forecasting and prediction of epidemics. Particle filters are a widely adopted approach to the problem and provide provably accurate approximations of the statistics of the state, but they perform poorly in high dimensions because of weight collapse. The ensemble Kalman filter does not suffer from this issue, as it relies on an interacting particle system with equal weights. Despite its wide adoption in the geophysical sciences, mathematical analysis of the accuracy of this filter is predominantly confined to the setting of linear dynamical models and linear observations operators, and analysis beyond the linear Gaussian setting is still in its infancy. In this short note, we provide an accessible overview of recent work in which the authors take first steps to analyze the accuracy of the filter beyond the linear Gaussian setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16154v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>E. Calvello, J. A. Carrillo, F. Hoffmann, P. Monmarch\'e, A. M. Stuart, U. Vaes</dc:creator>
    </item>
    <item>
      <title>Systemic Risk Management via Maximum Independent Set in Extremal Dependence Networks</title>
      <link>https://arxiv.org/abs/2503.15534</link>
      <description>arXiv:2503.15534v1 Announce Type: cross 
Abstract: The failure of key financial institutions may accelerate risk contagion due to their interconnections within the system. In this paper, we propose a robust portfolio strategy to mitigate systemic risks during extreme events. We use the stock returns of key financial institutions as an indicator of their performance, apply extreme value theory to assess the extremal dependence among stocks of financial institutions, and construct a network model based on a threshold approach that captures extremal dependence. Our analysis reveals different dependence structures in the Chinese and U.S. financial systems. By applying the maximum independent set (MIS) from graph theory, we identify a subset of institutions with minimal extremal dependence, facilitating the construction of diversified portfolios resilient to risk contagion. We also compare the performance of our proposed portfolios with that of the market portfolios in the two economies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15534v1</guid>
      <category>q-fin.PM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Hui, Tiandong Wang</dc:creator>
    </item>
    <item>
      <title>The Gaussian central limit theorem for a stationary time series with infinite varia</title>
      <link>https://arxiv.org/abs/2503.15894</link>
      <description>arXiv:2503.15894v1 Announce Type: cross 
Abstract: We consider a borderline case: the central limit theorem for a strictly stationary time series with infinite variance but a Gaussian limit. In the iid case a well-known sufficient condition for this central limit theorem is regular variation of the marginal distribution with tail index $\alpha=2$. In the dependent case we assume the stronger condition of sequential regular variation of the time series with tail index $\alpha=2$. We assume that a sample of size $n$ from this time series can be split into $k_n$ blocks of size $r_n\to\infty$ such that $r_n/n\to 0$ as $n\to\infty$ and that the block sums are asymptotically independent. Then we apply classical central limit theory for row-wise iid triangular arrays. The necessary and sufficient conditions for such independent block sums will be verified by using large deviation results for the time series. We derive the central limit theorem for $m$-dependent sequences, linear processes, stochastic volatility processes and solutions to affine stochastic recurrence equations whose marginal distributions have infinite variance and are regularly varying with tail index $\alpha=2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15894v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muneya Matsui, Thomas Mikosch</dc:creator>
    </item>
    <item>
      <title>Sequential Monte Carlo with Gaussian Mixture Distributions for Infinite-Dimensional Statistical Inverse Problems</title>
      <link>https://arxiv.org/abs/2503.16028</link>
      <description>arXiv:2503.16028v1 Announce Type: cross 
Abstract: By formulating the inverse problem of partial differential equations (PDEs) as a statistical inference problem, the Bayesian approach provides a general framework for quantifying uncertainties. In the inverse problem of PDEs, parameters are defined on an infinite-dimensional function space, and the PDEs induce a computationally intensive likelihood function. Additionally, sparse data tends to lead to a multi-modal posterior. These features make it difficult to apply existing sequential Monte Carlo (SMC) algorithms. To overcome these difficulties, we propose new conditions for the likelihood functions, construct a Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and demonstrate the universal approximation property of the infinite-dimensional Gaussian mixture probability measure. By combining these three novel tools, we propose a new SMC algorithm, named SMC-GM. For this new algorithm, we obtain a convergence theorem that allows Gaussian priors, illustrating that the sequential particle filter actually reproduces the true posterior distribution. Furthermore, the proposed new algorithm is rigorously defined on the infinite-dimensional function space, naturally exhibiting the discretization-invariant property. Numerical experiments demonstrate that the new approach has a strong ability to probe the multi-modality of the posterior, significantly reduces the computational burden, and numerically exhibits the discretization-invariant property (important for large-scale problems).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16028v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyu Lu, Junxiong Jia, Deyu Meng</dc:creator>
    </item>
    <item>
      <title>Uniformly consistent proportion estimation for composite hypotheses via integral equations: "the case of Gamma random variables"</title>
      <link>https://arxiv.org/abs/1906.10246</link>
      <description>arXiv:1906.10246v3 Announce Type: replace 
Abstract: We consider estimating the proportion of random variables for two types of composite null hypotheses: (i) the means of the random variables belonging to a non-empty, bounded interval; (ii) the means of the random variables belonging to an unbounded interval that is not the whole real line. For each type of composite null hypotheses, uniformly consistent estimators of the proportion of false null hypotheses are constructed for random variables whose distributions are members of the Gamma family. Further, uniformly consistent estimators of certain functions of a bounded null on the means are provided for the random variables mentioned earlier. These functions are continuous and of bounded variation. The estimators are constructed via solutions to Lebesgue-Stieltjes integral equations and harmonic analysis, do not rely on a concept of p-value, and have various applications.ce via mixture models, and may be used to estimate the sparsity level in high-dimensional Gaussian linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:1906.10246v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Annals of the Institute of Statistical Mathematics, 2025</arxiv:journal_reference>
      <dc:creator>Xiongzhi Chen</dc:creator>
    </item>
    <item>
      <title>Robust mean change point testing in high-dimensional data with heavy tails</title>
      <link>https://arxiv.org/abs/2305.18987</link>
      <description>arXiv:2305.18987v3 Announce Type: replace 
Abstract: We study mean change point testing problems for high-dimensional data, with exponentially- or polynomially-decaying tails. In each case, depending on the $\ell_0$-norm of the mean change vector, we separately consider dense and sparse regimes. We characterise the boundary between the dense and sparse regimes under the above two tail conditions for the first time in the change point literature and propose novel testing procedures that attain optimal rates in each of the four regimes up to a poly-iterated logarithmic factor. By comparing with previous results under Gaussian assumptions, our results quantify the costs of heavy-tailedness on the fundamental difficulty of change point testing problems for high-dimensional data.
  To be specific, when the error distributions possess exponentially-decaying tails, a CUSUM-type statistic is shown to achieve a minimax testing rate up to $\sqrt{\log\log(8n)}$. As for polynomially-decaying tails, admitting bounded $\alpha$-th moments for some $\alpha \geq 4$, we introduce a median-of-means-type test statistic that achieves a near-optimal testing rate in both dense and sparse regimes. In the sparse regime, we further propose a computationally-efficient test to achieve optimality. Our investigation in the even more challenging case of $2 \leq \alpha &lt; 4$, unveils a new phenomenon that the minimax testing rate has no sparse regime, i.e.\ testing sparse changes is information-theoretically as hard as testing dense changes. Finally, we consider various extensions where we also obtain near-optimal performances, including testing against multiple change points, allowing temporal dependence as well as fewer than two finite moments in the data generating mechanisms. We also show how sub-Gaussian rates can be achieved when an additional minimal spacing condition is imposed under the alternative hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18987v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengchu Li, Yudong Chen, Tengyao Wang, Yi Yu</dc:creator>
    </item>
    <item>
      <title>A re-examination to the SCoTLASS problems for SPCA and two projection-based methods for them</title>
      <link>https://arxiv.org/abs/2307.00516</link>
      <description>arXiv:2307.00516v2 Announce Type: replace 
Abstract: SCoTLASS is the first sparse principal component analysis (SPCA) model which imposes extra l1 norm constraints on the measured variables to obtain sparse loadings. Due to the the difficulty of finding projections on the intersection of an l1 ball/sphere and an l2 ball/sphere, early approaches to solving the SCoTLASS problems were focused on penalty function methods or conditional gradient methods. In this paper, we re-examine the SCoTLASS problems, denoted by SPCA-P1, SPCA-P2 or SPCA-P3 when using the intersection of an l1 ball and an l2 ball, an l1 sphere and an l2 sphere, or an l1 ball and an l2 sphere as constrained set, respectively. We prove the equivalence of the solutions to SPCA-P1 and SPCA-P3, and the solutions to SPCA-P2 and SPCA-P3 are the same in most case. Then by employing the projection method onto the intersection of an l1 ball/sphere and an l2 ball/sphere, we design a gradient projection method (GPSPCA for short) and an approximate Newton algorithm (ANSPCA for short) for SPCA-P1, SPCA-P2 and SPCA-P3 problems, and prove the global convergence of the proposed GPSPCA and ANSPCA algorithms. Finally, we conduct several numerical experiments in MATLAB environment to evaluate the performance of our proposed GPSPCA and ANSPCA algorithms. Simulation results confirm the assertions that the solutions to SPCA-P1 and SPCA-P3 are the same, and the solutions to SPCA-P2 and SPCA-P3 are the same in most case, and show that ANSPCA is faster than GPSPCA for large-scale data. Furthermore, GPSPCA and ANSPCA perform well as a whole comparing with the typical SPCA methods: the l0-constrained GPBB algorithm, the l1-constrained BCD-SPCAl1 algorithm, the l1-penalized ConGradU and Gpowerl1 algorithms, and can be used for large-scale computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00516v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiye Zhang, Kuoyue Li</dc:creator>
    </item>
    <item>
      <title>A Shrinkage Likelihood Ratio Test for High-Dimensional Subgroup Analysis with a Logistic-Normal Mixture Model</title>
      <link>https://arxiv.org/abs/2307.10272</link>
      <description>arXiv:2307.10272v4 Announce Type: replace 
Abstract: In subgroup analysis, testing the existence of a subgroup with a differential treatment effect serves as protection against spurious subgroup discovery. Despite its importance, this hypothesis testing possesses a complicated nature: parameter characterizing subgroup classification is not identified under the null hypothesis of no subgroup. Due to this irregularity, the existing methods have the following two limitations. First, the asymptotic null distribution of test statistics often takes an intractable form, which necessitates computationally demanding resampling methods to calculate the critical value. Second, the dimension of personal attributes characterizing subgroup membership is not allowed to be of high dimension. To solve these two problems simultaneously, this study develops a shrinkage likelihood ratio test for the existence of a subgroup using a logistic-normal mixture model. The proposed test statistics are built on a modified likelihood function that shrinks possibly high-dimensional unidentified parameters toward zero under the null hypothesis while retaining power under the alternative. This shrinkage helps handle the irregularity and restore the simple chi-square-type asymptotics even under the high-dimensional regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10272v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shota Takeishi</dc:creator>
    </item>
    <item>
      <title>Asymptotically Optimal Sequential Multiple Testing Procedures for Correlated Normal</title>
      <link>https://arxiv.org/abs/2309.16657</link>
      <description>arXiv:2309.16657v2 Announce Type: replace 
Abstract: Simultaneous statistical inference has been a cornerstone in the statistics methodology literature because of its fundamental theory and paramount applications. The mainstream multiple testing literature has traditionally considered two frameworks: the sample size is deterministic, and the test statistics corresponding to different tests are independent. However, in many modern scientific avenues, these assumptions are often violated. There is little study that explores the multiple testing problem in a sequential framework where the test statistics corresponding to the various streams are dependent. This work fills this gap in a unified way by considering the classical means-testing problem in an equicorrelated Gaussian and sequential framework. We focus on sequential test procedures that control the type I and type II familywise error probabilities at pre-specified levels. We establish that our proposed test procedures achieve the optimal expected sample sizes under every possible signal configuration asymptotically, as the two error probabilities vanish at arbitrary rates. Towards this, we elucidate that the ratio of the expected sample size of our proposed rule and that of the classical SPRT goes to one asymptotically, thus illustrating their connection. Generalizing this, we show that our proposed procedures, with appropriately adjusted critical values, are asymptotically optimal for controlling any multiple testing error metric lying between multiples of FWER in a certain sense. This class of metrics includes FDR/FNR, pFDR/pFNR, the per-comparison and per-family error rates, and the false positive rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16657v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monitirtha Dey, Subir Kumar Bhandari</dc:creator>
    </item>
    <item>
      <title>Two improved algorithms for sparse generalized canonical correlation analysis</title>
      <link>https://arxiv.org/abs/2311.01126</link>
      <description>arXiv:2311.01126v2 Announce Type: replace 
Abstract: Regularized generalized canonical correlation analysis (RGCCA) is a generalization of regularized canonical correlation analysis to three or more sets of variables, which is a component-based approach aiming to study the relationships between several sets of variables. Sparse generalized canonical correlation analysis (SGCCA) (proposed in Tenenhaus et al. (2014)), combines RGCCA with an `1-penalty, in which blocks are not necessarily fully connected, makes SGCCA a flexible method for analyzing a wide variety of practical problems, such as biology, chemistry, sensory analysis, marketing, food research, etc. In Tenenhaus et al. (2014), an iterative algorithm for SGCCA was designed based on the solution to the subproblem (LM-P1 for short) of maximizing a linear function on the intersection of an `1-norm ball and a unit `2-norm sphere proposed in Witten et al. (2009). However, the solution to the subproblem (LM-P1) proposed in Witten et al. (2009) is not correct, which may become the reason that the iterative algorithm for SGCCA is slow and not always convergent. For this, we first characterize the solution to the subproblem LM-P1, and the subproblems LM-P2 and LM-P3, which maximize a linear function on the intersection of an `1-norm sphere and a unit `2-norm sphere, and an `1-norm ball and a unit `2-norm sphere, respectively. Then we provide more efficient block coordinate descent (BCD) algorithms for SGCCA and its two variants, called SGCCA-BCD1, SGCCA-BCD2 and SGCCA-BCD3, corresponding to the subproblems LM-P1, LM-P2 and LM-P3, respectively, prove that they all globally converge to their stationary points. We further propose gradient projected (GP) methods for SGCCA and its two variants when using the Horst scheme, called SGCCA-GP1, SGCCA-GP2 and SGCCA-GP3, corresponding to the subproblems LM-P1, LM-P2 and LM-P3, respectively, and prove that they all</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01126v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuo-Yue Li, Qi-Ye Zhang, Yong-Han Sun</dc:creator>
    </item>
    <item>
      <title>Combining exchangeable p-values</title>
      <link>https://arxiv.org/abs/2404.03484</link>
      <description>arXiv:2404.03484v5 Announce Type: replace 
Abstract: The problem of combining p-values is an old and fundamental one, and the classic assumption of independence is often violated or unverifiable in many applications. There are many well-known rules that can combine a set of arbitrarily dependent p-values (for the same hypothesis) into a single p-value. We show that essentially all these existing rules can be strictly improved when the p-values are exchangeable, or when external randomization is allowed (or both). For example, we derive randomized and/or exchangeable improvements of well known rules like ``twice the median'' and ``twice the average'', as well as geometric and harmonic means. Exchangeable p-values are often produced one at a time (for example, under repeated tests involving data splitting), and our rules can combine them sequentially as they are produced, stopping when the combined p-values stabilize. Our work also improves rules for combining arbitrarily dependent p-values, since the latter becomes exchangeable if they are presented to the analyst in a random order. The main technical advance is to show that all existing combination rules can be obtained by calibrating the p-values to e-values (using an $\alpha$-dependent calibrator), averaging those e-values, converting to a level-$\alpha$ test using Markov's inequality, and finally obtaining p-values by combining this family of tests; the improvements are delivered via recent randomized and exchangeable variants of Markov's inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03484v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1073/pnas.2410849122</arxiv:DOI>
      <dc:creator>Matteo Gasparin, Ruodu Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Asymptotics for conformal inference</title>
      <link>https://arxiv.org/abs/2409.12019</link>
      <description>arXiv:2409.12019v2 Announce Type: replace 
Abstract: Conformal inference is a versatile tool for building prediction sets in regression or classification. We study the false coverage proportion (FCP) in a transductive setting with a calibration sample of $n$ points and a test sample of $m$ points. We identify the exact, distribution-free, asymptotic distribution of the FCP when both $n$ and $m$ tend to infinity. This shows in particular that FCP control can be achieved by using the well-known Kolmogorov distribution, and puts forward that the asymptotic variance is decreasing in the ratio $n/m$. We then provide a number of extensions by considering the novelty detection problem, weighted conformal inference and distribution shift between the calibration sample and the test sample. In particular, our asymptotic results allow to accurately quantify the asymptotic behavior of the errors when weighted conformal inference is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12019v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ulysse Gazin</dc:creator>
    </item>
    <item>
      <title>Asymptotic non-linear shrinkage and eigenvector overlap for weighted sample covariance</title>
      <link>https://arxiv.org/abs/2410.14420</link>
      <description>arXiv:2410.14420v2 Announce Type: replace 
Abstract: We compute asymptotic non-linear shrinkage formulas for covariance and precision matrix estimators for weighted sample covariances, and the joint sample-population eigenvector overlap distribution, in the spirit of Ledoit and P\'ech\'e. We detail explicitly the formulas for exponentially-weighted sample covariances. We propose an algorithm to numerically compute those formulas. Experimentally, we show the performance of the asymptotic non-linear shrinkage estimators. Finally, we test the robustness of the theory to a heavy-tailed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14420v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benoit Oriol</dc:creator>
    </item>
    <item>
      <title>Pointwise Minimax Vector Field Reconstruction from Noisy ODE</title>
      <link>https://arxiv.org/abs/2503.08355</link>
      <description>arXiv:2503.08355v3 Announce Type: replace 
Abstract: This work addresses the problem of estimating a vector field from a noisy Ordinary Differential Equation (ODE) in a non-parametric regression setting with a random design for initial values. More specifically, given a vector field $ f:\mathbb{R}^{D}\rightarrow \mathbb{R}^{D}$ governing a dynamical system defined by the autonomous ODE: $y' = f(y)$, we assume that the observations are $\tilde{y}_{X_{i}}(t_{j}) = y_{X_{i}}(t_{j}) + \varepsilon_{i,j}$ where $y_{X_{i}}(t_{j})$ is the solution of the ODE at time $t_{j}$ with initial condition $y(0) = X_{i}$, $X_{i}$ is sampled from a probability distribution $\mu$, and $\varepsilon_{i,j}$ some noise. In this context, we investigate, from a minimax perspective, the pointwise reconstruction of $f$ within the envelope of trajectories originating from the support of $\mu$. We propose an estimation strategy based on preliminary flow reconstruction and techniques from derivative estimation in non-parametric regression. Under mild assumptions on $f$, we establish convergence rates that depend on the temporal resolution, the number of sampled initial values and the mass concentration of $\mu$. Importantly, we show that these rates are minimax optimal. Furthermore, we discuss the implications of our results in a manifold learning setting, providing insights into how our approach can mitigate the curse of dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08355v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Henneuse</dc:creator>
    </item>
    <item>
      <title>Generalizing the de Finetti--Hewitt--Savage theorem</title>
      <link>https://arxiv.org/abs/2008.08754</link>
      <description>arXiv:2008.08754v5 Announce Type: replace-cross 
Abstract: A sequence of random variables is called \textit{exchangeable} if its joint distribution is invariant under permutations of indices. The original formulation of de Finetti's theorem roughly says that any exchangeable sequence of $\{0,1\}$-valued random variables can be thought of as a mixture of independent and identically distributed sequences. Hewitt and Savage were able to obtain the same conclusion for exchangeable sequences of random variables taking values in more general state spaces under some topological conditions.
  Using tools from nonstandard analysis we prove that an exchangeable sequence of Radon-distributed random variables taking values in any Hausdorff state space must be representable as a mixture of sequences of independent and identically distributed random variables.
  Our presentation of this work follows the style of \textit{lecture notes} intended for broad graduate-level mathematical audiences -- the main body of the manuscript starts with a historically grounded introduction to the problem, foreshadowing our techniques that are developed via a series of appendices. These techniques are used to provide self-contained proofs of our main results in a short section following the introduction.
  We have provided a self-contained philosophically motivated introduction to nonstandard analysis in the first appendix, thus rendering first courses in measure theoretic probability and point-set topology as the only prerequisites for the work. This introduction aims to develop some new ideologies about the subject that might be of interest to mathematicians, philosophers, and mathematics educators alike. One highlight of the rest of the appendices is a new generalization of Prokhorov's theorem in the setting of the space of all probability measures on arbitrary Hausdorff spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.08754v5</guid>
      <category>math.PR</category>
      <category>math.LO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Irfan Alam</dc:creator>
    </item>
    <item>
      <title>An Efficient Permutation-Based Kernel Two-Sample Test</title>
      <link>https://arxiv.org/abs/2502.13570</link>
      <description>arXiv:2502.13570v2 Announce Type: replace-cross 
Abstract: Two-sample hypothesis testing-determining whether two sets of data are drawn from the same distribution-is a fundamental problem in statistics and machine learning with broad scientific applications. In the context of nonparametric testing, maximum mean discrepancy (MMD) has gained popularity as a test statistic due to its flexibility and strong theoretical foundations. However, its use in large-scale scenarios is plagued by high computational costs. In this work, we use a Nystr\"om approximation of the MMD to design a computationally efficient and practical testing algorithm while preserving statistical guarantees. Our main result is a finite-sample bound on the power of the proposed test for distributions that are sufficiently separated with respect to the MMD. The derived separation rate matches the known minimax optimal rate in this setting. We support our findings with a series of numerical experiments, emphasizing realistic scientific data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13570v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Chatalic, Marco Letizia, Nicolas Schreuder, Lorenzo Rosasco</dc:creator>
    </item>
    <item>
      <title>Proposal for the Application of Fractional Operators in Polynomial Regression Models to Enhance the Determination Coefficient $R^2$ on Unseen Data</title>
      <link>https://arxiv.org/abs/2503.11749</link>
      <description>arXiv:2503.11749v2 Announce Type: replace-cross 
Abstract: Since polynomial regression models are generally quite reliable for data with a linear trend, it is important to note that, in some cases, they may encounter overfitting issues during the training phase, which could result in negative values of the coefficient of determination $R^2$ for unseen data. For this reason, this work proposes the partial implementation of fractional operators in polynomial regression models to generate a fractional regression model. The goal of this proposal is to attempt to mitigate overfitting, which could improve the value of the coefficient of determination for unseen data, compared to the polynomial model, under the assumption that this would contribute to generating predictive models with better performance. The methodology for constructing these fractional regression models is detailed, and examples applicable to both Riemann-Liouville and Caputo fractional operators are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11749v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anthony Torres-Hernandez</dc:creator>
    </item>
  </channel>
</rss>
