<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Mar 2025 04:01:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Nonparametric Bayesian Model to Adjust for Monitoring Bias with an Application to Identifying Environments Stressed by Climate Change</title>
      <link>https://arxiv.org/abs/2503.04924</link>
      <description>arXiv:2503.04924v1 Announce Type: new 
Abstract: We propose a new method to adjust for the bias that occurs when an individual monitors a location and reports the status of an event. For example, a monitor may visit a plant each week and report whether the plant is in flower or not. The goal is to estimate the time the event occurred at that location. The problem is that popular estimators often incur bias both because the event may not coincide with the arrival of the monitor and because the monitor may report the status in error. To correct for this bias, we propose a nonparametric Bayesian model that uses monotonic splines to estimate the event time. We first demonstrate the problem and our proposed solution using simulated data. We then apply our method to a real-world example from phenology in which lilac are monitored by citizen scientists in the northeastern United States, and the timing of the flowering is used to study anthropogenic warming. Our analysis suggests that common methods fail to account for monitoring bias and underestimate the peak bloom date of the lilac by 48 days on average. In addition, after adjusting for monitoring bias, several locations had anomalously late bloom dates that did not appear anomalous before adjustment. Our findings underscore the importance of accounting for monitoring bias in event-time estimation. By applying our nonparametric Bayesian model with monotonic splines, we provide a more accurate approach to estimating bloom dates, revealing previously undetected anomalies and improving the reliability of citizen science data for environmental monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04924v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Auerbach, Theresa M. Crimmins, David Kepplinger, Ruishan Lin, E. M. Wolkovich</dc:creator>
    </item>
    <item>
      <title>A Novel Framework for Modeling Quarantinable Disease Transmission</title>
      <link>https://arxiv.org/abs/2503.04951</link>
      <description>arXiv:2503.04951v1 Announce Type: new 
Abstract: The COVID-19 pandemic has significantly challenged traditional epidemiological models due to factors such as delayed diagnosis, asymptomatic transmission, isolation-induced contact changes, and underreported mortality. In response to these complexities, this paper introduces a novel CURNDS model prioritizing compartments and transmissions based on contact levels, rather than merely on symptomatic severity or hospitalization status. The framework surpasses conventional uniform mixing and static rate assumptions by incorporating adaptive power laws, dynamic transmission rates, and spline-based smoothing techniques. The CURNDS model provides accurate estimates of undetected infections and undocumented deaths from COVID-19 data, uncovering the pandemic's true impact. Our analysis challenges the assumption of homogeneous mixing between infected and non-infected individuals in traditional epidemiological models. By capturing the nuanced transmission dynamics of infection and confirmation, our model offers new insights into the spread of different COVID-19 strains. Overall, CURNDS provides a robust framework for understanding the complex transmission patterns of highly contagious, quarantinable diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04951v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenchen Liu, Chang Liu, Dehui Wang, Yiyuan She</dc:creator>
    </item>
    <item>
      <title>A Scorecard Model Using Survival Analysis Framework</title>
      <link>https://arxiv.org/abs/2503.05023</link>
      <description>arXiv:2503.05023v1 Announce Type: new 
Abstract: Credit risk assessment is a crucial aspect of financial decision-making, enabling institutions to predict the likelihood of default and make informed lending choices. Two prominent methodologies in risk modeling are logistic regression and survival analysis. Logistic regression is widely used for creating scorecard models due to its simplicity, interpretability, and effectiveness in estimating the probability of binary outcomes, such as default versus non-default. On the other hand, survival analysis, particularly the hazard rate framework, offers insights into the timing of events, such as the time until default. By integrating logistic regression with survival analysis, traditional scorecard models can be enhanced to account not only for the probability of default but also for the dynamics of default over time. This combined approach provides a comprehensive view of credit risk, empowering institutions to manage risk proactively and tailor strategies to individual borrower profiles. In this article, the process of developing a scorecard model using logistic regression and augmenting data with survival analysis techniques to incorporate time-varying risk factors are presented. The process includes data preparation, model construction, evaluation metrics, and model implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05023v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cheng Lee, Hsi Lee</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of restricted mean survival time adjusted for covariates using pseudo-observations</title>
      <link>https://arxiv.org/abs/2503.05225</link>
      <description>arXiv:2503.05225v1 Announce Type: new 
Abstract: The difference in restricted mean survival time (RMST) is a clinically meaningful measure to quantify treatment effect in randomized controlled trials, especially when the proportional hazards assumption does not hold. Several frequentist methods exist to estimate RMST adjusted for covariates based on modeling and integrating the survival function. A more natural approach may be a regression model on RMST using pseudo-observations, which allows for a direct estimation without modeling the survival function. Only a few Bayesian methods exist, and each requires a model of the survival function. We developed a new Bayesian method that combines the use of pseudo-observations with the generalized method of moments. This offers RMST estimation adjusted for covariates without the need to model the survival function, making it more attractive than existing Bayesian methods. A simulation study was conducted with different time-dependent treatment effects (early, delayed, and crossing survival) and covariate effects, showing that our approach provides valid results, aligns with existing methods, and shows improved precision after covariate adjustment. For illustration, we applied our approach to a phase III trial in prostate cancer, providing estimates of the treatment effect on RMST, comparable to existing methods. In addition, our approach provided the effect of other covariates on RMST and determined the posterior probability of the difference in RMST exceeds any given time threshold for any covariate, allowing for nuanced and interpretable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05225v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'ea Orsini (CESP, U1018), Emmanuel Lesaffre (KU Leuven), Guosheng Yin (DSAS), Caroline Brard (U1018), David Dejardin (U1018), Gw\'ena\"el Le Teuff (U1018)</dc:creator>
    </item>
    <item>
      <title>PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs</title>
      <link>https://arxiv.org/abs/2503.05529</link>
      <description>arXiv:2503.05529v1 Announce Type: new 
Abstract: This paper introduces PoSSUM, an open-source protocol for unobtrusive polling of social-media users via multimodal Large Language Models (LLMs). PoSSUM leverages users' real-time posts, images, and other digital traces to create silicon samples that capture information not present in the LLM's training data. To obtain representative estimates, PoSSUM employs Multilevel Regression and Post-Stratification (MrP) with structured priors to counteract the observable selection biases of social-media platforms. The protocol is validated during the 2024 U.S. Presidential Election, for which five PoSSUM polls were conducted and published on GitHub and X. In the final poll, fielded October 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately predicted the outcomes in 50 of 51 states and assigned the Republican candidate a win probability of 0.65. Notably, it also exhibited lower state-level bias than most established pollsters. These results demonstrate PoSSUM's potential as a fully automated, unobtrusive alternative to traditional survey methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05529v1</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Cerina</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Relative Efficiency (BRE): Examining in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2503.04775</link>
      <description>arXiv:2503.04775v1 Announce Type: cross 
Abstract: Traditional Relative Efficiency (RE), based solely on variance, has limitations in estimator performance evaluation, especially in planned missing data designs. We introduce Bhirkuti's Relative Efficiency (BRE), a novel metric that integrates precision and accuracy for a more robust assessment. BRE is computed using interquartile range (IQR) overlap for precision and a bias adjustment factor based on the absolute median relative bias (AMRB). Monte Carlo simulations using a Latent Growth Model (LGM) with planned missing data (SWMD-6) illustrate that BRE remains theoretically consistent and interpretable, avoiding paradoxes such as RE exceeding 100%. Visualizations via boxplots and ridgeline plots confirm that BRE provides a stable and meaningful estimator efficiency evaluation, making it a valuable advancement in psychometric and statistical modeling. By addressing fundamental weaknesses in traditional RE, BRE provides a superior, theoretically justified alternative for relative efficiency assessment in psychometric modeling, structural equation modeling, and missing data research. This advancement enhances data-driven decision-making and offers a methodologically rigorous tool for researchers analyzing incomplete datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04775v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>A comparison of the Alkire-Foster method and a Markov random field approach in the analysis of multidimensional poverty</title>
      <link>https://arxiv.org/abs/2503.05676</link>
      <description>arXiv:2503.05676v1 Announce Type: cross 
Abstract: Multidimensional poverty measurement is crucial for capturing deprivation beyond income-based metrics. This study compares the Alkire-Foster (AF) method and a Markov Random Field (MRF) approach for classifying multidimensional poverty using a simulation-based analysis. The AF method applies a deterministic threshold-based classification, while the MRF approach leverages probabilistic graphical modelling to account for correlations between deprivation indicators. Using a synthetic dataset of 50,000 individuals with ten binary deprivation indicators, we assess classification accuracy, false positive/negative trade-offs, and agreement between the methods. Results show that AF achieves higher classification accuracy (89.5%) compared to MRF (75.4%), with AF minimizing false negatives and MRF reducing false positives. The overall agreement between the two methods is 65%, with discrepancies primarily occurring when AF classifies individuals as poor while MRF does not. While AF is transparent and easy to implement, it does not capture interdependencies among indicators, potentially leading to misclassification. MRF, though computationally intensive, offers a more nuanced understanding of deprivation clusters. These findings highlight the trade-offs in multidimensional poverty measurement and provide insights for policymakers on method selection based on data availability and policy objectives. Future research should extend these approaches to non-binary indicators and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05676v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joseph Lam</dc:creator>
    </item>
    <item>
      <title>A hybrid predictive and prescriptive modelling framework for long-term mental healthcare workforce planning</title>
      <link>https://arxiv.org/abs/2406.17463</link>
      <description>arXiv:2406.17463v2 Announce Type: replace 
Abstract: Over the past decade, there has been a severe staffing shortage in mental healthcare, exacerbated by increased demand for mental health services due to COVID-19. This demand is projected to increase over the next decade or so, necessitating proactive workforce planning to ensure sufficient staffing for ongoing service delivery. Despite the subject's critical significance, the present literature lacks thorough research dedicated to developing a model that addresses the long-term workforce needs required for efficient mental healthcare planning. Furthermore, our interactions with mental health practitioners within the United Kingdom's National Health Service (NHS) revealed the practical need for such a model. To address this gap, we aim to develop a hybrid predictive and prescriptive modelling framework, which combines long-term probabilistic forecasting with an analytical stock-flow model, designed specifically for mental health workforce planning. Given the vital role of nurses, who account for one-third of the total mental health workforce, we focus on modelling the headcount of nurses, but the proposed model can be generalised to other types of workforce planning in the healthcare sector. Using statistical and machine learning approaches and real-world data from NHS, we first identify factors contributing to variations in workforce requirements, then develop a long-term forecasting model to estimate future workforce needs, and finally integrate it into an analytical stock-flow method to provide policy analysis. Our findings highlight the unsustainable nature of present staffing plans, showing a growing nursing shortage. Furthermore, the policy analysis demonstrates the ineffectiveness of blanket remedies, highlighting the need for regional-level policy developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17463v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harsha Chamara Hewage, Bahman Rostami-Tabar</dc:creator>
    </item>
    <item>
      <title>Monitoring time to event in registry data using CUSUMs based on relative survival models</title>
      <link>https://arxiv.org/abs/2411.09353</link>
      <description>arXiv:2411.09353v3 Announce Type: replace 
Abstract: An aspect of interest in surveillance of diseases is whether the survival time distribution changes over time. By following data in health registries over time, this can be monitored, either in real time or retrospectively. With relevant risk factors registered, these can be taken into account in the monitoring as well. A challenge in monitoring survival times based on registry data is that the information related to cause of death might either be missing or uncertain. To quantify the burden of disease in such cases, relative survival methods can be used, where the total hazard is modelled as the population hazard plus the excess hazard due to the disease.
  We propose a CUSUM procedure for monitoring for changes in the survival time distribution in cases where use of excess hazard models is relevant. The CUSUM chart is based on a survival log-likelihood ratio and extends previously suggested methods for monitoring of time to event data to the excess hazard setting. The procedure takes into account changes in the population risk over time, as well as changes in the excess hazard which is explained by observed covariates. Properties, challenges and an application to cancer registry data will be presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09353v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jimmy Huy Tran, Jan Terje Kval{\o}y, Hartwig K{\o}rner</dc:creator>
    </item>
    <item>
      <title>Method for recovering data on unreported low-severity crashes</title>
      <link>https://arxiv.org/abs/2503.04529</link>
      <description>arXiv:2503.04529v2 Announce Type: replace 
Abstract: Objective: Many low-severity crashes are not reported due to sampling criteria, introducing missing not at random (MNAR) bias. If not addressed, MNAR bias can lead to inaccurate safety analyses. This paper illustrates a statistical method to address such bias. Methods: We defined a custom probability distribution for the observed data as a product of an exponential population distribution and a logistic reporting function. We used modern Bayesian probabilistic programming techniques. Results: Using simulated data, we verified the correctness of the procedure. Applying it to real crash data, we estimated the {\Delta}v distribution for passenger vehicles involved in personal damage-only (PDO) rear-end crashes. We found that about 77% of cases are unreported. Conclusions: The method preserves the original data and it accounts well for uncertainty from both modeling assumptions and input data. It can improve safety assessments and it applies broadly to other MNAR cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04529v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Morando</dc:creator>
    </item>
    <item>
      <title>Granular mortality modeling with temperature and epidemic shocks: a three-state regime-switching approach</title>
      <link>https://arxiv.org/abs/2503.04568</link>
      <description>arXiv:2503.04568v2 Announce Type: replace 
Abstract: This paper develops a granular regime-switching framework to model mortality deviations from seasonal baseline trends driven by temperature and epidemic shocks. The framework features three states: (1) a baseline state that captures observed seasonal mortality patterns, (2) an environmental shock state for heat waves, and (3) a respiratory shock state that addresses mortality deviations caused by strong outbreaks of respiratory diseases due to influenza and COVID-19. Transition probabilities between states are modeled using covariate-dependent multinomial logit functions. These functions incorporate, among others, lagged temperature and influenza incidence rates as predictors, allowing dynamic adjustments to evolving shocks. Calibrated on weekly mortality data across 21 French regions and six age groups, the regime-switching framework accounts for spatial and demographic heterogeneity. Under various projection scenarios for temperature and influenza, we quantify uncertainty in mortality forecasts through prediction intervals constructed using an extensive bootstrap approach. These projections can guide healthcare providers and hospitals in managing risks and planning resources for potential future shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04568v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jens Robben, Karim Barigou, Torsten Kleinow</dc:creator>
    </item>
    <item>
      <title>Tensor PCA for Factor Models</title>
      <link>https://arxiv.org/abs/2212.12981</link>
      <description>arXiv:2212.12981v3 Announce Type: replace-cross 
Abstract: Modern empirical analysis often relies on high-dimensional panel datasets with non-negligible cross-sectional and time-series correlations. Factor models are natural for capturing such dependencies. A tensor factor model describes the $d$-dimensional panel as a sum of a reduced rank component and an idiosyncratic noise, generalizing traditional factor models for two-dimensional panels. We consider a tensor factor model corresponding to the notion of a reduced multilinear rank of a tensor. We show that for a strong factor model, a simple tensor principal component analysis algorithm is optimal for estimating factors and loadings. When the factors are weak, the convergence rate of simple TPCA can be improved with alternating least-squares iterations. We also provide inferential results for factors and loadings and propose the first test to select the number of factors. The new tools are applied to the problem of imputing missing values in a multidimensional panel of firm characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12981v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrii Babii, Eric Ghysels, Junsu Pan</dc:creator>
    </item>
    <item>
      <title>Robust Functional Principal Component Analysis for Non-Euclidean Random Objects</title>
      <link>https://arxiv.org/abs/2312.07741</link>
      <description>arXiv:2312.07741v2 Announce Type: replace-cross 
Abstract: Functional data analysis offers a diverse toolkit of statistical methods tailored for analyzing samples of real-valued random functions. Recently, samples of time-varying random objects, such as time-varying networks, have been increasingly encountered in modern data analysis. These data structures represent elements within general metric spaces that lack local or global linear structures, rendering traditional functional data analysis methods inapplicable. Moreover, the existing methodology for time-varying random objects does not work well in the presence of outlying objects. In this paper, we propose a robust method for analysing time-varying random objects. Our method employs pointwise Fr\'{e}chet medians and then constructs pointwise distance trajectories between the individual time courses and the sample Fr\'{e}chet medians. This representation effectively transforms time-varying objects into functional data. A novel robust approach to functional principal component analysis based on a Winsorized U-statistic estimator of the covariance structure is introduced. The proposed robust analysis of these distance trajectories is able to identify key features of time-varying objects and is useful for downstream analysis. To illustrate the efficacy of our approach, numerical studies focusing on dynamic networks are conducted. The results indicate that the proposed method exhibits good all-round performance and surpasses the existing approach in terms of robustness, showcasing its superior performance in handling time-varying objects data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07741v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiazhen Xu, Andrew T. A. Wood, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Statistical Inference and A/B Testing in Fisher Markets and Paced Auctions</title>
      <link>https://arxiv.org/abs/2406.15522</link>
      <description>arXiv:2406.15522v3 Announce Type: replace-cross 
Abstract: We initiate the study of statistical inference and A/B testing for two market equilibrium models: linear Fisher market (LFM) equilibrium and first-price pacing equilibrium (FPPE). LFM arises from fair resource allocation systems such as allocation of food to food banks and notification opportunities to different types of notifications. For LFM, we assume that the data observed is captured by the classical finite-dimensional Fisher market equilibrium, and its steady-state behavior is modeled by a continuous limit Fisher market. The second type of equilibrium we study, FPPE, arises from internet advertising where advertisers are constrained by budgets and advertising opportunities are sold via first-price auctions. For platforms that use pacing-based methods to smooth out the spending of advertisers, FPPE provides a hindsight-optimal configuration of the pacing method. We propose a statistical framework for the FPPE model, in which a continuous limit FPPE models the steady-state behavior of the auction platform, and a finite FPPE provides the data to estimate primitives of the limit FPPE. Both LFM and FPPE have an Eisenberg-Gale convex program characterization, the pillar upon which we derive our statistical theory. We start by deriving basic convergence results for the finite market to the limit market. We then derive asymptotic distributions, and construct confidence intervals. Furthermore, we establish the asymptotic local minimax optimality of estimation based on finite markets. We then show that the theory can be used for conducting statistically valid A/B testing on auction platforms. Synthetic and semi-synthetic experiments verify the validity and practicality of our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15522v3</guid>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>The Expected Peak-to-Average Power Ratio of White Gaussian Noise in Sampled I/Q Data</title>
      <link>https://arxiv.org/abs/2501.11261</link>
      <description>arXiv:2501.11261v2 Announce Type: replace-cross 
Abstract: One of the fundamental endeavors in radio frequency (RF) metrology is to measure the power of signals, where a common aim is to estimate the peak-to-average power ratio (PAPR), which quantifies the ratio of the maximum (peak) to the mean value. For a finite number of discrete-time samples of baseband in-phase and quadrature (I/Q) white Gaussian noise (WGN) that are independent and identically distributed with zero mean, we derive a closed-form, exact formula for mean PAPR that is well-approximated by the natural logarithm of the number of samples plus Euler's constant. Additionally, we give related theoretical results for the mean crest factor (CF). After comparing our main result to previously published approximate formulas, we examine how violations of the WGN assumptions in sampled I/Q data result in deviations from the expected value of PAPR. Finally, utilizing a measured RF I/Q acquisition, we illustrate how our formula for mean PAPR can be applied to spectral analysis with spectrograms to verify when measured RF emissions are WGN in a given frequency band.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11261v2</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIM.2025.3544288</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Instrumentation and Measurement, 2025</arxiv:journal_reference>
      <dc:creator>Adam Wunderlich, Aric Sanders</dc:creator>
    </item>
    <item>
      <title>Filtered Markovian Projection: Dimensionality Reduction in Filtering for Stochastic Reaction Networks</title>
      <link>https://arxiv.org/abs/2502.07918</link>
      <description>arXiv:2502.07918v2 Announce Type: replace-cross 
Abstract: Stochastic reaction networks (SRNs) model stochastic effects for various applications, including intracellular chemical or biological processes and epidemiology. A typical challenge in practical problems modeled by SRNs is that only a few state variables can be dynamically observed. Given the measurement trajectories, one can estimate the conditional probability distribution of unobserved (hidden) state variables by solving a stochastic filtering problem. In this setting, the conditional distribution evolves over time according to an extensive or potentially infinite-dimensional system of coupled ordinary differential equations with jumps, known as the filtering equation. The current numerical filtering techniques, such as the Filtered Finite State Projection (D'Ambrosio et al., 2022), are hindered by the curse of dimensionality, significantly affecting their computational performance. To address these limitations, we propose to use a dimensionality reduction technique based on the Markovian projection (MP), initially introduced for forward problems (Ben Hammouda et al., 2024). In this work, we explore how to adapt the existing MP approach to the filtering problem and introduce a novel version of the MP, the Filtered MP, that guarantees the consistency of the resulting estimator. The novel method employs a reduced-variance particle filter for estimating the jump intensities of the projected model and solves the filtering equations in a low-dimensional space. The analysis and empirical results highlight the superior computational efficiency of projection methods compared to the existing filtered finite state projection in the large dimensional setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07918v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiheb Ben Hammouda, Maksim Chupin, Sophia M\"unker, Ra\'ul Tempone</dc:creator>
    </item>
    <item>
      <title>A Novel Hybrid Approach to Contraceptive Demand Forecasting: Integrating Point Predictions with Probabilistic Distributions</title>
      <link>https://arxiv.org/abs/2502.09685</link>
      <description>arXiv:2502.09685v2 Announce Type: replace-cross 
Abstract: Accurate demand forecasting is vital for ensuring reliable access to contraceptive products, supporting key processes like procurement, inventory, and distribution. However, forecasting contraceptive demand in developing countries presents challenges, including incomplete data, poor data quality, and the need to account for multiple geographical and product factors. Current methods often rely on simple forecasting techniques, which fail to capture demand uncertainties arising from these factors, warranting expert involvement. Our study aims to improve contraceptive demand forecasting by combining probabilistic forecasting methods with expert knowledge. We developed a hybrid model that combines point forecasts from domain-specific model with probabilistic distributions from statistical and machine learning approaches, enabling human input to fine-tune and enhance the system-generated forecasts. This approach helps address the uncertainties in demand and is particularly useful in resource-limited settings. We evaluate different forecasting methods, including time series, Bayesian, machine learning, and foundational time series methods alongside our new hybrid approach. By comparing these methods, we provide insights into their strengths, weaknesses, and computational requirements. Our research fills a gap in forecasting contraceptive demand and offers a practical framework that combines algorithmic and human expertise. Our proposed model can also be generalized to other humanitarian contexts with similar data patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09685v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harsha Chamara Hewage, Bahman Rostami-Tabar, Aris Syntetos, Federico Liberatore, Glenn Milano</dc:creator>
    </item>
  </channel>
</rss>
