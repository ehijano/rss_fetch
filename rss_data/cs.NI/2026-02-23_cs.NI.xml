<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Feb 2026 07:44:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Graph-Neural Multi-Agent Coordination for Distributed Access-Point Selection in Cell-Free Massive MIMO</title>
      <link>https://arxiv.org/abs/2602.17954</link>
      <description>arXiv:2602.17954v1 Announce Type: new 
Abstract: Cell-free massive MIMO (CFmMIMO) systems require scalable and reliable distributed coordination mechanisms to operate under stringent communication and latency constraints. A central challenge is the Access Point Selection (APS) problem, which seeks to determine the subset of serving Access Points (APs) for each User Equipment (UE) that can satisfy UEs' Spectral Efficiency (SE) requirements while minimizing network power consumption. We introduce APS-GNN, a scalable distributed multi-agent learning framework that decomposes APS into agents operating at the granularity of individual AP-UE connections. Agents coordinate via local observation exchange over a novel Graph Neural Network (GNN) architecture and share parameters to reuse their knowledge and experience. APS-GNN adopts a constrained reinforcement learning approach to provide agents with explicit observability of APS' conflicting objectives, treating SE satisfaction as a cost and power reduction as a reward. Both signals are defined locally, facilitating effective credit assignment and scalable coordination in large networks. To further improve training stability and exploration efficiency, the policy is initialized via supervised imitation learning from a heuristic APS baseline. We develop a realistic CFmMIMO simulator and demonstrate that APS-GNN delivers the target SE while activating 50-70% fewer APs than heuristic and centralized Multi-agent Reinforcement Learning (MARL) baselines in different evaluation scenarios. Moreover, APS-GNN achieves one to two orders of magnitude lower inference latency than centralized MARL approaches due to its fully parallel and distributed execution. These results establish APS-GNN as a practical and scalable solution for APS in large-scale CFmMIMO networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17954v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Zangooei, Lou Sala\"un, Chung Shue Chen, Raouf Boutaba</dc:creator>
    </item>
    <item>
      <title>Rethinking Beam Management: Generalization Limits Under Hardware Heterogeneity</title>
      <link>https://arxiv.org/abs/2602.18151</link>
      <description>arXiv:2602.18151v1 Announce Type: new 
Abstract: Hardware heterogeneity across diverse user devices poses new challenges for beam-based communication in 5G and beyond. This heterogeneity limits the applicability of machine learning (ML)-based algorithms. This article highlights the critical need to treat hardware heterogeneity as a first-class design concern in ML-aided beam management. We analyze key failure modes in the presence of heterogeneity and present case studies demonstrating their performance impact. Finally, we discuss potential strategies to improve generalization in beam management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18151v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nikita Zeulin, Olga Galinina, Ibrahim Kilinc, Sergey Andreev, Robert W. Heath Jr</dc:creator>
    </item>
    <item>
      <title>Noise Mitigation Methods for Digital Visible Light Communication</title>
      <link>https://arxiv.org/abs/2602.18187</link>
      <description>arXiv:2602.18187v1 Announce Type: new 
Abstract: Visible Light Communication (VLC) using Light Emitting Diodes (LEDs) has gained attention due to its low power consumption, long lifetime, and fast response. However, VLC suffers from optical noise generated by ambient light sources such as fluorescent lamps, which leads to waveform distortion and increased bit error rates (BER). In this paper, we propose two noise reduction methods for Digital Visible Light Communication (DVLC) systems. The first method exploits the periodic nature of interference caused by AC-powered-line illumination and reduces interference by subtracting sampled noise waveforms from the received signal. Second, inspired by Active Noise Control (ANC) techniques, an additional photodiode is introduced for noise reception, and subtraction circuits are employed to attenuate noise in real time. Experimental results show that both methods improve BER performance compared with conventional receivers, with the ANC-inspired approach achieving superior performance under all tested conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18187v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5121/ijcnc.2026.18104</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Computer Networks &amp; Communications (IJCNC) Vol.18, No.1, January 2026</arxiv:journal_reference>
      <dc:creator>Wataru Uemura, Takumi Hamano</dc:creator>
    </item>
    <item>
      <title>A traffic incident management framework for vehicular ad hoc networks</title>
      <link>https://arxiv.org/abs/2602.18208</link>
      <description>arXiv:2602.18208v1 Announce Type: new 
Abstract: Vehicular Ad Hoc Networks (VANETs) support the information dissemination among vehicles, Roadside Units (RSUs), and a Trust Authority (TA). A trust model evaluates an entity or data or both to determine truthfulness. A security model confirms authentication, integrity, availability, non repudiation issues. With these aspects in mind, many models have been proposed in literature. Furthermore, many information dissemination approaches are proposed. However, the lack of a model that can manage traffic incidents completely inspires this work. This paper details how and when a message needs to be generated and relayed so that the incidents can be reported and managed in a timely manner. This paper addresses this challenge by providing a traffic incident management model to manage several traffic incidents efficiently. Additionally, we simulate this model using the VEINS simulator with vehicles, RSUs, and a TA. From the experiments, we measure the average number of transmissions required for reporting a single traffic incident while varying the vehicle density and relaying considerations. We consider two types of relaying. In one series of experiments, messages from regular vehicles and RSUs are relayed up to four hops. In another series of experiments, messages from the regular vehicles and RSUs are relayed until their generation time reaches sixty seconds. Additionally, messages from the official vehicles are relayed when they approach an incident or when the incident is cleared. Results from the simulations show that more vehicles are informed with four-hop relaying than sixty-second relaying in both cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18208v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5121/ijwmn.2025.17202</arxiv:DOI>
      <dc:creator>Rezvi Shahariar, Chris Phillips</dc:creator>
    </item>
    <item>
      <title>VaN3Twin: the Multi-Technology V2X Digital Twin with Ray-Tracing in the Loop</title>
      <link>https://arxiv.org/abs/2505.14184</link>
      <description>arXiv:2505.14184v2 Announce Type: replace 
Abstract: This paper presents VaN3Twin-the first open-source, full-stack Network Digital Twin (NDT) framework for simulating the coexistence of multiple Vehicle-to-Everything (V2X) communication technologies with accurate physical-layer modeling via ray tracing. VaN3Twin extends the ms-van3t simulator by integrating Sionna Ray Tracer (RT) in the loop, enabling high-fidelity representation of wireless propagation, including diverse Line-of-Sight (LoS) conditions with focus on LoS blockage due to other vehicles' meshes, Doppler effect, and site-dependent effects-e.g., scattering and diffraction. Unlike conventional simulation tools, the proposed framework supports realistic coexistence analysis across DSRC and C-V2X technologies operating over shared spectrum. A dedicated interference tracking module captures cross-technology interference at the time-frequency resource block level and enhances signal-to-interference-plus-noise ratio (SINR) estimation by eliminating artifacts such as the bimodal behavior induced by separate LoS/NLoS propagation models. Compared to field measurements, VaN3Twin reduces application-layer disagreement by 50% in rural and over 70% in urban environments with respect to current state-of-the-art simulation tools, demonstrating its value for scalable and accurate digital twin-based V2X coexistence simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14184v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Pegurri, Diego Gasco, Francesco Linsalata, Marco Rapelli, Eugenio Moro, Francesco Raviglione, Claudio Casetti</dc:creator>
    </item>
    <item>
      <title>NetForge: A Programmable Substrate for Bottleneck-Centric Network Data Generation</title>
      <link>https://arxiv.org/abs/2507.13476</link>
      <description>arXiv:2507.13476v3 Announce Type: replace 
Abstract: The behavior of Internet applications is shaped by congestion dynamics at bottleneck links, yet data capturing application behavior across diverse bottleneck regimes remains scarce. Bridging this gap requires a data-generation substrate that simultaneously provides controllability, composability, fidelity, and replicability--capabilities existing approaches struggle to achieve simultaneously. This paper introduces NetForge, a programmable substrate for bottleneck-centric data generation guided by progressive disaggregation: NetForge (i) decouples bottleneck intent from execution, (ii) separates static bottleneck attributes from dynamic congestion pressure, and (iii) disaggregates observed demand dynamics from their original trace context via Cross-Traffic Profiles (CTPs). CTPs transform passive packet traces into reusable, composable pressure signals that can be selected and transformed to specify dynamic bottleneck behavior. Our evaluation shows that NetForge satisfies the four requirements and, in an ABR case study, generates data that remains realistic, expands coverage into underrepresented regimes, and, in turn, improves model performance by up to 47% by reducing transmission-time prediction error of the Fugu model. Together, these results establish NetForge as a practical substrate for studying Internet application behavior across diverse bottleneck regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13476v3</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jaber Daneshamooz, Satyandra Guthula, Jessica Nguyen, William Chen, Sanjay Chandrasekaran, Ankit Gupta, Arpit Gupta, Walter Willinger</dc:creator>
    </item>
    <item>
      <title>DH-EAC: Design of a Dynamic, Hierarchical Entanglement Access Control Protocol</title>
      <link>https://arxiv.org/abs/2510.02895</link>
      <description>arXiv:2510.02895v2 Announce Type: replace 
Abstract: We propose Dynamic, Hierarchical Entanglement Access Control (DH-EAC), a pure-quantum protocol for fair and anonymous allocation of scarce entanglement across wide-area quantum networks composed of many quantum LANs (QLANs). Prior Dicke-state-based pure-quantum MACs resolve contention by local measurements without classical signaling, but they mainly target a single QLAN under static conditions; extending them to wide-area, dynamic settings while avoiding post-selection reconciliation remains open. DH-EAC adopts a two-layer pure-quantum lottery: the outer layer selects winning QLANs and the inner layer selects winning nodes within each winning QLAN. A key design principle is that both the winning set and the per-QLAN quota are fixed by measurements alone, so the contention loop requires no classical round trip. The protocol thus aims to jointly satisfy anonymity (no node IDs revealed until decisions are fixed) and fairness (bias suppression under heterogeneous QLAN sizes). We also provide analytical models for success probability and latency under a standard i.i.d. loss model, and we evaluate DH-EAC against two baselines - single-layer Dicke within one QLAN and a classical GO-driven allocator - using a minimal, reproducible set of scenarios. Metrics include success probability, end-to-end latency, throughput, and Jain's fairness index. The results indicate that DH-EAC offers an implementable design point in the space of entanglement access control, balancing pure-quantum contention resolution, anonymity, and scalability for multi-QLAN networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02895v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akihisa Takahashi, Yoshito Tobe</dc:creator>
    </item>
    <item>
      <title>FGGM: Formal Grey-box Gradient Method for Attacking DRL-based MU-MIMO Scheduler</title>
      <link>https://arxiv.org/abs/2510.26075</link>
      <description>arXiv:2510.26075v2 Announce Type: replace 
Abstract: In 5G mobile communication systems, MU-MIMO has been applied to enhance spectral efficiency and support high data rates. To maximize spectral efficiency while providing fairness among users, the base station (BS) needs to selects a subset of users for data transmission. Given that this problem is NP-hard, DRL-based methods have been proposed to infer the near-optimal solutions in real-time, yet this approach has an intrinsic security problem. This paper investigates how a group of adversarial users can exploit unsanitized raw CSIs to launch a throughput degradation attack. Most existing studies only focused on systems in which adversarial users can obtain the exact values of victims' CSIs, but this is impractical in the case of uplink transmission in LTE/5G mobile systems. We note that the DRL policy contains an observation normalizer which has the mean and variance of the observation to improve training convergence. Adversarial users can then estimate the upper and lower bounds of the local observations including the CSIs of victims based solely on that observation normalizer. We develop an attacking scheme FGGM by leveraging polytope abstract domains, a technique used to bound the outputs of a neural network given the input ranges. Our goal is to find one set of intentionally manipulated CSIs which can achieve the attacking goals for the whole range of local observations of victims. Experimental results demonstrate that FGGM can determine a set of adversarial CSI vector controlled by adversarial users, then reuse those CSIs throughout the simulation to reduce the network throughput of a victim up to 70\% without knowing the exact value of victims' local observations. This study serves as a case study and can be applied to many other DRL-based problems, such as a knapsack-oriented resource allocation problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26075v2</guid>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thanh Le, Hai Duong, Yusheng Ji, ThanhVu Nguyen, John C. S. Lui</dc:creator>
    </item>
    <item>
      <title>Bitcoin Under Stress: Measuring Infrastructure Resilience 2014-2025</title>
      <link>https://arxiv.org/abs/2602.14372</link>
      <description>arXiv:2602.14372v2 Announce Type: replace 
Abstract: Bitcoin's design promises resilience through decentralization, yet the physical infrastructure supporting the network creates hidden dependencies. We present the first longitudinal study of Bitcoin's resilience to submarine cable failures, using 11 years of P2P network data (2014--2025) and 68 verified cable fault events. Applying a Buldyrev-style cascade model at country level, we find that Bitcoin's clearnet (non-TOR) critical failure threshold $p_c \approx 0.72$--$0.92$ for random failures, meaning the vast majority of inter-country cables must fail before significant node disconnection. Targeted attacks are an order of magnitude more effective ($p_c = 0.05$--$0.20$). To address the majority of nodes now using TOR with unobservable locations, we develop a 4-layer multiplex model incorporating TOR relay infrastructure. Because relay bandwidth concentrates in well-connected European countries, TOR adoption increases resilience under current relay geography ($\Delta p_c \approx +0.02$--$+0.10$) rather than introducing hidden fragility. Empirical validation confirms weak physical-layer coupling: 87% of historical cable faults caused less than 5% node impact. We contribute: (1) a multiplex percolation framework for overlay-underlay coupling, including a 4-layer TOR relay model; (2) the first empirical measurement of Bitcoin's physical-layer resilience over a decade; and (3) evidence that TOR adoption amplifies resilience, with distributional bounds quantifying uncertainty under partial observability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14372v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenbin Wu, Alexander Neumueller</dc:creator>
    </item>
  </channel>
</rss>
