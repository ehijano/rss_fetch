<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 May 2025 02:30:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Particle Filter for Bayesian Inference on Privatized Data</title>
      <link>https://arxiv.org/abs/2505.00877</link>
      <description>arXiv:2505.00877v1 Announce Type: new 
Abstract: Differential Privacy (DP) is a probabilistic framework that protects privacy while preserving data utility. To protect the privacy of the individuals in the dataset, DP requires adding a precise amount of noise to a statistic of interest; however, this noise addition alters the resulting sampling distribution, making statistical inference challenging. One of the main DP goals in Bayesian analysis is to make statistical inference based on the private posterior distribution. While existing methods have strengths in specific conditions, they can be limited by poor mixing, strict assumptions, or low acceptance rates. We propose a novel particle filtering algorithm, which features (i) consistent estimates, (ii) Monte Carlo error estimates and asymptotic confidence intervals, (iii) computational efficiency, and (iv) accommodation to a wide variety of priors, models, and privacy mechanisms with minimal assumptions. We empirically evaluate our algorithm through a variety of simulation settings as well as an application to a 2021 Canadian census dataset, demonstrating the efficacy and adaptability of the proposed sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00877v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu-Wei Chen, Pranav Sanghi, Jordan Awan</dc:creator>
    </item>
    <item>
      <title>A Sequential Quadratic Hamiltonian-Based Estimation Method for Box-Cox Transformation Cure Model</title>
      <link>https://arxiv.org/abs/2505.01097</link>
      <description>arXiv:2505.01097v1 Announce Type: new 
Abstract: We propose an enhanced estimation method for the Box-Cox transformation (BCT) cure rate model parameters by introducing a generic maximum likelihood estimation algorithm, the sequential quadratic Hamiltonian (SQH) scheme, which is based on a gradient-free approach. We apply the SQH algorithm to the BCT cure model and, through an extensive simulation study, compare its model fitting results with those obtained using the recently developed non-linear conjugate gradient (NCG) algorithm. Since the NCG method has already been shown to outperform the well-known expectation maximization algorithm, our focus is on demonstrating the superiority of the SQH algorithm over NCG. First, we show that the SQH algorithm produces estimates with smaller bias and root mean square error for all BCT cure model parameters, resulting in more accurate and precise cure rate estimates. We then demonstrate that, being gradient-free, the SQH algorithm requires less CPU time to generate estimates compared to the NCG algorithm, which only computes the gradient and not the Hessian. These advantages make the SQH algorithm the preferred estimation method over the NCG method for the BCT cure model. Finally, we apply the SQH algorithm to analyze a well-known melanoma dataset and present the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01097v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Phuong Bui, Varun Jadhav, Suvra Pal, Souvik Roy</dc:creator>
    </item>
    <item>
      <title>Modeling and Analyzing Urban Networks and Amenities with OSMnx</title>
      <link>https://arxiv.org/abs/2505.00736</link>
      <description>arXiv:2505.00736v1 Announce Type: cross 
Abstract: OSMnx is a Python package for downloading, modeling, analyzing, and visualizing urban networks and any other geospatial features from OpenStreetMap data. A large and growing body of literature uses it to conduct scientific studies across the disciplines of geography, urban planning, transport engineering, computer science, and others. The OSMnx project has recently developed and implemented many new features, modeling capabilities, and analytical methods. The package now encompasses substantially more functionality than was previously documented in the literature. This article introduces OSMnx's modern capabilities, usage, and design -- in addition to the scientific theory and logic underlying them. It shares lessons learned in geospatial software development and reflects on open science's implications for urban modeling and analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00736v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.MS</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>nlin.AO</category>
      <category>physics.app-ph</category>
      <category>stat.CO</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Geographical Analysis, 2025</arxiv:journal_reference>
      <dc:creator>Geoff Boeing</dc:creator>
    </item>
    <item>
      <title>Bayesian Forensic DNA Mixture Deconvolution Using a Novel String Similarity Measure</title>
      <link>https://arxiv.org/abs/2505.00934</link>
      <description>arXiv:2505.00934v1 Announce Type: cross 
Abstract: Mixture interpretation is a central challenge in forensic science, where evidence often contains contributions from multiple sources. In the context of DNA analysis, biological samples recovered from crime scenes may include genetic material from several individuals, necessitating robust statistical tools to assess whether a specific person of interest (POI) is among the contributors. Methods based on capillary electrophoresis (CE) are currently in use worldwide, but offer limited resolution in complex mixtures. Advancements in massively parallel sequencing (MPS) technologies provide a richer, more detailed representation of DNA mixtures, but require new analytical strategies to fully leverage this information. In this work, we present a Bayesian framework for evaluating whether a POIs DNA is present in an MPS-based forensic sample. The model accommodates known contributors, such as the victim, and uses a novel string edit distance to quantify similarity between observed alleles and sequencing artifacts. The resulting Bayes factors enable effective discrimination between samples that do and do not contain the POIs DNA, demonstrating strong performance in both hypothesis testing and classification settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00934v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.CO</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taylor Petty, Jan Hannig, Hari Iyer</dc:creator>
    </item>
    <item>
      <title>StablePCA: Learning Shared Representations across Multiple Sources via Minimax Optimization</title>
      <link>https://arxiv.org/abs/2505.00940</link>
      <description>arXiv:2505.00940v1 Announce Type: cross 
Abstract: When synthesizing multisource high-dimensional data, a key objective is to extract low-dimensional feature representations that effectively approximate the original features across different sources. Such general feature extraction facilitates the discovery of transferable knowledge, mitigates systematic biases such as batch effects, and promotes fairness. In this paper, we propose Stable Principal Component Analysis (StablePCA), a novel method for group distributionally robust learning of latent representations from high-dimensional multi-source data. A primary challenge in generalizing PCA to the multi-source regime lies in the nonconvexity of the fixed rank constraint, rendering the minimax optimization nonconvex. To address this challenge, we employ the Fantope relaxation, reformulating the problem as a convex minimax optimization, with the objective defined as the maximum loss across sources. To solve the relaxed formulation, we devise an optimistic-gradient Mirror Prox algorithm with explicit closed-form updates. Theoretically, we establish the global convergence of the Mirror Prox algorithm, with the convergence rate provided from the optimization perspective. Furthermore, we offer practical criteria to assess how closely the solution approximates the original nonconvex formulation. Through extensive numerical experiments, we demonstrate StablePCA's high accuracy and efficiency in extracting robust low-dimensional representations across various finite-sample scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00940v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Molei Liu, Jing Lei, Francis Bach, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Gaussian Differential Private Bootstrap by Subsampling</title>
      <link>https://arxiv.org/abs/2505.01197</link>
      <description>arXiv:2505.01197v1 Announce Type: cross 
Abstract: Bootstrap is a common tool for quantifying uncertainty in data analysis. However, besides additional computational costs in the application of the bootstrap on massive data, a challenging problem in bootstrap based inference under Differential Privacy consists in the fact that it requires repeated access to the data. As a consequence, bootstrap based differentially private inference requires a significant increase of the privacy budget, which on the other hand comes with a substantial loss in statistical accuracy.
  A potential solution to reconcile the conflicting goals of statistical accuracy and privacy is to analyze the data under parametric model assumptions and in the last decade, several parametric bootstrap methods for inference under privacy have been investigated. However, uncertainty quantification by parametric bootstrap is only valid if the the quantities of interest can be identified as the parameters of a statistical model and the imposed model assumptions are (at least approximately) satisfied. An alternative to parametric methods is the empirical bootstrap that is a widely used tool for non-parametric inference and well studied in the non-private regime. However, under privacy, less insight is available. In this paper, we propose a private empirical $m$ out of $n$ bootstrap and validate its consistency and privacy guarantees under Gaussian Differential Privacy. Compared to the the private $n$ out of $n$ bootstrap, our approach has several advantages. First, it comes with less computational costs, in particular for massive data. Second, the proposed procedure needs less additional noise in the bootstrap iterations, which leads to an improved statistical accuracy while asymptotically guaranteeing the same level of privacy. Third, we demonstrate much better finite sample properties compared to the currently available procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01197v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Holger Dette, Carina Graw</dc:creator>
    </item>
    <item>
      <title>Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm</title>
      <link>https://arxiv.org/abs/2308.08852</link>
      <description>arXiv:2308.08852v2 Announce Type: replace-cross 
Abstract: Graphical models have exhibited their performance in numerous tasks ranging from biological analysis to recommender systems. However, graphical models with hub nodes are computationally difficult to fit, particularly when the dimension of the data is large. To efficiently estimate the hub graphical models, we introduce a two-phase algorithm. The proposed algorithm first generates a good initial point via a dual alternating direction method of multipliers (ADMM), and then warm starts a semismooth Newton (SSN) based augmented Lagrangian method (ALM) to compute a solution that is accurate enough for practical tasks. We fully excavate the sparsity structure of the generalized Jacobian arising from the hubs in the graphical models, which ensures that the algorithm can obtain a nice solution very efficiently. Comprehensive experiments on both synthetic data and real data show that it obviously outperforms the existing state-of-the-art algorithms. In particular, in some high dimensional tasks, it can save more than 70\% of the execution time, meanwhile still achieves a high-quality estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08852v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Chengjing Wang, Peipei Tang, Wenling He, Meixia Lin</dc:creator>
    </item>
  </channel>
</rss>
