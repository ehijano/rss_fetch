<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Mar 2024 04:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Enhanced Cauchy Schwarz inequality and some of its statistical applications</title>
      <link>https://arxiv.org/abs/2403.13964</link>
      <description>arXiv:2403.13964v1 Announce Type: new 
Abstract: We present a general refinement of the Cauchy-Schwarz inequality over complete inner product spaces and show that it can be of interest for some statistical applications. This generalizes and simplifies previous results on the same subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13964v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Scarlatti</dc:creator>
    </item>
    <item>
      <title>Large parameter asymptotic analysis for homogeneous normalized random measures with independent increments</title>
      <link>https://arxiv.org/abs/2403.14032</link>
      <description>arXiv:2403.14032v1 Announce Type: new 
Abstract: Homogeneous normalized random measures with independent increments (hNRMIs) represent a broad class of Bayesian nonparametric priors and thus are widely used. In this paper, we obtain the strong law of large numbers, the central limit theorem and the functional central limit theorem of hNRMIs when the concentration parameter $a$ approaches infinity. To quantify the convergence rate of the obtained central limit theorem, we further study the Berry-Esseen bound, which turns out to be of the form $O \left( \frac{1}{\sqrt{a}}\right)$. As an application of the central limit theorem, we present the functional delta method, which can be employed to obtain the limit of the quantile process of hNRMIs. As an illustration of the central limit theorems, we demonstrate the convergence numerically for the Dirichlet processes and the normalized inverse Gaussian processes with various choices of the concentration parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14032v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junxi Zhang, Shui Feng, Yaozhong Hu</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: II. Central Moments</title>
      <link>https://arxiv.org/abs/2403.14570</link>
      <description>arXiv:2403.14570v1 Announce Type: new 
Abstract: In descriptive statistics, $U$-statistics arise naturally in producing minimum-variance unbiased estimators. In 1984, Serfling considered the distribution formed by evaluating the kernel of the $U$-statistics and proposed generalized $L$-statistics which includes Hodges-Lehamnn estimator and Bickel-Lehmann spread as special cases. However, the structures of the kernel distributions remain unclear. In 1954, Hodges and Lehmann demonstrated that if $X$ and $Y$ are independently sampled from the same unimodal distribution, $X-Y$ will exhibit symmetrical unimodality with its peak centered at zero. Building upon this foundational work, the current study delves into the structure of the kernel distribution. It is shown that the $\mathbf{k}$th central moment kernel distributions ($\mathbf{k}&gt;2$) derived from a unimodal distribution exhibit location invariance and is also nearly unimodal with the mode and median close to zero. This article provides an approach to study the general structure of kernel distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14570v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuobang Li</dc:creator>
    </item>
    <item>
      <title>Optimal State Estimation in the Presence of Non-Gaussian Uncertainty via Wasserstein Distance Minimization</title>
      <link>https://arxiv.org/abs/2403.13828</link>
      <description>arXiv:2403.13828v1 Announce Type: cross 
Abstract: This paper presents a novel distribution-agnostic Wasserstein distance-based estimation framework. The goal is to determine an optimal map combining prior estimate with measurement likelihood such that posterior estimation error optimally reaches the Dirac delta distribution with minimal effort. The Wasserstein metric is used to quantify the effort of transporting from one distribution to another. We hypothesize that minimizing the Wasserstein distance between the posterior error and the Dirac delta distribution results in optimal information fusion and posterior state uncertainty. Framework validation is demonstrated by the successful recovery of the classical Kalman filter for linear systems with Gaussian uncertainties. Notably, the proposed Wasserstein filter does not rely on particle representation of uncertainty. Furthermore, the classical result for the Gaussian Sum Filter (GSF) is retrieved from the Wasserstein framework. This approach analytically exhibits the suboptimality of GSF and enables the use of nonlinear optimization techniques to enhance the accuracy of the Gaussian sum estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13828v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Himanshu Prabhat, Raktim Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Optimal Transport for Fairness: Archival Data Repair using Small Research Data Sets</title>
      <link>https://arxiv.org/abs/2403.13864</link>
      <description>arXiv:2403.13864v1 Announce Type: cross 
Abstract: With the advent of the AI Act and other regulations, there is now an urgent need for algorithms that repair unfairness in training data. In this paper, we define fairness in terms of conditional independence between protected attributes ($S$) and features ($X$), given unprotected attributes ($U$). We address the important setting in which torrents of archival data need to be repaired, using only a small proportion of these data, which are $S|U$-labelled (the research data). We use the latter to design optimal transport (OT)-based repair plans on interpolated supports. This allows {\em off-sample}, labelled, archival data to be repaired, subject to stationarity assumptions. It also significantly reduces the size of the supports of the OT plans, with correspondingly large savings in the cost of their design and of their {\em sequential\/} application to the off-sample data. We provide detailed experimental results with simulated and benchmark real data (the Adult data set). Our performance figures demonstrate effective repair -- in the sense of quenching conditional dependence -- of large quantities of off-sample, labelled (archival) data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13864v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abigail Langbridge, Anthony Quinn, Robert Shorten</dc:creator>
    </item>
    <item>
      <title>Analysing heavy-tail properties of Stochastic Gradient Descent by means of Stochastic Recurrence Equations</title>
      <link>https://arxiv.org/abs/2403.13868</link>
      <description>arXiv:2403.13868v1 Announce Type: cross 
Abstract: In recent works on the theory of machine learning, it has been observed that heavy tail properties of Stochastic Gradient Descent (SGD) can be studied in the probabilistic framework of stochastic recursions. In particular, G\"{u}rb\"{u}zbalaban et al. (arXiv:2006.04740) considered a setup corresponding to linear regression for which iterations of SGD can be modelled by a multivariate affine stochastic recursion $X_k=A_k X_{k-1}+B_k$, for independent and identically distributed pairs $(A_k, B_k)$, where $A_k$ is a random symmetric matrix and $B_k$ is a random vector. In this work, we will answer several open questions of the quoted paper and extend their results by applying the theory of irreducible-proximal (i-p) matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13868v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ewa Damek, Sebastian Mentemeier</dc:creator>
    </item>
    <item>
      <title>Moderate Deviation and Berry-Esseen Bounds in the $p$-Spin Curie-Weiss Model</title>
      <link>https://arxiv.org/abs/2403.14122</link>
      <description>arXiv:2403.14122v1 Announce Type: cross 
Abstract: Limit theorems for the magnetization in the $p$-spin Curie-Weiss model, for $p \geq 3$, has been derived recently by Mukherjee et al. (2021). In this paper, we strengthen these results by proving Cram\'er-type moderate deviation theorems and Berry-Esseen bounds for the magnetization (suitably centered and scaled). In particular, we show that the rate of convergence is $O(N^{-\frac{1}{2}})$ when the magnetization has asymptotically Gaussian fluctuations, and it is $O(N^{-\frac{1}{4}})$ when the fluctuations are non-Gaussian. As an application, we derive a Berry-Esseen bound for the maximum pseudolikelihood estimate of the inverse temperature in $p$-spin Curie-Weiss model with no external field, for all points in the parameter space where consistent estimation is possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14122v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Somabha Mukherjee, Tianyu Liu, Bhaswar B. Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Generalized Rosenbaum Bounds Sensitivity Analysis for Matched Observational Studies with Treatment Doses: Sufficiency, Consistency, and Efficiency</title>
      <link>https://arxiv.org/abs/2403.14152</link>
      <description>arXiv:2403.14152v1 Announce Type: cross 
Abstract: In matched observational studies with binary treatments, the Rosenbaum bounds framework is arguably the most widely used sensitivity analysis framework for assessing sensitivity to unobserved covariates. Unlike the binary treatment case, although widely needed in practice, sensitivity analysis for matched observational studies with treatment doses (i.e., non-binary treatments such as ordinal treatments or continuous treatments) still lacks solid foundations and valid methodologies. We fill in this blank by establishing theoretical foundations and novel methodologies under a generalized Rosenbaum bounds sensitivity analysis framework. First, we present a criterion for assessing the validity of sensitivity analysis in matched observational studies with treatment doses and use that criterion to justify the necessity of incorporating the treatment dose information into sensitivity analysis through generalized Rosenbaum sensitivity bounds. We also generalize Rosenbaum's classic sensitivity parameter $\Gamma$ to the non-binary treatment case and prove its sufficiency. Second, we study the asymptotic properties of sensitivity analysis by generalizing Rosenbaum's classic design sensitivity and Bahadur efficiency for testing Fisher's sharp null to the non-binary treatment case and deriving novel formulas for them. Our theoretical results showed the importance of appropriately incorporating the treatment dose into a test, which is an intrinsic distinction with the binary treatment case. Third, for testing Neyman's weak null (i.e., null sample average treatment effect), we propose the first valid sensitivity analysis procedure for matching with treatment dose through generalizing an existing optimization-based sensitivity analysis for the binary treatment case, built on the generalized Rosenbaum sensitivity bounds and large-scale mixed integer programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14152v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Convergence of Empirical Optimal Transport in Unbounded Settings</title>
      <link>https://arxiv.org/abs/2306.11499</link>
      <description>arXiv:2306.11499v2 Announce Type: replace 
Abstract: In compact settings, the convergence rate of the empirical optimal transport cost to its population value is well understood for a wide class of spaces and cost functions. In unbounded settings, however, hitherto available results require strong assumptions on the ground costs and the concentration of the involved measures. In this work, we pursue a decomposition-based approach to generalize the convergence rates found in compact spaces to unbounded settings under generic moment assumptions that are sharp up to an arbitrarily small $\epsilon &gt; 0$. Hallmark properties of empirical optimal transport on compact spaces, like the recently established adaptation to lower complexity, are shown to carry over to the unbounded case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11499v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Staudt, Shayan Hundrieser</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic statistical test of the diffusion coefficient of stochastic differential equations</title>
      <link>https://arxiv.org/abs/2307.10888</link>
      <description>arXiv:2307.10888v2 Announce Type: replace 
Abstract: We develop several statistical tests of the determinant of the diffusion coefficient of a stochastic differential equation, based on discrete observations on a time interval $[0,T]$ sampled with a time step $\Delta$. Our main contribution is to control the test Type I and Type II errors in a non asymptotic setting, i.e. when the number of observations and the time step are fixed. The test statistics are calculated from the process increments. In dimension 1, the density of the test statistic is explicit. In dimension 2, the test statistic has no explicit density but upper and lower bounds are proved. We also propose a multiple testing procedure in dimension greater than 2. Every test is proved to be of a given non-asymptotic level and separability conditions to control their power are also provided. A numerical study illustrates the properties of the tests for stochastic processes with known or estimated drifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10888v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Melnykova, Patricia Reynaud-Bouret, Adeline Samson</dc:creator>
    </item>
    <item>
      <title>On the consistency of supervised learning with missing values</title>
      <link>https://arxiv.org/abs/1902.06931</link>
      <description>arXiv:1902.06931v5 Announce Type: replace-cross 
Abstract: In many application settings, the data have missing entries which make analysis challenging. An abundant literature addresses missing values in an inferential framework: estimating parameters and their variance from incomplete tables. Here, we consider supervised-learning settings: predicting a target when missing values appear in both training and testing data. We show the consistency of two approaches in prediction. A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative. This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data, through multiple imputation. Finally, to compare imputation with learning directly with a model that accounts for missing values, we analyze further decision trees. These can naturally tackle empirical risk minimization with missing values, due to their ability to handle the half-discrete nature of incomplete variables. After comparing theoretically and empirically different missing values strategies in trees, we recommend using the "missing incorporated in attribute" method as it can handle both non-informative and informative missing values.</description>
      <guid isPermaLink="false">oai:arXiv.org:1902.06931v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julie Josse (XPOP, CMAP), Jacob M. Chen (CMAP, XPOP, PARIETAL), Nicolas Prost (CMAP, XPOP, PARIETAL), Erwan Scornet (X, CMAP, SU), Ga\"el Varoquaux (PARIETAL)</dc:creator>
    </item>
    <item>
      <title>Confidences in Hypotheses</title>
      <link>https://arxiv.org/abs/2111.10715</link>
      <description>arXiv:2111.10715v4 Announce Type: replace-cross 
Abstract: This article outlines a broadly-applicable new method of statistical analysis for situations involving two competing hypotheses. Hypotheses assessment is a frequentist procedure designed to answer the question: Given the sample evidence (and assumed model), what is the relative plausibility of each hypothesis? Our aim is to determine frequentist confidences in the hypotheses that are relevant to the data at hand and are as powerful as the particular application allows. Hypotheses assessments complement significance tests because providing confidences in the hypotheses in addition to test results can better inform applied researchers about the strength of evidence provided by the data. For simple hypotheses, the method produces minimum and maximum confidences in each hypothesis. The composite case is more complex, and we introduce two conventions to aid with understanding the strength of evidence. Assessments are qualitatively different from significance test and confidence interval outcomes, and thus fill a gap in the statistician's toolkit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.10715v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Graham N. Bornholt</dc:creator>
    </item>
    <item>
      <title>Identification and Statistical Decision Theory</title>
      <link>https://arxiv.org/abs/2204.11318</link>
      <description>arXiv:2204.11318v2 Announce Type: replace-cross 
Abstract: Econometricians have usefully separated study of estimation into identification and statistical components. Identification analysis, which assumes knowledge of the probability distribution generating observable data, places an upper bound on what may be learned about population parameters of interest with finite sample data. Yet Wald's statistical decision theory studies decision making with sample data without reference to identification, indeed without reference to estimation. This paper asks if identification analysis is useful to statistical decision theory. The answer is positive, as it can yield an informative and tractable upper bound on the achievable finite sample performance of decision criteria. The reasoning is simple when the decision relevant parameter is point identified. It is more delicate when the true state is partially identified and a decision must be made under ambiguity. Then the performance of some criteria, such as minimax regret, is enhanced by randomizing choice of an action. This may be accomplished by making choice a function of sample data. I find it useful to recast choice of a statistical decision function as selection of choice probabilities for the elements of the choice set. Using sample data to randomize choice conceptually differs from and is complementary to its traditional use to estimate population parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.11318v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles F. Manski</dc:creator>
    </item>
    <item>
      <title>Instance-dependent uniform tail bounds for empirical processes</title>
      <link>https://arxiv.org/abs/2209.10053</link>
      <description>arXiv:2209.10053v5 Announce Type: replace-cross 
Abstract: We formulate a uniform tail bound for empirical processes indexed by a class of functions, in terms of the individual deviations of the functions rather than the worst-case deviation in the considered class. The tail bound is established by introducing an initial "deflation" step to the standard generic chaining argument. The resulting tail bound is the sum of the complexity of the "deflated function class" in terms of a generalization of Talagrand's $\gamma$ functional, and the deviation of the function instance, both of which are formulated based on the natural seminorm induced by the corresponding Cram\'{e}r functions. We also provide certain approximations for the mentioned seminorm when the function class lies in a given (exponential type) Orlicz space, that can be used to make the complexity term and the deviation term more explicit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.10053v5</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sohail Bahmani</dc:creator>
    </item>
    <item>
      <title>Forster-Warmuth Counterfactual Regression: A Unified Learning Approach</title>
      <link>https://arxiv.org/abs/2307.16798</link>
      <description>arXiv:2307.16798v4 Announce Type: replace-cross 
Abstract: Series or orthogonal basis regression is one of the most popular non-parametric regression techniques in practice, obtained by regressing the response on features generated by evaluating the basis functions at observed covariate values. The most routinely used series estimator is based on ordinary least squares fitting, which is known to be minimax rate optimal in various settings, albeit under stringent restrictions on the basis functions and the distribution of covariates. In this work, inspired by the recently developed Forster-Warmuth (FW) learner, we propose an alternative series regression estimator that can attain the minimax estimation rate under strictly weaker conditions imposed on the basis functions and the joint law of covariates, than existing series estimators in the literature. Moreover, a key contribution of this work generalizes the FW-learner to a so-called counterfactual regression problem, in which the response variable of interest may not be directly observed (hence, the name ``counterfactual'') on all sampled units, and therefore needs to be inferred in order to identify and estimate the regression in view from the observed data. Although counterfactual regression is not entirely a new area of inquiry, we propose the first-ever systematic study of this challenging problem from a unified pseudo-outcome perspective. In fact, we provide what appears to be the first generic and constructive approach for generating the pseudo-outcome (to substitute for the unobserved response) which leads to the estimation of the counterfactual regression curve of interest with small bias, namely bias of second order. Several applications are used to illustrate the resulting FW-learner including many nonparametric regression problems in missing data and causal inference literature, for which we establish high-level conditions for minimax rate optimality of the proposed FW-learner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16798v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yachong Yang, Arun Kumar Kuchibhotla, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Weighted least-squares approximation with determinantal point processes and generalized volume sampling</title>
      <link>https://arxiv.org/abs/2312.14057</link>
      <description>arXiv:2312.14057v3 Announce Type: replace-cross 
Abstract: We consider the problem of approximating a function from $L^2$ by an element of a given $m$-dimensional space $V_m$, associated with some feature map $\varphi$, using evaluations of the function at random points $x_1,\dots,x_n$. After recalling some results on optimal weighted least-squares using independent and identically distributed points, we consider weighted least-squares using projection determinantal point processes (DPP) or volume sampling. These distributions introduce dependence between the points that promotes diversity in the selected features $\varphi(x_i)$. We first provide a generalized version of volume-rescaled sampling yielding quasi-optimality results in expectation with a number of samples $n = O(m\log(m))$, that means that the expected $L^2$ error is bounded by a constant times the best approximation error in $L^2$. Also, further assuming that the function is in some normed vector space $H$ continuously embedded in $L^2$, we further prove that the approximation is almost surely bounded by the best approximation error measured in the $H$-norm. This includes the cases of functions from $L^\infty$ or reproducing kernel Hilbert spaces. Finally, we present an alternative strategy consisting in using independent repetitions of projection DPP (or volume sampling), yielding similar error bounds as with i.i.d. or volume sampling, but in practice with a much lower number of samples. Numerical experiments illustrate the performance of the different strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14057v3</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony Nouy, Bertrand Michel</dc:creator>
    </item>
  </channel>
</rss>
