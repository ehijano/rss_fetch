<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Jun 2024 01:49:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Berry-Esseen theorem for incomplete U-statistics with Bernoulli sampling</title>
      <link>https://arxiv.org/abs/2406.05394</link>
      <description>arXiv:2406.05394v1 Announce Type: new 
Abstract: There has been a resurgence of interest in the asymptotic normality of incomplete U-statistics that only sum over roughly as many kernel evaluations as there are data samples, due to its computational efficiency and usefulness in quantifying the uncertainty for ensemble-based predictions. In this paper, we focus on the normal convergence of one such construction, the incomplete U-statistic with Bernoulli sampling, based on a raw sample of size $n$ and a computational budget $N$ in the same order as $n$. Under a minimalistic third moment assumption on the kernel, we offer an accompanying Berry-Esseen bound of the natural rate $1/\sqrt{\min(N, n)}$ that characterizes the normal approximating accuracy involved. Our key techniques include Stein's method specialized for the so-called Studentized nonlinear statistics, and an exponential lower tail bound for non-negative kernel U-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05394v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dennis Leung</dc:creator>
    </item>
    <item>
      <title>CLT for Generalized Linear Spectral Statistics of High-dimensional Sample Covariance Matrices and Applications</title>
      <link>https://arxiv.org/abs/2406.05811</link>
      <description>arXiv:2406.05811v1 Announce Type: new 
Abstract: In this paper, we introduce the $\mathbf{G}$eneralized $\mathbf{L}$inear $\mathbf{S}$pectral $\mathbf{S}$tatistics (GLSS) of a high-dimensional sample covariance matrix $\mathbf{S}_n$, denoted as $\operatorname{tr}f(\mathbf{S}_n)\mathbf{B}_n$, which effectively captures distinct spectral properties of $\mathbf{S}_n$ by involving an ancillary matrix $\mathbf{B}_n$ and a test function $f$. The joint asymptotic normality of GLSS associated with different test functions is established under weak assumptions on $\mathbf{B}_n$ and the underlying distribution, when the dimension $n$ and sample size $N$ are comparable. Specifically, we allow the rank of $\mathbf{B}_n$ to diverge with $n$. The convergence rate of GLSS is determined by $\sqrt{{N}/{\operatorname{rank}(\mathbf{B}_n)}}$. As a natural application, we propose a novel approach based on GLSS for hypothesis testing on eigenspaces of spiked covariance matrices. The theoretical accuracy of the results established for GLSS and the advantages of the newly suggested testing procedure are demonstrated through various numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05811v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanlin Hu, Qing Yang, Xiao Han</dc:creator>
    </item>
    <item>
      <title>Stochastic ordering of series and parallel systems lifetime in Archimedean copula under random shock</title>
      <link>https://arxiv.org/abs/2406.05834</link>
      <description>arXiv:2406.05834v1 Announce Type: new 
Abstract: In this manuscript, we studied the stochastic ordering behavior of series as well as parallel systems lifetimes comprising dependent and heterogeneous components, experiencing random shocks, and exhibiting distinct dependency structures. We establish certain conditions for the lifetime of individual components, the dependency among components defined by Archimedean copulas, and the impact of random shocks on the overall system lifetime to reach the conclusion. We consider components whose survival functions are either increasing log-concave or decreasing log-convex functions of the parameters involved and systems exhibit different dependency structures. These conditions make it possible to compare the lifetimes of two systems using the usual stochastic order framework. Additionally, we provide examples and graphical representations to elucidate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05834v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarikul Islam, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>The Concept of Statistical Evidence: Historical Roots and Current Developments</title>
      <link>https://arxiv.org/abs/2406.05843</link>
      <description>arXiv:2406.05843v1 Announce Type: new 
Abstract: One can argue that one of the main roles of the subject of statistics is to characterize what the evidence in collected data says about questions of scientific interest. There are two broad questions that we will refer to as the estimation question and the hypothesis assessment question. For estimation, the evidence in the data should determine a particular value of an object of interest together with a measure of the accuracy of the estimate, while for hypothesis assessment, the evidence in the data should provide evidence in favor of or against some hypothesized value of the object of interest together with a measure of the strength of the evidence. This will be referred to as the evidential approach to statistical reasoning which can be contrasted with the behavioristic or decision-theoretic approach where the notion of loss is introduced and the goal is to minimize expected losses. While the two approaches often lead to similar outcomes, this is not always the case and it is commonly argued that the evidential approach is more suited to scientific applications. This paper traces the history of the evidential approach and summarizes current developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05843v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Evans</dc:creator>
    </item>
    <item>
      <title>Some facts about the optimality of the LSE in the Gaussian sequence model with convex constraint</title>
      <link>https://arxiv.org/abs/2406.05911</link>
      <description>arXiv:2406.05911v1 Announce Type: new 
Abstract: We consider a convex constrained Gaussian sequence model and characterize necessary and sufficient conditions for the least squares estimator (LSE) to be optimal in a minimax sense. For a closed convex set $K\subset \mathbb{R}^n$ we observe $Y=\mu+\xi$ for $\xi\sim N(0,\sigma^2\mathbb{I}_n)$ and $\mu\in K$ and aim to estimate $\mu$. We characterize the worst case risk of the LSE in multiple ways by analyzing the behavior of the local Gaussian width on $K$. We demonstrate that optimality is equivalent to a Lipschitz property of the local Gaussian width mapping. We also provide theoretical algorithms that search for the worst case risk. We then provide examples showing optimality or suboptimality of the LSE on various sets, including $\ell_p$ balls for $p\in[1,2]$, pyramids, solids of revolution, and multivariate isotonic regression, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05911v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshay Prasadan, Matey Neykov</dc:creator>
    </item>
    <item>
      <title>Network two-sample test for block models</title>
      <link>https://arxiv.org/abs/2406.06014</link>
      <description>arXiv:2406.06014v1 Announce Type: new 
Abstract: We consider the two-sample testing problem for networks, where the goal is to determine whether two sets of networks originated from the same stochastic model. Assuming no vertex correspondence and allowing for different numbers of nodes, we address a fundamental network testing problem that goes beyond simple adjacency matrix comparisons. We adopt the stochastic block model (SBM) for network distributions, due to their interpretability and the potential to approximate more general models. The lack of meaningful node labels and vertex correspondence translate to a graph matching challenge when developing a test for SBMs. We introduce an efficient algorithm to match estimated network parameters, allowing us to properly combine and contrast information within and across samples, leading to a powerful test. We show that the matching algorithm, and the overall test are consistent, under mild conditions on the sparsity of the networks and the sample sizes, and derive a chi-squared asymptotic null distribution for the test. Through a mixture of theoretical insights and empirical validations, including experiments with both synthetic and real-world data, this study advances robust statistical inference for complex network data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06014v1</guid>
      <category>math.ST</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chung Kyong Nguen, Oscar Hernan Madrid Padilla, Arash A. Amini</dc:creator>
    </item>
    <item>
      <title>Heterogeneous extremes in the presence of random covariates and censoring</title>
      <link>https://arxiv.org/abs/2406.06113</link>
      <description>arXiv:2406.06113v1 Announce Type: new 
Abstract: The task of analyzing extreme events with censoring effects is considered under a framework allowing for random covariate information. A wide class of estimators that can be cast as product-limit integrals is considered, for when the conditional distributions belong to the Frechet max-domain of attraction. The main mathematical contribution is establishing uniform conditions on the families of the regularly varying tails for which the asymptotic behaviour of the resulting estimators is tractable. In particular, a decomposition of the integral estimators in terms of exchangeable sums is provided, which leads to a law of large numbers and several central limit theorems. Subsequently, the finite-sample behaviour of the estimators is explored through a simulation study, and through the analysis of two real-life datasets. In particular, the inclusion of covariates makes the model significantly versatile and, as a consequence, practically relevant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06113v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Christoffer {\O}hlenschl{\ae}ger</dc:creator>
    </item>
    <item>
      <title>Matrix norm shrinkage estimators and priors</title>
      <link>https://arxiv.org/abs/2406.06137</link>
      <description>arXiv:2406.06137v1 Announce Type: new 
Abstract: We develop a class of minimax estimators for a normal mean matrix under the Frobenius loss, which generalizes the James--Stein and Efron--Morris estimators. It shrinks the Schatten norm towards zero and works well for low-rank matrices. We also propose a class of superharmonic priors based on the Schatten norm, which generalizes Stein's prior and the singular value shrinkage prior. The generalized Bayes estimators and Bayesian predictive densities with respect to these priors are minimax. We examine the performance of the proposed estimators and priors in simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06137v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiao Li, Takeru Matsuda, Fumiyasu Komaki</dc:creator>
    </item>
    <item>
      <title>Topological Analysis for Detecting Anomalies (TADA) in Time Series</title>
      <link>https://arxiv.org/abs/2406.06168</link>
      <description>arXiv:2406.06168v1 Announce Type: new 
Abstract: This paper introduces new methodology based on the field of Topological Data Analysis for detecting anomalies in multivariate time series, that aims to detect global changes in the dependency structure between channels. The proposed approach is lean enough to handle large scale datasets, and extensive numerical experiments back the intuition that it is more suitable for detecting global changes of correlation structures than existing methods. Some theoretical guarantees for quantization algorithms based on dependent time sequences are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06168v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Chazal (DATASHAPE), Martin Royer (DATASHAPE), Cl\'ement Levrard (UR)</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Privatized Data with Unknown Sample Size</title>
      <link>https://arxiv.org/abs/2406.06231</link>
      <description>arXiv:2406.06231v1 Announce Type: new 
Abstract: We develop both theory and algorithms to analyze privatized data in the unbounded differential privacy(DP), where even the sample size is considered a sensitive quantity that requires privacy protection. We show that the distance between the sampling distributions under unbounded DP and bounded DP goes to zero as the sample size $n$ goes to infinity, provided that the noise used to privatize $n$ is at an appropriate rate; we also establish that ABC-type posterior distributions converge under similar assumptions. We further give asymptotic results in the regime where the privacy budget for $n$ goes to zero, establishing similarity of sampling distributions as well as showing that the MLE in the unbounded setting converges to the bounded-DP MLE. In order to facilitate valid, finite-sample Bayesian inference on privatized data in the unbounded DP setting, we propose a reversible jump MCMC algorithm which extends the data augmentation MCMC of Ju et al. (2022). We also propose a Monte Carlo EM algorithm to compute the MLE from privatized data in both bounded and unbounded DP. We apply our methodology to analyze a linear regression model as well as a 2019 American Time Use Survey Microdata File which we model using a Dirichlet distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06231v1</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Andres Felipe Barrientos, Nianqiao Ju</dc:creator>
    </item>
    <item>
      <title>Nodewise Loreg: Nodewise $L_0$-penalized Regression for High-dimensional Sparse Precision Matrix Estimation</title>
      <link>https://arxiv.org/abs/2406.06481</link>
      <description>arXiv:2406.06481v1 Announce Type: new 
Abstract: We propose Nodewise Loreg, a nodewise $L_0$-penalized regression method for estimating high-dimensional sparse precision matrices. We establish its asymptotic properties, including convergence rates, support recovery, and asymptotic normality under high-dimensional sub-Gaussian settings. Notably, the Nodewise Loreg estimator is asymptotically unbiased and normally distributed, eliminating the need for debiasing required by Nodewise Lasso. We also develop a desparsified version of Nodewise Loreg, similar to the desparsified Nodewise Lasso estimator. The asymptotic variances of the undesparsified Nodewise Loreg estimator are upper bounded by those of both desparsified Nodewise Loreg and Lasso estimators for Gaussian data, potentially offering more powerful statistical inference. Extensive simulations show that the undesparsified Nodewise Loreg estimator generally outperforms the two desparsified estimators in asymptotic normal behavior. Moreover, Nodewise Loreg surpasses Nodewise Lasso, CLIME, and GLasso in most simulations in terms of matrix norm losses, support recovery, and timing performance. Application to a breast cancer gene expression dataset further demonstrates Nodewise Loreg's superiority over the three $L_1$-norm based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06481v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hai Shu, Ziqi Chen, Yingjie Zhang, Hongtu Zhu</dc:creator>
    </item>
    <item>
      <title>Planning for Gold: Sample Splitting for Valid Powerful Design of Observational Studies</title>
      <link>https://arxiv.org/abs/2406.00866</link>
      <description>arXiv:2406.00866v1 Announce Type: cross 
Abstract: Observational studies are valuable tools for inferring causal effects in the absence of controlled experiments. However, these studies may be biased due to the presence of some relevant, unmeasured set of covariates. The design of an observational study has a prominent effect on its sensitivity to hidden biases, and the best design may not be apparent without examining the data. One approach to facilitate a data-inspired design is to split the sample into a planning sample for choosing the design and an analysis sample for making inferences. We devise a powerful and flexible method for selecting outcomes in the planning sample when an unknown number of outcomes are affected by the treatment. We investigate the theoretical properties of our method and conduct extensive simulations that demonstrate pronounced benefits, especially at higher levels of allowance for unmeasured confounding. Finally, we demonstrate our method in an observational study of the multi-dimensional impacts of a devastating flood in Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00866v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Bekerman, Abhinandan Dalal, Carlo del Ninno, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Thresholds for the Alignments of Partially Correlated Graphs</title>
      <link>https://arxiv.org/abs/2406.05428</link>
      <description>arXiv:2406.05428v1 Announce Type: cross 
Abstract: This paper studies the problem of recovering the hidden vertex correspondence between two correlated random graphs. We propose the partially correlated Erd\H{o}s-R\'enyi graphs model, wherein a pair of induced subgraphs with a certain number are correlated. We investigate the information-theoretic thresholds for recovering the latent correlated subgraphs and the hidden vertex correspondence. We prove that there exists an optimal rate for partial recovery for the number of correlated nodes, above which one can correctly match a fraction of vertices and below which correctly matching any positive fraction is impossible, and we also derive an optimal rate for exact recovery. In the proof of possibility results, we propose correlated functional digraphs, which partition the edges of the intersection graph into two types of components, and bound the error probability by lower-order cumulant generating functions. The proof of impossibility results build upon the generalized Fano's inequality and the recovery thresholds settled in correlated Erd\H{o}s-R\'enyi graphs model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05428v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dong Huang, Xianwen Song, Pengkun Yang</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Identification of Linear Regression Models with Residual-Permuted Sums</title>
      <link>https://arxiv.org/abs/2406.05440</link>
      <description>arXiv:2406.05440v1 Announce Type: cross 
Abstract: This letter studies a distribution-free, finite-sample data perturbation (DP) method, the Residual-Permuted Sums (RPS), which is an alternative of the Sign-Perturbed Sums (SPS) algorithm, to construct confidence regions. While SPS assumes independent (but potentially time-varying) noise terms which are symmetric about zero, RPS gets rid of the symmetricity assumption, but assumes i.i.d. noises. The main idea is that RPS permutes the residuals instead of perturbing their signs. This letter introduces RPS in a flexible way, which allows various design-choices. RPS has exact finite sample coverage probabilities and we provide the first proof that these permutation-based confidence regions are uniformly strongly consistent under general assumptions. This means that the RPS regions almost surely shrink around the true parameters as the sample size increases. The ellipsoidal outer-approximation (EOA) of SPS is also extended to RPS, and the effectiveness of RPS is validated by numerical experiments, as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05440v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Szabolcs Szentp\'eteri, Bal\'azs Csan\'ad Cs\'aji</dc:creator>
    </item>
    <item>
      <title>Causal Interpretation of Regressions With Ranks</title>
      <link>https://arxiv.org/abs/2406.05548</link>
      <description>arXiv:2406.05548v1 Announce Type: cross 
Abstract: In studies of educational production functions or intergenerational mobility, it is common to transform the key variables into percentile ranks. Yet, it remains unclear what the regression coefficient estimates with ranks of the outcome or the treatment. In this paper, we derive effective causal estimands for a broad class of commonly-used regression methods, including the ordinary least squares (OLS), two-stage least squares (2SLS), difference-in-differences (DiD), and regression discontinuity designs (RDD). Specifically, we introduce a novel primitive causal estimand, the Rank Average Treatment Effect (rank-ATE), and prove that it serves as the building block of the effective estimands of all the aforementioned econometrics methods. For 2SLS, DiD, and RDD, we show that direct applications to outcome ranks identify parameters that are difficult to interpret. To address this issue, we develop alternative methods to identify more interpretable causal parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05548v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lihua Lei</dc:creator>
    </item>
    <item>
      <title>Contextual Continuum Bandits: Static Versus Dynamic Regret</title>
      <link>https://arxiv.org/abs/2406.05714</link>
      <description>arXiv:2406.05714v1 Announce Type: cross 
Abstract: We study the contextual continuum bandits problem, where the learner sequentially receives a side information vector and has to choose an action in a convex set, minimizing a function associated to the context. The goal is to minimize all the underlying functions for the received contexts, leading to a dynamic (contextual) notion of regret, which is stronger than the standard static regret. Assuming that the objective functions are H\"older with respect to the contexts, we demonstrate that any algorithm achieving a sub-linear static regret can be extended to achieve a sub-linear dynamic regret. We further study the case of strongly convex and smooth functions when the observations are noisy. Inspired by the interior point method and employing self-concordant barriers, we propose an algorithm achieving a sub-linear dynamic regret. Lastly, we present a minimax lower bound, implying two key facts. First, no algorithm can achieve sub-linear dynamic regret over functions that are not continuous with respect to the context. Second, for strongly convex and smooth functions, the algorithm that we propose achieves, up to a logarithmic factor, the minimax optimal rate of dynamic regret as a function of the number of queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05714v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arya Akhavan, Karim Lounici, Massimiliano Pontil, Alexandre B. Tsybakov</dc:creator>
    </item>
    <item>
      <title>An Analysis of Elo Rating Systems via Markov Chains</title>
      <link>https://arxiv.org/abs/2406.05869</link>
      <description>arXiv:2406.05869v1 Announce Type: cross 
Abstract: We present a theoretical analysis of the Elo rating system, a popular method for ranking skills of players in an online setting. In particular, we study Elo under the Bradley--Terry--Luce model and, using techniques from Markov chain theory, show that Elo learns the model parameters at a rate competitive with the state of the art. We apply our results to the problem of efficient tournament design and discuss a connection with the fastest-mixing Markov chain problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05869v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Olesker-Taylor, Luca Zanetti</dc:creator>
    </item>
    <item>
      <title>Embedding Network Autoregression for time series analysis and causal peer effect inference</title>
      <link>https://arxiv.org/abs/2406.05944</link>
      <description>arXiv:2406.05944v1 Announce Type: cross 
Abstract: We propose an Embedding Network Autoregressive Model (ENAR) for multivariate networked longitudinal data. We assume the network is generated from a latent variable model, and these unobserved variables are included in a structural peer effect model or a time series network autoregressive model as additive effects. This approach takes a unified view of two related problems, (1) modeling and predicting multivariate time series data and (2) causal peer influence estimation in the presence of homophily from finite time longitudinal data. Our estimation strategy comprises estimating latent factors from the observed network adjacency matrix either through spectral embedding or maximum likelihood estimation, followed by least squares estimation of the network autoregressive model. We show that the estimated momentum and peer effect parameters are consistent and asymptotically normal in asymptotic setups with a growing number of network vertices N while including a growing number of time points T and finite T cases. We allow the number of latent vectors K to grow at appropriate rates, which improves upon existing rates when such results are available for related models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05944v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jae Ho Chang, Subhadeep Paul</dc:creator>
    </item>
    <item>
      <title>Unified Fourier bases for GSP on stochastic block model graphs</title>
      <link>https://arxiv.org/abs/2406.06306</link>
      <description>arXiv:2406.06306v1 Announce Type: cross 
Abstract: We consider a recently proposed approach to graph signal processing based on graphons. We show how the graphon-based approach to GSP applies to graphs sampled from a stochastic block model. We obtain a basis for the graphon Fourier transform on such samples directly from the link probability matrix and the block sizes of the model. This formulation allows us to bound the sensitivity of the Fourier transform to small changes in block sizes. We then focus on the case where the probability matrix corresponds to a (weighted) Cayley graph. If block sizes are equal, a nice Fourier basis can be derived from the underlying group. We explore how, in the case where block sizes are not equal, some or all nice properties of the group basis can be maintained. We complement the theoretical results with simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06306v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahya Ghandehari, Jeannette Janssen, Silo Murphy</dc:creator>
    </item>
    <item>
      <title>Differentially Private Best-Arm Identification</title>
      <link>https://arxiv.org/abs/2406.06408</link>
      <description>arXiv:2406.06408v1 Announce Type: cross 
Abstract: Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence in both the local and central models, i.e. $\epsilon$-local and $\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive lower bounds on the sample complexity of any $\delta$-correct BAI algorithm satisfying $\epsilon$-global DP or $\epsilon$-local DP. Our lower bounds suggest the existence of two privacy regimes. In the high-privacy regime, the hardness depends on a coupled effect of privacy and novel information-theoretic quantities involving the Total Variation. In the low-privacy regime, the lower bounds reduce to the non-private lower bounds. We propose $\epsilon$-local DP and $\epsilon$-global DP variants of a Top Two algorithm, namely CTB-TT and AdaP-TT*, respectively. For $\epsilon$-local DP, CTB-TT is asymptotically optimal by plugging in a private estimator of the means based on Randomised Response. For $\epsilon$-global DP, our private estimator of the mean runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off. By adapting the transportation costs, the expected sample complexity of AdaP-TT* reaches the asymptotic lower bound up to multiplicative constants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06408v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Achraf Azize, Marc Jourdan, Aymen Al Marjani, Debabrota Basu</dc:creator>
    </item>
    <item>
      <title>Multivariate Stochastic Dominance via Optimal Transport and Applications to Models Benchmarking</title>
      <link>https://arxiv.org/abs/2406.06425</link>
      <description>arXiv:2406.06425v1 Announce Type: cross 
Abstract: Stochastic dominance is an important concept in probability theory, econometrics and social choice theory for robustly modeling agents' preferences between random outcomes. While many works have been dedicated to the univariate case, little has been done in the multivariate scenario, wherein an agent has to decide between different multivariate outcomes. By exploiting a characterization of multivariate first stochastic dominance in terms of couplings, we introduce a statistic that assesses multivariate almost stochastic dominance under the framework of Optimal Transport with a smooth cost. Further, we introduce an entropic regularization of this statistic, and establish a central limit theorem (CLT) and consistency of the bootstrap procedure for the empirical statistic. Armed with this CLT, we propose a hypothesis testing framework as well as an efficient implementation using the Sinkhorn algorithm. We showcase our method in comparing and benchmarking Large Language Models that are evaluated on multiple metrics. Our multivariate stochastic dominance test allows us to capture the dependencies between the metrics in order to make an informed and statistically significant decision on the relative performance of the models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06425v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Rioux, Apoorva Nitsure, Mattia Rigotti, Kristjan Greenewald, Youssef Mroueh</dc:creator>
    </item>
    <item>
      <title>On high-dimensional wavelet eigenanalysis</title>
      <link>https://arxiv.org/abs/2102.05761</link>
      <description>arXiv:2102.05761v5 Announce Type: replace 
Abstract: In this paper, we characterize the asymptotic and large scale behavior of the eigenvalues of wavelet random matrices in high dimensions. We assume that possibly non-Gaussian, finite-variance $p$-variate measurements are made of a low-dimensional $r$-variate ($r \ll p$) fractional stochastic process with non-canonical scaling coordinates and in the presence of additive high-dimensional noise. The measurements are correlated both time-wise and between rows. We show that the $r$ largest eigenvalues of the wavelet random matrices, when appropriately rescaled, converge in probability to scale-invariant functions in the high-dimensional limit. By contrast, the remaining $p-r$ eigenvalues remain bounded in probability. Under additional assumptions, we show that the $r$ largest log-eigenvalues of wavelet random matrices exhibit asymptotically Gaussian distributions. The results have direct consequences for statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.05761v5</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrice Abry, B. Cooper Boniece, Gustavo Didier, Herwig Wendt</dc:creator>
    </item>
    <item>
      <title>Adaptive estimation of irregular mean and covariance functions</title>
      <link>https://arxiv.org/abs/2108.06507</link>
      <description>arXiv:2108.06507v3 Announce Type: replace 
Abstract: Nonparametric estimators for the mean and the covariance functions of functional data are proposed. The setup covers a wide range of practical situations. The random trajectories are, not necessarily differentiable, have unknown regularity, and are measured with error at discrete design points. The measurement error could be heteroscedastic. The design points could be either randomly drawn or common for all curves. The estimators depend on the local regularity of the stochastic process generating the functional data. We consider a simple estimator of this local regularity which exploits the replication and regularization features of functional data. Next, we use the ``smoothing first, then estimate'' approach for the mean and the covariance functions. They can be applied with both sparsely or densely sampled curves, are easy to calculate and to update, and perform well in simulations. Simulations built upon an example of real data set, illustrate the effectiveness of the new approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.06507v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Golovkine, Nicolas Klutchnikoff, Valentin Patilea</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit test for count distributions with finite second moment</title>
      <link>https://arxiv.org/abs/2112.11861</link>
      <description>arXiv:2112.11861v4 Announce Type: replace 
Abstract: A goodness-of-fit test for one-parameter count distributions with finite second moment is proposed. The test statistic is derived from the $L^1$ distance of a function of the probability generating function of the model under the null hypothesis and that of the random variable actually generating data, when the latter belongs to a suitable wide class of alternatives. The test statistic has a rather simple form and it is asymptotically normally distributed under the null hypothesis, allowing a straightforward implementation of the test. Moreover, the test is consistent for alternative distributions belonging to the class, but also for all the alternative distributions whose probability of zero is different from that under the null hypothesis. Thus, the use of the test is proposed and investigated also for alternatives not in the class. The finite-sample properties of the test are assessed by means of an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.11861v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10485252.2022.2137728</arxiv:DOI>
      <dc:creator>Antonio Di Noia, Lucio Barabesi, Marzia Marcheselli, Caterina Pisani, Luca Pratelli</dc:creator>
    </item>
    <item>
      <title>A family of consistent normally distributed tests for Poissonity</title>
      <link>https://arxiv.org/abs/2112.12201</link>
      <description>arXiv:2112.12201v4 Announce Type: replace 
Abstract: A family of consistent tests, derived from a characterization of the probability generating function, is proposed for assessing Poissonity against a wide class of count distributions, which includes some of the most frequently adopted alternatives to the Poisson distribution. Actually, the family of test statistics is based on the difference between the plug-in estimator of the Poisson cumulative distribution function and the empirical cumulative distribution function. The test statistics have an intuitive and simple form and are asymptotically normally distributed, allowing a straightforward implementation of the test. The finite sample properties of the test are investigated by means of an extensive simulation study. The test shows satisfactory behaviour compared to other tests with known limit distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.12201v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10182-023-00478-8</arxiv:DOI>
      <dc:creator>Antonio Di Noia, Marzia Marcheselli, Caterina Pisani, Luca Pratelli</dc:creator>
    </item>
    <item>
      <title>Censoring heavy-tail count distributions for parameter estimation with an application to stable distributions</title>
      <link>https://arxiv.org/abs/2212.11697</link>
      <description>arXiv:2212.11697v2 Announce Type: replace 
Abstract: A new approach based on censoring and moment criterion is introduced for parameter estimation of count distributions when the probability generating function is available even though a closed form of the probability mass function and/or finite moments do not exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.11697v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spl.2023.109903</arxiv:DOI>
      <dc:creator>Antonio Di Noia, Marzia Marcheselli, Caterina Pisani, Luca Pratelli</dc:creator>
    </item>
    <item>
      <title>Unbiased likelihood estimation of Wright-Fisher diffusion processes</title>
      <link>https://arxiv.org/abs/2303.05390</link>
      <description>arXiv:2303.05390v2 Announce Type: replace 
Abstract: In this paper we propose a Monte Carlo maximum likelihood estimation strategy for discretely observed Wright-Fisher diffusions. Our approach provides an unbiased estimator of the likelihood function and is based on exact simulation techniques that are of special interest for diffusion processes defined on a bounded domain, where numerical methods typically fail to remain within the required boundaries. We start by building unbiased likelihood estimators for scalar diffusions and later present an extension to the multidimensional case. Consistency results of our proposed estimator are also presented and the performance of our method is illustrated through numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05390v2</guid>
      <category>math.ST</category>
      <category>q-bio.PE</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Celia Garc\'ia-Pareja, Fabio Nobile</dc:creator>
    </item>
    <item>
      <title>Local approximations of inverse block Toeplitz matrices and Baxter-type theorems for long-memory processes</title>
      <link>https://arxiv.org/abs/2304.00470</link>
      <description>arXiv:2304.00470v2 Announce Type: replace 
Abstract: We derive sharp approximation error bounds for inverse block Toeplitz matrices associated with multivariate long-memory stationary processes. The error bounds are evaluated for both column and row sums. These results are used to prove the strong convergence of the solutions of general block Toeplitz systems. A crucial part of the proof is to bound sums consisting of the Fourier coefficients of the phase function attached to the singular symbol of the Toeplitz matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00470v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akihiko Inoue, Junho Yang</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes large-scale multiple testing for high-dimensional binary outcome data</title>
      <link>https://arxiv.org/abs/2307.05943</link>
      <description>arXiv:2307.05943v3 Announce Type: replace 
Abstract: This paper explores the multiple testing problem for sparse high-dimensional data with binary outcomes. We utilize the empirical Bayes posterior to construct multiple testing procedures and evaluate their performance on false discovery rate (FDR) control. We first show that the $\ell$-value (a.k.a. the local FDR) procedure can be overly conservative in estimating the FDR if choosing the conjugate spike and uniform slab prior. To address this, we propose two new procedures that calibrate the posterior to achieve correct FDR control. Sharp frequentist theoretical results are established for these procedures, and numerical experiments are conducted to validate our theory in finite samples. To the best of our knowledge, we obtain the first {\it uniform} FDR control result in multiple testing for high-dimensional data with binary outcomes under the sparsity assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05943v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yu-Chien Bo Ning</dc:creator>
    </item>
    <item>
      <title>The oracle property of the generalized outcome adaptive lasso</title>
      <link>https://arxiv.org/abs/2310.00250</link>
      <description>arXiv:2310.00250v2 Announce Type: replace 
Abstract: The generalized outcome-adaptive lasso (GOAL) is a variable selection for high-dimensional causal inference proposed by Bald\'e et al. [2023, {\em Biometrics} {\bfseries 79(1)}, 514--520]. When the dimension is high, it is now well established that an ideal variable selection method should have the oracle property to ensure the optimal large sample performance. However, the oracle property of GOAL has not been proven. In this paper, we show that the GOAL estimator enjoys the oracle property. Our simulation shows that the GOAL method deals with the collinearity problem better than the oracle-like method, the outcome-adaptive lasso (OAL).</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00250v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ismaila Bald\'e</dc:creator>
    </item>
    <item>
      <title>Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space</title>
      <link>https://arxiv.org/abs/2312.02849</link>
      <description>arXiv:2312.02849v2 Announce Type: replace 
Abstract: We develop a theory of finite-dimensional polyhedral subsets over the Wasserstein space and optimization of functionals over them via first-order methods. Our main application is to the problem of mean-field variational inference, which seeks to approximate a distribution $\pi$ over $\mathbb{R}^d$ by a product measure $\pi^\star$. When $\pi$ is strongly log-concave and log-smooth, we provide (1) approximation rates certifying that $\pi^\star$ is close to the minimizer $\pi^\star_\diamond$ of the KL divergence over a \emph{polyhedral} set $\mathcal{P}_\diamond$, and (2) an algorithm for minimizing $\text{KL}(\cdot\|\pi)$ over $\mathcal{P}_\diamond$ with accelerated complexity $O(\sqrt \kappa \log(\kappa d/\varepsilon^2))$, where $\kappa$ is the condition number of $\pi$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02849v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiheng Jiang, Sinho Chewi, Aram-Alexandre Pooladian</dc:creator>
    </item>
    <item>
      <title>Equality between two general ridge estimators and equivalence of their residual sums of squares</title>
      <link>https://arxiv.org/abs/2405.20023</link>
      <description>arXiv:2405.20023v2 Announce Type: replace 
Abstract: General ridge estimators are typical linear estimators in a general linear model. The class of them include some shrinkage estimators in addition to classical linear unbiased estimators such as the ordinary least squares estimator and the weighted least squares estimator. We derive necessary and sufficient conditions under which two general ridge estimators coincide. In particular, two noteworthy conditions are added to those from previous studies. The first condition is given as a seemingly column space relationship to the covariance matrix of the error term, and the second one is based on the biases of general ridge estimators. Another problem studied in this paper is to derive an equivalence condition such that equality between two residual sums of squares holds when general ridge estimators are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20023v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hirai Mukasa, Koji Tsukuda</dc:creator>
    </item>
    <item>
      <title>Classification of multivariate functional data on different domains with Partial Least Squares approaches</title>
      <link>https://arxiv.org/abs/2212.09145</link>
      <description>arXiv:2212.09145v3 Announce Type: replace-cross 
Abstract: Classification (supervised-learning) of multivariate functional data is considered when the elements of the random functional vector of interest are defined on different domains. In this setting, PLS classification and tree PLS-based methods for multivariate functional data are presented. From a computational point of view, we show that the PLS components of the regression with multivariate functional data can be obtained using only the PLS methodology with univariate functional data. This offers an alternative way to present the PLS algorithm for multivariate functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.09145v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Issam-Ali Moindjie, Sophie Dabo-Niang, Cristian Preda</dc:creator>
    </item>
    <item>
      <title>Efficient and Multiply Robust Risk Estimation under General Forms of Dataset Shift</title>
      <link>https://arxiv.org/abs/2306.16406</link>
      <description>arXiv:2306.16406v4 Announce Type: replace-cross 
Abstract: Statistical machine learning methods often face the challenge of limited data available from the population of interest. One remedy is to leverage data from auxiliary source populations, which share some conditional distributions or are linked in other ways with the target domain. Techniques leveraging such \emph{dataset shift} conditions are known as \emph{domain adaptation} or \emph{transfer learning}. Despite extensive literature on dataset shift, limited works address how to efficiently use the auxiliary populations to improve the accuracy of risk evaluation for a given machine learning task in the target population.
  In this paper, we study the general problem of efficiently estimating target population risk under various dataset shift conditions, leveraging semiparametric efficiency theory. We consider a general class of dataset shift conditions, which includes three popular conditions -- covariate, label and concept shift -- as special cases. We allow for partially non-overlapping support between the source and target populations. We develop efficient and multiply robust estimators along with a straightforward specification test of these dataset shift conditions. We also derive efficiency bounds for two other dataset shift conditions, posterior drift and location-scale shift. Simulation studies support the efficiency gains due to leveraging plausible dataset shift conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16406v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxiang Qiu, Eric Tchetgen Tchetgen, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Learning the Uncertainty Sets for Control Dynamics via Set Membership: A Non-Asymptotic Analysis</title>
      <link>https://arxiv.org/abs/2309.14648</link>
      <description>arXiv:2309.14648v2 Announce Type: replace-cross 
Abstract: This paper studies uncertainty set estimation for unknown linear systems. Uncertainty sets are crucial for the quality of robust control since they directly influence the conservativeness of the control design. Departing from the confidence region analysis of least squares estimation, this paper focuses on set membership estimation (SME). Though good numerical performances have attracted applications of SME in the control literature, the non-asymptotic convergence rate of SME for linear systems remains an open question. This paper provides the first convergence rate bounds for SME and discusses variations of SME under relaxed assumptions. We also provide numerical results demonstrating SME's practical promise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14648v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Li, Jing Yu, Lauren Conger, Taylan Kargin, Adam Wierman</dc:creator>
    </item>
    <item>
      <title>Risk Aware Benchmarking of Large Language Models</title>
      <link>https://arxiv.org/abs/2310.07132</link>
      <description>arXiv:2310.07132v3 Announce Type: replace-cross 
Abstract: We propose a distributional framework for benchmarking socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a metrics portfolio for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit theorems instantiated in practice via a bootstrap variance estimate. We use our framework to compare various large language models regarding risks related to drifting from instructions and outputting toxic content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07132v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apoorva Nitsure, Youssef Mroueh, Mattia Rigotti, Kristjan Greenewald, Brian Belgodere, Mikhail Yurochkin, Jiri Navratil, Igor Melnyk, Jerret Ross</dc:creator>
    </item>
    <item>
      <title>Improved Sample Complexity Bounds for Diffusion Model Training</title>
      <link>https://arxiv.org/abs/2311.13745</link>
      <description>arXiv:2311.13745v2 Announce Type: replace-cross 
Abstract: Diffusion models have become the most popular approach to deep generative modeling of images, largely due to their empirical performance and reliability. From a theoretical standpoint, a number of recent works~\cite{chen2022,chen2022improved,benton2023linear} have studied the iteration complexity of sampling, assuming access to an accurate diffusion model. In this work, we focus on understanding the \emph{sample complexity} of training such a model; how many samples are needed to learn an accurate diffusion model using a sufficiently expressive neural network? Prior work~\cite{BMR20} showed bounds polynomial in the dimension, desired Total Variation error, and Wasserstein error. We show an \emph{exponential improvement} in the dependence on Wasserstein error and depth, along with improved dependencies on other relevant parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13745v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivam Gupta, Aditya Parulekar, Eric Price, Zhiyang Xun</dc:creator>
    </item>
    <item>
      <title>Quantized Approximately Orthogonal Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2402.04012</link>
      <description>arXiv:2402.04012v2 Announce Type: replace-cross 
Abstract: In recent years, Orthogonal Recurrent Neural Networks (ORNNs) have gained popularity due to their ability to manage tasks involving long-term dependencies, such as the copy-task, and their linear complexity. However, existing ORNNs utilize full precision weights and activations, which prevents their deployment on compact devices.In this paper, we explore the quantization of the weight matrices in ORNNs, leading to Quantized approximately Orthogonal RNNs (QORNNs). The construction of such networks remained an open problem, acknowledged for its inherent instability. We propose and investigate two strategies to learn QORNN  by combining  quantization-aware training (QAT) and orthogonal projections. We also study  post-training quantization of the activations for pure integer computation of the recurrent loop. The most efficient models achieve results similar to state-of-the-art full-precision ORNN, LSTM and FastRNN on a variety of standard benchmarks, even with 4-bits quantization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04012v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Armand Foucault (IMT), Franck Mamalet (UT), Fran\c{c}ois Malgouyres (IMT)</dc:creator>
    </item>
    <item>
      <title>On Computationally Efficient Multi-Class Calibration</title>
      <link>https://arxiv.org/abs/2402.07821</link>
      <description>arXiv:2402.07821v2 Announce Type: replace-cross 
Abstract: Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$? Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.
  Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for all downstream decision makers who want to use the predictor for binary classification problems of the form: does the label belong to a subset $T \subseteq [k]$: e.g. is this an image of an animal? It ensures that the probabilities predicted by summing the probabilities assigned to labels in $T$ are close to some perfectly calibrated binary predictor for that task. We also show that natural strengthenings of our definition are computationally hard to achieve: they run into information theoretic barriers or computational intractability. Underlying both our upper and lower bounds is a tight connection that we prove between multi-class calibration and the well-studied problem of agnostic learning in the (standard) binary prediction setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07821v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parikshit Gopalan, Lunjia Hu, Guy N. Rothblum</dc:creator>
    </item>
    <item>
      <title>Asymptotics of Learning with Deep Structured (Random) Features</title>
      <link>https://arxiv.org/abs/2402.13999</link>
      <description>arXiv:2402.13999v2 Announce Type: replace-cross 
Abstract: For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large. This characterization is formulated in terms of the population covariance of the features. Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers. For such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices. We further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13999v2</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominik Schr\"oder, Daniil Dmitriev, Hugo Cui, Bruno Loureiro</dc:creator>
    </item>
    <item>
      <title>Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality</title>
      <link>https://arxiv.org/abs/2402.19442</link>
      <description>arXiv:2402.19442v2 Announce Type: replace-cross 
Abstract: We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression. We establish the global convergence of gradient flow under suitable choices of initialization. In addition, we prove that an interesting "task allocation" phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases -- a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flow is on par with the best possible multi-head softmax attention model up to a constant factor. Our analysis also delineates a strict separation in terms of the prediction accuracy of ICL between single-head and multi-head attention models. The key technique for our convergence analysis is to map the gradient flow dynamics in the parameter space to a set of ordinary differential equations in the spectral domain, where the relative magnitudes of the semi-singular values of the attention weights determines task allocation. To our best knowledge, our work provides the first convergence result for the multi-head softmax attention model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.19442v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Chen, Heejune Sheen, Tianhao Wang, Zhuoran Yang</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Cocycles</title>
      <link>https://arxiv.org/abs/2405.13844</link>
      <description>arXiv:2405.13844v2 Announce Type: replace-cross 
Abstract: Many interventions in causal inference can be represented as transformations. We identify a local symmetry property satisfied by a large class of causal models under such interventions. Where present, this symmetry can be characterized by a type of map called a cocycle, an object that is central to dynamical systems theory. We show that such cocycles exist under general conditions and are sufficient to identify interventional and counterfactual distributions. We use these results to derive cocycle-based estimators for causal estimands and show they achieve semiparametric efficiency under typical conditions. Since (infinitely) many distributions can share the same cocycle, these estimators make causal inference robust to mis-specification by sidestepping superfluous modelling assumptions. We demonstrate both robustness and state-of-the-art performance in several simulations, and apply our method to estimate the effects of 401(k) pension plan eligibility on asset accumulation using a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13844v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugh Dance, Benjamin Bloem-Reddy</dc:creator>
    </item>
    <item>
      <title>Quantumness and Randomness of Quantum Random Number Generators via Photon Statistics</title>
      <link>https://arxiv.org/abs/2405.14085</link>
      <description>arXiv:2405.14085v2 Announce Type: replace-cross 
Abstract: Several quantum random number generator (QRNG) models have been proposed to produce random numbers, which, due to the quantum theory, are more secure than their classical counterparts. Many QRNG devices are commercially available as off-the-shelf black-box devices. Computationally, it is not possible to distinguish between a PRNG and a QRNG just by observing their outputs. The common practice for testing quantumness is a direct comparison between the mean and the variance of the experimental photon count. However, statistically, this is not a feasible solution either, because of finite sample size. A QRNG that uses single photons as the quantum source produces true quantum random numbers. Since single photons follow sub-Poissonian statistics, by determining the underlying distribution one can conclude whether a QRNG is truly quantum or not. In this work, we point out the limitations of existing methods of such a decision-making processes, and propose a more efficient two-fold statistical method, which can ensure whether an optical source is quantum or not up to a desired confidence level. Also, QRNGs can not produce true random numbers without deterministic classical post-processing. In this work, we also show that the two models of QRNGs, one producing random numbers from exponential distribution and the other from uniform distribution, become essentially similar under device noise. Detector outputs of both the above models can be tested to quantify the randomness coming from the quantum source of a QRNG, which in turn, dictates how much post-processing is required to produce good random numbers retaining quantumness. In this context, we also derive a relation when the underlying sampling distributions of the QRNGs will be $\epsilon$-random. Depending on this relation, a suitable post-processing algorithm can be chosen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14085v2</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Goutam Paul, Nirupam Basak, Soumya Das</dc:creator>
    </item>
    <item>
      <title>Efficient algorithms for the sensitivities of the Pearson correlation coefficient and its statistical significance to online data</title>
      <link>https://arxiv.org/abs/2405.14686</link>
      <description>arXiv:2405.14686v3 Announce Type: replace-cross 
Abstract: Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates. Leveraging identities from Welford's online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data. Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms. Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics. Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14686v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Harary</dc:creator>
    </item>
    <item>
      <title>On the Convergence of the Sinkhorn-Knopp Algorithm with Sparse Cost Matrices</title>
      <link>https://arxiv.org/abs/2405.20528</link>
      <description>arXiv:2405.20528v2 Announce Type: replace-cross 
Abstract: This paper presents a theoretical analysis of the convergence rate of the Sinkhorn-Knopp algorithm when the cost matrix is sparse. We derive bounds on the convergence rate that depend on the sparsity pattern and the degree of nonsparsity of the cost matrix. We also explore connections to existing convergence results for dense cost matrices. Our analysis provides new insights into the behavior of the Sinkhorn-Knopp algorithm in the presence of sparsity and highlights potential avenues for algorithmic improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20528v2</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Rafael Espinosa Mena</dc:creator>
    </item>
  </channel>
</rss>
