<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 31 Jul 2024 01:45:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>MetaHive: A Cache-Optimized Metadata Management for Heterogeneous Key-Value Stores</title>
      <link>https://arxiv.org/abs/2407.19090</link>
      <description>arXiv:2407.19090v1 Announce Type: new 
Abstract: Cloud key-value (KV) stores provide businesses with a cost-effective and adaptive alternative to traditional on-premise data management solutions. KV stores frequently consist of heterogeneous clusters, characterized by varying hardware specifications of the deployment nodes, with each node potentially running a distinct version of the KV store software. This heterogeneity is accompanied by the diverse metadata that they need to manage. In this study, we introduce MetaHive, a cache-optimized approach to managing metadata in heterogeneous KV store clusters. MetaHive disaggregates the original data from its associated metadata to promote independence between them, while maintaining their interconnection during usage. This makes the metadata opaque from the downstream processes and the other KV stores in the cluster. MetaHive also ensures that the KV and metadata entries are stored in the vicinity of each other in memory and storage. This allows MetaHive to optimally utilize the caching mechanism without extra storage read overhead for metadata retrieval. We deploy MetaHive to ensure data integrity in RocksDB and demonstrate its rapid data validation with minimal effect on performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19090v1</guid>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>VLDB 2024</arxiv:journal_reference>
      <dc:creator>Alireza Heidari, Amirhossein Ahmadi, Zefeng Zhi, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Towards A More Reasonable Semantic Web</title>
      <link>https://arxiv.org/abs/2407.19095</link>
      <description>arXiv:2407.19095v1 Announce Type: new 
Abstract: We aim to accelerate the original vision of the semantic web by revisiting design decisions that have defined the semantic web up until now. We propose a shift in direction that more broadly embraces existing data infrastructure by reconsidering the semantic web's logical foundations. We argue to shift attention away from description logic, which has so far underpinned the semantic web, to a different fragment of first-order logic. We argue, using examples from the (geo)spatial domain, that by doing so, the semantic web can be approached as a traditional data migration and integration problem at a massive scale. That way, a huge amount of existing tools and theories can be deployed to the semantic web's benefit, and the original vision of ontology as shared abstraction be reinvigorated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19095v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vleer Doing, Ryan Wisnesky</dc:creator>
    </item>
    <item>
      <title>Evaluating LLMs for Text-to-SQL Generation With Complex SQL Workload</title>
      <link>https://arxiv.org/abs/2407.19517</link>
      <description>arXiv:2407.19517v1 Announce Type: new 
Abstract: This study presents a comparative analysis of the a complex SQL benchmark, TPC-DS, with two existing text-to-SQL benchmarks, BIRD and Spider. Our findings reveal that TPC-DS queries exhibit a significantly higher level of structural complexity compared to the other two benchmarks. This underscores the need for more intricate benchmarks to simulate realistic scenarios effectively. To facilitate this comparison, we devised several measures of structural complexity and applied them across all three benchmarks. The results of this study can guide future research in the development of more sophisticated text-to-SQL benchmarks.
  We utilized 11 distinct Language Models (LLMs) to generate SQL queries based on the query descriptions provided by the TPC-DS benchmark. The prompt engineering process incorporated both the query description as outlined in the TPC-DS specification and the database schema of TPC-DS. Our findings indicate that the current state-of-the-art generative AI models fall short in generating accurate decision-making queries. We conducted a comparison of the generated queries with the TPC-DS gold standard queries using a series of fuzzy structure matching techniques based on query features. The results demonstrated that the accuracy of the generated queries is insufficient for practical real-world application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19517v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Limin Ma, Ken Pu, Ying Zhu</dc:creator>
    </item>
    <item>
      <title>Urban Traffic Accident Risk Prediction Revisited: Regionality, Proximity, Similarity and Sparsity</title>
      <link>https://arxiv.org/abs/2407.19668</link>
      <description>arXiv:2407.19668v1 Announce Type: new 
Abstract: Traffic accidents pose a significant risk to human health and property safety. Therefore, to prevent traffic accidents, predicting their risks has garnered growing interest. We argue that a desired prediction solution should demonstrate resilience to the complexity of traffic accidents. In particular, it should adequately consider the regional background, accurately capture both spatial proximity and semantic similarity, and effectively address the sparsity of traffic accidents. However, these factors are often overlooked or difficult to incorporate. In this paper, we propose a novel multi-granularity hierarchical spatio-temporal network. Initially, we innovate by incorporating remote sensing data, facilitating the creation of hierarchical multi-granularity structure and the comprehension of regional background. We construct multiple high-level risk prediction tasks to enhance model's ability to cope with sparsity. Subsequently, to capture both spatial proximity and semantic similarity, region feature and multi-view graph undergo encoding processes to distill effective representations. Additionally, we propose message passing and adaptive temporal attention module that bridges different granularities and dynamically captures time correlations inherent in traffic accident patterns. At last, a multivariate hierarchical loss function is devised considering the complexity of the prediction purpose. Extensive experiments on two real datasets verify the superiority of our model against the state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19668v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3627673.3679567</arxiv:DOI>
      <dc:creator>Minxiao Chen, Haitao Yuan, Nan Jiang, Zhifeng Bao, Shangguang Wang</dc:creator>
    </item>
    <item>
      <title>Rosetta Statements: Lowering the Barrier for Semantic Parsing and Increasing the Cognitive Interoperability of Knowledge Graphs</title>
      <link>https://arxiv.org/abs/2407.20007</link>
      <description>arXiv:2407.20007v1 Announce Type: new 
Abstract: Machines need data and metadata to be machine-actionable and FAIR (findable, accessible, interoperable, reusable) to manage increasing data volumes. Knowledge graphs and ontologies are key to this, but their use is hampered by high access barriers due to required prior knowledge in semantics and data modelling. The Rosetta Statement approach proposes modeling English natural language statements instead of a mind-independent reality. We propose a metamodel for creating semantic schema patterns for simple statement types. The approach supports versioning of statements and provides a detailed editing history. Each Rosetta Statement pattern has a dynamic label for displaying statements as natural language sentences. Implemented in the Open Research Knowledge Graph (ORKG) as a use case, this approach allows domain experts to define data schema patterns without needing semantic knowledge. Future plans include combining Rosetta Statements with semantic units to organize ORKG into meaningful subgraphs, improving usability. A search interface for querying statements without needing SPARQL or Cypher knowledge is also planned, along with tools for data entry and display using Large Language Models and NLP. The Rosetta Statement metamodel supports a two-step knowledge graph construction procedure. Domain experts can model semantic content without support from ontology engineers, lowering entry barriers and increasing cognitive interoperability. The second level involves developing semantic graph patterns for reasoning, requiring collaboration with ontology engineers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20007v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars Vogt, Marcel Konrad, Kheir Eddine Farfar, Manuel Prinz, Allard Oelen</dc:creator>
    </item>
    <item>
      <title>Shapley Value Computation in Ontology-Mediated Query Answering</title>
      <link>https://arxiv.org/abs/2407.20058</link>
      <description>arXiv:2407.20058v1 Announce Type: cross 
Abstract: The Shapley value, originally introduced in cooperative game theory for wealth distribution, has found use in KR and databases for the purpose of assigning scores to formulas and database tuples based upon their contribution to obtaining a query result or inconsistency. In the present paper, we explore the use of Shapley values in ontology-mediated query answering (OMQA) and present a detailed complexity analysis of Shapley value computation (SVC) in the OMQA setting. In particular, we establish a PF/#P-hard dichotomy for SVC for ontology-mediated queries (T,q) composed of an ontology T formulated in the description logic ELHI_\bot and a connected constant-free homomorphism-closed query q. We further show that the #P-hardness side of the dichotomy can be strengthened to cover possibly disconnected queries with constants. Our results exploit recently discovered connections between SVC and probabilistic query evaluation and allow us to generalize existing results on probabilistic OMQA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20058v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meghyn Bienvenu, Diego Figueira, Pierre Lafourcade</dc:creator>
    </item>
    <item>
      <title>RelBench: A Benchmark for Deep Learning on Relational Databases</title>
      <link>https://arxiv.org/abs/2407.20060</link>
      <description>arXiv:2407.20060v1 Announce Type: cross 
Abstract: We present RelBench, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RelBench provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. We use RelBench to conduct the first comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, we conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RelBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20060v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Robinson, Rishabh Ranjan, Weihua Hu, Kexin Huang, Jiaqi Han, Alejandro Dobles, Matthias Fey, Jan E. Lenssen, Yiwen Yuan, Zecheng Zhang, Xinwei He, Jure Leskovec</dc:creator>
    </item>
    <item>
      <title>Memory-Efficient Sequential Pattern Mining with Hybrid Tries</title>
      <link>https://arxiv.org/abs/2202.06834</link>
      <description>arXiv:2202.06834v3 Announce Type: replace 
Abstract: This paper develops a memory-efficient approach for Sequential Pattern Mining (SPM), a fundamental topic in knowledge discovery that faces a well-known memory bottleneck for large data sets. Our methodology involves a novel hybrid trie data structure that exploits recurring patterns to compactly store the data set in memory; and a corresponding mining algorithm designed to effectively extract patterns from this compact representation. Numerical results on small to medium-sized real-life test instances show an average improvement of 85% in memory consumption and 49% in computation time compared to the state of the art. For large data sets, our algorithm stands out as the only capable SPM approach within 256GB of system memory, potentially saving 1.7TB in memory consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.06834v3</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amin Hosseininasab, Willem-Jan van Hoeve, Andre A. Cire</dc:creator>
    </item>
    <item>
      <title>Performance Analysis: Discovering Semi-Markov Models From Event Logs</title>
      <link>https://arxiv.org/abs/2206.14415</link>
      <description>arXiv:2206.14415v3 Announce Type: replace 
Abstract: Process mining is a well-established discipline of data analysis focused on the discovery of process models from information systems' event logs. Recently, an emerging subarea of process mining - stochastic process discovery has started to evolve. Stochastic process discovery considers frequencies of events in the event data and allows for more comprehensive analysis. In particular, when durations of activities are presented in the event log, performance characteristics of the discovered stochastic models can be analyzed, e.g., the overall process execution time can be estimated. Existing performance analysis techniques usually discover stochastic process models from event data and then simulate these models to evaluate their execution times. These methods rely on empirical approaches. This paper proposes analytical techniques for performance analysis allowing for the derivation of statistical characteristics of the overall processes' execution times in the presence of arbitrary time distributions of events modeled by semi-Markov processes. The proposed methods can significantly simplify the what-if analysis of processes by providing solutions without resorting to simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.14415v3</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Kalenkova, Lewis Mitchell, Matthew Roughan</dc:creator>
    </item>
    <item>
      <title>Unbalanced Triangle Detection and Enumeration Hardness for Unions of Conjunctive Queries</title>
      <link>https://arxiv.org/abs/2210.11996</link>
      <description>arXiv:2210.11996v3 Announce Type: replace 
Abstract: We study the enumeration of answers to Unions of Conjunctive Queries (UCQs) with optimal time guarantees. More precisely, we wish to identify the queries that can be solved with linear preprocessing time and constant delay. Despite the basic nature of this problem, it was shown only recently that UCQs can be solved within these time bounds if they admit free-connex union extensions, even if all individual CQs in the union are intractable with respect to the same complexity measure. Our goal is to understand whether there exist additional tractable UCQs, not covered by the currently known algorithms. As a first step, we show that some previously unclassified UCQs are hard using the classic 3SUM hypothesis, via a known reduction from 3SUM to triangle listing in graphs. As a second step, we identify a question about a variant of this graph task that is unavoidable if we want to classify all self-join-free UCQs: is it possible to decide the existence of a triangle in a vertex-unbalanced tripartite graph in linear time? We prove that this task is equivalent in hardness to some family of UCQs. Finally, we show a dichotomy for unions of two self-join-free CQs if we assume the answer to this question is negative. In conclusion, this paper pinpoints a computational barrier in the form of a single decision problem that is key to advancing our understanding of the enumeration complexity of many UCQs. Without a breakthrough for unbalanced triangle detection, we have no hope of finding an efficient algorithm for additional unions of two self-join-free CQs. On the other hand, a sufficiently efficient unbalanced triangle detection algorithm can be turned into an efficient algorithm for a family of UCQs currently not known to be tractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.11996v3</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karl Bringmann, Nofar Carmeli</dc:creator>
    </item>
    <item>
      <title>On-Demand JSON: A Better Way to Parse Documents?</title>
      <link>https://arxiv.org/abs/2312.17149</link>
      <description>arXiv:2312.17149v2 Announce Type: replace 
Abstract: JSON is a popular standard for data interchange on the Internet. Ingesting JSON documents can be a performance bottleneck. A popular parsing strategy consists in converting the input text into a tree-based data structure -- sometimes called a Document Object Model or DOM. We designed and implemented a novel JSON parsing interface -- called On-Demand -- that appears to the programmer like a conventional DOM-based approach. However, the underlying implementation is a pointer iterating through the content, only materializing the results (objects, arrays, strings, numbers) lazily.On recent commodity processors, an implementation of our approach provides superior performance in multiple benchmarks. To ensure reproducibility, our work is freely available as open source software. Several systems use On Demand: e.g., Apache Doris, the Node.js JavaScript runtime, Milvus, and Velox.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17149v2</guid>
      <category>cs.DB</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/spe.3313</arxiv:DOI>
      <arxiv:journal_reference>Software: Practice and Experience 54 (6), 2024</arxiv:journal_reference>
      <dc:creator>John Keiser, Daniel Lemire</dc:creator>
    </item>
    <item>
      <title>The Dawn of Natural Language to SQL: Are We Fully Ready?</title>
      <link>https://arxiv.org/abs/2406.01265</link>
      <description>arXiv:2406.01265v3 Announce Type: replace 
Abstract: Translating users' natural language questions into SQL queries (i.e., NL2SQL) significantly lowers the barriers to accessing relational databases. The emergence of Large Language Models has introduced a novel paradigm in NL2SQL tasks, enhancing capabilities dramatically. However, this raises a critical question: Are we fully prepared to deploy NL2SQL models in production?
  To address the posed questions, we present a multi-angle NL2SQL evaluation framework, NL2SQL360, to facilitate the design and test of new NL2SQL methods for researchers. Through NL2SQL360, we conduct a detailed comparison of leading NL2SQL methods across a range of application scenarios, such as different data domains and SQL characteristics, offering valuable insights for selecting the most appropriate NL2SQL methods for specific needs. Moreover, we explore the NL2SQL design space, leveraging NL2SQL360 to automate the identification of an optimal NL2SQL solution tailored to user-specific needs. Specifically, NL2SQL360 identifies an effective NL2SQL method, SuperSQL, distinguished under the Spdier dataset using the execution accuracy metric. Remarkably, SuperSQL achieves competitive performance with execution accuracy of 87% and 62.66% on the Spider and BIRD test sets, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01265v3</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.14778/3681954.3682003</arxiv:DOI>
      <dc:creator>Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, Nan Tang</dc:creator>
    </item>
    <item>
      <title>Unique Characterisability and Learnability of Temporal Queries Mediated by an Ontology</title>
      <link>https://arxiv.org/abs/2306.07662</link>
      <description>arXiv:2306.07662v3 Announce Type: replace-cross 
Abstract: Algorithms for learning database queries from examples and unique characterisations of queries by examples are prominent starting points for developing automated support for query construction and explanation. We investigate how far recent results and techniques on learning and unique characterisations of atemporal queries mediated by an ontology can be extended to temporal data and queries. Based on a systematic review of the relevant approaches in the atemporal case, we obtain general transfer results identifying conditions under which temporal queries composed of atemporal ones are (polynomially) learnable and uniquely characterisable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07662v3</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean Christoph Jung, Vladislav Ryzhikov, Frank Wolter, Michael Zakharyaschev</dc:creator>
    </item>
  </channel>
</rss>
