<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Apr 2024 04:01:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Bayesian Regression Approach for Estimating the Impact of COVID-19 on Consumer Behavior in the Restaurant Industry</title>
      <link>https://arxiv.org/abs/2404.08670</link>
      <description>arXiv:2404.08670v1 Announce Type: new 
Abstract: The COVID-19 pandemic has had a long-term impact on industries worldwide, with the hospitality and food industry facing significant challenges, leading to the permanent closure of many restaurants and the loss of jobs. In this study, we developed an innovative analytical framework using Hamiltonian Monte Carlo for predictive modeling with Bayesian regression, aiming to estimate the change point in consumer behavior towards different types of restaurants due to COVID-19. Our approach emphasizes a novel method in computational analysis, providing insights into customer behavior changes before and after the pandemic. This research contributes to understanding the effects of COVID-19 on the restaurant industry and is valuable for restaurant owners and policymakers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08670v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>H. Hinduja, N. Mandal</dc:creator>
    </item>
    <item>
      <title>Seasonal and Periodic Patterns of PM2.5 in Manhattan using the Variable Bandpass Periodic Block Bootstrap</title>
      <link>https://arxiv.org/abs/2404.08738</link>
      <description>arXiv:2404.08738v1 Announce Type: new 
Abstract: Air quality is a critical component of environmental health. Monitoring and analysis of particulate matter with a diameter of 2.5 micrometers or smaller (PM2.5) plays a pivotal role in understanding air quality changes. This study focuses on the application of a new bandpass bootstrap approach, termed the Variable Bandpass Periodic Block Bootstrap (VBPBB), for analyzing time series data which provides modeled predictions of daily mean PM2.5 concentrations over 16 years in Manhattan, New York, the United States. The VBPBB can be used to explore periodically correlated (PC) principal components for this daily mean PM2.5 dataset. This method uses bandpass filters to isolate distinct PC components from datasets, removing unwanted interference including noise, and bootstraps the PC components. This preserves the PC structure and permits a better understanding of the periodic characteristics of time series data. The results of the VBPBB are compared against outcomes from alternative block bootstrapping techniques. The findings of this research indicate potential trends of elevated PM2.5 levels, providing evidence of significant semi-annual and weekly patterns missed by other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08738v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yanan Sun, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>PDXpower: A Power Analysis Tool for Experimental Design in Pre-clinical Xenograft Studies for Uncensored and Censored Outcomes</title>
      <link>https://arxiv.org/abs/2404.08927</link>
      <description>arXiv:2404.08927v1 Announce Type: new 
Abstract: In cancer research, leveraging patient-derived xenografts (PDXs) in pre-clinical experiments is a crucial approach for assessing innovative therapeutic strategies. Addressing the inherent variability in treatment response among and within individual PDX lines is essential. However, the current literature lacks a user-friendly statistical power analysis tool capable of concurrently determining the required number of PDX lines and animals per line per treatment group in this context. In this paper, we present a simulation-based R package for sample size determination, named `\textbf{PDXpower}', which is publicly available at The Comprehensive R Archive Network \url{https://CRAN.R-project.org/package=PDXpower}. The package is designed to estimate the necessary number of both PDX lines and animals per line per treatment group for the design of a PDX experiment, whether for an uncensored outcome, or a censored time-to-event outcome. Our sample size considerations rely on two widely used analytical frameworks: the mixed effects ANOVA model for uncensored outcomes and Cox's frailty model for censored data outcomes, which effectively account for both inter-PDX variability and intra-PDX correlation in treatment response. Step-by-step illustrations for utilizing the developed package are provided, catering to scenarios with or without preliminary data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08927v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shanpeng Li, Donatello Telesca, Harley I. Kornblum, David Nathanson, Frank Pajonk, Elvis Han Cui, Joycelynne Palmer, Gang Li</dc:creator>
    </item>
    <item>
      <title>Statistics of extremes for natural hazards: landslides and earthquakes</title>
      <link>https://arxiv.org/abs/2404.09156</link>
      <description>arXiv:2404.09156v1 Announce Type: new 
Abstract: In this chapter, we illustrate the use of split bulk-tail models and subasymptotic models motivated by extreme-value theory in the context of hazard assessment for earthquake-induced landslides. A spatial joint areal model is presented for modeling both landslides counts and landslide sizes, paying particular attention to extreme landslides, which are the most devastating ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09156v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishikesh Yadav, Luigi Lombardo, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>Statistics of Extremes for Neuroscience</title>
      <link>https://arxiv.org/abs/2404.09157</link>
      <description>arXiv:2404.09157v1 Announce Type: new 
Abstract: This chapter illustrates how tools from univariate and multivariate statistics of extremes can complement classical methods used to study brain signals and enhance the understanding of brain activity and connectivity during specific cognitive tasks or abnormal episodes, such as an epileptic seizure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09157v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo V. Redondo, Matheus B. Guerrero, Rapha\"el Huser, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Unraveling stochastic fundamental diagrams considering empirical knowledge: modeling, limitation and further discussion</title>
      <link>https://arxiv.org/abs/2404.09318</link>
      <description>arXiv:2404.09318v1 Announce Type: new 
Abstract: Traffic flow modeling relies heavily on fundamental diagrams. However, deterministic fundamental diagrams, such as single or multi-regime models, cannot capture the uncertainty pattern that underlies traffic flow. To address this limitation, a sparse non-parametric regression model is proposed in this paper to formulate the stochastic fundamental diagram. Unlike parametric stochastic fundamental diagram models, a non-parametric model is insensitive to parameters, flexible, and applicable. The computation complexity and the huge memory required for training in the Gaussian process regression have been reduced by introducing the sparse Gaussian process regression. The paper also discusses how empirical knowledge influences the modeling process. The paper analyzes the influence of modeling empirical knowledge in the prior of the stochastic fundamental diagram model and whether empirical knowledge can improve the robustness and accuracy of the proposed model. By introducing several well-known single-regime fundamental diagram models as the prior and testing the model's robustness and accuracy with different sampling methods given real-world data, the authors find that empirical knowledge can only benefit the model under small inducing samples given a relatively clean and large dataset. A pure data-driven approach is sufficient to estimate and describe the pattern of the density-speed relationship.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09318v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan-Zheng Lei, Yaobang Gong, Xianfeng Terry Yang</dc:creator>
    </item>
    <item>
      <title>Human Vs. Machines: Who Wins In Semiconductor Market Forecasting?</title>
      <link>https://arxiv.org/abs/2404.09334</link>
      <description>arXiv:2404.09334v1 Announce Type: new 
Abstract: "If you ask ten experts, you will get ten different opinions." This common proverb illustrates the common association of expert forecasts with personal bias and lack of consistency. On the other hand, digitization promises consistency and explainability through data-driven forecasts employing machine learning (ML) and statistical models. In the following, we compare such forecasts to expert forecasts from the World Semiconductor Trade Statistics (WSTS), a leading semiconductor market data provider.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09334v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Steinmeister, Markus Pauly</dc:creator>
    </item>
    <item>
      <title>Traffic State Estimation and Uncertainty Quantification at Signalized Intersections with Low Penetration Rate Vehicle Trajectory Data</title>
      <link>https://arxiv.org/abs/2404.08667</link>
      <description>arXiv:2404.08667v1 Announce Type: cross 
Abstract: This paper studies the traffic state estimation problem at signalized intersections with low penetration rate vehicle trajectory data. While many existing studies have proposed different methods to estimate unknown traffic states and parameters (e.g., penetration rate, queue length) with this data, most of them only provide a point estimation without knowing the uncertainty of these estimated values. It is important to quantify the estimation uncertainty caused by limited available data since it can explicitly inform us whether the available data is sufficient to satisfy the desired estimation accuracy. To fill this gap, we formulate the partially observable system as a hidden Markov model (HMM) based on the recently developed probabilistic time-space (PTS) model. The PTS model is a stochastic traffic flow model that is designed for modeling traffic flow dynamics near signalized intersections. Based on the HMM formulation, a single recursive program is developed for the Bayesian estimation of both traffic states and parameters. As a Bayesian approach, the proposed method provides the distributional estimation outcomes and directly quantifies the estimation uncertainty. We validate the proposed method with simulation studies and showcase its applicability to real-world vehicle trajectory data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08667v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingmin Wang, Zihao Wang, Zachary Jerome, Henry X. Liu</dc:creator>
    </item>
    <item>
      <title>Differentially Private Log-Location-Scale Regression Using Functional Mechanism</title>
      <link>https://arxiv.org/abs/2404.08715</link>
      <description>arXiv:2404.08715v1 Announce Type: cross 
Abstract: This article introduces differentially private log-location-scale (DP-LLS) regression models, which incorporate differential privacy into LLS regression through the functional mechanism. The proposed models are established by injecting noise into the log-likelihood function of LLS regression for perturbed parameter estimation. We will derive the sensitivities utilized to determine the magnitude of the injected noise and prove that the proposed DP-LLS models satisfy $\epsilon$-differential privacy. In addition, we will conduct simulations and case studies to evaluate the performance of the proposed models. The findings suggest that predictor dimension, training sample size, and privacy budget are three key factors impacting the performance of the proposed DP-LLS regression models. Moreover, the results indicate that a sufficiently large training dataset is needed to simultaneously ensure decent performance of the proposed models and achieve a satisfactory level of privacy protection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08715v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiewen Sheng, Xiaolei Fang</dc:creator>
    </item>
    <item>
      <title>Early detection of disease outbreaks and non-outbreaks using incidence data</title>
      <link>https://arxiv.org/abs/2404.08893</link>
      <description>arXiv:2404.08893v1 Announce Type: cross 
Abstract: Forecasting the occurrence and absence of novel disease outbreaks is essential for disease management. Here, we develop a general model, with no real-world training data, that accurately forecasts outbreaks and non-outbreaks. We propose a novel framework, using a feature-based time series classification method to forecast outbreaks and non-outbreaks. We tested our methods on synthetic data from a Susceptible-Infected-Recovered model for slowly changing, noisy disease dynamics. Outbreak sequences give a transcritical bifurcation within a specified future time window, whereas non-outbreak (null bifurcation) sequences do not. We identified incipient differences in time series of infectives leading to future outbreaks and non-outbreaks. These differences are reflected in 22 statistical features and 5 early warning signal indicators. Classifier performance, given by the area under the receiver-operating curve, ranged from 0.99 for large expanding windows of training data to 0.7 for small rolling windows. Real-world performances of classifiers were tested on two empirical datasets, COVID-19 data from Singapore and SARS data from Hong Kong, with two classifiers exhibiting high accuracy. In summary, we showed that there are statistical features that distinguish outbreak and non-outbreak sequences long before outbreaks occur. We could detect these differences in synthetic and real-world data sets, well before potential outbreaks occur.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08893v1</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Gao, Amit K. Chakraborty, Russell Greiner, Mark A. Lewis, Hao Wang</dc:creator>
    </item>
    <item>
      <title>ALICE: Combining Feature Selection and Inter-Rater Agreeability for Machine Learning Insights</title>
      <link>https://arxiv.org/abs/2404.09053</link>
      <description>arXiv:2404.09053v1 Announce Type: cross 
Abstract: This paper presents a new Python library called Automated Learning for Insightful Comparison and Evaluation (ALICE), which merges conventional feature selection and the concept of inter-rater agreeability in a simple, user-friendly manner to seek insights into black box Machine Learning models. The framework is proposed following an overview of the key concepts of interpretability in ML. The entire architecture and intuition of the main methods of the framework are also thoroughly discussed and results from initial experiments on a customer churn predictive modeling task are presented, alongside ideas for possible avenues to explore for the future. The full source code for the framework and the experiment notebooks can be found at: https://github.com/anasashb/aliceHU</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09053v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bachana Anasashvili, Vahidin Jeleskovic</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Genomic Data with Multiple Heterogeneous Outcomes</title>
      <link>https://arxiv.org/abs/2404.09119</link>
      <description>arXiv:2404.09119v1 Announce Type: cross 
Abstract: With the evolution of single-cell RNA sequencing techniques into a standard approach in genomics, it has become possible to conduct cohort-level causal inferences based on single-cell-level measurements. However, the individual gene expression levels of interest are not directly observable; instead, only repeated proxy measurements from each individual's cells are available, providing a derived outcome to estimate the underlying outcome for each of many genes. In this paper, we propose a generic semiparametric inference framework for doubly robust estimation with multiple derived outcomes, which also encompasses the usual setting of multiple outcomes when the response of each unit is available. To reliably quantify the causal effects of heterogeneous outcomes, we specialize the analysis to the standardized average treatment effects and the quantile treatment effects. Through this, we demonstrate the use of the semiparametric inferential results for doubly robust estimators derived from both Von Mises expansions and estimating equations. A multiple testing procedure based on the Gaussian multiplier bootstrap is tailored for doubly robust estimators to control the false discovery exceedance rate. Applications in single-cell CRISPR perturbation analysis and individual-level differential expression analysis demonstrate the utility of the proposed methods and offer insights into the usage of different estimands for causal inference in genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09119v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Zhenghao Zeng, Edward H. Kennedy, Larry Wasserman, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Heterogeneity and Importance Measures for Multivariate Continuous Treatments</title>
      <link>https://arxiv.org/abs/2404.09126</link>
      <description>arXiv:2404.09126v1 Announce Type: cross 
Abstract: Estimating the joint effect of a multivariate, continuous exposure is crucial, particularly in environmental health where interest lies in simultaneously evaluating the impact of multiple environmental pollutants on health. We develop novel methodology that addresses two key issues for estimation of treatment effects of multivariate, continuous exposures. We use nonparametric Bayesian methodology that is flexible to ensure our approach can capture a wide range of data generating processes. Additionally, we allow the effect of the exposures to be heterogeneous with respect to covariates. Treatment effect heterogeneity has not been well explored in the causal inference literature for multivariate, continuous exposures, and therefore we introduce novel estimands that summarize the nature and extent of the heterogeneity, and propose estimation procedures for new estimands related to treatment effect heterogeneity. We provide theoretical support for the proposed models in the form of posterior contraction rates and show that it works well in simulated examples both with and without heterogeneity. We apply our approach to a study of the health effects of simultaneous exposure to the components of PM$_{2.5}$ and find that the negative health effects of exposure to these environmental pollutants is exacerbated by low socioeconomic status and age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09126v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejun Shin, Antonio Linero, Michelle Audirac, Kezia Irene, Danielle Braun, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>A Unified Combination Framework for Dependent Tests with Applications to Microbiome Association Studies</title>
      <link>https://arxiv.org/abs/2404.09353</link>
      <description>arXiv:2404.09353v1 Announce Type: cross 
Abstract: We introduce a novel meta-analysis framework to combine dependent tests under a general setting, and utilize it to synthesize various microbiome association tests that are calculated from the same dataset. Our development builds upon the classical meta-analysis methods of aggregating $p$-values and also a more recent general method of combining confidence distributions, but makes generalizations to handle dependent tests. The proposed framework ensures rigorous statistical guarantees, and we provide a comprehensive study and compare it with various existing dependent combination methods. Notably, we demonstrate that the widely used Cauchy combination method for dependent tests, referred to as the vanilla Cauchy combination in this article, can be viewed as a special case within our framework. Moreover, the proposed framework provides a way to address the problem when the distributional assumptions underlying the vanilla Cauchy combination are violated. Our numerical results demonstrate that ignoring the dependence among the to-be-combined components may lead to a severe size distortion phenomenon. Compared to the existing $p$-value combination methods, including the vanilla Cauchy combination method, the proposed combination framework can handle the dependence accurately and utilizes the information efficiently to construct tests with accurate size and enhanced power. The development is applied to Microbiome Association Studies, where we aggregate information from multiple existing tests using the same dataset. The combined tests harness the strengths of each individual test across a wide range of alternative spaces, %resulting in a significant enhancement of testing power across a wide range of alternative spaces, enabling more efficient and meaningful discoveries of vital microbiome associations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09353v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiufan Yu, Linjun Zhang, Arun Srinivasan, Min-ge Xie, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>A Bayesian Joint Modelling for Misclassified Interval-censoring and Competing Risks</title>
      <link>https://arxiv.org/abs/2404.09362</link>
      <description>arXiv:2404.09362v1 Announce Type: cross 
Abstract: In active surveillance of prostate cancer, cancer progression is interval-censored and the examination to detect progression is subject to misclassification, usually false negatives. Meanwhile, patients may initiate early treatment before progression detection, constituting a competing risk. We developed the Misclassification-Corrected Interval-censored Cause-specific Joint Model (MCICJM) to estimate the association between longitudinal biomarkers and cancer progression in this setting. The sensitivity of the examination is considered in the likelihood of this model via a parameter that may be set to a specific value if the sensitivity is known, or for which a prior distribution can be specified if the sensitivity is unknown. Our simulation results show that misspecification of the sensitivity parameter or ignoring it entirely impacts the model parameters, especially the parameter uncertainty and the baseline hazards. Moreover, specification of a prior distribution for the sensitivity parameter may reduce the risk of misspecification in settings where the exact sensitivity is unknown, but may cause identifiability issues. Thus, imposing restrictions on the baseline hazards is recommended. A trade-off between modelling with a sensitivity constant at the risk of misspecification and a sensitivity prior at the cost of flexibility needs to be decided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09362v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenwei Yang, Dimitris Rizopoulos, Eveline A. M. Heijnsdijk, Lisa F. Newcomb, Nicole S. Erler</dc:creator>
    </item>
    <item>
      <title>Integrating Marketing Channels into Quantile Transformation and Bayesian Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process Models</title>
      <link>https://arxiv.org/abs/2404.09386</link>
      <description>arXiv:2404.09386v1 Announce Type: cross 
Abstract: This study introduces an innovative Gaussian Process (GP) model utilizing an ensemble kernel that integrates Radial Basis Function (RBF), Rational Quadratic, and Mat\'ern kernels for product sales forecasting. By applying Bayesian optimization, we efficiently find the optimal weights for each kernel, enhancing the model's ability to handle complex sales data patterns. Our approach significantly outperforms traditional GP models, achieving a notable 98\% accuracy and superior performance across key metrics including Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination ($R^2$). This advancement underscores the effectiveness of ensemble kernels and Bayesian optimization in improving predictive accuracy, offering profound implications for machine learning applications in sales forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09386v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shahin Mirshekari, Negin Hayeri Motedayen, Mohammad Ensaf</dc:creator>
    </item>
    <item>
      <title>Overfitting Reduction in Convex Regression</title>
      <link>https://arxiv.org/abs/2404.09528</link>
      <description>arXiv:2404.09528v1 Announce Type: cross 
Abstract: Convex regression is a method for estimating an unknown function $f_0$ from a data set of $n$ noisy observations when $f_0$ is known to be convex. This method has played an important role in operations research, economics, machine learning, and many other areas. It has been empirically observed that the convex regression estimator produces inconsistent estimates of $f_0$ and extremely large subgradients near the boundary of the domain of $f_0$ as $n$ increases. In this paper, we provide theoretical evidence of this overfitting behaviour. We also prove that the penalised convex regression estimator, one of the variants of the convex regression estimator, exhibits overfitting behaviour. To eliminate this behaviour, we propose two new estimators by placing a bound on the subgradients of the estimated function. We further show that our proposed estimators do not exhibit the overfitting behaviour by proving that (a) they converge to $f_0$ and (b) their subgradients converge to the gradient of $f_0$, both uniformly over the domain of $f_0$ with probability one as $n \rightarrow \infty$. We apply the proposed methods to compute the cost frontier function for Finnish electricity distribution firms and confirm their superior performance in predictive power over some existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09528v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqiang Liao, Sheng Dai, Eunji Lim, Timo Kuosmanen</dc:creator>
    </item>
    <item>
      <title>Optimal Cut-Point Estimation for functional digital biomarkers: Application to Continuous Glucose Monitoring</title>
      <link>https://arxiv.org/abs/2404.09716</link>
      <description>arXiv:2404.09716v1 Announce Type: cross 
Abstract: Establish optimal cut points plays a crucial role in epidemiology and biomarker discovery, enabling the development of effective and practical clinical decision criteria. While there is extensive literature to define optimal cut off over scalar biomarkers, there is a notable lack of general methodologies for analyzing statistical objects in more complex spaces of functions and graphs, which are increasingly relevant in digital health applications. This paper proposes a new general methodology to define optimal cut points for random objects in separable Hilbert spaces. The paper is motivated by the need for creating new clinical rules for diabetes mellitus disease, exploiting the functional information of a continuous diabetes monitor (CGM) as a digital biomarker. More specifically, we provide the functional cut off to identify diabetes cases with CGM information based on glucose distributional functional representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09716v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Lado-Baleato, Marcos Matabuena, Carla D\'iaz-Louzao, Francisco Gude</dc:creator>
    </item>
    <item>
      <title>Pseudo P-values for Assessing Covariate Balance in a Finite Study Population with Application to the California Sugar Sweetened Beverage Tax Study</title>
      <link>https://arxiv.org/abs/2404.09960</link>
      <description>arXiv:2404.09960v1 Announce Type: cross 
Abstract: Assessing covariate balance (CB) is a common practice in various types of evaluation studies. Two-sample descriptive statistics, such as the standardized mean difference, have been widely applied in the scientific literature to assess the goodness of CB. Studies in health policy, health services research, built and social environment research, and many other fields often involve a finite number of units that may be subject to different treatment levels. Our case study, the California Sugar Sweetened Beverage (SSB) Tax Study, include 332 study cities in the state of California, among which individual cities may elect to levy a city-wide excise tax on SSB sales. Evaluating the balance of covariates between study cities with and without the tax policy is essential for assessing the effects of the policy on health outcomes of interest. In this paper, we introduce the novel concepts of the pseudo p-value and the standardized pseudo p-value, which are descriptive statistics to assess the overall goodness of CB between study arms in a finite study population. While not meant as a hypothesis test, the pseudo p-values bear superficial similarity to the classic p-value, which makes them easy to apply and interpret in applications. We discuss some theoretical properties of the pseudo p-values and present an algorithm to calculate them. We report a numerical simulation study to demonstrate their performance. We apply the pseudo p-values to the California SSB Tax study to assess the balance of city-level characteristics between the two study arms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09960v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bing Han, Margo A. Sidell</dc:creator>
    </item>
    <item>
      <title>Combining Probabilistic Forecasts of Intermittent Demand</title>
      <link>https://arxiv.org/abs/2304.03092</link>
      <description>arXiv:2304.03092v4 Announce Type: replace 
Abstract: In recent decades, new methods and approaches have been developed for forecasting intermittent demand series. However, the majority of research has focused on point forecasting, with little exploration into probabilistic intermittent demand forecasting. This is despite the fact that probabilistic forecasting is crucial for effective decision-making under uncertainty and inventory management. Additionally, most literature on this topic has focused solely on forecasting performance and has overlooked the inventory implications, which are directly relevant to intermittent demand. To address these gaps, this study aims to construct probabilistic forecasting combinations for intermittent demand while considering both forecasting accuracy and inventory control utility in obtaining combinations and evaluating forecasts. Our empirical findings demonstrate that combinations perform better than individual approaches for forecasting intermittent demand, but there is a trade-off between forecasting and inventory performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03092v4</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengjie Wang, Yanfei Kang, Fotios Petropoulos</dc:creator>
    </item>
    <item>
      <title>Analyzing Taiwanese traffic patterns on consecutive holidays through forecast reconciliation and prediction-based anomaly detection techniques</title>
      <link>https://arxiv.org/abs/2307.09537</link>
      <description>arXiv:2307.09537v2 Announce Type: replace 
Abstract: This study explores traffic patterns on Taiwanese highways during consecutive holidays and focuses on understanding Taiwanese highway traffic behavior. We propose a prediction-based detection method for finding highway traffic anomalies using reconciled ordinary least squares (OLS) forecasts and bootstrap prediction intervals. Two fundamental features of traffic flow time series -- namely, seasonality and spatial autocorrelation -- are captured by adding Fourier terms in OLS models, spatial aggregation (as a hierarchical structure mimicking the geographical division in regions, cities, and stations), and a reconciliation step. Our approach, although simple, is able to model complex traffic datasets with reasonable accuracy. Being based on OLS, it is efficient and permits avoiding the computational burden of more complex methods. Analyses of Taiwan's consecutive holidays in 2019, 2020, and 2021 (73 days) showed strong variations in anomalies across different directions and highways. Specifically, we detected some areas and highways comprising a high number of traffic anomalies (north direction-central and southern regions-highways No. 1 and 3, south direction-southern region-highway No.3), and others with generally normal traffic (east and west direction). These results could provide important decision-support information to traffic authorities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.09537v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahsa Ashouri, Frederick Kin Hing Phoa, Marzia A. Cremona</dc:creator>
    </item>
    <item>
      <title>Unsupervised Ensembling of Multiple Software Sensors with Phase Synchronization: A Robust Approach For Electrocardiogram-derived Respiration</title>
      <link>https://arxiv.org/abs/2006.13054</link>
      <description>arXiv:2006.13054v3 Announce Type: replace-cross 
Abstract: Objective: We aimed to fuse the outputs of different electrocardiogram-derived respiration (EDR) algorithms to create one EDR signal that is of higher quality. Methods: We viewed each EDR algorithm as a software sensor that recorded breathing activity from a different vantage point, identified high-quality software sensors based on the respiratory signal quality index, aligned the highest-quality EDRs with a phase synchronization technique based on the graph connection Laplacian, and finally fused those aligned, high-quality EDRs. We refer to the output as the sync-ensembled EDR signal. The proposed algorithm was evaluated on two large-scale databases of whole-night polysomnograms. We evaluated the performance of the proposed algorithm using three respiratory signals recorded from different hardware sensors, and compared it with other existing EDR algorithms. A sensitivity analysis was carried out for a total of five cases: fusion by taking the mean of EDR signals, and the four cases of EDR signal alignment without and with synchronization and without and with signal quality selection. Results: The sync-ensembled EDR algorithm outperforms existing EDR algorithms when evaluated by the synchronized correlation ({\gamma}-score), optimal transport (OT) distance, and estimated average respiratory rate (EARR) score, all with statistical significance. The sensitivity analysis shows that the signal quality selection and EDR signal alignment are both critical for the performance, both with statistical significance. Conclusion: The sync-ensembled EDR provides robust respiratory information from electrocardiogram. Significance: Phase synchronization is not only theoretically rigorous but also practical to design a robust EDR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.13054v3</guid>
      <category>eess.SP</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob McErlean, John Malik, Yu-Ting Lin, Ronen Talmon, Hau-Tieng Wu</dc:creator>
    </item>
    <item>
      <title>clrng: A tool set for parallel random numbergeneration on GPUs in R</title>
      <link>https://arxiv.org/abs/2201.06604</link>
      <description>arXiv:2201.06604v4 Announce Type: replace-cross 
Abstract: We introduce the R package clrng which leverages the gpuR package and is able to generate random numbers in parallel on a Graphics Processing Unit (GPU) with the clRNG (OpenCL) library. Parallel processing with GPU's can speed up computationally intensive tasks, which when combined with R, it can largely improve R's downsides in terms of slow speed, memory usage and computation mode. clrng enables reproducible research by setting random initial seeds for streams on GPU and CPU, and can thus accelerate several types of statistical simulation and modelling. The random number generator in clrng guarantees independent parallel samples even when R is used interactively in an ad-hoc manner, with sessions being interrupted and restored. This package is portable and flexible, developers can use its random number generation kernel for various other purposes and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.06604v4</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyong Xu, Patrick Brown, Pierre L'Ecuyer</dc:creator>
    </item>
    <item>
      <title>Penalized Estimation of Frailty-Based Illness-Death Models for Semi-Competing Risks</title>
      <link>https://arxiv.org/abs/2202.00618</link>
      <description>arXiv:2202.00618v3 Announce Type: replace-cross 
Abstract: Semi-competing risks refers to the survival analysis setting where the occurrence of a non-terminal event is subject to whether a terminal event has occurred, but not vice versa. Semi-competing risks arise in a broad range of clinical contexts, with a novel example being the pregnancy condition preeclampsia, which can only occur before the `terminal' event of giving birth. Models that acknowledge semi-competing risks enable investigation of relationships between covariates and the joint timing of the outcomes, but methods for model selection and prediction of semi-competing risks in high dimensions are lacking. Instead, researchers commonly analyze only a single or composite outcome, losing valuable information and limiting clinical utility -- in the obstetric setting, this means ignoring valuable insight into timing of delivery after preeclampsia has onset. To address this gap we propose a novel penalized estimation framework for frailty-based illness-death multi-state modeling of semi-competing risks. Our approach combines non-convex and structured fusion penalization, inducing global sparsity as well as parsimony across submodels. We perform estimation and model selection via a pathwise routine for non-convex optimization, and prove the first statistical error bound results in this setting. We present a simulation study investigating estimation error and model selection performance, and a comprehensive application of the method to joint risk modeling of preeclampsia and timing of delivery using pregnancy data from an electronic health record.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.00618v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1111/biom.13761</arxiv:DOI>
      <arxiv:journal_reference>Biometrics, 79(3), 1657-1669</arxiv:journal_reference>
      <dc:creator>Harrison T. Reeder, Junwei Lu, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>Tracking the distance to criticality in systems with unknown noise</title>
      <link>https://arxiv.org/abs/2310.14791</link>
      <description>arXiv:2310.14791v3 Announce Type: replace-cross 
Abstract: Many real-world systems undergo abrupt changes in dynamics as they move across critical points, often with dramatic consequences. Much existing theory on identifying the time-series signatures of nearby critical points -- such as increased variance and slower timescales -- is derived for the case of fixed, low-amplitude noise. However, real-world systems are often corrupted by unknown levels of noise that can distort these temporal signatures. Here we aimed to develop noise-robust indicators of the distance to criticality (DTC) for systems affected by dynamical noise in two cases: when the noise amplitude is fixed, or is unknown and variable across recordings. To approach this problem, we compare the ability of over 7000 candidate time-series features to track the DTC in the vicinity of a supercritical Hopf bifurcation. We recover existing theory in the fixed-noise case, highlighting conventional time-series features that accurately track the DTC. But in the variable-noise setting, where these conventional indicators perform poorly, we highlight new types of high-performing time-series features and show that their success is accomplished by capturing the shape of the invariant density (which depends on both the DTC and the noise amplitude) relative to the spread of fast fluctuations (which depends on the noise amplitude). We introduce a new high-performing time-series statistic, the Rescaled Auto-Density (RAD), that combines these two algorithmic components. We then use RAD to provide new evidence that brain regions higher in the visual hierarchy are positioned closer to criticality, supporting existing hypotheses about patterns of brain organization that are not detected using conventional metrics of the DTC. Our results demonstrate how large-scale algorithmic comparison can yield theoretical insights that can motivate new theory and interpretable algorithms for real-world problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14791v3</guid>
      <category>physics.data-an</category>
      <category>math.DS</category>
      <category>nlin.CD</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Brendan Harris, Leonardo L. Gollo, Ben D. Fulcher</dc:creator>
    </item>
    <item>
      <title>Factor copula models for non-Gaussian longitudinal data</title>
      <link>https://arxiv.org/abs/2402.00668</link>
      <description>arXiv:2402.00668v2 Announce Type: replace-cross 
Abstract: This article presents factor copula approaches to model temporal dependency of non-Gaussian (continuous/discrete) longitudinal data. Factor copula models are canonical vine copulas which explain the underlying dependence structure of a multivariate data through latent variables, and therefore can be easily interpreted and implemented to unbalanced longitudinal data. We develop regression models for continuous, binary and ordinal longitudinal data including covariates, by using factor copula constructions with subject-specific latent variables. Considering homogeneous within-subject dependence, our proposed models allow for feasible parametric inference in moderate to high dimensional situations, using two-stage (IFM) estimation method. We assess the finite sample performance of the proposed models with extensive simulation studies. In the empirical analysis, the proposed models are applied for analysing different longitudinal responses of two real world data sets. Moreover, we compare the performances of these models with some widely used random effect models using standard model selection techniques and find substantial improvements. Our studies suggest that factor copula models can be good alternatives to random effect models and can provide better insights to temporal dependency of longitudinal data of arbitrary nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00668v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Hypergraph adjusted plus-minus</title>
      <link>https://arxiv.org/abs/2403.20214</link>
      <description>arXiv:2403.20214v2 Announce Type: replace-cross 
Abstract: In team sports, traditional ranking statistics do not allow for the simultaneous evaluation of both individuals and combinations of players. Metrics for individual player rankings often fail to consider the interaction effects between groups of players, while methods for assessing full lineups cannot be used to identify the value of lower-order combinations of players (pairs, trios, etc.). Given that player and lineup rankings are inherently dependent on each other, these limitations may affect the accuracy of performance evaluations. To address this, we propose a novel adjusted box score plus-minus (APM) approach that allows for the simultaneous ranking of individual players, lower-order combinations of players, and full lineups. The method adjusts for the complete dependency structure and is motivated by the connection between APM and the hypergraph representation of a team. We discuss the similarities of our approach to other advanced metrics, demonstrate it using NBA data from 2012-2022, and suggest potential directions for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20214v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Josephs, Elizabeth Upton</dc:creator>
    </item>
  </channel>
</rss>
