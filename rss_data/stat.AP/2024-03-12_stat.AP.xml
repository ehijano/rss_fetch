<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Mar 2024 04:01:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Studying a Surgery Service Occupation through a Queues Model</title>
      <link>https://arxiv.org/abs/2403.05554</link>
      <description>arXiv:2403.05554v1 Announce Type: new 
Abstract: A method to study and evaluate the occupation of a Hospital Surgery Service, with some specificity in its activity, is outlined in this work. Its application is exemplified with real data, and it is shown that it is simple, practical, and useful and allows a practical management of the service occupation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05554v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.12988/AMS.2015.55390</arxiv:DOI>
      <dc:creator>Manuel Alberto M. Ferreira</dc:creator>
    </item>
    <item>
      <title>Bringing Age Back In: Accounting for Population Age Distribution in Forecasting Migration</title>
      <link>https://arxiv.org/abs/2403.05566</link>
      <description>arXiv:2403.05566v1 Announce Type: new 
Abstract: The link between age and migration propensity is long established, but existing models of country-level net migration ignore the effect of population age distribution on past and projected migration rates. We propose a method to estimate and forecast international net migration rates for the 200 most populous countries, taking account of changes in population age structure. We use age-standardized estimates of country-level net migration rates and in-migration rates over quinquennial periods from 1990 through 2020 to decompose past net migration rates into in-migration rates and out-migration rates. We then recalculate historic migration rates on a scale that removes the influence of the population age distribution. This is done by scaling past and projected migration rates in terms of a reference population and period. We show that this can be done very simply, using a quantity we call the migration age structure index (MASI). We use a Bayesian hierarchical model to generate joint probabilistic forecasts of total and age- and sex- specific net migration rates over five-year periods for all countries from 2020 through 2100. We find that accounting for population age structure in historic and forecast net migration rates leads to narrower prediction intervals by the end of the century for most countries. Also, applying a Rogers &amp; Castro-like migration age schedule to migration outflows reduces uncertainty in population pyramid forecasts. Finally, accounting for population age structure leads to less out-migration among countries with rapidly aging populations that are forecast to contract most rapidly by the end of the century. This leads to less drastic population declines than are forecast without accounting for population age structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05566v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan G. Welch, Hana \v{S}ev\v{c}\'ikov\'a, Adrian E. Raftery</dc:creator>
    </item>
    <item>
      <title>Simultaneous test of the mean vectors and covariance matrices for high-dimensional data using RMT</title>
      <link>https://arxiv.org/abs/2403.05760</link>
      <description>arXiv:2403.05760v1 Announce Type: new 
Abstract: In this paper, we propose a new modified likelihood ratio test (LRT) for simultaneously testing mean vectors and covariance matrices of two-sample populations in high-dimensional settings. By employing tools from Random Matrix Theory (RMT), we derive the limiting null distribution of the modified LRT for generally distributed populations. Furthermore, we compare the proposed test with existing tests using simulation results, demonstrating that the modified LRT exhibits favorable properties in terms of both size and power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05760v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenzhen Niu, Jianghao Li, Wenya Luo, Zhidong Bai</dc:creator>
    </item>
    <item>
      <title>Stationary Proportional Hazard Processes via Complementary Power Function Distribution Processes</title>
      <link>https://arxiv.org/abs/2403.05813</link>
      <description>arXiv:2403.05813v1 Announce Type: new 
Abstract: In the following, we introduce new proportional hazard (PH) processes, which are derived by a marginal transformation applied to complementary power function distribution (CPFD) processes. Also, we introduce two new Pareto processes, which are derived from the proportional hazard family. We discuss distributional features of such processes, explore inferential aspects and include an example of applications of the new processes to real-life data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05813v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Barry C. Arnold, B. G. Manjunath, S. Sachdeva</dc:creator>
    </item>
    <item>
      <title>Seasonal and Periodic Patterns in US COVID-19 Mortality using the Variable Bandpass Periodic Block Bootstrap</title>
      <link>https://arxiv.org/abs/2403.06343</link>
      <description>arXiv:2403.06343v1 Announce Type: new 
Abstract: Since the emergence of the SARS-CoV-2 virus, research into the existence, extent, and pattern of seasonality has been of the highest importance for public health preparation. This study uses a novel bandpass bootstrap approach called the Variable Bandpass Periodic Block Bootstrap (VBPBB) to investigate the periodically correlated (PC) components including seasonality within US COVID-19 mortality. Bootstrapping to produce confidence intervals (CI) for periodic characteristics such as the seasonal mean requires preservation of the PC component's correlation structure during resampling. While existing bootstrap methods can preserve the PC component correlation structure, filtration of that PC component's frequency from interference is critical to bootstrap the PC component's characteristics accurately and efficiently. The VBPBB filters the PC time series to reduce interference from other components such as noise. This greatly reduces bootstrapped CI size and outperforms the statistical power and accuracy of other methods when estimating the periodic mean sampling distribution. VBPBB analysis of US COVID-19 mortality PC components are provided and compared against alternative bootstrapping methods. These results reveal crucial evidence supporting the presence of a seasonal PC pattern and existence of additional PC components, their timing, and CIs for their effect which will aid prediction and preparation for future COVID-19 responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06343v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Valachovic, Ekaterina Shishova</dc:creator>
    </item>
    <item>
      <title>Unified Occupancy on a Public Transport Network through Combination of AFC and APC Data</title>
      <link>https://arxiv.org/abs/2403.05546</link>
      <description>arXiv:2403.05546v1 Announce Type: cross 
Abstract: In a transport network, the onboard occupancy is key for gaining insights into travelers' habits and adjusting the offer. Traditionally, operators have relied on field studies to evaluate ridership of a typical workday. However, automated fare collection (AFC) and automatic passenger counting (APC) data, which provide complete temporal coverage, are often available but underexploited. It should be noted, however, that each data source comes with its own biases: AFC data may not account for fraud, while not all vehicles are equipped with APC systems.
  This paper introduces the unified occupancy method, a geostatistical model to extrapolate occupancy to every course of a public transportation network by combining AFC and APC data with partial coverage. Unified occupancy completes missing APC information for courses on lines where other courses have APC measures, as well as for courses on lines where no APC data is available at all. The accuracy of this method is evaluated on real data from several public transportation networks in France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05546v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amir Dib, No\"elie Cherrier, Martin Graive, Baptiste R\'erolle, Eglantine Schmitt</dc:creator>
    </item>
    <item>
      <title>TSSS: A Novel Triangulated Spherical Spline Smoothing for Surface-based Data</title>
      <link>https://arxiv.org/abs/2403.05644</link>
      <description>arXiv:2403.05644v1 Announce Type: cross 
Abstract: Surface-based data is commonly observed in diverse practical applications spanning various fields. In this paper, we introduce a novel nonparametric method to discover the underlying signals from data distributed on complex surface-based domains. Our approach involves a penalized spline estimator defined on a triangulation of surface patches, which enables effective signal extraction and recovery. The proposed method offers several advantages over existing methods, including superior handling of "leakage" or "boundary effects" over complex domains, enhanced computational efficiency, and potential applications in analyzing sparse and irregularly distributed data on complex objects. We provide rigorous theoretical guarantees for the proposed method, including convergence rates of the estimator in both the $L_2$ and supremum norms, as well as the asymptotic normality of the estimator. We also demonstrate that the convergence rates achieved by our estimation method are optimal within the framework of nonparametric estimation. Furthermore, we introduce a bootstrap method to quantify the uncertainty associated with the proposed estimators accurately. The superior performance of the proposed method is demonstrated through simulation experiments and data applications on cortical surface functional magnetic resonance imaging data and oceanic near-surface atmospheric data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05644v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhiling Gu, Shan Yu, Guannan Wang, Ming-Jun Lai, Li Wang</dc:creator>
    </item>
    <item>
      <title>Dynamic clustering for heterophilic stochastic block models with time-varying node memberships</title>
      <link>https://arxiv.org/abs/2403.05654</link>
      <description>arXiv:2403.05654v1 Announce Type: cross 
Abstract: We consider a time-ordered sequence of networks stemming from stochastic block models where nodes gradually change memberships over time and no network at any single time point contains sufficient signal strength to recover its community structure. To estimate the time-varying community structure, we develop KD-SoS (kernel debiased sum-of-square), a method performing spectral clustering after a debiased sum-of-squared aggregation of adjacency matrices. Our theory demonstrates via a novel bias-variance decomposition that KD-SoS achieves consistent community detection of each network even when heterophilic networks do not require smoothness in the time-varying dynamics of between-community connectivities. We also prove the identifiability of aligning community structures across time based on how rapidly nodes change communities, and develop a data-adaptive bandwidth tuning procedure for KD-SoS. We demonstrate the utility and advantages of KD-SoS through simulations and a novel analysis of the time-varying dynamics in gene coordination in the human developing brain system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05654v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Z Lin, Jing Lei</dc:creator>
    </item>
    <item>
      <title>Debiased Projected Two-Sample Comparisonscfor Single-Cell Expression Data</title>
      <link>https://arxiv.org/abs/2403.05679</link>
      <description>arXiv:2403.05679v1 Announce Type: cross 
Abstract: We study several variants of the high-dimensional mean inference problem motivated by modern single-cell genomics data. By taking advantage of low-dimensional and localized signal structures commonly seen in such data, our proposed methods not only have the usual frequentist validity but also provide useful information on the potential locations of the signal if the null hypothesis is rejected. Our method adaptively projects the high-dimensional vector onto a low-dimensional space, followed by a debiasing step using the semiparametric double-machine learning framework. Our analysis shows that debiasing is unnecessary under the global null, but necessary under a ``projected null'' that is of scientific interest. We also propose an ``anchored projection'' to maximize the power while avoiding the degeneracy issue under the null. Experiments on synthetic data and a real single-cell sequencing dataset demonstrate the effectiveness and interpretability of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05679v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Non-robustness of diffusion estimates on networks with measurement error</title>
      <link>https://arxiv.org/abs/2403.05704</link>
      <description>arXiv:2403.05704v1 Announce Type: cross 
Abstract: Network diffusion models are used to study things like disease transmission, information spread, and technology adoption. However, small amounts of mismeasurement are extremely likely in the networks constructed to operationalize these models. We show that estimates of diffusions are highly non-robust to this measurement error. First, we show that even when measurement error is vanishingly small, such that the share of missed links is close to zero, forecasts about the extent of diffusion will greatly underestimate the truth. Second, a small mismeasurement in the identity of the initial seed generates a large shift in the locations of expected diffusion path. We show that both of these results still hold when the vanishing measurement error is only local in nature. Such non-robustness in forecasting exists even under conditions where the basic reproductive number is consistently estimable. Possible solutions, such as estimating the measurement error or implementing widespread detection efforts, still face difficulties because the number of missed links are so small. Finally, we conduct Monte Carlo simulations on simulated networks, and real networks from three settings: travel data from the COVID-19 pandemic in the western US, a mobile phone marketing campaign in rural India, and in an insurance experiment in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05704v1</guid>
      <category>econ.EM</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arun G. Chandrasekhar, Paul Goldsmith-Pinkham, Tyler H. McCormick, Samuel Thau, Jerry Wei</dc:creator>
    </item>
    <item>
      <title>Sample Size Selection under an Infill Asymptotic Domain</title>
      <link>https://arxiv.org/abs/2403.05969</link>
      <description>arXiv:2403.05969v1 Announce Type: cross 
Abstract: Experimental studies often fail to appropriately account for the number of collected samples within a fixed time interval for functional responses. Data of this nature appropriately falls under an Infill Asymptotic domain that is constrained by time and not considered infinite. Therefore, the sample size should account for this infill asymptotic domain. This paper provides general guidance on selecting an appropriate size for an experimental study for various simple linear regression models and tuning parameter values of the covariance structure used under an asymptotic domain, an Ornstein-Uhlenbeck process. Selecting an appropriate sample size is determined based on the percent of total variation that is captured at any given sample size for each parameter. Additionally, guidance on the selection of the tuning parameter is given by linking this value to the signal-to-noise ratio utilized for power calculations under design of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05969v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cory W. Natoli, Edward D. White, Beau A. Nunnally, Alex J. Gutman, Raymond R. Hill</dc:creator>
    </item>
    <item>
      <title>Societal and scientific impact of policy research: A large-scale empirical study of some explanatory factors using Altmetric and Overton</title>
      <link>https://arxiv.org/abs/2403.06714</link>
      <description>arXiv:2403.06714v1 Announce Type: cross 
Abstract: This study investigates how scientific research influences policymaking by analyzing citations of research articles in policy documents (policy impact) for nearly 125,000 articles across 434 public policy journals. We reveal distinct citation patterns between policymakers and other stakeholders like researchers, journalists, and the public. News and blog mentions, social media engagement, and open access publications (excluding fully open access) significantly increase the likelihood of a research article being cited in policy documents. Conversely, articles locked behind paywalls and those published under the full open access model (based on Altmetric data) have a lower chance of being policy-cited. Publication year and policy type show no significant influence. Our findings emphasize the crucial role of science communication channels like news media and social media in bridging the gap between research and policy. Interestingly, academic citations hold a weaker influence on policy citations compared to news mentions, suggesting a potential disconnect between how researchers reference research and how policymakers utilize it. This highlights the need for improved communication strategies to ensure research informs policy decisions more effectively. This study provides valuable insights for researchers, policymakers, and science communicators. Researchers can tailor their dissemination efforts to reach policymakers through media channels. Policymakers can leverage these findings to identify research with higher policy relevance. Science communicators can play a critical role in translating research for policymakers and fostering dialogue between the scientific and policymaking communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06714v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Dorta-Gonz\'alez, Alejandro Rodr\'iguez-Caro, Mar\'ia Isabel Dorta-Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>Communication network dynamics in a large organizational hierarchy</title>
      <link>https://arxiv.org/abs/2208.01208</link>
      <description>arXiv:2208.01208v2 Announce Type: replace 
Abstract: Most businesses impose a supervisory hierarchy on employees to facilitate management, decision-making, and collaboration, yet routine inter-employee communication patterns within workplaces tend to emerge more naturally as a consequence of both supervisory relationships and the needs of the organization. What then is the relationship between a formal organizational structure and the emergent communications between its employees? Understanding the nature of this relationship is critical for the successful management of an organization. While scholars of organizational management have proposed theories relating organizational trees to communication dynamics, and separately, network scientists have studied the topological structure of communication patterns in different types of organizations, existing empirical analyses are both lacking in representativeness and limited in size. In fact, much of the methodology used to study the relationship between organizational hierarchy and communication patterns comes from analyses of the Enron email corpus, reflecting a uniquely dysfunctional corporate environment. In this paper, we develop new methodology for assessing the relationship between organizational hierarchy and communication dynamics and apply it to Microsoft Corporation, currently the highest valued company in the world, consisting of approximately 200,000 employees divided into 88 teams. This reveals distinct communication network structures within and between teams. We then characterize the relationship of routine employee communication patterns to these team supervisory hierarchies, while empirically evaluating several theories of organizational management and performance. To do so, we propose new measures of communication reciprocity and new shortest-path distances for trees to track the frequency of messages passed up, down, and across the organizational hierarchy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.01208v2</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Josephs, Sida Peng, Forrest W. Crawford</dc:creator>
    </item>
    <item>
      <title>Filling the Gaps: A Multiple Imputation Approach to Estimating Aging Curves in Baseball</title>
      <link>https://arxiv.org/abs/2210.02383</link>
      <description>arXiv:2210.02383v3 Announce Type: replace 
Abstract: In sports, an aging curve depicts the relationship between average performance and age in athletes' careers. This paper investigates the aging curves for offensive players in Major League Baseball. We study this problem in a missing data context and account for different types of dropouts of baseball players during their careers. We employ a multiple imputation framework for multilevel data to impute the player performance associated with the missing seasons, and estimate the aging curves based on the imputed datasets. We then evaluate the effects of different dropout mechanisms on the aging curves through simulation, before applying our method to analyze MLB player data from past seasons. Results suggest an overestimation of the aging curves constructed without considering the unobserved seasons, whereas estimates obtained from multiple imputation address this shortcoming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.02383v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quang Nguyen, Gregory J. Matthews</dc:creator>
    </item>
    <item>
      <title>Bayesian Federated Inference for estimating Statistical Models based on Non-shared Multicenter Data sets</title>
      <link>https://arxiv.org/abs/2302.07677</link>
      <description>arXiv:2302.07677v2 Announce Type: replace 
Abstract: Identifying predictive factors for an outcome of interest via a multivariable analysis is often difficult when the data set is small. Combining data from different medical centers into a single (larger) database would alleviate this problem, but is in practice challenging due to regulatory and logistic problems. Federated Learning (FL) is a machine learning approach that aims to construct from local inferences in separate data centers what would have been inferred had the data sets been merged. It seeks to harvest the statistical power of larger data sets without actually creating them. The FL strategy is not always efficient and precise. Therefore, in this paper we refine and implement an alternative Bayesian Federated Inference (BFI) framework for multicenter data with the same aim as FL. The BFI framework is designed to cope with small data sets by inferring locally not only the optimal parameter values, but also additional features of the posterior parameter distribution, capturing information beyond what is used in FL. BFI has the additional benefit that a single inference cycle across the centers is sufficient, whereas FL needs multiple cycles. We quantify the performance of the proposed methodology on simulated and real life data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07677v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marianne A. Jonker, Hassan Pazira, Anthony CC Coolen</dc:creator>
    </item>
    <item>
      <title>Nonparametric Modeling of Diffusion MRI Signal in Q-space</title>
      <link>https://arxiv.org/abs/2304.14188</link>
      <description>arXiv:2304.14188v3 Announce Type: replace 
Abstract: This paper describes a novel nonparametric model for modeling diffusion MRI signals in q-space. In q-space, diffusion MRI signal is measured for a sequence of magnetic strengths (b-values) and magnetic gradient directions (b-vectors). We propose a Poly-RBF model, which employs a bidirectional framework with polynomial bases to model the signal along the b-value direction and Gaussian radial bases across the b-vectors. The model can accommodate sparse data on b-values and moderately dense data on b-vectors. The utility of Poly-RBF is inspected for two applications: 1) prediction of the dMRI signal, and 2) harmonization of dMRI data collected under different acquisition protocols with different scanners. Our results indicate that the proposed Poly-RBF model can more accurately predict the unmeasured diffusion signal than its competitors such as the Gaussian process model in {\tt Eddy} of FSL. Applying it to harmonizing the diffusion signal can significantly improve the reproducibility of derived white matter microstructure measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.14188v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Zhou Lan, Zhengwu Zhang</dc:creator>
    </item>
    <item>
      <title>The piranha problem: Large effects swimming in a small pond</title>
      <link>https://arxiv.org/abs/2105.13445</link>
      <description>arXiv:2105.13445v4 Announce Type: replace-cross 
Abstract: In some scientific fields, it is common to have certain variables of interest that are of particular importance and for which there are many studies indicating a relationship with different explanatory variables. In such cases, particularly those where no relationships are known among the explanatory variables, it is worth asking under what conditions it is possible for all such claimed effects to exist simultaneously. This paper addresses this question by reviewing some theorems from multivariate analysis showing that, unless the explanatory variables also have sizable dependencies with each other, it is impossible to have many such large effects. We discuss implications for the replication crisis in social science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.13445v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Tosh, Philip Greengard, Ben Goodrich, Andrew Gelman, Aki Vehtari, Daniel Hsu</dc:creator>
    </item>
    <item>
      <title>Incorporating nonparametric methods for estimating causal excursion effects in mobile health with zero-inflated count outcomes</title>
      <link>https://arxiv.org/abs/2310.18905</link>
      <description>arXiv:2310.18905v2 Announce Type: replace-cross 
Abstract: In mobile health, tailoring interventions for real-time delivery is of paramount importance. Micro-randomized trials have emerged as the "gold-standard" methodology for developing such interventions. Analyzing data from these trials provides insights into the efficacy of interventions and the potential moderation by specific covariates. The "causal excursion effect", a novel class of causal estimand, addresses these inquiries. Yet, existing research mainly focuses on continuous or binary data, leaving count data largely unexplored. The current work is motivated by the Drink Less micro-randomized trial from the UK, which focuses on a zero-inflated proximal outcome, i.e., the number of screen views in the subsequent hour following the intervention decision point. To be specific, we revisit the concept of causal excursion effect, specifically for zero-inflated count outcomes, and introduce novel estimation approaches that incorporate nonparametric techniques. Bidirectional asymptotics are established for the proposed estimators. Simulation studies are conducted to evaluate the performance of the proposed methods. As an illustration, we also implement these methods to the Drink Less trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18905v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xueqing Liu, Tianchen Qian, Lauren Bell, Bibhas Chakraborty</dc:creator>
    </item>
    <item>
      <title>Optimizing Heat Alert Issuance with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2312.14196</link>
      <description>arXiv:2312.14196v2 Announce Type: replace-cross 
Abstract: A key strategy in societal adaptation to climate change is the use of alert systems to reduce the adverse health impacts of extreme heat events by prompting preventative action. In this work, we investigate reinforcement learning (RL) as a tool to optimize the effectiveness of such systems. Our contributions are threefold. First, we introduce a novel RL environment enabling the evaluation of the effectiveness of heat alert policies to reduce heat-related hospitalizations. The rewards model is trained from a comprehensive dataset of historical weather, Medicare health records, and socioeconomic/geographic features. We use variational Bayesian techniques to address low-signal effects and spatial heterogeneity, which are commonly encountered in climate &amp; health settings. The transition model incorporates real historical weather patterns enriched by a data augmentation mechanism based on climate region similarity. Second, we use this environment to evaluate standard RL algorithms in the context of heat alert issuance. Our analysis shows that policy constraints are needed to improve the initially poor performance of RL. Lastly, a post hoc contrastive analysis provides insight into scenarios where our modified heat alert-RL policies yield significant gains/losses over the current National Weather Service alert policy in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14196v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ellen M. Considine, Rachel C. Nethery, Gregory A. Wellenius, Francesca Dominici, Mauricio Tec</dc:creator>
    </item>
    <item>
      <title>Assessment of Sports Concussion in Female Athletes: A Role for Neuroinformatics?</title>
      <link>https://arxiv.org/abs/2401.13045</link>
      <description>arXiv:2401.13045v2 Announce Type: replace-cross 
Abstract: Over the past decade, the intricacies of sports-related concussions among female athletes have become readily apparent. Traditional clinical methods for diagnosing concussions suffer limitations when applied to female athletes, often failing to capture subtle changes in brain structure and function. Advanced neuroinformatics techniques and machine learning models have become invaluable assets in this endeavor. While these technologies have been extensively employed in understanding concussion in male athletes, there remains a significant gap in our comprehension of their effectiveness for female athletes. With its remarkable data analysis capacity, machine learning offers a promising avenue to bridge this deficit. By harnessing the power of machine learning, researchers can link observed phenotypic neuroimaging data to sex-specific biological mechanisms, unraveling the mysteries of concussions in female athletes. Furthermore, embedding methods within machine learning enable examining brain architecture and its alterations beyond the conventional anatomical reference frame. In turn, allows researchers to gain deeper insights into the dynamics of concussions, treatment responses, and recovery processes. To guarantee that female athletes receive the optimal care they deserve, researchers must employ advanced neuroimaging techniques and sophisticated machine-learning models. These tools enable an in-depth investigation of the underlying mechanisms responsible for concussion symptoms stemming from neuronal dysfunction in female athletes. This paper endeavors to address the crucial issue of sex differences in multimodal neuroimaging experimental design and machine learning approaches within female athlete populations, ultimately ensuring that they receive the tailored care they require when facing the challenges of concussions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13045v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel Edelstein, Sterling Gutterman, Benjamin Newman, John Darrell Van Horn</dc:creator>
    </item>
    <item>
      <title>Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population</title>
      <link>https://arxiv.org/abs/2401.14512</link>
      <description>arXiv:2401.14512v3 Announce Type: replace-cross 
Abstract: Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We apply our methodology to extend inferences from the Starting Treatment with Agonist Replacement Therapies (START) trial -- investigating the effectiveness of medication for opioid use disorder -- to the real-world population represented by the Treatment Episode Dataset: Admissions (TEDS-A). By refining target populations using ROOT, our framework offers a systematic approach to enhance decision-making accuracy and inform future trials in diverse populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14512v3</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Harsh Parikh, Rachael Ross, Elizabeth Stuart, Kara Rudolph</dc:creator>
    </item>
    <item>
      <title>Fusing Individualized Treatment Rules Using Secondary Outcomes</title>
      <link>https://arxiv.org/abs/2402.08828</link>
      <description>arXiv:2402.08828v3 Announce Type: replace-cross 
Abstract: An individualized treatment rule (ITR) is a decision rule that recommends treatments for patients based on their individual feature variables. In many practices, the ideal ITR for the primary outcome is also expected to cause minimal harm to other secondary outcomes. Therefore, our objective is to learn an ITR that not only maximizes the value function for the primary outcome, but also approximates the optimal rule for the secondary outcomes as closely as possible. To achieve this goal, we introduce a fusion penalty to encourage the ITRs based on different outcomes to yield similar recommendations. Two algorithms are proposed to estimate the ITR using surrogate loss functions. We prove that the agreement rate between the estimated ITR of the primary outcome and the optimal ITRs of the secondary outcomes converges to the true agreement rate faster than if the secondary outcomes are not taken into consideration. Furthermore, we derive the non-asymptotic properties of the value function and misclassification rate for the proposed method. Finally, simulation studies and a real data example are used to demonstrate the finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08828v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daiqi Gao, Yuanjia Wang, Donglin Zeng</dc:creator>
    </item>
    <item>
      <title>Improving generalisation via anchor multivariate analysis</title>
      <link>https://arxiv.org/abs/2403.01865</link>
      <description>arXiv:2403.01865v2 Announce Type: replace-cross 
Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01865v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Homer Durand, Gherardo Varando, Nathan Mankovich, Gustau Camps-Valls</dc:creator>
    </item>
  </channel>
</rss>
