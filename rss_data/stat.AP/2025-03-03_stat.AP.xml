<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Mar 2025 05:01:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Modelling the Spatially Varying Non-Linear Effects of Heat Exposure</title>
      <link>https://arxiv.org/abs/2502.20745</link>
      <description>arXiv:2502.20745v1 Announce Type: new 
Abstract: Exposure to high ambient temperatures is a significant driver of preventable mortality, with non-linear health effects and elevated risks in specific regions. To capture this complexity and account for spatial dependencies across small areas, we propose a Bayesian framework that integrates non-linear functions with the Besag, York, and Mollie (BYM2) model. Applying this framework to all-cause mortality data in Switzerland, we quantified spatial inequalities in heat-related mortality. We retrieved daily all-cause mortality at small areas (2,145 municipalities) for people older than 65 years from the Swiss Federal Office of Public Health and daily mean temperature at 1km$\times$1km grid from the Swiss Federal Office of Meteorology. By fully propagating uncertainties, we derived key epidemiological metrics, including heat-related excess mortality and minimum mortality temperature (MMT). Heat-related excess mortality rates were higher in northern Switzerland, while lower MMTs were observed in mountainous regions. Further, we explored the role of the proportion of individuals older than 85 years, green space, average temperature, deprivation, urbanicity, and language regions in explaining these discrepancies. We found that spatial disparities in heat-related excess mortality were primarily driven by population age distribution, green space, and vulnerabilities associated with elevated temperature exposure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20745v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Chen, Marta Blangiardo, Connor Gascoigne, Garyfallos Konstantinoudis</dc:creator>
    </item>
    <item>
      <title>Collective Reasoning Among LLMs A Framework for Answer Validation Without Ground Truth</title>
      <link>https://arxiv.org/abs/2502.20758</link>
      <description>arXiv:2502.20758v1 Announce Type: new 
Abstract: We present a collaborative framework where multiple large language models, namely GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and Gemini-1.5-Flash, work together to generate and respond to complex PhD-level probability questions in the absence of definitive ground truth. This study explores how inter-model consensus enhances response reliability and serves as a proxy for assessing the quality of generated questions. To quantify agreement and consistency, we employ statistical methods including chi-square tests, Fleiss' Kappa, and confidence interval analysis, measuring both response precision and question clarity. Our findings highlight that Claude and Gemini generate well-structured and less ambiguous questions, leading to higher inter-model agreement. This is reflected in their narrower confidence intervals and stronger alignment with answering models. Conversely, LLaMA demonstrates increased variability and lower reliability in question formulation, as indicated by broader confidence intervals and reduced consensus rates. These results suggest that multi-model collaboration not only enhances the reliability of responses but also provides a valuable framework for assessing and improving question quality in the absence of explicit ground truth. This research offers meaningful insights into optimizing AI-driven reasoning through collaborative large-language model interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20758v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Seyed Pouyan Mousavi Davoudi, Alireza Shafiee Fard, Alireza Amiri-Margavi</dc:creator>
    </item>
    <item>
      <title>Adding smoothing splines to the SAM model improves stock assessment</title>
      <link>https://arxiv.org/abs/2502.20788</link>
      <description>arXiv:2502.20788v1 Announce Type: new 
Abstract: The stock assessment model SAM contains a large number of age-dependent parameters that must be manually grouped together to obtain robust inference. This can make the model selection process slow, non-extensive and highly subjective, while producing unrealistic looking parameter estimates with discrete jumps. We propose to model age-dependent SAM parameters using smoothing spline functions. This can lead to more smooth parameter estimates, while speeding up and making the model selection process more automatic and less subjective. We develop different spline models and compare them with already existing SAM models for a selection of 17 different fish stocks, using cross- and forward-validation methods. The results show that our automated spline models overall outcompete the officially developed SAM models. We also demonstrate how the developed spline models can be employed as a diagnostics tool for improving and better understanding properties of the officially developed SAM models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20788v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silius M. Vandeskog, Magne Aldrin, Daniel Howell, Edvin Fuglebakk</dc:creator>
    </item>
    <item>
      <title>Forecasting Monthly Residential Natural Gas Demand Using Just-In-Time-Learning Modeling</title>
      <link>https://arxiv.org/abs/2502.20989</link>
      <description>arXiv:2502.20989v1 Announce Type: new 
Abstract: Natural gas (NG) is relatively a clean source of energy, particularly compared to fossil fuels, and worldwide consumption of NG has been increasing almost linearly in the last two decades. A similar trend can also be seen in Turkey, while another similarity is the high dependence on imports for the continuous NG supply. It is crucial to accurately forecast future NG demand (NGD) in Turkey, especially, for import contracts; in this respect, forecasts of monthly NGD for the following year are of utmost importance. In the current study, the historical monthly NG consumption data between 2014 and 2024 provided by SOCAR, the local residential NG distribution company for two cities in Turkey, Bursa and Kayseri, was used to determine out-of-sample monthly NGD forecasts for a period of one year and nine months using various time series models, including SARIMA and ETS models, and a novel proposed machine learning method. The proposed method, named Just-in-Time-Learning-Gaussian Process Regression (JITL-GPR), uses a novel feature representation for the past NG demand values; instead of using past demand values as column-wise separate features, they are placed on a two-dimensional (2-D) grid of year-month values. For each test point, a kernel function, tailored for the NGD predictions, is used in GPR to predict the query point. Since a model is constructed separately for each test point, the proposed method is, indeed, an example of JITL. The JITL-GPR method is easy to use and optimize, and offers a reduction in forecast errors compared to traditional time series methods and a state-of-the-art combination model; therefore, it is a promising tool for NGD forecasting in similar settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20989v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Burak Alakent, Erkan Isikli, Cigdem Kadaifci, Tonguc S. Taspinar</dc:creator>
    </item>
    <item>
      <title>Negative correlations in Ising models of credit risk</title>
      <link>https://arxiv.org/abs/2502.21199</link>
      <description>arXiv:2502.21199v1 Announce Type: new 
Abstract: We analyze a subclass of Ising models in the context of credit risk, focusing on Dandelion models when the correlations $\rho$ between the central node and each non-central node are negative. We establish the possible range of values for $\rho$ and derive an explicit formula linking the correlation between any pair of non-central nodes to $\rho$. The paper concludes with a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21199v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiara Emonti, Roberto Fontana</dc:creator>
    </item>
    <item>
      <title>A decision analysis model for colorectal cancer screening</title>
      <link>https://arxiv.org/abs/2502.21210</link>
      <description>arXiv:2502.21210v1 Announce Type: new 
Abstract: Background and Objective. With minor differences, most national colorectal cancer (CRC) screening programs in Europe consist of one-size-fits-all aged-based strategies. This paper provides a decision analysis-based approach to personalized CRC screening, supporting decisions concerning whether and which screening method to consider and/or whether a colonoscopy should be administered.
  Methods. We use an influence diagram which characterizes CRC risk with respect to different variables of interest and includes comfort, costs, complications, and information as decision criteria, the last one assessed through information theory measures. The criteria are integrated with a multi-attribute utility model. Optimal screening policies are then computed.
  Results. The proposed model is used to support personalized individual screening based on relevant characteristics. It serves to assess existing national screening programs and design new ones. In particular, it suggests replacing current age-based strategies followed in many European countries by more personalized strategies based on the type of model proposed. Additionally, the model facilitates benchmarking of novel screening devices.
  Conclusions. This work creates a framework supporting personalized CRC screening improving upon current age-based screening strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21210v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daniel Corrales, David R\'ios Insua, Marino J. Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>Modeling times to multiple events under informative censoring with C-vine copula</title>
      <link>https://arxiv.org/abs/2502.20608</link>
      <description>arXiv:2502.20608v1 Announce Type: cross 
Abstract: The study of times to nonterminal events of different types and their interrelation is a compelling area of interest. The primary challenge in analyzing such multivariate event times is the presence of informative censoring by the terminal event. While numerous statistical methods have been proposed for a single nonterminal event, i.e., semi-competing risks data, there remains a dearth of tools for analyzing times to multiple nonterminal events. These events involve more complex dependence structures between nonterminal and terminal events and between nonterminal events themselves. This paper introduces a novel modeling framework leveraging the vine copula to directly estimate the joint distribution of the multivariate times to nonterminal and terminal events. Unlike the few existing methods based on multivariate or nested copulas, our model excels in capturing the heterogeneous dependence between each pair of event times in terms of strength and structure. Furthermore, our model allows regression modeling for all the marginal distributions of times to nonterminal and terminal events, a feature lacking in existing methods. We propose a likelihood-based estimation and inference procedure, which can be implemented efficiently in sequential stages. Through simulation studies, we demonstrate the satisfactory finite-sample performance of our proposed stage-wise estimators and analytical variance estimators, as well as their superiority over existing methods. We apply our approach to data from a crowdfunding platform to investigate the relationship between creator-backer interactions of various types and a creator's lifetime on the platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20608v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Yiwei Li, Qian M. Zhou</dc:creator>
    </item>
    <item>
      <title>Nonparanormal Modeling Framework for Prognostic Biomarker Assessment with Application to Amyotrophic Lateral Sclerosis</title>
      <link>https://arxiv.org/abs/2502.20892</link>
      <description>arXiv:2502.20892v1 Announce Type: cross 
Abstract: Identifying reliable biomarkers for predicting clinical events in longitudinal studies is important for accurate disease prognosis and the development of new treatments. However, prognostic studies are often not randomized, making it difficult to account for patient heterogeneity. In amyotrophic lateral sclerosis (ALS), factors such as age, site of disease onset and genetics impact both survival duration and biomarker levels, yet their impact on the prognostic accuracy of biomarkers over different time horizons remains unclear. While existing methods for time-dependent receiver operating characteristic (ROC) analysis have been adapted for censored time-to-event outcomes, most do not adjust for patient covariates. To address this, we propose the nonparanormal prognostic biomarker (NPB) framework, which models the joint dependence between biomarker and event time distributions while accounting for covariates. This provides covariate-specific ROC curves which assess a potential biomarker's accuracy for a given time horizon. We apply this framework to evaluate serum neurofilament light (NfL) as a biomarker in ALS and demonstrate that its prognostic accuracy varies over time and across patient subgroups. The NPB framework is broadly applicable to other conditions and has the potential to improve clinical trial efficiency by refining patient stratification and reducing sample size requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20892v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ainesh Sewak, Vanda Inacio, Joanne Wuu, Michael Benatar, Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>When Respondents Don't Care Anymore: Identifying the Onset of Careless Responding</title>
      <link>https://arxiv.org/abs/2303.07167</link>
      <description>arXiv:2303.07167v3 Announce Type: replace-cross 
Abstract: Questionnaires in the behavioral and organizational sciences tend to be lengthy. However, literature suggests that survey length is a contributing factor to careless responding, with longer questionnaires yielding higher probability that participants start responding carelessly. Consequently, in long surveys a large number of participants may engage in careless responding, posing a major threat to internal validity. We propose a novel method for identifying the onset of careless responding (or an absence thereof) that searches for a changepoint in combined measurements of multiple dimensions in which carelessness may manifest, such as inconsistency and invariability. It is highly flexible, based on machine learning, and provides statistical guarantees for controlling the false positive rate. In simulation experiments, the proposed method achieves high accuracy in identifying carelessness onset and discriminates well between attentive and various types of careless responding, even when a large number of careless respondents are present. An empirical application highlights how identifying partial carelessness uncovers novel insights on careless responding behavior. Furthermore, we provide the freely available open source software package "carelessonset" to facilitate adoption by empirical researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07167v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Welz, Andreas Alfons</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium</title>
      <link>https://arxiv.org/abs/2402.02303</link>
      <description>arXiv:2402.02303v5 Announce Type: replace-cross 
Abstract: The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02303v5</guid>
      <category>math.ST</category>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>A Deep Learning Approach to Multi-Fiber Parameter Estimation and Uncertainty Quantification in Diffusion MRI</title>
      <link>https://arxiv.org/abs/2405.13655</link>
      <description>arXiv:2405.13655v2 Announce Type: replace-cross 
Abstract: Diffusion MRI (dMRI) is the primary imaging modality used to study brain microstructure in vivo. Reliable and computationally efficient parameter inference for common dMRI biophysical models is a challenging inverse problem, due to factors such as variable dimensionalities (reflecting the unknown number of distinct white matter fiber populations in a voxel), low signal-to-noise ratios, and non-linear forward models. These challenges have led many existing methods to use biologically implausible simplified models to stabilize estimation, for instance, assuming shared microstructure across all fiber populations within a voxel. In this work, we introduce a novel sequential method for multi-fiber parameter inference that decomposes the task into a series of manageable subproblems. These subproblems are solved using deep neural networks tailored to problem-specific structure and symmetry, and trained via simulation. The resulting inference procedure is largely amortized, enabling scalable parameter estimation and uncertainty quantification across all model parameters. Simulation studies and real imaging data analysis using the Human Connectome Project (HCP) demonstrate the advantages of our method over standard alternatives. In the case of the standard model of diffusion, our results show that under HCP-like acquisition schemes, estimates for extra-cellular parallel diffusivity are highly uncertain, while those for the intra-cellular volume fraction can be estimated with relatively high precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13655v2</guid>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Consagra, Lipeng Ning, Yogesh Rathi</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Attributable Effects in Case$^2$ Studies</title>
      <link>https://arxiv.org/abs/2405.16046</link>
      <description>arXiv:2405.16046v2 Announce Type: replace-cross 
Abstract: The case$^2$ study, also referred to as the case-case study design, is a valuable approach for conducting inference for treatment effects. Unlike traditional case-control studies, the case$^2$ design compares treatment in two types of cases with the same disease. A key quantity of interest is the attributable effect, which is the number of cases of disease among treated units which are caused by the treatment. Two key assumptions that are usually made for making inferences about the attributable effect in case$^2$ studies are 1.) treatment does not cause the second type of case, and 2.) the treatment does not alter an individual's case type. However, these assumptions are not realistic in many real-data applications. In this article, we present a sensitivity analysis framework to scrutinize the impact of deviations from these assumptions on obtained results. We also include sensitivity analyses related to the assumption of unmeasured confounding, recognizing the potential bias introduced by unobserved covariates. The proposed methodology is exemplified through an investigation into whether having violent behavior in the last year of life increases suicide risk via 1993 National Mortality Followback Survey dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16046v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kan Chen, Ting Ye, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Revisiting Optimal Proportions for Binary Responses: Insights from Incorporating the Absent Perspective of Type-I Error Rate Control</title>
      <link>https://arxiv.org/abs/2502.06381</link>
      <description>arXiv:2502.06381v2 Announce Type: replace-cross 
Abstract: This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation - an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive two optimal proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power and the other minimizes the total number failures in a trial while maintaining a predefined power level. Through simulations based on an early-phase and a confirmatory trial we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials and alternative measures of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06381v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Sof\'ia S. Villar, William F. Rosenberger</dc:creator>
    </item>
    <item>
      <title>Stochastic Block Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2502.11332</link>
      <description>arXiv:2502.11332v2 Announce Type: replace-cross 
Abstract: Motivated by a neuroscience application we study the problem of statistical estimation of a high-dimensional covariance matrix with a block structure. The block model embeds a structural assumption: the population of items (neurons) can be divided into latent sub-populations with shared associative covariation within blocks and shared associative or dis-associative covariation across blocks. Unlike the block diagonal assumption, our block structure incorporates positive or negative pairwise correlation between blocks. In addition to offering reasonable modeling choices in neuroscience and economics, the block covariance matrix assumption is interesting purely from the perspective of statistical estimation theory: (a) it offers in-built dimension reduction and (b) it resembles a regularized factor model without the need of choosing the number of factors. We discuss a hierarchical Bayesian estimation method to simultaneously recover the latent blocks and estimate the overall covariance matrix. We show with numerical experiments that a hierarchical structure and a shrinkage prior are essential to accurate recovery when several blocks are present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11332v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunran Chen, Surya T Tokdar, Jennifer M Groh</dc:creator>
    </item>
  </channel>
</rss>
