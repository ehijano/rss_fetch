<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Apr 2025 04:01:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An introduction to R package `mvs`</title>
      <link>https://arxiv.org/abs/2504.17546</link>
      <description>arXiv:2504.17546v1 Announce Type: new 
Abstract: In biomedical science, a set of objects or persons can often be described by multiple distinct sets of features obtained from different data sources or modalities (called "multi-view data"). Classical machine learning methods ignore the multi-view structure of such data, limiting model interpretability and performance. The R package `mvs` provides methods that were designed specifically for dealing with multi-view data, based on the multi-view stacking (MVS) framework. MVS is a form of supervised (machine) learning used to train multi-view classification or prediction models. MVS works by training a learning algorithm on each view separately, estimating the predictive power of each view-specific model through cross-validation, and then using another learning algorithm to assign weights to the view-specific models based on their estimated predictions. MVS is a form of ensemble learning, dividing the large multi-view learning problem into smaller sub-problems. Most of these sub-problems can be solved in parallel, making it computationally attractive. Additionally, the number of features of the sub-problems is greatly reduced compared with the full multi-view learning problem. This makes MVS especially useful when the total number of features is larger than the number of observations (i.e., high-dimensional data). MVS can still be applied even if the sub-problems are themselves high-dimensional by adding suitable penalty terms to the learning algorithms. Furthermore, MVS can be used to automatically select the views which are most important for prediction. The R package `mvs` makes fitting MVS models, including such penalty terms, easily and openly accessible. `mvs` allows for the fitting of stacked models with any number of levels, with different penalty terms, different outcome distributions, and provides several options for missing data handling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17546v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter van Loon</dc:creator>
    </item>
    <item>
      <title>Fuzzy clustering and community detection: an integrated approach</title>
      <link>https://arxiv.org/abs/2504.17733</link>
      <description>arXiv:2504.17733v1 Announce Type: new 
Abstract: This paper addresses the ambitious goal of merging two different approaches to group detection in complex domains: one based on fuzzy clustering and the other on community detection theory. To achieve this, two clustering algorithms are proposed: Fuzzy C-Medoids Clustering with Modularity Spatial Correction and Fuzzy C-Modes Clustering with Modularity Spatial Correction. The former is designed for quantitative data, while the latter is intended for qualitative data. The concept of fuzzy modularity is introduced into the standard objective function of fuzzy clustering algorithms as a spatial regularization term, whose contribution to the clustering criterion based on attributes is controlled by an exogenous parameter. An extensive simulation study is conducted to support the theoretical framework, complemented by two applications to real-world data related to the theme of sustainability. The first application involves data from the 2030 Agenda for Sustainable Development, while the second focuses on urban green spaces in Italian provincial capitals and metropolitan cities. Both the simulation results and the applications demonstrate the advantages of this new methodological proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17733v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Domenico Cangemi, Pierpaolo D'Urso, Livia De Giovanni, Lorenzo Federico, Vincenzina Vitale</dc:creator>
    </item>
    <item>
      <title>A Delayed Acceptance Auxiliary Variable MCMC for Spatial Models with Intractable Likelihood Function</title>
      <link>https://arxiv.org/abs/2504.17147</link>
      <description>arXiv:2504.17147v1 Announce Type: cross 
Abstract: A large class of spatial models contains intractable normalizing functions, such as spatial lattice models, interaction spatial point processes, and social network models. Bayesian inference for such models is challenging since the resulting posterior distribution is doubly intractable. Although auxiliary variable MCMC (AVM) algorithms are known to be the most practical, they are computationally expensive due to the repeated auxiliary variable simulations. To address this, we propose delayed-acceptance AVM (DA-AVM) methods, which can reduce the number of auxiliary variable simulations. The first stage of the kernel uses a cheap surrogate to decide whether to accept or reject the proposed parameter value. The second stage guarantees detailed balance with respect to the posterior. The auxiliary variable simulation is performed only on the parameters accepted in the first stage. We construct various surrogates specifically tailored for doubly intractable problems, including subsampling strategy, Gaussian process emulation, and frequentist estimator-based approximation. We validate our method through simulated and real data applications, demonstrating its practicality for complex spatial models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17147v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jong Hyeon Lee, Jongmin Kim, Heesang Lee, Jaewoo Park</dc:creator>
    </item>
    <item>
      <title>Practical aspects of the virtual noise convex optimum design approach for correlated responses</title>
      <link>https://arxiv.org/abs/2504.17651</link>
      <description>arXiv:2504.17651v1 Announce Type: cross 
Abstract: In this paper we present several practically-oriented extensions and considerations for the virtual noise method in optimal design under correlation. First we introduce a slightly modified virtual noise representation which further illuminates the parallels to the classical design approach for uncorrelated observations. We suggest more efficient algorithms to obtain the design measures. Furthermore, we show that various convex relaxation methods used for sensor selection are special cases of our approach and can be solved within our framework. Finally, we provide practical guidelines on how to generally approach a design problem with correlated observations and demonstrate how to utilize the virtual noise method in this context in a meaningful way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17651v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Hainy, Werner G. M\"uller, Andrej P\'azman</dc:creator>
    </item>
    <item>
      <title>regMMD: An R package for parametric estimation and regression with maximum mean discrepancy</title>
      <link>https://arxiv.org/abs/2503.05297</link>
      <description>arXiv:2503.05297v2 Announce Type: replace 
Abstract: The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for nonparametric tests and estimation. Recently, it has also been studied as an objective function for parametric estimation, as it has been shown to yield robust estimators. We have implemented MMD minimization for parameter inference in a wide range of statistical models, including various regression models, within an R package called regMMD. This paper provides an introduction to the regMMD package. We describe the available kernels and optimization procedures, as well as the default settings. Detailed applications to simulated and real data are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05297v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Alquier, Mathieu Gerber</dc:creator>
    </item>
    <item>
      <title>Prior Sensitivity Analysis without Model Re-fit</title>
      <link>https://arxiv.org/abs/2409.19729</link>
      <description>arXiv:2409.19729v2 Announce Type: replace-cross 
Abstract: Prior sensitivity analysis is a fundamental method to check the effects of prior distributions on the posterior distribution in Bayesian inference. Exploring the posteriors under several alternative priors can be computationally intensive, particularly for complex latent variable models. To address this issue, we propose a novel method for quantifying the prior sensitivity that does not require model re-fit. Specifically, we present a method to compute the Hellinger and Kullback-Leibler distances between two posterior distributions with base and alternative priors, using Monte Carlo integration based only on the base posterior distribution, through novel integral expressions of the two distances. We also extend the above approach for assessing the influence of hyperpriors in general latent variable models. We demonstrate the proposed method through examples of a simple normal distribution model, hierarchical binomial-beta model, and Gaussian process regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19729v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Exponential speed up in Monte Carlo sampling through Radial Updates</title>
      <link>https://arxiv.org/abs/2411.18218</link>
      <description>arXiv:2411.18218v2 Announce Type: replace-cross 
Abstract: Recently, it has been shown that the hybrid Monte Carlo (HMC) algorithm is guaranteed to converge exponentially to a given target probability distribution $p(x)\propto e^{-V(x)}$ on non-compact spaces if augmented by an appropriate radial update. In this work we present a simple way to derive efficient radial updates meeting the necessary requirements for any potential $V$. We reduce the problem to finding a substitution for the radial direction $||x||=f(z)$ so that the effective potential $V(f(z))$ grows exponentially with $z\rightarrow\pm\infty$. Any additive update of $z$ then leads to the desired convergence. We show that choosing this update from a normal distribution with standard deviation $\sigma\approx 1/\sqrt{d}$ in $d$ dimensions yields very good results. We further generalise the previous results on radial updates to a wide class of Markov chain Monte Carlo (MCMC) algorithms beyond the HMC and we quantify the convergence behaviour of MCMC algorithms with badly chosen radial update. Finally, we apply the radial update to the sampling of heavy-tailed distributions and achieve a speed up of many orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18218v2</guid>
      <category>physics.comp-ph</category>
      <category>cs.NA</category>
      <category>hep-lat</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johann Ostmeyer</dc:creator>
    </item>
    <item>
      <title>Theoretical and Practical Limits of Signal Strength Estimate Precision for Kolmogorov-Zurbenko Periodograms with Dynamic Smoothing</title>
      <link>https://arxiv.org/abs/2412.07735</link>
      <description>arXiv:2412.07735v2 Announce Type: replace-cross 
Abstract: This investigation establishes the theoretical and practical limits of signal strength estimate precision for Kolmogorov-Zurbenko periodograms with dynamic smoothing and compares them to those of standard log-periodograms with static smoothing. Previous research has established the sensitivity, accuracy, resolution, and robustness of Kolmogorov-Zurbenko periodograms with dynamic smoothing in estimating signal frequencies. However, the precision with which they estimate signal strength has never been evaluated. To this point, the width of the confidence interval for a signal strength estimate can serve as a criterion for assessing the precision of such estimates: the narrower the confidence interval, the more precise the estimate. The statistical background for confidence intervals of periodograms is presented, followed by candidate functions to compute and plot them when using Kolmogorov-Zurbenko periodograms with dynamic smoothing. Given an identified signal frequency, a static smoothing window and its smoothing window width can be selected such that its confidence interval is narrower and, thus, its signal strength estimate more precise, than that of dynamic smoothing windows, all while maintaining a level of frequency resolution as good as or better than that of a dynamic smoothing window. These findings suggest the need for a two-step protocol in spectral analysis: computation of a Kolmogorov-Zurbenko periodogram with dynamic smoothing to detect, identify, and separate signal frequencies, followed by computation of a Kolmogorov-Zurbenko periodogram with static smoothing to precisely estimate signal strength and compute its confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07735v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barry Loneck, Igor Zurbenko, Edward Valachovic</dc:creator>
    </item>
  </channel>
</rss>
