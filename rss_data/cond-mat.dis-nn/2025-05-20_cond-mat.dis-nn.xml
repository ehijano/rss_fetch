<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 May 2025 04:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Universal Matrix Ensemble that Unifies Eigenspectrum Laws via Neural Network Models</title>
      <link>https://arxiv.org/abs/2505.11948</link>
      <description>arXiv:2505.11948v1 Announce Type: new 
Abstract: Random matrix theory, which characterizes the spectrum distribution of infinitely large matrices, plays a central role in theories across diverse fields, including high-dimensional data analysis, ecology, neuroscience, and machine learning. Among its celebrated achievements, the Marchenko--Pastur law and the elliptic law have served as key results for numerous applications. However, the relationship between these two laws remains elusive, and the existence of a universal framework unifying them is unclear. Inspired by a neural network model, we establish a universal matrix ensemble that unifies these laws as special cases. Through an analysis based on the saddle-node equation, we derive an explicit expression for the spectrum distribution of the ensemble. As a direct application, we reveal how the universal law clarifies the stability of a class of associative memory neural networks. By uncovering a fundamental law of random matrix theory, our results deepen the understanding of high-dimensional systems and advance the integration of theories across multiple disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11948v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arata Tomoto, Jun-nosuke Teramae</dc:creator>
    </item>
    <item>
      <title>Phase transitions from linear to nonlinear information processing in neural networks</title>
      <link>https://arxiv.org/abs/2505.13003</link>
      <description>arXiv:2505.13003v1 Announce Type: new 
Abstract: We investigate a phase transition from linear to nonlinear information processing in echo state networks, a widely used framework in reservoir computing. The network consists of randomly connected recurrent nodes perturbed by a noise and the output is obtained through linear regression on the network states. By varying the standard deviation of the input weights, we systematically control the nonlinearity of the network. For small input standard deviations, the network operates in an approximately linear regime, resulting in limited information processing capacity. However, beyond a critical threshold, the capacity increases rapidly, and this increase becomes sharper as the network size grows. Our results indicate the presence of a discontinuous transition in the limit of infinitely many nodes. This transition is fundamentally different from the conventional order-to-chaos transition in neural networks, which typically leads to a loss of long-term predictability and a decline in the information processing capacity. Furthermore, we establish a scaling law relating the critical nonlinearity to the noise intensity, which implies that the critical nonlinearity vanishes in the absence of noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13003v1</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masaya Matsumura, Taiki Haga</dc:creator>
    </item>
    <item>
      <title>Disorder-Driven Exceptional Points and Concurrent Topological Phase Transitions in Non-Hermitian Lattice</title>
      <link>https://arxiv.org/abs/2505.13057</link>
      <description>arXiv:2505.13057v1 Announce Type: new 
Abstract: Exceptional point (EP) and topological phase transition (TPT) in non-Hermitian systems have recently garnered significant attention owing to their fundamental importance and potential applications in sensing and topological devices. Beyond the EP induced by non-reciprocal hopping, we show that random disorder can also drive the valence and conduction bands across EPs, even twice in the non-Hermitian regime. Remarkably, a TPT can occur concurrently with an EP as disorder strength increases. These disorder-driven EPs and concurrent TPTs are well captured by effective medium theory. The analysis reveals that their emergence results from the interplay between disorder-induced energy level renormalization and non-reciprocal hopping-induced inter-level coupling, which fundamentally restructures the spectral properties of the system. The phase diagram in the parameter space of non-reciprocal hopping and disorder strength identifies robust EP lines. Interestingly, two EP lines can emerge from the TPT point in the Hermitian limit. As non-reciprocal hopping increases, these lines split, with one aligning the TPT, leading to distinct disorder-induced EPs. Our results uncover a robust, disorder-driven mechanism for generating EPs and concurrent TPTs, offering a new direction for exploring non-Hermitian topological matter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13057v1</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Cheng, Tiantao Qu, Yaqing Yang, Lei Zhang, Jun Chen</dc:creator>
    </item>
    <item>
      <title>Integrability and exact large deviations of the weakly-asymmetric exclusion process</title>
      <link>https://arxiv.org/abs/2505.12034</link>
      <description>arXiv:2505.12034v1 Announce Type: cross 
Abstract: The weakly asymmetric exclusion process (WASEP) in one dimension is a paradigmatic system of interacting particles described by the macroscopic fluctuation theory (MFT) in the presence of driving. We consider an initial condition with densities $\rho_1,\rho_2$ on either side of the origin, so that for $\rho_1=\rho_2$ the gas is stationary. Starting from the microscopic description, we obtain exact formulae for the cumulant generating functions, and large deviation rate functions of the time-integrated current and the position of a tracer. As the asymmetry/driving is increased, these describe the crossover between the symmetric exclusion process (SSEP) and the weak noise regime of the Kardar-Parisi-Zhang (KPZ) equation: we recover the two limits and describe the crossover from the WASEP cubic tail to the $5/2$ and $3/2$ KPZ tail exponents. Finally, we show that the MFT of the WASEP is classically integrable, by exhibiting the explicit Lax pairs, which are obtained through a novel mapping between the MFT of the WASEP and a complex extension of the classical anisotropic Landau-Lifshitz spin chain. This shows integrability of all MFTs of asymmetric models with quadratic mobility as well as their dual versions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12034v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <category>nlin.SI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Krajenbrink, Pierre Le Doussal</dc:creator>
    </item>
    <item>
      <title>Neural Thermodynamics I: Entropic Forces in Deep and Universal Representation Learning</title>
      <link>https://arxiv.org/abs/2505.12387</link>
      <description>arXiv:2505.12387v1 Announce Type: cross 
Abstract: With the rapid discovery of emergent phenomena in deep learning and large language models, explaining and understanding their cause has become an urgent need. Here, we propose a rigorous entropic-force theory for understanding the learning dynamics of neural networks trained with stochastic gradient descent (SGD) and its variants. Building on the theory of parameter symmetries and an entropic loss landscape, we show that representation learning is crucially governed by emergent entropic forces arising from stochasticity and discrete-time updates. These forces systematically break continuous parameter symmetries and preserve discrete ones, leading to a series of gradient balance phenomena that resemble the equipartition property of thermal systems. These phenomena, in turn, (a) explain the universal alignment of neural representations between AI models and lead to a proof of the Platonic Representation Hypothesis, and (b) reconcile the seemingly contradictory observations of sharpness- and flatness-seeking behavior of deep learning optimization. Our theory and experiments demonstrate that a combination of entropic forces and symmetry breaking is key to understanding emergent phenomena in deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12387v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liu Ziyin, Yizhou Xu, Isaac Chuang</dc:creator>
    </item>
    <item>
      <title>Anomalous persistent current in a 1D dimerized ring with aperiodic site potential: Non-interacting and interacting cases</title>
      <link>https://arxiv.org/abs/2505.12939</link>
      <description>arXiv:2505.12939v1 Announce Type: cross 
Abstract: In this work, we investigate the magnetic response by examining flux-driven circular currents in a Su-Schrieffer-Heeger (SSH) tight-binding (TB) ring threaded by an Aharonov-Bohm (AB) flux, $\phi$. We consider both non-interacting and interacting electrons, where site energies are modulated by a slowly varying cosine form. Repulsive electron-electron interaction is incorporated through an on-site Hubbard term, and we analyze the system using the Hartree-Fock (HF) mean-field (MF) approximation. We discuss the characteristics of flux-driven circular currents to aperiodic potentials, dimerized hopping integrals, and Hubbard interactions. For the chosen aperiodic potential, both the strength and configuration play a crucial role, and we explore these aspects in depth. Interestingly, we observe a counterintuitive delocalizing effect as the aperiodic potential increases, unlike in conventional disordered rings. The effects of system size, filling factor, the presence of circular spin current, and the accuracy of MF results are also discussed. Finally, we provide a brief description of possible experimental realizations of our chosen quantum system. This investigation can be extended to explore additional properties in various loop substructures, promising further insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12939v1</guid>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.str-el</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Souvik Roy, Santanu K. Maiti, David Laroze</dc:creator>
    </item>
    <item>
      <title>Implicit bias produces neural scaling laws in learning curves, from perceptrons to deep networks</title>
      <link>https://arxiv.org/abs/2505.13230</link>
      <description>arXiv:2505.13230v1 Announce Type: cross 
Abstract: Scaling laws in deep learning - empirical power-law relationships linking model performance to resource growth - have emerged as simple yet striking regularities across architectures, datasets, and tasks. These laws are particularly impactful in guiding the design of state-of-the-art models, since they quantify the benefits of increasing data or model size, and hint at the foundations of interpretability in machine learning. However, most studies focus on asymptotic behavior at the end of training or on the optimal training time given the model size. In this work, we uncover a richer picture by analyzing the entire training dynamics through the lens of spectral complexity norms. We identify two novel dynamical scaling laws that govern how performance evolves during training. These laws together recover the well-known test error scaling at convergence, offering a mechanistic explanation of generalization emergence. Our findings are consistent across CNNs, ResNets, and Vision Transformers trained on MNIST, CIFAR-10 and CIFAR-100. Furthermore, we provide analytical support using a solvable model: a single-layer perceptron trained with binary cross-entropy. In this setting, we show that the growth of spectral complexity driven by the implicit bias mirrors the generalization behavior observed at fixed norm, allowing us to connect the performance dynamics to classical learning rules in the perceptron.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13230v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Francesco D'Amico, Dario Bocchi, Matteo Negri</dc:creator>
    </item>
    <item>
      <title>Heterogeneous diffusion in an harmonic potential: the role of the interpretation</title>
      <link>https://arxiv.org/abs/2505.13363</link>
      <description>arXiv:2505.13363v1 Announce Type: cross 
Abstract: Diffusion in heterogeneous energy and diffusivity landscapes is widespread in biological systems. However, solving the Langevin equation in such environments introduces ambiguity due to the interpretation parameter $\alpha$, which depends on the underlying physics and can take values in the range $0&lt;\alpha&lt;1$. The typical interpretations are It\^o ($\alpha=0$), Stratonovich ($\alpha=1/2$), and H\"anggi-Klimontovich ($\alpha=1$). Here, we analyse the motion of a particle in an harmonic potential -- modelled as an Ornstein-Uhlenbeck process -- with diffusivity that varies in space. Our focus is on two-phase systems with a discontinuity in environmental properties at $x=0$. We derive the probability density of the particle position for the process, and consider two paradigmatic situations. In the first one, the damping coefficient remains constant, and fluctuation-dissipation relations are not satisfied. In the second one, these relations are enforced, leading to a position-dependent damping coefficient. In both cases, we provide solutions as a function of the interpretation parameter $\alpha$, with particular attention to the It\^o, Stratonovich, and H\"anggi-Klimontovich interpretations, revealing fundamentally different behaviours, in particular with respect to an interface located at the potential minimum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13363v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian Pacheco-Pozo, Igor M. Sokolov, Ralf Metzler, Diego Krapf</dc:creator>
    </item>
    <item>
      <title>Discrete Time Crystal in quantum Sherrington-Kirkpatrick model</title>
      <link>https://arxiv.org/abs/2504.19378</link>
      <description>arXiv:2504.19378v2 Announce Type: replace 
Abstract: Discrete time crystals (DTC) have emerged as a significant phase of matter for out-of-equilibrium many-body systems. We study how long-range interactions and disorder contribute to the stability of the DTC phase. Generally, a stable DTC phase is believed to be realized in disordered systems with short-range interactions. In this work, we study periodically driven quantum Sherrington-Kirkpatrick (SK) model of Ising spin-glass in which all spins are randomly coupled. We investigate the possibilities of DTC phase in the SK model within three different driving protocols and found that the quantum SK model exhibits a robust DTC phase despite the long-range nature of interactions. The DTC phase in quantum SK model persists for a larger range of parameters if the $XY$ coupling or a transverse field is also random. This suggests that disorder in the $XY$ coupling or transverse field is also crucial for stabilising a broad DTC phase, despite the SK model's random couplings. Our analysis shows that the stability of the DTC phase is determined by the non-ergodic nature of the eigenstates of the quantum SK model, with the DTC order-parameter closely following the Shannon entropy of eigenstates. We compare the periodically driven SK model to alternative models of long-range interactions with uniform coefficients and found that the DTC phase is absent in these models for most of the driving protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19378v2</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aarya Bothra, Arti Garg</dc:creator>
    </item>
    <item>
      <title>Gaussian Universality in Neural Network Dynamics with Generalized Structured Input Distributions</title>
      <link>https://arxiv.org/abs/2405.00642</link>
      <description>arXiv:2405.00642v3 Announce Type: replace-cross 
Abstract: Bridging the gap between the practical performance of deep learning and its theoretical foundations often involves analyzing neural networks through stochastic gradient descent (SGD). Expanding on previous research that focused on modeling structured inputs under a simple Gaussian setting, we analyze the behavior of a deep learning system trained on inputs modeled as Gaussian mixtures to better simulate more general structured inputs. Through empirical analysis and theoretical investigation, we demonstrate that under certain standardization schemes, the deep learning model converges toward Gaussian setting behavior, even when the input data follow more complex or real-world distributions. This finding exhibits a form of universality in which diverse structured distributions yield results consistent with Gaussian assumptions, which can support the theoretical understanding of deep learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00642v3</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaeyong Bae, Hawoong Jeong</dc:creator>
    </item>
    <item>
      <title>Vibrational similarities in jamming-unjamming of polycrystalline and disordered granular packings</title>
      <link>https://arxiv.org/abs/2411.03030</link>
      <description>arXiv:2411.03030v3 Announce Type: replace-cross 
Abstract: We investigate the vibrational properties of polycrystalline monodisperse and disordered bidisperse granular packings during jamming and unjamming using discrete element method simulations. Both systems deviate from Debye scaling at low frequencies $(\omega)$, but only bidisperse packings exhibit a low-$\omega$ plateau. The low $\omega$ exponent ($\alpha$) in bidisperse packings evolves smoothly from zero (plateau) to near one (Debye scaling) with increasing packing fraction, whereas in polycrystalline packings, it changes discontinuously near jamming/unjamming, due to the nature of the contact network rearrangements. Despite structural modifications during the compression-decompression cycle, the exponent remains unchanged at the same distance from jamming density, regardless of the history. Nonaffine displacements and contact orientational order further confirm that structural features that impact low-$\omega$ vibrational states and, hence, mechanical properties are largely restored upon decompression, reinforcing vibrational similarities between jamming and unjamming states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03030v3</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan C. Petit, Saswati Ganguly, Matthias Sperl</dc:creator>
    </item>
    <item>
      <title>The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model</title>
      <link>https://arxiv.org/abs/2501.16226</link>
      <description>arXiv:2501.16226v3 Announce Type: replace-cross 
Abstract: Self-distillation (SD), a technique where a model improves itself using its own predictions, has attracted attention as a simple yet powerful approach in machine learning. Despite its widespread use, the mechanisms underlying its effectiveness remain unclear. In this study, we investigate the efficacy of hyperparameter-tuned multi-stage SD with a linear classifier for binary classification on noisy Gaussian mixture data. For the analysis, we employ the replica method from statistical physics. Our findings reveal that the primary driver of SD's performance improvement is denoising through hard pseudo-labels, with the most notable gains observed in moderately sized datasets. We also identify two practical heuristics to enhance SD: early stopping that limits the number of stages, which is broadly effective, and bias parameter fixing, which helps under label imbalance. To empirically validate our theoretical findings derived from our toy model, we conduct additional experiments on CIFAR-10 classification using pretrained ResNet backbone. These results provide both theoretical and practical insights, advancing our understanding and application of SD in noisy settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16226v3</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaito Takanami, Takashi Takahashi, Ayaka Sakata</dc:creator>
    </item>
    <item>
      <title>Additional jamming transition in 2D bidisperse granular packings</title>
      <link>https://arxiv.org/abs/2502.17266</link>
      <description>arXiv:2502.17266v2 Announce Type: replace-cross 
Abstract: We present a jamming diagram for 2D bidisperse granular systems, capturing two distinct jamming transitions. The first occurs as large particles form a jammed structure, while the second, emerging at a critical small-particle concentration, $X_{\mathrm{S}}^{*} \approx 0.21$, and size ratio, $\delta^{*} \approx 0.25$, involves small particles jamming into the voids of the existing large-particle structure upon further compression. Below this threshold, small particles fill voids within the large-particle network, increasing packing density. Beyond this point, excess small particles disrupt efficient packing, resulting in looser structures. \jp{These results, consistent with previous 3D studies, demonstrate that the second transition occurs at a well-defined point in the $(X_{\mathrm{S}}, \delta)$ plane, independent of dimensionality, likely driven by the geometric saturation of available space around particles, void closure, and structural arrangement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17266v2</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/mtcv-dbpd</arxiv:DOI>
      <dc:creator>Juan C. Petit, Matthias Sperl</dc:creator>
    </item>
  </channel>
</rss>
