<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.OS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.OS</link>
    <description>cs.OS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.OS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Oct 2025 02:07:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Joyride: Rethinking Linux's network stack design for better performance, security, and reliability</title>
      <link>https://arxiv.org/abs/2509.25015</link>
      <description>arXiv:2509.25015v1 Announce Type: new 
Abstract: Contemporary distributed computing workloads, including scientific computation, data mining, and machine learning, increasingly demand OS networking with minimal latency as well as high throughput, security, and reliability. However, Linux's conventional TCP/IP stack becomes increasingly problematic for high-end NICs, particularly those operating at 100 Gbps and beyond.
  These limitations come mainly from overheads associated with kernel space processing, mode switching, and data copying in the legacy architecture. Although kernel bypass techniques such as DPDK and RDMA offer alternatives, they introduce significant adoption barriers: both often require extensive application redesign, and RDMA is not universally available on commodity hardware.
  This paper proposes Joyride, a high performance framework with a grand vision of replacing Linux's legacy network stack while providing compatibility with existing applications. Joyride aims to integrate kernel bypass ideas, specifically DPDK and a user-space TCP/IP stack, while designing a microkernel-style architecture for Linux networking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25015v1</guid>
      <category>cs.OS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3765889.3767045</arxiv:DOI>
      <arxiv:journal_reference>3rd Workshop on Kernel Isolation, Safety and Verification (KISV 2025)</arxiv:journal_reference>
      <dc:creator>Yanlin Du, Ruslan Nikolaev</dc:creator>
    </item>
    <item>
      <title>ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights</title>
      <link>https://arxiv.org/abs/2509.23693</link>
      <description>arXiv:2509.23693v1 Announce Type: cross 
Abstract: Lossless compression imposes significant computational over head on datacenters when performed on CPUs. Hardware compression and decompression processing units (CDPUs) can alleviate this overhead, but optimal algorithm selection, microarchitectural design, and system-level placement of CDPUs are still not well understood. We present the design of an ASIC-based in-storage CDPU and provide a comprehensive end-to-end evaluation against two leading ASIC accelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant CDPU placement regimes: peripheral, on-chip, and in-storage. Our results reveal: (i) acute sensitivity of throughput and latency to CDPU placement and interconnection, (ii) strong correlation between compression efficiency and data patterns/layouts, (iii) placement-driven divergences between microbenchmark gains and real-application speedups, (iv) discrepancies between module and system-level power efficiency, and (v) scalability and multi-tenant interference is sues of various CDPUs. These findings motivate a placement-aware, cross-layer rethinking of hardware (de)compression for hyperscale storage infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23693v1</guid>
      <category>cs.AR</category>
      <category>cs.OS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3767295.3769384</arxiv:DOI>
      <dc:creator>Tao Lu, Jiapin Wang, Yelin Shan, Xiangping Zhang, Xiang Chen</dc:creator>
    </item>
    <item>
      <title>Demystifying Serverless Costs on Public Platforms: Bridging Billing, Architecture, and OS Scheduling</title>
      <link>https://arxiv.org/abs/2506.01283</link>
      <description>arXiv:2506.01283v2 Announce Type: replace-cross 
Abstract: Public cloud serverless platforms have attracted a large user base due to their high scalability, plug-and-play deployment model, and pay-per-use billing. However, compared to virtual machines and container hosting services, modern serverless offerings typically impose higher per-unit time and resource charges. Additionally, billing practices such as wall-clock time allocation-based billing, invocation fees, and usage rounding up can further increase costs.
  This work, for the first time, holistically demystifies these costs by conducting an in-depth, top-down characterization and analysis from user-facing billing models, through request serving architectures, and down to operating system scheduling on major public serverless platforms. We quantify, for the first time, how current billing practices inflate billable resources up to 4.35x beyond actual consumption. Also, our analysis reveals previously unreported cost drivers, such as operational patterns of serving architectures that create overheads, details of resource allocation during keep-alive periods, and OS scheduling granularity effects that directly impact both performance and billing. By tracing the sources of costs from billing models down to OS scheduling, we uncover the rationale behind today's expensive serverless billing model and practices and provide insights for designing performant and cost-effective serverless systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01283v2</guid>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3767295.3769374</arxiv:DOI>
      <dc:creator>Changyuan Lin, Yuanzhi Ma, Mohammad Shahrad</dc:creator>
    </item>
  </channel>
</rss>
