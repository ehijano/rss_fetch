<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Nov 2025 03:49:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Diabetes Lifestyle Medicine Treatment Assistance Using Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2510.26807</link>
      <description>arXiv:2510.26807v1 Announce Type: new 
Abstract: Type 2 diabetes prevention and treatment can benefit from personalized lifestyle prescriptions. However, the delivery of personalized lifestyle medicine prescriptions is limited by the shortage of trained professionals and the variability in physicians' expertise. We propose an offline contextual bandit approach that learns individualized lifestyle prescriptions from the aggregated NHANES profiles of 119,555 participants by minimizing the Magni glucose risk-reward function. The model encodes patient status and generates lifestyle medicine prescriptions, which are trained using a mixed-action Soft Actor-Critic algorithm. The task is treated as a single-step contextual bandit. The model is validated against lifestyle medicine prescriptions issued by three certified physicians from Xiangya Hospital. These results demonstrate that offline mixed-action SAC can generate risk-aware lifestyle medicine prescriptions from cross-sectional NHANES data, warranting prospective clinical validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26807v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhan Tang</dc:creator>
    </item>
    <item>
      <title>A Machine Learning-Based Framework to Shorten the Questionnaire for Assessing Autism Intervention</title>
      <link>https://arxiv.org/abs/2510.26808</link>
      <description>arXiv:2510.26808v1 Announce Type: new 
Abstract: Caregivers of individuals with autism spectrum disorder (ASD) often find the 77-item Autism Treatment Evaluation Checklist (ATEC) burdensome, limiting its use for routine monitoring. This study introduces a generalizable machine learning framework that seeks to shorten assessments while maintaining evaluative accuracy. Using longitudinal ATEC data from 60 autistic children receiving therapy, we applied feature selection and cross-validation techniques to identify the most predictive items across two assessment goals: longitudinal therapy tracking and point-in-time severity estimation. For progress monitoring, the framework identified 16 items (21% of the original questionnaire) that retained strong correlation with total score change and full subdomain coverage. We also generated smaller subsets (1-7 items) for efficient approximations. For point-in-time severity assessment, our model achieved over 80% classification accuracy using just 13 items (17% of the original set). While demonstrated on ATEC, the methodology-based on subset optimization, model interpretability, and statistical rigor-is broadly applicable to other high-dimensional psychometric tools. The resulting framework could potentially enable more accessible, frequent, and scalable assessments and offer a data-driven approach for AI-supported interventions across neurodevelopmental and psychiatric contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26808v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Audrey Dong, Claire Xu, Samuel R. Guo, Kevin Yang, Xue-Jun Kong</dc:creator>
    </item>
    <item>
      <title>A generalisation of the signal-to-noise ratio using proper scoring rules</title>
      <link>https://arxiv.org/abs/2510.26809</link>
      <description>arXiv:2510.26809v1 Announce Type: new 
Abstract: A generalised concept of the signal-to-noise ratio (or equivalently the ratio of predictable components, or RPC) is provided, based on proper scoring rules. This definition is the natural generalisation of the classical RPC, yet it allows one to define and analyse the signal-to-noise properties of any type of forecast that is amenable to scoring, thus drastically widening the applicability of these concepts. The methodology is illustrated for ensemble forecasts, scored using the continuous ranked probability score (CRPS), and for probability forecasts of a binary event, scored using the logarithmic score. Numerical examples are demonstrated using synthetic data with prescribed signal-to-noise ratios as well as seasonal ensemble hindcasts of the North Atlantic Oscillation (NAO) index. For the synthetic data, the RPC statistic as well as the scoring rule--based ones agree regarding which data sets exhibit anomalous signal-to-noise ratios, but exhibit different variance, indicating different statistical properties. For the NAO data, on the other hand, the results among the different statistics are more equivocal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26809v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jochen Br\"ocker, Eviatar Bach</dc:creator>
    </item>
    <item>
      <title>Emergent Dynamical Spatial Boundaries in Emergency Medical Services: A Navier-Stokes Framework from First Principles</title>
      <link>https://arxiv.org/abs/2510.26810</link>
      <description>arXiv:2510.26810v1 Announce Type: new 
Abstract: Emergency medical services (EMS) response times are critical determinants of patient survival, yet existing approaches to spatial coverage analysis rely on discrete distance buffers or ad-hoc geographic information system (GIS) isochrones without theoretical foundation. This paper derives continuous spatial boundaries for emergency response from first principles using fluid dynamics (Navier-Stokes equations), demonstrating that response effectiveness decays exponentially with time: $\tau(t) = \tau_0 \exp(-\kappa t)$, where $\tau_0$ is baseline effectiveness and $\kappa$ is the temporal decay rate. Using 10,000 simulated emergency incidents from the National Emergency Medical Services Information System (NEMSIS), I estimate decay parameters and calculate critical boundaries $d^*$ where response effectiveness falls below policy-relevant thresholds. The framework reveals substantial demographic heterogeneity: elderly populations (85+) experience 8.40-minute average response times versus 7.83 minutes for younger adults (18-44), with 33.6\% of poor-access incidents affecting elderly populations despite representing 5.2\% of the sample. Non-parametric kernel regression validation confirms exponential decay is appropriate (mean squared error 8-12 times smaller than parametric), while traditional difference-in-differences analysis validates treatment effect existence (DiD coefficient = -1.35 minutes, $p &lt; 0.001$). The analysis identifies vulnerable populations--elderly, rural, and low-income communities--facing systematically longer response times, informing optimal EMS station placement and resource allocation to reduce health disparities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26810v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Proxy Variable in OECD Database: Application of Parametric Quantile Regression and Median Based Unit Rayleigh Distribution</title>
      <link>https://arxiv.org/abs/2510.26811</link>
      <description>arXiv:2510.26811v1 Announce Type: new 
Abstract: This paper presents an in-depth exploration of the innovative Median-based unit Rayleigh (MBUR) distribution, previously introduced by the author. This new approach is specifically designed for conducting quantile regression analysis, enabling researchers to gain valuable insights into real-world data applications. The author effectively demonstrates the feasible advantage of the MBUR distribution, highlighting its potential to connect advanced statistical theory with meaningful results in data analysis. The author utilized OECD data in employing the parametric MBUR quantile regression using the response variables which are distributed as MBUR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26811v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohamed Attia</dc:creator>
    </item>
    <item>
      <title>Towards Gaussian processes modelling to study the late effects of radiotherapy in children and young adults with brain tumours</title>
      <link>https://arxiv.org/abs/2510.26814</link>
      <description>arXiv:2510.26814v1 Announce Type: new 
Abstract: Survivors of childhood cancer need lifelong monitoring for side effects from radiotherapy. However, longitudinal data from routine monitoring is often infrequently and irregularly sampled, and subject to inaccuracies. Due to this, measurements are often studied in isolation, or simple relationships (e.g., linear) are used to impute missing timepoints. In this study, we investigated the potential role of Gaussian Processes (GP) modelling to make population-based and individual predictions, using insulin-like growth factor 1 (IGF-1) measurements as a test case. With training data of 23 patients with a median (range) of 4 (1-16) timepoints we identified a trend within the range of literature reported values. In addition, with 8 test cases, individual predictions were made with an average root mean squared error of 31.9 (10.1 - 62.3) ng/ml and 27.4 (0.02 - 66.1) ng/ml for two approaches. GP modelling may overcome limitations of routine longitudinal data and facilitate analysis of late effects of radiotherapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26814v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angela Davey, Arthur Leroy, Eliana Vasquez Osorio, Kate Vaughan, Peter Clayton, Marcel van Herk, Mauricio A Alvarez, Martin McCabe, Marianne Aznar</dc:creator>
    </item>
    <item>
      <title>Toward precision soil health: A regional framework for site-specific management across Missouri</title>
      <link>https://arxiv.org/abs/2510.26815</link>
      <description>arXiv:2510.26815v1 Announce Type: new 
Abstract: Effective soil health management is crucial for sustaining agriculture, adopting ecosystem resilience, and preserving water quality. However, Missouri's diverse landscapes limit the effectiveness of broad generalized management recommendations. The lack of resolution in existing soil grouping systems necessitates data driven, site specific insights to guide tailored interventions. To address these critical challenges, a regional soil clustering framework designed to support precision soil health management strategies across the state. The methodology leveraged high resolution SSURGO dataset, explicitly processing soil properties aggregated across the 0 to 30 cm root zone. Multivariate analysis incorporating a variational autoencoder and KMeans clustering was used to group soils with similar properties. The derived clusters were validated using statistical metrics, including silhouette scores and checks against existing taxonomic units, to confirm their spatial coherence. This approach enabled us to delineate soil groups that capture textures, hydraulic properties, chemical fertility, and biological indicators unique to Missouri's diverse agroecological regions. The clustering map identified ten distinct soil health management zones. This alignment of 10 clusters was selected as optimal because it was sufficiently large to capture inherited soil patterns while remaining manageable for practical statewide application. Rooting depth limitation and saturated hydraulic conductivity emerged as principal variables driving soil differentiation. Each management zone is defined by a unique combination of clay, organic matter, pH, and available water capacity. This framework bridges sophisticated data analysis with actionable, site targeted recommendations, enabling conservation planners, and agronomists to optimize management practices and enhance resource efficiency statewide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26815v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dipal Shah (School of Natural Resources, University of Missouri, Columbia, MO, USA), Jordon Wade (Crop Protection Research &amp; Development, Syngenta Group, Basel, Switzerland), Timothy Haithcoat (Institute for Data Science and Informatics, University of Missouri, Columbia, MO, USA), Robert Myers (School of Plant Science &amp; Technology, University of Missouri, Columbia, MO, USA), Kelly Wilson (School of Natural Resources, University of Missouri, Columbia, MO, USA)</dc:creator>
    </item>
    <item>
      <title>Systematic Absence of Low-Confidence Nighttime Fire Detections in VIIRS Active Fire Product: Evidence of Undocumented Algorithmic Filtering</title>
      <link>https://arxiv.org/abs/2510.26816</link>
      <description>arXiv:2510.26816v1 Announce Type: new 
Abstract: The Visible Infrared Imaging Radiometer Suite (VIIRS) active fire product is widely used for global fire monitoring, yet its confidence classification scheme exhibits an undocumented systematic pattern. Through analysis of 21,540,921 fire detections spanning one year (January 2023 - January 2024), I demonstrate a complete absence of low-confidence classifications during nighttime observations. Of 6,007,831 nighttime fires, zero were classified as low confidence, compared to an expected 696,908 under statistical independence (chi-squared = 1,474,795, p &lt; 10^-15, Z = -833). This pattern persists globally across all months, latitude bands, and both NOAA-20 and Suomi-NPP satellites. Machine learning reverse-engineering (88.9% accuracy), bootstrap simulation (1,000 iterations), and spatial-temporal analysis confirm this is an algorithmic constraint rather than a geophysical phenomenon. Brightness temperature analysis reveals nighttime fires below approximately 295K are likely excluded entirely rather than flagged as low-confidence, while daytime fires show normal confidence distributions. This undocumented behavior affects 27.9% of all VIIRS fire detections and has significant implications for fire risk assessment, day-night detection comparisons, confidence-weighted analyses, and any research treating confidence levels as uncertainty metrics. I recommend explicit documentation of this algorithmic constraint in VIIRS user guides and reprocessing strategies for affected analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26816v1</guid>
      <category>stat.AP</category>
      <category>astro-ph.IM</category>
      <category>cs.AI</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohit Rajendra Dhage</dc:creator>
    </item>
    <item>
      <title>Functional Analysis of Loss-development Patterns in P&amp;C Insurance</title>
      <link>https://arxiv.org/abs/2510.27204</link>
      <description>arXiv:2510.27204v1 Announce Type: new 
Abstract: We analyze loss development in NAIC Schedule P loss triangles using functional data analysis methods. Adopting the functional viewpoint, our dataset comprises 3300+ curves of incremental loss ratios (ILR) of workers' compensation lines over 24 accident years. Relying on functional data depth, we first study similarities and differences in development patterns based on company-specific covariates, as well as identify anomalous ILR curves.
  The exploratory findings motivate the probabilistic forecasting framework developed in the second half of the paper. We propose a functional model to complete partially developed ILR curves based on partial least squares regression of PCA scores. Coupling the above with functional bootstrapping allows us to quantify future ILR uncertainty jointly across all future lags. We demonstrate that our method has much better probabilistic scores relative to Chain Ladder and in particular can provide accurate functional predictive intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27204v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arthur Charpentier, Qiheng Guo, Mike Ludkovski</dc:creator>
    </item>
    <item>
      <title>Bias correction of satellite and reanalysis products for daily rainfall occurrence and intensity</title>
      <link>https://arxiv.org/abs/2510.27456</link>
      <description>arXiv:2510.27456v1 Announce Type: new 
Abstract: Satellite and reanalysis rainfall products (SREs) can serve as valuable complements or alternatives in data-sparse regions, but their significant biases necessitate correction. This study rigorously evaluates a suite of bias correction (BC) methods, including statistical approaches (LOCI, QM), machine learning (SVR, GPR), and hybrid techniques (LOCI-GPR, QM-GPR), applied to seven SREs across 38 stations in Ghana and Zambia, aimed at assessing their performance in rainfall detection and intensity estimation. Results indicate that the ENACTS product, which uniquely integrates a large number of station records, was the most corrigible SRE; in Zambia, nearly all BC methods successfully reduced the mean error on daily rainfall amounts at over 70% of stations. However, this performance requires further validation at independent stations not incorporated into the ENACTS product. Overall, the statistical methods (QM and LOCI) generally outperformed other techniques, although QM exhibited a tendency to inflate rainfall values. All corrected SREs demonstrated a high capability for detecting dry days (POD $\ge$ 0.80), suggesting their potential utility for drought applications. A critical limitation persisted, however, as most SREs and BC methods consistently failed to improve the detection of heavy and violent rainfall events (POD $\leq$ 0.2), highlighting a crucial area for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27456v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>John Bagiliko, David Stern, Denis Ndanguza, Francis Feehi Torgbor, Danny Parsons, Samuel Owusu Ansah</dc:creator>
    </item>
    <item>
      <title>Bayesian Source Apportionment of Spatio-temporal air pollution data</title>
      <link>https://arxiv.org/abs/2510.27551</link>
      <description>arXiv:2510.27551v1 Announce Type: new 
Abstract: Understanding the sources that contribute to fine particulate matter (PM$_{2.5}$) is of crucial importance for designing and implementing targeted air pollution mitigation strategies. Determining what factors contribute to a pollutant's concentration goes under the name of source apportionment and it is a problem long studied by atmospheric scientists and statisticians alike. In this paper, we propose a Bayesian model for source apportionment, that advances the literature on source apportionment by allowing estimation of the number of sources and accounting for spatial and temporal dependence in the observed pollutants' concentrations. Taking as example observations of six species of fine particulate matter observed over the course of a year, we present a latent functional factor model that expresses the space-time varying observations of log concentrations of the six pollutant as a linear combination of space-time varying emissions produced by an unknown number of sources each multiplied by the corresponding source's relative contribution to the pollutant. Estimation of the number of sources is achieved by introducing source-specific shrinkage parameters. Application of the model to simulated data showcases its ability to retrieve the true number of sources and to reliably estimate the functional latent factors, whereas application to PM$_{2.5}$ speciation data in California identifies 3 major sources for the six PM$_{2.5}$ species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27551v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michela Frigeri, Veronica Berrocal, Alessandra Guglielmi</dc:creator>
    </item>
    <item>
      <title>Multi-Representation Attention Framework for Underwater Bioacoustic Denoising and Recognition</title>
      <link>https://arxiv.org/abs/2510.26838</link>
      <description>arXiv:2510.26838v1 Announce Type: cross 
Abstract: Automated monitoring of marine mammals in the St. Lawrence Estuary faces extreme challenges: calls span low-frequency moans to ultrasonic clicks, often overlap, and are embedded in variable anthropogenic and environmental noise. We introduce a multi-step, attention-guided framework that first segments spectrograms to generate soft masks of biologically relevant energy and then fuses these masks with the raw inputs for multi-band, denoised classification. Image and mask embeddings are integrated via mid-level fusion, enabling the model to focus on salient spectrogram regions while preserving global context. Using real-world recordings from the Saguenay St. Lawrence Marine Park Research Station in Canada, we demonstrate that segmentation-driven attention and mid-level fusion improve signal discrimination, reduce false positive detections, and produce reliable representations for operational marine mammal monitoring across diverse environmental conditions and signal-to-noise ratios. Beyond in-distribution evaluation, we further assess the generalization of Mask-Guided Classification (MGC) under distributional shifts by testing on spectrograms generated with alternative acoustic transformations. While high-capacity baseline models lose accuracy in this Out-of-distribution (OOD) setting, MGC maintains stable performance, with even simple fusion mechanisms (gated, concat) achieving comparable results across distributions. This robustness highlights the capacity of MGC to learn transferable representations rather than overfitting to a specific transformation, thereby reinforcing its suitability for large-scale, real-world biodiversity monitoring. We show that in all experimental settings, the MGC framework consistently outperforms baseline architectures, yielding substantial gains in accuracy on both in-distribution and OOD data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26838v1</guid>
      <category>eess.AS</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amine Razig, Youssef Soulaymani, Loubna Benabbou, Pierre Cauchy</dc:creator>
    </item>
    <item>
      <title>A novel generalized additive scalar-on-function regression model for partially observed multidimensional functional data: An application to air quality classification</title>
      <link>https://arxiv.org/abs/2510.26917</link>
      <description>arXiv:2510.26917v1 Announce Type: cross 
Abstract: In this work we propose a generalized additive functional regression model for partially observed functional data. Our approach accommodates functional predictors of varying dimensions without requiring imputation of missing observations. Both the functional coefficients and covariates are represented using basis function expansions, with B-splines used in this study, though the method is not restricted to any specific basis choice. Model coefficients are estimated via penalized likelihood, leveraging the mixed model representation of penalized splines for efficient computation and smoothing parameter estimation.The performance of the proposed approach is assessed through two simulation studies: one involving two one-dimensional functional covariates, and another using a two-dimensional functional covariate. Finally, we demonstrate the practical utility of our method in an application to air-pollution classification in Dimapur, India, where images are treated as observations of a two-dimensional functional variable. This case study highlights the models ability to effectively handle incomplete functional data and to accurately discriminate between pollution levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26917v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavel Hern\'andez-Amaro, Maria Durban, M. Carmen Aguilera-Morillo</dc:creator>
    </item>
    <item>
      <title>Flexible model for varying levels of zeros and outliers in count data</title>
      <link>https://arxiv.org/abs/2510.27365</link>
      <description>arXiv:2510.27365v1 Announce Type: cross 
Abstract: Count regression models are necessary for examining discrete dependent variables alongside covariates. Nonetheless, when data display outliers, overdispersion, and an abundance of zeros, traditional methods like the zero-inflated negative binomial (ZINB) model sometimes do not yield a satisfactory fit, especially in the tail regions. This research presents a versatile, heavy-tailed discrete model as a resilient substitute for the ZINB model. The suggested framework is built by extending the generalized Pareto distribution and its zero-inflated version to the discrete domain. This formulation efficiently addresses both overdispersion and zero inflation, providing increased flexibility for heavy-tailed count data. Through intensive simulation studies and real-world implementations, the proposed models are thoroughly tested to see how well they work. The results show that our models always do better than classic negative binomial and zero-inflated negative binomial regressions when it comes to goodness-of-fit. This is especially true for datasets with a lot of zeros and outliers. These results highlight the proposed framework's potential as a strong and flexible option for modeling complicated count data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27365v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Touqeer Ahmad, Abid Hussain</dc:creator>
    </item>
    <item>
      <title>Testing Inequalities Linear in Nuisance Parameters</title>
      <link>https://arxiv.org/abs/2510.27633</link>
      <description>arXiv:2510.27633v1 Announce Type: cross 
Abstract: This paper proposes a new test for inequalities that are linear in possibly partially identified nuisance parameters. This type of hypothesis arises in a broad set of problems, including subvector inference for linear unconditional moment (in)equality models, specification testing of such models, and inference for parameters bounded by linear programs. The new test uses a two-step test statistic and a chi-squared critical value with data-dependent degrees of freedom that can be calculated by an elementary formula. Its simple structure and tuning-parameter-free implementation make it attractive for practical use. We establish uniform asymptotic validity of the test, demonstrate its finite-sample size and power in simulations, and illustrate its use in an empirical application that analyzes women's labor supply in response to a welfare policy reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27633v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Fletcher Cox, Xiaoxia Shi, Yuya Shimizu</dc:creator>
    </item>
    <item>
      <title>Dimensionality reduction can be used as a surrogate model for high-dimensional forward uncertainty quantification</title>
      <link>https://arxiv.org/abs/2402.04582</link>
      <description>arXiv:2402.04582v2 Announce Type: replace 
Abstract: We introduce a method to construct a stochastic surrogate model from the results of dimensionality reduction in forward uncertainty quantification. The hypothesis is that the high-dimensional input augmented by the output of a computational model admits a low-dimensional representation. This assumption can be met by numerous uncertainty quantification applications with physics-based computational models. The proposed approach differs from a sequential application of dimensionality reduction followed by surrogate modeling, as we "extract" a surrogate model from the results of dimensionality reduction in the input-output space. This feature becomes desirable when the input space is genuinely high-dimensional. The proposed method also diverges from the Probabilistic Learning on Manifold, as a reconstruction mapping from the feature space to the input-output space is circumvented. The final product of the proposed method is a stochastic simulator that propagates a deterministic input into a stochastic output, preserving the convenience of a sequential "dimensionality reduction + Gaussian process regression" approach while overcoming some of its limitations. The proposed method is demonstrated through two uncertainty quantification problems characterized by high-dimensional input uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04582v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ress.2025.111474</arxiv:DOI>
      <dc:creator>Jungho Kim, Sang-ri Yi, Ziqi Wang</dc:creator>
    </item>
    <item>
      <title>A composition of simplified physics-based model with neural operator for trajectory-level seismic response predictions of structural systems</title>
      <link>https://arxiv.org/abs/2506.10569</link>
      <description>arXiv:2506.10569v2 Announce Type: replace 
Abstract: Accurate prediction of nonlinear structural responses is essential for earthquake risk assessment and management. While high-fidelity nonlinear time history analysis provides the most comprehensive and accurate representation of the responses, it becomes computationally prohibitive for complex structural system models and repeated simulations under varying ground motions. To address this challenge, we propose a composite learning framework that integrates simplified physics-based models with a Fourier neural operator to enable efficient and accurate trajectory-level seismic response prediction. In the proposed architecture, a simplified physics-based model, obtained from techniques such as linearization, modal reduction, or solver relaxation, serves as a preprocessing operator to generate structural response trajectories that capture coarse dynamic characteristics. A neural operator is then trained to correct the discrepancy between these initial approximations and the true nonlinear responses, allowing the composite model to capture hysteretic and path-dependent behaviors. Additionally, a linear regression-based postprocessing scheme is introduced to further refine predictions and quantify associated uncertainty with negligible additional computational effort. The proposed approach is validated on three representative structural systems subjected to synthetic or recorded ground motions. Results show that the proposed approach consistently improves prediction accuracy over baseline models, particularly in data-scarce regimes. These findings demonstrate the potential of physics-guided operator learning for reliable and data-efficient modeling of nonlinear structural seismic responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10569v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.strusafe.2025.102668</arxiv:DOI>
      <dc:creator>Jungho Kim, Sang-ri Yi, Ziqi Wang</dc:creator>
    </item>
    <item>
      <title>Robust Spectral Fuzzy Clustering of Multivariate Time Series with Applications to Electroencephalogram</title>
      <link>https://arxiv.org/abs/2506.22861</link>
      <description>arXiv:2506.22861v2 Announce Type: replace 
Abstract: Clustering multivariate time series (MTS) is challenging due to non-stationary cross-dependencies, noise contamination, and gradual or overlapping state boundaries. We introduce a robust fuzzy clustering framework in the spectral domain that leverages Kendall's tau-based canonical coherence to extract frequency-specific monotonic relationships across variables. Our method takes advantage of dominant frequency-based cross-regional connectivity patterns to improve clustering accuracy while remaining resilient to outliers, making the approach broadly applicable to noisy, high-dimensional MTS. Each series is projected onto vectors generated from a spectral matrix specifically tailored to capture the underlying fuzzy partitions. Numerical experiments demonstrate the superiority of our framework over existing methods. As a flagship application, we analyze electroencephalogram recordings, where our approach uncovers frequency- and connectivity-specific markers of latent cognitive states such as alertness and drowsiness, revealing discriminative patterns and ambiguous transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22861v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziling Ma, Mara Sherlin Talento, Ying Sun, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Generative Adversarial Networks for High-Dimensional Item Factor Analysis: A Deep Adversarial Learning Algorithm</title>
      <link>https://arxiv.org/abs/2502.10650</link>
      <description>arXiv:2502.10650v3 Announce Type: replace-cross 
Abstract: Advances in deep learning and representation learning have transformed item factor analysis (IFA) in the item response theory (IRT) literature by enabling more efficient and accurate parameter estimation. Variational Autoencoders (VAEs) have been one of the most impactful techniques in modeling high-dimensional latent variables in this context. However, the limited expressiveness of the inference model based on traditional VAEs can still hinder the estimation performance. We introduce Adversarial Variational Bayes (AVB) algorithms as an improvement to VAEs for IFA with improved flexibility and accuracy. By bridging the strengths of VAEs and Generative Adversarial Networks (GANs), AVB incorporates an auxiliary discriminator network to reframe the estimation process as a two-player adversarial game and removes the restrictive assumption of standard normal distributions in the inference model. Theoretically, AVB can achieve similar or higher likelihood compared to VAEs. A further enhanced algorithm, Importance-weighted Adversarial Variational Bayes (IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE). In an exploratory analysis of empirical data, IWAVB demonstrated superior expressiveness by achieving a higher likelihood compared to IWAE. In confirmatory analysis with simulated data, IWAVB achieved similar mean-square error results to IWAE while consistently achieving higher likelihoods. When latent variables followed a multimodal distribution, IWAVB outperformed IWAE. With its innovative use of GANs, IWAVB is shown to have the potential to extend IFA to handle large-scale data, facilitating the potential integration of psychometrics and multimodal data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10650v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nanyu Luo, Feng Ji</dc:creator>
    </item>
    <item>
      <title>Valid F-screening in linear regression</title>
      <link>https://arxiv.org/abs/2505.23113</link>
      <description>arXiv:2505.23113v2 Announce Type: replace-cross 
Abstract: Suppose that a data analyst wishes to report the results of a least squares linear regression only if the overall null hypothesis, $H_0^{1:p}: \beta_1= \beta_2 = \ldots = \beta_p=0$, is rejected. This practice, which we refer to as F-screening (since the overall null hypothesis is typically tested using an $F$-statistic), is in fact common practice across a number of applied fields. Unfortunately, it poses a problem: standard guarantees for the inferential outputs of linear regression, such as Type 1 error control of hypothesis tests and nominal coverage of confidence intervals, hold unconditionally, but fail to hold conditional on rejection of the overall null hypothesis. In this paper, we develop an inferential toolbox for the coefficients in a least squares model that are valid conditional on rejection of the overall null hypothesis. We develop selective p-values that lead to tests that are consistent and control the selective Type 1 error, i.e., the Type 1 error conditional on having rejected the overall null hypothesis. Furthermore, they can be computed without access to the raw data, i.e., using only the standard outputs of a least squares linear regression, and therefore are suitable for use in a retrospective analysis of a published study. We also develop confidence intervals that attain nominal selective coverage, and point estimates that account for having rejected the overall null hypothesis. We derive an expression for the Fisher information about the coefficients resulting from the proposed approach, and compare this to the Fisher information that results from an alternative approach that relies on sample splitting. We investigate the proposed approach in simulation and via re-analysis of two datasets from the biomedical literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23113v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivia McGough, Daniela Witten, Daniel Kessler</dc:creator>
    </item>
    <item>
      <title>Modeling US Climate Policy Uncertainty: From Causal Identification to Probabilistic Forecasting</title>
      <link>https://arxiv.org/abs/2507.12276</link>
      <description>arXiv:2507.12276v2 Announce Type: replace-cross 
Abstract: Accurately forecasting Climate Policy Uncertainty (CPU) is critical for designing effective climate strategies that balance economic growth with environmental objectives. Elevated CPU levels deter investment in green technologies, delay regulatory implementation, and amplify public resistance to policy reforms, particularly during economic stress. Despite the growing literature highlighting the economic relevance of CPU, the mechanisms through which macroeconomic and financial conditions influence its fluctuations remain insufficiently explored. This study addresses this gap by integrating four complementary causal inference techniques to identify statistically and economically significant determinants of the United States (US) CPU index. Impulse response analysis confirms their dynamic effects on CPU, highlighting the role of housing market activity, credit conditions, and financial market sentiment in shaping CPU fluctuations. The identified predictors, along with sentiment based Google Trends indicators, are incorporated into a Bayesian Structural Time Series (BSTS) framework for probabilistic forecasting. The inclusion of Google Trends data captures behavioral and attention based dynamics, leading to notable improvements in forecast accuracy. Numerical experiments demonstrate the superior performance of BSTS over state of the art classical and modern architectures for medium and long term forecasts, which are most relevant for climate policy implementation. The feature importance plot provides evidence that the spike-and-slab prior mechanism provides interpretable variable selection. The credible intervals quantify forecast uncertainty, thereby enhancing the model's transparency and policy relevance by enabling strategic decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12276v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Donia Besher, Anirban Sengupta, Tanujit Chakraborty</dc:creator>
    </item>
    <item>
      <title>DETERring more than Deforestation: Environmental Enforcement Reduces Violence in the Amazon</title>
      <link>https://arxiv.org/abs/2509.06076</link>
      <description>arXiv:2509.06076v2 Announce Type: replace-cross 
Abstract: We estimate the impact of environmental law enforcement on violence in the Brazilian Amazon. The introduction of the Real-Time Deforestation Detection System (DETER), which enabled the government to monitor deforestation in real time and issue fines for illegal clearing, significantly reduced homicides in the region. To identify causal effects, we exploit exogenous variation in satellite monitoring generated by cloud cover as an instrument for enforcement intensity. Our estimates imply that the expansion of state presence through DETER prevented approximately 1,477 homicides per year, a 15% reduction in homicides. These results show that curbing deforestation produces important social co-benefits, strengthening state presence and reducing violence in regions marked by institutional fragility and resource conflict.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06076v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rafael Araujo, Vitor Possebom, Gabriela Setti</dc:creator>
    </item>
    <item>
      <title>Neural Posterior Estimation for Cataloging Astronomical Images from the Legacy Survey of Space and Time</title>
      <link>https://arxiv.org/abs/2510.15315</link>
      <description>arXiv:2510.15315v2 Announce Type: replace-cross 
Abstract: The Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) will commence full-scale operations in 2026, yielding an unprecedented volume of astronomical images. Constructing an astronomical catalog, a table of imaged stars, galaxies, and their properties, is a fundamental step in most scientific workflows based on astronomical image data. Traditional deterministic cataloging methods lack statistical coherence as cataloging is an ill-posed problem, while existing probabilistic approaches suffer from computational inefficiency, inaccuracy, or the inability to perform inference with multiband coadded images, the primary output format for LSST images. In this article, we explore a recently developed Bayesian inference method called neural posterior estimation (NPE) as an approach to cataloging. NPE leverages deep learning to achieve both computational efficiency and high accuracy. When evaluated on the DC2 Simulated Sky Survey -- a highly realistic synthetic dataset designed to mimic LSST data -- NPE systematically outperforms the standard LSST pipeline in light source detection, flux measurement, star/galaxy classification, and galaxy shape measurement. Additionally, NPE provides well-calibrated posterior approximations. These promising results, obtained using simulated data, illustrate the potential of NPE in the absence of model misspecification. Although some degree of model misspecification is inevitable in the application of NPE to real LSST images, there are a variety of strategies to mitigate its effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15315v2</guid>
      <category>astro-ph.IM</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yicun Duan, Xinyue Li, Camille Avestruz, Jeffrey Regier</dc:creator>
    </item>
    <item>
      <title>A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data</title>
      <link>https://arxiv.org/abs/2510.16026</link>
      <description>arXiv:2510.16026v2 Announce Type: replace-cross 
Abstract: We provide an accessible description of a peer-reviewed generalizable causal machine learning pipeline to (i) discover latent causal sources of large-scale electronic health records observations, and (ii) quantify the source causal effects on clinical outcomes. We illustrate how imperfect multimodal clinical data can be processed, decomposed into probabilistic independent latent sources, and used to train taskspecific causal models from which individual causal effects can be estimated. We summarize the findings of the two real-world applications of the approach to date as a demonstration of its versatility and utility for medical discovery at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16026v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Barbero-Mota, Eric V. Strobl, John M. Still, William W. Stead, Thomas A. Lasko</dc:creator>
    </item>
  </channel>
</rss>
