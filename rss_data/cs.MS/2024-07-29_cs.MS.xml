<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MS</link>
    <description>cs.MS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jul 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>HOBOTAN: Efficient Higher Order Binary Optimization Solver with Tensor Networks and PyTorch</title>
      <link>https://arxiv.org/abs/2407.19987</link>
      <description>arXiv:2407.19987v1 Announce Type: new 
Abstract: In this study, we introduce HOBOTAN, a new solver designed for Higher Order Binary Optimization (HOBO). HOBOTAN supports both CPU and GPU, with the GPU version developed based on PyTorch, offering a fast and scalable system. This solver utilizes tensor networks to solve combinatorial optimization problems, employing a HOBO tensor that maps the problem and performs tensor contractions as needed. Additionally, by combining techniques such as batch processing for tensor optimization and binary-based integer encoding, we significantly enhance the efficiency of combinatorial optimization. In the future, the utilization of increased GPU numbers is expected to harness greater computational power, enabling efficient collaboration between multiple GPUs for high scalability. Moreover, HOBOTAN is designed within the framework of quantum computing, thus providing insights for future quantum computer applications. This paper details the design, implementation, performance evaluation, and scalability of HOBOTAN, demonstrating its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19987v1</guid>
      <category>cs.MS</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shoya Yasuda, Shunsuke Sotobayashi, Yuichiro Minato</dc:creator>
    </item>
    <item>
      <title>JAX-SSO: Differentiable Finite Element Analysis Solver for Structural Optimization and Seamless Integration with Neural Networks</title>
      <link>https://arxiv.org/abs/2407.20026</link>
      <description>arXiv:2407.20026v1 Announce Type: new 
Abstract: Differentiable numerical simulations of physical systems have gained rising attention in the past few years with the development of automatic differentiation tools. This paper presents JAX-SSO, a differentiable finite element analysis solver built with JAX, Google's high-performance computing library, to assist efficient structural design in the built environment. With the adjoint method and automatic differentiation feature, JAX-SSO can efficiently evaluate gradients of physical quantities in an automatic way, enabling accurate sensitivity calculation in structural optimization problems. Written in Python and JAX, JAX-SSO is naturally within the machine learning ecosystem so it can be seamlessly integrated with neural networks to train machine learning models with inclusion of physics. Moreover, JAX-SSO supports GPU acceleration to further boost finite element analysis. Several examples are presented to showcase the capabilities and efficiency of JAX-SSO: i) shape optimization of grid-shells and continuous shells; ii) size (thickness) optimization of continuous shells; iii) simultaneous shape and topology optimization of continuous shells; and iv) training of physics-informed neural networks for structural optimization. We believe that JAX-SSO can facilitate research related to differentiable physics and machine learning to further address problems in structural and architectural design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20026v1</guid>
      <category>cs.MS</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaoyuan Wu</dc:creator>
    </item>
    <item>
      <title>Fast multiplication by two's complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation</title>
      <link>https://arxiv.org/abs/2311.09922</link>
      <description>arXiv:2311.09922v3 Announce Type: replace 
Abstract: We demonstrate a multiplication method based on numbers represented as set of polynomial radix 2 indices stored as an integer list. The 'polynomial integer index multiplication' method is a set of algorithms implemented in python code. We demonstrate the method to be faster than both the Number Theoretic Transform (NTT) and Karatsuba for multiplication within a certain bit range. Also implemented in python code for comparison purposes with the polynomial radix 2 integer method. We demonstrate that it is possible to express any integer or real number as a list of integer indices, representing a finite series in base two. The finite series of integer index representation of a number can then be stored and distributed across multiple CPUs / GPUs. We show that operations of addition and multiplication can be applied as two's complement additions operating on the index integer representations and can be fully distributed across a given CPU / GPU architecture. We demonstrate fully distributed arithmetic operations such that the 'polynomial integer index multiplication' method overcomes the current limitation of parallel multiplication methods. Ie, the need to share common core memory and common disk for the calculation of results and intermediate results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09922v3</guid>
      <category>cs.MS</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Stocks</dc:creator>
    </item>
    <item>
      <title>GenML: A Python Library to Generate the Mittag-Leffler Correlated Noise</title>
      <link>https://arxiv.org/abs/2403.04273</link>
      <description>arXiv:2403.04273v2 Announce Type: replace 
Abstract: Mittag-Leffler correlated noise (M-L noise) plays a crucial role in the dynamics of complex systems, yet the scientific community has lacked tools for its direct generation. Addressing this gap, our work introduces GenML, a Python library specifically designed for generating M-L noise. We detail the architecture and functionalities of GenML and its underlying algorithmic approach, which enables the precise simulation of M-L noise. The effectiveness of GenML is validated through quantitative analyses of autocorrelation functions and diffusion behaviors, showcasing its capability to accurately replicate theoretical noise properties. Our contribution with GenML enables the effective application of M-L noise data in numerical simulation and data-driven methods for describing complex systems, moving beyond mere theoretical modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04273v2</guid>
      <category>cs.MS</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Qu, Hui Zhao, Wenjie Cai, Gongyi Wang, Zihan Huang</dc:creator>
    </item>
  </channel>
</rss>
