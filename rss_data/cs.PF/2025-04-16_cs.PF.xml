<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Apr 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improving Upon the generalized c-mu rule: a Whittle approach</title>
      <link>https://arxiv.org/abs/2504.10622</link>
      <description>arXiv:2504.10622v1 Announce Type: new 
Abstract: Scheduling a stream of jobs whose holding cost changes over time is a classic and practical problem. Specifically, each job is associated with a holding cost (penalty), where a job's instantaneous holding cost is some increasing function of its class and current age (the time it has spent in the system since its arrival). The goal is to schedule the jobs to minimize the time-average total holding cost across all jobs.
  The seminal paper on this problem, by Van Mieghem in 1995, introduced the generalized c-mu rule for scheduling jobs. Since then, this problem has attracted significant interest but remains challenging due to the absence of a finite-dimensional state space formulation. Consequently, subsequent works focus on more tractable versions of this problem.
  This paper returns to the original problem, deriving a heuristic that empirically improves upon the generalized c-mu rule and all existing heuristics. Our approach is to first translate the holding cost minimization problem to a novel Restless Multi-Armed Bandit (R-MAB) problem with a finite number of arms. Based on our R-MAB, we derive a novel Whittle Index policy, which is both elegant and intuitive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10622v1</guid>
      <category>cs.PF</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhouzi Li, Keerthana Gurushankar, Mor Harchol-Balter, Alan Scheller-Wolf</dc:creator>
    </item>
    <item>
      <title>PlantD: Performance, Latency ANalysis, and Testing for Data Pipelines -- An Open Source Measurement, Testing, and Simulation Framework</title>
      <link>https://arxiv.org/abs/2504.10692</link>
      <description>arXiv:2504.10692v1 Announce Type: new 
Abstract: As the volume of data available from sensor-enabled devices such as vehicles expands, it is increasingly hard for companies to make informed decisions about the cost of capturing, processing, and storing the data from every device. Business teams may forecast costs associated with deployments and use patterns of devices that they sell, yet lack ways of forecasting the cost and performance of the data pipelines needed to support their devices. Without such forecasting, a company's safest choice is to make worst-case capacity estimates, and pay for overprovisioned infrastructure. Existing data pipeline benchmarking tools can measure latency, cost, and throughput as needed for development, but cannot easily close the gap in communicating the implications with business teams to inform cost forecasting. In this paper, we introduce an open-source tool, PlantD, a harness for measuring data pipelines as they are being developed, and for interpreting that data in a business context. PlantD collects a complete suite of metrics and visualizations, when developing or evaluating data pipeline architectures, configurations, and business use cases. It acts as a metaphorical data pipeline wind tunnel, enabling experiments with synthetic data to characterize and compare the performance of pipelines. It then uses those results to allow modeling of expected annual cost and performance under projected real-world loads. We describe the architecture of PlantD, walk through an example of using it to measure and compare three variants of a pipeline for processing automotive telemetry, and demonstrate how business and engineering teams can simulate scenarios together and answer "what-if" questions about the pipeline's performance under different business assumptions, allowing them to intelligently predict performance and cost measures of their critical, high-data generation infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10692v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Bogart, Rajeev Chhajer, Baljit Singh, Tony Fontana, Majd Sakr</dc:creator>
    </item>
    <item>
      <title>Denoising Application Performance Models with Noise-Resilient Priors</title>
      <link>https://arxiv.org/abs/2504.10996</link>
      <description>arXiv:2504.10996v1 Announce Type: new 
Abstract: When scaling parallel codes to larger machines, performance models help identify potential bottlenecks. Since analytically designing these mathematical representations is usually challenging, empirical models based on performance measurements offer a practical alternative. Yet, measurements on HPC systems are typically affected by noise, leading to potentially misleading model predictions. To reduce the influence of noise, we introduce application-specific dynamic priors into the modeling process, which we derive from noise-resilient measurements of computational effort and knowledge of typical algorithms used in communication routines. These priors then narrow the search space for our performance models, excluding complexity classes that reflect noise rather than performance. Our approach keeps the models much closer to theoretical expectations and significantly improves their predictive power. Finally, it cuts experimental costs in half by minimizing the number of repeated measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10996v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gustavo de Morais, Alexander Gei{\ss}, Alexandru Calotoiu, Gregor Corbin, Ahmad Tarraf, Torsten Hoefler, Bernd Mohr, Felix Wolf</dc:creator>
    </item>
    <item>
      <title>Where Should I Deploy My Contracts? A Practical Experience Report</title>
      <link>https://arxiv.org/abs/2504.10535</link>
      <description>arXiv:2504.10535v1 Announce Type: cross 
Abstract: Blockchain networks provide a reliable trust anchor to decentralized applications (DApps) backed by smart contracts. The Ethereum ecosystem now encompasses most blockchain networks that provide compatible support for smart contracts code. Recently, many Ethereum Layer 2 (L2) rollup solutions emerged, meant to scale the base Layer 1 (L1) network, consequently decreasing transaction fees and diversifying the usage scenarios. Furthermore, the number of blockchain providers that offer access to the network infrastructure for both L1 and L2 continuously increases. A developer is faced with a multitude of deployment options and must weigh between the gains in costs and the losses in trust that are still an issue with L2. A decisive factor in this trade-off can be the use case itself, depending on its security requirements. Still, the evaluation of costs and performance cannot be ignored and should rely on a set of measurable metrics, although choosing the right metrics can be complicated. In this practical experience report, we explore the relevance of several such metrics in choosing between different providers and rollups. For this purpose, we perform evaluations for two use cases of DApps: a voting DApp with high security demands, suited for L1 deployment, and a cost-sensitive supply chain DApp, where L2 can be an option. We analyze a set of basic metrics by comparing these between two highly used access providers, Alchemy and Infura, for the L1 deployment case, and between two of the most popular rollups, Arbitrum One and OP Mainnet (Optimism), for the L2 deployment scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10535v1</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\u{a}t\u{a}lina Laz\u{a}r, Gabriela Secrieru, Emanuel Onica</dc:creator>
    </item>
    <item>
      <title>Improving Multiresource Job Scheduling with Markovian Service Rate Policies</title>
      <link>https://arxiv.org/abs/2504.08094</link>
      <description>arXiv:2504.08094v2 Announce Type: replace 
Abstract: Modern cloud computing workloads are composed of multiresource jobs that require a variety of computational resources in order to run, such as CPU cores, memory, disk space, or hardware accelerators. A single cloud server can typically run many multiresource jobs in parallel, but only if the server has sufficient resources to satisfy the demands of every job. A scheduling policy must therefore select sets of multiresource jobs to run in parallel in order to minimize the mean response time across jobs -- the average time from when a job arrives to the system until it is completed. Unfortunately, achieving low response times by selecting sets of jobs that fully utilize the available server resources has proven to be a difficult problem.
  In this paper, we develop and analyze a new class of policies for scheduling multiresource jobs, called Markovian Service Rate (MSR) policies. While prior scheduling policies for multiresource jobs are either highly complex to analyze or hard to implement, our MSR policies are simple to implement and are amenable to response time analysis. We show that the class of MSR policies is throughput-optimal in that we can use an MSR policy to stabilize the system whenever it is possible to do so. We also derive bounds on the mean response time under an MSR algorithm that are tight up to an additive constant. These bounds can be applied to systems with different preemption behaviors, such as fully preemptive systems, non-preemptive systems, and systems that allow preemption with setup times. We show how our theoretical results can be used to select a good MSR policy as a function of the system arrival rates, job service requirements, the server's resource capacities, and the resource demands of the jobs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08094v2</guid>
      <category>cs.PF</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongrui Chen, Isaac Grosof, Benjamin Berg</dc:creator>
    </item>
    <item>
      <title>SABLE: Staging Blocked Evaluation of Sparse Matrix Computations</title>
      <link>https://arxiv.org/abs/2407.00829</link>
      <description>arXiv:2407.00829v3 Announce Type: replace-cross 
Abstract: Structured sparsity, like regions of non-zero elements in sparse matrices, can offer optimization opportunities often overlooked by existing solutions that treat matrices as entirely dense or sparse. Block-based approaches, such as BCSR, partially address this issue by choosing between fixed-size blocks which results in wasted computation on zero elements. On the other hand, variable-sized blocks introduce overheads due to variable loop bounds unknown at compile time.
  We present SABLE, a novel staging framework that achieves the best of both approaches by generating region-specific code tailored for variable-sized blocks. SABLE partitions the matrix to identify profitable blocks and specializes generated code for vectorization. We evaluate SABLE on the SpMV kernel using the SuiteSparse collection. SABLE achieves a geomean of $1.07$, $2.73$ and $1.9$ speedup over the state of the art systems: Intel MKL, CSR5 and Partially-Strided Codelets, respectively, single threaded and even more when parallelized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00829v3</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratyush Das, Amirhossein Basareh, Adhitha Dias, Artem Pelenitsyn, Kirshanthan Sundararajah, Milind Kulkarni</dc:creator>
    </item>
    <item>
      <title>OPMOS: Ordered Parallel Algorithm for Multi-Objective Shortest-Paths</title>
      <link>https://arxiv.org/abs/2411.16667</link>
      <description>arXiv:2411.16667v2 Announce Type: replace-cross 
Abstract: The Multi-Objective Shortest-Path (MOS) problem finds a set of Pareto-optimal solutions from a start node to a destination node in a multi-attribute graph. The literature explores multi-objective A*-style algorithmic approaches to solving the NP-hard MOS problem. These approaches use consistent heuristics to compute an exact set of solutions for the goal node. A generalized MOS algorithm maintains a "frontier" of partial paths at each node and performs ordered processing to ensure that Pareto-optimal paths are generated to reach the goal node. The algorithm becomes computationally intractable at a higher number of objectives due to a rapid increase in the search space for non-dominated paths and the significant increase in Pareto-optimal solutions. While prior works have focused on algorithmic methods to reduce the complexity, we tackle this challenge by exploiting parallelism to accelerate the MOS problem. The key insight is that MOS algorithms rely on the ordered execution of partial paths to maintain high work efficiency. The proposed parallel algorithm (OPMOS) unlocks ordered parallelism and efficiently exploits the concurrent execution of multiple paths in MOS. Experimental evaluation using the NVIDIA GH200 Superchip's 72-core Arm-based CPU shows the performance scaling potential of OPMOS on work efficiency and parallelism using a real-world application to ship routing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16667v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3721145.3725781</arxiv:DOI>
      <dc:creator>Leo Gold, Adam Bienkowski, David Sidoti, Krishna Pattipati, Omer Khan</dc:creator>
    </item>
    <item>
      <title>Performant Automatic BLAS Offloading on Unified Memory Architecture with OpenMP First-Touch Style Data Movement</title>
      <link>https://arxiv.org/abs/2501.00279</link>
      <description>arXiv:2501.00279v3 Announce Type: replace-cross 
Abstract: BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00279v3</guid>
      <category>cs.DC</category>
      <category>cs.MS</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junjie Li</dc:creator>
    </item>
  </channel>
</rss>
