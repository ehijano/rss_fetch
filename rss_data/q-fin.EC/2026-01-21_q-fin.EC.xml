<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.EC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.EC</link>
    <description>q-fin.EC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.EC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Jan 2026 05:02:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On Analyzing the Conditions for Stability of Opportunistic Supply Chains Under Network Growth</title>
      <link>https://arxiv.org/abs/2601.11566</link>
      <description>arXiv:2601.11566v1 Announce Type: new 
Abstract: Even large firms such as Walmart, Apple, and Coca-Cola face persistent fluctuations in costs, demand, and raw material availability. These are not \textit{rare events} and cannot be evaluated using traditional disruption models focused on infrequent events. Instead, sustained volatility induces opportunistic behavior, as firms repeatedly reconfigure partners in absence of long-term contracts, often due to trust deficits. The resulting web of transient relationships forms opportunistic supply chains (OSCs). To capture OSC evolution, we develop an integrated mathematical framework combining a Geometric Brownian Motion (GBM) model to represent stochastic price volatility, a Bayesian learning model to describe adaptive belief updates regarding partner reliability, and a Latent Order Logistic (LOLOG) network model for endogenous changes in network structure. This framework is implemented in an agent-based simulation to examine how volatility, trust, and network structure jointly shape SC resilience. Our modeling approach identifies critical volatility threshold; a tipping point beyond which the network shifts from a stable, link-preserving regime to a fragmented regime marked by rapid relationship dissolution. We analytically establish monotonic effects of volatility on profitability, trust, and link activation; derive formal stability conditions and volatility-driven phase transitions, and show how these mechanisms shape node importance and procurement behavior. These theoretical mechanisms are illustrated through computational experiments reflecting industry behaviors in fast fashion, electronics, and perishables. Overall, our contribution is to develop an integrated GBM-Bayesian-LOLOG framework to analyze OSC stability and our model can be extended to other OSCs including humanitarian, pharmaceutical, and poultry networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11566v1</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gurkirat Wadhwa, Priyank Sinha</dc:creator>
    </item>
    <item>
      <title>Measuring growth and convergence at the mesoscale</title>
      <link>https://arxiv.org/abs/2601.12158</link>
      <description>arXiv:2601.12158v1 Announce Type: new 
Abstract: Global inequality has shifted inward, with rising dispersion increasingly occurring within countries rather than between them. Using 8,790 newly harmonised Functional Urban Areas (FUAs), micro-founded labour-market regions encompassing 3.9 billion people and representing approximately 80% of global GDP, we show that national aggregates systematically, and increasingly, misrepresent the dynamics of growth, convergence, and structural change. Drawing on high-resolution global GDP data and country-level capability measures, we find that the middle-income trampoline that previously drove global convergence is flattening. This divergence in the lower-income regime does not reflect poverty traps: low-income FUAs exhibit positive expected growth, and the transition curve displays no stable low-income equilibrium. Instead, productive capabilities, proxied by the Economic Complexity Index, define distinct growth regimes. FUAs converge within capability strata but diverge across them, and capability upgrading follows a predictable J-curve marked by short-run disruption and medium-run acceleration. These findings suggest that national convergence policies may be systematically misaligned with the geographic scale at which capability accumulation operates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12158v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaak Mengesha, Debraj Roy</dc:creator>
    </item>
    <item>
      <title>The Economics of Digital Intelligence Capital: Endogenous Depreciation and the Structural Jevons Paradox</title>
      <link>https://arxiv.org/abs/2601.12339</link>
      <description>arXiv:2601.12339v1 Announce Type: new 
Abstract: This paper develops a micro-founded economic theory of the AI industry by modeling large language models as a distinct asset class-Digital Intelligence Capital-characterized by data-compute complementarities, increasing returns to scale, and relative (rather than absolute) valuation. We show that these features fundamentally reshape industry dynamics along three dimensions. First, because downstream demand depends on relative capability, innovation by one firm endogenously depreciates the economic value of rivals' existing capital, generating a persistent innovation pressure we term the Red Queen Effect. Second, falling inference prices induce downstream firms to adopt more compute-intensive agent architectures, rendering aggregate demand for compute super-elastic and producing a structural Jevons paradox. Third, learning from user feedback creates a data flywheel that can destabilize symmetric competition: when data accumulation outpaces data decay, the market bifurcates endogenously toward a winner-takes-all equilibrium. We further characterize conditions under which expanding upstream capabilities erode downstream application value (the Wrapper Trap). A calibrated agent-based model confirms these mechanisms and their quantitative implications. Together, the results provide a unified framework linking intelligence production upstream with agentic demand downstream, offering new insights into competition, scalability, and regulation in the AI economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12339v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukun Zhang, Tianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Economic complexity and regional development in India: Insights from a state-industry bipartite network</title>
      <link>https://arxiv.org/abs/2601.12356</link>
      <description>arXiv:2601.12356v1 Announce Type: new 
Abstract: This study investigates the economic complexity of Indian states by constructing a state-industry bipartite network using firm-level data on registered companies and their paid-up capital. We compute the Economic Complexity Index and apply the fitness-complexity algorithm to quantify the diversity and sophistication of productive capabilities across the Indian states and two union territories. The results reveal substantial heterogeneity in regional capability structures, with states such as Maharashtra, Karnataka, and Delhi exhibiting consistently high complexity, while others remain concentrated in ubiquitous, low-value industries. The analysis also shows a strong positive relationship between complexity metrics and per-capita Gross State Domestic Product, underscoring the role of capability accumulation in shaping economic performance. Additionally, the number of active firms in India demonstrates a persistent exponential growth at an annual rate of 11.2%, reflecting ongoing formalization and industrial expansion. The ordered binary matrix displays the characteristic triangular structure observed in complexity studies, validating the applicability of complexity frameworks at the sub-national level. This work highlights the usefulness of firm-based data for assessing regional productive structures and emphasizes the importance of capability-oriented strategies for fostering balanced and sustainable development across Indian states. By demonstrating the usefulness of firm registry data in data constrained environments, this study advances the empirical application of economic complexity methods and provides a quantitative foundation for capability-oriented industrial and regional policy in India.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12356v1</guid>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joel M Thomas, Abhijit Chakraborty</dc:creator>
    </item>
    <item>
      <title>Generative AI as a Non-Convex Supply Shock: Market Bifurcation and Welfare Analysis</title>
      <link>https://arxiv.org/abs/2601.12488</link>
      <description>arXiv:2601.12488v1 Announce Type: new 
Abstract: The diffusion of Generative AI (GenAI) constitutes a supply shock of a fundamentally different nature: while marginal production costs approach zero, content generation creates congestion externalities through information pollution. We develop a three-layer general equilibrium framework to study how this non-convex technology reshapes market structure, transition dynamics, and social welfare. In a static vertical differentiation model, we show that the GenAI cost shock induces a kinked production frontier that bifurcates the market into exit, AI, and human segments, generating a ``middle-class hollow'' in the quality distribution. To analyze adjustment paths, we embed this structure in a mean-field evolutionary system and a calibrated agent-based model with bounded rationality. The transition to the AI-integrated equilibrium is non-monotonic: rather than smooth diffusion, the economy experiences a temporary ecological collapse driven by search frictions and delayed skill adaptation, followed by selective recovery. Survival depends on asymmetric skill reconfiguration, whereby humans retreat from technical execution toward semantic creativity. Finally, we show that the welfare impact of AI adoption is highly sensitive to pollution intensity: low congestion yields monotonic welfare gains, whereas high pollution produces an inverted-U relationship in which further AI expansion reduces total welfare. These results imply that laissez-faire adoption can be inefficient and that optimal governance must shift from input regulation toward output-side congestion management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12488v1</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukun Zhang, Tianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Liability Sharing and Staffing in AI-Assisted Online Medical Consultation</title>
      <link>https://arxiv.org/abs/2601.12817</link>
      <description>arXiv:2601.12817v1 Announce Type: new 
Abstract: Liability sharing and staffing jointly determine service quality in AI-assisted online medical consultation, yet their interaction is rarely examined in an integrated framework linking contracts to congestion via physician responses. This paper develops a Stackelberg queueing model where the platform selects a liability share and a staffing level while physicians choose between AI-assisted and independent diagnostic modes. Physician mode choice exhibits a threshold structure, with the critical liability share decreasing in loss severity and increasing in the effort cost of independent diagnosis. Optimal platform policy sets liability below this threshold to trade off risk transfer against compliance costs, revealing that liability sharing and staffing function as substitute safety mechanisms. Higher congestion or staffing costs tilt optimal policy toward AI-assisted operation, whereas elevated loss severity shifts the preferred regime toward independent diagnosis. The welfare gap between platform and social optima widens with loss severity, suggesting greater scope for incentive alignment in high-stakes settings. By endogenizing physician mode choice within a congested service system, this study clarifies how liability design propagates through queueing dynamics, offering guidance for calibrating contracts and capacity in AI-assisted medical consultation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12817v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Xiao</dc:creator>
    </item>
    <item>
      <title>AI Skills Improve Job Prospects: Causal Evidence from a Hiring Experiment</title>
      <link>https://arxiv.org/abs/2601.13286</link>
      <description>arXiv:2601.13286v1 Announce Type: new 
Abstract: The growing adoption of artificial intelligence (AI) technologies has heightened interest in the labour market value of AI-related skills, yet causal evidence on their role in hiring decisions remains scarce. This study examines whether AI skills serve as a positive hiring signal and whether they can offset conventional disadvantages such as older age or lower formal education. We conduct an experimental survey with 1,700 recruiters from the United Kingdom and the United States. Using a paired conjoint design, recruiters evaluated hypothetical candidates represented by synthetically designed resumes. Across three occupations - graphic designer, office assistant, and software engineer - AI skills significantly increase interview invitation probabilities by approximately 8 to 15 percentage points. AI skills also partially or fully offset disadvantages related to age and lower education, with effects strongest for office assistants, where formal AI certification plays an additional compensatory role. Effects are weaker for graphic designers, consistent with more skeptical recruiter attitudes toward AI in creative work. Finally, recruiters' own background and AI usage significantly moderate these effects. Overall, the findings demonstrate that AI skills function as a powerful hiring signal and can mitigate traditional labour market disadvantages, with implications for workers' skill acquisition strategies and firms' recruitment practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13286v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Stephany, Ole Teutloff, Angelo Leone</dc:creator>
    </item>
    <item>
      <title>Human-AI Collaboration in Radiology: The Case of Pulmonary Embolism</title>
      <link>https://arxiv.org/abs/2601.13379</link>
      <description>arXiv:2601.13379v1 Announce Type: new 
Abstract: We study how radiologists use AI to diagnose pulmonary embolism (PE), tracking over 100,000 scans interpreted by nearly 400 radiologists during the staggered rollout of a real-world FDA-approved diagnostic platform in a hospital system. When AI flags PE, radiologists agree 84% of the time; when AI predicts no PE, they agree 97%. Disagreement evolves substantially: radiologists initially reject AI-positive PEs in 30% of cases, dropping to 12% by year two. Despite a 16% increase in scan volume, diagnostic speed remains stable while per-radiologist monthly volumes nearly double, with no change in patient mortality -- suggesting AI improves workflow without compromising outcomes. We document significant heterogeneity in AI collaboration: some radiologists reject AI-flagged PEs half the time while others accept nearly always; female radiologists are 6 percentage points less likely to override AI than male radiologists. Moderate AI engagement is associated with the highest agreement, whereas both low and high engagement show more disagreement. Follow-up imaging reveals that when radiologists override AI to diagnose PE, 54% of subsequent scans show both agreeing on no PE within 30 days.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13379v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Goldsmith-Pinkham, Chenhao Tan, Alexander K. Zentefis</dc:creator>
    </item>
    <item>
      <title>Liabilities for the social cost of carbon</title>
      <link>https://arxiv.org/abs/2601.13834</link>
      <description>arXiv:2601.13834v1 Announce Type: new 
Abstract: We estimate the national social cost of carbon using a recent meta-analysis of the total impact of climate change and a standard integrated assessment model. The average social cost of carbon closely follows per capita income, the national social cost of carbon the size of the population. The national social cost of carbon measures self-harm. Net liability is defined as the harm done by a country's emissions on other countries minus the harm done to a country by other countries' emissions. Net liability is positive in middle-income, carbon-intensive countries. Poor and rich countries would be compensated because their current emissions are relatively low, poor countries additionally because they are vulnerable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13834v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew K. Agrawala, Richard S. J. Tol</dc:creator>
    </item>
    <item>
      <title>How Disruptive is Financial Technology?</title>
      <link>https://arxiv.org/abs/2601.14071</link>
      <description>arXiv:2601.14071v1 Announce Type: new 
Abstract: We study whether Fintech disrupts the banking sector by intensifying competition for scarce deposits funds and raising deposit rates. Using difference-in-difference estimation around the exogenous removal of marketplace platform investing restrictions by US states, we show the cost of deposits increase by approximately 11.5% within small financial institutions. However, these price changes are effective in preventing a drain of liquidity. Size and geographical diversification through branch networks can mitigate the effects of Fintech competition by sourcing deposits from less competitive markets. The findings highlight the unintended consequences of the growing Fintech sector on banks and offer policy insights for regulators and managers into the ongoing development and impact of technology on the banking sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14071v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas Cumming, Hisham Farag, Santosh Koirala, Danny McGowan</dc:creator>
    </item>
    <item>
      <title>Hot Days, Unsafe Schools? The Impact of Heat on School Shootings</title>
      <link>https://arxiv.org/abs/2601.14094</link>
      <description>arXiv:2601.14094v1 Announce Type: new 
Abstract: Using data on school shooting incidents in U.S. K--12 schools from 1981 to 2022, we estimate the causal effects of high temperatures on school shootings and assess the implications of climate change. We find that days with maximum temperatures exceeding 90$^\circ$F lead to a 80\% increase in school shootings relative to days below 70$^\circ$F. Consistent with theories linking heat exposure to aggression, high temperatures increase homicidal and threat-related shootings but have no effect on accidental or suicidal shootings. Heat-induced shootings occur disproportionately during periods of greater student mobility and reduced supervision, including before and after school hours and lunch periods. Higher temperatures increase shootings involving both student and non-student perpetrators. We project that climate change will increase homicidal and threat-related school shootings in the U.S. by 8\% under SSP2--4.5 (moderate emissions) and by 14\% under SSP5--8.5 (high emissions) by 2091--2100, corresponding to approximately 23 and 39 additional shootings per decade, respectively. The present discounted value of the resulting social costs is \$343 million and \$592 million (2025 dollars), respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14094v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seunghyun Lee, Goeun Lee</dc:creator>
    </item>
    <item>
      <title>Foreign influencer operations: How TikTok shapes American perceptions of China</title>
      <link>https://arxiv.org/abs/2601.14118</link>
      <description>arXiv:2601.14118v1 Announce Type: new 
Abstract: How do authoritarian regimes strengthen global support for nondemocratic political systems? Roughly half of the users of the social media platform TikTok report getting news from social media influencers. Against this backdrop, authoritarian regimes have increasingly outsourced content creation to these influencers. To gain understanding of the extent of this phenomenon and the persuasive capabilities of these influencers, we collect comprehensive data on pro-China influencers on TikTok. We show that pro-China influencers have more engagement than state media. We then create a realistic clone of the TikTok app, and conduct a randomized experiment in which over 8,500 Americans are recruited to use this app and view a random sample of actual TikTok content. We show that pro-China foreign influencers are strikingly effective at increasing favorability toward China, while traditional Chinese state media causes backlash. The findings highlight the importance of influencers in shaping global public opinion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14118v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trevor Incerti, Jonathan Elkobi, Daniel Mattingly</dc:creator>
    </item>
    <item>
      <title>Trade relationships during and after a crisis</title>
      <link>https://arxiv.org/abs/2601.14150</link>
      <description>arXiv:2601.14150v1 Announce Type: new 
Abstract: I study how firms adjust to temporary disruptions in international trade relationships organized through relational contracts. I exploit an extreme, plausibly exogenous weather shock during the 2010-11 La Ni\~na season that restricted Colombian flower exporters' access to cargo terminals. Using transaction-level data from the Colombian-U.S. flower trade, I show that importers with less-exposed supplier portfolios are less likely to terminate disrupted relationships, instead tolerating shipment delays. In contrast, firms facing greater exposure experience higher partner turnover and are more likely to exit the market, with exit accounting for a substantial share of relationship separations. These findings demonstrate that idiosyncratic shocks to buyer-seller relationships can propagate into persistent changes in firms' trading portfolios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14150v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alejandra Martinez</dc:creator>
    </item>
    <item>
      <title>Latent Variable Phillips Curve</title>
      <link>https://arxiv.org/abs/2601.11601</link>
      <description>arXiv:2601.11601v1 Announce Type: cross 
Abstract: This paper re-examines the empirical Phillips curve (PC) model and its usefulness in the context of medium-term inflation forecasting. A latent variable Phillips curve hypothesis is formulated and tested using 3,968 randomly generated factor combinations. Evidence from US core PCE inflation between Q1 1983 and Q1 2025 suggests that latent variable PC models reliably outperform traditional PC models six to eight quarters ahead and stand a greater chance of outperforming a univariate benchmark. Incorporating an MA(1) residual process improves the accuracy of empirical PC models across the board, although the gains relative to univariate models remain small. The findings presented in this paper have two important implications: First, they corroborate a new conceptual view on the Phillips curve theory; second, they offer a novel path towards improving the competitiveness of Phillips curve forecasts in future empirical work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11601v1</guid>
      <category>q-fin.ST</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniil Bargman, Francesca Medda, Akash Sedai Sharma</dc:creator>
    </item>
    <item>
      <title>The Dynamic and Endogenous Behavior of Re-Offense Risk: An Agent-Based Simulation Study of Treatment Allocation in Incarceration Diversion Programs</title>
      <link>https://arxiv.org/abs/2601.12441</link>
      <description>arXiv:2601.12441v1 Announce Type: cross 
Abstract: Incarceration-diversion treatment programs aim to improve societal reintegration and reduce recidivism, but limited capacity forces policymakers to make prioritization decisions that often rely on risk assessment tools. While predictive, these tools typically treat risk as a static, individual attribute, which overlooks how risk evolves over time and how treatment decisions shape outcomes through social interactions. In this paper, we develop a new framework that models reoffending risk as a human-system interaction, linking individual behavior with system-level dynamics and endogenous community feedback. Using an agent-based simulation calibrated to U.S. probation data, we evaluate treatment allocation policies under different capacity constraints and incarceration settings. Our results show that no single prioritization policy dominates. Instead, policy effectiveness depends on temporal windows and system parameters: prioritizing low-risk individuals performs better when long-term trajectories matter, while prioritizing high-risk individuals becomes more effective in the short term or when incarceration leads to shorter monitoring periods. These findings highlight the need to evaluate risk-based decision systems as sociotechnical systems with long-term accountability, rather than as isolated predictive tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12441v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chuwen Zhang, Pengyi Shi, Amy Ward</dc:creator>
    </item>
    <item>
      <title>Conservation priorities to prevent the next pandemic</title>
      <link>https://arxiv.org/abs/2601.13349</link>
      <description>arXiv:2601.13349v1 Announce Type: cross 
Abstract: Diseases originating from wildlife pose a significant threat to global health, causing human and economic losses each year. The transmission of disease from animals to humans occurs at the interface between humans, livestock, and wildlife reservoirs, influenced by abiotic factors and ecological mechanisms. Although evidence suggests that intact ecosystems can reduce transmission, disease prevention has largely been neglected in conservation efforts and remains underfunded compared to mitigation. A major constraint is the lack of reliable, spatially explicit information to guide efforts effectively. Given the increasing rate of new disease emergence, accelerated by climate change and biodiversity loss, identifying priority areas for mitigating the risk of disease transmission is more crucial than ever. We present new high-resolution (1 km) maps of priority areas for targeted ecological countermeasures aimed at reducing the likelihood of zoonotic spillover, along with a methodology adaptable to local contexts. Our study compiles data on well-documented risk factors, protection status, forest restoration potential, and opportunity cost of the land to map areas with high potential for cost-effective interventions. We identify low-cost priority areas across 50 countries, including 277,000 km2 where environmental restoration could mitigate the risk of zoonotic spillover and 198,000 km2 where preventing deforestation could do the same, 95% of which are not currently under protection. The resulting layers, covering tropical regions globally, are freely available alongside an interactive no-code platform that allows users to adjust parameters and identify priority areas at multiple scales. Ecological countermeasures can be a cost-effective strategy for reducing the emergence of new pathogens; however, our study highlights the extent to which current conservation efforts fall short of this goal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13349v1</guid>
      <category>q-bio.PE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Leonardo Viotti, Luis Diego Herrera, Garo Batmanian, Franck Berthe, Rachael Kramp</dc:creator>
    </item>
    <item>
      <title>A uniformity principle for spatial matching</title>
      <link>https://arxiv.org/abs/2601.13426</link>
      <description>arXiv:2601.13426v1 Announce Type: cross 
Abstract: Platforms matching spatially distributed supply to demand face a fundamental design choice: given a fixed total budget of service range, how should it be allocated across supply nodes ex ante, i.e. before supply and demand locations are realized, to maximize fulfilled demand? We model this problem using bipartite random geometric graphs where $n$ supply and $m$ demand nodes are uniformly distributed on $[0,1]^k$ ($k \ge 1$), and edges form when demand falls within a supply node's service region, the volume of which is determined by its service range. Since each supply node serves at most one demand, platform performance is determined by the expected size of a maximum matching. We establish a uniformity principle: whenever one service range allocation is more uniform than the other, the more uniform allocation yields a larger expected matching. This principle emerges from diminishing marginal returns to range expanding service range, and limited interference between supply nodes due to bounded ranges naturally fragmenting the graph. For $k=1$, we further characterize the expected matching size through a Markov chain embedding and derive closed-form expressions for special cases. Our results provide theoretical guidance for optimizing service range allocation and designing incentive structures in ride-hailing, on-demand labor markets, and drone delivery networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13426v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taha Ameen, Flore Sentenac, Sophie H. Yu</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap Between Estimated and True Regret Towards Reliable Regret Estimation in Deep Learning based Mechanism Design</title>
      <link>https://arxiv.org/abs/2601.13489</link>
      <description>arXiv:2601.13489v1 Announce Type: cross 
Abstract: Recent advances, such as RegretNet, ALGnet, RegretFormer and CITransNet, use deep learning to approximate optimal multi item auctions by relaxing incentive compatibility (IC) and measuring its violation via ex post regret. However, the true accuracy of these regret estimates remains unclear. Computing exact regret is computationally intractable, and current models rely on gradient based optimizers whose outcomes depend heavily on hyperparameter choices. Through extensive experiments, we reveal that existing methods systematically underestimate actual regret (In some models, the true regret is several hundred times larger than the reported regret), leading to overstated claims of IC and revenue. To address this issue, we derive a lower bound on regret and introduce an efficient item wise regret approximation. Building on this, we propose a guided refinement procedure that substantially improves regret estimation accuracy while reducing computational cost. Our method provides a more reliable foundation for evaluating incentive compatibility in deep learning based auction mechanisms and highlights the need to reassess prior performance claims in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13489v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuyuan You, Zhiqiang Zhuang, Kewen Wang, Zhe Wang</dc:creator>
    </item>
    <item>
      <title>BallotRank: A Condorcet Completion Method for Graphs</title>
      <link>https://arxiv.org/abs/2601.14015</link>
      <description>arXiv:2601.14015v1 Announce Type: cross 
Abstract: We introduce BallotRank, a ranked preference aggregation method derived from a modified PageRank algorithm. It is a Condorcet-consistent method without damping, and empirical examination of nearly 2,000 ranked choice elections and over 20,000 internet polls confirms that BallotRank always identifies the Condorcet winner at conventional values of the damping parameter. We also prove that the method satisfies many of the same social choice criteria as other well-known Condorcet completion methods, but it has the advantage of being a natural social welfare function that provides a full ranking of the candidates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14015v1</guid>
      <category>cs.GT</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ismar Volic, Jason Douglas Todd</dc:creator>
    </item>
    <item>
      <title>A simple model of interbank trading with tiered remuneration</title>
      <link>https://arxiv.org/abs/2006.10946</link>
      <description>arXiv:2006.10946v2 Announce Type: replace 
Abstract: Many countries have adopted negative interest rate policies with tiering remuneration, which allows for exemption from negative rates. This practice has led to higher interbank trading volumes, with market rates ranging between zero and the negative remuneration rates. This study proposes a basic model of an interbank market with tiering remuneration that can be tested with actual market data because of its simplicity and can indicate the level of the market rate created by the different exemption levels. By generalizing the model, we found that a tiering system is also suitable for maintaining a higher trading activity, regardless of the level of the remuneration rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.10946v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toshifumi Nakamura</dc:creator>
    </item>
    <item>
      <title>Market-Based Asset Price Probability</title>
      <link>https://arxiv.org/abs/2205.07256</link>
      <description>arXiv:2205.07256v5 Announce Type: replace 
Abstract: The random values and volumes of consecutive trades made at the exchange with shares of security determine its mean, variance, and higher statistical moments. The volume weighted average price (VWAP) is the simplest example of such a dependence. We derive the dependence of the market-based variance and 3rd statistical moment of prices on the means, variances, covariances, and 3rd moments of the values and volumes of market trades. The usual frequency-based assessments of statistical moments of prices are the limited case of market-based statistical moments if we assume that all volumes of consecutive trades with security are constant during the averaging interval. To forecast market-based variance of price, one should predict the first two statistical moments and the correlation of values and volumes of consecutive trades at the same horizon. We explain how that limits the number of predicted statistical moments of prices by the first two and the accuracy of the forecasts of the price probability by the Gaussian distribution. This limitation also reduces the reliability of Value-at-Risk by Gaussian approximation. The accounting for the randomness of trade volumes and the use of VWAP results in zero price-volume correlations. To study the price-volume empirical statistical dependence, one should calculate correlations of prices and squares of trade volumes or correlations of squares of prices and volumes. To improve the accuracy and reliability of large macroeconomic and market models like those developed by BlackRock's Aladdin, JP Morgan, and the U.S. Fed., the developers should explicitly account for the impact of random trade volumes and use market-based statistical moments of asset prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.07256v5</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.GN</category>
      <category>q-fin.PR</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Olkhov</dc:creator>
    </item>
    <item>
      <title>Effective and Scalable Programs to Facilitate Labor Market Transitions for Women in Technology</title>
      <link>https://arxiv.org/abs/2211.09968</link>
      <description>arXiv:2211.09968v5 Announce Type: replace 
Abstract: We evaluate two interventions facilitating technology-sector transitions for women in Poland: Mentoring, focused on expanding professional networks, and Challenges, focused on building credible skill signals. Randomizing oversubscribed admissions, we find both programs substantially increase technology employment at twelve months - by 15 percentage points for Mentoring and 11 p.p. for Challenges. The distinct mechanisms through which the programs operate translate to heterogeneous treatment effects across geography, career stage, and baseline credentials. These differential effects create scope for improved allocation: algorithmic targeting across programs outperforms random assignment by 86% and experts' selection into Mentoring by 11%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.09968v5</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susan Athey, Emil Palikot</dc:creator>
    </item>
    <item>
      <title>Database for the meta-analysis of the social cost of carbon (v2026.1)</title>
      <link>https://arxiv.org/abs/2402.09125</link>
      <description>arXiv:2402.09125v4 Announce Type: replace 
Abstract: A new version of the database for the meta-analysis of estimates of the social cost of carbon is presented. New records were added, and new fields on gender and stochasticity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09125v4</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard S. J. Tol</dc:creator>
    </item>
    <item>
      <title>The Turing Valley: How AI Capabilities Shape Labor Income</title>
      <link>https://arxiv.org/abs/2408.16443</link>
      <description>arXiv:2408.16443v2 Announce Type: replace 
Abstract: Current AI systems are better than humans in some knowledge dimensions but weaker in others. Guided by the long-standing vision of machine intelligence inspired by the Turing Test, AI developers increasingly seek to eliminate this "jagged" nature by pursuing Artificial General Intelligence (AGI) that surpasses human knowledge across domains. This pursuit has sparked an important debate, with leading economists arguing that AGI risks eroding the value of human capital. We contribute to this debate by showing how AI capabilities in different dimensions shape labor income in a multidimensional knowledge economy. AI improvements in dimensions where it is stronger than humans always increase labor income, but the effects of AI progress in dimensions where it is weaker than humans depend on the nature of human-AI communication. When communication allows the integration of partial solutions, improvements in AI's weak dimensions reduce the marginal product of labor, and labor income is maximized by a deliberately jagged form of AI. In contrast, when communication is limited to sharing full solutions, improvements in AI's weak dimensions can raise the marginal product of labor, and labor income can be maximized when AI achieves high performance across all dimensions. These results point to the importance of empirically assessing the additivity properties of human-AI communication for understanding the labor-market consequences of progress toward AGI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16443v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Enrique Ide, Eduard Talam\`as</dc:creator>
    </item>
    <item>
      <title>Uncertain and Asymmetric Forecasts</title>
      <link>https://arxiv.org/abs/2411.05938</link>
      <description>arXiv:2411.05938v2 Announce Type: replace 
Abstract: This paper develops distribution-based measures that extract policy-relevant information from subjective probability distributions beyond point forecasts. We introduce two complementary indicators that operationalize the second and third moments of beliefs. First, a Normalized Uncertainty measure applies a variance-stabilizing transformation that removes mechanical level effects around policy-relevant anchors. Empirically, uncertainty behaves as a state variable: it amplifies perceived de-anchoring following monetary-policy shocks and weakens and delays pass-through to credit conditions, particularly across loan maturities. Second, an Asymmetry Coherence indicator combines the median and skewness of subjective distributions to identify coherent directional tail risks. Directional asymmetry is largely orthogonal to uncertainty and is primarily reflected in monetary-policy responses rather than real activity. Overall, the results show that properly measured uncertainty governs state-dependent transmission, while distributional asymmetries convey distinct information about macroeconomic risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05938v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Vansteenberghe</dc:creator>
    </item>
    <item>
      <title>Sectorial Exclusion Criteria in the Marxist Analysis of the Average Rate of Profit: The United States Case (1960-2020)</title>
      <link>https://arxiv.org/abs/2501.06270</link>
      <description>arXiv:2501.06270v2 Announce Type: replace 
Abstract: The long term estimation of the Marxist average rate of profit does not adhere to a theoretically grounded standard regarding which economic activities should or should not be included for such purposes, which is relevant because methodological non uniformity can be a significant source of overestimation or underestimation, generating a less accurate reflection of the capital accumulation dynamics. This research aims to provide a standard Marxist decision criterion regarding the inclusion and exclusion of economic activities for the calculation of the Marxist average profit rate for the case of United States economic sectors from 1960 to 2020, based on the Marxist definition of productive labor, its location in the circuit of capital, and its relationship with the production of surplus value. Using wavelet transformed Daubechies filters with increased symmetry, empirical mode decomposition, Hodrick Prescott filter embedded in unobserved components model, and a wide variety of unit root tests the internal theoretical consistency of the presented criteria is evaluated. Also, the objective consistency of the theory is evaluated by a dynamic factor autoregressive model, Principal Component Analysis via Singular Value Decomposition, and regularized Horseshoe regression. The results are consistent both theoretically and econometrically with the logic of Classical Marxist political economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06270v2</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Mauricio Gomez Julian</dc:creator>
    </item>
    <item>
      <title>Trade and pollution: Evidence from India</title>
      <link>https://arxiv.org/abs/2502.09289</link>
      <description>arXiv:2502.09289v3 Announce Type: replace 
Abstract: What happens to pollution when developing countries open their borders to trade? Theoretical predictions are ambiguous, and empirical evidence remains limited. We study the effects of the 1991 Indian trade liberalization reform on water pollution. The reform abruptly and unexpectedly lowered import tariffs, increasing exposure to trade. Larger tariff reductions are associated with relative increases in water pollution. The estimated effects imply a 0.11 standard deviation increase in water pollution for the median district exposed to the tariff reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09289v3</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malin Niemi, Nicklas Nordfors, Anna Tompsett</dc:creator>
    </item>
    <item>
      <title>Do Determinants of EV Purchase Intent vary across the Spectrum? Evidence from Bayesian Analysis of US Survey Data</title>
      <link>https://arxiv.org/abs/2504.09854</link>
      <description>arXiv:2504.09854v3 Announce Type: replace 
Abstract: While electric vehicle (EV) adoption has been widely studied, most research focuses on the average effects of predictors on purchase intent, overlooking variation across the distribution of EV purchase intent. This paper makes a threefold contribution by analyzing four unique explanatory variables, leveraging large-scale US survey data from 2021 to 2023, and employing Bayesian ordinal probit and Bayesian ordinal quantile modeling to evaluate the effects of these variables-while controlling for other commonly used covariates-on EV purchase intent, both on average and across its full distribution. By modeling purchase intent as an ordered outcome-from "not at all likely" to "very likely"-we reveal how covariate effects differ across levels of interest. This is the first application of ordinal quantile modeling in the EV adoption literature, uncovering heterogeneity in how potential buyers respond to key factors. For instance, confidence in development of charging infrastructure and belief in environmental benefits are linked not only to higher interest among likely adopters but also to reduced resistance among more skeptical respondents. Notably, we identify a gap between the prevalence and influence of key predictors: although few respondents report strong infrastructure confidence or frequent EV information exposure, both factors are strongly associated with increased intent across the spectrum. These findings suggest clear opportunities for targeted communication and outreach, alongside infrastructure investment, to support widespread EV adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09854v3</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nafisa Lohawala, Mohammad Arshad Rahman</dc:creator>
    </item>
    <item>
      <title>A Unified Metric Architecture for AI Infrastructure: A Cross-Layer Taxonomy Integrating Performance, Efficiency, and Cost</title>
      <link>https://arxiv.org/abs/2511.21772</link>
      <description>arXiv:2511.21772v4 Announce Type: replace 
Abstract: The growth of large-scale AI systems is increasingly constrained by infrastructure limits: power availability, thermal and water constraints, interconnect scaling, memory pressure, data-pipeline throughput, and rapidly escalating lifecycle cost. Across hyperscale clusters, these constraints interact, yet the main metrics remain fragmented. Existing metrics, ranging from facility measures (PUE) and rack power density to network metrics (all-reduce latency), data-pipeline measures, and financial metrics (TCO series), each capture only their own domain and provide no integrated view of how physical, computational, and economic constraints interact. This fragmentation obscures the structural relationships among energy, computation, and cost, preventing a coherent optimization across sector and how bottlenecks emerge, propagate, and jointly determine the efficiency frontier of AI infrastructure.
  This paper develops an integrated framework that unifies these disparate metrics through a three-domain semantic classification and a six-layer architectural decomposition, producing a 6x3 taxonomy that maps how various sectors propagate across the AI infrastructure stack. The taxonomy is grounded in a systematic review and meta-analysis of all metrics with economic and financial relevance, identifying the most widely used measures, their research intensity, and their cross-domain interdependencies. Building on this evidence base, the Metric Propagation Graph (MPG) formalizes cross-layer dependencies, enabling systemwide interpretation, composite-metric construction, and multi-objective optimization of energy, carbon, and cost.
  The framework offers a coherent foundation for benchmarking, cluster design, capacity planning, and lifecycle economic analysis by linking physical operations, computational efficiency, and cost outcomes within a unified analytic structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21772v4</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi He</dc:creator>
    </item>
    <item>
      <title>Decision Rules in Choice Under Risk</title>
      <link>https://arxiv.org/abs/2601.02964</link>
      <description>arXiv:2601.02964v2 Announce Type: replace 
Abstract: We study choice among lotteries in which the decision maker chooses from a small library of decision rules. At each menu, the applied rule must make the realized choice a strict improvement under a dominance benchmark on perceived lotteries. We characterize the maximal Herfindahl-Hirschman concentration of rule shares over all locally admissible assignments, and diagnostics that distinguish rules that unify behavior across many menus from rules that mainly act as substitutes. We provide a MIQP formulation, a scalable heuristic, and a finite-sample permutation test of excess concentration relative to a menu-independent random-choice benchmark. Applied to the CPC18 dataset (N=686 subjects, each making 500-700 repeated binary lottery choices), the mean MRCI is 0.545, and 64.1% of subjects reject random choice at the 1% level. Concentration gains are primarily driven by modal-payoff focusing, salience-thinking, and regret-based comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02964v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avner Seror</dc:creator>
    </item>
    <item>
      <title>Two-Step Regularized HARX to Measure Volatility Spillovers in Multi-Dimensional Systems</title>
      <link>https://arxiv.org/abs/2601.03146</link>
      <description>arXiv:2601.03146v3 Announce Type: replace 
Abstract: We identify volatility spillovers across commodities, equities, and treasuries using a hybrid HAR-ElasticNet framework on daily realized volatility for six futures markets over 2002--2025. Our two step procedure estimates own-volatility dynamics via OLS to preserve persistence, then applies ElasticNet regularization to cross-market spillovers. The sparse network structure that emerges shows equity markets (ES, NQ) act as the primary volatility transmitters, while crude oil (CL) ends up being the largest receiver of cross-market shocks. Agricultural commodities stay isolated from the larger network. A simple univariate HAR model achieves equally performing point forecasts as our model, but our approach reveals network structure that univariate models cannot. Joint Impulse Response Functions trace how shocks propagate through the network. Our contribution is to demonstrate that hybrid estimation methods can identify meaningful spillover pathways while preserving forecast performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03146v3</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mindy L. Mallory</dc:creator>
    </item>
    <item>
      <title>The Connection Between Monetary Policy and Housing Prices: Public Perception and Expert Communication</title>
      <link>https://arxiv.org/abs/2601.08957</link>
      <description>arXiv:2601.08957v2 Announce Type: replace 
Abstract: We study how the general public perceives the link between monetary policy and housing markets. Using a large-scale, cross-country survey experiment in Austria, Germany, Italy, Sweden, and the United Kingdom, we examine households' understanding of monetary policy, their beliefs about its impact on house prices, and how these beliefs respond to expert information. We find that while most respondents grasp the basic mechanisms of conventional monetary policy and recognize the connection between interest rates and house prices, literacy regarding unconventional monetary policy is very low. Beliefs about the monetary policy-housing nexus are malleable and respond to information, particularly when it is provided by academic economists rather than central bankers. Monetary policy literacy is strongly related to education, gender, age, and experience in housing and mortgage markets. Our results highlight the central role of housing in how households interpret monetary policy and point to the importance of credible and inclusive communication strategies for effective policy transmission.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08957v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Philipp Poyntner, Sofie R. Waltl</dc:creator>
    </item>
  </channel>
</rss>
