<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 Aug 2024 04:01:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Convex Geometry of Network Flows</title>
      <link>https://arxiv.org/abs/2408.12761</link>
      <description>arXiv:2408.12761v1 Announce Type: new 
Abstract: In this paper, we derive a number of interesting properties and extensions of the convex flow problem from the perspective of convex geometry. We show that the sets of allowable flows always can be imbued with a downward closure property, which leads to a useful `calculus' of flows, allowing easy combination and splitting of edges. We then derive a conic form for the convex flow problem, which we show is equivalent to the original problem and almost self-dual. Using this conic form, we consider the nonconvex flow problem with fixed costs on the edges, i.e., where there is some fixed cost to send any nonzero flow over an edge. We show that this problem has almost integral solutions by a Shapley--Folkman argument, and we describe a rounding scheme that works well in practice. Additionally, we provide a heuristic for this nonconvex problem which is a simple modification of our original algorithm. We conclude by discussing a number of interesting avenues for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12761v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Theo Diamandis, Guillermo Angeris</dc:creator>
    </item>
    <item>
      <title>Stochastic linear-quadratic differential game with Markovian jumps in an infinite horizon</title>
      <link>https://arxiv.org/abs/2408.12818</link>
      <description>arXiv:2408.12818v1 Announce Type: new 
Abstract: This paper investigates a two-person non-homogeneous linear-quadratic stochastic differential game (LQ-SDG, for short) in an infinite horizon for a system regulated by a time-invariant Markov chain. Both non-zero-sum and zero-sum LQ-SDG problems are studied. It is shown that the zero-sum LQ-SDG problem can be considered a special non-zero-sum LQ-SDG problem. The open-loop Nash equilibrium point of non-zero-sum (zero-sum, respectively) LQ-SDG problem is characterized by the solvability of a system of constrained forward-backward stochastic differential equations (FBSDEs, for short) in an infinite horizon and the convexity (convexity-concavity, respectively) of the performance functional and their corresponding closed-loop Nash equilibrium strategy are characterized by the solvability of a system of constrained coupled algebra Riccati equations (CAREs, for short) with certain stabilizing conditions. In addition, the closed-loop representation of open-loop Nash equilibrium point for non-zero-sum (zero-sum, respectively) LQ-SDG is provided by the non-symmetric (symmetric, respectively) solution to a system CAREs. At the end of this paper, we provide three concrete examples and solve their open-loop/closed-loop Nash equilibrium strategy based on the obtained results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12818v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Wu, Xun Li, Jie Xiong, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>Nonzero-sum Discrete-time Stochastic Games with Risk-sensitive Ergodic Cost Criterion</title>
      <link>https://arxiv.org/abs/2408.12849</link>
      <description>arXiv:2408.12849v1 Announce Type: new 
Abstract: In this paper we study infinite horizon nonzero-sum stochastic games for controlled discrete-time Markov chains on a Polish state space with risk-sensitive ergodic cost criterion. Under suitable assumptions we show that the associated ergodic optimality equations admit unique solutions. Finally, the existence of Nash-equilibrium in randomized stationary strategies is established by showing that an appropriate set-valued map has a fixed point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12849v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bivakar Bose, Chandan Pal, Somnath Pradhan, Subhamay Saha</dc:creator>
    </item>
    <item>
      <title>CMA-ES for Discrete and Mixed-Variable Optimization on Sets of Points</title>
      <link>https://arxiv.org/abs/2408.13046</link>
      <description>arXiv:2408.13046v1 Announce Type: new 
Abstract: Discrete and mixed-variable optimization problems have appeared in several real-world applications. Most of the research on mixed-variable optimization considers a mixture of integer and continuous variables, and several integer handlings have been developed to inherit the optimization performance of the continuous optimization methods to mixed-integer optimization. In some applications, acceptable solutions are given by selecting possible points in the disjoint subspaces. This paper focuses on the optimization on sets of points and proposes an optimization method by extending the covariance matrix adaptation evolution strategy (CMA-ES), termed the CMA-ES on sets of points (CMA-ES-SoP). The CMA-ES-SoP incorporates margin correction that maintains the generation probability of neighboring points to prevent premature convergence to a specific non-optimal point, which is an effective integer-handling technique for CMA-ES. In addition, because margin correction with a fixed margin value tends to increase the marginal probabilities for a portion of neighboring points more than necessary, the CMA-ES-SoP updates the target margin value adaptively to make the average of the marginal probabilities close to a predefined target probability. Numerical simulations demonstrated that the CMA-ES-SoP successfully optimized the optimization problems on sets of points, whereas the naive CMA-ES failed to optimize them due to premature convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13046v1</guid>
      <category>math.OC</category>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kento Uchida, Ryoki Hamano, Masahiro Nomura, Shota Saito, Shinichi Shirakawa</dc:creator>
    </item>
    <item>
      <title>Constructing Tight Quadratic Relaxations for Global Optimization: I. Outer-Approximating Twice-Differentiable Convex Functions</title>
      <link>https://arxiv.org/abs/2408.13053</link>
      <description>arXiv:2408.13053v1 Announce Type: new 
Abstract: When computing bounds, spatial branch-and-bound algorithms often linearly outer approximate convex relaxations for non-convex expressions in order to capitalize on the efficiency and robustness of linear programming solvers. Considering that linear outer approximations sacrifice accuracy when approximating highly nonlinear functions and recognizing the recent advancements in the efficiency and robustness of available methods to solve optimization problems with quadratic objectives and constraints, we contemplate here the construction of quadratic outer approximations of twice-differentiable convex functions for use in deterministic global optimization. To this end, we present a novel cutting-plane algorithm that determines the tightest scaling parameter, $\alpha$, in the second-order Taylor series approximation quadratic underestimator proposed by Su et al. We use a representative set of convex functions extracted from optimization benchmark libraries to showcase--qualitatively and quantitatively--the tightness of the constructed quadratic underestimators and to demonstrate the overall computational efficiency of our algorithm. Furthermore, we extend our construction procedure to generate even tighter quadratic underestimators by allowing overestimation in infeasible polyhedral regions of optimization problems, as informed by the latter's linear constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13053v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William R. Strahl, Arvind U. Raghunathan, Nikolaos V. Sahinidis, Chrysanthos E. Gounaris</dc:creator>
    </item>
    <item>
      <title>Constructing Tight Quadratic Relaxations for Global Optimization: II. Underestimating Difference-of-Convex (D.C.) Functions</title>
      <link>https://arxiv.org/abs/2408.13058</link>
      <description>arXiv:2408.13058v1 Announce Type: new 
Abstract: Recent advances in the efficiency and robustness of algorithms solving convex quadratically constrained quadratic programming (QCQP) problems motivate developing techniques for creating convex quadratic relaxations that, although more expensive to compute, provide tighter bounds than their classical linear counterparts. In the first part of this two-paper series [Strahl et al., 2024], we developed a cutting plane algorithm to construct convex quadratic underestimators for twice-differentiable convex functions, which we extend here to address the case of non-convex difference-of-convex (d.c.) functions as well. Furthermore, we generalize our approach to consider a hierarchy of quadratic forms, thereby allowing the construction of even tighter underestimators. On a set of d.c. functions extracted from benchmark libraries, we demonstrate noteworthy reduction in the hypervolume between our quadratic underestimators and linear ones constructed at the same points. Additionally, we construct convex QCQP relaxations at the root node of a spatial branch-and-bound tree for a set of systematically created d.c. optimization problems in up to four dimensions, and we show that our relaxations reduce the gap between the lower bound computed by the state-of-the-art global optimization solver BARON and the optimal solution by an excess of 90%, on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13058v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William R. Strahl, Arvind U. Raghunathan, Nikolaos V. Sahinidis, Chrysanthos E. Gounaris</dc:creator>
    </item>
    <item>
      <title>Adaptive Backtracking For Faster Optimization</title>
      <link>https://arxiv.org/abs/2408.13150</link>
      <description>arXiv:2408.13150v1 Announce Type: new 
Abstract: Backtracking line search is foundational in numerical optimization. The basic idea is to adjust the step size of an algorithm by a constant factor until some chosen criterion (e.g. Armijo, Goldstein, Descent Lemma) is satisfied. We propose a new way for adjusting step sizes, replacing the constant factor used in regular backtracking with one that takes into account the degree to which the chosen criterion is violated, without additional computational burden. For convex problems, we prove adaptive backtracking requires fewer adjustments to produce a feasible step size than regular backtracking does for two popular line search criteria: the Armijo condition and the descent lemma. For nonconvex smooth problems, we additionally prove adaptive backtracking enjoys the same guarantees of regular backtracking. Finally, we perform a variety of experiments on over fifteen real world datasets, all of which confirm that adaptive backtracking often leads to significantly faster optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13150v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joao V. Cavalcanti, Laurent Lessard, Ashia C. Wilson</dc:creator>
    </item>
    <item>
      <title>Robust Confidence Bands for Stochastic Processes Using Simulation</title>
      <link>https://arxiv.org/abs/2408.13183</link>
      <description>arXiv:2408.13183v1 Announce Type: new 
Abstract: We propose a robust optimization approach for constructing confidence bands for stochastic processes using a finite number of simulated sample paths. Our approach can be used to quantify uncertainty in realizations of stochastic processes or validate stochastic simulation models by checking whether historical paths from the actual system fall within the constructed confidence band. Unlike existing approaches in the literature, our methodology is widely applicable and directly addresses optimization bias within the constraints, producing tight confidence bands with accurate coverage probabilities. It is tractable, being only slightly more complex than the state-of-the-art baseline approach, and easy to use, as it employs standard techniques. Additionally, our approach is also applicable to continuous-time processes after appropriately discretizing time. In our first case study, we show that our approach achieves the desired coverage probabilities with an order-of-magnitude fewer sample paths than the state-of-the-art baseline approach. In our second case study, we illustrate how our approach can be used to validate stochastic simulation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13183v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Timothy Chan, Jangwon Park, Vahid Sarhangian</dc:creator>
    </item>
    <item>
      <title>Quantization-free Lossy Image Compression Using Integer Matrix Factorization</title>
      <link>https://arxiv.org/abs/2408.12691</link>
      <description>arXiv:2408.12691v1 Announce Type: cross 
Abstract: Lossy image compression is essential for efficient transmission and storage. Traditional compression methods mainly rely on discrete cosine transform (DCT) or singular value decomposition (SVD), both of which represent image data in continuous domains and therefore necessitate carefully designed quantizers. Notably, SVD-based methods are more sensitive to quantization errors than DCT-based methods like JPEG. To address this issue, we introduce a variant of integer matrix factorization (IMF) to develop a novel quantization-free lossy image compression method. IMF provides a low-rank representation of the image data as a product of two smaller factor matrices with bounded integer elements, thereby eliminating the need for quantization. We propose an efficient, provably convergent iterative algorithm for IMF using a block coordinate descent (BCD) scheme, with subproblems having closed-form solutions. Our experiments on the Kodak and CLIC 2024 datasets demonstrate that our IMF compression method consistently outperforms JPEG at low bit rates below 0.25 bits per pixel (bpp) and remains comparable at higher bit rates. We also assessed our method's capability to preserve visual semantics by evaluating an ImageNet pre-trained classifier on compressed images. Remarkably, our method improved top-1 accuracy by over 5 percentage points compared to JPEG at bit rates under 0.25 bpp. The project is available at https://github.com/pashtari/lrf .</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12691v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pooya Ashtari, Pourya Behmandpoor, Fateme Nateghi Haredasht, Jonathan H. Chen, Panagiotis Patrinos, Sabine Van Huffel</dc:creator>
    </item>
    <item>
      <title>Asymptotics for Optimal Empirical Quantization of Measures</title>
      <link>https://arxiv.org/abs/2408.12924</link>
      <description>arXiv:2408.12924v1 Announce Type: cross 
Abstract: We investigate the minimal error in approximating a general probability measure $\mu$ on $\mathbb{R}^d$ by the uniform measure on a finite set with prescribed cardinality $n$. The error is measured in the $p$-Wasserstein distance. In particular, when $1\le p&lt;d$, we establish asymptotic upper and lower bounds as $n \to \infty$ on the rescaled minimal error that have the same, explicit dependency on $\mu$.
  In some instances, we prove that the rescaled minimal error has a limit. These include general measures in dimension $d = 2$ with $1 \le p &lt; 2$, and uniform measures in arbitrary dimension with $1 \le p &lt; d$. For some uniform measures, we prove the limit existence for $p \ge d$ as well.
  For a class of compactly supported measures with H\"older densities, we determine the convergence speed of the minimal error for every $p \ge 1$.
  Furthermore, we establish a new Pierce-type (i.e., nonasymptotic) upper estimate of the minimal error when $1 \le p &lt; d$.
  In the initial sections, we survey the state of the art and draw connections with similar problems, such as classical and random quantization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12924v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Quattrocchi</dc:creator>
    </item>
    <item>
      <title>Level-set shape optimization via polytopic discontinuous Galerkin methods</title>
      <link>https://arxiv.org/abs/2408.13206</link>
      <description>arXiv:2408.13206v1 Announce Type: cross 
Abstract: We introduce a new level-set shape optimization approach based on polytopic (i.e., polygonal in two and polyhedral in three spatial dimensions) discontinuous Galerkin methods. The approach benefits from the geometric mesh flexibility of polytopic discontinuous Galerkin methods to resolve the zero-level set accurately and efficiently. Additionally, we employ suitable Runge-Kutta discontinuous Galerkin methods to update the level-set function on a fine underlying simplicial mesh. We discuss the construction and implementation of the approach, explaining how to modify shape derivate formulas to compute consistent shape gradient approximations using discontinuous Galerkin methods, and how to recover dG functions into smoother ones. Numerical experiments on unconstrained and PDE-constrained test cases evidence the good properties of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13206v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael E. Fernandes, Emmanuil H. Georgoulis, Alberto Paganini</dc:creator>
    </item>
    <item>
      <title>Time-Dependent Blackwell Approachability and Application to Absorbing Games</title>
      <link>https://arxiv.org/abs/2303.04956</link>
      <description>arXiv:2303.04956v2 Announce Type: replace 
Abstract: Blackwell's approachability (Blackwell, 1954, 1956) is a very general online learning framework where a Decision Maker obtains vector-valued outcomes, and aims at the convergence of the average outcome to a given ``target'' set. Blackwell gave a sufficient condition for the decision maker having a strategy guaranteeing such a convergence against an adversarial environment, as well as what we now call the Blackwell's algorithm, which then ensures convergence. Blackwell's approachability has since been applied to numerous problems, in regret minimization and game theory, in particular. We extend this framework by allowing the outcome function and the inner product to be time-dependent. We establish a general guarantee for the natural extension to this framework of Blackwell's algorithm. In the case where the target set is an orthant, we present a family of time-dependent inner products which yields different convergence speeds for each coordinate of the average outcome. We apply this framework to absorbing games (an important class of stochastic games) for which we construct $\varepsilon$-uniformly optimal strategies using Blackwell's algorithm in a well-chosen auxiliary approachability problem, thereby giving a novel illustration of the relevance of online learning tools for solving games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04956v2</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joon Kwon, Yijun Wan, Bruno Ziliotto</dc:creator>
    </item>
    <item>
      <title>Everything is possible: constructing spectrahedra with prescribed facial dimensions</title>
      <link>https://arxiv.org/abs/2312.04419</link>
      <description>arXiv:2312.04419v3 Announce Type: replace 
Abstract: Given any finite set of nonnegative integers, there exists a closed convex set whose facial dimension signature coincides with this set of integers, that is, the dimensions of its nonempty faces comprise exactly this set of integers. In this work, we show that such sets can be realised as solution sets of systems of finitely many convex quadratic inequalities, and hence are representable via second-order cone programming problems, and are, in particular, spectrahedral. It also follows that these sets are facially exposed, in contrast to earlier constructions. We obtain a lower bound on the minimum number of convex quadratic inequalities needed to represent a closed convex set with prescribed facial dimension signature, and show that our bound is tight for some special cases. Finally, we relate the question of finding efficient representations with indecomposability of integer sequences and other topics, and discuss a substantial number of open questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04419v3</guid>
      <category>math.OC</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vera Roshchina, Levent Tun\c{c}el</dc:creator>
    </item>
    <item>
      <title>Cuts and semidefinite liftings for the complex cut polytope</title>
      <link>https://arxiv.org/abs/2402.04731</link>
      <description>arXiv:2402.04731v3 Announce Type: replace 
Abstract: We consider the complex cut polytope: the convex hull of Hermitian rank 1 matrices $xx^{\mathrm{H}}$, where the elements of $x \in \mathbb{C}^n$ are $m$th unit roots. These polytopes have applications in ${\text{MAX-3-CUT}}$, digital communication technology, angular synchronization and more generally, complex quadratic programming. For ${m=2}$, the complex cut polytope corresponds to the well-known cut polytope. We generalize valid cuts for this polytope to cuts for any complex cut polytope with finite $m&gt;2$ and provide a framework to compare them. Further, we consider a second semidefinite lifting of the complex cut polytope for $m=\infty$. This lifting is proven to be equivalent to other complex Lasserre-type liftings of the same order proposed in the literature, while being of smaller size. Our theoretical findings are supported by numerical experiments on various optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04731v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lennart Sinjorgo, Renata Sotirov, Miguel F. Anjos</dc:creator>
    </item>
    <item>
      <title>On a variant of Schu's lemma</title>
      <link>https://arxiv.org/abs/2406.16378</link>
      <description>arXiv:2406.16378v2 Announce Type: replace 
Abstract: In this note, we demonstrate that an incorrect statement has been propagated in multiple papers, stemming from the substitution of ``lim'' with ``limsup'' for a sequence in Lemma 1.3 of the paper [J. Schu: Weak and strong convergence to fixed points of asymptotically nonexpansive mappings, Bull.\ Austral.\ Math.\ Soc.\ 43 (1991), 153--159]. This occurred over a span of more than 20 years, with the earliest paper we identified using this incorrect statement dating back to 2002.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16378v2</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. I. Bot, C. Zalinescu</dc:creator>
    </item>
    <item>
      <title>A Decentralized Shotgun Approach for Team Deception</title>
      <link>https://arxiv.org/abs/2406.17160</link>
      <description>arXiv:2406.17160v2 Announce Type: replace 
Abstract: Deception is helpful for agents masking their intentions from an observer. We consider a team of agents deceiving their supervisor. The supervisor defines nominal behavior for the agents via reference policies, but the agents share an alternate task that they can only achieve by deviating from these references. As such, the agents use deceptive policies to complete the task while ensuring that their behaviors remain plausible to the supervisor. We propose a setting with centralized deceptive policy synthesis and decentralized execution. We model each agent with a Markov decision process and constrain the agents' deceptive policies so that, with high probability, at least one agent achieves the task. We then provide an algorithm to synthesize deceptive policies that ensure the deviations of all agents are small by minimizing the worst Kullback-Leibler divergence between any agent's deceptive and reference policies. Thanks to decentralization, this algorithm scales linearly with the number of agents and also facilitates the efficient synthesis of reference policies. We then explore a more general version of the deceptive policy synthesis problem. In particular, we consider a supervisor who selects a subset of agents to eliminate based on the agents' behaviors. We give algorithms to synthesize deceptive policies so that, after the supervisor eliminates some agents, the remaining agents complete the task with high probability. We demonstrate the developed methods in a package delivery example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17160v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caleb Probine, Mustafa O. Karabag, Ufuk Topcu</dc:creator>
    </item>
    <item>
      <title>Optimal Gaussian Strategies for Vector-valued Witsenhausen Counterexample with Non-causal State Estimator</title>
      <link>https://arxiv.org/abs/2408.02807</link>
      <description>arXiv:2408.02807v2 Announce Type: replace 
Abstract: In this study, we investigate a vector-valued Witsenhausen model where the second decision maker (DM) acquires a vector of observations before selecting a vector of estimations. Here, the first DM acts causally whereas the second DM estimates non-causally. When the vector length grows, we characterize, via a single-letter expression, the optimal trade-off between the power cost at the first DM and the estimation cost at the second DM. In this paper, we show that the best linear scheme is achieved by using the time-sharing method between two affine strategies, which coincides with the convex envelope of the solution of Witsenhausen in 1968. Here also, Witsenhausen's two-point strategy and the scheme of Grover and Sahai in 2010 where both devices operate non-causally, outperform our best linear scheme. Therefore, gains obtained with block-coding schemes are only attainable if all DMs operate non-causally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02807v2</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengyuan Zhao, Tobias J. Oechtering, Ma\"el Le Treust</dc:creator>
    </item>
    <item>
      <title>Robust Implicit Regularization via Weight Normalization</title>
      <link>https://arxiv.org/abs/2305.05448</link>
      <description>arXiv:2305.05448v4 Announce Type: replace-cross 
Abstract: Overparameterized models may have many interpolating solutions; implicit regularization refers to the hidden preference of a particular optimization method towards a certain interpolating solution among the many. A by now established line of work has shown that (stochastic) gradient descent tends to have an implicit bias towards low rank and/or sparse solutions when used to train deep linear networks, explaining to some extent why overparameterized neural network models trained by gradient descent tend to have good generalization performance in practice. However, existing theory for square-loss objectives often requires very small initialization of the trainable weights, which is at odds with the larger scale at which weights are initialized in practice for faster convergence and better generalization performance. In this paper, we aim to close this gap by incorporating and analyzing gradient flow (continuous-time version of gradient descent) with weight normalization, where the weight vector is reparameterized in terms of polar coordinates, and gradient flow is applied to the polar coordinates. By analyzing key invariants of the gradient flow and using Lojasiewicz Theorem, we show that weight normalization also has an implicit bias towards sparse solutions in the diagonal linear model, but that in contrast to plain gradient flow, weight normalization enables a robust bias that persists even if the weights are initialized at practically large scale. Experiments suggest that the gains in both convergence speed and robustness of the implicit bias are improved dramatically by using weight normalization in overparameterized diagonal linear network models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05448v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hung-Hsu Chou, Holger Rauhut, Rachel Ward</dc:creator>
    </item>
    <item>
      <title>A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems</title>
      <link>https://arxiv.org/abs/2308.16904</link>
      <description>arXiv:2308.16904v2 Announce Type: replace-cross 
Abstract: Large-scale linear systems, $Ax=b$, frequently arise in practice and demand effective iterative solvers. Often, these systems are noisy due to operational errors or faulty data-collection processes. In the past decade, the randomized Kaczmarz (RK) algorithm has been studied extensively as an efficient iterative solver for such systems. However, the convergence study of RK in the noisy regime is limited and considers measurement noise in the right-hand side vector, $b$. Unfortunately, in practice, that is not always the case; the coefficient matrix $A$ can also be noisy. In this paper, we analyze the convergence of RK for {\textit{doubly-noisy} linear systems, i.e., when the coefficient matrix, $A$, has additive or multiplicative noise, and $b$ is also noisy}. In our analyses, the quantity $\tilde R=\| \tilde A^{\dagger} \|^2 \|\tilde A \|_F^2$ influences the convergence of RK, where $\tilde A$ represents a noisy version of $A$. We claim that our analysis is robust and realistically applicable, as we do not require information about the noiseless coefficient matrix, $A$, and considering different conditions on noise, we can control the convergence of RK. {We perform numerical experiments to substantiate our theoretical findings.}</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.16904v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>El Houcine Bergou, Soumia Boucherouite, Aritra Dutta, Xin Li, Anna Ma</dc:creator>
    </item>
    <item>
      <title>Quantitative 2D propagation of smallness and control for 1D heat equations with power growth potentials</title>
      <link>https://arxiv.org/abs/2403.07643</link>
      <description>arXiv:2403.07643v2 Announce Type: replace-cross 
Abstract: We study the relation between propagation of smallness in the plane and control for heat equations. The former has been proved by Zhu who showed how the value of solutions in some small set propagates to a larger domain. By reviewing his proof, we establish a quantitative version with the explicit dependence of parameters. Using this explicit version, we establish new exact null-controllability results of 1D heat equations with any nonnegative power growth potentials $V\in\mathcal{C}(\mathbb{R})$. As a key ingredient, new spectral inequalities are established. The control set $\Omega$ that we consider satisfy \begin{equation*}
  \left|\Omega\cap [x-L\langle x\rangle ^{-s},x+L\langle x\rangle ^{-s}]\right|\ge \gamma^{\langle x\rangle^\tau}2L\langle x\rangle^{-s} \end{equation*} for some $\gamma\in(0,1)$, $L&gt;0$, $\tau,s\ge 0$, and $\langle x\rangle:=(1+|x|^2)^{1 /2} $. In particular, the null-controllability result for the case of thick sets that allow the decay of the density (\textit{i.e.}, $s=0$ and $\tau\ge 0$) is included. These extend the Zhu-Zhuge's results from $\Omega$ being the union of equidistributive open sets to thick sets in the 1-dimensional case, and Su-Sun-Yuan's result from bounded potentials to certain unbounded ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07643v2</guid>
      <category>math.AP</category>
      <category>math.CA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunlei Wang</dc:creator>
    </item>
  </channel>
</rss>
