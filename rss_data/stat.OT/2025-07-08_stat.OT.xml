<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.OT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.OT</link>
    <description>stat.OT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.OT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Jul 2025 01:41:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>When Numbers Mislead Us</title>
      <link>https://arxiv.org/abs/2507.03628</link>
      <description>arXiv:2507.03628v1 Announce Type: new 
Abstract: The belief that numbers offer a single, objective description of reality overlooks a crucial truth: data does not speak for itself. Every dataset results from choices-what to measure, how, when, and with whom-which inevitably reflect implicit, and sometimes ideological, assumptions about what is worth quantifying. Moreover, in any analysis, what remains unmeasured can be just as significant as what is captured. When a key variable is omitted-whether by neglect, design, or ignorance-it can distort the observed relationships between other variables. This phenomenon, known as omitted variable bias, may produce misleading correlations or conceal genuine effects. In some cases, accounting for this hidden factor can completely overturn the conclusions drawn from a superficial analysis. This is precisely the mechanism behind Simpson's paradox.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03628v1</guid>
      <category>stat.OT</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Charpentier</dc:creator>
    </item>
    <item>
      <title>Grammatical structures in mathematics: a personal view</title>
      <link>https://arxiv.org/abs/2410.07569</link>
      <description>arXiv:2410.07569v3 Announce Type: replace 
Abstract: The ability to read, write, and speak mathematics is critical to students becoming comfortable with statistical models and skills. Faster development of those skills may act as encouragement to further engage with the discipline. Vocabulary has been the focus of scholarship in existing literature on the linguistics of mathematics and statistics but there are structures such as grammar that go beyond the content of words and symbols. Here I introduce ideas for grammar structures through a sequence of examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07569v3</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tess O'Brien</dc:creator>
    </item>
    <item>
      <title>Treatment, evidence, imitation, and chat</title>
      <link>https://arxiv.org/abs/2506.23040</link>
      <description>arXiv:2506.23040v2 Announce Type: replace 
Abstract: Large language models are thought to have potential to aid in medical decision making. We investigate this here. We start with the treatment problem, the patient's core medical decision-making task, which is solved in collaboration with a healthcare provider. We discuss approaches to solving the treatment problem, including -- within evidence-based medicine -- trials and observational data. We then discuss the chat problem, and how this differs from the treatment problem -- in particular as it relates to imitation. We then discuss how a large language model might be used to solve the treatment problem and highlight some of the challenges that emerge. We finally discuss how these challenges relate to evidence-based medicine, and how this might inform next steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23040v2</guid>
      <category>stat.OT</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel J. Weisenthal</dc:creator>
    </item>
    <item>
      <title>Measures of non-simplifyingness for conditional copulas and vines</title>
      <link>https://arxiv.org/abs/2504.07704</link>
      <description>arXiv:2504.07704v2 Announce Type: replace-cross 
Abstract: In copula modeling, the simplifying assumption has recently been the object of much interest. Although it is very useful to reduce the computational burden, it remains far from obvious whether it is actually satisfied in practice. We propose a theoretical framework which aims at giving a precise meaning to the following question: how non-simplified or close to be simplified is a given conditional copula? For this, we propose a new framework centered at the notion of measure of non-constantness. Then we discuss generalizations of the simplifying assumption to the case where the conditional marginal distributions may not be continuous, and corresponding measures of non-simplifyingness in this case. The simplifying assumption is of particular importance for vine copula models, and we therefore propose a notion of measure of non-simplifyingness of a given copula for a particular vine structure, as well as different scores measuring how non-simplified such a vine decompositions would be for a general vine. Finally, we propose estimators for these measures of non-simplifyingness given an observed dataset. A small simulation study shows the performance of a few estimators of these measures of non-simplifyingness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07704v2</guid>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Derumigny</dc:creator>
    </item>
    <item>
      <title>On the retraining frequency of global forecasting models</title>
      <link>https://arxiv.org/abs/2505.00356</link>
      <description>arXiv:2505.00356v2 Announce Type: replace-cross 
Abstract: In an era of increasing computational capabilities and growing environmental consciousness, organizations face a critical challenge in balancing the accuracy of forecasting models with computational efficiency and sustainability. Global forecasting models, lowering the computational time, have gained significant attention over the years. However, the common practice of retraining these models with new observations raises important questions about the costs of forecasting. Using ten different machine learning and deep learning models, we analyzed various retraining scenarios, ranging from continuous updates to no retraining at all, across two large retail datasets. We showed that less frequent retraining strategies maintain the forecast accuracy while reducing the computational costs, providing a more sustainable approach to large-scale forecasting. We also found that machine learning models are a marginally better choice to reduce the costs of forecasting when coupled with less frequent model retraining strategies as the frequency of the data increases. Our findings challenge the conventional belief that frequent retraining is essential for maintaining forecasting accuracy. Instead, periodic retraining offers a good balance between predictive performance and efficiency, both in the case of point and probabilistic forecasting. These insights provide actionable guidelines for organizations seeking to optimize forecasting pipelines while reducing costs and energy consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00356v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Zanotti</dc:creator>
    </item>
  </channel>
</rss>
