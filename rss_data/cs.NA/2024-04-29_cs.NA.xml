<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Apr 2024 04:01:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Three-Field Multiscale Method</title>
      <link>https://arxiv.org/abs/2404.16978</link>
      <description>arXiv:2404.16978v1 Announce Type: new 
Abstract: "A Three-Field Domain Decomposition Method" is the title of a seminal paper by F. Brezzi and L. D. Marini which introduces a three-field formulation for elliptic partial differential equations. Based on that, we propose the Multiscale-Hybrid-Hybrid Method (MH$^2$M) for the Darcy model, a multiscale finite element method that yields, after a series of formal manipulations, a symmetric positive definite formulation that depends only on the trace of the solution. We show stability and convergence results for a family of finite element spaces and establish relationships with other multiscale finite element methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16978v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franklin de Barros, Alexandre L. Madureira, Fr\'ed\'eric Valentin</dc:creator>
    </item>
    <item>
      <title>An explicit construction of optimized interpolation points on the 4-simplex</title>
      <link>https://arxiv.org/abs/2404.17102</link>
      <description>arXiv:2404.17102v1 Announce Type: new 
Abstract: In this work, a family of symmetric interpolation points are generated on the four-dimensional simplex (i.e. the pentatope). These points are optimized in order to minimize the Lebesgue constant. The process of generating these points closely follows that outlined by Warburton in "An explicit construction of interpolation nodes on the simplex," Journal of Engineering Mathematics, 2006. Here, Warburton generated optimal interpolation points on the triangle and tetrahedron by formulating explicit geometric warping and blending functions, and applying these functions to equidistant nodal distributions. The locations of the resulting points were Lebesgue-optimized. In our work, we extend this procedure to four dimensions, and construct interpolation points on the pentatope up to order ten. The Lebesgue constants of our nodal sets are calculated, and are shown to outperform those of equidistant nodal distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17102v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trenton J. Gobel, David M. Williams</dc:creator>
    </item>
    <item>
      <title>Intractability results for integration in tensor product spaces</title>
      <link>https://arxiv.org/abs/2404.17163</link>
      <description>arXiv:2404.17163v1 Announce Type: new 
Abstract: We study lower bounds on the worst-case error of numerical integration in tensor product spaces. As reference we use the $N$-th minimal error of linear rules that use $N$ function values. The information complexity is the minimal number $N$ of function evaluations that is necessary such that the $N$-th minimal error is less than a factor $\varepsilon$ times the initial error. We are interested to which extent the information complexity depends on the number $d$ of variables of the integrands. If the information complexity grows exponentially fast in $d$, then the integration problem is said to suffer from the curse of dimensionality.
  Under the assumption of the existence of a worst-case function for the uni-variate problem we present two methods for providing good lower bounds on the information complexity. The first method is based on a suitable decomposition of the worst-case function. This method can be seen as a generalization of the method of decomposable reproducing kernels, that is often successfully applied when integration in Hilbert spaces with a reproducing kernel is studied. The second method, although only applicable for positive quadrature rules, has the advantage, that it does not require a suitable decomposition of the worst-case function. Rather, it is based on a spline approximation of the worst-case function and can be used for analytic functions.
  The methods presented can be applied to problems beyond the Hilbert space setting. For demonstration purposes we apply them to several examples, notably to uniform integration over the unit-cube, weighted integration over the whole space, and integration of infinitely smooth functions over the cube. Some of these results have interesting consequences in discrepancy theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17163v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erich Novak, Friedrich Pillichshammer</dc:creator>
    </item>
    <item>
      <title>On the invertibility of matrices with a double saddle-point structure</title>
      <link>https://arxiv.org/abs/2404.17168</link>
      <description>arXiv:2404.17168v1 Announce Type: new 
Abstract: We establish necessary and sufficient conditions for invertiblility of symmetric three-by-three block matrices having a double saddle-point structure that guarantee the unique solvability of double saddle-point systems. We consider various scenarios, including the case where all diagonal blocks are allowed to be rank deficient. Under certain conditions related to the ranks of the blocks and intersections of their kernels, an explicit formula for the inverse is derived.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17168v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatemeh P. A. Beik, Chen Greif, Manfred Trummer</dc:creator>
    </item>
    <item>
      <title>Efficient Orthogonal Decomposition with Automatic Basis Extraction for Low-Rank Matrix Approximation</title>
      <link>https://arxiv.org/abs/2404.17290</link>
      <description>arXiv:2404.17290v1 Announce Type: new 
Abstract: Low-rank matrix approximation play a ubiquitous role in various applications such as image processing, signal processing, and data analysis. Recently, random algorithms of low-rank matrix approximation have gained widespread adoption due to their speed, accuracy, and robustness, particularly in their improved implementation on modern computer architectures. Existing low-rank approximation algorithms often require prior knowledge of the rank of the matrix, which is typically unknown. To address this bottleneck, we propose a low-rank approximation algorithm termed efficient orthogonal decomposition with automatic basis extraction (EOD-ABE) tailored for the scenario where the rank of the matrix is unknown. Notably, we introduce a randomized algorithm to automatically extract the basis that reveals the rank. The efficacy of the proposed algorithms is theoretically and numerically validated, demonstrating superior speed, accuracy, and robustness compared to existing methods. Furthermore, we apply the algorithms to image reconstruction, achieving remarkable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17290v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijie Shen, Weiwei Xu, Lei Zhu</dc:creator>
    </item>
    <item>
      <title>CEM-GMsFEM for Poisson equations in heterogeneous perforated domains</title>
      <link>https://arxiv.org/abs/2404.17372</link>
      <description>arXiv:2404.17372v1 Announce Type: new 
Abstract: In this paper, we propose a novel multiscale model reduction strategy tailored to address the Poisson equation within heterogeneous perforated domains. The numerical simulation of this intricate problem is impeded by its multiscale characteristics, necessitating an exceptionally fine mesh to adequately capture all relevant details. To overcome the challenges inherent in the multiscale nature of the perforations, we introduce a coarse space constructed using the Constraint Energy Minimizing Generalized Multiscale Finite Element Method (CEM-GMsFEM). This involves constructing basis functions through a sequence of local energy minimization problems over eigenspaces containing localized information pertaining to the heterogeneities. Through our analysis, we demonstrate that the oversampling layers depend on the local eigenvalues, thereby implicating the local geometry as well. Additionally, we provide numerical examples to illustrate the efficacy of the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17372v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Xie, Yin Yang, Eric Chung, Yunqing Huang</dc:creator>
    </item>
    <item>
      <title>Multicontinuum homogenization in perforated domains</title>
      <link>https://arxiv.org/abs/2404.17471</link>
      <description>arXiv:2404.17471v1 Announce Type: new 
Abstract: In this paper, we develop a general framework for multicontinuum homogenization in perforated domains. The simulations of problems in perforated domains are expensive and, in many applications, coarse-grid macroscopic models are developed. Many previous approaches include homogenization, multiscale finite element methods, and so on. In our paper, we design multicontinuum homogenization based on our recently proposed framework. In this setting, we distinguish different spatial regions in perforations based on their sizes. For example, very thin perforations are considered as one continua, while larger perforations are considered as another continua. By differentiating perforations in this way, we are able to predict flows in each of them more accurately. We present a framework by formulating cell problems for each continuum using appropriate constraints for the solution averages and their gradients. These cell problem solutions are used in a multiscale expansion and in deriving novel macroscopic systems for multicontinuum homogenization. Our proposed approaches are designed for problems without scale separation. We present numerical results for two continuum problems and demonstrate the accuracy of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17471v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Xie, Yalchin Efendiev, Yunqing Huang, Wing Tat Leung, Yin Yang</dc:creator>
    </item>
    <item>
      <title>Consistent Second Moment Methods with Scalable Linear Solvers for Radiation Transport</title>
      <link>https://arxiv.org/abs/2404.17473</link>
      <description>arXiv:2404.17473v1 Announce Type: new 
Abstract: Second Moment Methods (SMMs) are developed that are consistent with the Discontinuous Galerkin (DG) spatial discretization of the discrete ordinates (or \Sn) transport equations. The low-order (LO) diffusion system of equations is discretized with fully consistent \Pone, Local Discontinuous Galerkin (LDG), and Interior Penalty (IP) methods. A discrete residual approach is used to derive SMM correction terms that make each of the LO systems consistent with the high-order (HO) discretization. We show that the consistent methods are more accurate and have better solution quality than independently discretized LO systems, that they preserve the diffusion limit, and that the LDG and IP consistent SMMs can be scalably solved in parallel on a challenging, multi-material benchmark problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17473v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Olivier, Ben S. Southworth, James S. Warsa, HyeongKae Park</dc:creator>
    </item>
    <item>
      <title>Space-Variant Total Variation boosted by learning techniques in few-view tomographic imaging</title>
      <link>https://arxiv.org/abs/2404.16900</link>
      <description>arXiv:2404.16900v1 Announce Type: cross 
Abstract: This paper focuses on the development of a space-variant regularization model for solving an under-determined linear inverse problem. The case study is a medical image reconstruction from few-view tomographic noisy data. The primary objective of the proposed optimization model is to achieve a good balance between denoising and the preservation of fine details and edges, overcoming the performance of the popular and largely used Total Variation (TV) regularization through the application of appropriate pixel-dependent weights. The proposed strategy leverages the role of gradient approximations for the computation of the space-variant TV weights. For this reason, a convolutional neural network is designed, to approximate both the ground truth image and its gradient using an elastic loss function in its training. Additionally, the paper provides a theoretical analysis of the proposed model, showing the uniqueness of its solution, and illustrates a Chambolle-Pock algorithm tailored to address the specific problem at hand. This comprehensive framework integrates innovative regularization techniques with advanced neural network capabilities, demonstrating promising results in achieving high-quality reconstructions from low-sampled tomographic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16900v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Morotti, Davide Evangelista, Andrea Sebastiani, Elena Loli Piccolomini</dc:creator>
    </item>
    <item>
      <title>Efficient Variational Quantum Linear Solver for Structured Sparse Matrices</title>
      <link>https://arxiv.org/abs/2404.16991</link>
      <description>arXiv:2404.16991v1 Announce Type: cross 
Abstract: We develop a novel approach for efficiently applying variational quantum linear solver (VQLS) in context of structured sparse matrices. Such matrices frequently arise during numerical solution of partial differential equations which are ubiquitous in science and engineering. Conventionally, Pauli basis is used for linear combination of unitary (LCU) decomposition of the underlying matrix to facilitate the evaluation the global/local VQLS cost functions. However, Pauli basis in worst case can result in number of LCU terms that scale quadratically with respect to the matrix size. We show that by using an alternate basis one can better exploit the sparsity and underlying structure of matrix leading to number of tensor product terms which scale only logarithmically with respect to the matrix size. Given this new basis is comprised of non-unitary operators, we employ the concept of unitary completion to design efficient quantum circuits for computing the global/local VQLS cost functions. We compare our approach with other related concepts in the literature including unitary dilation and measurement in Bell basis, and discuss its pros/cons while using VQLS applied to Heat equation as an example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16991v1</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abeynaya Gnanasekaran, Amit Surana</dc:creator>
    </item>
    <item>
      <title>Differentiating Through Linear Solvers</title>
      <link>https://arxiv.org/abs/2404.17039</link>
      <description>arXiv:2404.17039v1 Announce Type: cross 
Abstract: Computer programs containing calls to linear solvers are a known challenge for automatic differentiation. Previous publications advise against differentiating through the low-level solver implementation, and instead advocate for high-level approaches that express the derivative in terms of a modified linear system that can be solved with a separate solver call. Despite this ubiquitous advice, we are not aware of prior work comparing the accuracy of both approaches. With this article we thus empirically study a simple question: What happens if we ignore common wisdom, and differentiate through linear solvers?</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17039v1</guid>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Paul Hovland, Jan H\"uckelheim</dc:creator>
    </item>
    <item>
      <title>Portable, Massively Parallel Implementation of a Material Point Method for Compressible Flows</title>
      <link>https://arxiv.org/abs/2404.17057</link>
      <description>arXiv:2404.17057v1 Announce Type: cross 
Abstract: The recent evolution of software and hardware technologies is leading to a renewed computational interest in Particle-In-Cell (PIC) methods such as the Material Point Method (MPM). Indeed, provided some critical aspects are properly handled, PIC methods can be cast in formulations suitable to the requirements of data locality and fine-grained parallelism of modern hardware accelerators as Graphics Processing Units (GPUs). Such a rapid and continuous technological development increases also the importance of generic and portable implementations. While continuum mechanics simulations have already shown the capabilities of MPM on a wide range of phenomena, the use of the method in compressible fluid dynamics is less frequent, especially in the supersonic regime. In this paper we present a portable, highly parallel, GPU based MPM solver for compressible gas dynamics. The implementation aims to reach a good compromise between portability and efficiency and to give a first assessment of the potential of this approach in reproducing high speed gas flows, also taking into account solid obstacles. The proposed model constitutes a new step towards the realization of a monolithic MPM solver for Fluid-Structure Interaction (FSI) problems at all Mach numbers up to the supersonic regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17057v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Joseph Baioni, Tommaso Benacchio, Luigi Capone, Carlo de Falco</dc:creator>
    </item>
    <item>
      <title>Fast Evaluation of Additive Kernels: Feature Arrangement, Fourier Methods, and Kernel Derivatives</title>
      <link>https://arxiv.org/abs/2404.17344</link>
      <description>arXiv:2404.17344v1 Announce Type: cross 
Abstract: One of the main computational bottlenecks when working with kernel based learning is dealing with the large and typically dense kernel matrix. Techniques dealing with fast approximations of the matrix vector product for these kernel matrices typically deteriorate in their performance if the feature vectors reside in higher-dimensional feature spaces. We here present a technique based on the non-equispaced fast Fourier transform (NFFT) with rigorous error analysis. We show that this approach is also well suited to allow the approximation of the matrix that arises when the kernel is differentiated with respect to the kernel hyperparameters; a problem often found in the training phase of methods such as Gaussian processes. We also provide an error analysis for this case. We illustrate the performance of the additive kernel scheme with fast matrix vector products on a number of data sets. Our code is available at https://github.com/wagnertheresa/NFFTAddKer</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17344v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Theresa Wagner, Franziska Nestler, Martin Stoll</dc:creator>
    </item>
    <item>
      <title>Relations between Kondratiev spaces and refined localization Triebel-Lizorkin spaces</title>
      <link>https://arxiv.org/abs/2404.17359</link>
      <description>arXiv:2404.17359v1 Announce Type: cross 
Abstract: We investigate the close relation between certain weighted Sobolev spaces (Kondratiev spaces) and refined localization spaces from introduced by Triebel [39,40]. In particular, using a characterization for refined localization spaces from Scharf [32], we considerably improve an embedding from Hansen [17]. This embedding is of special interest in connection with convergence rates for adaptive approximation schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17359v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Hansen, Benjamin Scharf, Cornelia Schneider</dc:creator>
    </item>
    <item>
      <title>A mesh-constrained discrete point method for incompressible flows with moving boundaries</title>
      <link>https://arxiv.org/abs/2404.17542</link>
      <description>arXiv:2404.17542v1 Announce Type: cross 
Abstract: Particle-based methods are a practical tool in computational fluid dynamics, and novel types of methods have been proposed. However, widely developed Lagrangian-type formulations suffer from the nonuniform distribution of particles, which is enhanced over time and result in problems in computational efficiency and parallel computations. To mitigate these problems, a mesh-constrained discrete point (MCD) method was developed for stationary boundary problems (Matsuda et al., 2022). Although the MCD method is a meshless method that uses moving least-squares approximation, the arrangement of particles (or discrete points (DPs)) is specialized so that their positions are constrained in background meshes to obtain a closely uniform distribution. This achieves a reasonable approximation for spatial derivatives with compact stencils without encountering any ill-posed condition and leads to good performance in terms of computational efficiency. In this study, a novel meshless method based on the MCD method for incompressible flows with moving boundaries is proposed. To ensure the mesh constraint of each DP in moving boundary problems, a novel updating algorithm for the DP arrangement is developed so that the position of DPs is not only rearranged but the DPs are also reassigned the role of being on the boundary or not. The proposed method achieved reasonable results in numerical experiments for well-known moving boundary problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17542v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takeharu Matsuda, Satoshi Ii</dc:creator>
    </item>
    <item>
      <title>Hybrid Localized Spectral Decomposition for multiscale problems</title>
      <link>https://arxiv.org/abs/1706.08941</link>
      <description>arXiv:1706.08941v3 Announce Type: replace 
Abstract: We consider a finite element method for elliptic equation with heterogeneous and possibly high-contrast coefficients based on primal hybrid formulation. A space decomposition as in FETI and BDCC allows a sequential computations of the unknowns through elliptic problems and satisfies equilibrium constraints. One of the resulting problems is non-local but with exponentially decaying solutions, enabling a practical scheme where the basis functions have an extended, but still local, support. We obtain quasi-optimal a priori error estimates for low-contrast problems assuming minimal regularity of the solutions.
  To also consider the high-contrast case, we propose a variant of our method, enriching the space solution via local eigenvalue problems and obtaining optimal a priori error estimate that mitigates the effect of having coefficients with different magnitudes and again assuming no regularity of the solution. The technique developed is dimensional independent and easy to extend to other problems such as elasticity.</description>
      <guid isPermaLink="false">oai:arXiv.org:1706.08941v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre L. Madureira, Marcus Sarkis</dc:creator>
    </item>
    <item>
      <title>A globally conservative finite element MHD code and its application to the study of compact torus formation, levitation and magnetic compression</title>
      <link>https://arxiv.org/abs/1907.13283</link>
      <description>arXiv:1907.13283v4 Announce Type: replace 
Abstract: The DELiTE (Differential Equations on Linear Triangular Elements) framework was developed for spatial discretisation of partial differential equations on an unstructured triangular grid in axisymmetric geometry. The framework is based on discrete differential operators in matrix form, which are derived using linear finite elements and mimic some of the properties of their continuous counterparts. A single-fluid two-temperature MHD code is implemented in this framework. The inherent properties of the operators are used in the code to ensure global conservation of energy, particle count, toroidal flux, and angular momentum. The code was applied to study a novel experiment in which a compact torus (CT), produced with a magnetized Marshall gun, is magnetically levitated off an insulating wall and then magnetically compressed through the action of currents in the levitation/compression coils located outside the wall. We present numerical models for CT formation, levitation, and magnetic compression, and comparisons between simulated and experimental diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:1907.13283v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.plasm-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carl Dunlea, Ivan Khalzov</dc:creator>
    </item>
    <item>
      <title>Multi-domain FEM-BEM coupling for acoustic scattering</title>
      <link>https://arxiv.org/abs/2305.09278</link>
      <description>arXiv:2305.09278v2 Announce Type: replace 
Abstract: We model time-harmonic acoustic scattering by an object composed of piece-wise homogeneous parts and an arbitrarily heterogeneous part. We propose and analyze new formulations that couple, adopting a Costabel-type approach, boundary integral equations for the homogeneous subdomains with volume variational formulations for the heterogeneous subdomain. This is an extension of the Costabel FEM-BEM coupling to a multi-domain configuration, with  cross-points allowed, i.e. points where three or more subdomains are adjacent. While generally just the exterior unbounded subdomain is treated with the BEM, here we wish to exploit the advantages of BEM whenever it is applicable, that is, for all the homogeneous parts of the scattering object. Our formulation is based on the multi-trace formalism, which initially was introduced for acoustic scattering by piece-wise homogeneous objects. Instead, here we allow the wavenumber to vary arbitrarily in a part of the domain. We prove that the bilinear form associated with the proposed formulation satisfies a G{\aa}rding coercivity inequality, which ensures stability of the variational problem if it is uniquely solvable. We identify conditions for injectivity and construct modified versions immune to spurious resonances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09278v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcella Bonazzoli (IDEFIX), Xavier Claeys (LJLL)</dc:creator>
    </item>
    <item>
      <title>Dynamical Low-Rank Approximation for Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2308.11581</link>
      <description>arXiv:2308.11581v2 Announce Type: replace 
Abstract: In this paper, we set the mathematical foundations of the Dynamical Low-Rank Approximation (DLRA) method for stochastic differential equations. DLRA aims at approximating the solution as a linear combination of a small number of basis vectors with random coefficients (low rank format) with the peculiarity that both the basis vectors and the random coefficients vary in time. While the formulation and properties of DLRA are now well understood for random/parametric equations, the same cannot be said for SDEs and this work aims to fill this gap. We start by rigorously formulating a Dynamically Orthogonal (DO) approximation (an instance of DLRA successfully used in applications) for SDEs, which we then generalize to define a parametrization independent DLRA for SDEs. We show local well-posedness of the DO equations and their equivalence with the DLRA formulation. We also characterize the explosion time of the DO solution by a loss of linear independence of the random coefficients defining the solution expansion and give sufficient conditions for global existence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11581v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshihito Kazashi, Fabio Nobile, Fabio Zoccolan</dc:creator>
    </item>
    <item>
      <title>Property-preserving numerical approximations of a Cahn-Hilliard-Navier-Stokes model with variable densities and degenerate mobility</title>
      <link>https://arxiv.org/abs/2310.01522</link>
      <description>arXiv:2310.01522v2 Announce Type: replace 
Abstract: In this paper, we present a new computational framework using coupled and decoupled approximations for a Cahn-Hilliard-Navier-Stokes model with variable densities and degenerate mobility. In this sense, the coupled approximation is shown to conserve the mass of the fluid, preserve the point-wise bounds of the density and decrease an energy functional. In contrast, the decoupled scheme is presented as a more computationally efficient alternative but the discrete energy-decreasing property can not be assured. Both schemes are based on a finite element approximation for the Navier-Stokes fluid flow with discontinuous pressure and an upwind discontinuous Galerkin scheme for the Cahn-Hilliard part. Finally, several numerical experiments contrasting both approaches are conducted. In particular, results for a convergence test, a simple qualitative comparison and some well-known benchmark problems are shown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01522v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daniel Acosta-Soba, Francisco Guill\'en-Gonz\'alez, J. Rafael Rodr\'iguez-Galv\'an, Jin Wang</dc:creator>
    </item>
    <item>
      <title>Assigning Stationary Distributions to Sparse Stochastic Matrices</title>
      <link>https://arxiv.org/abs/2312.16011</link>
      <description>arXiv:2312.16011v3 Announce Type: replace 
Abstract: The target stationary distribution problem (TSDP) is the following: given an irreducible stochastic matrix $G$ and a target stationary distribution $\hat \mu$, construct a minimum norm perturbation, $\Delta$, such that $\hat G = G+\Delta$ is also stochastic and has the prescribed target stationary distribution, $\hat \mu$. In this paper, we revisit the TSDP under a constraint on the support of $\Delta$, that is, on the set of non-zero entries of $\Delta$. This is particularly meaningful in practice since one cannot typically modify all entries of $G$. We first show how to construct a feasible solution $\hat G$ that has essentially the same support as the matrix $G$. Then we show how to compute globally optimal and sparse solutions using the component-wise $\ell_1$ norm and linear optimization. We propose an efficient implementation that relies on a column-generation approach which allows us to solve sparse problems of size up to $10^5 \times 10^5$ in a few minutes. We illustrate the proposed algorithms with several numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16011v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Gillis, Paul Van Dooren</dc:creator>
    </item>
    <item>
      <title>Enhancing Arterial Blood Flow Simulations through Physics-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2404.16347</link>
      <description>arXiv:2404.16347v2 Announce Type: replace 
Abstract: This study introduces a computational approach leveraging Physics-Informed Neural Networks (PINNs) for the efficient computation of arterial blood flows, particularly focusing on solving the incompressible Navier-Stokes equations by using the domain decomposition technique. Unlike conventional computational fluid dynamics methods, PINNs offer advantages by eliminating the need for discretized meshes and enabling the direct solution of partial differential equations (PDEs). In this paper, we propose the weighted Extended Physics-Informed Neural Networks (WXPINNs) and weighted Conservative Physics-Informed Neural Networks (WCPINNs), tailored for detailed hemodynamic simulations based on generalized space-time domain decomposition techniques. The inclusion of multiple neural networks enhances the representation capacity of the weighted PINN methods. Furthermore, the weighted PINNs can be efficiently trained in parallel computing frameworks by employing separate neural networks for each sub-domain. We show that PINNs simulation results circumvent backflow instabilities, underscoring a notable advantage of employing PINNs over traditional numerical methods to solve such complex blood flow models. They naturally address such challenges within their formulations. The presented numerical results demonstrate that the proposed weighted PINNs outperform traditional PINNs settings, where sub-PINNs are applied to each subdomain separately. This study contributes to the integration of deep learning methodologies with fluid mechanics, paving the way for accurate and efficient high-fidelity simulations in biomedical applications, particularly in modeling arterial blood flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16347v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivam Bhargava, Nagaiah Chamakuri</dc:creator>
    </item>
    <item>
      <title>A Test Problem for Flow Codes</title>
      <link>https://arxiv.org/abs/2404.16798</link>
      <description>arXiv:2404.16798v2 Announce Type: replace 
Abstract: We propose a test problem for Navier-Stokes solvers based on the flow around a cylinder. We choose a range of Reynolds numbers for which the flow is time-dependent but can be characterized as essentially two-dimensional. The test problem requires accurate resolution of chaotic dynamics over a long time interval. It also requires the use of a relatively large computational domain, part of which is curved, and it requires evaluation of derivatives of the solution and pressure on the curved boundary. We review the performance of different finite element methods for the proposed range of Reynolds numbers. These tests indicate that some of the most established methods do not capture the correct behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16798v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry von Wahl, L. Ridgway Scott</dc:creator>
    </item>
    <item>
      <title>Predictive Modelling of Critical Variables for Improving HVOF Coating using Gamma Regression Models</title>
      <link>https://arxiv.org/abs/2311.01194</link>
      <description>arXiv:2311.01194v3 Announce Type: replace-cross 
Abstract: Thermal spray coating is a critical process in many industries, involving the application of coatings to surfaces to enhance their functionality. This paper proposes a framework for modelling and predicting critical target variables in thermal spray coating processes, based on the application of statistical design of experiments (DoE) and the modelling of the data using generalized linear models (GLMs) with a particular emphasis on gamma regression. Experimental data obtained from thermal spray coating trials are used to validate the presented approach, demonstrating that it is able to accurately model and predict critical target variables. As such, the framework has significant potential for the optimization of thermal spray coating processes, and can contribute to the development of more efficient and effective coating technologies in various industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01194v3</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.app-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wolfgang Rannetbauer, Simon Hubmer, Carina Hambrock, Ronny Ramlau</dc:creator>
    </item>
    <item>
      <title>MATLAB Simulator of Level-Index Arithmetic</title>
      <link>https://arxiv.org/abs/2402.02301</link>
      <description>arXiv:2402.02301v2 Announce Type: replace-cross 
Abstract: Level-index arithmetic appeared in the 1980s. One of its principal purposes is to abolish the issues caused by underflows and overflows in floating point. However, level-index arithmetic does not expand the set of numbers but spaces out the numbers of large magnitude even more than floating-point representations to move the infinities further away from zero: gaps between numbers on both ends of the range become very large. We revisit level index by presenting a custom precision simulator in MATLAB. This toolbox is useful for exploring performance of level-index arithmetic in research projects, such as using 8-bit and 16-bit representations in machine learning algorithms where narrow bit-width is desired but overflow/underflow of floating-point representations causes difficulties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02301v2</guid>
      <category>cs.MS</category>
      <category>cs.AR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mantas Mikaitis</dc:creator>
    </item>
    <item>
      <title>Numerical investigation of stabilization in the Hybridizable Discontinuous Galerkin method for linear anisotropic elastic equation</title>
      <link>https://arxiv.org/abs/2403.02862</link>
      <description>arXiv:2403.02862v2 Announce Type: replace-cross 
Abstract: This work is concerned with implementing the hybridizable discontinuous Galerkin (HDG) method to solve the linear anisotropic elastic equation in the frequency domain. First-order formulation with the compliance tensor and Voigt notation are employed to provide a compact description of the discretized problem and flexibility with highly heterogeneous media. We further focus on the question of optimal choices of stabilization in the definition of HDG numerical traces. For this purpose, we construct a hybridized Godunov-upwind flux for anisotropic elastic media possessing three distinct wavespeeds. This stabilization removes the need to choose a scaling factor, contrary to the identity and Kelvin-Christoffel based stabilizations which are popular choices in the literature. We carry out comparisons among these families for isotropic and anisotropic material, with constant background and highly heterogeneous ones, in two and three dimensions. These experiments establish the optimality of the Godunov stabilization which can be used as a reference choice for a generic material in which different types of waves propagate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02862v2</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ha Pham, Florian Faucher, H\'el\`ene Barucq</dc:creator>
    </item>
  </channel>
</rss>
