<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 02:49:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Simple demonstration of different types of coupling in multiphysics numerical problems</title>
      <link>https://arxiv.org/abs/2502.07791</link>
      <description>arXiv:2502.07791v1 Announce Type: new 
Abstract: Numerical modelling of coupled multiphysics phenomena is becoming an increasingly important subject in applied mathematics. The main challenge in teaching this subject is the complexity of both the mathematical models and their numerical implementation. In this note, a simple demonstrator is proposed that enables demonstration of and some hands-on experience with three types of coupling commonly used in computational science: one-way coupling, explicit sequential coupling, and full coupling. It makes use of a familiar nonlinear heat equation in 1D, with the solution being a propagating heat wave.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07791v1</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexandre Lavrov</dc:creator>
    </item>
    <item>
      <title>ChemZIP: Accelerated Modeling of Complex Aerothermochemical Interactions in Novel Turbomachines for Sustainable High-Temperature Chemical Processes</title>
      <link>https://arxiv.org/abs/2502.08232</link>
      <description>arXiv:2502.08232v1 Announce Type: new 
Abstract: This paper introduces a new platform to accelerate the modeling of complex aerothermochemical interactions in new turbomachines, turbo-reactors, to decarbonise chemical processes. While previous work has aerothermally demonstrated the potential to decarbonize the heat input to the reaction, optimizing the reaction efficiency has been a challenge. This is because measuring reaction performance with aerochemical simulations is computationally prohibitive due to the uniquely complex aerodynamics and chemistry within turbomachines. To address this, we introduce a new multifidelity machine-learning-assisted methodology, called ChemZIP, to mitigate this bottleneck. Although data-driven methodologies exist for combustion, modeling reactive flows along the bladed path of a turbomachine poses new challenges. This has led to a novel training data generation process, which allows rich dynamic responses of the chemical system to be embedded into the training dataset at a fraction of the cost of reacting flow simulations. The resulting high-dimensional composition vector is compressed into a low-dimensional basis using an autoencoder-like neural network, inspired by but more universal than traditional flamelet-generated manifolds. Verification against 10,000 unseen one-dimensional test conditions shows an R2 score exceeding 95% across all quantities of interest. Following this, ChemZIP is coupled into a fully-fledged viscous computational fluid dynamics solver. For a set of process-relevant three-dimensional configurations entirely different from the training data, the predictive accuracy of the thermochemical state remains within 10% of an industry-standard solver while convergence is achieved 50 times faster, even for a small mechanism. Therefore, numerical computations are sufficiently fast that aerothermochemical optimization is now feasible for the first time in the design cycle</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08232v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan Rubini, Budimir Rosic</dc:creator>
    </item>
    <item>
      <title>Checkerboard Target Measurement in Unordered Point Clouds with Coloured ICP</title>
      <link>https://arxiv.org/abs/2502.08525</link>
      <description>arXiv:2502.08525v1 Announce Type: new 
Abstract: In this work, we investigate the problem of measuring a the centre checkerboard target in an 3D point cloud. This is an important problem which has applications in registration, long term monitoring and linking to other sensor systems. We use a 3D template matching approach based on the coloured ICP algorithm to solve the problem. We tackle the problem under the additional constraints that we assume no structure in the 3D data in order to be able to handle unordered point clouds. This gives us the capability to process data from the new generation of low-cost LIDAR sensors. This category of sensors also suffers from increased noise in range and reflectivity measurement. We provide extensive simulation results using synthetic data to capture the potential of the approach. We then give the detailed steps for handling real sensor data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08525v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>June Moh Goo, Jialun Li, Darmawan Wicaksono, Jan Boehm</dc:creator>
    </item>
    <item>
      <title>Input convex neural networks: universal approximation theorem and implementation for isotropic polyconvex hyperelastic energies</title>
      <link>https://arxiv.org/abs/2502.08534</link>
      <description>arXiv:2502.08534v1 Announce Type: new 
Abstract: This paper presents a novel framework of neural networks for isotropic hyperelasticity that enforces necessary physical and mathematical constraints while simultaneously satisfying the universal approximation theorem. The two key ingredients are an input convex network architecture and a formulation in the elementary polynomials of the signed singular values of the deformation gradient. In line with previously published networks, it can rigorously capture frame-indifference and polyconvexity - as well as further constraints like balance of angular momentum and growth conditions. However and in contrast to previous networks, a universal approximation theorem for the proposed approach is proven. To be more explicit, the proposed network can approximate any frame-indifferent, isotropic polyconvex energy (provided the network is large enough). This is possible by working with a sufficient and necessary criterion for frame-indifferent, isotropic polyconvex functions. Comparative studies with existing approaches identify the advantages of the proposed method, particularly in approximating non-polyconvex energies as well as computing polyconvex hulls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08534v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gian-Luca Geuken, Patrick Kurzeja, David Wiedemann, J\"orn Mosler</dc:creator>
    </item>
    <item>
      <title>Data-Driven Socio-Economic Deprivation Prediction via Dimensionality Reduction: The Power of Diffusion Maps</title>
      <link>https://arxiv.org/abs/2312.09830</link>
      <description>arXiv:2312.09830v2 Announce Type: cross 
Abstract: This research proposes a model to predict the location of the most deprived areas in a city using data from the census. Census data is very high-dimensional and needs to be simplified. We use the diffusion map algorithm to reduce dimensionality and find patterns. Features are defined by eigenvectors of the Laplacian matrix that defines the diffusion map. The eigenvectors corresponding to the smallest eigenvalues indicate specific characteristics of the population. Previous work has found qualitatively that the second most important dimension for describing the census data in Bristol, UK is linked to deprivation. In this research, we analyse how good this dimension is as a model for predicting deprivation by comparing it with the recognised measures. The Pearson correlation coefficient was found to be greater than 0.7. The top 10 per cent of deprived areas in the UK, which are also located in Bristol, are extracted to test the accuracy of the model. There are 52 of the most deprived areas, and 38 areas are correctly identified by comparing them to the model. The influence of scores of IMD domains that do not correlate with the models and Eigenvector 2 entries of non-deprived Output Areas cause the model to fail the prediction of 14 deprived areas. The model demonstrates strong performance in predicting future deprivation in the project areas, which is expected to assist in government resource allocation and funding greatly. The codes can be accessed here: https://github.com/junegoo94/diffusion_maps</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09830v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>June Moh Goo</dc:creator>
    </item>
    <item>
      <title>DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Prediction</title>
      <link>https://arxiv.org/abs/2403.08055</link>
      <description>arXiv:2403.08055v2 Announce Type: replace-cross 
Abstract: This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of 3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional neural network model, both aimed at aerodynamic car design through machine learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million surface mesh faces and comprehensive aerodynamic performance data comprising of full 3D pressure, velocity fields, and wall-shear stresses, addresses the critical need for extensive datasets to train deep learning models in engineering applications. It is 60\% larger than the previously available largest public dataset of cars, and is the only open-source dataset that also models wheels and underbody. RegDGCNN leverages this large-scale dataset to provide high-precision drag estimates directly from 3D meshes, bypassing traditional limitations such as the need for 2D image rendering or Signed Distance Fields (SDF). By enabling fast drag estimation in seconds, RegDGCNN facilitates rapid aerodynamic assessments, offering a substantial leap towards integrating data-driven methods in automotive design. Together, DrivAerNet and RegDGCNN promise to accelerate the car design process and contribute to the development of more efficient cars. To lay the groundwork for future innovations in the field, the dataset and code used in our study are publicly accessible at https://github.com/Mohamedelrefaie/DrivAerNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08055v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>physics.flu-dyn</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1115/DETC2024-143593</arxiv:DOI>
      <dc:creator>Mohamed Elrefaie, Angela Dai, Faez Ahmed</dc:creator>
    </item>
    <item>
      <title>Demystifying Domain-adaptive Post-training for Financial LLMs</title>
      <link>https://arxiv.org/abs/2501.04961</link>
      <description>arXiv:2501.04961v2 Announce Type: replace-cross 
Abstract: Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain adaptive post-training of LLMs for the finance domain. Our approach consists of four key components: FinCap, which defines the core capabilities required for the target domain; FinRec, an effective training recipe that jointly optimizes continual pre-training and instruction-following, along with a novel preference data distillation method leveraging process signals from a generative reward model; FinTrain, a curated set of training datasets supporting FinRec; and FinEval, a comprehensive evaluation suite aligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04961v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zixuan Ke, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty</dc:creator>
    </item>
  </channel>
</rss>
