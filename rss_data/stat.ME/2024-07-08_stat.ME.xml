<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Jul 2024 02:38:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>missForestPredict -- Missing data imputation for prediction settings</title>
      <link>https://arxiv.org/abs/2407.03379</link>
      <description>arXiv:2407.03379v1 Announce Type: new 
Abstract: Prediction models are used to predict an outcome based on input variables. Missing data in input variables often occurs at model development and at prediction time. The missForestPredict R package proposes an adaptation of the missForest imputation algorithm that is fast, user-friendly and tailored for prediction settings. The algorithm iteratively imputes variables using random forests until a convergence criterion (unified for continuous and categorical variables and based on the out-of-bag error) is met. The imputation models are saved for each variable and iteration and can be applied later to new observations at prediction time. The missForestPredict package offers extended error monitoring, control over variables used in the imputation and custom initialization. This allows users to tailor the imputation to their specific needs. The missForestPredict algorithm is compared to mean/mode imputation, linear regression imputation, mice, k-nearest neighbours, bagging, miceRanger and IterativeImputer on eight simulated datasets with simulated missingness (48 scenarios) and eight large public datasets using different prediction models. missForestPredict provides competitive results in prediction settings within short computation times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03379v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Albu, Shan Gao, Laure Wynants, Ben Van Calster</dc:creator>
    </item>
    <item>
      <title>Continuous Optimization for Offline Change Point Detection and Estimation</title>
      <link>https://arxiv.org/abs/2407.03383</link>
      <description>arXiv:2407.03383v1 Announce Type: new 
Abstract: This work explores use of novel advances in best subset selection for regression modelling via continuous optimization for offline change point detection and estimation in univariate Gaussian data sequences. The approach exploits reformulating the normal mean multiple change point model into a regularized statistical inverse problem enforcing sparsity. After introducing the problem statement, criteria and previous investigations via Lasso-regularization, the recently developed framework of continuous optimization for best subset selection (COMBSS) is briefly introduced and related to the problem at hand. Supervised and unsupervised perspectives are explored with the latter testing different approaches for the choice of regularization penalty parameters via the discrepancy principle and a confidence bound. The main result is an adaptation and evaluation of the COMBSS approach for offline normal mean multiple change-point detection via experimental results on simulated data for different choices of regularisation parameters. Results and future directions are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03383v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans Reimann, Sarat Moka, Georgy Sofronov</dc:creator>
    </item>
    <item>
      <title>A Deterministic Information Bottleneck Method for Clustering Mixed-Type Data</title>
      <link>https://arxiv.org/abs/2407.03389</link>
      <description>arXiv:2407.03389v1 Announce Type: new 
Abstract: In this paper, we present an information-theoretic method for clustering mixed-type data, that is, data consisting of both continuous and categorical variables. The method is a variant of the Deterministic Information Bottleneck algorithm which optimally compresses the data while retaining relevant information about the underlying structure. We compare the performance of the proposed method to that of three well-established clustering methods (KAMILA, K-Prototypes, and Partitioning Around Medoids with Gower's dissimilarity) on simulated and real-world datasets. The results demonstrate that the proposed approach represents a competitive alternative to conventional clustering techniques under specific conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03389v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Efthymios Costa, Ioanna Papatsouma, Angelos Markos</dc:creator>
    </item>
    <item>
      <title>Balancing events, not patients, maximizes power of the logrank test: and other insights on unequal randomization in survival trials</title>
      <link>https://arxiv.org/abs/2407.03420</link>
      <description>arXiv:2407.03420v1 Announce Type: new 
Abstract: We revisit the question of what randomization ratio (RR) maximizes power of the logrank test in event-driven survival trials under proportional hazards (PH). By comparing three approximations of the logrank test (Schoenfeld, Freedman, Rubinstein) to empirical simulations, we find that the RR that maximizes power is the RR that balances number of events across treatment arms at the end of the trial. This contradicts the common misconception implied by Schoenfeld's approximation that 1:1 randomization maximizes power. Besides power, we consider other factors that might influence the choice of RR (accrual, trial duration, sample size, etc.). We perform simulations to better understand how unequal randomization might impact these factors in practice. Altogether, we derive 6 insights to guide statisticians in the design of survival trials considering unequal randomization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03420v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Ray Lin, Yi Liu</dc:creator>
    </item>
    <item>
      <title>Population Size Estimation with Many Lists and Heterogeneity: A Conditional Log-Linear Model Among the Unobserved</title>
      <link>https://arxiv.org/abs/2407.03539</link>
      <description>arXiv:2407.03539v1 Announce Type: new 
Abstract: We contribute a general and flexible framework to estimate the size of a closed population in the presence of $K$ capture-recapture lists and heterogeneous capture probabilities. Our novel identifying strategy leverages the fact that it is sufficient for identification that a subset of the $K$ lists are not arbitrarily dependent \textit{within the subset of the population unobserved by the remaining lists}, conditional on covariates. This identification approach is interpretable and actionable, interpolating between the two predominant approaches in the literature as special cases: (conditional) independence across lists and log-linear models with no highest-order interaction. We derive nonparametric doubly-robust estimators for the resulting identification expression that are nearly optimal and approximately normal for any finite sample size, even when the heterogeneous capture probabilities are estimated nonparametrically using machine learning methods. Additionally, we devise a sensitivity analysis to show how deviations from the identification assumptions affect the resulting population size estimates, allowing for the integration of domain-specific knowledge into the identification and estimation processes more transparently. We empirically demonstrate the advantages of our method using both synthetic data and real data from the Peruvian internal armed conflict to estimate the number of casualties. The proposed methodology addresses recent critiques of capture-recapture models by allowing for a weaker and more interpretable identifying assumption and accommodating complex heterogeneous capture probabilities depending on high-dimensional or continuous covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03539v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mateo Dulce Rubio, Edward Kennedy</dc:creator>
    </item>
    <item>
      <title>Aggregated Sure Independence Screening for Variable Selection with Interaction Structures</title>
      <link>https://arxiv.org/abs/2407.03558</link>
      <description>arXiv:2407.03558v1 Announce Type: new 
Abstract: A new method called the aggregated sure independence screening is proposed for the computational challenges in variable selection of interactions when the number of explanatory variables is much higher than the number of observations (i.e., $p\gg n$). In this problem, the two main challenges are the strong hierarchical restriction and the number of candidates for the main effects and interactions. If $n$ is a few hundred and $p$ is ten thousand, then the memory needed for the augmented matrix of the full model is more than $100{\rm GB}$ in size, beyond the memory capacity of a personal computer. This issue can be solved by our proposed method but not by our competitors. Two advantages are that the proposed method can include important interactions even if the related main effects are weak or absent, and it can be combined with an arbitrary variable selection method for interactions. The research addresses the main concern for variable selection of interactions because it makes previous methods applicable to the case when $p$ is extremely large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03558v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tonglin Zhang</dc:creator>
    </item>
    <item>
      <title>When can weak latent factors be statistically inferred?</title>
      <link>https://arxiv.org/abs/2407.03616</link>
      <description>arXiv:2407.03616v1 Announce Type: new 
Abstract: This article establishes a new and comprehensive estimation and inference theory for principal component analysis (PCA) under the weak factor model that allow for cross-sectional dependent idiosyncratic components under nearly minimal the factor strength relative to the noise level or signal-to-noise ratio. Our theory is applicable regardless of the relative growth rate between the cross-sectional dimension $N$ and temporal dimension $T$. This more realistic assumption and noticeable result requires completely new technical device, as the commonly-used leave-one-out trick is no longer applicable to the case with cross-sectional dependence. Another notable advancement of our theory is on PCA inference $ - $ for example, under the regime where $N\asymp T$, we show that the asymptotic normality for the PCA-based estimator holds as long as the signal-to-noise ratio (SNR) grows faster than a polynomial rate of $\log N$. This finding significantly surpasses prior work that required a polynomial rate of $N$. Our theory is entirely non-asymptotic, offering finite-sample characterizations for both the estimation error and the uncertainty level of statistical inference. A notable technical innovation is our closed-form first-order approximation of PCA-based estimator, which paves the way for various statistical tests. Furthermore, we apply our theories to design easy-to-implement statistics for validating whether given factors fall in the linear spans of unknown latent factors, testing structural breaks in the factor loadings for an individual unit, checking whether two units have the same risk exposures, and constructing confidence intervals for systematic risks. Our empirical studies uncover insightful correlations between our test results and economic cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03616v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Yuling Yan, Yuheng Zheng</dc:creator>
    </item>
    <item>
      <title>Multivariate Representations of Univariate Marked Hawkes Processes</title>
      <link>https://arxiv.org/abs/2407.03619</link>
      <description>arXiv:2407.03619v1 Announce Type: new 
Abstract: Univariate marked Hawkes processes are used to model a range of real-world phenomena including earthquake aftershock sequences, contagious disease spread, content diffusion on social media platforms, and order book dynamics. This paper illustrates a fundamental connection between univariate marked Hawkes processes and multivariate Hawkes processes. Exploiting this connection renders a framework that can be built upon for expressive and flexible inference on diverse data. Specifically, multivariate unmarked Hawkes representations are introduced as a tool to parameterize univariate marked Hawkes processes. We show that such multivariate representations can asymptotically approximate a large class of univariate marked Hawkes processes, are stationary given the approximated process is stationary, and that resultant conditional intensity parameters are identifiable. A simulation study demonstrates the efficacy of this approach, and provides heuristic bounds for error induced by the relatively larger parameter space of multivariate Hawkes processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03619v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis Davis, Conor Kresin, Boris Baeumer, Ting Wang</dc:creator>
    </item>
    <item>
      <title>Robust CATE Estimation Using Novel Ensemble Methods</title>
      <link>https://arxiv.org/abs/2407.03690</link>
      <description>arXiv:2407.03690v2 Announce Type: new 
Abstract: The estimation of Conditional Average Treatment Effects (CATE) is crucial for understanding the heterogeneity of treatment effects in clinical trials. We evaluate the performance of common methods, including causal forests and various meta-learners, across a diverse set of scenarios revealing that each of the methods fails in one or more of the tested scenarios. Given the inherent uncertainty of the data-generating process in real-life scenarios, the robustness of a CATE estimator to various scenarios is critical for its reliability.
  To address this limitation of existing methods, we propose two new ensemble methods that integrate multiple estimators to enhance prediction stability and performance - Stacked X-Learner which uses the X-Learner with model stacking for estimating the nuisance functions, and Consensus Based Averaging (CBA), which averages only the models with highest internal agreement. We show that these models achieve good performance across a wide range of scenarios varying in complexity, sample size and structure of the underlying-mechanism, including a biologically driven model for PD-L1 inhibition pathway for cancer treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03690v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Oshri Machluf, Tzviel Frostig, Gal Shoham, Tomer Milo, Elad Berkman, Raviv Pryluk</dc:creator>
    </item>
    <item>
      <title>Absolute average and median treatment effects as causal estimands on metric spaces</title>
      <link>https://arxiv.org/abs/2407.03726</link>
      <description>arXiv:2407.03726v1 Announce Type: new 
Abstract: We define the notions of absolute average and median treatment effects as causal estimands on general metric spaces such as Riemannian manifolds, propose estimators using stratification, and prove several properties, including strong consistency. In the process, we also demonstrate the strong consistency of the weighted sample Fr\'echet means and geometric medians. Stratification allows these estimators to be utilized beyond the narrow constraints of a completely randomized experiment. After constructing confidence intervals using bootstrapping, we outline how to use the proposed estimates to test Fisher's sharp null hypothesis that the absolute average or median treatment effect is zero. Empirical evidence for the strong consistency of the estimators and the reasonable asymptotic coverage of the confidence intervals is provided through simulations in both randomized experiments and observational study settings. We also apply our methods to real data from an observational study to investigate the causal relationship between Alzheimer's disease and the shape of the corpus callosum, rejecting the aforementioned null hypotheses in cases where conventional Euclidean methods fail to do so. Our proposed methods are more generally applicable than past studies in dealing with general metric spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03726v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ha-Young Shin, Kyusoon Kim, Kwonsang Lee, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>Mixture Modeling for Temporal Point Processes with Memory</title>
      <link>https://arxiv.org/abs/2407.03774</link>
      <description>arXiv:2407.03774v1 Announce Type: new 
Abstract: We propose a constructive approach to building temporal point processes that incorporate dependence on their history. The dependence is modeled through the conditional density of the duration, i.e., the interval between successive event times, using a mixture of first-order conditional densities for each one of a specific number of lagged durations. Such a formulation for the conditional duration density accommodates high-order dynamics, and it thus enables flexible modeling for point processes with memory. The implied conditional intensity function admits a representation as a local mixture of first-order hazard functions. By specifying appropriate families of distributions for the first-order conditional densities, with different shapes for the associated hazard functions, we can obtain either self-exciting or self-regulating point processes. From the perspective of duration processes, we develop a method to specify a stationary marginal density. The resulting model, interpreted as a dependent renewal process, introduces high-order Markov dependence among identically distributed durations. Furthermore, we provide extensions to cluster point processes. These can describe duration clustering behaviors attributed to different factors, thus expanding the scope of the modeling framework to a wider range of applications. Regarding implementation, we develop a Bayesian approach to inference, model checking, and prediction. We investigate point process model properties analytically, and illustrate the methodology with both synthetic and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03774v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaotian Zheng, Athanasios Kottas, Bruno Sans\'o</dc:creator>
    </item>
    <item>
      <title>Three- and four-parameter item response model in factor analysis framework</title>
      <link>https://arxiv.org/abs/2407.04071</link>
      <description>arXiv:2407.04071v1 Announce Type: new 
Abstract: This work proposes a 4-parameter factor analytic (4P FA) model for multi-item measurements composed of binary items as an extension to the dichotomized single latent variable FA model. We provide an analytical derivation of the relationship between the newly proposed 4P FA model and its counterpart in the item response theory (IRT) framework, the 4P IRT model. A Bayesian estimation method for the proposed 4P FA model is provided to estimate the four item parameters, the respondents' latent scores, and the scores cleaned of the guessing and inattention effects. The newly proposed algorithm is implemented in R and Python, and the relationship between the 4P FA and 4P IRT is empirically demonstrated using real datasets from admission tests and the assessment of anxiety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04071v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\'an Pavlech, Patr\'icia Martinkov\'a</dc:creator>
    </item>
    <item>
      <title>Network-based Neighborhood regression</title>
      <link>https://arxiv.org/abs/2407.04104</link>
      <description>arXiv:2407.04104v1 Announce Type: new 
Abstract: Given the ubiquity of modularity in biological systems, module-level regulation analysis is vital for understanding biological systems across various levels and their dynamics. Current statistical analysis on biological modules predominantly focuses on either detecting the functional modules in biological networks or sub-group regression on the biological features without using the network data. This paper proposes a novel network-based neighborhood regression framework whose regression functions depend on both the global community-level information and local connectivity structures among entities. An efficient community-wise least square optimization approach is developed to uncover the strength of regulation among the network modules while enabling asymptotic inference. With random graph theory, we derive non-asymptotic estimation error bounds for the proposed estimator, achieving exact minimax optimality. Unlike the root-n consistency typical in canonical linear regression, our model exhibits linear consistency in the number of nodes n, highlighting the advantage of incorporating neighborhood information. The effectiveness of the proposed framework is further supported by extensive numerical experiments. Application to whole-exome sequencing and RNA-sequencing Autism datasets demonstrates the usage of the proposed method in identifying the association between the gene modules of genetic variations and the gene modules of genomic differential expressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04104v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaoming Zhen, Jin-Hong Du</dc:creator>
    </item>
    <item>
      <title>Bayesian Structured Mediation Analysis With Unobserved Confounders</title>
      <link>https://arxiv.org/abs/2407.04142</link>
      <description>arXiv:2407.04142v1 Announce Type: new 
Abstract: We explore methods to reduce the impact of unobserved confounders on the causal mediation analysis of high-dimensional mediators with spatially smooth structures, such as brain imaging data. The key approach is to incorporate the latent individual effects, which influence the structured mediators, as unobserved confounders in the outcome model, thereby potentially debiasing the mediation effects. We develop BAyesian Structured Mediation analysis with Unobserved confounders (BASMU) framework, and establish its model identifiability conditions. Theoretical analysis is conducted on the asymptotic bias of the Natural Indirect Effect (NIE) and the Natural Direct Effect (NDE) when the unobserved confounders are omitted in mediation analysis. For BASMU, we propose a two-stage estimation algorithm to mitigate the impact of these unobserved confounders on estimating the mediation effect. Extensive simulations demonstrate that BASMU substantially reduces the bias in various scenarios. We apply BASMU to the analysis of fMRI data in the Adolescent Brain Cognitive Development (ABCD) study, focusing on four brain regions previously reported to exhibit meaningful mediation effects. Compared with the existing image mediation analysis method, BASMU identifies two to four times more voxels that have significant mediation effects, with the NIE increased by 41%, and the NDE decreased by 26%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04142v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yuliang Xu, Shu Yang, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Random-Effect Meta-Analysis with Robust Between-Study Variance</title>
      <link>https://arxiv.org/abs/2407.04446</link>
      <description>arXiv:2407.04446v1 Announce Type: new 
Abstract: Meta-analyses are widely employed to demonstrate strong evidence across numerous studies. On the other hand, in the context of rare diseases, meta-analyses are often conducted with a limited number of studies in which the analysis methods are based on theoretical frameworks assuming that the between-study variance is known. That is, the estimate of between-study variance is substituted for the true value, neglecting the randomness with the between-study variance estimated from the data. Consequently, excessively narrow confidence intervals for the overall treatment effect for meta-analyses have been constructed in only a few studies. In the present study, we propose overcoming this problem by estimating the distribution of between-study variance using the maximum likelihood-like estimator. We also suggest an approach for estimating the overall treatment effect via the distribution of the between-study variance. Our proposed method can extend many existing approaches to allow more adequate estimation under a few studies. Through simulation and analysis of real data, we demonstrate that our method remains consistently conservative compared to existing methods, which enables meta-analyses to consider the randomness of the between-study variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04446v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keisuke Hanada, Tomoyuki Sugimoto</dc:creator>
    </item>
    <item>
      <title>A spatial-correlated multitask linear mixed-effects model for imaging genetics</title>
      <link>https://arxiv.org/abs/2407.04530</link>
      <description>arXiv:2407.04530v1 Announce Type: new 
Abstract: Imaging genetics aims to uncover the hidden relationship between imaging quantitative traits (QTs) and genetic markers (e.g. single nucleotide polymorphism (SNP)), and brings valuable insights into the pathogenesis of complex diseases, such as cancers and cognitive disorders (e.g. the Alzheimer's Disease). However, most linear models in imaging genetics didn't explicitly model the inner relationship among QTs, which might miss some potential efficiency gains from information borrowing across brain regions. In this work, we developed a novel Bayesian regression framework for identifying significant associations between QTs and genetic markers while explicitly modeling spatial dependency between QTs, with the main contributions as follows. Firstly, we developed a spatial-correlated multitask linear mixed-effects model (LMM) to account for dependencies between QTs. We incorporated a population-level mixed effects term into the model, taking full advantage of the dependent structure of brain imaging-derived QTs. Secondly, we implemented the model in the Bayesian framework and derived a Markov chain Monte Carlo (MCMC) algorithm to achieve the model inference. Further, we incorporated the MCMC samples with the Cauchy combination test (CCT) to examine the association between SNPs and QTs, which avoided computationally intractable multi-test issues. The simulation studies indicated improved power of our proposed model compared to classic models where inner dependencies of QTs were not modeled. We also applied the new spatial model to an imaging dataset obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04530v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhibin Pu, Shufei Ge</dc:creator>
    </item>
    <item>
      <title>Simulation-based Calibration of Uncertainty Intervals under Approximate Bayesian Estimation</title>
      <link>https://arxiv.org/abs/2407.04659</link>
      <description>arXiv:2407.04659v1 Announce Type: new 
Abstract: The mean field variational Bayes (VB) algorithm implemented in Stan is relatively fast and efficient, making it feasible to produce model-estimated official statistics on a rapid timeline. Yet, while consistent point estimates of parameters are achieved for continuous data models, the mean field approximation often produces inaccurate uncertainty quantification to the extent that parameters are correlated a posteriori. In this paper, we propose a simulation procedure that calibrates uncertainty intervals for model parameters estimated under approximate algorithms to achieve nominal coverages. Our procedure detects and corrects biased estimation of both first and second moments of approximate marginal posterior distributions induced by any estimation algorithm that produces consistent first moments under specification of the correct model. The method generates replicate datasets using parameters estimated in an initial model run. The model is subsequently re-estimated on each replicate dataset, and we use the empirical distribution over the re-samples to formulate calibrated confidence intervals of parameter estimates of the initial model run that are guaranteed to asymptotically achieve nominal coverage. We demonstrate the performance of our procedure in Monte Carlo simulation study and apply it to real data from the Current Employment Statistics survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04659v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Terrance D. Savitsky, Julie Gershunskaya</dc:creator>
    </item>
    <item>
      <title>The diameter of a stochastic matrix: A new measure for sensitivity analysis in Bayesian networks</title>
      <link>https://arxiv.org/abs/2407.04667</link>
      <description>arXiv:2407.04667v1 Announce Type: new 
Abstract: Bayesian networks are one of the most widely used classes of probabilistic models for risk management and decision support because of their interpretability and flexibility in including heterogeneous pieces of information. In any applied modelling, it is critical to assess how robust the inferences on certain target variables are to changes in the model. In Bayesian networks, these analyses fall under the umbrella of sensitivity analysis, which is most commonly carried out by quantifying dissimilarities using Kullback-Leibler information measures. In this paper, we argue that robustness methods based instead on the familiar total variation distance provide simple and more valuable bounds on robustness to misspecification, which are both formally justifiable and transparent. We introduce a novel measure of dependence in conditional probability tables called the diameter to derive such bounds. This measure quantifies the strength of dependence between a variable and its parents. We demonstrate how such formal robustness considerations can be embedded in building a Bayesian network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04667v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuele Leonelli, Jim Q. Smith, Sophia K. Wright</dc:creator>
    </item>
    <item>
      <title>Efficient and Precise Calculation of the Confluent Hypergeometric Function</title>
      <link>https://arxiv.org/abs/2407.03336</link>
      <description>arXiv:2407.03336v1 Announce Type: cross 
Abstract: Kummer's function, also known as the confluent hypergeometric function (CHF), is an important mathematical function, in particular due to its many special cases, which include the Bessel function, the incomplete Gamma function and the error function (erf). The CHF has no closed form expression, but instead is most commonly expressed as an infinite sum of ratios of rising factorials, which makes its precise and efficient calculation challenging. It is a function of three parameters, the first two being the rising factorial base of the numerator and denominator, and the third being a scale parameter. Accurate and efficient calculation for large values of the scale parameter is particularly challenging due to numeric underflow and overflow which easily occur when summing the underlying component terms. This work presents an elegant and precise mathematical algorithm for the calculation of the CHF, which is of particular advantage for large values of the scale parameter. This method massively reduces the number and range of component terms which need to be summed to achieve any required precision, thus obviating the need for the computationally intensive transformations needed by current algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03336v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alan Herschtal</dc:creator>
    </item>
    <item>
      <title>Under the null of valid specification, pre-tests of valid specification do not distort inference</title>
      <link>https://arxiv.org/abs/2407.03725</link>
      <description>arXiv:2407.03725v1 Announce Type: cross 
Abstract: Consider a parameter of interest, which can be consistently estimated under some conditions. Suppose also that we can at least partly test these conditions with specification tests. We consider the common practice of conducting inference on the parameter of interest conditional on not rejecting these tests. We show that if the tested conditions hold, conditional inference is valid but possibly conservative. This holds generally, without imposing any assumption on the asymptotic dependence between the estimator of the parameter of interest and the specification test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03725v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cl\'ement de Chaisemartin, Xavier D'Haultf{\oe}uille</dc:creator>
    </item>
    <item>
      <title>Block-diagonal idiosyncratic covariance estimation in high-dimensional factor models for financial time series</title>
      <link>https://arxiv.org/abs/2407.03781</link>
      <description>arXiv:2407.03781v1 Announce Type: cross 
Abstract: Estimation of high-dimensional covariance matrices in latent factor models is an important topic in many fields and especially in finance. Since the number of financial assets grows while the estimation window length remains of limited size, the often used sample estimator yields noisy estimates which are not even positive definite. Under the assumption of latent factor models, the covariance matrix is decomposed into a common low-rank component and a full-rank idiosyncratic component. In this paper we focus on the estimation of the idiosyncratic component, under the assumption of a grouped structure of the time series, which may arise due to specific factors such as industries, asset classes or countries. We propose a generalized methodology for estimation of the block-diagonal idiosyncratic component by clustering the residual series and applying shrinkage to the obtained blocks in order to ensure positive definiteness. We derive two different estimators based on different clustering methods and test their performance using simulation and historical data. The proposed methods are shown to provide reliable estimates and outperform other state-of-the-art estimators based on thresholding methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03781v1</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>q-fin.MF</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jocs.2024.102348</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational Science, Volume 81, 2024, 102348</arxiv:journal_reference>
      <dc:creator>Lucija \v{Z}igni\'c, Stjepan Begu\v{s}i\'c, Zvonko Kostanj\v{c}ar</dc:creator>
    </item>
    <item>
      <title>Online Bayesian changepoint detection for network Poisson processes with community structure</title>
      <link>https://arxiv.org/abs/2407.04138</link>
      <description>arXiv:2407.04138v1 Announce Type: cross 
Abstract: Network point processes often exhibit latent structure that govern the behaviour of the sub-processes. It is not always reasonable to assume that this latent structure is static, and detecting when and how this driving structure changes is often of interest. In this paper, we introduce a novel online methodology for detecting changes within the latent structure of a network point process. We focus on block-homogeneous Poisson processes, where latent node memberships determine the rates of the edge processes. We propose a scalable variational procedure which can be applied on large networks in an online fashion via a Bayesian forgetting factor applied to sequential variational approximations to the posterior distribution. The proposed framework is tested on simulated and real-world data, and it rapidly and accurately detects changes to the latent edge process rates, and to the latent node group memberships, both in an online manner. In particular, in an application on the Santander Cycles bike-sharing network in central London, we detect changes within the network related to holiday periods and lockdown restrictions between 2019 and 2020.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04138v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Corneck, Edward A. K. Cohen, James S. Martin, Francesco Sanna Passino</dc:creator>
    </item>
    <item>
      <title>Investigating symptom duration using current status data: a case study of post-acute COVID-19 syndrome</title>
      <link>https://arxiv.org/abs/2407.04214</link>
      <description>arXiv:2407.04214v1 Announce Type: cross 
Abstract: For infectious diseases, characterizing symptom duration is of clinical and public health importance. Symptom duration may be assessed by surveying infected individuals and querying symptom status at the time of survey response. For example, in a SARS-CoV-2 testing program at the University of Washington, participants were surveyed at least 28 days after testing positive and asked to report current symptom status. This study design yielded current status data: Outcome measurements for each respondent consisted only of the time of survey response and a binary indicator of whether symptoms had resolved by that time. Such study design benefits from limited risk of recall bias, but analyzing the resulting data necessitates specialized statistical tools. Here, we review methods for current status data and describe a novel application of modern nonparametric techniques to this setting. The proposed approach is valid under weaker assumptions compared to existing methods, allows use of flexible machine learning tools, and handles potential survey nonresponse. From the university study, we estimate that 19% of participants experienced ongoing symptoms 30 days after testing positive, decreasing to 7% at 90 days. Female sex, history of seasonal allergies, fatigue during acute infection, and higher viral load were associated with slower symptom resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04214v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles J. Wolock, Susan Jacob, Julia C. Bennett, Anna Elias-Warren, Jessica O'Hanlon, Avi Kenny, Nicholas P. Jewell, Andrea Rotnitzky, Ana A. Weil, Helen Y. Chu, Marco Carone</dc:creator>
    </item>
    <item>
      <title>The $s$-value: evaluating stability with respect to distributional shifts</title>
      <link>https://arxiv.org/abs/2105.03067</link>
      <description>arXiv:2105.03067v4 Announce Type: replace 
Abstract: Common statistical measures of uncertainty such as $p$-values and confidence intervals quantify the uncertainty due to sampling, that is, the uncertainty due to not observing the full population. However, sampling is not the only source of uncertainty. In practice, distributions change between locations and across time. This makes it difficult to gather knowledge that transfers across data sets. We propose a measure of instability that quantifies the distributional instability of a statistical parameter with respect to Kullback-Leibler divergence, that is, the sensitivity of the parameter under general distributional perturbations within a Kullback-Leibler divergence ball. In addition, we quantify the instability of parameters with respect to directional or variable-specific shifts. Measuring instability with respect to directional shifts can be used to detect the type of shifts a parameter is sensitive to. We discuss how such knowledge can inform data collection for improved estimation of statistical parameters under shifted distributions. We evaluate the performance of the proposed measure on real data and show that it can elucidate the distributional instability of a parameter with respect to certain shifts and can be used to improve estimation accuracy under shifted distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.03067v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suyash Gupta, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Causal inference in multi-cohort studies using the target trial framework to identify and minimize sources of bias</title>
      <link>https://arxiv.org/abs/2206.11117</link>
      <description>arXiv:2206.11117v5 Announce Type: replace 
Abstract: Longitudinal cohort studies, which follow a group of individuals over time, provide the opportunity to examine causal effects of complex exposures on long-term health outcomes. Utilizing data from multiple cohorts has the potential to add further benefit by improving precision of estimates through data pooling and by allowing examination of effect heterogeneity through replication of analyses across cohorts. However, the interpretation of findings can be complicated by biases that may be compounded when pooling data, or, contribute to discrepant findings when analyses are replicated. The "target trial" is a powerful tool for guiding causal inference in single-cohort studies. Here we extend this conceptual framework to address the specific challenges that can arise in the multi-cohort setting. By representing a clear definition of the target estimand, the target trial provides a central point of reference against which biases arising in each cohort and from data pooling can be systematically assessed. Consequently, analyses can be designed to reduce these biases and the resulting findings appropriately interpreted in light of potential remaining biases. We use a case study to demonstrate the framework and its potential to strengthen causal inference in multi-cohort studies through improved analysis design and clarity in the interpretation of findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.11117v5</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marnie Downes, Meredith O'Connor, Craig A. Olsson, David Burgner, Sharon Goldfeld, Elizabeth A. Spry, George Patton, Margarita Moreno-Betancur</dc:creator>
    </item>
    <item>
      <title>Active sampling: A machine-learning-assisted framework for finite population inference with optimal subsamples</title>
      <link>https://arxiv.org/abs/2212.10024</link>
      <description>arXiv:2212.10024v3 Announce Type: replace 
Abstract: Data subsampling has become widely recognized as a tool to overcome computational and economic bottlenecks in analyzing massive datasets. We contribute to the development of adaptive design for estimation of finite population characteristics, using active learning and adaptive importance sampling. We propose an active sampling strategy that iterates between estimation and data collection with optimal subsamples, guided by machine learning predictions on yet unseen data. The method is illustrated on virtual simulation-based safety assessment of advanced driver assistance systems. Substantial performance improvements are demonstrated compared to traditional sampling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10024v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00401706.2024.2374554</arxiv:DOI>
      <dc:creator>Henrik Imberg, Xiaomi Yang, Carol Flannagan, Jonas B\"argman</dc:creator>
    </item>
    <item>
      <title>Inference through innovation processes tested in the authorship attribution task</title>
      <link>https://arxiv.org/abs/2306.05186</link>
      <description>arXiv:2306.05186v3 Announce Type: replace 
Abstract: Urn models for innovation capture fundamental empirical laws shared by several real-world processes. The so-called urn model with triggering includes, as particular cases, the urn representation of the two-parameter Poisson-Dirichlet process and the Dirichlet process, seminal in Bayesian non-parametric inference. In this work, we leverage this connection to introduce a general approach for quantifying closeness between symbolic sequences and test it within the framework of the authorship attribution problem. The method demonstrates high accuracy when compared to other related methods in different scenarios, featuring a substantial gain in computational efficiency and theoretical transparency. Beyond the practical convenience, this work demonstrates how the recently established connection between urn models and non-parametric Bayesian inference can pave the way for designing more efficient inference methods. In particular, the hybrid approach that we propose allows us to relax the exchangeability hypothesis, which can be particularly relevant for systems exhibiting complex correlation patterns and non-stationary dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05186v3</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.app-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulio Tani Raffaelli, Margherita Lalli, Francesca Tria</dc:creator>
    </item>
    <item>
      <title>Engression: Extrapolation through the Lens of Distributional Regression</title>
      <link>https://arxiv.org/abs/2307.00835</link>
      <description>arXiv:2307.00835v3 Announce Type: replace 
Abstract: Distributional regression aims to estimate the full conditional distribution of a target variable, given covariates. Popular methods include linear and tree-ensemble based quantile regression. We propose a neural network-based distributional regression methodology called `engression'. An engression model is generative in the sense that we can sample from the fitted conditional distribution and is also suitable for high-dimensional outcomes. Furthermore, we find that modelling the conditional distribution on training data can constrain the fitted function outside of the training support, which offers a new perspective to the challenging extrapolation problem in nonlinear regression. In particular, for `pre-additive noise' models, where noise is added to the covariates before applying a nonlinear transformation, we show that engression can successfully perform extrapolation under some assumptions such as monotonicity, whereas traditional regression approaches such as least-squares or quantile regression fall short under the same assumptions. Our empirical results, from both simulated and real data, validate the effectiveness of the engression method and indicate that the pre-additive noise model is typically suitable for many real-world scenarios. The software implementations of engression are available in both R and Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00835v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinwei Shen, Nicolai Meinshausen</dc:creator>
    </item>
    <item>
      <title>A Multivariate Unimodality Test Harnessing the Dip Statistic of Mahalanobis Distances Over Random Projections</title>
      <link>https://arxiv.org/abs/2311.16614</link>
      <description>arXiv:2311.16614v4 Announce Type: replace 
Abstract: Unimodality, pivotal in statistical analysis, offers insights into dataset structures and drives sophisticated analytical procedures. While unimodality's confirmation is straightforward for one-dimensional data using methods like Silverman's approach and Hartigans' dip statistic, its generalization to higher dimensions remains challenging. By extrapolating one-dimensional unimodality principles to multi-dimensional spaces through linear random projections and leveraging point-to-point distancing, our method, rooted in $\alpha$-unimodality assumptions, presents a novel multivariate unimodality test named mud-pod. Both theoretical and empirical studies confirm the efficacy of our method in unimodality assessment of multidimensional datasets as well as in estimating the number of clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16614v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prodromos Kolyvakis, Aristidis Likas</dc:creator>
    </item>
    <item>
      <title>MacroPARAFAC for handling rowwise and cellwise outliers in incomplete multi-way data</title>
      <link>https://arxiv.org/abs/2312.01168</link>
      <description>arXiv:2312.01168v2 Announce Type: replace 
Abstract: Multi-way data extend two-way matrices into higher-dimensional tensors, often explored through dimensional reduction techniques. In this paper, we study the Parallel Factor Analysis (PARAFAC) model for handling multi-way data, representing it more compactly through a concise set of loading matrices and scores. We assume that the data may be incomplete and could contain both rowwise and cellwise outliers, signifying cases that deviate from the majority and outlying cells dispersed throughout the data array. To address these challenges, we present a novel algorithm designed to robustly estimate both loadings and scores. Additionally, we introduce an enhanced outlier map to distinguish various patterns of outlying behavior. Through simulations and the analysis of fluorescence Excitation-Emission Matrix (EEM) data, we demonstrate the robustness of our approach. Our results underscore the effectiveness of diagnostic tools in identifying and interpreting unusual patterns within the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01168v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.chemolab.2024.105170</arxiv:DOI>
      <dc:creator>Mia Hubert, Mehdi Hirari</dc:creator>
    </item>
    <item>
      <title>Modelling clusters in network time series with an application to presidential elections in the USA</title>
      <link>https://arxiv.org/abs/2401.09381</link>
      <description>arXiv:2401.09381v2 Announce Type: replace 
Abstract: Network time series are becoming increasingly relevant in the study of dynamic processes characterised by a known or inferred underlying network structure. Generalised Network Autoregressive (GNAR) models provide a parsimonious framework for exploiting the underlying network, even in the high-dimensional setting. We extend the GNAR framework by presenting the $\textit{community}$-$\alpha$ GNAR model that exploits prior knowledge and/or exogenous variables for identifying and modelling dynamic interactions across communities in the network. We further analyse the dynamics of $\textit{ Red, Blue}$ and $\textit{Swing}$ states throughout presidential elections in the USA. Our analysis suggests interesting global and communal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09381v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Guy Nason, Daniel Salnikov, Mario Cortina-Borja</dc:creator>
    </item>
    <item>
      <title>Integrating representative and non-representative survey data for efficient inference</title>
      <link>https://arxiv.org/abs/2404.02283</link>
      <description>arXiv:2404.02283v3 Announce Type: replace 
Abstract: Non-representative surveys are commonly used and widely available but suffer from selection bias that generally cannot be entirely eliminated using weighting techniques. Instead, we propose a Bayesian method to synthesize longitudinal representative unbiased surveys with non-representative biased surveys by estimating the degree of selection bias over time. We show using a simulation study that synthesizing biased and unbiased surveys together out-performs using the unbiased surveys alone, even if the selection bias may evolve in a complex manner over time. Using COVID-19 vaccination data, we are able to synthesize two large sample biased surveys with an unbiased survey to reduce uncertainty in now-casting and inference estimates while simultaneously retaining the empirical credible interval coverage. Ultimately, we are able to conceptually obtain the properties of a large sample unbiased survey if the assumed unbiased survey, used to anchor the estimates, is unbiased for all time-points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02283v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Dyrkton, Paul Gustafson, Harlan Campbell</dc:creator>
    </item>
    <item>
      <title>Mining Invariance from Nonlinear Multi-Environment Data: Binary Classification</title>
      <link>https://arxiv.org/abs/2404.15245</link>
      <description>arXiv:2404.15245v2 Announce Type: replace 
Abstract: Making predictions in an unseen environment given data from multiple training environments is a challenging task. We approach this problem from an invariance perspective, focusing on binary classification to shed light on general nonlinear data generation mechanisms. We identify a unique form of invariance that exists solely in a binary setting that allows us to train models invariant over environments. We provide sufficient conditions for such invariance and show it is robust even when environmental conditions vary greatly. Our formulation admits a causal interpretation, allowing us to compare it with various frameworks. Finally, we propose a heuristic prediction method and conduct experiments using real and synthetic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15245v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Austin Goddard, Kang Du, Yu Xiang</dc:creator>
    </item>
    <item>
      <title>Volatility Forecasting Using Similarity-based Parameter Correction and Aggregated Shock Information</title>
      <link>https://arxiv.org/abs/2406.08738</link>
      <description>arXiv:2406.08738v2 Announce Type: replace 
Abstract: We develop a procedure for forecasting the volatility of a time series immediately following a news shock. Adapting the similarity-based framework of Lin and Eck (2020), we exploit series that have experienced similar shocks. We aggregate their shock-induced excess volatilities by positing the shocks to be affine functions of exogenous covariates. The volatility shocks are modeled as random effects and estimated as fixed effects. The aggregation of these estimates is done in service of adjusting the $h$-step-ahead GARCH forecast of the time series under study by an additive term. The adjusted and unadjusted forecasts are evaluated using the unobservable but easily-estimated realized volatility (RV). A real-world application is provided, as are simulation results suggesting the conditions and hyperparameters under which our method thrives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08738v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David P. Lundquist, Daniel J. Eck</dc:creator>
    </item>
    <item>
      <title>Monte Carlo Integration in Simple and Complex Simulation Designs</title>
      <link>https://arxiv.org/abs/2406.15285</link>
      <description>arXiv:2406.15285v2 Announce Type: replace 
Abstract: Simulation studies are used to evaluate and compare the properties of statistical methods in controlled experimental settings. In most cases, performing a simulation study requires knowledge of the true value of the parameter, or estimand, of interest. However, in many simulation designs, the true value of the estimand is difficult to compute analytically. Here, we illustrate the use of Monte Carlo integration to compute true estimand values in simple and complex simulation designs. We provide general pseudocode that can be replicated in any software program of choice to demonstrate key principles in using Monte Carlo integration in two scenarios: a simple three variable simulation where interest lies in the marginally adjusted odds ratio; and a more complex causal mediation analysis where interest lies in the controlled direct effect in the presence of mediator-outcome confounders affected by the exposure. We discuss general strategies that can be used to minimize Monte Carlo error, and to serve as checks on the simulation program to avoid coding errors. R programming code is provided illustrating the application of our pseudocode in these settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15285v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashley I. Naimi, David Benkeser, Jacqueline E. Rudolph</dc:creator>
    </item>
    <item>
      <title>Covariate-dependent hierarchical Dirichlet process</title>
      <link>https://arxiv.org/abs/2407.02676</link>
      <description>arXiv:2407.02676v2 Announce Type: replace 
Abstract: The intricacies inherent in contemporary real datasets demand more advanced statistical models to effectively address complex challenges. In this article we delve into problems related to identifying clusters across related groups, when additional covariate information is available. We formulate a novel Bayesian nonparametric approach based on mixture models, integrating ideas from the hierarchical Dirichlet process and "single-atoms" dependent Dirichlet process. The proposed method exhibits exceptional generality and flexibility, accommodating both continuous and discrete covariates through the utilization of appropriate kernel functions. We construct a robust and efficient Markov chain Monte Carlo (MCMC) algorithm involving data augmentation to tackle the intractable normalized weights. The versatility of the proposed model extends our capability to discern the relationship between covariates and clusters. Through testing on both simulated and real-world datasets, our model demonstrates its capacity to identify meaningful clusters across groups, providing valuable insights for a spectrum of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02676v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Huizi Zhang, Sara Wade, Natalia Bochkina</dc:creator>
    </item>
    <item>
      <title>Robust Validation: Confident Predictions Even When Distributions Shift</title>
      <link>https://arxiv.org/abs/2008.04267</link>
      <description>arXiv:2008.04267v3 Announce Type: replace-cross 
Abstract: While the traditional viewpoint in machine learning and statistics assumes training and testing samples come from the same population, practice belies this fiction. One strategy -- coming from robust statistics and optimization -- is thus to build a model robust to distributional perturbations. In this paper, we take a different approach to describe procedures for robust predictive inference, where a model provides uncertainty estimates on its predictions rather than point predictions. We present a method that produces prediction sets (almost exactly) giving the right coverage level for any test distribution in an $f$-divergence ball around the training population. The method, based on conformal inference, achieves (nearly) valid coverage in finite samples, under only the condition that the training data be exchangeable. An essential component of our methodology is to estimate the amount of expected future data shift and build robustness to it; we develop estimators and prove their consistency for protection and validity of uncertainty estimates under shifts. By experimenting on several large-scale benchmark datasets, including Recht et al.'s CIFAR-v4 and ImageNet-V2 datasets, we provide complementary empirical results that highlight the importance of robust predictive validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.04267v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2023.2298037</arxiv:DOI>
      <dc:creator>Maxime Cauchois, Suyash Gupta, Alnur Ali, John C. Duchi</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for stochastic oscillatory systems using the phase-corrected Linear Noise Approximation</title>
      <link>https://arxiv.org/abs/2205.05955</link>
      <description>arXiv:2205.05955v4 Announce Type: replace-cross 
Abstract: Likelihood-based inference in stochastic non-linear dynamical systems, such as those found in chemical reaction networks and biological clock systems, is inherently complex and has largely been limited to small and unrealistically simple systems. Recent advances in analytically tractable approximations to the underlying conditional probability distributions enable long-term dynamics to be accurately modelled, and make the large number of model evaluations required for exact Bayesian inference much more feasible. We propose a new methodology for inference in stochastic non-linear dynamical systems exhibiting oscillatory behaviour and show the parameters in these models can be realistically estimated from simulated data. Preliminary analyses based on the Fisher Information Matrix of the model can guide the implementation of Bayesian inference. We show that this parameter sensitivity analysis can predict which parameters are practically identifiable. Several Markov chain Monte Carlo algorithms are compared, with our results suggesting a parallel tempering algorithm consistently gives the best approach for these systems, which are shown to frequently exhibit multi-modal posterior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.05955v4</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ben Swallow, David A. Rand, Giorgos Minas</dc:creator>
    </item>
    <item>
      <title>High-dimensional variable clustering based on maxima of a weakly dependent random process</title>
      <link>https://arxiv.org/abs/2302.00934</link>
      <description>arXiv:2302.00934v3 Announce Type: replace-cross 
Abstract: We propose a new class of models for variable clustering called Asymptotic Independent block (AI-block) models, which defines population-level clusters based on the independence of the maxima of a multivariate stationary mixing random process among clusters. This class of models is identifiable, meaning that there exists a maximal element with a partial order between partitions, allowing for statistical inference. We also present an algorithm depending on a tuning parameter that recovers the clusters of variables without specifying the number of clusters \emph{a priori}. Our work provides some theoretical insights into the consistency of our algorithm, demonstrating that under certain conditions it can effectively identify clusters in the data with a computational complexity that is polynomial in the dimension. A data-driven selection method for the tuning parameter is also proposed. To further illustrate the significance of our work, we applied our method to neuroscience and environmental real-datasets. These applications highlight the potential and versatility of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.00934v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexis Boulin, Elena Di Bernardino, Thomas Lalo\"e, Gwladys Toulemonde</dc:creator>
    </item>
    <item>
      <title>Bootstrap-Assisted Inference for Generalized Grenander-type Estimators</title>
      <link>https://arxiv.org/abs/2303.13598</link>
      <description>arXiv:2303.13598v3 Announce Type: replace-cross 
Abstract: Westling and Carone (2020) proposed a framework for studying the large sample distributional properties of generalized Grenander-type estimators, a versatile class of nonparametric estimators of monotone functions. The limiting distribution of those estimators is representable as the left derivative of the greatest convex minorant of a Gaussian process whose monomial mean can be of unknown order (when the degree of flatness of the function of interest is unknown). The standard nonparametric bootstrap is unable to consistently approximate the large sample distribution of the generalized Grenander-type estimators even if the monomial order of the mean is known, making statistical inference a challenging endeavour in applications. To address this inferential problem, we present a bootstrap-assisted inference procedure for generalized Grenander-type estimators. The procedure relies on a carefully crafted, yet automatic, transformation of the estimator. Moreover, our proposed method can be made ``flatness robust'' in the sense that it can be made adaptive to the (possibly unknown) degree of flatness of the function of interest. The method requires only the consistent estimation of a single scalar quantity, for which we propose an automatic procedure based on numerical derivative estimation and the generalized jackknife. Under random sampling, our inference method can be implemented using a computationally attractive exchangeable bootstrap procedure. We illustrate our methods with examples and we also provide a small simulation study. The development of formal results is made possible by some technical results that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13598v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Michael Jansson, Kenichi Nagasawa</dc:creator>
    </item>
    <item>
      <title>Gotta match 'em all: Solution diversification in graph matching matched filters</title>
      <link>https://arxiv.org/abs/2308.13451</link>
      <description>arXiv:2308.13451v3 Announce Type: replace-cross 
Abstract: We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13451v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhirui Li, Ben Johnson, Daniel L. Sussman, Carey E. Priebe, Vince Lyzinski</dc:creator>
    </item>
    <item>
      <title>Detecting influential observations in single-index Fr\'echet regression</title>
      <link>https://arxiv.org/abs/2311.17246</link>
      <description>arXiv:2311.17246v3 Announce Type: replace-cross 
Abstract: Regression with random data objects is becoming increasingly common in modern data analysis. Unfortunately, this novel regression method is not immune to the trouble caused by unusual observations. A metric Cook's distance extending the original Cook's distances of Cook (1977) to regression between metric-valued response objects and Euclidean predictors is proposed. The performance of the metric Cook's distance is demonstrated in regression across four different response spaces in an extensive experimental study. Two real data applications involving the analyses of distributions of COVID-19 transmission in the State of Texas and the analyses of the structural brain connectivity networks are provided to illustrate the utility of the proposed method in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17246v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul-Nasah Soale</dc:creator>
    </item>
    <item>
      <title>Planetary Causal Inference: Implications for the Geography of Poverty</title>
      <link>https://arxiv.org/abs/2406.02584</link>
      <description>arXiv:2406.02584v2 Announce Type: replace-cross 
Abstract: Earth observation data such as satellite imagery can, when combined with machine learning, can have far-reaching impacts on our understanding of the geography of poverty through the prediction of living conditions, especially where government-derived economic indicators are either unavailable or potentially untrustworthy. Recent work has progressed in using Earth Observation (EO) data not only to predict spatial economic outcomes but also to explore cause and effect, an understanding which is critical for downstream policy analysis. In this review, we first document the growth of interest in using satellite images together with EO data in causal analysis. We then trace the relationship between spatial statistics and machine learning methods before discussing four ways in which EO data has been used in causal machine learning pipelines -- (1.) poverty outcome imputation for downstream causal analysis, (2.) EO image deconfounding, (3.) EO-based treatment effect heterogeneity, and (4.) EO-based transportability analysis. We conclude by providing a step-by-step workflow for how researchers can incorporate EO data in causal ML analysis going forward, outlining major choices of data, models, and evaluation metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02584v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kazuki Sakamoto, Connor T. Jerzak, Adel Daoud</dc:creator>
    </item>
    <item>
      <title>Estimating Treatment Effects under Recommender Interference: A Structured Neural Networks Approach</title>
      <link>https://arxiv.org/abs/2406.14380</link>
      <description>arXiv:2406.14380v3 Announce Type: replace-cross 
Abstract: Recommender systems are essential for content-sharing platforms by curating personalized content. To evaluate updates to recommender systems targeting content creators, platforms frequently rely on creator-side randomized experiments. The treatment effect measures the change in outcomes when a new algorithm is implemented compared to the status quo. We show that the standard difference-in-means estimator can lead to biased estimates due to recommender interference that arises when treated and control creators compete for exposure. We propose a "recommender choice model" that describes which item gets exposed from a pool containing both treated and control items. By combining a structural choice model with neural networks, this framework directly models the interference pathway while accounting for rich viewer-content heterogeneity. We construct a debiased estimator of the treatment effect and prove it is $\sqrt n$-consistent and asymptotically normal with potentially correlated samples. We validate our estimator's empirical performance with a field experiment on Weixin short-video platform. In addition to the standard creator-side experiment, we conduct a costly double-sided randomization design to obtain a benchmark estimate free from interference bias. We show that the proposed estimator yields results comparable to the benchmark, whereas the standard difference-in-means estimator can exhibit significant bias and even produce reversed signs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14380v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruohan Zhan, Shichao Han, Yuchen Hu, Zhenling Jiang</dc:creator>
    </item>
  </channel>
</rss>
