<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Feb 2025 02:55:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Certified Decisions</title>
      <link>https://arxiv.org/abs/2502.17830</link>
      <description>arXiv:2502.17830v1 Announce Type: new 
Abstract: Hypothesis tests and confidence intervals are ubiquitous in empirical research, yet their connection to subsequent decision-making is often unclear. We develop a theory of certified decisions that pairs recommended decisions with inferential guarantees. Specifically, we attach P-certificates -- upper bounds on loss that hold with probability at least $1-\alpha$ -- to recommended actions. We show that such certificates allow "safe," risk-controlling adoption decisions for ambiguity-averse downstream decision-makers. We further prove that it is without loss to limit attention to P-certificates arising as minimax decisions over confidence sets, or what Manski (2021) terms "as-if decisions with a set estimate." A parallel argument applies to E-certified decisions obtained from e-values in settings with unbounded loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17830v1</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaiah Andrews, Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Minimum Distance Estimation of Quantile Panel Data Models</title>
      <link>https://arxiv.org/abs/2502.18242</link>
      <description>arXiv:2502.18242v1 Announce Type: new 
Abstract: We propose a minimum distance estimation approach for quantile panel data models where unit effects may be correlated with covariates. This computationally efficient method involves two stages: first, computing quantile regression within each unit, then applying GMM to the first-stage fitted values. Our estimators apply to (i) classical panel data, tracking units over time, and (ii) grouped data, where individual-level data are available, but treatment varies at the group level. Depending on the exogeneity assumptions, this approach provides quantile analogs of classic panel data estimators, including fixed effects, random effects, between, and Hausman-Taylor estimators. In addition, our method offers improved precision for grouped (instrumental) quantile regression compared to existing estimators. We establish asymptotic properties as the number of units and observations per unit jointly diverge to infinity. Additionally, we introduce an inference procedure that automatically adapts to the potentially unknown convergence rate of the estimator. Monte Carlo simulations demonstrate that our estimator and inference procedure perform well in finite samples, even when the number of observations per unit is moderate. In an empirical application, we examine the impact of the food stamp program on birth weights. We find that the program's introduction increased birth weights predominantly at the lower end of the distribution, highlighting the ability of our method to capture heterogeneous effects across the outcome distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18242v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Blaise Melly, Martina Pons</dc:creator>
    </item>
    <item>
      <title>Network regression and supervised centrality estimation</title>
      <link>https://arxiv.org/abs/2111.12921</link>
      <description>arXiv:2111.12921v3 Announce Type: replace 
Abstract: The centrality in a network is often used to measure nodes' importance and model network effects on a certain outcome. Empirical studies widely adopt a two-stage procedure, which first estimates the centrality from the observed noisy network and then infers the network effect from the estimated centrality, even though it lacks theoretical understanding. We propose a unified modeling framework to study the properties of centrality estimation and inference and the subsequent network regression analysis with noisy network observations. Furthermore, we propose a supervised centrality estimation methodology, which aims to simultaneously estimate both centrality and network effect. We showcase the advantages of our method compared with the two-stage method both theoretically and numerically via extensive simulations and a case study in predicting currency risk premiums from the global trade network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.12921v3</guid>
      <category>econ.EM</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junhui Cai, Dan Yang, Ran Chen, Wu Zhu, Haipeng Shen, Linda Zhao</dc:creator>
    </item>
    <item>
      <title>Approximate Factor Models for Functional Time Series</title>
      <link>https://arxiv.org/abs/2201.02532</link>
      <description>arXiv:2201.02532v4 Announce Type: replace 
Abstract: We propose a novel approximate factor model tailored for analyzing time-dependent curve data. Our model decomposes such data into two distinct components: a low-dimensional predictable factor component and an unpredictable error term. These components are identified through the autocovariance structure of the underlying functional time series. The model parameters are consistently estimated using the eigencomponents of a cumulative autocovariance operator and an information criterion is proposed to determine the appropriate number of factors. Applications to mortality and yield curve modeling illustrate key advantages of our approach over the widely used functional principal component analysis, as it offers parsimonious structural representations of the underlying dynamics along with gains in out-of-sample forecast performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.02532v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sven Otto, Nazarii Salish</dc:creator>
    </item>
    <item>
      <title>A projection based approach for interactive fixed effects panel data models</title>
      <link>https://arxiv.org/abs/2201.11482</link>
      <description>arXiv:2201.11482v3 Announce Type: replace 
Abstract: This paper introduces a straightforward sieve-based approach for estimating and conducting inference on regression parameters in panel data models with interactive fixed effects. The method's key assumption is that factor loadings can be decomposed into an unknown smooth function of individual characteristics plus an idiosyncratic error term. Our estimator offers advantages over existing approaches by taking a simple partial least squares form, eliminating the need for iterative procedures or preliminary factor estimation. In deriving the asymptotic properties, we discover that the limiting distribution exhibits a discontinuity that depends on how well our basis functions explain the factor loadings, as measured by the variance of the error factor loadings. This finding reveals that conventional ``plug-in'' methods using the estimated asymptotic covariance can produce excessively conservative coverage probabilities. We demonstrate that uniformly valid non-conservative inference can be achieved through the cross-sectional bootstrap method. Monte Carlo simulations confirm the estimator's strong performance in terms of mean squared error and good coverage results for the bootstrap procedure. We demonstrate the practical relevance of our methodology by analyzing growth rate determinants across OECD countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.11482v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Georg Keilbar, Juan M. Rodriguez-Poo, Alexandra Soberon, Weining Wang</dc:creator>
    </item>
    <item>
      <title>Fixed-Population Causal Inference for Models of Equilibrium</title>
      <link>https://arxiv.org/abs/2501.19394</link>
      <description>arXiv:2501.19394v2 Announce Type: replace 
Abstract: In contrast to problems of interference in (exogenous) treatments, models of interference in unit-specific (endogenous) outcomes do not usually produce a reduced-form representation where outcomes depend on other units' treatment status only at a short network distance, or only through a known exposure mapping. This remains true if the structural mechanism depends on outcomes of peers only at a short network distance, or through a known exposure mapping. In this paper, we first define causal estimands that are identified and estimable from a single experiment on the network under minimal assumptions on the structure of interference, and which represent average partial causal responses which generally vary with other global features of the realized assignment. Under a fixed-population, design-based approach, we show unbiasedness, consistency and asymptotic normality for inverse-probability weighting (IPW) estimators for those causal parameters from a randomized experiment on a single network. We also analyze more closely the case of marginal interventions in a model of equilibrium with smooth response functions where we can recover LATE-type weighted averages of derivatives of those response functions. Under additional structural assumptions, these "agnostic" causal estimands can be combined to recover model parameters, but also retain their less restrictive causal interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19394v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konrad Menzel</dc:creator>
    </item>
    <item>
      <title>Conditional Triple Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2502.16126</link>
      <description>arXiv:2502.16126v2 Announce Type: replace 
Abstract: Triple difference-in-differences (TDID) designs are widely used in empirical research to estimate causal effects. In practice, most implementations rely on a specification with controls. However, we show that such approaches introduce bias due to differences in covariate distributions across groups. To address this issue, we propose a re-weighted estimator that correctly identifies a causal estimand of interest by aligning covariate distributions across groups. For estimation we develop a double-robust approach. A R package is provided for general use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16126v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dor Leventer</dc:creator>
    </item>
    <item>
      <title>Decision synthesis in monetary policy</title>
      <link>https://arxiv.org/abs/2406.03321</link>
      <description>arXiv:2406.03321v2 Announce Type: replace-cross 
Abstract: The macroeconomy is a sophisticated dynamic system involving significant uncertainties that complicate modelling. In response, decision-makers consider multiple models that provide different predictions and policy recommendations which are then synthesized into a policy decision. In this setting, we develop Bayesian predictive decision synthesis (BPDS) to formalize monetary policy decision processes. BPDS draws on recent developments in model combination and statistical decision theory that yield new opportunities in combining multiple models, emphasizing the integration of decision goals, expectations and outcomes into the model synthesis process. Our case study concerns central bank policy decisions about target interest rates with a focus on implications for multi-step macroeconomic forecasting. This application also motivates new methodological developments in conditional forecasting and BPDS, presented and developed here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03321v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Chernis, Gary Koop, Emily Tallman, Mike West</dc:creator>
    </item>
  </channel>
</rss>
