<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Dec 2025 03:45:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Cumulative Residual Mathai--Haubold Entropy and its Non-parametric Inference</title>
      <link>https://arxiv.org/abs/2512.10997</link>
      <description>arXiv:2512.10997v1 Announce Type: new 
Abstract: We introduce the cumulative residual Mathai--Haubold entropy (CRMHE) and investigate its properties. We then propose a dynamic counterpart, the dynamic cumulative residual Mathai--Haubold entropy (DCRMHE), and establish its uniqueness in characterizing the distribution function. Non-parametric estimators for the CRMHE and DCRMHE are developed based on the kernel density estimation of the survival function. The efficacy of the estimators is assessed through a comprehensive Monte Carlo simulation study. The relevance of the proposed DCRMHE estimator is illustrated using two real-world datasets: on the failure times of 70 aircraft windshields and failure times of 40 randomly selected mechanical switches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10997v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anija C. R, Smitha S., Sudheesh K. Kattumannil</dc:creator>
    </item>
    <item>
      <title>On a class of constrained Bayesian filters and their numerical implementation in high-dimensional state-space Markov models</title>
      <link>https://arxiv.org/abs/2512.11012</link>
      <description>arXiv:2512.11012v1 Announce Type: new 
Abstract: Bayesian filtering is a key tool in many problems that involve the online processing of data, including data assimilation, optimal control, nonlinear tracking and others. Unfortunately, the implementation of filters for nonlinear, possibly high-dimensional, dynamical systems is far from straightforward, as computational methods have to meet a delicate trade-off involving stability, accuracy and computational cost. In this paper we investigate the design, and theoretical features, of constrained Bayesian filters for state space models. The constraint on the filter is given by a sequence of compact subsets of the state space that determines the sources and targets of the Markov transition kernels in the dynamical model. Subject to such constraints, we provide sufficient conditions for filter stability and approximation error rates with respect to the original (unconstrained) Bayesian filter. Then, we look specifically into the implementation of constrained filters in a continuous-discrete setting where the state of the system is a continuous-time stochastic It\^o process but data are collected sequentially over a time grid. We propose an implementation of the constraint that relies on a data-driven modification of the drift of the It\^o process using barrier functions, and discuss the relation of this scheme with methods based on the Doob $h$-transform. Finally, we illustrate the theoretical results and the performance of the proposed methods in computer experiments for a partially-observed stochastic Lorenz 96 model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11012v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Utku Erdogan, Gabriel J. Lord, Joaquin Miguez</dc:creator>
    </item>
    <item>
      <title>Gaussian random field's anisotropy using excursion sets</title>
      <link>https://arxiv.org/abs/2512.11085</link>
      <description>arXiv:2512.11085v1 Announce Type: new 
Abstract: This paper addresses the problem of detecting and estimating the anisotropy of a stationary real-valued random field from a single realization of one of its excursion sets. This setting is challenging as it relies on observing a binary image without prior knowledge of the field's mean, variance, or the specific threshold value.
  Our first contribution is to propose a generalization of Caba\~na's contour method to arbitrary dimensions by analyzing the Palm distribution of normal vectors along the excursion set boundaries. We demonstrate that the anisotropy parameters can be recovered by solving a smooth and strongly convex optimization problem involving the eigenvalues of the empirical covariance matrix of these normal vectors.
  Our second main contribution is a new, model-agnostic statistical test for isotropy in dimension two. We introduce a statistic based on the contour method which is asymptotically distributed as a chi-squared variable with two degrees of freedom under the null hypothesis of quasi-isotropy. Unlike existing methods based on Lipschitz-Killing curvatures, this procedure does not require knowledge of the random field's covariance structure.
  Extensive numerical experiments show that our test is well-calibrated and more powerful than model-based alternatives as well as that the estimation of the anisotropy parameters, including the directions, is robust and efficient. Finally, we apply this framework to test the quasi-isotropy of the Cosmic Microwave Background (CMB) using the Planck data release 3 mission.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11085v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jean-Marc Aza\"is, Federico Dalmao, Yohann De Castro</dc:creator>
    </item>
    <item>
      <title>Autotune: fast, accurate, and automatic tuning parameter selection for Lasso</title>
      <link>https://arxiv.org/abs/2512.11139</link>
      <description>arXiv:2512.11139v2 Announce Type: new 
Abstract: Least absolute shrinkage and selection operator (Lasso), a popular method for high-dimensional regression, is now used widely for estimating high-dimensional time series models such as the vector autoregression (VAR). Selecting its tuning parameter efficiently and accurately remains a challenge, despite the abundance of available methods for doing so. We propose $\mathsf{autotune}$, a strategy for Lasso to automatically tune itself by optimizing a penalized Gaussian log-likelihood alternately over regression coefficients and noise standard deviation. Using extensive simulation experiments on regression and VAR models, we show that $\mathsf{autotune}$ is faster, and provides better generalization and model selection than established alternatives in low signal-to-noise regimes. In the process, $\mathsf{autotune}$ provides a new estimator of noise standard deviation that can be used for high-dimensional inference, and a new visual diagnostic procedure for checking the sparsity assumption on regression coefficients. Finally, we demonstrate the utility of $\mathsf{autotune}$ on a real-world financial data set. An R package based on C++ is also made publicly available on Github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11139v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tathagata Sadhukhan, Ines Wilms, Stephan Smeekes, Sumanta Basu</dc:creator>
    </item>
    <item>
      <title>Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems</title>
      <link>https://arxiv.org/abs/2512.11150</link>
      <description>arXiv:2512.11150v1 Announce Type: new 
Abstract: LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11150v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eddie Landesberg</dc:creator>
    </item>
    <item>
      <title>Estimation of Contextual Exposure to HIV from GPS Data</title>
      <link>https://arxiv.org/abs/2512.11159</link>
      <description>arXiv:2512.11159v1 Announce Type: new 
Abstract: We present a comprehensive statistical methodological framework for estimating contextual exposure to HIV that includes local (grid-cell level) estimation of HIV prevalence and human activity space estimation based on GPS data. The development of our framework was necessary to analyze HIV surveillance and sociodemographic survey data in conjunction with GPS data collected in rural KwaZulu-Natal, South Africa, to study the mobility patterns of young people. Based on mobility and contextual exposure measures, we examine whether the sex and age of study participants systematically influence the extent and structure of their mobility patterns. We discuss techniques for investigating how the study participants' contextual exposure to HIV changes as their activity spaces expand beyond residential locations, as well as methods for identifying study participants who may be at increased risk of acquiring HIV. KEYWORDS: Contextual HIV exposure; GPS-based mobility analysis; Activity space; HIV prevalence mapping</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11159v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyang Wu, Zhaoxing Wu, Thulile Mathenjwa, Elphas Okango, Khai Hoan Tram, Margot Otto, Maxime Inghels, Paul Mee, Diego Cuadros, Hae-Young Kim, Till Barnighausen, Frank Tanser, Adrian Dobra</dc:creator>
    </item>
    <item>
      <title>Network Estimation for Stationary Time Series</title>
      <link>https://arxiv.org/abs/2512.11406</link>
      <description>arXiv:2512.11406v1 Announce Type: new 
Abstract: High-dimensional multivariate time series are common in many scientific and industrial applications, where the interest lies in identifying key dependence structure within the data for subsequent analysis tasks, such as forecasting. An important avenue to achieve this is through the estimation of the conditional independence graph via graphical models, although for time series data settings the underpinning temporal dependence can make this task challenging. In this article, we propose a novel wavelet domain technique that allows the data-driven inference of the (sparse) conditional independence graph of a high-dimensional stationary multivariate time series. By adopting the locally stationary wavelet modelling framework, we repose the estimation problem as a well-principled wavelet domain graphical lasso formulation. Theoretical results establish that our associated estimation scheme enjoys good consistency properties when determining sparse dependence structure in input time series data. The performance of the proposed method is illustrated using extensive simulations and we demonstrate its applicability on a real-world dataset representing hospitalisations of COVID-19 patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11406v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madeline A. Shelley, Chiara Boetti, Marina I. Knight, Matthew A. Nunes</dc:creator>
    </item>
    <item>
      <title>Conditional Copula models using loss-based Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2512.11427</link>
      <description>arXiv:2512.11427v1 Announce Type: new 
Abstract: The study of dependence between random variables under external influences is a challenging problem in multivariate analysis. We address this by proposing a novel semi-parametric approach for conditional copula models using Bayesian additive regression trees (BART) models. BART is becoming a popular approach in statistical modelling due to its simple ensemble type formulation complemented by its ability to provide inferential insights. Although BART allows us to model complex functional relationships, it tends to suffer from overfitting. In this article, we exploit a loss-based prior for the tree topology that is designed to reduce the tree complexity. In addition, we propose a novel adaptive Reversible Jump Markov Chain Monte Carlo algorithm that is ergodic in nature and requires very few assumptions allowing us to model complex and non-smooth likelihood functions with ease. Moreover, we show that our method can efficiently recover the true tree structure and approximate a complex conditional copula parameter, and that our adaptive routine can explore the true likelihood region under a sub-optimal proposal variance. Lastly, we provide case studies concerning the effect of gross domestic product on the dependence between the life expectancies and literacy rates of the male and female populations of different countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11427v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tathagata Basu, Fabrizio Leisen, Cristiano Villa, Kevin Wilson</dc:creator>
    </item>
    <item>
      <title>Advances in Ontology--Based Mining of Adverse Drug Reactions</title>
      <link>https://arxiv.org/abs/2512.11452</link>
      <description>arXiv:2512.11452v1 Announce Type: new 
Abstract: Post--marketing pharmacovigilance is essential for identifying adverse drug reactions (ADRs) that elude detection during pre--marketing clinical trials. This study explores a novel approach that integrates an adverse event (AE) ontology into a zero--inflated negative binomial model to improve ADR detection. By accounting for the biological similarities among correlated AEs and addressing the excess of zero counts, this method more effectively disentangles AE associations. Statistical significance is evaluated using a permutation--based maximum statistic that preserves AE correlations within individual reports. Simulations and an application to real data from the Veneto drug safety database demonstrate that the ontology--based model consistently outperforms classical models such as the Gamma--Poisson Shrinker (GPS). For post--selection inference, we furthermore explore a data thinning technique for convolution--closed families, enabling the creation of independent training and validation datasets while retaining all drug--AE pairs. This approach is compared with conventional random train/test splitting, which may leave some drugs or AEs absent from one subset, and stratified splitting, which requires expanding aggregated counts into individual instances. The data--thinning technique and stratified splitting yield very similar results, with stratified splitting showing a slight benefit, and both clearly outperform random splitting in ensuring reliable and consistent model evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11452v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenenisa Tadesse Dame, Pietro Belloni, Ugo Moretti, Fabio Scapini, Marco Tuccori, Alessandra R. Brazzale</dc:creator>
    </item>
    <item>
      <title>Bounds for causal mediation effects</title>
      <link>https://arxiv.org/abs/2512.11549</link>
      <description>arXiv:2512.11549v1 Announce Type: new 
Abstract: Several frameworks have been proposed for studying causal mediation analysis. What these frameworks have in common is that they all make assumptions for point identifications that can be violated even when treatment is randomized. When a causal effect is not point-identified, one can sometimes derive bounds, i.e. a range of possible values that are consistent with the observed data. In this work, we study causal bounds for mediation effects under both the natural effects framework and the separable effects framework. In particular, we show that when there are unmeasured confounders for the intermediate variables(s) the sharp symbolic bounds on separable (in)direct effect coincide with existing bounds for natural (in)direct effects in the analogous setting. We compare these bounds to valid bounds for the natural direct effects when only the cross-world independence assumption does not hold. Furthermore, we demonstrate the use and compare the results of the bounds on data from a trial investigating the effect of peanut consumption on the development of peanut allergy in infants through specific pathways of measured immunological biomarkers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11549v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie S. Breum, Vanessa Didelez, Erin E. Gabriel, Michael C. Sachs</dc:creator>
    </item>
    <item>
      <title>Detecting changes in the mean of spatial random fields on a regular grid</title>
      <link>https://arxiv.org/abs/2512.11599</link>
      <description>arXiv:2512.11599v1 Announce Type: new 
Abstract: We propose statistical procedures for detecting changes in the mean of spatial random fields observed on regular grids. The proposed framework provides a general approach to change detection in spatial processes. Extending a block-based method originally developed for time series, we introduce two test statistics, one based on Gini's mean difference and a novel variance-based variant. Under mild moment conditions, we derive asymptotic normality of the variance-based statistic and prove its consistency against almost all non-constant mean functions (in a sense of positive Lebesgue measure). To accommodate spatial dependence, we further develop a de-correlation algorithm based on estimated autocovariances. Monte Carlo simulations demonstrate that both tests maintain appropriate size and power for both independent and dependent data. In an application to satellite images, especially our variance-based test reliably detects regions undergoing deforestation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11599v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheila T. G\"orz, Roland Fried</dc:creator>
    </item>
    <item>
      <title>Spatially Varying Gene Regulatory Networks via Bayesian Nonparametric Covariate-Dependent Directed Cyclic Graphical Models</title>
      <link>https://arxiv.org/abs/2512.11732</link>
      <description>arXiv:2512.11732v1 Announce Type: new 
Abstract: Spatial transcriptomics technologies enable the measurement of gene expression with spatial context, providing opportunities to understand how gene regulatory networks vary across tissue regions. However, existing graphical models focus primarily on undirected graphs or directed acyclic graphs, limiting their ability to capture feedback loops that are prevalent in gene regulation. Moreover, ensuring the so-called stability condition of cyclic graphs, while allowing graph structures to vary continuously with spatial covariates, presents significant statistical and computational challenges. We propose BNP-DCGx, a Bayesian nonparametric approach for learning spatially varying gene regulatory networks via covariate-dependent directed cyclic graphical models. Our method introduces a covariate-dependent random partition as an intermediary layer in a hierarchical model, which discretizes the covariate space into clusters with cluster-specific stable directed cyclic graphs. Through partition averaging, we obtain smoothly varying graph structures over space while maintaining theoretical guarantees of stability. We develop an efficient parallel tempered Markov chain Monte Carlo algorithm for posterior inference and demonstrate through simulations that our method accurately recovers both piecewise constant and continuously varying graph structures. Application to spatial transcriptomics data from human dorsolateral prefrontal cortex reveals spatially varying regulatory networks with feedback loops, identifies potential cell subtypes within established cell types based on distinct regulatory mechanisms, and provides new insights into spatial organization of gene regulation in brain tissue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11732v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trisha Dawn, Yang Ni</dc:creator>
    </item>
    <item>
      <title>Forest Kernel Balancing Weights: Outcome-Guided Features for Causal Inference</title>
      <link>https://arxiv.org/abs/2512.11751</link>
      <description>arXiv:2512.11751v1 Announce Type: new 
Abstract: While balancing covariates between groups is central for observational causal inference, selecting which features to balance remains a challenging problem. Kernel balancing is a promising approach that first estimates a kernel that captures similarity across units and then balances a (possibly low-dimensional) summary of that kernel, indirectly learning important features to balance. In this paper, we propose forest kernel balancing, which leverages the underappreciated fact that tree-based machine learning models, namely random forests and Bayesian additive regression trees (BART), implicitly estimate a kernel based on the co-occurrence of observations in the same terminal leaf node. Thus, even though the resulting kernel is solely a function of baseline features, the selected nonlinearities and other interactions are important for predicting the outcome -- and therefore are important for addressing confounding. Through simulations and applied illustrations, we show that forest kernel balancing leads to meaningful computational and statistical improvement relative to standard kernel methods, which do not incorporate outcome information when learning features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11751v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andy A. Shen, Eli Ben-Michael, Avi Feller, Luke Keele, Jared Murray</dc:creator>
    </item>
    <item>
      <title>Covariate-assisted graph matching</title>
      <link>https://arxiv.org/abs/2512.11761</link>
      <description>arXiv:2512.11761v1 Announce Type: new 
Abstract: Data integration is essential across diverse domains, from historical records to biomedical research, facilitating joint statistical inference. A crucial initial step in this process involves merging multiple data sources based on matching individual records, often in the absence of unique identifiers. When the datasets are networks, this problem is typically addressed through graph matching methodologies. For such cases, auxiliary features or covariates associated with nodes or edges can be instrumental in achieving improved accuracy. However, most existing graph matching techniques do not incorporate this information, limiting their performance against non-identifiable and erroneous matches. To overcome these limitations, we propose two novel covariate-assisted seeded graph matching methods, where a partial alignment for a set of nodes, called seeds, is known. The first one solves a quadratic assignment problem (QAP) over the whole graph, while the second one only leverages the local neighborhood structure of seed nodes for computational scalability. Both methods are grounded in a conditional modeling framework, where elements of one graph's adjacency matrix are modeled using a generalized linear model (GLM), given the other graph and the available covariates. We establish theoretical guarantees for model estimation error and exact recovery of the solution of the QAP. The effectiveness of our methods is demonstrated through numerical experiments and in an application to matching the statistics academic genealogy and the collaboration networks. By leveraging additional covariates, we achieve improved alignment accuracy. Our work highlights the power of integrating covariate information in the classical graph matching setup, offering a practical and improved framework for combining network data with wide-ranging applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11761v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trisha Dawn, Jes\'us Arroyo</dc:creator>
    </item>
    <item>
      <title>A Doubled Adjacency Spectral Embedding Approach to Graph Clustering</title>
      <link>https://arxiv.org/abs/2512.11777</link>
      <description>arXiv:2512.11777v1 Announce Type: new 
Abstract: Spectral clustering is a popular tool in network data analysis, with applications in a variety of scientific application areas. However, many studies have shown that spectral clustering does not perform well on certain network structures, particularly core-periphery networks. To improve clustering performance in core-periphery structures, Adjacency Spectral Embedding (ASE) has been introduced, which performs clustering via a network's adjacency matrix instead of the graph Laplacian. Despite its advantages in this setting, the optimal performance of ASE is limited to dense networks, whilst network data observed in practice is often sparse in nature. To address this limitation, we propose a new approach which we term Doubled Adjacency Spectral Embedding (DASE), motivated by the observation that the squared adjacency matrix will leverage the fewer connections in sparse structures more efficiently in clustering applications. Theoretical results establish that DASE enjoys good consistency properties when determining sparse community structure. The performance and general applicability of the proposed method is evaluated using extensive simulations on both directed and undirected networks. Our results highlight the improved clustering performance on both sparse and dense networks in the presence of core-periphery structures. We illustrate our proposed technique on real-world employment and transportation datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11777v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sinyoung Park, Matthew Nunes, Sandipan Roy</dc:creator>
    </item>
    <item>
      <title>Provable Recovery of Locally Important Signed Features and Interactions from Random Forest</title>
      <link>https://arxiv.org/abs/2512.11081</link>
      <description>arXiv:2512.11081v1 Announce Type: cross 
Abstract: Feature and Interaction Importance (FII) methods are essential in supervised learning for assessing the relevance of input variables and their interactions in complex prediction models. In many domains, such as personalized medicine, local interpretations for individual predictions are often required, rather than global scores summarizing overall feature importance. Random Forests (RFs) are widely used in these settings, and existing interpretability methods typically exploit tree structures and split statistics to provide model-specific insights. However, theoretical understanding of local FII methods for RF remains limited, making it unclear how to interpret high importance scores for individual predictions. We propose a novel, local, model-specific FII method that identifies frequent co-occurrences of features along decision paths, combining global patterns with those observed on paths specific to a given test point. We prove that our method consistently recovers the true local signal features and their interactions under a Locally Spike Sparse (LSS) model and also identifies whether large or small feature values drive a prediction. We illustrate the usefulness of our method and theoretical results through simulation studies and a real-world data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11081v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kata Vuk, Nicolas Alexander Ihlo, Merle Behr</dc:creator>
    </item>
    <item>
      <title>Algorithms for Reconstructing B Cell Lineages in the Presence of Context-Dependent Somatic Hypermutation</title>
      <link>https://arxiv.org/abs/2512.11693</link>
      <description>arXiv:2512.11693v1 Announce Type: cross 
Abstract: We introduce a method for approximating posterior probabilities of phylogenetic trees and reconstructing ancestral sequences under models of sequence evolution with site-dependence, where standard phylogenetic likelihood computations (pruning) fail. Our approach uses a combined data-augmentation and importance sampling scheme. A key advantage of our approach is the ability to leverage existing highly optimized phylogenetic software. We apply our approach to the reconstruction of B cell receptor affinity maturation lineages from high-throughput repertoire sequencing data and evaluate the impact of incorporating site-dependence on the reconstruction accuracy of both trees and ancestral sequences. We show that accounting for context-dependence during inference always improves the estimates of both ancestral sequences and lineage trees on simulated datasets. We also examine the impact of incorporating priors based on VDJ recombination models, and find that they significantly improve ancestral sequence reconstruction in germline-encoded regions, but increase errors in non-templated nucleotides. We propose a modified, piecewise prior to address this demonstrate that it improves empirical reconstruction accuracy. We apply our approach to the analysis of the HIV broadly neutralizing antibodies DH270 and CH235 which are important targets of current vaccine design efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11693v1</guid>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongkang Li, Kevin J. Wiehe, Scott C. Schmidler</dc:creator>
    </item>
    <item>
      <title>More powerful multiple testing under dependence via randomization</title>
      <link>https://arxiv.org/abs/2305.11126</link>
      <description>arXiv:2305.11126v4 Announce Type: replace 
Abstract: We develop a technique to improve the power of any e-value by a simple randomization involving one independent uniform random variable. Using this framework, we show that two procedures for false discovery rate (FDR) control -- the Benjamini-Yekutieli procedure for dependent p-values, and the e-Benjamini-Hochberg procedure for dependent e-values -- can be improved through randomization. We also improve the Hommel test under dependence, and post-selection inference procedures for confidence intervals with false coverage rate (FCR) that allow for arbitrary selection rules and dependence. Importantly, our randomized improvements are never worse than the originals and are typically strictly more powerful, with marked improvements in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11126v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Post-Integrated Inference with Surrogate Control Outcomes</title>
      <link>https://arxiv.org/abs/2410.04996</link>
      <description>arXiv:2410.04996v5 Announce Type: replace 
Abstract: Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference method that accounts for latent heterogeneity by utilizing control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects using negative control outcomes. By utilizing surrogate control outcomes as an extension of negative control outcomes, we develop semiparametric inference on projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated using random forests through simulations and analysis of single-cell CRISPR perturbed datasets, which may contain potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04996v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Bounding causal effects with an unknown mixture of informative and non-informative missingness</title>
      <link>https://arxiv.org/abs/2411.16902</link>
      <description>arXiv:2411.16902v3 Announce Type: replace 
Abstract: In experimental and observational data settings, researchers often have limited knowledge of the reasons for missing outcomes. To address this uncertainty, we propose bounds on causal effects for missing outcomes, accommodating the scenario where missingness is an unobserved mixture of informative and non-informative components. Within this mixed missingness framework, we explore several assumptions to derive bounds on causal effects, including bounds expressed as a function of user-specified sensitivity parameters. We develop influence-function based estimators of these bounds to enable flexible, non-parametric, and machine learning based estimation, achieving root-n convergence rates and asymptotic normality under relatively mild conditions. We further consider the identification and estimation of bounds for other causal quantities that remain meaningful when informative missingness reflects a competing outcome, such as death. We conduct simulation studies and illustrate our methodology with a study on the causal effect of antipsychotic drugs on diabetes risk using a health insurance dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16902v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Rubinstein, Denis Agniel, Larry Han, Marcela Horvitz-Lennon, Sharon-Lise Normand</dc:creator>
    </item>
    <item>
      <title>Confirmatory Biomarker Identification with k-FWER Control Using Derandomized Knockoffs with Cox Regression</title>
      <link>https://arxiv.org/abs/2504.03907</link>
      <description>arXiv:2504.03907v2 Announce Type: replace 
Abstract: Selecting important features in high-dimensional survival analysis is critical for identifying confirmatory biomarkers while maintaining rigorous error control. In this paper, we propose a derandomized knockoffs procedure for Cox regression that enhances stability in feature selection while maintaining rigorous control over the k-familywise error rate (k-FWER). By aggregating across multiple randomized knockoff realizations, our approach mitigates the instability commonly observed with conventional knockoffs. Through extensive simulations, we demonstrate that our method consistently outperforms standard knockoffs in both selection power and error control. Moreover, we apply our procedure to a clinical dataset on primary biliary cirrhosis (PBC) to identify key prognostic biomarkers associated with patient survival. The results confirm the superior stability of the derandomized knockoffs method, allowing for a more reliable identification of important clinical variables. Additionally, our approach is applicable to datasets containing both continuous and categorical covariates, broadening its utility in real-world biomedical studies. This framework provides a robust and interpretable solution for high-dimensional survival analysis, making it particularly suitable for applications requiring precise and stable variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03907v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Liu, Nan Sun</dc:creator>
    </item>
    <item>
      <title>Independent vector analysis -- an introduction for statisticians</title>
      <link>https://arxiv.org/abs/2506.16175</link>
      <description>arXiv:2506.16175v2 Announce Type: replace 
Abstract: Blind source separation (BSS), particularly independent component analysis (ICA), has been widely used in various fields of science such as biomedical signal processing to recover latent source signals from the observed mixture. While ICA is typically applied to individual datasets, many real-world applications share underlying sources across datasets. Independent vector analysis (IVA) extends ICA to jointly analyze multiple datasets by exploiting statistical dependencies across them. While various IVA methods have been presented in signal processing literature, the statistical properties of methods remains largely unexplored. This article introduces the IVA model, numerous density models used in IVA, and various classical IVA methods to statistics community highlighting the need for further theoretical developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16175v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miro Arvila, Klaus Nordhausen, Mika Sipil\"a, Sara Taskinen</dc:creator>
    </item>
    <item>
      <title>A Bayesian Dirichlet Auto-Regressive Conditional Heteroskedasticity Model for Forecasting Currency Shares</title>
      <link>https://arxiv.org/abs/2507.14132</link>
      <description>arXiv:2507.14132v3 Announce Type: replace 
Abstract: We analyze daily Airbnb service-fee shares across eleven settlement currencies, a compositional series that shows bursts of volatility after shocks such as the COVID-19 pandemic. Standard Dirichlet time series models assume constant precision and therefore miss these episodes. We introduce B-DARMA-DARCH, a Bayesian Dirichlet autoregressive moving average model with a Dirichlet ARCH component, which lets the precision parameter follow an ARMA recursion. The specification preserves the Dirichlet likelihood so forecasts remain valid compositions while capturing clustered volatility. Simulations and out-of-sample tests show that B-DARMA-DARCH lowers forecast error and improves interval calibration relative to Dirichlet ARMA and log-ratio VARMA benchmarks, providing a concise framework for settings where both the level and the volatility of proportions matter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14132v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Robert E. Weiss</dc:creator>
    </item>
    <item>
      <title>Bias-Reduced Estimation of Structural Equation Models</title>
      <link>https://arxiv.org/abs/2509.25419</link>
      <description>arXiv:2509.25419v2 Announce Type: replace 
Abstract: Finite-sample bias is a pervasive challenge in the estimation of structural equation models (SEMs), especially when sample sizes are small or measurement reliability is low. A range of methods have been proposed to improve finite-sample bias in the SEM literature, ranging from analytic bias corrections to resampling-based techniques, with each carrying trade-offs in scope, computational burden, and statistical performance. We apply the reduced-bias M-estimation framework (RBM, Kosmidis &amp; Lunardon, 2024, J. R. Stat. Soc. Series B Stat. Methodol.) to SEMs. The RBM framework is attractive as it requires only first- and second-order derivatives of the log-likelihood, which renders it both straightforward to implement, and computationally more efficient compared to resampling-based alternatives such as bootstrap and jackknife. It is also robust to departures from modelling assumptions. Using the same simulation setup as in Dhaene and Rosseel (2022), we illustrate that RBM estimators consistently reduce mean bias in the estimation of SEMs without inflating mean squared error. They also deliver improvements in both median bias and inference relative to maximum likelihood estimators, while maintaining robustness under non-normality. Our findings suggest that RBM offers a promising, practical, and broadly applicable tool for mitigating bias in the estimation of SEMs, particularly in small-sample research contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25419v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haziq Jamil, Yves Rosseel, Oliver Kemp, Ioannis Kosmidis</dc:creator>
    </item>
    <item>
      <title>A sensitivity analysis for the average derivative effect</title>
      <link>https://arxiv.org/abs/2511.06243</link>
      <description>arXiv:2511.06243v2 Announce Type: replace 
Abstract: In observational studies, exposures are often continuous rather than binary or discrete. At the same time, sensitivity analysis is an important tool that can help determine the robustness of a causal conclusion to a certain level of unmeasured confounding, which can never be ruled out in an observational study. Sensitivity analysis approaches for continuous exposures have now been proposed for several causal estimands. In this article, we focus on the average derivative effect (ADE). We obtain closed-form bounds for the ADE under a sensitivity model that constrains the odds ratio (at any two dose levels) between the latent and observed generalized propensity score. We propose flexible, efficient estimators for the bounds, as well as point-wise and simultaneous (over the sensitivity parameter) confidence intervals. We examine the finite sample performance of the methods through simulations and illustrate the methods on a study assessing the effect of parental income on educational attainment and a study assessing the price elasticity of petrol.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06243v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Zhang</dc:creator>
    </item>
    <item>
      <title>Designing Efficient Hybrid and Single-Arm Trials: External Control Borrowing and Sample Size Calculation</title>
      <link>https://arxiv.org/abs/2511.09353</link>
      <description>arXiv:2511.09353v2 Announce Type: replace 
Abstract: External controls (ECs) from historical trials or real-world data have gained increasing attention as a way to augment hybrid and single-arm trials, especially when balanced randomization is infeasible. While most existing work has focused on post-trial inference using ECs, their role in prospective trial design remains less explored. We address this gap by focusing on the sample size determination and power analysis for an experimental design problem that encompasses standard randomized controlled trials (RCTs), hybrid trials, and single-arm trials. Building on estimators derived from the efficient influence function, we develop hybrid and single-arm design strategies that leverage comparable EC data to reduce the required sample size of the current study. We derive asymptotic variance expressions for these estimators in terms of interpretable, population-level quantities and introduce a pre-experimental variance estimation procedure to guide sample size calculation, ensuring prespecified type I error and power for the relevant hypothesis test. Simulation studies demonstrate that the proposed hybrid and single-arm designs maintain valid type I error and achieve target power across diverse scenarios while requiring substantially fewer subjects in the current study than RCT designs. A real data application further illustrates the practical utility and advantages of the proposed hybrid and single-arm designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09353v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujing Gao, Xiang Zhang, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Estimating the Effects of Heatwaves on Health: A Causal Inference Framework</title>
      <link>https://arxiv.org/abs/2511.11433</link>
      <description>arXiv:2511.11433v2 Announce Type: replace 
Abstract: The harmful relationship between heatwaves and health has been extensively documented in medical and epidemiological literature. However, most evidence is associational and cannot be interpreted causally unless strong assumptions are made. In this paper, we first make explicit the assumptions underlying the statistical methods frequently used in the heatwave literature and demonstrate when these assumptions might break down in heatwave contexts. To address these shortcomings, we propose a causal inference framework that transparently elicits causal identification assumptions. Within this new framework, we first introduce synthetic controls (SC) for estimating heatwave effects, then propose a spatially augmented Bayesian synthetic control (SA-SC) method that accounts for spatial dependence and spillovers. Empirical Monte Carlo simulations show both methods perform well, with SA-SC reducing root mean squared error and improving posterior interval coverage under spillovers and spatial dependence. Finally, we apply the proposed methods to estimate the causal effects of heatwaves on Medicare heat-related hospitalizations among 13,753,273 beneficiaries residing in Northeastern U.S. from 2000 to 2019. This causal inference framework provides spatially coherent counterfactual outcomes and robust, interpretable, and transparent causal estimates while explicitly addressing the unexamined assumptions in existing methods that pervade the heatwave effect literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11433v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giulio Grossi, Leo Vanciu, Veronica Ballerini, Danielle Braun, Falco J. Bargagli Stoffi</dc:creator>
    </item>
    <item>
      <title>Multi-source Learning for Target Population by High-dimensional Calibration</title>
      <link>https://arxiv.org/abs/2512.04412</link>
      <description>arXiv:2512.04412v2 Announce Type: replace 
Abstract: Multi-source learning is an emerging area of research in statistics, where information from multiple datasets with heterogeneous distributions is combined to estimate the parameter of interest for a target population without observed responses. We propose a high-dimensional debiased calibration (HDC) method and a multi-source HDC (MHDC) estimator for general estimating equations. The HDC method uses a novel approach to achieve Neyman orthogonality for the target parameter via high-dimensional covariate balancing on an augmented set of covariates. It avoids the augmented inverse probability weighting formulation and leads to an easier optimization algorithm for the target parameter in estimating equations and M-estimation. The proposed MHDC estimator integrates multi-source data while supporting flexible specifications for both density ratio and outcome regression models, achieving multiple robustness against model misspecification. Its asymptotic normality is established, and a specification test is proposed to examine the transferability condition for the multi-source data. Compared to the linear combination of single-source HDC estimators, the MHDC estimator improves efficiency by jointly utilizing all data sources. Through simulation studies, we show that the MHDC estimator accommodates multiple sources and multiple working models effectively and performs better than the existing doubly robust estimators for multi-source learning. An empirical analysis of a meteorological dataset demonstrates the utility of the proposed method in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04412v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxiang Zhan, Jae Kwang Kim, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>Perturbation-based Inference for Extreme Value Index</title>
      <link>https://arxiv.org/abs/2512.08258</link>
      <description>arXiv:2512.08258v2 Announce Type: replace 
Abstract: The extreme value index (EVI) characterizes the tail behavior of a distribution and is crucial for extreme value theory. Inference on the EVI is challenging due to data scarcity in the tail region. We propose a novel method for constructing confidence intervals for the EVI using synthetic exceedances generated via perturbation. Rather than perturbing the entire sample, we add noise to exceedances above a high threshold and apply the generalized Pareto distribution (GPD) approximation. Confidence intervals are derived by simulating the distribution of pivotal statistics from the perturbed data. We show that the pivotal statistic is consistent, ensuring the proposed method provides consistent intervals for the EVI. Additionally, we demonstrate that the perturbed data is differentially private. When the GPD approximation is inadequate, we introduce a refined perturbation method. Simulation results show that our approach outperforms existing methods, providing robust and reliable inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08258v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiwei Tang, Judy Huixia Wang, Deyuan Li</dc:creator>
    </item>
    <item>
      <title>Approaches to biological species delimitation based on genetic and spatial dissimilarity</title>
      <link>https://arxiv.org/abs/2401.12126</link>
      <description>arXiv:2401.12126v5 Announce Type: replace-cross 
Abstract: The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. We investigate the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Existing methods such as the partial Mantel test and jackknife-based distance-distance regression are considered. New approaches, i.e., an adaptation of a mixed effects model, a bootstrap approach, and a jackknife version of partial Mantel, are proposed. All these methods address the issue that distance data violate the independence assumption for standard inference regarding correlation and regression; a standard linear regression is also considered. The approaches are compared on simulated meta-populations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Simulations show that the new jackknife version of the partial Mantel test provides a good compromise between power and respecting the nominal type I error rate. Mixed-effects models have larger power than jackknife-based methods, but tend to display type I error rates slightly above the significance level. An application on brassy ringlets concludes the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12126v5</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele d'Angella, Christian Hennig</dc:creator>
    </item>
    <item>
      <title>Linear Regression Using Principal Components from General Hilbert-Space-Valued Covariates</title>
      <link>https://arxiv.org/abs/2504.16780</link>
      <description>arXiv:2504.16780v2 Announce Type: replace-cross 
Abstract: We present a new method of linear regression based on principal components using Hilbert-space-valued covariates. We develop a computationally efficient approach to estimation and derive asymptotic theory for the regression parameter estimates under mild assumptions. We demonstrate the approach in simulation studies as well as in data analysis using two-dimensional brain images as predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16780v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Li, Margaret Hoch, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Differentially Private Stochastic Gradient Descent</title>
      <link>https://arxiv.org/abs/2507.20560</link>
      <description>arXiv:2507.20560v2 Announce Type: replace-cross 
Abstract: Privacy preservation in machine learning, particularly through Differentially Private Stochastic Gradient Descent (DP-SGD), is critical for sensitive data analysis. However, existing statistical inference methods for SGD predominantly focus on cyclic subsampling, while DP-SGD requires randomized subsampling. This paper first bridges this gap by establishing the asymptotic properties of SGD under the randomized rule and extending these results to DP-SGD. For the output of DP-SGD, we show that the asymptotic variance decomposes into statistical, sampling, and privacy-induced components. Two methods are proposed for constructing valid confidence intervals: the plug-in method and the random scaling method. We also perform extensive numerical analysis, which shows that the proposed confidence intervals achieve nominal coverage rates while maintaining privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20560v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xintao Xia, Linjun Zhang, Zhanrui Cai</dc:creator>
    </item>
    <item>
      <title>Stopping Rules for Monte Carlo Methods of Martingale Difference Type</title>
      <link>https://arxiv.org/abs/2510.22690</link>
      <description>arXiv:2510.22690v2 Announce Type: replace-cross 
Abstract: We establish a practical and easy-to-implement sequential stopping rule for the martingale central limit theorem, focusing on Monte Carlo methods for estimating the mean of a non-iid sequence of martingale difference type. Starting with an impractical scheme based on the standard martingale central limit theorem, we progressively address its limitations from implementation perspectives in the non-asymptotic regime. Along the way, we compare the proposed schemes with their counterparts in the asymptotic regime. The developed framework has potential applications in various domains. Numerical results are provided to demonstrate the effectiveness of the developed stopping rules in terms of reliability and complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22690v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiezhong Wu, Reiichiro Kawai</dc:creator>
    </item>
    <item>
      <title>Two Datasets Are Better Than One: Method of Double Moments for 3-D Reconstruction in Cryo-EM</title>
      <link>https://arxiv.org/abs/2511.07438</link>
      <description>arXiv:2511.07438v2 Announce Type: replace-cross 
Abstract: Cryo-electron microscopy (cryo-EM) is a powerful imaging technique for reconstructing three-dimensional molecular structures from noisy tomographic projection images of randomly oriented particles. We introduce a new data fusion framework, termed the method of double moments (MoDM), which reconstructs molecular structures from two instances of the second-order moment of projection images obtained under distinct orientation distributions: one uniform, the other non-uniform and unknown. We prove that these moments generically uniquely determine the underlying structure, up to a global rotation and reflection, and we develop a convex-relaxation-based algorithm that achieves accurate recovery using only second-order statistics. Our results demonstrate the advantage of collecting and modeling multiple datasets under different experimental conditions, illustrating that leveraging dataset diversity can substantially enhance reconstruction quality in computational imaging tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07438v2</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joe Kileel, Oscar Mickelin, Amit Singer, Sheng Xu</dc:creator>
    </item>
  </channel>
</rss>
