<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Oct 2024 04:01:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Lipschitz-free Projected Subgradient Method with Time-varying Step-size</title>
      <link>https://arxiv.org/abs/2410.22336</link>
      <description>arXiv:2410.22336v1 Announce Type: new 
Abstract: We introduce a novel time-varying step-size for the classical projected subgradient method, offering optimal ergodic convergence. Importantly, this approach does not depend on the Lipschitz assumption of the objective function, thereby broadening the convergence result of projected subgradient method to non-Lipschitz case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22336v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yong Xia, Yanhao Zhang, Zhihan Zhu</dc:creator>
    </item>
    <item>
      <title>The $s$-Energy and Its Applications</title>
      <link>https://arxiv.org/abs/2410.22341</link>
      <description>arXiv:2410.22341v1 Announce Type: new 
Abstract: Averaging dynamics drives countless processes in physics, biology, engineering, and the social sciences. In recent years, the $s$-energy has emerged as a useful tool for bounding the convergence rates of time-varying averaging systems. We derive new bounds on the $s$-energy, which we use to resolve a number of open questions in the areas of bird flocking, opinion dynamics, and distributed motion coordination. We also use our results to provide a theoretical validation for the idea of the "Overton Window" as an attracting manifold of viable group opinions. Our new bounds on the $s$-energy highlight its dependency on the connectivity of the underlying networks. In this vein, we use the $s$-energy to explain the exponential gap in the convergence rates of stationary and time-varying consensus systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22341v1</guid>
      <category>math.OC</category>
      <category>cs.MA</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernard Chazelle, Kritkorn Karntikoon</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis and optimal control for a friction problem in the linear elastic model</title>
      <link>https://arxiv.org/abs/2410.22356</link>
      <description>arXiv:2410.22356v1 Announce Type: new 
Abstract: This paper investigates, without any regularization procedure, the sensitivity analysis of a mechanical friction problem involving the (nonsmooth) Tresca friction law in the linear elastic model. To this aim a recent methodology based on advanced tools from convex and variational analyses is used. Precisely we express the solution to the so-called Tresca friction problem thanks to the proximal operator associated with the corresponding Tresca friction functional. Then, using an extended version of twice epi-differentiability, we prove the differentiability of the solution to the parameterized Tresca friction problem, characterizing its derivative as the solution to a boundary value problem involving tangential Signorini's unilateral conditions. Finally our result is used to investigate and numerically solve an optimal control problem associated with the Tresca friction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22356v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lo\"ic Bourdin, Fabien Caubet, Aymeric Jacob de Cordemoy</dc:creator>
    </item>
    <item>
      <title>Zeroth-order Stochastic Cubic Newton Method with Low-rank Hessian Estimation</title>
      <link>https://arxiv.org/abs/2410.22357</link>
      <description>arXiv:2410.22357v1 Announce Type: new 
Abstract: This paper focuses on the problem of minimizing a finite-sum loss $ \frac{1}{N}$ $ \sum_{\xi=1}^N f (\mathbf{x}; \xi) $, where only function evaluations of $ f (\cdot; \xi) $ is allowed. For a fixed $ \xi $, which represents a (batch of) training data, the Hessian matrix $ \nabla^2 f (\mathbf{x}; \xi) $ is usually low-rank. We develop a stochastic zeroth-order cubic Newton method for such problems, and prove its efficiency. More specifically, we show that when $ \nabla^2 f (\mathbf{x}; \xi) \in \mathbb{R}^{n\times n } $ is of rank-$r$, $ \mathcal{O}\left(\frac{n}{\eta^{\frac{7}{2}}}\right)+\widetilde{\mathcal{O}}\left(\frac{n^2 r^2 }{\eta^{\frac{5}{2}}}\right) $ function evaluations guarantee a second order $\eta$-stationary point with high probability. This result improves the dependence on dimensionality compared to the existing state-of-the-art. This improvement is achieved via a new Hessian estimation method, which can be efficiently computed by finite-difference operations, and does not require any incoherence assumptions. Numerical experiments are provided to demonstrate the effectiveness of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22357v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Liu, Weibin Peng, Tianyu Wang, Jiajia Yu</dc:creator>
    </item>
    <item>
      <title>Branch-and-bound algorithm for efficient reliability analysis of general coherent systems</title>
      <link>https://arxiv.org/abs/2410.22363</link>
      <description>arXiv:2410.22363v1 Announce Type: new 
Abstract: Branch and bound algorithms have been developed for reliability analysis of coherent systems. They exhibit a set of advantages; in particular, they can find a computationally efficient representation of a system failure or survival event, which can be re-used when the input probability distributions change over time or when new data is available. However, existing branch-and-bound algorithms can handle only a limited set of system performance functions, mostly network connectivity and maximum flow. Furthermore, they run redundant analyses on component vector states whose system state can be inferred from previous analysis results. This study addresses these limitations by proposing branch and bound for reliability analysis of general coherent systems} (BRC) algorithm: an algorithm that automatically finds minimal representations of failure/survival events of general coherent systems. Computational efficiency is attained by dynamically inferring importance of component events from hitherto obtained results. We demonstrate advantages of the BRC method as a real-time risk management tool by application to the Eastern Massachusetts highway benchmark network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22363v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ji-Eun Byun, Hyeuk Ryu, Daniel Straub</dc:creator>
    </item>
    <item>
      <title>Scalable computation of input-normal/output-diagonal balanced realization for control-affine polynomial systems</title>
      <link>https://arxiv.org/abs/2410.22435</link>
      <description>arXiv:2410.22435v1 Announce Type: new 
Abstract: We present a scalable tensor-based approach to computing input-normal/output-diagonal nonlinear balancing transformations for control-affine systems with polynomial nonlinearities. This transformation is necessary to determine the states that can be truncated when forming a reduced-order model. Given a polynomial representation for the controllability and observability energy functions, we derive the explicit equations to compute a polynomial transformation to induce input-normal/output-diagonal structure in the energy functions in the transformed coordinates. The transformation is computed degree-by-degree, similar to previous Taylor-series approaches in the literature. However, unlike previous works, we provide a detailed analysis of the transformation equations in Kronecker product form to enable a scalable implementation. We derive the explicit algebraic structure for the equations, present rigorous analyses for the solvability and algorithmic complexity of those equations, and provide general purpose open-source software implementations for the proposed algorithms to stimulate broader use of nonlinear balanced truncation model. We demonstrate that with our efficient implementation, computing the nonlinear transformation is approximately as expensive as computing the energy functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22435v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas A. Corbin, Arijit Sarkar, Jacquelien M. A. Scherpen, Boris Kramer</dc:creator>
    </item>
    <item>
      <title>Continuous-Time Line-of-Sight Constrained Trajectory Planning for 6-Degree of Freedom Systems</title>
      <link>https://arxiv.org/abs/2410.22596</link>
      <description>arXiv:2410.22596v1 Announce Type: new 
Abstract: Perception algorithms are ubiquitous in modern autonomy stacks, providing necessary environmental information to operate in the real world. Many of these algorithms depend on the visibility of keypoints, which must remain within the robot's line-of-sight (LoS), for reliable operation. This paper tackles the challenge of maintaining LoS on such keypoints during robot movement. We propose a novel method that addresses these issues by ensuring applicability to various sensor footprints, adaptability to arbitrary nonlinear dynamics, and constant enforcement of LoS throughout the robot's path. Through our experiments, we show that the proposed approach achieves significantly reduced LoS violation and runtime compared to existing state-of-the-art methods in several representative and challenging scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22596v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher R. Hayner, John M. Carson III, Beh\c{c}et A\c{c}{\i}kme\c{s}e, Karen Leung</dc:creator>
    </item>
    <item>
      <title>Inverse of the Gomory Corner Relaxation of Integer Programs</title>
      <link>https://arxiv.org/abs/2410.22653</link>
      <description>arXiv:2410.22653v1 Announce Type: new 
Abstract: We analyze the inverse of the Gomory corner relaxation (GCR) of a pure integer program (IP). We prove the inverse GCR is equivalent to the inverse of a shortest path problem, yielding a polyhedral representation of the GCR inverse-feasible region. We present a linear programming (LP) formulation for solving the inverse GCR under the $L_{1}$ and $L_{\infty}$ norms, with significantly fewer variables and constraints than existing LP formulations for solving the inverse IP in literature. We show that the inverse GCR bounds the inverse IP optimal value as tightly as the inverse LP relaxation under mild conditions. We provide sufficient conditions for the inverse GCR to exactly solve the inverse IP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22653v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fatemeh Nosrat, George Lyu, Andrew J. Schaefer</dc:creator>
    </item>
    <item>
      <title>Inexact Augmented Lagrangian Methods for Conic Programs: Quadratic Growth and Linear Convergence</title>
      <link>https://arxiv.org/abs/2410.22683</link>
      <description>arXiv:2410.22683v1 Announce Type: new 
Abstract: Augmented Lagrangian Methods (ALMs) are widely employed in solving constrained optimizations, and some efficient solvers are developed based on this framework. Under the quadratic growth assumption, it is known that the dual iterates and the Karush-Kuhn-Tucker (KKT) residuals of ALMs applied to semidefinite programs (SDPs) converge linearly. In contrast, the convergence rate of the primal iterates has remained elusive. In this paper, we resolve this challenge by establishing new $\textit{quadratic growth}$ and $\textit{error bound}$ properties for primal and dual SDPs under the strict complementarity condition. Our main results reveal that both primal and dual iterates of the ALMs converge linearly contingent solely upon the assumption of strict complementarity and a bounded solution set. This finding provides a positive answer to an open question regarding the asymptotically linear convergence of the primal iterates of ALMs applied to semidefinite optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22683v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feng-Yi Liao, Lijun Ding, Yang Zheng</dc:creator>
    </item>
    <item>
      <title>Optimality of Linear Policies for Distributionally Robust Linear Quadratic Gaussian Regulator with Stationary Distributions</title>
      <link>https://arxiv.org/abs/2410.22826</link>
      <description>arXiv:2410.22826v1 Announce Type: new 
Abstract: We prove that linear policies remain optimal for solving the LQG regulation problem against the worst-case process and measurement noise distributions, when these are independent, stationary, and known to be within a radius (in the Wasserstein sense) to some reference gaussian noise distributions. When the reference noise distributions are not gaussian, we provide a closed form solution for the worst-case distribution. Our main result suggests a computational complexity that scales only with the dimensionality of the system, and provides a less conservative alternative to recent work in distributionally robust control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22826v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lanzetti Nicolas, Terpin Antonio, D\"orfler Florian</dc:creator>
    </item>
    <item>
      <title>Necessary conditions for a minimum in variational problems with delay in the presence of degeneracies</title>
      <link>https://arxiv.org/abs/2410.22885</link>
      <description>arXiv:2410.22885v1 Announce Type: new 
Abstract: This article explores minimum of an extremal in the variational problem with delay under the degeneracy of the Weierstrass condition. Here for study the minimality of extremal, variations of the Weierstrass type are used in two forms: in the form of variations on the right with respect to the given point, and in the form of variations on the left with respect to the same point. Further, using these variations, formulas for the increments of the functional are obtained. The exploring of the minimality of the extremal with the help of these formulas is conducted under the assumption that the Weierstrass condition degenerates. As a result, considering different forms of degenerations (degeneracy of the Weieristrass condition at a single point and at points of a certain interval), we obtain the necessary conditions of the inequality type and the equality type for a strong and weak local minimum. A specific example is given to demonstrate the effectiveness of the results obtained in this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22885v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. J. Mardanov, T. K. Melikov, G. V. Hajiyeva</dc:creator>
    </item>
    <item>
      <title>Regularity and stability for the Gibbs conditioning principle on path space via McKean-Vlasov control</title>
      <link>https://arxiv.org/abs/2410.23016</link>
      <description>arXiv:2410.23016v1 Announce Type: new 
Abstract: We consider a system of diffusion processes interacting through their empirical distribution. Assuming that the empirical average of a given observable can be observed at any time, we derive regularity and quantitative stability results for the optimal solutions in the associated version of the Gibbs conditioning principle. The proofs rely on the analysis of a McKean-Vlasov control problem with distributional constraints. Some new estimates are derived for Hamilton-Jacobi-Bellman equations and the Hessian of the log-density of diffusion processes, which are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23016v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis-Pierre Chaintron (DMA), Giovanni Conforti</dc:creator>
    </item>
    <item>
      <title>Beyond hypergraph acyclicity: limits of tractability for pseudo-Boolean optimization</title>
      <link>https://arxiv.org/abs/2410.23045</link>
      <description>arXiv:2410.23045v1 Announce Type: new 
Abstract: In this paper, we study the problem of minimizing a polynomial function with literals over all binary points, often referred to as pseudo-Boolean optimization. We investigate the fundamental limits of computation for this problem by providing new necessary conditions and sufficient conditions for tractability. On one hand, we obtain the first intractability results for pseudo-Boolean optimization problems on signed hypergraphs with bounded rank, in terms of the treewidth of the intersection graph. On the other hand, we introduce the nest-set gap, a new hypergraph-theoretic notion that enables us to move beyond hypergraph acyclicity, and obtain a polynomial-size extended formulation for the pseudo-Boolean polytope of a class of signed hypergraphs whose underlying hypergraphs contain beta-cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23045v1</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Del Pia, Aida Khajavirad</dc:creator>
    </item>
    <item>
      <title>Strongly quasiconvex functions: what we know (so far)</title>
      <link>https://arxiv.org/abs/2410.23055</link>
      <description>arXiv:2410.23055v1 Announce Type: new 
Abstract: Introduced by Polyak in 1966, the class of strongly quasiconvex functions includes some interesting nonconvex members, like the square root of the Euclidean norm or ratios with a nonnegative strongly convex numerator and a concave and positive denominator. This survey collects the vast majority of the results involving strongly quasiconvex functions available in the literature at the moment, presenting, in particular, algorithms for minimizing such functions, and suggests some directions where additional investigations would be welcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23055v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sorin-Mihai Grad, Felipe Lara, Ra\'ul T. Marcavillaca</dc:creator>
    </item>
    <item>
      <title>A template for gradient norm minimization</title>
      <link>https://arxiv.org/abs/2410.23135</link>
      <description>arXiv:2410.23135v1 Announce Type: new 
Abstract: The gradient mapping norm is a strong and easily verifiable stopping criterion for first-order methods on composite problems. When the objective exhibits the quadratic growth property, the gradient mapping norm minimization problem can be solved by online parameter-free and adaptive first-order schemes with near-optimal worst-case rates. In this work we address problems where quadratic growth is absent, a class for which no methods with all the aforementioned properties are known to exist. We formulate a template whose instantiation recovers the existing Performance Estimation derived approaches. Our framework provides a simple human-readable interpretation along with runtime convergence rates for these algorithms. Moreover, our template can be used to construct a quasi-online parameter-free method applicable to the entire class of composite problems while retaining the optimal worst-case rates with the best known proportionality constant. The analysis also allows for adaptivity. Preliminary simulation results suggest that our scheme is highly competitive in practice with the existing approaches, either obtained via Performance Estimation or employing Accumulative Regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23135v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mihai I. Florea</dc:creator>
    </item>
    <item>
      <title>Identifiability of the Optimal Transport Cost on Finite Spaces</title>
      <link>https://arxiv.org/abs/2410.23146</link>
      <description>arXiv:2410.23146v1 Announce Type: new 
Abstract: The goal of optimal transport (OT) is to find optimal assignments or matchings between data sets which minimize the total cost for a given cost function. However, sometimes the cost function is unknown but we have access to (parts of) the solution to the OT problem, e.g. the OT plan or the value of the objective function. Recovering the cost from such information is called inverse OT and has become recently of certain interest triggered by novel applications, e.g. in social science and economics. This raises the issue under which circumstances such cost is identifiable, i.e., it can be uniquely recovered from other OT quantities. In this work we provide sufficient and necessary conditions for the identifiability of the cost function on finite ground spaces. We find that such conditions correspond to the combinatorial structure of the corresponding linear program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23146v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Gonz\'alez-Sanz, Michel Groppe, Axel Munk</dc:creator>
    </item>
    <item>
      <title>Performative Control for Linear Dynamical Systems</title>
      <link>https://arxiv.org/abs/2410.23251</link>
      <description>arXiv:2410.23251v1 Announce Type: new 
Abstract: We introduce the framework of performative control, where the policy chosen by the controller affects the underlying dynamics of the control system. This results in a sequence of policy-dependent system state data with policy-dependent temporal correlations. Following the recent literature on performative prediction [21], we introduce the concept of a performatively stable control (PSC) solution. We first propose a sufficient condition for the performative control problem to admit a unique PSC solution with a problem-specific structure of distributional sensitivity propagation and aggregation. We further analyze the impacts of system stability on the existence of the PSC solution. Specifically, for almost surely strongly stable policy-dependent dynamics, the PSC solution exists if the sum of the distributional sensitivities is small enough. However, for almost surely unstable policy-dependent dynamics, the existence of the PSC solution will necessitate a temporally backward decaying of the distributional sensitivities. We finally provide a repeated stochastic gradient descent scheme that converges to the PSC solution and analyze its non-asymptotic convergence rate. Numerical results validate our theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23251v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Songfu Cai, Fei Han, Xuanyu Cao</dc:creator>
    </item>
    <item>
      <title>Vertical Federated Learning with Missing Features During Training and Inference</title>
      <link>https://arxiv.org/abs/2410.22564</link>
      <description>arXiv:2410.22564v1 Announce Type: cross 
Abstract: Vertical federated learning trains models from feature-partitioned datasets across multiple clients, who collaborate without sharing their local data. Standard approaches assume that all feature partitions are available during both training and inference. Yet, in practice, this assumption rarely holds, as for many samples only a subset of the clients observe their partition. However, not utilizing incomplete samples during training harms generalization, and not supporting them during inference limits the utility of the model. Moreover, if any client leaves the federation after training, its partition becomes unavailable, rendering the learned model unusable. Missing feature blocks are therefore a key challenge limiting the applicability of vertical federated learning in real-world scenarios. To address this, we propose LASER-VFL, a vertical federated learning method for efficient training and inference of split neural network-based models that is capable of handling arbitrary sets of partitions. Our approach is simple yet effective, relying on the strategic sharing of model parameters and on task-sampling to train a family of predictors. We show that LASER-VFL achieves a $\mathcal{O}({1}/{\sqrt{T}})$ convergence rate for nonconvex objectives in general, $\mathcal{O}({1}/{T})$ for sufficiently large batch sizes, and linear convergence under the Polyak-{\L}ojasiewicz inequality. Numerical experiments show improved performance of LASER-VFL over the baselines. Remarkably, this is the case even in the absence of missing features. For example, for CIFAR-100, we see an improvement in accuracy of $21.4\%$ when each of four feature blocks is observed with a probability of 0.5 and of $12.2\%$ when all features are observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22564v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Valdeira, Shiqiang Wang, Yuejie Chi</dc:creator>
    </item>
    <item>
      <title>Solving Minimum-Cost Reach Avoid using Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2410.22600</link>
      <description>arXiv:2410.22600v1 Announce Type: cross 
Abstract: Current reinforcement-learning methods are unable to directly learn policies that solve the minimum cost reach-avoid problem to minimize cumulative costs subject to the constraints of reaching the goal and avoiding unsafe states, as the structure of this new optimization problem is incompatible with current methods. Instead, a surrogate problem is solved where all objectives are combined with a weighted sum. However, this surrogate objective results in suboptimal policies that do not directly minimize the cumulative cost. In this work, we propose RC-PPO, a reinforcement-learning-based method for solving the minimum-cost reach-avoid problem by using connections to Hamilton-Jacobi reachability. Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to 57% lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator. The project page can be found at https://oswinso.xyz/rcppo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22600v1</guid>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oswin So, Cheng Ge, Chuchu Fan</dc:creator>
    </item>
    <item>
      <title>An Iterative Algorithm for Regularized Non-negative Matrix Factorizations</title>
      <link>https://arxiv.org/abs/2410.22698</link>
      <description>arXiv:2410.22698v1 Announce Type: cross 
Abstract: We generalize the non-negative matrix factorization algorithm of Lee and Seung to accept a weighted norm, and to support ridge and Lasso regularization. We recast the Lee and Seung multiplicative update as an additive update which does not get stuck on zero values. We apply the companion R package rnnmf to the problem of finding a reduced rank representation of a database of cocktails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22698v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Steven E. Pav</dc:creator>
    </item>
    <item>
      <title>Dynamic Matching with Post-allocation Service and its Application to Refugee Resettlement</title>
      <link>https://arxiv.org/abs/2410.22992</link>
      <description>arXiv:2410.22992v1 Announce Type: cross 
Abstract: Motivated by our collaboration with a major refugee resettlement agency in the U.S., we study a dynamic matching problem where each new arrival (a refugee case) must be matched immediately and irrevocably to one of the static resources (a location with a fixed annual quota). In addition to consuming the static resource, each case requires post-allocation service from a server, such as a translator. Given the time-consuming nature of service, a server may not be available at a given time, thus we refer to it as a dynamic resource. Upon matching, the case will wait to avail service in a first-come-first-serve manner. Bursty matching to a location may result in undesirable congestion at its corresponding server. Consequently, the central planner (the agency) faces a dynamic matching problem with an objective that combines the matching reward (captured by pair-specific employment outcomes) with the cost for congestion for dynamic resources and over-allocation for the static ones. Motivated by the observed fluctuations in the composition of refugee pools across the years, we design algorithms that do not rely on distributional knowledge constructed based on past years' data. To that end, we develop learning-based algorithms that are asymptotically optimal in certain regimes, easy to interpret, and computationally fast. Our design is based on learning the dual variables of the underlying optimization problem; however, the main challenge lies in the time-varying nature of the dual variables associated with dynamic resources. To overcome this challenge, our theoretical development brings together techniques from Lyapunov analysis, adversarial online learning, and stochastic optimization. On the application side, when tested on real data from our partner agency, our method outperforms existing ones making it a viable candidate for replacing the current practice upon experimentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22992v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirk Bansak, Soonbong Lee, Vahideh Manshadi, Rad Niazadeh, Elisabeth Paulson</dc:creator>
    </item>
    <item>
      <title>Accurate Solutions to Optimal Control Problems via a Flexible Mesh and Integrated Residual Transcription</title>
      <link>https://arxiv.org/abs/2410.23037</link>
      <description>arXiv:2410.23037v1 Announce Type: cross 
Abstract: We propose joining a flexible mesh design with an integrated residual transcription in order to improve the accuracy of numerical solutions to optimal control problems. This approach is particularly useful when state or input trajectories are non-smooth, but it may also be beneficial when dynamics constraints are stiff. Additionally, we implement an initial phase that will ensure a feasible solution is found and can be implemented immediately in real-time controllers. Subsequent iterations with warm-starting will improve the solution until optimality is achieved. Optimizing over the mesh node locations allows for discontinuities to be captured exactly, while integrated residuals account for the approximation error in-between the nodal points. First, we numerically show the improved convergence order for the flexible mesh. We then present the feasibility-driven approach to solve control problems and show how flexible meshing and integrated residual methods can be used in practice. The presented numerical examples demonstrate for the first time the numerical implementation of a flexible mesh for an integrated residual transcription. The results show that our proposed method can be more than two times more accurate than conventional fixed mesh collocation for the same computational time and more than three times more accurate for the same problem size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23037v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucian Nita, Eric C. Kerrigan</dc:creator>
    </item>
    <item>
      <title>Method of Moments for Estimation of Noisy Curves</title>
      <link>https://arxiv.org/abs/2410.23220</link>
      <description>arXiv:2410.23220v1 Announce Type: cross 
Abstract: In this paper, we study the problem of recovering a ground truth high dimensional piecewise linear curve $C^*(t):[0, 1]\to\mathbb{R}^d$ from a high noise Gaussian point cloud with covariance $\sigma^2I$ centered around the curve. We establish that the sample complexity of recovering $C^*$ from data scales with order at least $\sigma^6$. We then show that recovery of a piecewise linear curve from the third moment is locally well-posed, and hence $O(\sigma^6)$ samples is also sufficient for recovery. We propose methods to recover a curve from data based on a fitting to the third moment tensor with a careful initialization strategy and conduct some numerical experiments verifying the ability of our methods to recover curves. All code for our numerical experiments is publicly available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23220v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.TH</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Phillip Lo, Yuehaw Khoo</dc:creator>
    </item>
    <item>
      <title>Generalized Short Path Algorithms: Towards Super-Quadratic Speedup over Markov Chain Search for Combinatorial Optimization</title>
      <link>https://arxiv.org/abs/2410.23270</link>
      <description>arXiv:2410.23270v1 Announce Type: cross 
Abstract: We analyze generalizations of algorithms based on the short-path framework first proposed by Hastings [Quantum 2, 78 (2018)], which has been extended and shown by Dalzell et al. [STOC '22] to achieve super-Grover speedups for certain binary optimization problems. We demonstrate that, under some commonly satisfied technical conditions, an appropriate generalization can achieve super-quadratic speedups not only over unstructured search but also over a classical optimization algorithm that searches for the optimum by drawing samples from the stationary distribution of a Markov Chain. We employ this framework to obtain algorithms for problems including variants of Max-Bisection, Max Independent Set, the Ising Model, and the Sherrington Kirkpatrick Model, whose runtimes are asymptotically faster than those obtainable from previous short path techniques. For random regular graphs of sufficiently high degree, our algorithm is super-quadratically faster than the best rigorously proven classical runtimes for regular graphs. Our results also shed light on the quantum nature of short path algorithms, by identifying a setting where our algorithm is super-quadratically faster than any polynomial time Gibbs sampler, unless NP = RP. We conclude the paper with a numerical analysis that guides the choice of parameters for short path algorithms and raises the possibility of super-quadratic speedups in settings that are currently beyond our theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23270v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shouvanik Chakrabarti, Dylan Herman, Guneykan Ozgul, Shuchen Zhu, Brandon Augustino, Tianyi Hao, Zichang He, Ruslan Shaydulin, Marco Pistoia</dc:creator>
    </item>
    <item>
      <title>Provable acceleration for diffusion models under minimal assumptions</title>
      <link>https://arxiv.org/abs/2410.23285</link>
      <description>arXiv:2410.23285v1 Announce Type: cross 
Abstract: While score-based diffusion models have achieved exceptional sampling quality, their sampling speeds are often limited by the high computational burden of score function evaluations. Despite the recent remarkable empirical advances in speeding up the score-based samplers, theoretical understanding of acceleration techniques remains largely limited. To bridge this gap, we propose a novel training-free acceleration scheme for stochastic samplers. Under minimal assumptions -- namely, $L^2$-accurate score estimates and a finite second-moment condition on the target distribution -- our accelerated sampler provably achieves $\varepsilon$-accuracy in total variation within $\widetilde{O}(d^{5/4}/\sqrt{\varepsilon})$ iterations, thereby significantly improving upon the $\widetilde{O}(d/\varepsilon)$ iteration complexity of standard score-based samplers. Notably, our convergence theory does not rely on restrictive assumptions on the target distribution or higher-order score estimation guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23285v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gen Li, Changxiao Cai</dc:creator>
    </item>
    <item>
      <title>AN-SPS: Adaptive Sample Size Nonmonotone Line Search Spectral Projected Subgradient Method for Convex Constrained Optimization Problems</title>
      <link>https://arxiv.org/abs/2208.10616</link>
      <description>arXiv:2208.10616v4 Announce Type: replace 
Abstract: We consider convex optimization problems with a possibly nonsmooth objective function in the form of a mathematical expectation. The proposed framework (AN-SPS) employs Sample Average Approximations (SAA) to approximate the objective function, which is either unavailable or too costly to compute. The sample size is chosen in an adaptive manner, which eventually pushes the SAA error to zero almost surely (a.s.). The search direction is based on a scaled subgradient and a spectral coefficient, both related to the SAA function. The step size is obtained via a nonmonotone line search over a predefined interval, which yields a theoretically sound and practically efficient algorithm. The method retains feasibility by projecting the resulting points onto a feasible set. The a.s. convergence of AN-SPS method is proved without the assumption of a bounded feasible set or bounded iterates. Preliminary numerical results on Hinge loss problems reveal the advantages of the proposed adaptive scheme. In addition, a study of different nonmonotone line search strategies in combination with different spectral coefficients within AN-SPS framework is also conducted, yielding some hints for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.10616v4</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nata\v{s}a Krklec Jerinki\'c, Tijana Ostoji\'c</dc:creator>
    </item>
    <item>
      <title>The Benefit of Uncertainty Coupling in Robust and Adaptive Robust Optimization</title>
      <link>https://arxiv.org/abs/2302.10369</link>
      <description>arXiv:2302.10369v2 Announce Type: replace 
Abstract: Despite the modeling power for problems under uncertainty, robust optimization (RO) and adaptive robust optimization (ARO) can exhibit too conservative solutions in terms of objective value degradation compared to the nominal case. One of the main reasons behind this conservatism is that, in many practical applications, uncertain constraints are directly designed as constraint-wise without taking into account couplings over multiple constraints. In this paper, we define a coupled uncertainty set as the intersection between a constraint-wise uncertainty set and a coupling set. We study the benefit of coupling in alleviating conservatism in RO and ARO. We provide theoretical tight and computable upper and lower bounds on the objective value improvement of RO and ARO problems under coupled uncertainty over constraint-wise uncertainty. In addition, we relate the power of adaptability over static solutions with the coupling of uncertainty set. Computational results demonstrate the benefit of coupling in applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10369v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitris Bertsimas, Liangyuan Na, Bartolomeo Stellato, Irina Wang</dc:creator>
    </item>
    <item>
      <title>Online Control with Adversarial Disturbance for Continuous-time Linear Systems</title>
      <link>https://arxiv.org/abs/2306.01952</link>
      <description>arXiv:2306.01952v4 Announce Type: replace 
Abstract: We study online control for continuous-time linear systems with finite sampling rates, where the objective is to design an online procedure that learns under non-stochastic noise and performs comparably to a fixed optimal linear controller. We present a novel two-level online algorithm, by integrating a higher-level learning strategy and a lower-level feedback control strategy. This method offers a practical and robust solution for online control, which achieves sublinear regret. Our work provides the first nonasymptotic results for controlling continuous-time linear systems with finite number of interactions with the system. Moreover, we examine how to train an agent in domain randomization environments from a non-stochastic control perspective. By applying our method to the SAC (Soft Actor-Critic) algorithm, we achieved improved results in multiple reinforcement learning tasks within domain randomization environments. Our work provides new insights into non-asymptotic analyses of controlling continuous-time systems. Furthermore, our work brings practical intuition into controller learning under non-stochastic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01952v4</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingwei Li, Jing Dong, Can Chang, Baoxiang Wang, Jingzhao Zhang</dc:creator>
    </item>
    <item>
      <title>Mixed variable structural optimization using mixed variable system Monte Carlo tree search formulation</title>
      <link>https://arxiv.org/abs/2309.14231</link>
      <description>arXiv:2309.14231v2 Announce Type: replace 
Abstract: A novel method called mixed variable system Monte Carlo tree search (MVSMCTS) formulation is presented for optimization problems considering various types of variables with single and mixed continuous-discrete system. This method utilizes a reinforcement learning algorithm with improved Monte Carlo tree search (IMCTS) formulation. For sizing and shape optimization of truss structures, the design variables are the cross-sectional areas of the members and the nodal coordinates of the joints. MVSMCTS incorporates update process and accelerating technique for continuous variable and combined scheme for single and mixed system. Update process indicates that once a solution is determined by MCTS with automatic mesh generation in continuous space, it is used as the initial solution for next search tree. The search region should be expanded from the mid-point, which is the design variable for initial state. Accelerating technique is developed by decreasing the range of search region and the width of search tree based on the number of meshes during update process. Combined scheme means that various types of variables are coupled in only one search tree. Through several examples, it is demonstrated that this framework is suitable for mixed variable structural optimization. Moreover, the agent can find optimal solution in a reasonable time, stably generates an optimal design, and is applicable for practical engineering problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14231v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fu-Yao Ko, Katsuyuki Suzuki, Kazuo Yonekura</dc:creator>
    </item>
    <item>
      <title>Higher-order Lie bracket approximation and averaging of control-affine systems with application to extremum seeking</title>
      <link>https://arxiv.org/abs/2310.07092</link>
      <description>arXiv:2310.07092v5 Announce Type: replace 
Abstract: This paper provides a rigorous derivation for what is known in the literature as the Lie bracket approximation of control-affine systems in a more general and sequential framework for higher-orders. In fact, by using chronological calculus, we show that said Lie bracket approximations can be derived, and considered, as higher-order averaging terms. Hence, the theory provided in this paper unifies both averaging and approximation theories of control-affine systems. In particular, the Lie bracket approximation of order ($n$) turns out to be a higher-order averaging of order ($n+1$). The derivation and formulation provided in this paper can be directly reduced to the first and second-order Lie bracket approximations available in the literature. However, we do not need to make many of the unproven assumptions provided in the literature and show that they are in fact natural corollaries from our work. Moreover, we use our results to show that important and useful information about control-affine extremum seeking systems can be obtained and used for significant performance improvement, including a faster convergence rate influenced by higher-order derivatives. We provide multiple numerical simulations to demonstrate both the conceptual elements of this work as well as the significance of our results on extremum seeking with comparison against the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07092v5</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sameer Pokhrel, Sameh A. Eisa</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of Non-Strongly-Monotone Stochastic Quasi-Variational Inequalities</title>
      <link>https://arxiv.org/abs/2401.03076</link>
      <description>arXiv:2401.03076v2 Announce Type: replace 
Abstract: While Variational Inequality (VI) is a well-established mathematical framework that subsumes Nash equilibrium and saddle-point problems, less is known about its extension, Quasi-Variational Inequalities (QVI). QVI allows for cases where the constraint set changes as the decision variable varies allowing for a more versatile setting. In this paper, we propose extra-gradient and gradient-based methods for solving a class of monotone Stochastic Quasi-Variational Inequalities (SQVI) and establish a rigorous convergence rate analysis for these methods. Our approach not only advances the theoretical understanding of SQVI but also demonstrates its practical applicability. Specifically, we highlight its effectiveness in reformulating and solving problems such as generalized Nash Equilibrium, bilevel optimization, and saddle-point problems with coupling constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03076v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeinab Alizadeh, Afrooz Jalilzadeh</dc:creator>
    </item>
    <item>
      <title>On the controllability of nonlinear systems with a periodic drift</title>
      <link>https://arxiv.org/abs/2403.04718</link>
      <description>arXiv:2403.04718v2 Announce Type: replace 
Abstract: Sufficient and necessary conditions are established for controllability of affine control systems where the control is constrained to a set whose convex hull contains the origin but is not necessarily, in contrast with previously known results, a neighborhood of the origin. Part of the results, in particular these on global controllability, are specific to systems where all solutions with zero control (drift) are periodic. These conditions are expressed by means of pushforwards along the flow of the drift, rather than in terms of Lie brackets; it turns out that they amount to local controllability of a time-varying linear approximation with constrained controls. Global and local results are given, as well as a few illustrative examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04718v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Baptiste Caillau (Universit\'e C\^ote d'Azur, Inria, CNRS), Lamberto Dell'Elce (Universit\'e C\^ote d'Azur, Inria, CNRS), Alesia Herasimenka (Universit\'e C\^ote d'Azur, Inria, CNRS), Jean-Baptiste Pomet (Universit\'e C\^ote d'Azur, Inria, CNRS)</dc:creator>
    </item>
    <item>
      <title>Generalized Dimension Reduction Using Semi-Relaxed Gromov-Wasserstein Distance</title>
      <link>https://arxiv.org/abs/2405.15959</link>
      <description>arXiv:2405.15959v2 Announce Type: replace 
Abstract: Dimension reduction techniques typically seek an embedding of a high-dimensional point cloud into a low-dimensional Euclidean space which optimally preserves the geometry of the input data. Based on expert knowledge, one may instead wish to embed the data into some other manifold or metric space in order to better reflect the geometry or topology of the point cloud. We propose a general method for manifold-valued multidimensional scaling based on concepts from optimal transport. In particular, we establish theoretical connections between the recently introduced semi-relaxed Gromov-Wasserstein (srGW) framework and multidimensional scaling by solving the Monge problem in this setting. We also derive novel connections between srGW distance and Gromov-Hausdorff distance. We apply our computational framework to analyze ensembles of political redistricting plans for states with two Congressional districts, achieving an effective visualization of the ensemble as a distribution on a circle which can be used to characterize typical neutral plans, and to flag outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15959v2</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ranthony A. Clark, Tom Needham, Thomas Weighill</dc:creator>
    </item>
    <item>
      <title>Quadratic optimal transportation problem with a positive semi definite structure on the cost function</title>
      <link>https://arxiv.org/abs/2408.05161</link>
      <description>arXiv:2408.05161v2 Announce Type: replace 
Abstract: Optimal transportation problem seeks for a coupling $\pi$ of two probability measures $\mu$ and $\nu$ which minimize the total cost $\int c d\pi$, which is linear in $\pi$. In this paper, we introduce a variation of optimal transportation problem which we call quadratic transportation problem that considers a total cost $\iint c d\pi d\pi$ which is quadratic in $\pi$. We compare this problem with other variations of optimal transportation problem, and prove some properties of the solutions to the problem. Then, we introduce squared cost function, which let us consider the total cost $\iint c d\pi d\pi$ as a positive semi-definite bilinear operator on probability measures, and show Kantorovich duality formula when we have a squared cost function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05161v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seonghyeon Jeong</dc:creator>
    </item>
    <item>
      <title>Extensions of $\mathcal{KL}$ and Lyapunov Functions for Discrete-time Dynamical System Peaks Analysis</title>
      <link>https://arxiv.org/abs/2410.04896</link>
      <description>arXiv:2410.04896v2 Announce Type: replace 
Abstract: In this paper, we extend two classes of functions classically involved in asymptotic stability analyses for studying a maximization problem on the reachable values of a discrete-time dynamical system. This maximization problem is called a peaks computation problem. The problem is to find a couple composed of an initial state and a time which maximizes a given function over states. The paper focuses on the time component of the optimal solution which is an integer as the time is discrete. We develop a method to provide an upper bound of this integer from a formula which requires a pair of a strictly increasing and continuous function on [0,1] and a scalar in (0,1). A first result proves that the formula provides, in theory, the optimal integer. However, in practice, the computation cannot be so precise. Then, we develop two alternative methods. The first is based on discontinuous and non strictly increasing/decreasing $\mathcal{KL}$-like functions named $\klgen$ functions. We prove that the existence of a $mathcal{KL}_{\rm gen}$ upper bound is equivalent to the existence of a pair of a strictly increasing and continuous function on [0,1] and a scalar in (0,1). The construction of the strictly increasing continuous function from a $mathcal{KL}_{\rm gen}$ function needs an extension of the famous Sontag's lemma. Finally, we construct a new type of Lyapunov functions, called Opt-Lyapunov functions, well designed for our peaks computation problem. Opt-Lyapunov functions are well designed as we establish an equivalence theorem between the existence of an Opt-Lyapunov function and of a pair of a strictly increasing and continuous function on $[0,1]$ and a scalar in (0,1). The construction of a Opt-Lyapunov function from a pair of a strictly increasing and continuous function on [0,1] and a convergent geometric sequence is insipred by the Yoshizawa construction of Lyapunov functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04896v2</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Assal\'e Adj\'e</dc:creator>
    </item>
    <item>
      <title>MPPI-IPDDP: Hybrid Method of Collision-Free Smooth Trajectory Generation for Autonomous Robots</title>
      <link>https://arxiv.org/abs/2208.02439</link>
      <description>arXiv:2208.02439v2 Announce Type: replace-cross 
Abstract: This paper presents a hybrid trajectory optimization method designed to generate collision-free, smooth trajectories for autonomous mobile robots. By combining sampling-based Model Predictive Path Integral (MPPI) control with gradient-based Interior-Point Differential Dynamic Programming (IPDDP), we leverage their respective strengths in exploration and smoothing. The proposed method, MPPI-IPDDP, involves three steps: First, MPPI control is used to generate a coarse trajectory. Second, a collision-free convex corridor is constructed. Third, IPDDP is applied to smooth the coarse trajectory, utilizing the collision-free corridor from the second step. To demonstrate the effectiveness of our approach, we apply the proposed algorithm to trajectory optimization for differential-drive wheeled mobile robots and point-mass quadrotors. In comparisons with other MPPI variants and continuous optimization-based solvers, our method shows superior performance in terms of computational robustness and trajectory smoothness.
  Code: https://github.com/i-ASL/mppi-ipddp Video: https://youtu.be/-oUAt5sd9Bk</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02439v2</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Min-Gyeom Kim, Minchan Jung, JunGee Hong, Kwang-Ki K. Kim</dc:creator>
    </item>
    <item>
      <title>Block Sparse Bayesian Learning: A Diversified Scheme</title>
      <link>https://arxiv.org/abs/2402.04646</link>
      <description>arXiv:2402.04646v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data. By allowing diversification on intra-block variance and inter-block correlation matrices, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting. Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation. Moreover, we establish the global and local optimality theory of our model. Experiments validate the advantages of DivSBL over existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04646v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanhao Zhang, Zhihan Zhu, Yong Xia</dc:creator>
    </item>
    <item>
      <title>On the stability of gradient descent with second order dynamics for time-varying cost functions</title>
      <link>https://arxiv.org/abs/2405.13765</link>
      <description>arXiv:2405.13765v2 Announce Type: replace-cross 
Abstract: Gradient based optimization algorithms deployed in Machine Learning (ML) applications are often analyzed and compared by their convergence rates or regret bounds. While these rates and bounds convey valuable information they don't always directly translate to stability guarantees. Stability and similar concepts, like robustness, will become ever more important as we move towards deploying models in real-time and safety critical systems. In this work we build upon the results in Gaudio et al. 2021 and Moreu &amp; Annaswamy 2022 for gradient descent with second order dynamics when applied to explicitly time varying cost functions and provide more general stability guarantees. These more general results can aid in the design and certification of these optimization schemes so as to help ensure safe and reliable deployment for real-time learning applications. We also hope that the techniques provided here will stimulate and cross-fertilize the analysis that occurs on the same algorithms from the online learning and stochastic optimization communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13765v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Travis E. Gibson, Sawal Acharya, Anjali Parashar, Joseph E. Gaudio, Anurdha M. Annaswamy</dc:creator>
    </item>
    <item>
      <title>The Road Less Scheduled</title>
      <link>https://arxiv.org/abs/2405.15682</link>
      <description>arXiv:2405.15682v4 Announce Type: replace-cross 
Abstract: Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available at https://github.com/facebookresearch/schedule_free. Schedule-Free AdamW is the core algorithm behind our winning entry to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15682v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Defazio, Xingyu Alice Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, Ashok Cutkosky</dc:creator>
    </item>
    <item>
      <title>Adam with model exponential moving average is effective for nonconvex optimization</title>
      <link>https://arxiv.org/abs/2405.18199</link>
      <description>arXiv:2405.18199v2 Announce Type: replace-cross 
Abstract: In this work, we offer a theoretical analysis of two modern optimization techniques for training large and complex models: (i) adaptive optimization algorithms, such as Adam, and (ii) the model exponential moving average (EMA). Specifically, we demonstrate that a clipped version of Adam with model EMA achieves the optimal convergence rates in various nonconvex optimization settings, both smooth and nonsmooth. Moreover, when the scale varies significantly across different coordinates, we demonstrate that the coordinate-wise adaptivity of Adam is provably advantageous. Notably, unlike previous analyses of Adam, our analysis crucially relies on its core elements -- momentum and discounting factors -- as well as model EMA, motivating their wide applications in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18199v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kwangjun Ahn, Ashok Cutkosky</dc:creator>
    </item>
    <item>
      <title>Improving Generalization and Convergence by Enhancing Implicit Regularization</title>
      <link>https://arxiv.org/abs/2405.20763</link>
      <description>arXiv:2405.20763v3 Announce Type: replace-cross 
Abstract: In this work, we propose an Implicit Regularization Enhancement (IRE) framework to accelerate the discovery of flat solutions in deep learning, thereby improving generalization and convergence. Specifically, IRE decouples the dynamics of flat and sharp directions, which boosts the sharpness reduction along flat directions while maintaining the training stability in sharp directions. We show that IRE can be practically incorporated with {\em generic base optimizers} without introducing significant computational overload. Experiments show that IRE consistently improves the generalization performance for image classification tasks across a variety of benchmark datasets (CIFAR-10/100, ImageNet) and models (ResNets and ViTs). Surprisingly, IRE also achieves a $2\times$ {\em speed-up} compared to AdamW in the pre-training of Llama models (of sizes ranging from 60M to 229M) on datasets including Wikitext-103, Minipile, and Openwebtext. Moreover, we provide theoretical guarantees, showing that IRE can substantially accelerate the convergence towards flat minima in Sharpness-aware Minimization (SAM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20763v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingze Wang, Jinbo Wang, Haotian He, Zilin Wang, Guanhua Huang, Feiyu Xiong, Zhiyu Li, Weinan E, Lei Wu</dc:creator>
    </item>
    <item>
      <title>The Sample-Communication Complexity Trade-off in Federated Q-Learning</title>
      <link>https://arxiv.org/abs/2408.16981</link>
      <description>arXiv:2408.16981v2 Announce Type: replace-cross 
Abstract: We consider the problem of federated Q-learning, where $M$ agents aim to collaboratively learn the optimal Q-function of an unknown infinite-horizon Markov decision process with finite state and action spaces. We investigate the trade-off between sample and communication complexities for the widely used class of intermittent communication algorithms. We first establish the converse result, where it is shown that a federated Q-learning algorithm that offers any speedup with respect to the number of agents in the per-agent sample complexity needs to incur a communication cost of at least an order of $\frac{1}{1-\gamma}$ up to logarithmic factors, where $\gamma$ is the discount factor. We also propose a new algorithm, called Fed-DVR-Q, which is the first federated Q-learning algorithm to simultaneously achieve order-optimal sample and communication complexities. Thus, together these results provide a complete characterization of the sample-communication complexity trade-off in federated Q-learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16981v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sudeep Salgia, Yuejie Chi</dc:creator>
    </item>
    <item>
      <title>Quantum Boltzmann machine learning of ground-state energies</title>
      <link>https://arxiv.org/abs/2410.12935</link>
      <description>arXiv:2410.12935v2 Announce Type: replace-cross 
Abstract: Estimating the ground-state energy of Hamiltonians is a fundamental task for which it is believed that quantum computers can be helpful. Several approaches have been proposed toward this goal, including algorithms based on quantum phase estimation and hybrid quantum-classical optimizers involving parameterized quantum circuits, the latter falling under the umbrella of the variational quantum eigensolver. Here, we analyze the performance of quantum Boltzmann machines for this task, which is a less explored ansatz based on parameterized thermal states and which is not known to suffer from the barren-plateau problem. We delineate a hybrid quantum-classical algorithm for this task and rigorously prove that it converges to an $\varepsilon$-approximate stationary point of the energy function optimized over parameter space, while using a number of parameterized-thermal-state samples that is polynomial in $\varepsilon^{-1}$, the number of parameters, and the norm of the Hamiltonian being optimized. Our algorithm estimates the gradient of the energy function efficiently by means of a novel quantum circuit construction that combines classical sampling, Hamiltonian simulation, and the Hadamard test, thus overcoming a key obstacle to quantum Boltzmann machine learning that has been left open since [Amin et al., Phys. Rev. X 8, 021050 (2018)]. Additionally supporting our main claims are calculations of the gradient and Hessian of the energy function, as well as an upper bound on the matrix elements of the latter that is used in the convergence analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12935v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dhrumil Patel, Daniel Koch, Saahil Patel, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>On a Geometric Interpretation Of the Subset Sum Problem</title>
      <link>https://arxiv.org/abs/2410.19024</link>
      <description>arXiv:2410.19024v2 Announce Type: replace-cross 
Abstract: For $S \in \mathbb{N}^n$ and $T \in \mathbb{N}$, the Subset Sum Problem (SSP) $\exists^? x \in \{0,1\}^n $ such that $S^T\cdot x = T$ can be interpreted as the problem of deciding whether the intersection of the positive unit hypercube $Q_n = [0,1]^n$ with the hyperplane $S^T\cdot \left(x - \frac{S}{\|S\|^2 }\cdot T \right) = 0$ contains at least a vertex. In this paper, we give an algorithm of complexity $\mathcal{O}\left( \frac{1}{\epsilon}\cdot n^b \right)$, for some absolute constant $b$, which either proves that there are no vertices in a slab of thickness $\epsilon$ either finds a vertex in the slab of thickness $4\cdot \epsilon$. It is shown that any vertex $P$ in a slab of thickness $\epsilon$ meets $\left| \frac{S^T\cdot P}{T} - 1 \right| \leq \epsilon$, therefore making the proposed algorithm a FPTAS for the SSP. The results are then applied to the study of the so called Simultaneous Subset-Sum Problem (SSSP).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19024v2</guid>
      <category>cs.CG</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Costandin</dc:creator>
    </item>
    <item>
      <title>Super Gradient Descent: Global Optimization requires Global Gradient</title>
      <link>https://arxiv.org/abs/2410.19706</link>
      <description>arXiv:2410.19706v2 Announce Type: replace-cross 
Abstract: Global minimization is a fundamental challenge in optimization, especially in machine learning, where finding the global minimum of a function directly impacts model performance and convergence. This article introduces a novel optimization method that we called Super Gradient Descent, designed specifically for one-dimensional functions, guaranteeing convergence to the global minimum for any k-Lipschitz function defined on a closed interval [a, b]. Our approach addresses the limitations of traditional optimization algorithms, which often get trapped in local minima. In particular, we introduce the concept of global gradient which offers a robust solution for precise and well-guided global optimization. By focusing on the global minimization problem, this work bridges a critical gap in optimization theory, offering new insights and practical advancements in different optimization problems in particular Machine Learning problems like line search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19706v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seifeddine Achour</dc:creator>
    </item>
  </channel>
</rss>
