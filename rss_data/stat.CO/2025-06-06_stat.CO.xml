<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 04:01:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Amortized variational transdimensional inference</title>
      <link>https://arxiv.org/abs/2506.04749</link>
      <description>arXiv:2506.04749v1 Announce Type: new 
Abstract: The expressiveness of flow-based models combined with stochastic variational inference (SVI) has, in recent years, expanded the application of optimization-based Bayesian inference to include problems with complex data relationships. However, until now, SVI using flow-based models has been limited to problems of fixed dimension. We introduce CoSMIC, normalizing flows (COntextually-Specified Masking for Identity-mapped Components), an extension to neural autoregressive conditional normalizing flow architectures that enables using a single amortized variational density for inference over a transdimensional target distribution. We propose a combined stochastic variational transdimensional inference (VTI) approach to training CoSMIC flows using techniques from Bayesian optimization and Monte Carlo gradient estimation. Numerical experiments demonstrate the performance of VTI on challenging problems that scale to high-cardinality model spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04749v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laurence Davies, Dan Mackinlay, Rafael Oliveira, Scott A. Sisson</dc:creator>
    </item>
    <item>
      <title>Mechanistic models for panel data: Analysis of ecological experiments with four interacting species</title>
      <link>https://arxiv.org/abs/2506.04508</link>
      <description>arXiv:2506.04508v1 Announce Type: cross 
Abstract: In an ecological context, panel data arise when time series measurements are made on a collection of ecological processes. Each process may correspond to a spatial location for field data, or to an experimental ecosystem in a designed experiment. Statistical models for ecological panel data should capture the high levels of nonlinearity, stochasticity, and measurement uncertainty inherent in ecological systems. Furthermore, the system dynamics may depend on unobservable variables. This study applies iterated particle filtering techniques to explore new possibilities for likelihood-based statistical analysis of these complex systems. We analyze data from a mesocosm experiment in which two species of the freshwater planktonic crustacean genus, {\it Daphnia}, coexist with an alga and a fungal parasite. Time series data were collected on replicated mesocosms under six treatment conditions. Iterated filtering enables maximization of the likelihood for scientifically motivated nonlinear partially observed Markov process models, providing access to standard likelihood-based methods for parameter estimation, confidence intervals, hypothesis testing, model selection and diagnostics. This toolbox allows scientists to propose and evaluate scientifically motivated stochastic dynamic models for panel data, constrained only by the requirement to write code to simulate from the model and to specify a measurement distribution describing how the system state is observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04508v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Yang, Jesse Wheeler, Meghan A. Duffy, Aaron A. King, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>Assessing parameter identifiability of a hemodynamics PDE model using spectral surrogates and dimension reduction</title>
      <link>https://arxiv.org/abs/2506.04538</link>
      <description>arXiv:2506.04538v1 Announce Type: cross 
Abstract: Computational inverse problems for biomedical simulators suffer from limited data and relatively high parameter dimensionality. This often requires sensitivity analysis, where parameters of the model are ranked based on their influence on the specific quantities of interest. This is especially important for simulators used to build medical digital twins, as the amount of data is typically limited. For expensive models, such as blood flow models, emulation is employed to expedite the simulation time. Parameter ranking and fixing using sensitivity analysis are often heuristic, though, and vary with the specific application or simulator used. The present study provides an innovative solution to this problem by leveraging polynomial chaos expansions (PCEs) for both multioutput global sensitivity analysis and formal parameter identifiability. For the former, we use dimension reduction to efficiently quantify time-series sensitivity of a one-dimensional pulmonary hemodynamics model. We consider both Windkessel and structured tree boundary conditions. We then use PCEs to construct profile-likelihood confidence intervals to formally assess parameter identifiability, and show how changes in experimental design improve identifiability. Our work presents a novel approach to determining parameter identifiability and leverages a common emulation strategy for enabling profile-likelihood analysis in problems governed by partial differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04538v1</guid>
      <category>q-bio.TO</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mitchel J. Colebank</dc:creator>
    </item>
    <item>
      <title>Unsupervised Machine Learning for Scientific Discovery: Workflow and Best Practices</title>
      <link>https://arxiv.org/abs/2506.04553</link>
      <description>arXiv:2506.04553v1 Announce Type: cross 
Abstract: Unsupervised machine learning is widely used to mine large, unlabeled datasets to make data-driven discoveries in critical domains such as climate science, biomedicine, astronomy, chemistry, and more. However, despite its widespread utilization, there is a lack of standardization in unsupervised learning workflows for making reliable and reproducible scientific discoveries. In this paper, we present a structured workflow for using unsupervised learning techniques in science. We highlight and discuss best practices starting with formulating validatable scientific questions, conducting robust data preparation and exploration, using a range of modeling techniques, performing rigorous validation by evaluating the stability and generalizability of unsupervised learning conclusions, and promoting effective communication and documentation of results to ensure reproducible scientific discoveries. To illustrate our proposed workflow, we present a case study from astronomy, seeking to refine globular clusters of Milky Way stars based upon their chemical composition. Our case study highlights the importance of validation and illustrates how the benefits of a carefully-designed workflow for unsupervised learning can advance scientific discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04553v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andersen Chang, Tiffany M. Tang, Tarek M. Zikry, Genevera I. Allen</dc:creator>
    </item>
    <item>
      <title>Semi-Implicit Variational Inference via Kernelized Path Gradient Descent</title>
      <link>https://arxiv.org/abs/2506.05088</link>
      <description>arXiv:2506.05088v1 Announce Type: cross 
Abstract: Semi-implicit variational inference (SIVI) is a powerful framework for approximating complex posterior distributions, but training with the Kullback-Leibler (KL) divergence can be challenging due to high variance and bias in high-dimensional settings. While current state-of-the-art semi-implicit variational inference methods, particularly Kernel Semi-Implicit Variational Inference (KSIVI), have been shown to work in high dimensions, training remains moderately expensive. In this work, we propose a kernelized KL divergence estimator that stabilizes training through nonparametric smoothing. To further reduce the bias, we introduce an importance sampling correction. We provide a theoretical connection to the amortized version of the Stein variational gradient descent, which estimates the score gradient via Stein's identity, showing that both methods minimize the same objective, but our semi-implicit approach achieves lower gradient variance. In addition, our method's bias in function space is benign, leading to more stable and efficient optimization. Empirical results demonstrate that our method outperforms or matches state-of-the-art SIVI methods in both performance and training efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05088v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Pielok, Bernd Bischl, David R\"ugamer</dc:creator>
    </item>
    <item>
      <title>Pooling information in likelihood-free inference</title>
      <link>https://arxiv.org/abs/2212.02658</link>
      <description>arXiv:2212.02658v2 Announce Type: replace-cross 
Abstract: Likelihood-free inference (LFI) methods, such as approximate Bayesian computation, have become commonplace for conducting inference in complex models. Many approaches are based on summary statistics or discrepancies derived from synthetic data. However, determining which summary statistics or discrepancies to use for constructing the posterior remains a challenging question, both practically and theoretically. Instead of relying on a single vector of summaries for inference, we propose a new pooled posterior that optimally combines inferences from multiple LFI posteriors. This pooled approach eliminates the need to select a single vector of summaries or even a specific LFI algorithm. Our approach is straightforward to implement and avoids performing a high-dimensional LFI analysis involving all summary statistics. We give theoretical guarantees for the improved performance of the pooled posterior mean in terms of asymptotic frequentist risk and demonstrate the effectiveness of the approach in a number of benchmark examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02658v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David T. Frazier, Christopher Drovandi, Lucas Kock, David J. Nott</dc:creator>
    </item>
    <item>
      <title>Smoothing Variances Across Time: Adaptive Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2408.11315</link>
      <description>arXiv:2408.11315v4 Announce Type: replace-cross 
Abstract: We introduce a novel Bayesian framework for estimating time-varying volatility by extending the Random Walk Stochastic Volatility (RWSV) model with Dynamic Shrinkage Processes (DSP) in log-variances. Unlike the classical Stochastic Volatility (SV) or GARCH-type models with restrictive parametric stationarity assumptions, our proposed Adaptive Stochastic Volatility (ASV) model provides smooth yet dynamically adaptive estimates of evolving volatility and its uncertainty. We further enhance the model by incorporating a nugget effect, allowing it to flexibly capture small-scale variability while preserving smoothness elsewhere. We derive the theoretical properties of the global-local shrinkage prior DSP. Through simulation studies, we show that ASV exhibits remarkable misspecification resilience and low prediction error across various data-generating processes. Furthermore, ASV's capacity to yield locally smooth and interpretable estimates facilitates a clearer understanding of the underlying patterns and trends in volatility. As an extension, we develop the Bayesian Trend Filter with ASV (BTF-ASV) which allows joint modeling of the mean and volatility with abrupt changes. Finally, our proposed models are applied to time series data from finance, econometrics, and environmental science, highlighting their flexibility and broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11315v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason B. Cho, David S. Matteson</dc:creator>
    </item>
  </channel>
</rss>
