<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Mar 2025 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Questioning Normality: A study of wavelet leaders distribution</title>
      <link>https://arxiv.org/abs/2503.08821</link>
      <description>arXiv:2503.08821v1 Announce Type: new 
Abstract: The motivation of this article is to estimate multifractality classification and model selection parameters: the first-order scaling exponent $c_1$ and the second-order scaling exponent (or intermittency coefficient) $c_2$. These exponents are built on wavelet leaders, which therefore constitute fundamental tools in applied multifractal analysis. While most estimation methods, particularly Bayesian approaches, rely on the assumption of log-normality, we challenge this hypothesis by statistically testing the normality of log-leaders. Upon rejecting this common assumption, we propose instead a novel model based on log-concave distributions. We validate this new model on well-known stochastic processes, including fractional Brownian motion, the multifractal random walk, and the canonical Mandelbrot cascade, as well as on real-world marathon runner data. Furthermore, we revisit the estimation procedure for $c_1$, providing confidence intervals, and for $c_2$, applying it to fractional Brownian motions with various Hurst indices as well as to the multifractal random walk. Finally, we establish several theoretical results on the distribution of log-leaders in random wavelet series, which are consistent with our numerical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08821v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wejdene Ben Nasr, H\'el\`ene Halconruy, St\'ephane Jaffard</dc:creator>
    </item>
    <item>
      <title>WOMBAT v2.S: A Bayesian inversion framework for attributing global CO$_2$ flux components from multiprocess data</title>
      <link>https://arxiv.org/abs/2503.09065</link>
      <description>arXiv:2503.09065v1 Announce Type: new 
Abstract: Contributions from photosynthesis and other natural components of the carbon cycle present the largest uncertainties in our understanding of carbon dioxide (CO$_2$) sources and sinks. While the global spatiotemporal distribution of the net flux (the sum of all contributions) can be inferred from atmospheric CO$_2$ concentrations through flux inversion, attributing the net flux to its individual components remains challenging. The advent of solar-induced fluorescence (SIF) satellite observations provides an opportunity to isolate natural components by anchoring gross primary productivity (GPP), the photosynthetic component of the net flux. Here, we introduce a novel statistical flux-inversion framework that simultaneously assimilates observations of SIF and CO$_2$ concentration, extending WOMBAT v2.0 (WOllongong Methodology for Bayesian Assimilation of Trace-gases, version 2.0) with a hierarchical model of spatiotemporal dependence between GPP and SIF processes. We call the new framework WOMBAT v2.S, and we apply it to SIF and CO$_2$ data from NASA's Orbiting Carbon Observatory-2 (OCO-2) satellite and other instruments to estimate natural fluxes over the globe during a recent six-year period. In a simulation experiment that matches OCO-2's retrieval characteristics, the inclusion of SIF improves accuracy and uncertainty quantification of component flux estimates. Comparing estimates from WOMBAT v2.S, v2.0, and the independent FLUXCOM initiative, we observe that linking GPP to SIF has little effect on net flux, as expected, but leads to spatial redistribution and more realistic seasonal structure in natural flux components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09065v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josh Jacobson, Michael Bertolacci, Andrew Zammit-Mangion, Andrew Schuh, Noel Cressie</dc:creator>
    </item>
    <item>
      <title>Frequency selection for the diagnostic characterization of human brain tumours</title>
      <link>https://arxiv.org/abs/2503.08756</link>
      <description>arXiv:2503.08756v1 Announce Type: cross 
Abstract: The diagnosis of brain tumours is an extremely sensitive and complex clinical task that must rely upon information gathered through non-invasive techniques. One such technique is magnetic resonance, in the modalities of imaging or spectroscopy. The latter provides plenty of metabolic information about the tumour tissue, but its high dimensionality makes resorting to pattern recognition techniques advisable. In this brief paper, an international database of brain tumours is analyzed resorting to an ad hoc spectral frequency selection procedure combined with nonlinear classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08756v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Arizmendi, Alfredo Vellido, Enrique Romero</dc:creator>
    </item>
    <item>
      <title>Comprehensive Benchmarking of Machine Learning Methods for Risk Prediction Modelling from Large-Scale Survival Data: A UK Biobank Study</title>
      <link>https://arxiv.org/abs/2503.08870</link>
      <description>arXiv:2503.08870v1 Announce Type: cross 
Abstract: Predictive modelling is vital to guide preventive efforts. Whilst large-scale prospective cohort studies and a diverse toolkit of available machine learning (ML) algorithms have facilitated such survival task efforts, choosing the best-performing algorithm remains challenging. Benchmarking studies to date focus on relatively small-scale datasets and it is unclear how well such findings translate to large datasets that combine omics and clinical features. We sought to benchmark eight distinct survival task implementations, ranging from linear to deep learning (DL) models, within the large-scale prospective cohort study UK Biobank (UKB). We compared discrimination and computational requirements across heterogenous predictor matrices and endpoints. Finally, we assessed how well different architectures scale with sample sizes ranging from n = 5,000 to n = 250,000 individuals. Our results show that discriminative performance across a multitude of metrices is dependent on endpoint frequency and predictor matrix properties, with very robust performance of (penalised) COX Proportional Hazards (COX-PH) models. Of note, there are certain scenarios which favour more complex frameworks, specifically if working with larger numbers of observations and relatively simple predictor matrices. The observed computational requirements were vastly different, and we provide solutions in cases where current implementations were impracticable. In conclusion, this work delineates how optimal model choice is dependent on a variety of factors, including sample size, endpoint frequency and predictor matrix properties, thus constituting an informative resource for researchers working on similar datasets. Furthermore, we showcase how linear models still display a highly effective and scalable platform to perform risk modelling at scale and suggest that those are reported alongside non-linear ML models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08870v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rafael R. Oexner, Robin Schmitt, Hyunchan Ahn, Ravi A. Shah, Anna Zoccarato, Konstantinos Theofilatos, Ajay M. Shah</dc:creator>
    </item>
    <item>
      <title>A Deep Bayesian Nonparametric Framework for Robust Mutual Information Estimation</title>
      <link>https://arxiv.org/abs/2503.08902</link>
      <description>arXiv:2503.08902v1 Announce Type: cross 
Abstract: Mutual Information (MI) is a crucial measure for capturing dependencies between variables, but exact computation is challenging in high dimensions with intractable likelihoods, impacting accuracy and robustness. One idea is to use an auxiliary neural network to train an MI estimator; however, methods based on the empirical distribution function (EDF) can introduce sharp fluctuations in the MI loss due to poor out-of-sample performance, destabilizing convergence. We present a Bayesian nonparametric (BNP) solution for training an MI estimator by constructing the MI loss with a finite representation of the Dirichlet process posterior to incorporate regularization in the training process. With this regularization, the MI loss integrates both prior knowledge and empirical data to reduce the loss sensitivity to fluctuations and outliers in the sample data, especially in small sample settings like mini-batches. This approach addresses the challenge of balancing accuracy and low variance by effectively reducing variance, leading to stabilized and robust MI loss gradients during training and enhancing the convergence of the MI approximation while offering stronger theoretical guarantees for convergence. We explore the application of our estimator in maximizing MI between the data space and the latent space of a variational autoencoder. Experimental results demonstrate significant improvements in convergence over EDF-based methods, with applications across synthetic and real datasets, notably in 3D CT image generation, yielding enhanced structure discovery and reduced overfitting in data synthesis. While this paper focuses on generative models in application, the proposed estimator is not restricted to this setting and can be applied more broadly in various BNP learning procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08902v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Forough Fazeliasl, Michael Minyi Zhang, Bei Jiang, Linglong Kong</dc:creator>
    </item>
    <item>
      <title>Multilevel Primary Aim Analyses of Clustered SMARTs: With Applications in Health Policy</title>
      <link>https://arxiv.org/abs/2503.08987</link>
      <description>arXiv:2503.08987v1 Announce Type: cross 
Abstract: In many health policy settings, adaptive interventions target a population of clusters (e.g., schools), with the ultimate intent of impacting outcomes at the level of individuals within the clusters. Health policy researchers can use clustered, sequential, multiple assignment, randomized trials (SMARTs) to answer important scientific questions concerning clustered adaptive interventions. A common primary aim is to compare the mean of a nested, end-of-study outcome between two clustered adaptive interventions. However, existing methods are not suitable when the primary outcome in a clustered SMART is nested and longitudinal (e.g., repeated outcome measures nested within mental healthcare providers, and mental healthcare providers nested within schools). This manuscript proposes a three-level marginal mean modeling and estimation approach for comparing adaptive interventions in a clustered SMART. The proposed method enables policy analysts to answer a wider array of scientific questions in the marginal comparison of clustered adaptive interventions. Further, relative to using an existing two-level method with a nested end-of-study outcome, the proposed method benefits from improved statistical efficiency. With this approach, we examine longitudinal comparisons of adaptive interventions for improving school-based mental healthcare and contrast its performance with existing approaches for studying static end-of-study outcomes. Methods were motivated by the Adaptive School-Based Implementation of CBT (ASIC) study, a clustered SMART designed to construct an adaptive health policy to improve the adoption of evidence-based CBT by mental healthcare professionals in high schools across Michigan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08987v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Durham, Anil Battalahalli, Amy Kilbourne, Andrew Quanbeck, Wenchu Pan, Tim Lycurgus, Daniel Almirall</dc:creator>
    </item>
    <item>
      <title>GENEOnet: Statistical analysis supporting explainability and trustworthiness</title>
      <link>https://arxiv.org/abs/2503.09199</link>
      <description>arXiv:2503.09199v1 Announce Type: cross 
Abstract: Group Equivariant Non-Expansive Operators (GENEOs) have emerged as mathematical tools for constructing networks for Machine Learning and Artificial Intelligence. Recent findings suggest that such models can be inserted within the domain of eXplainable Artificial Intelligence (XAI) due to their inherent interpretability. In this study, we aim to verify this claim with respect to GENEOnet, a GENEO network developed for an application in computational biochemistry by employing various statistical analyses and experiments. Such experiments first allow us to perform a sensitivity analysis on GENEOnet's parameters to test their significance. Subsequently, we show that GENEOnet exhibits a significantly higher proportion of equivariance compared to other methods. Lastly, we demonstrate that GENEOnet is on average robust to perturbations arising from molecular dynamics. These results collectively serve as proof of the explainability, trustworthiness, and robustness of GENEOnet and confirm the beneficial use of GENEOs in the context of Trustworthy Artificial Intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09199v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.BM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Bocchi, Patrizio Frosini, Alessandra Micheletti, Alessandro Pedretti, Carmen Gratteri, Filippo Lunghini, Andrea Rosario Beccari, Carmine Talarico</dc:creator>
    </item>
    <item>
      <title>How To Make Your Cell Tracker Say "I dunno!"</title>
      <link>https://arxiv.org/abs/2503.09244</link>
      <description>arXiv:2503.09244v1 Announce Type: cross 
Abstract: Cell tracking is a key computational task in live-cell microscopy, but fully automated analysis of high-throughput imaging requires reliable and, thus, uncertainty-aware data analysis tools, as the amount of data recorded within a single experiment exceeds what humans are able to overlook. We here propose and benchmark various methods to reason about and quantify uncertainty in linear assignment-based cell tracking algorithms. Our methods take inspiration from statistics and machine learning, leveraging two perspectives on the cell tracking problem explored throughout this work: Considering it as a Bayesian inference problem and as a classification problem. Our methods admit a framework-like character in that they equip any frame-to-frame tracking method with uncertainty quantification. We demonstrate this by applying it to various existing tracking algorithms including the recently presented Transformer-based trackers. We demonstrate empirically that our methods yield useful and well-calibrated tracking uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09244v1</guid>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard D. Paul, Johannes Seiffarth, David R\"ugamer, Hanno Scharr, Katharina N\"oh</dc:creator>
    </item>
    <item>
      <title>On the Wisdom of Crowds (of Economists)</title>
      <link>https://arxiv.org/abs/2503.09287</link>
      <description>arXiv:2503.09287v1 Announce Type: cross 
Abstract: We study the properties of macroeconomic survey forecast response averages as the number of survey respondents grows. Such averages are "portfolios" of forecasts. We characterize the speed and pattern of the gains from diversification and their eventual decrease with portfolio size (the number of survey respondents) in both (1) the key real-world data-based environment of the U.S. Survey of Professional Forecasters (SPF), and (2) the theoretical model-based environment of equicorrelated forecast errors. We proceed by proposing and comparing various direct and model-based "crowd size signature plots," which summarize the forecasting performance of k-average forecasts as a function of k, where k is the number of forecasts in the average. We then estimate the equicorrelation model for growth and inflation forecast errors by choosing model parameters to minimize the divergence between direct and model-based signature plots. The results indicate near-perfect equicorrelation model fit for both growth and inflation, which we explicate by showing analytically that, under conditions, the direct and fitted equicorrelation model-based signature plots are identical at a particular model parameter configuration, which we characterize. We find that the gains from diversification are greater for inflation forecasts than for growth forecasts, but that both gains nevertheless decrease quite quickly, so that fewer SPF respondents than currently used may be adequate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09287v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Francis X. Diebold, Aaron Mora, Minchul Shin</dc:creator>
    </item>
    <item>
      <title>Neural Network-Based Change Point Detection for Large-Scale Time-Evolving Data</title>
      <link>https://arxiv.org/abs/2503.09541</link>
      <description>arXiv:2503.09541v1 Announce Type: cross 
Abstract: The paper studies the problem of detecting and locating change points in multivariate time-evolving data. The problem has a long history in statistics and signal processing and various algorithms have been developed primarily for simple parametric models. In this work, we focus on modeling the data through feed-forward neural networks and develop a detection strategy based on the following two-step procedure. In the first step, the neural network is trained over a prespecified window of the data, and its test error function is calibrated over another prespecified window. Then, the test error function is used over a moving window to identify the change point. Once a change point is detected, the procedure involving these two steps is repeated until all change points are identified. The proposed strategy yields consistent estimates for both the number and the locations of the change points under temporal dependence of the data-generating process. The effectiveness of the proposed strategy is illustrated on synthetic data sets that provide insights on how to select in practice tuning parameters of the algorithm and in real data sets. Finally, we note that although the detection strategy is general and can work with different neural network architectures, the theoretical guarantees provided are specific to feed-forward neural architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09541v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialiang Geng, George Michailidis</dc:creator>
    </item>
    <item>
      <title>The TruEnd-procedure: Treating trailing zero-valued balances in credit data</title>
      <link>https://arxiv.org/abs/2404.17008</link>
      <description>arXiv:2404.17008v3 Announce Type: replace-cross 
Abstract: A novel procedure is presented for finding the true but latent endpoints within the repayment histories of individual loans. The monthly observations beyond these true endpoints are false, largely due to operational failures that delay account closure, thereby corrupting some loans. Detecting these false observations is difficult at scale since each affected loan history might have a different sequence of trailing zero (or very small) month-end balances. Identifying these trailing balances requires an exact definition of a "small balance", which our method informs. We demonstrate this procedure and isolate the ideal small-balance definition using South African residential mortgages. Evidently, corrupted loans are remarkably prevalent and have excess histories that are surprisingly long, which ruin the timing of risk events and compromise any subsequent time-to-event model, e.g., survival analysis. Having discarded these excess histories, we demonstrably improve the accuracy of both the predicted timing and severity of risk events, without materially impacting the portfolio. The resulting estimates of credit losses are lower and less biased, which augurs well for raising accurate credit impairments under IFRS 9. Our work therefore addresses a pernicious data error, which highlights the pivotal role of data preparation in producing credible forecasts of credit risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17008v3</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster, Roelinde Bester</dc:creator>
    </item>
    <item>
      <title>A flexible model for Record Linkage</title>
      <link>https://arxiv.org/abs/2407.06835</link>
      <description>arXiv:2407.06835v2 Announce Type: replace-cross 
Abstract: Combining data from various sources empowers researchers to explore innovative questions, for example those raised by conducting healthcare monitoring studies. However, the lack of a unique identifier often poses challenges. Record linkage procedures determine whether pairs of observations collected on different occasions belong to the same individual using partially identifying variables (e.g. birth year, postal code). Existing methodologies typically involve a compromise between computational efficiency and accuracy. Traditional approaches simplify this task by condensing information, yet they neglect dependencies among linkage decisions and disregard the one-to-one relationship required to establish coherent links. Modern approaches offer a comprehensive representation of the data generation process, at the expense of computational overhead and reduced flexibility. We propose a flexible method, that adapts to varying data complexities, addressing registration errors and accommodating changes of the identifying information over time. Our approach balances accuracy and scalability, estimating the linkage using a Stochastic Expectation Maximisation algorithm on a latent variable model. We illustrate the ability of our methodology to connect observations using large real data applications and demonstrate the robustness of our model to the linking variables quality in a simulation study. The proposed algorithm FlexRL is implemented and available in an open source R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06835v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssc/qlaf016</arxiv:DOI>
      <dc:creator>Kayan\'e Robach, St\'ephanie L van der Pas, Mark A van de Wiel, Michel H Hof</dc:creator>
    </item>
    <item>
      <title>Hypothesis tests and model parameter estimation on data sets with missing correlation information</title>
      <link>https://arxiv.org/abs/2410.22333</link>
      <description>arXiv:2410.22333v4 Announce Type: replace-cross 
Abstract: Ideally, all analyses of normally distributed data should include the full covariance information between all data points. In practice, the full covariance matrix between all data points is not always available. Either because a result was published without a covariance matrix, or because one tries to combine multiple results from separate publications. For simple hypothesis tests, it is possible to define robust test statistics that will behave conservatively in the presence on unknown correlations. For model parameter fits, one can inflate the variance by a factor to ensure that things remain conservative at least up to a chosen confidence level. This paper describes a class of robust test statistics for simple hypothesis tests, as well as an algorithm to determine the necessary inflation factor for model parameter fits and Goodness of Fit tests and composite hypothesis tests. It then presents some example applications of the methods to real neutrino interaction data and model comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22333v4</guid>
      <category>stat.ME</category>
      <category>hep-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
    <item>
      <title>Model-based bi-clustering using multivariate Poisson-lognormal with general block-diagonal covariance matrix and its applications</title>
      <link>https://arxiv.org/abs/2503.05961</link>
      <description>arXiv:2503.05961v2 Announce Type: replace-cross 
Abstract: While several Gaussian mixture models-based biclustering approaches currently exist in the literature for continuous data, approaches to handle discrete data have not been well researched. A multivariate Poisson-lognormal (MPLN) model-based bi-clustering approach that utilizes a block-diagonal covariance structure is introduced to allow for a more flexible structure of the covariance matrix. Two variations of the algorithm are developed where the number of column clusters: 1) are assumed equal across groups or 2) can vary across groups. Variational Gaussian approximation is utilized for parameter estimation, and information criteria are used for model selection. The proposed models are investigated in the context of clustering multivariate count data. Using simulated data the models display strong accuracy and computational efficiency and is applied to breast cancer RNA-sequence data from The Cancer Genome Atlas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05961v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caitlin Kral, Evan Chance, Ryan Browne, Sanjeena Subedi</dc:creator>
    </item>
  </channel>
</rss>
