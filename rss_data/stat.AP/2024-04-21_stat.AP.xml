<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Apr 2024 04:01:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Assessing the Longitudinal Impact of Environmental Chemical Mixtures on Children's Neurodevelopment: A Bayesian Approach</title>
      <link>https://arxiv.org/abs/2404.12553</link>
      <description>arXiv:2404.12553v1 Announce Type: new 
Abstract: This manuscript presents a novel Bayesian varying coefficient quantile regression (BVCQR) model designed to assess the longitudinal effects of chemical exposure mixtures on children's neurodevelopment. Recognizing the complexity and high-dimensionality of environmental exposures, the proposed approach addresses critical gaps in existing research by offering a method that can manage the sparsity of data and provide interpretable results. The proposed BVCQR model estimates the effects of mixtures on neurodevelopmental outcomes at specific ages, leveraging a horseshoe prior for sparsity and utilizing a Bayesian method for uncertainty quantification. Our simulations demonstrate the model's robustness and effectiveness in handling high-dimensional data, offering significant improvements over traditional models. The model's application to the Health Outcomes and Measures of the Environment (HOME) Study further illustrates its utility in identifying significant chemical exposures affecting children's growth and development. The findings underscore the potential of BVCQR in environmental health research, providing a sophisticated tool for analyzing the longitudinal impact of complex chemical mixtures, with implications for future studies aimed at understanding and mitigating environmental risks to child health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12553v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Jia, Roman Jandarov</dc:creator>
    </item>
    <item>
      <title>Analyzing whale calling through Hawkes process modeling</title>
      <link>https://arxiv.org/abs/2404.12583</link>
      <description>arXiv:2404.12583v1 Announce Type: new 
Abstract: Sound is assumed to be the primary modality of communication among marine mammal species. Analyzing acoustic recordings helps to understand the function of the acoustic signals as well as the possible impact of anthropogenic noise on acoustic behavior. Motivated by a dataset from a network of hydrophones in Cape Cod Bay, Massachusetts, utilizing automatically detected calls in recordings, we study the communication process of the endangered North Atlantic right whale. For right whales an "up-call" is known as a contact call, and ensuing counter-calling between individuals is presumed to facilitate group cohesion. We present novel spatiotemporal excitement modeling consisting of a background process and a counter-call process. The background process intensity incorporates the influences of diel patterns and ambient noise on occurrence. The counter-call intensity captures potential excitement, that calling elicits calling behavior. Call incidence is found to be clustered in space and time; a call seems to excite more calls nearer to it in time and space. We find evidence that whales make more calls during twilight hours, respond to other whales nearby, and are likely to remain quiet in the presence of increased ambient noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12583v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bokgyeong Kang, Erin M. Schliep, Alan E. Gelfand, Tina M. Yack, Christopher W. Clark, Robert S. Schick</dc:creator>
    </item>
    <item>
      <title>Corporate Financial Distress Prediction: Based on Multi-source Data and Feature Selection</title>
      <link>https://arxiv.org/abs/2404.12610</link>
      <description>arXiv:2404.12610v1 Announce Type: new 
Abstract: The advent of the era of big data provides new ideas for financial distress prediction. In order to evaluate the financial status of listed companies more accurately, this study establishes a financial distress prediction indicator system based on multi-source data by integrating three data sources: the company's internal management, the external market and online public opinion. This study addresses the redundancy and dimensional explosion problems of multi-source data integration, feature selection of the fused data, and a financial distress prediction model based on maximum relevance and minimum redundancy and support vector machine recursive feature elimination (MRMR-SVM-RFE). To verify the effectiveness of the model, we used back propagation (BP), support vector machine (SVM), and gradient boosted decision tree (GBDT) classification algorithms, and conducted an empirical study on China's listed companies based on different financial distress prediction indicator systems. MRMR-SVM-RFE feature selection can effectively extract information from multi-source fused data. The new feature dataset obtained by selection has higher prediction accuracy than the original data, and the BP classification model is better than linear regression (LR), decision tree (DT), and random forest (RF).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12610v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Ding, Chun Yan</dc:creator>
    </item>
    <item>
      <title>Proposer selection in EIP-7251</title>
      <link>https://arxiv.org/abs/2404.12657</link>
      <description>arXiv:2404.12657v1 Announce Type: new 
Abstract: Immediate settlement, or single-slot finality (SSF), is a long-term goal for Ethereum. The growing active validator set size is placing an increasing computational burden on the network, making SSF more challenging. EIP-7251 aims to reduce the number of validators by giving stakers the option to merge existing validators. Key to the success of this proposal therefore is whether stakers choose to merge their validators once EIP-7251 is implemented. It is natural to assume stakers participate only if they anticipate greater expected utility (risk-adjusted returns) as a single large validator. In this paper, we focus on one of the duties that a validator performs, viz. being the proposer for the next block. This duty can be quite lucrative, but happens infrequently. Based on previous analysis, we may assume that EIP-7251 implies no change to the security of the protocol. We confirm that the probability of a validator being selected as block proposer is equivalent under each consolidation regime. This result ensures that the decision of one staker to merge has no impact on the opportunity of another to propose the next block, in turn ensuring there is no major systemic change to the economics of the protocol with respect to proposer selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12657v1</guid>
      <category>stat.AP</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandra Johnson, Kerrie Mengersen, Patrick O'Callaghan, Anders L. Madsen</dc:creator>
    </item>
    <item>
      <title>Optimized Dynamic Mode Decomposition for Reconstruction and Forecasting of Atmospheric Chemistry Data</title>
      <link>https://arxiv.org/abs/2404.12396</link>
      <description>arXiv:2404.12396v1 Announce Type: cross 
Abstract: We introduce the optimized dynamic mode decomposition algorithm for constructing an adaptive and computationally efficient reduced order model and forecasting tool for global atmospheric chemistry dynamics. By exploiting a low-dimensional set of global spatio-temporal modes, interpretable characterizations of the underlying spatial and temporal scales can be computed. Forecasting is also achieved with a linear model that uses a linear superposition of the dominant spatio-temporal features. The DMD method is demonstrated on three months of global chemistry dynamics data, showing its significant performance in computational speed and interpretability. We show that the presented decomposition method successfully extracts known major features of atmospheric chemistry, such as summertime surface pollution and biomass burning activities. Moreover, the DMD algorithm allows for rapid reconstruction of the underlying linear model, which can then easily accommodate non-stationary data and changes in the dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12396v1</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meghana Velegar, Christoph Keller, J. Nathan Kutz</dc:creator>
    </item>
    <item>
      <title>Spatially Selected and Dependent Random Effects for Small Area Estimation with Application to Rent Burden</title>
      <link>https://arxiv.org/abs/2404.12463</link>
      <description>arXiv:2404.12463v1 Announce Type: cross 
Abstract: Area-level models for small area estimation typically rely on areal random effects to shrink design-based direct estimates towards a model-based predictor. Incorporating the spatial dependence of the random effects into these models can further improve the estimates when there are not enough covariates to fully account for spatial dependence of the areal means. A number of recent works have investigated models that include random effects for only a subset of areas, in order to improve the precision of estimates. However, such models do not readily handle spatial dependence. In this paper, we introduce a model that accounts for spatial dependence in both the random effects as well as the latent process that selects the effects. We show how this model can significantly improve predictive accuracy via an empirical simulation study based on data from the American Community Survey, and illustrate its properties via an application to estimate county-level median rent burden.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12463v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sho Kawano, Paul A. Parker, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>A Multivariate Copula-based Bayesian Framework for Doping Detection</title>
      <link>https://arxiv.org/abs/2404.12499</link>
      <description>arXiv:2404.12499v1 Announce Type: cross 
Abstract: Doping control is an essential component of anti-doping organizations for protecting clean sports competitions. Since 2009, this mission has been complemented worldwide by the Athlete Biological Passport (ABP), used to monitor athletes' individual profiles over time. The practical implementation of the ABP is based on a Bayesian framework, called ADAPTIVE, intended to identify individual reference ranges outside of which an observation may indicate doping abuse. Currently, this method follows a univariate approach, relying on simultaneous univariate analysis of different markers. This work extends the ADAPTIVE method to a multivariate testing framework, making use of copula models to couple the marginal distribution of biomarkers with their dependency structure. After introducing the proposed copula-based hierarchical model, we discuss our approach to inference, grounded in a Bayesian spirit, and present an extension to multidimensional predictive reference regions. Focusing on the hematological module of the ABP, we evaluate the proposed framework in both data-driven simulations and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12499v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nina Deliu, Brunero Liseo</dc:creator>
    </item>
    <item>
      <title>Algorithmic Changes Are Not Enough: Evaluating the Removal of Race Adjustment from the eGFR Equation</title>
      <link>https://arxiv.org/abs/2404.12812</link>
      <description>arXiv:2404.12812v1 Announce Type: cross 
Abstract: Changing clinical algorithms to remove race adjustment has been proposed and implemented for multiple health conditions. Removing race adjustment from estimated glomerular filtration rate (eGFR) equations may reduce disparities in chronic kidney disease (CKD), but has not been studied in clinical practice after implementation. Here, we assessed whether implementing an eGFR equation (CKD-EPI 2021) without adjustment for Black or African American race modified quarterly rates of nephrology referrals and visits within a single healthcare system, Stanford Health Care (SHC). Our cohort study analyzed 547,194 adult patients aged 21 and older who had at least one recorded serum creatinine or serum cystatin C between January 1, 2019 and September 1, 2023. During the study period, implementation of CKD-EPI 2021 did not modify rates of quarterly nephrology referrals in those documented as Black or African American or in the overall cohort. After adjusting for capacity at SHC nephrology clinics, estimated rates of nephrology referrals and visits with CKD-EPI 2021 were 34 (95% CI 29, 39) and 188 (175, 201) per 10,000 patients documented as Black or African American. If race adjustment had not been removed, estimated rates were nearly identical: 38 (95% CI: 28, 53) and 189 (165, 218) per 10,000 patients. Changes to the eGFR equation are likely insufficient to achieve health equity in CKD care decision-making as many other structural inequities remain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12812v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marika M. Cusick, Glenn M. Chertow, Douglas K. Owens, Michelle Y. Williams, Sherri Rose</dc:creator>
    </item>
    <item>
      <title>Scalable Data Assimilation with Message Passing</title>
      <link>https://arxiv.org/abs/2404.12968</link>
      <description>arXiv:2404.12968v1 Announce Type: cross 
Abstract: Data assimilation is a core component of numerical weather prediction systems. The large quantity of data processed during assimilation requires the computation to be distributed across increasingly many compute nodes, yet existing approaches suffer from synchronisation overhead in this setting. In this paper, we exploit the formulation of data assimilation as a Bayesian inference problem and apply a message-passing algorithm to solve the spatial inference problem. Since message passing is inherently based on local computations, this approach lends itself to parallel and distributed computation. In combination with a GPU-accelerated implementation, we can scale the algorithm to very large grid sizes while retaining good accuracy and compute and memory requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12968v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Key, So Takao, Daniel Giles, Marc Peter Deisenroth</dc:creator>
    </item>
    <item>
      <title>Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts</title>
      <link>https://arxiv.org/abs/2310.05898</link>
      <description>arXiv:2310.05898v5 Announce Type: replace-cross 
Abstract: Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.
  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves this through the incorporation of decoupled weight decay, where $\lambda$ represents the weight decay coefficient. Our analysis is made possible by the development of a new Lyapunov function for the Lion updates. It applies to a broader family of Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion is replaced by the subgradient of a convex function $\kappa$, leading to the solution of a general composite optimization problem of $\min_x f(x) + \kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion and pave the way for further improvements and extensions of Lion-related algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05898v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lizhang Chen, Bo Liu, Kaizhao Liang, Qiang Liu</dc:creator>
    </item>
    <item>
      <title>Time-Varying Identification of Monetary Policy Shocks</title>
      <link>https://arxiv.org/abs/2311.05883</link>
      <description>arXiv:2311.05883v3 Announce Type: replace-cross 
Abstract: We propose a new Bayesian heteroskedastic Markov-switching structural vector autoregression with data-driven time-varying identification. The model selects alternative exclusion restrictions over time and, as a condition for the search, allows to verify identification through heteroskedasticity within each regime. Based on four alternative monetary policy rules, we show that a monthly six-variable system supports time variation in US monetary policy shock identification. In the sample-dominating first regime, systematic monetary policy follows a Taylor rule extended by the term spread, effectively curbing inflation. In the second regime, occurring after 2000 and gaining more persistence after the global financial and COVID crises, it is characterized by a money-augmented Taylor rule. This regime's unconventional monetary policy provides economic stimulus, features the liquidity effect, and is complemented by a pure term spread shock. Absent the specific monetary policy of the second regime, inflation would be over one percentage point higher on average after 2008.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05883v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annika Camehl (Erasmus University Rotterdam), Tomasz Wo\'zniak (University of Melbourne)</dc:creator>
    </item>
    <item>
      <title>Geodesic slice sampling on Riemannian manifolds</title>
      <link>https://arxiv.org/abs/2312.00417</link>
      <description>arXiv:2312.00417v2 Announce Type: replace-cross 
Abstract: We propose a theoretically justified and practically applicable slice sampling based Markov chain Monte Carlo (MCMC) method for approximate sampling from probability measures on Riemannian manifolds. The latter naturally arise as posterior distributions in Bayesian inference of matrix-valued parameters, for example belonging to either the Stiefel or the Grassmann manifold. Our method, called geodesic slice sampling, is reversible with respect to the distribution of interest, and generalizes Hit-and-run slice sampling on $\mathbb{R}^{d}$ to Riemannian manifolds by using geodesics instead of straight lines. We demonstrate the robustness of our sampler's performance compared to other MCMC methods dealing with manifold valued distributions through extensive numerical experiments, on both synthetic and real data. In particular, we illustrate its remarkable ability to cope with anisotropic target densities, without using gradient information and preconditioning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00417v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Durmus, Samuel Gruffaz, Mareike Hasenpflug, Daniel Rudolf</dc:creator>
    </item>
  </channel>
</rss>
