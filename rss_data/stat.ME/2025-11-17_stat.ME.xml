<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Nov 2025 05:05:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improving Variance and Confidence Interval Estimation in Small-Sample Propensity Score Analyses: Bootstrap vs. Asymptotic Methods</title>
      <link>https://arxiv.org/abs/2511.10911</link>
      <description>arXiv:2511.10911v1 Announce Type: new 
Abstract: Propensity score (PS) methods are widely used to estimate treatment effects in non-randomized studies. Variance is typically estimated using sandwich or bootstrap methods, which can either treat the PS as estimated or fixed. The latter is thought to be conservative. Comparisons between the sandwich and bootstrap estimators have been compared in moderate to large sample sizes, favoring the bootstrap estimator. With the growing interest in treatments for rare disease and externally controlled clinical trials, very small sample sizes are not uncommon and the asymptotic properties of sandwich estimators may not hold. Bootstrap methods that allow for PS re-estimation can also generate problems with quasi-separation in small samples. It is unclear whether it is safe to prefer sandwich estimators or to assume that treating the PS as fixed is conservative. We conducted a Monte Carlo simulation to compare the performance of bootstrap versus sandwich variance and CI estimators for average treatment effects estimated with PS methods. We systematically evaluated the impact of treating the PS as fixed versus re-estimating it. These methodological comparisons were performed using Inverse Probability of Treatment Weighting (IPTW) and Augmented Inverse Probability of Treatment Weighting (AIPW) estimators. Simulations assessed performance under various conditions, including small sample sizes and different outcome and treatment prevalences. We illustrate the differences in our motivating example, the LIMIT-JIA trial. We show that the sandwich estimators can perform quite poorly in small samples, and fixed PS methods are not necessarily conservative. A stratified bootstrap avoids quasi-separation and performs well. Differences were large enough to alter statistical conclusions in our motivating example, LIMIT-JIA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10911v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baoshan Zhang, Sean M. O'Brien, Yuan Wu, Laine E. Thomas</dc:creator>
    </item>
    <item>
      <title>Multivariate longitudinal modeling of cross-sectional and lagged associations between a continuous time-varying endogenous covariate and a non-Gaussian outcome</title>
      <link>https://arxiv.org/abs/2511.11114</link>
      <description>arXiv:2511.11114v1 Announce Type: new 
Abstract: In longitudinal studies, time-varying covariates are often endogenous, meaning their values depend on both their own history and that of the outcome variable. This violates key assumptions of Generalized Linear Mixed Effects Models (GLMMs), leading to biased and inconsistent estimates. Additionally, missing data and non-concurrent measurements between covariates and outcomes further complicate analysis, especially in rare or degenerative diseases where data is limited. To address these challenges, we propose an alternative use of two well-known multivariate models, each assuming a different form of the association. One induces the association by jointly modeling the random effects, called Joint Mixed Model (JMM); the other quantifies the association using a scaling factor, called Joint Scaled Model (JSM). We extend these models to accommodate continuous endogenous covariates and a wide range of longitudinal outcome types. A limitation in both cases is that the interpretation of the association is neither straightforward nor easy to communicate to scientists. Hence, we have numerically derived an association coefficient that measures the marginal relation between the outcome and the endogenous covariate. The proposed method provides interpretable, population-level estimates of cross-sectional associations (capturing relationships between covariates and outcomes measured at the same time point) and lagged associations (quantifying how past covariate values influence future outcomes), enabling clearer clinical insights. We fitted the JMM and JSM using a flexible Bayesian estimation approach, known as Integrated Nested Laplace Approximation (INLA), to overcome computation burden problems. These models will be presented along with the results of a simulation study and a natural history study on patients with Duchenne Muscular Dystrophy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11114v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chiara Degan, Bart J. A. Mertens, Jelle Goeman, Nadine A. Ikelaar, Erik H. Niks, Pietro Spitali, Roula Tsonaka</dc:creator>
    </item>
    <item>
      <title>Knockoffs for low dimensions: changing the nominal level post-hoc to gain power while controlling the FDR</title>
      <link>https://arxiv.org/abs/2511.11166</link>
      <description>arXiv:2511.11166v1 Announce Type: new 
Abstract: Knockoffs are a powerful tool for controlled variable selection with false discovery rate (FDR) control. However, while they are frequently used in high-dimensional regressions, they lack power in low-dimensional and sparse signal settings. One of the main reasons is that knockoffs require a minimum number of selections, depending on the nominal FDR level. In this paper, we leverage e-values to allow the nominal level to be switched after looking at the data and applying the knockoff procedure. In this way, we can increase the nominal level in cases where the original knockoff procedure does not make any selections to potentially make discoveries. Also, in cases where the original knockoff procedure makes discoveries, we can often decrease the nominal level to increase the precision. These improvements come without any costs, meaning the results of our post-hoc knockoff procedure are always more informative than the results of the original knockoff procedure. Furthermore, we apply our technique to recently proposed derandomized knockoff procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11166v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Konstantinos Sechidis</dc:creator>
    </item>
    <item>
      <title>Online Spectral Density Estimation</title>
      <link>https://arxiv.org/abs/2511.11296</link>
      <description>arXiv:2511.11296v1 Announce Type: new 
Abstract: This paper develops the first online algorithms for estimating the spectral density function -- a fundamental object of interest in time series analysis -- that satisfies the three core requirements of streaming inference: fixed memory, fixed computational complexity, and temporal adaptivity. Our method builds on the concept of forgetting factors, allowing the estimator to adapt to gradual or abrupt changes in the data-generating process without prior knowledge of its dynamics. We introduce a novel online forgetting-factor periodogram and show that, under stationarity, it asymptotically recovers the properties of its offline counterpart. Leveraging this, we construct an online Whittle estimator, and further develop an adaptive online spectral estimator that dynamically tunes its forgetting factor using the Whittle likelihood as a loss. Through extensive simulation studies and an application to ocean drifter velocity data, we demonstrate the method's ability to track time-varying spectral properties in real-time with strong empirical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11296v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahriar Hasnat Kazi, Niall Adams, Edward A. K. Cohen</dc:creator>
    </item>
    <item>
      <title>Extreme-PLS with missing data under weak dependence</title>
      <link>https://arxiv.org/abs/2511.11338</link>
      <description>arXiv:2511.11338v1 Announce Type: new 
Abstract: This paper develops a theoretical framework for Extreme Partial Least Squares (EPLS) dimension reduction in the presence of missing data and weak temporal dependence. Building upon the recent EPLS methodology for modeling extremal dependence between a response variable and high-dimensional covariates, we extend the approach to more realistic data settings where both serial correlation and missing-ness occur. Specifically, we consider a single-index inverse regression model under heavy-tailed conditions and introduce a Missing-at-Random (MAR) mechanism acting on the covariates, whose probability depends on the extremeness of the response. The asymptotic behavior of the proposed estimator is established within an alpha-mixing framework, leading to consistency results under regularly varying tails. Extensive Monte-Carlo experiments covering eleven dependence schemes (including ARMA, GARCH, and nonlinear ESTAR processes) demonstrate that the method performs robustly across a wide range of heavy-tailed and dependent scenarios, even when substantial portions of data are missing. A real-world application to environmental data further confirms the method's capacity to recover meaningful tail directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11338v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>St\'ephane Girard, Cambyse Pakzad</dc:creator>
    </item>
    <item>
      <title>Interpolated stochastic interventions based on propensity scores, target policies and treatment-specific costs</title>
      <link>https://arxiv.org/abs/2511.11353</link>
      <description>arXiv:2511.11353v1 Announce Type: new 
Abstract: We introduce families of stochastic interventions for discrete treatments that connect causal modeling to cost-sensitive decision making. The interventions arise from a cost-penalized information projection of the independent product of the organic propensity and a user-specified target, yielding closed-form Boltzmann-Gibbs couplings. The induced marginals define modified stochastic policies that interpolate smoothly, via a single tilt parameter, from the organic law or from the target distribution toward a product-of-experts limit when all destination costs are strictly positive. One of these families recovers and extends incremental propensity score interventions, retaining identification without global positivity. For inference, we derive efficient influence functions under a nonparametric model for the expected outcomes after these policies and construct one-step estimators with uniform confidence bands. In simulations, the proposed estimators improve stability and robustness to nuisance misspecification relative to plug-in baselines. The framework can operationalize graded scientific hypotheses under realistic constraints: because inputs are modular, analysts can sweep feasible policy spaces, prototype candidates, and align interventions with budgets and logistics before committing experimental resources. This could help close the loop between observational evidence and resource-aware experimental design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11353v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Johan de Aguas</dc:creator>
    </item>
    <item>
      <title>Model Class Selection</title>
      <link>https://arxiv.org/abs/2511.11355</link>
      <description>arXiv:2511.11355v1 Announce Type: new 
Abstract: Classical model selection seeks to find a single model within a particular class that optimizes some pre-specified criteria, such as maximizing a likelihood or minimizing a risk. More recently, there has been an increased interest in model set selection (MSS), where the aim is to identify a (confidence) set of near-optimal models. Here, we generalize the MSS framework further by introducing the idea of model class selection (MCS). In MCS, multiple model collections are evaluated, and all collections that contain at least one optimal model are sought for identification. Under mild conditions, data splitting based approaches are shown to provide general solutions for MCS. As a direct consequence, for particular datasets we are able to investigate formally whether classes of simpler and more interpretable statistical models are able to perform on par with more complex black-box machine learning models. A variety of simulated and real-data experiments are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11355v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Cecil, Lucas Mentch</dc:creator>
    </item>
    <item>
      <title>Estimating the Effects of Heatwaves on Health: A Causal Inference Framework</title>
      <link>https://arxiv.org/abs/2511.11433</link>
      <description>arXiv:2511.11433v1 Announce Type: new 
Abstract: The harmful relationship between heatwaves and health has been extensively documented in medical and epidemiological literature. However, most evidence is associational and cannot be interpreted causally unless strong assumptions are made. In this paper, we first make explicit the assumptions underlying the statistical methods frequently used in the heatwave literature and demonstrate when these assumptions might break down in heatwave contexts. To address these shortcomings, we propose a causal inference framework that transparently elicits causal identification assumptions. Within this new framework, we first introduce synthetic controls (SC) for estimating heatwave effects, then propose a spatially augmented Bayesian synthetic control (SA-SC) method that accounts for spatial dependence and spillovers. Empirical Monte Carlo simulations show both methods perform well, with SA-SC reducing root mean squared error and improving posterior interval coverage under spillovers and spatial dependence. Finally, we apply the proposed methods to estimate the causal effects of heatwaves on Medicare heat-related hospitalizations among 13,753,273 beneficiaries residing in Northeastern U.S. from 2000 to 2019. This causal inference framework provides spatially coherent counterfactual outcomes and robust, interpretable, and transparent causal estimates while explicitly addressing the unexamined assumptions in existing methods that pervade the heatwave effect literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11433v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giulio Grossi, Leo Vanciu, Veronica Ballerini, Danielle Braun, Falco J. Bargagli Stoffi</dc:creator>
    </item>
    <item>
      <title>A Recursive Theory of Variational State Estimation: The Dynamic Programming Approach</title>
      <link>https://arxiv.org/abs/2511.11497</link>
      <description>arXiv:2511.11497v1 Announce Type: new 
Abstract: In this article, variational state estimation is examined from the dynamic programming perspective. This leads to two different value functional recursions depending on whether backward or forward dynamic programming is employed. The result is a theory of variational state estimation that corresponds to the classical theory of Bayesian state estimation. More specifically, in the backward method, the value functional corresponds to a likelihood that is upper bounded by the state likelihood from the Bayesian backward recursion. In the forward method, the value functional corresponds to an unnormalized density that is upper bounded by the unnormalized filtering density. Both methods can be combined to arrive at a variational two-filter formula. Additionally, it is noted that optimal variational filtering is generally of quadratic time-complexity in the sequence length. This motivates the notion of sub-optimal variational filtering, which also lower bounds the evidence but is of linear time-complexity. Another problem is the fact that the value functional recursions are generally intractable. This is briefly discussed and a simple approximation is suggested that retrieves the filter proposed by Courts et al. (2021). The methodology is examined in a jump Gauss--Markov system, where it is observed that the value functional recursions are tractable under a certain factored Markov process approximation. A simulation study demonstrates that the posterior approximation is of adequate quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11497v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filip Tronarp</dc:creator>
    </item>
    <item>
      <title>Estimating Total Effects in Bipartite Experiments with Spillovers and Partial Eligibility</title>
      <link>https://arxiv.org/abs/2511.11564</link>
      <description>arXiv:2511.11564v1 Announce Type: new 
Abstract: We study randomized experiments in bipartite systems where only a subset of treatment-side units are eligible for assignment while all units continue to interact, generating interference. We formalize eligibility-constrained bipartite experiments and define estimands aligned with full deployment: the Primary Total Treatment Effect (PTTE) on eligible units and the Secondary Total Treatment Effect (STTE) on ineligible units. Under randomization within the eligible set, we give identification conditions and develop interference-aware ensemble estimators that combine exposure mappings, generalized propensity scores, and flexible machine learning. We further introduce a projection that links treatment- and outcome-level estimands; this mapping is exact under a Linear Additive Edges condition and enables estimation on the (typically much smaller) treatment side with deterministic aggregation to outcomes. In simulations with known ground truth across realistic exposure regimes, the proposed estimators recover PTTE and STTE with low bias and variance and reduce the bias that could arise when interference is ignored. Two field experiments illustrate practical relevance: our method corrects the direction of expected interference bias for a pre-specified metric in both studies and reverses the sign and significance of the primary decision metric in one case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11564v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Albert Tan, Mohsen Bayati, James Nordlund, Roman Istomin</dc:creator>
    </item>
    <item>
      <title>Online Price Competition under Generalized Linear Demands</title>
      <link>https://arxiv.org/abs/2511.10718</link>
      <description>arXiv:2511.10718v1 Announce Type: cross 
Abstract: We study sequential price competition among $N$ sellers, each influenced by the pricing decisions of their rivals. Specifically, the demand function for each seller $i$ follows the single index model $\lambda_i(\mathbf{p}) = \mu_i(\langle \boldsymbol{\theta}_{i,0}, \mathbf{p} \rangle)$, with known increasing link $\mu_i$ and unknown parameter $\boldsymbol{\theta}_{i,0}$, where the vector $\mathbf{p}$ denotes the vector of prices offered by all the sellers simultaneously at a given instant. Each seller observes only their own realized demand -- unobservable to competitors -- and the prices set by rivals. Our framework generalizes existing approaches that focus solely on linear demand models. We propose a novel decentralized policy, PML-GLUCB, that combines penalized MLE with an upper-confidence pricing rule, removing the need for coordinated exploration phases across sellers -- which is integral to previous linear models -- and accommodating both binary and real-valued demand observations. Relative to a dynamic benchmark policy, each seller achieves $O(N^{2}\sqrt{T}\log(T))$ regret, which essentially matches the optimal rate known in the linear setting. A significant technical contribution of our work is the development of a variant of the elliptical potential lemma -- typically applied in single-agent systems -- adapted to our competitive multi-agent environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10718v1</guid>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Bracale, Moulinath Banerjee, Cong Shi, Yuekai Sun</dc:creator>
    </item>
    <item>
      <title>Neighborhood Stability in Double/Debiased Machine Learning with Dependent Data</title>
      <link>https://arxiv.org/abs/2511.10995</link>
      <description>arXiv:2511.10995v1 Announce Type: cross 
Abstract: This paper studies double/debiased machine learning (DML) methods applied to weakly dependent data. We allow observations to be situated in a general metric space that accommodates spatial and network data. Existing work implements cross-fitting by excluding from the training fold observations sufficiently close to the evaluation fold. We find in simulations that this can result in exceedingly small training fold sizes, particularly with network data. We therefore seek to establish the validity of DML without cross-fitting, building on recent work by Chen et al. (2022). They study i.i.d. data and require the machine learner to satisfy a natural stability condition requiring insensitivity to data perturbations that resample a single observation. We extend these results to dependent data by strengthening stability to "neighborhood stability," which requires insensitivity to resampling observations in any slowly growing neighborhood. We show that existing results on the stability of various machine learners can be adapted to verify neighborhood stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10995v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianfei Cao, Michael P. Leung</dc:creator>
    </item>
    <item>
      <title>Bayesian ICA with super-Gaussian Source Priors</title>
      <link>https://arxiv.org/abs/2406.17058</link>
      <description>arXiv:2406.17058v3 Announce Type: replace 
Abstract: Independent Component Analysis (ICA) plays a central role in modern machine learning as a flexible framework for feature extraction. We introduce a horseshoe-type prior with a latent Polya-Gamma scale mixture representation, yielding scalable algorithms for both point estimation via expectation-maximization (EM) and full posterior inference via Markov chain Monte Carlo (MCMC). This hierarchical formulation unifies several previously disparate estimation strategies within a single Bayesian framework. We also establish the first theoretical guarantees for hierarchical Bayesian ICA, including posterior contraction and local asymptotic normality results for the unmixing matrix. Comprehensive simulation studies demonstrate that our methods perform competitively with widely used ICA tools. We further discuss implementation of conditional posteriors, envelope-based optimization, and possible extensions to flow-based architectures for nonlinear feature extraction and deep learning. Finally, we outline several promising directions for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17058v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Soham Ghosh, Nicholas G. Polson</dc:creator>
    </item>
    <item>
      <title>Particle Based Inference for Continuous-Discrete State Space Models</title>
      <link>https://arxiv.org/abs/2407.15666</link>
      <description>arXiv:2407.15666v2 Announce Type: replace 
Abstract: This article develops a methodology allowing application of the complete machinery of particle-based inference methods upon the class of continuous-discrete State Space Models (CD-SSMs). Such models correspond to a latent continuous-time It\^o diffusion process which is observed with noise at discrete time instances. Due to the continuous-time nature of the hidden signal, standard Feynman-Kac formulations and their accompanying particle-based approximations have to overcome several challenges, arising mainly due to the following considerations: (i) finite-time transition densities of the signal are typically intractable; (ii) ancestors of sampled signals are determined w.p.~1, thus cannot be resampled; (iii) diffusivity parameters given a sampled signal yield Dirac distributions. We overcome all above issues by introducing a framework based on carefully designed path proposals and reparameterisations thereof. That is, we obtain new expressions for the Feynman-Kac model that accommodate the effects of a continuous-time signal and overcome induced degeneracies. The constructed formulations enable use of the full range of particle-based algorithms for CD-SSMs: for filtering/smoothing and parameter inference, whether online or offline. Our framework is compatible with guided proposals in the filtering steps that are essential for efficient algorithmic performance in the presence of informative observations or in higher dimensions, and is applicable for a very general class of CD-SSMs, including the case when the signal is modelled as a hypo-elliptic diffusion. We incorporate our methods into an established probabilistic programming package and present several numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15666v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Stanton, Alexandros Beskos</dc:creator>
    </item>
    <item>
      <title>Information borrowing in Bayesian clinical trials: choice of tuning parameters for the robust mixture prior</title>
      <link>https://arxiv.org/abs/2412.03185</link>
      <description>arXiv:2412.03185v2 Announce Type: replace 
Abstract: External data borrowing in clinical trial designs has increased in recent years. This is accomplished in the Bayesian framework by specifying informative prior distributions. To mitigate the impact of potential inconsistency (bias) between external and current data, robust approaches have been proposed. One such approach is the robust mixture prior arising as a mixture of an informative prior and a more dispersed prior inducing dynamic borrowing. This prior requires the choice of four quantities: the mixture weight, mean, dispersion and parametric form of the robust component. To address the challenge associated with choosing these quantities, we perform a case-by-case study of their impact on specific operating characteristics in one-arm and hybrid-control trials with a normal endpoint. All four quantities were found to strongly impact the operating characteristics. As already known, variance of the robust component is linked to robustness. Less known, however, is that its location can have severe impact on test and estimation error. Further, the impact of the weight choice is strongly linked with the robust component's location and variance. We provide recommendations for the choice of the robust component parameters, prior weight, alternative functional form for this component and considerations for evaluating operating characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03185v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vivienn Weru, Annette Kopp-Schneider, Manuel Wiesenfarth, Sebastian Weber, Silvia Calderazzo</dc:creator>
    </item>
    <item>
      <title>A note on parameter orthogonality for multi-parameter distributions</title>
      <link>https://arxiv.org/abs/2501.08093</link>
      <description>arXiv:2501.08093v3 Announce Type: replace 
Abstract: This note addresses issues raised by Cox and Reid in their seminal paper in 1987 regarding parameter orthogonality in statistical inference. We extend the orthogonality condition to cases with multiple parameters of interest and demonstrate its existence at a global level for some generally important distributions, despite previously expressed pessimism by them. Numerical results with the location-scale $t$-distribution reveal substantial gains in estimation accuracy and savings in computation time, thanks to the existence. We next show that the local parameter orthogonality can lead to efficient computational algorithms with the celebrated Whittle algorithm for multivariate autoregressive modeling as a showcase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08093v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changle Shen, Dong Li, Howell Tong</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory for Regularized Estimation in Functional Time Series Models</title>
      <link>https://arxiv.org/abs/2506.05922</link>
      <description>arXiv:2506.05922v2 Announce Type: replace 
Abstract: Functional autoregressive (FAR) models provide a fundamental framework for analyzing temporally dependent functional data. However, the infinite-dimensional nature of the underlying Hilbert space introduces intrinsic ill-posedness, as the autocovariance operators are compact and lack bounded inverses. This paper develops a new theoretical framework for the regularized estimation and asymptotic analysis of FAR models. Leveraging Hilbert space theory, we rigorously characterize the distinction between finite- and infinite-dimensional time series analysis and formalize the necessity of regularization. To stabilize the estimation of autoregressive operators, we introduce a Tikhonov regularization scheme and derive Yule-Walker-type estimators in a general Hilbert space, and further specialize to the $L^2$ space for explicit forms. Within this unified framework, we establish the consistency and asymptotic normality of the regularized estimators and reveal that asymptotic normality can be achieved only for the predictors rather than the operator estimates themselves. Furthermore, we derive the mean squared prediction error (MSPE) and decompose its bias-variance structure. A comprehensive simulation study and an application to high-frequency functional data from wearable devices demonstrate the practical validity of the theory and the ability of FAR models to capture dynamic functional patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05922v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Niu, Yuwei Zhao, Zhao Chen, Christina Dan Wang</dc:creator>
    </item>
    <item>
      <title>Neural Conditional Simulation for Complex Spatial Processes</title>
      <link>https://arxiv.org/abs/2508.20067</link>
      <description>arXiv:2508.20067v2 Announce Type: replace 
Abstract: A key objective in spatial statistics is to simulate from the distribution of a spatial process at a selection of unobserved locations conditional on observations (i.e., a predictive distribution) to enable spatial prediction and uncertainty quantification. However, exact conditional simulation from this predictive distribution is intractable or inefficient for many spatial process models. In this paper, we propose neural conditional simulation (NCS), a general method for spatial conditional simulation that is based on neural diffusion models. Specifically, using spatial masks, we implement a conditional score-based diffusion model that evolves Gaussian noise into samples from a predictive distribution when given a partially observed spatial field and spatial process parameters as inputs. The diffusion model relies on a neural network that only requires unconditional samples from the spatial process for training. Once trained, the diffusion model is amortized with respect to the observations in the partially observed field, the number and locations of those observations, and the spatial process parameters, and can therefore be used to conditionally simulate from a broad class of predictive distributions without retraining the neural network. We assess the NCS-generated simulations against simulations from the true conditional distribution of a Gaussian process model, and against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnick process model for spatial extremes. In the latter case, we show that it is more efficient and accurate to conditionally simulate using NCS than classical MCMC techniques implemented in standard software. We conclude that NCS enables efficient and accurate conditional simulation from spatial predictive distributions that are challenging to sample from using traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20067v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Walchessen, Andrew Zammit-Mangion, Rapha\"el Huser, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>Some are observed, all leave traces: whole-population modeling of French elite civil servants' career paths</title>
      <link>https://arxiv.org/abs/2311.15257</link>
      <description>arXiv:2311.15257v3 Announce Type: replace-cross 
Abstract: Elite civil servants may come and go between the public and private sectors throughout their career, a process of particular interest for the public and social scientists. However, data to document such processes are rarely completely available: we need inference tools that can account for many missing values. We consider public-private paths of elite French civil servants and introduce binary Markov switching models with Bayesian data augmentation. Our procedure relies on two complementary data sources: (1) detailed observations of some individual trajectories obtained from LinkedIn; (2) less informative ``traces'' left by all individuals in the administrative record, which we model for missing data imputation. This model class maintains the properties of hidden Markov models and enables a tailored sampler to target the posterior, yet allows for varying parameters across individuals and time. By integrating the two sources, we can consider the whole population rather than just a sample, and avoid the biases that would stem from using only a single source. We demonstrate this allows to properly test substantive hypotheses on career paths across a variety of public organizations. We notably show that the probability for ENA graduates to exit the public sector has not increased since 1990, but that the probability they return has increased. We identify three clusters of organizations, with distinct patterns of public-private behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15257v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Voldoire, Robin J. Ryder, Ryan Lahfa</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Linear Bandits under Stochastic Latent Heterogeneity</title>
      <link>https://arxiv.org/abs/2502.00423</link>
      <description>arXiv:2502.00423v2 Announce Type: replace-cross 
Abstract: This paper addresses the critical challenge of stochastic latent heterogeneity in online decision-making, where individuals' responses to actions vary not only with observable contexts but also with unobserved, randomly realized subgroups. Existing data-driven approaches largely capture observable heterogeneity through contextual features but fail when the sources of variation are latent and stochastic. We propose a latent heterogeneous bandit framework that explicitly models probabilistic subgroup membership and group-specific reward functions, using promotion targeting as a motivating example. Our phased EM-greedy algorithm jointly learns latent group probabilities and reward parameters in high dimensions, achieving optimal estimation and classification guarantees. Our analysis reveals a new phenomenon unique to decision-making with stochastic latent subgroups: randomness in group realizations creates irreducible classification uncertainty, making sub-linear regret against a fully informed strong oracle fundamentally impossible. We establish matching upper and minimax lower bounds for both the strong and regular regrets, corresponding, respectively, to oracles with and without access to realized group memberships. The strong regret necessarily grows linearly, while the regular regret achieves a minimax-optimal sublinear rate. These findings uncover a fundamental stochastic barrier in online decision-making and point to potential remedies through simple strategic interventions and mechanism-design-based elicitation of latent information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00423v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elynn Chen, Xi Chen, Wenbo Jing, Xiao Liu</dc:creator>
    </item>
    <item>
      <title>Sequentially Auditing Differential Privacy</title>
      <link>https://arxiv.org/abs/2509.07055</link>
      <description>arXiv:2509.07055v2 Announce Type: replace-cross 
Abstract: We propose a practical sequential test for auditing differential privacy guarantees of black-box mechanisms. The test processes streams of mechanisms' outputs providing anytime-valid inference while controlling Type I error, overcoming the fixed sample size limitation of previous batch auditing methods. Experiments show this test detects violations with sample sizes that are orders of magnitude smaller than existing methods, reducing this number from 50K to a few hundred examples, across diverse realistic mechanisms. Notably, it identifies DP-SGD privacy violations in \textit{under} one training run, unlike prior methods needing full model training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07055v2</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom\'as Gonz\'alez, Mateo Dulce-Rubio, Aaditya Ramdas, M\'onica Ribero</dc:creator>
    </item>
    <item>
      <title>Fuzzy Prediction Sets: Conformal Prediction with E-values</title>
      <link>https://arxiv.org/abs/2509.13130</link>
      <description>arXiv:2509.13130v2 Announce Type: replace-cross 
Abstract: We make three contributions to conformal prediction. First, we propose fuzzy conformal prediction sets that offer a degree of exclusion, generalizing beyond the binary inclusion/exclusion offered by classical prediction sets. We connect fuzzy prediction sets to e-values to show this degree of exclusion is equivalent to an exclusion at different confidence levels, capturing precisely what e-values bring to conformal prediction. We show that a fuzzy prediction set is a predictive distribution with an arguably more appropriate error guarantee. Second, we derive optimal conformal prediction sets by interpreting the minimization of the expected measure of a prediction set as an optimal testing problem against a particular alternative. We use this to characterize exactly in what sense traditional conformal prediction is optimal, and show how this may generally be used to construct optimal (fuzzy) prediction sets. Third, we generalize the inheritance of guarantees by subsequent minimax decisions from prediction sets to fuzzy prediction sets. All results generalize beyond the conformal setting to prediction sets for arbitrary models. In particular, we find that constructing a (fuzzy) prediction set for a model is equivalent to constructing a test (e-value) for that model as a hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13130v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>Data reuse enables cost-efficient randomized trials of medical AI models</title>
      <link>https://arxiv.org/abs/2511.08986</link>
      <description>arXiv:2511.08986v2 Announce Type: replace-cross 
Abstract: Randomized controlled trials (RCTs) are indispensable for establishing the clinical value of medical artificial-intelligence (AI) tools, yet their high cost and long timelines hinder timely validation as new models emerge rapidly. Here, we propose BRIDGE, a data-reuse RCT design for AI-based risk models. AI risk models support a broad range of interventions, including screening, treatment selection, and clinical alerts. BRIDGE trials recycle participant-level data from completed trials of AI models when legacy and updated models make concordant predictions, thereby reducing the enrollment requirement for subsequent trials. We provide a practical checklist for investigators to assess whether reusing data from previous trials allows for valid causal inference and preserves type I error. Using real-world datasets across breast cancer, cardiovascular disease, and sepsis, we demonstrate concordance between successive AI models, with up to 64.8% overlap in top 5% high-risk cohorts. We then simulate a series of breast cancer screening studies, where our design reduced required enrollment by 46.6%--saving over US$2.8 million--while maintaining 80% power. By transforming trials into adaptive, modular studies, our proposed design makes Level I evidence generation feasible for every model iteration, thereby accelerating cost-effective translation of AI into routine care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08986v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Nercessian, Wenxin Zhang, Alexander Schubert, Daphne Yang, Maggie Chung, Ahmed Alaa, Adam Yala</dc:creator>
    </item>
  </channel>
</rss>
