<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Sep 2024 01:43:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Flexible survival regression with variable selection for heterogeneous population</title>
      <link>https://arxiv.org/abs/2409.10771</link>
      <description>arXiv:2409.10771v1 Announce Type: new 
Abstract: Survival regression is widely used to model time-to-events data, to explore how covariates may influence the occurrence of events. Modern datasets often encompass a vast number of covariates across many subjects, with only a subset of the covariates significantly affecting survival. Additionally, subjects often belong to an unknown number of latent groups, where covariate effects on survival differ significantly across groups. The proposed methodology addresses both challenges by simultaneously identifying the latent sub-groups in the heterogeneous population and evaluating covariate significance within each sub-group. This approach is shown to enhance the predictive accuracy for time-to-event outcomes, via uncovering varying risk profiles within the underlying heterogeneous population and is thereby helpful to device targeted disease management strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10771v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Mandal, Abhisek Chakraborty</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Chi-square Statistics or F-Statistics Based on Multiple Imputation</title>
      <link>https://arxiv.org/abs/2409.10812</link>
      <description>arXiv:2409.10812v1 Announce Type: new 
Abstract: Missing data is a common issue in medical, psychiatry, and social studies. In literature, Multiple Imputation (MI) was proposed to multiply impute datasets and combine analysis results from imputed datasets for statistical inference using Rubin's rule. However, Rubin's rule only works for combined inference on statistical tests with point and variance estimates and is not applicable to combine general F-statistics or Chi-square statistics. In this manuscript, we provide a solution to combine F-test statistics from multiply imputed datasets, when the F-statistic has an explicit fractional form (that is, both the numerator and denominator of the F-statistic are reported). Then we extend the method to combine Chi-square statistics from multiply imputed datasets. Furthermore, we develop methods for two commonly applied F-tests, Welch's ANOVA and Type-III tests of fixed effects in mixed effects models, which do not have the explicit fractional form. SAS macros are also developed to facilitate applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10812v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Binhuan Wang, Yixin Fang, Man Jin</dc:creator>
    </item>
    <item>
      <title>BMRMM: An R Package for Bayesian Markov (Renewal) Mixed Models</title>
      <link>https://arxiv.org/abs/2409.10835</link>
      <description>arXiv:2409.10835v1 Announce Type: new 
Abstract: We introduce the BMRMM package implementing Bayesian inference for a class of Markov renewal mixed models which can characterize the stochastic dynamics of a collection of sequences, each comprising alternative instances of categorical states and associated continuous duration times, while being influenced by a set of exogenous factors as well as a 'random' individual. The default setting flexibly models the state transition probabilities using mixtures of Dirichlet distributions and the duration times using mixtures of gamma kernels while also allowing variable selection for both. Modeling such data using simpler Markov mixed models also remains an option, either by ignoring the duration times altogether or by replacing them with instances of an additional category obtained by discretizing them by a user-specified unit. The option is also useful when data on duration times may not be available in the first place. We demonstrate the package's utility using two data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10835v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutong Wu, Abhra Sarkar</dc:creator>
    </item>
    <item>
      <title>Calibrated Multivariate Regression with Localized PIT Mappings</title>
      <link>https://arxiv.org/abs/2409.10855</link>
      <description>arXiv:2409.10855v1 Announce Type: new 
Abstract: Calibration ensures that predicted uncertainties align with observed uncertainties. While there is an extensive literature on recalibration methods for univariate probabilistic forecasts, work on calibration for multivariate forecasts is much more limited. This paper introduces a novel post-hoc recalibration approach that addresses multivariate calibration for potentially misspecified models. Our method involves constructing local mappings between vectors of marginal probability integral transform values and the space of observations, providing a flexible and model free solution applicable to continuous, discrete, and mixed responses. We present two versions of our approach: one uses K-nearest neighbors, and the other uses normalizing flows. Each method has its own strengths in different situations. We demonstrate the effectiveness of our approach on two real data applications: recalibrating a deep neural network's currency exchange rate forecast and improving a regression model for childhood malnutrition in India for which the multivariate response has both discrete and continuous components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10855v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Kock, G. S. Rodrigues, Scott A. Sisson, Nadja Klein, David J. Nott</dc:creator>
    </item>
    <item>
      <title>Cointegrated Matrix Autoregression Models</title>
      <link>https://arxiv.org/abs/2409.10860</link>
      <description>arXiv:2409.10860v1 Announce Type: new 
Abstract: We propose a novel cointegrated autoregressive model for matrix-valued time series, with bi-linear cointegrating vectors corresponding to the rows and columns of the matrix data. Compared to the traditional cointegration analysis, our proposed matrix cointegration model better preserves the inherent structure of the data and enables corresponding interpretations. To estimate the cointegrating vectors as well as other coefficients, we introduce two types of estimators based on least squares and maximum likelihood. We investigate the asymptotic properties of the cointegrated matrix autoregressive model under the existence of trend and establish the asymptotic distributions for the cointegrating vectors, as well as other model parameters. We conduct extensive simulations to demonstrate its superior performance over traditional methods. In addition, we apply our proposed model to Fama-French portfolios and develop a effective pairs trading strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10860v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zebang Li, Han Xiao</dc:creator>
    </item>
    <item>
      <title>Comparison of g-estimation approaches for handling symptomatic medication at multiple timepoints in Alzheimer's Disease with a hypothetical strategy</title>
      <link>https://arxiv.org/abs/2409.10943</link>
      <description>arXiv:2409.10943v1 Announce Type: new 
Abstract: For handling intercurrent events in clinical trials, one of the strategies outlined in the ICH E9(R1) addendum targets the hypothetical scenario of non-occurrence of the intercurrent event. While this strategy is often implemented by setting data after the intercurrent event to missing even if they have been collected, g-estimation allows for a more efficient estimation by using the information contained in post-IE data. As the g-estimation methods have largely developed outside of randomised clinical trials, optimisations for the application in clinical trials are possible. In this work, we describe and investigate the performance of modifications to the established g-estimation methods, leveraging the assumption that some intercurrent events are expected to have the same impact on the outcome regardless of the timing of their occurrence. In a simulation study in Alzheimer disease, the modifications show a substantial efficiency advantage for the estimation of an estimand that applies the hypothetical strategy to the use of symptomatic treatment while retaining unbiasedness and adequate type I error control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10943v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Lasch, Lorenzo Guizzaro, Wen Wei Loh</dc:creator>
    </item>
    <item>
      <title>Estimation and imputation of missing data in longitudinal models with Zero-Inflated Poisson response variable</title>
      <link>https://arxiv.org/abs/2409.11040</link>
      <description>arXiv:2409.11040v1 Announce Type: new 
Abstract: This research deals with the estimation and imputation of missing data in longitudinal models with a Poisson response variable inflated with zeros. A methodology is proposed that is based on the use of maximum likelihood, assuming that data is missing at random and that there is a correlation between the response variables. In each of the times, the expectation maximization (EM) algorithm is used: in step E, a weighted regression is carried out, conditioned on the previous times that are taken as covariates. In step M, the estimation and imputation of the missing data are performed. The good performance of the methodology in different loss scenarios is demonstrated in a simulation study comparing the model only with complete data, and estimating missing data using the mode of the data of each individual. Furthermore, in a study related to the growth of corn, it is tested on real data to develop the algorithm in a practical scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11040v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>D. S. Martinez-Lobo, O. O. Melo, N. A. Cruz</dc:creator>
    </item>
    <item>
      <title>E-Values for Exponential Families: the General Case</title>
      <link>https://arxiv.org/abs/2409.11134</link>
      <description>arXiv:2409.11134v1 Announce Type: new 
Abstract: We analyze common types of e-variables and e-processes for composite exponential family nulls: the optimal e-variable based on the reverse information projection (RIPr), the conditional (COND) e-variable, and the universal inference (UI) and sequen\-tialized RIPr e-processes. We characterize the RIPr prior for simple and Bayes-mixture based alternatives, either precisely (for Gaussian nulls and alternatives) or in an approximate sense (general exponential families). We provide conditions under which the RIPr e-variable is (again exactly vs. approximately) equal to the COND e-variable. Based on these and other interrelations which we establish, we determine the e-power of the four e-statistics as a function of sample size, exactly for Gaussian and up to $o(1)$ in general. For $d$-dimensional null and alternative, the e-power of UI tends to be smaller by a term of $(d/2) \log n + O(1)$ than that of the COND e-variable, which is the clear winner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11134v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunda Hao, Peter Gr\"unwald</dc:creator>
    </item>
    <item>
      <title>Chasing Shadows: How Implausible Assumptions Skew Our Understanding of Causal Estimands</title>
      <link>https://arxiv.org/abs/2409.11162</link>
      <description>arXiv:2409.11162v1 Announce Type: new 
Abstract: The ICH E9 (R1) addendum on estimands, coupled with recent advancements in causal inference, has prompted a shift towards using model-free treatment effect estimands that are more closely aligned with the underlying scientific question. This represents a departure from traditional, model-dependent approaches where the statistical model often overshadows the inquiry itself. While this shift is a positive development, it has unintentionally led to the prioritization of an estimand's theoretical appeal over its practical learnability from data under plausible assumptions. We illustrate this by scrutinizing assumptions in the recent clinical trials literature on principal stratum estimands, demonstrating that some popular assumptions are not only implausible but often inevitably violated. We advocate for a more balanced approach to estimand formulation, one that carefully considers both the scientific relevance and the practical feasibility of estimation under realistic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11162v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stijn Vansteelandt, Kelly Van Lancker</dc:creator>
    </item>
    <item>
      <title>Poisson and Gamma Model Marginalisation and Marginal Likelihood calculation using Moment-generating Functions</title>
      <link>https://arxiv.org/abs/2409.11167</link>
      <description>arXiv:2409.11167v1 Announce Type: new 
Abstract: We present a new analytical method to derive the likelihood function that has the population of parameters marginalised out in Bayesian hierarchical models. This method is also useful to find the marginal likelihoods in Bayesian models or in random-effect linear mixed models. The key to this method is to take high-order (sometimes fractional) derivatives of the prior moment-generating function if particular existence and differentiability conditions hold.
  In particular, this analytical method assumes that the likelihood is either Poisson or gamma. Under Poisson likelihoods, the observed Poisson count determines the order of the derivative. Under gamma likelihoods, the shape parameter, which is assumed to be known, determines the order of the fractional derivative.
  We also present some examples validating this new analytical method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11167v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Siyang Li, David van Dyk, Maximilian Autenrieth</dc:creator>
    </item>
    <item>
      <title>Performance of Cross-Validated Targeted Maximum Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2409.11265</link>
      <description>arXiv:2409.11265v2 Announce Type: new 
Abstract: Background: Advanced methods for causal inference, such as targeted maximum likelihood estimation (TMLE), require certain conditions for statistical inference. However, in situations where there is not differentiability due to data sparsity or near-positivity violations, the Donsker class condition is violated. In such situations, TMLE variance can suffer from inflation of the type I error and poor coverage, leading to conservative confidence intervals. Cross-validation of the TMLE algorithm (CVTMLE) has been suggested to improve on performance compared to TMLE in settings of positivity or Donsker class violations. We aim to investigate the performance of CVTMLE compared to TMLE in various settings.
  Methods: We utilised the data-generating mechanism as described in Leger et al. (2022) to run a Monte Carlo experiment under different Donsker class violations. Then, we evaluated the respective statistical performances of TMLE and CVTMLE with different super learner libraries, with and without regression tree methods.
  Results: We found that CVTMLE vastly improves confidence interval coverage without adversely affecting bias, particularly in settings with small sample sizes and near-positivity violations. Furthermore, incorporating regression trees using standard TMLE with ensemble super learner-based initial estimates increases bias and variance leading to invalid statistical inference.
  Conclusions: It has been shown that when using CVTMLE the Donsker class condition is no longer necessary to obtain valid statistical inference when using regression trees and under either data sparsity or near-positivity violations. We show through simulations that CVTMLE is much less sensitive to the choice of the super learner library and thereby provides better estimation and inference in cases where the super learner library uses more flexible candidates and is prone to overfitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11265v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew J. Smith, Rachael V. Phillips, Camille Maringe, Miguel Angel Luque-Fernandez</dc:creator>
    </item>
    <item>
      <title>Probability-scale residuals for event-time data</title>
      <link>https://arxiv.org/abs/2409.11385</link>
      <description>arXiv:2409.11385v2 Announce Type: new 
Abstract: The probability-scale residual (PSR) is defined as $E\{sign(y, Y^*)\}$, where $y$ is the observed outcome and $Y^*$ is a random variable from the fitted distribution. The PSR is particularly useful for ordinal and censored outcomes for which fitted values are not available without additional assumptions. Previous work has defined the PSR for continuous, binary, ordinal, right-censored, and current status outcomes; however, development of the PSR has not yet been considered for data subject to general interval censoring. We develop extensions of the PSR, first to mixed-case interval-censored data, and then to data subject to several types of common censoring schemes. We derive the statistical properties of the PSR and show that our more general PSR encompasses several previously defined PSR for continuous and censored outcomes as special cases. The performance of the residual is illustrated in real data from the Caribbean, Central, and South American Network for HIV Epidemiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11385v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eric S. Kawaguchi, Bryan E. Shepherd, Chun Li</dc:creator>
    </item>
    <item>
      <title>Data-driven stochastic 3D modeling of the nanoporous binder-conductive additive phase in battery cathodes</title>
      <link>https://arxiv.org/abs/2409.11080</link>
      <description>arXiv:2409.11080v1 Announce Type: cross 
Abstract: A stochastic 3D modeling approach for the nanoporous binder-conductive additive phase in hierarchically structured cathodes of lithium-ion batteries is presented. The binder-conductive additive phase of these electrodes consists of carbon black, polyvinylidene difluoride binder and graphite particles. For its stochastic 3D modeling, a three-step procedure based on methods from stochastic geometry is used. First, the graphite particles are described by a Boolean model with ellipsoidal grains. Second, the mixture of carbon black and binder is modeled by an excursion set of a Gaussian random field in the complement of the graphite particles. Third, large pore regions within the mixture of carbon black and binder are described by a Boolean model with spherical grains. The model parameters are calibrated to 3D image data of cathodes in lithium-ion batteries acquired by focused ion beam scanning electron microscopy. Subsequently, model validation is performed by comparing model realizations with measured image data in terms of various morphological descriptors that are not used for model fitting. Finally, we use the stochastic 3D model for predictive simulations, where we generate virtual, yet realistic, image data of nanoporous binder-conductive additives with varying amounts of graphite particles. Based on these virtual nanostructures, we can investigate structure-property relationships. In particular, we quantitatively study the influence of graphite particles on effective transport properties in the nanoporous binder-conductive additive phase, which have a crucial impact on electrochemical processes in the cathode and thus on the performance of battery cells.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11080v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Phillip Gr\"afensteiner, Markus Osenberg, Andr\'e Hilger, Nicole Bohn, Joachim R. Binder, Ingo Manke, Volker Schmidt, Matthias Neumann</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction With Conditional Guarantees</title>
      <link>https://arxiv.org/abs/2305.12616</link>
      <description>arXiv:2305.12616v4 Announce Type: replace 
Abstract: We consider the problem of constructing distribution-free prediction sets with finite-sample conditional guarantees. Prior work has shown that it is impossible to provide exact conditional coverage universally in finite samples. Thus, most popular methods only guarantee marginal coverage over the covariates or are restricted to a limited set of conditional targets, e.g. coverage over a finite set of pre-specified subgroups. This paper bridges this gap by defining a spectrum of problems that interpolate between marginal and conditional validity. We motivate these problems by reformulating conditional coverage as coverage over a class of covariate shifts. When the target class of shifts is finite-dimensional, we show how to simultaneously obtain exact finite-sample coverage over all possible shifts. For example, given a collection of subgroups, our prediction sets guarantee coverage over each group. For more flexible, infinite-dimensional classes where exact coverage is impossible, we provide a procedure for quantifying the coverage errors of our algorithm. Moreover, by tuning interpretable hyperparameters, we allow the practitioner to control the size of these errors across shifts of interest. Our methods can be incorporated into existing split conformal inference pipelines, and thus can be used to quantify the uncertainty of modern black-box algorithms without distributional assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12616v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac Gibbs, John J. Cherian, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>A Robust Framework for Graph-based Two-Sample Tests Using Weights</title>
      <link>https://arxiv.org/abs/2307.12325</link>
      <description>arXiv:2307.12325v3 Announce Type: replace 
Abstract: Graph-based tests are a class of non-parametric two-sample tests useful for analyzing high-dimensional data. The framework offers both flexibility and power in a wide-range of testing scenarios. The test statistics are constructed from similarity graphs (such as K-nearest neighbor graphs) and consequently, their performance is sensitive to the structure of the graph. When the graph has problematic structures, as is common for high-dimensional data, this can result in poor or unstable performance among existing graph-based tests. We address this challenge and develop graph-based test statistics that are robust to problematic structures of the graph. The limiting null distribution of the robust test statistics is derived. We illustrate the new tests via simulation studies and a real-world application on Chicago taxi trip-data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12325v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichuan Bai, Lynna Chu</dc:creator>
    </item>
    <item>
      <title>A Unified Three-State Model Framework for Analysis of Treatment Crossover in Survival Trials</title>
      <link>https://arxiv.org/abs/2401.17008</link>
      <description>arXiv:2401.17008v2 Announce Type: replace 
Abstract: We present a unified three-state model (TSM) framework for evaluating treatment effects in clinical trials in the presence of treatment crossover. Researchers have proposed diverse methodologies to estimate the treatment effect that would have hypothetically been observed if treatment crossover had not occurred. However, there is little work on understanding the connections between these different approaches from a statistical point of view. The proposed TSM framework unifies existing methods, effectively identifying potential biases, model assumptions, and inherent limitations for each method. This can guide researchers in understanding when these methods are appropriate and choosing a suitable approach for their data. The TSM framework also facilitates the creation of new methods to adjust for confounding effects from treatment crossover. To illustrate this capability, we introduce a new imputation method that falls under its scope. Through simulation experiments, we demonstrate the performance of different approaches for estimating the treatment effects. Codes for implementing the methods within the TSM framework are available at https://github.com/JasonZhao111/TSM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17008v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zile Zhao, Ye Li, Xiaodong Luo, Ray Bai</dc:creator>
    </item>
    <item>
      <title>Bipartite causal inference with interference, time series data, and a random network</title>
      <link>https://arxiv.org/abs/2404.04775</link>
      <description>arXiv:2404.04775v2 Announce Type: replace 
Abstract: In bipartite causal inference with interference there are two distinct sets of units: those that receive the treatment, termed interventional units, and those on which the outcome is measured, termed outcome units. Which interventional units' treatment can drive which outcome units' outcomes is often depicted in a bipartite network. We study bipartite causal inference with interference from observational data across time and with a changing bipartite network. Under an exposure mapping framework, we define causal effects specific to each outcome unit, representing average contrasts of potential outcomes across time. We establish unconfoundedness of the exposure received by the outcome units based on unconfoundedness assumptions on the interventional units' treatment assignment and the random graph, hence respecting the bipartite structure of the problem. By harvesting the time component of our setting, causal effects are estimable while controlling only for temporal trends and time-varying confounders. Our results hold for binary, continuous, and multivariate exposure mappings. In the case of a binary exposure, we propose three matching algorithms to estimate the causal effect based on matching exposed to unexposed time periods for the same outcome unit, and we show that the bias of the resulting estimators is bounded. We illustrate our approach with an extensive simulation study and an application on the effect of wildfire smoke on transportation by bicycle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04775v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhaoyan Song, Georgia Papadogeorgou</dc:creator>
    </item>
    <item>
      <title>Statistical Jump Model for Mixed-Type Data with Missing Data Imputation</title>
      <link>https://arxiv.org/abs/2409.01208</link>
      <description>arXiv:2409.01208v2 Announce Type: replace 
Abstract: In this paper, we address the challenge of clustering mixed-type data with temporal evolution by introducing the statistical jump model for mixed-type data. This novel framework incorporates regime persistence, enhancing interpretability and reducing the frequency of state switches, and efficiently handles missing data. The model is easily interpretable through its state-conditional means and modes, making it accessible to practitioners and policymakers. We validate our approach through extensive simulation studies and an empirical application to air quality data, demonstrating its superiority in inferring persistent air quality regimes compared to the traditional air quality index. Our contributions include a robust method for mixed-type temporal clustering, effective missing data management, and practical insights for environmental monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01208v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico P. Cortese, Antonio Pievatolo</dc:creator>
    </item>
    <item>
      <title>Marginal Structural Modeling of Representative Treatment Trajectories</title>
      <link>https://arxiv.org/abs/2409.04933</link>
      <description>arXiv:2409.04933v2 Announce Type: replace 
Abstract: Marginal structural models (MSMs) are widely used in observational studies to estimate the causal effect of time-varying treatments. Despite its popularity, limited attention has been paid to summarizing the treatment history in the outcome model, which proves particularly challenging when individuals' treatment trajectories exhibit complex patterns over time. Commonly used metrics such as the average treatment level fail to adequately capture the treatment history, hindering causal interpretation. For scenarios where treatment histories exhibit distinct temporal patterns, we develop a new approach to parameterize the outcome model. We apply latent growth curve analysis to identify representative treatment trajectories from the observed data and use the posterior probability of latent class membership to summarize the different treatment trajectories. We demonstrate its use in parameterizing the MSMs, which facilitates the interpretations of the results. We apply the method to analyze data from an existing cohort of lung transplant recipients to estimate the effect of Tacrolimus concentrations on the risk of incident chronic kidney disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04933v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiewen Liu, Todd A. Miano, Stephen Griffiths, Michael G. S. Shashaty, Wei Yang</dc:creator>
    </item>
    <item>
      <title>Why you should also use OLS estimation of tail exponents</title>
      <link>https://arxiv.org/abs/2409.10448</link>
      <description>arXiv:2409.10448v2 Announce Type: replace 
Abstract: Even though practitioners often estimate Pareto exponents running OLS rank-size regressions, the usual recommendation is to use the Hill MLE with a small-sample correction instead, due to its unbiasedness and efficiency. In this paper, we advocate that you should also apply OLS in empirical applications. On the one hand, we demonstrate that, with a small-sample correction, the OLS estimator is also unbiased. On the other hand, we show that the MLE assigns significantly greater weight to smaller observations. This suggests that the OLS estimator may outperform the MLE in cases where the distribution is (i) strictly Pareto but only in the upper tail or (ii) regularly varying rather than strictly Pareto. We substantiate our theoretical findings with Monte Carlo simulations and real-world applications, demonstrating the practical relevance of the OLS method in estimating tail exponents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10448v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thiago Trafane Oliveira Santos (Central Bank of Brazil, Bras\'ilia, Brazil. Department of %Economics, University of Brasilia, Brazil), Daniel Oliveira Cajueiro (Department of Economics, University of Brasilia, Brazil. National Institute of Science and Technology for Complex Systems)</dc:creator>
    </item>
    <item>
      <title>A unified analysis of likelihood-based estimators in the Plackett--Luce model</title>
      <link>https://arxiv.org/abs/2306.02821</link>
      <description>arXiv:2306.02821v3 Announce Type: replace-cross 
Abstract: The Plackett--Luce model has been extensively used for rank aggregation in social choice theory. A central question in this model concerns estimating the utility vector that governs the model's likelihood. In this paper, we investigate the asymptotic theory of utility vector estimation by maximizing different types of likelihood, such as full, marginal, and quasi-likelihood. Starting from interpreting the estimating equations of these estimators to gain some initial insights, we analyze their asymptotic behavior as the number of compared objects increases. In particular, we establish both the uniform consistency and asymptotic normality of these estimators and discuss the trade-off between statistical efficiency and computational complexity. For generality, our results are proven for deterministic graph sequences under appropriate graph topology conditions. These conditions are shown to be revealing and sharp when applied to common sampling scenarios, such as nonuniform random hypergraph models and hypergraph stochastic block models. Numerical results are provided to support our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.02821v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruijian Han, Yiming Xu</dc:creator>
    </item>
    <item>
      <title>False discovery proportion envelopes with m-consistency</title>
      <link>https://arxiv.org/abs/2306.07819</link>
      <description>arXiv:2306.07819v2 Announce Type: replace-cross 
Abstract: We provide new non-asymptotic false discovery proportion (FDP) confidence envelopes in several multiple testing settings relevant for modern high dimensional-data methods. We revisit the multiple testing scenarios considered in the recent work of Katsevich and Ramdas (2020): top-$k$, preordered (including knockoffs), online. Our emphasis is on obtaining FDP confidence bounds that both have non-asymptotic coverage and are asymptotically accurate in a specific sense, as the number $m$ of tested hypotheses grows. Namely, we introduce and study the property (which we call $m$-consistency) that the confidence bound converges to or below the desired level $\alpha$ when applied to a specific reference $\alpha$-level false discovery rate (FDR) controlling procedure. In this perspective, we derive new bounds that provide improvements over existing ones, both theoretically and practically, and are suitable for situations where at least a moderate number of rejections is expected. These improvements are illustrated with numerical experiments and real data examples. In particular, the improvement is significant in the knockoffs setting, which shows the impact of the method for a practical use. As side results, we introduce a new confidence envelope for the empirical cumulative distribution function of i.i.d. uniform variables, and we provide new power results in sparse cases, both being of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07819v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iqraa Meah, Gilles Blanchard, Etienne Roquain</dc:creator>
    </item>
  </channel>
</rss>
