<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jun 2025 04:05:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Proof of The TAP Free Energy for High-Dimensional Linear Regression with Spherical Priors at All Temperatures</title>
      <link>https://arxiv.org/abs/2506.20768</link>
      <description>arXiv:2506.20768v1 Announce Type: new 
Abstract: Variational inference (VI) has emerged as an efficient framework for approximating intractable posteriors. Traditional mean-field VI methods fail to capture the complex dependencies and can be inconsistent in high-dimensional settings. In contrast, it has been conjectured, based on statistical physics heuristics and conditions for the stationary points, that the more refined Thouless-Anderson-Palmer (TAP) approximation is consistent in the high-dimensional regimes. Existing results, however, have only proved such TAP representations in high-temperature regimes, even in the basic setting of spherical priors (Qiu and Sen, Ann. Appl. Probab. 2023). In this work, we prove the asymptotic correctness of TAP free energy approximation for Bayesian linear regression under uniform spherical priors at any temperature ($\Delta&gt;0$). The key idea in our proof is to find a ridge regression functional that dominates the TAP free energy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20768v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyuan Yu, Jingbo Liu</dc:creator>
    </item>
    <item>
      <title>Prokhorov Metric Convergence of the Partial Sum Process for Reconstructed Functional Data</title>
      <link>https://arxiv.org/abs/2506.21172</link>
      <description>arXiv:2506.21172v1 Announce Type: new 
Abstract: Motivated by applications in functional data analysis, we study the partial sum process of sparsely observed, random functions. A key novelty of our analysis are bounds for the distributional distance between the limit Brownian motion and the entire partial sum process in the function space. To measure the distance between distributions, we employ the Prokhorov and Wasserstein metrics. We show that these bounds have important probabilistic implications, including strong invariance principles and new couplings between the partial sums and their Gaussian limits. Our results are formulated for weakly dependent, nonstationary time series in the Banach space of d-dimensional, continuous functions. Mathematically, our approach rests on a new, two-step proof strategy: First, using entropy bounds from empirical process theory, we replace the function-valued partial sum process by a high-dimensional discretization. Second, using Gaussian approximations for weakly dependent, high-dimensional vectors, we obtain bounds on the distance. As a statistical application of our coupling results, we validate an open-ended monitoring scheme for sparse functional data. Existing probabilistic tools were not appropriate for this task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21172v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Kutta, Piotr Kokoszka</dc:creator>
    </item>
    <item>
      <title>Bayes linear estimator in the general linear model</title>
      <link>https://arxiv.org/abs/2506.21192</link>
      <description>arXiv:2506.21192v1 Announce Type: new 
Abstract: Bayes linear estimators are derived by minimizing the Bayes risk with respect to the squared loss function. Non-unbiased estimators such as an ordinary ridge estimator, a typical shrinkage estimator, and a fractional rank estimator are either Bayes linear estimators or their limit points. First, we prove that Bayes linear estimators are linearly complete, but not necessarily linearly sufficient. Second, we derive a necessary and sufficient condition under which two Bayes linear estimators coincide. Finally, we obtain an equivalence condition such that the equality between two residual sums of squares holds when Bayes linear estimators are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21192v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hirai Mukasa</dc:creator>
    </item>
    <item>
      <title>Causal inference via implied interventions</title>
      <link>https://arxiv.org/abs/2506.21501</link>
      <description>arXiv:2506.21501v1 Announce Type: new 
Abstract: In the context of having an instrumental variable, the standard practice in causal inference begins by targeting an effect of interest and proceeds by formulating assumptions enabling identification of this effect. We turn this around by simply not making assumptions anymore and just adhere to the interventions we can identify, rather than starting with a desired causal estimand and imposing untestable hypotheses. The randomization of an instrument and its exclusion restriction define a class of auxiliary stochastic interventions on the treatment that are implied by stochastic interventions on the instrument. This mapping effectively characterizes the identifiable causal effects of the treatment on the outcome given the observable probability distribution, leading to an explicit transparent G-computation formula under hidden confounding. Alternatively, searching for an intervention on the instrument whose implied one best approximates a desired target -- whose causal effect the user aims to estimate -- naturally leads to a projection on a function space representing the closest identifiable treatment effect. The generality of this projection allows to select different norms and indexing sets for the function class that turn optimization into different estimation procedures with the Highly Adaptive Lasso. This shift from identification under assumptions to identification under observation redefines how the problem of causal inference is approached.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21501v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Garc\'ia Meixide, Mark J. van der Laan</dc:creator>
    </item>
    <item>
      <title>Robust Alignment via Partial Gromov-Wasserstein Distances</title>
      <link>https://arxiv.org/abs/2506.21507</link>
      <description>arXiv:2506.21507v1 Announce Type: new 
Abstract: The Gromov-Wasserstein (GW) problem provides a powerful framework for aligning heterogeneous datasets by matching their internal structures in a way that minimizes distortion. However, GW alignment is sensitive to data contamination by outliers, which can greatly distort the resulting matching scheme. To address this issue, we study robust GW alignment, where upon observing contaminated versions of the clean data distributions, our goal is to accurately estimate the GW alignment cost between the original (uncontaminated) measures. We propose an estimator based on the partial GW distance, which trims out a fraction of the mass from each distribution before optimally aligning the rest. The estimator is shown to be minimax optimal in the population setting and is near-optimal in the finite-sample regime, where the optimality gap originates only from the suboptimality of the plug-in estimator in the empirical estimation setting (i.e., without contamination). Towards the analysis, we derive new structural results pertaining to the approximate pseudo-metric structure of the partial GW distance. Overall, our results endow the partial GW distance with an operational meaning by posing it as a robust surrogate of the classical distance when the observed data may be contaminated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21507v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyun Gong, Sloan Nietert, Ziv Goldfeld</dc:creator>
    </item>
    <item>
      <title>Asymptotic Inference for Exchangeable Gibbs Partition</title>
      <link>https://arxiv.org/abs/2506.21527</link>
      <description>arXiv:2506.21527v1 Announce Type: new 
Abstract: We study the asymptotic properties of parameter estimation and predictive inference under the exchangeable Gibbs partition, characterized by a discount parameter $\alpha\in(0,1)$ and a triangular array $v_{n,k}$ satisfying a backward recursion. Assuming that $v_{n,k}$ admits a mixture representation over the Ewens--Pitman family $(\alpha, \theta)$, with $\theta$ integrated by an unknown mixing distribution, we show that the (quasi) maximum likelihood estimator $\hat\alpha_n$ (QMLE) for $\alpha$ is asymptotically mixed normal. This generalizes earlier results for the Ewens--Pitman model to a more general class. We further study the predictive task of estimating the probability simplex $\mathsf{p}_n$, which governs the allocation of the $(n+1)$-th item, conditional on the current partition of $[n]$. Based on the asymptotics of the QMLE $\hat{\alpha}_n$, we construct an estimator $\hat{\mathsf{p}}_n$ and derive the limit distributions of the $f$-divergence $\mathsf{D}_f(\hat{\mathsf{p}}_n||\mathsf{p}_n)$ for general convex functions $f$, including explicit results for the TV distance and KL divergence. These results lead to asymptotically valid confidence intervals for both parameter estimation and prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21527v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Koriyama</dc:creator>
    </item>
    <item>
      <title>Detecting weighted hidden cliques</title>
      <link>https://arxiv.org/abs/2506.21543</link>
      <description>arXiv:2506.21543v1 Announce Type: new 
Abstract: We study a generalization of the classical hidden clique problem to graphs with real-valued edge weights. Formally, we define a hypothesis testing problem. Under the null hypothesis, edges of a complete graph on $n$ vertices are associated with independent and identically distributed edge weights from a distribution $P$. Under the alternate hypothesis, $k$ vertices are chosen at random and the edge weights between them are drawn from a distribution $Q$, while the remaining are sampled from $P$. The goal is to decide, upon observing the edge weights, which of the two hypotheses they were generated from. We investigate the problem under two different scenarios: (1) when $P$ and $Q$ are completely known, and (2) when there is only partial information of $P$ and $Q$. In the first scenario, we obtain statistical limits on $k$ when the two hypotheses are distinguishable, and when they are not. Additionally, in each of the scenarios, we provide bounds on the minimal risk of the hypothesis testing problem when $Q$ is not absolutely continuous with respect to $P$. We also provide computationally efficient spectral tests that can distinguish the two hypotheses as long as $k=\Omega(\sqrt{n})$ in both the scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21543v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Urmisha Chatterjee, Karissa Huang, Ritabrata Karmakar, B. R. Vinay Kumar, G\'abor Lugosi, Nandan Malhotra, Anirban Mandal, Maruf Alam Tarafdar</dc:creator>
    </item>
    <item>
      <title>The final solution of the Hitchhiker's problem #5</title>
      <link>https://arxiv.org/abs/2506.20672</link>
      <description>arXiv:2506.20672v1 Announce Type: cross 
Abstract: A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R. Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the dependence modeling community in spite of the lack of statistical interpretation of quasi-copulas. In our previous work (arXiv:2410.19339, accepted in Fuzzy Sets and Systems), we addressed the question of extreme values of the mass distribution associated with multivariate quasi-copulas. Using a linear programming approach, we were able to solve Open Problem 5 of the "Guide" up to dimension d = 17 and disprove a recent conjecture on the solution to that problem. In this paper, we use an analytical approach to provide a complete answer to the original question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20672v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matja\v{z} Omladi\v{c}, Martin Vuk, Alja\v{z} Zalar</dc:creator>
    </item>
    <item>
      <title>Central limit theory for Peaks-over-Threshold partial sums of long memory linear time series</title>
      <link>https://arxiv.org/abs/2506.20789</link>
      <description>arXiv:2506.20789v1 Announce Type: cross 
Abstract: Over the last 30 years, extensive work has been devoted to developing central limit theory for partial sums of subordinated long memory linear time series. A much less studied problem, motivated by questions that are ubiquitous in extreme value theory, is the asymptotic behavior of such partial sums when the subordination mechanism has a threshold depending on sample size, so as to focus on the right tail of the time series. This article substantially extends longstanding asymptotic techniques by allowing the subordination mechanism to depend on the sample size in this way and to grow at a polynomial rate, while permitting the innovation process to have infinite variance. The cornerstone of our theoretical approach is a tailored $L^r(\mathbf{P})$ reduction principle, which enables the use of classical results on partial sums of long memory linear processes. In this way we obtain asymptotic theory for certain Peaks-over-Threshold estimators with deterministic or random thresholds. Applications comprise both the heavy- and light-tailed regimes -- yielding unexpected results which, to the best of our knowledge, are new to the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20789v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ioan Scheffel, Marco Oesting, Gilles Stupfler</dc:creator>
    </item>
    <item>
      <title>Lower Bounds on the Size of Markov Equivalence Classes</title>
      <link>https://arxiv.org/abs/2506.20933</link>
      <description>arXiv:2506.20933v1 Announce Type: cross 
Abstract: Causal discovery algorithms typically recover causal graphs only up to their Markov equivalence classes unless additional parametric assumptions are made. The sizes of these equivalence classes reflect the limits of what can be learned about the underlying causal graph from purely observational data. Under the assumptions of acyclicity, causal sufficiency, and a uniform model prior, Markov equivalence classes are known to be small on average. In this paper, we show that this is no longer the case when any of these assumptions is relaxed. Specifically, we prove exponentially large lower bounds for the expected size of Markov equivalence classes in three settings: sparse random directed acyclic graphs, uniformly random acyclic directed mixed graphs, and uniformly random directed cyclic graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20933v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik Jahn, Frederick Eberhardt, Leonard J. Schulman</dc:creator>
    </item>
    <item>
      <title>Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution</title>
      <link>https://arxiv.org/abs/2506.21278</link>
      <description>arXiv:2506.21278v1 Announce Type: cross 
Abstract: We propose a novel variational autoencoder (VAE) architecture that employs a spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy provides a more natural hyperspherical representation of latent variables, better capturing directional data while maintaining flexibility. Its heavy-tailed nature prevents over-regularization, ensuring efficient latent space utilization while offering a more expressive representation. Additionally, spCauchy circumvents the numerical instabilities inherent to vMF, which arise from computing normalization constants involving Bessel functions. Instead, it enables a fully differentiable and efficient reparameterization trick via M\"obius transformations, allowing for stable and scalable training. The KL divergence can be computed through a rapidly converging power series, eliminating concerns of underflow or overflow associated with evaluation of ratios of hypergeometric functions. These properties make spCauchy a compelling alternative for VAEs, offering both theoretical advantages and practical efficiency in high-dimensional generative modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21278v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Sablica, Kurt Hornik</dc:creator>
    </item>
    <item>
      <title>Wild refitting for black box prediction</title>
      <link>https://arxiv.org/abs/2506.21460</link>
      <description>arXiv:2506.21460v1 Announce Type: cross 
Abstract: We describe and analyze a computionally efficient refitting procedure for computing high-probability upper bounds on the instance-wise mean-squared prediction error of penalized nonparametric estimates based on least-squares minimization. Requiring only a single dataset and black box access to the prediction method, it consists of three steps: computing suitable residuals, symmetrizing and scaling them with a pre-factor $\rho$, and using them to define and solve a modified prediction problem recentered at the current estimate. We refer to it as wild refitting, since it uses Rademacher residual symmetrization as in a wild bootstrap variant. Under relatively mild conditions allowing for noise heterogeneity, we establish a high probability guarantee on its performance, showing that the wild refit with a suitably chosen wild noise scale $\rho$ gives an upper bound on prediction error. This theoretical analysis provides guidance into the design of such procedures, including how the residuals should be formed, the amount of noise rescaling in the wild sub-problem needed for upper bounds, and the local stability properties of the block-box procedure. We illustrate the applicability of this procedure to various problems, including non-rigid structure-from-motion recovery with structured matrix penalties; plug-and-play image restoration with deep neural network priors; and randomized sketching with kernel methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21460v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin J. Wainwright</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the diffusion coefficient from S.D.E. paths</title>
      <link>https://arxiv.org/abs/2307.03960</link>
      <description>arXiv:2307.03960v4 Announce Type: replace 
Abstract: Consider a diffusion process X=(X_t), with t in [0,1], observed at discrete times and high frequency, solution of a stochastic differential equation whose drift and diffusion coefficients are assumed to be unknown. In this article, we focus on the nonparametric esstimation of the diffusion coefficient. We propose ridge estimators of the square of the diffusion coefficient from discrete observations of X and that are obtained by minimization of the least squares contrast. We prove that the estimators are consistent and derive rates of convergence as the size of the sample paths tends to infinity, and the discretization step of the time interval [0,1] tend to zero. The theoretical results are completed with a numerical study over synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03960v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Eddy Ella-Mintsa</dc:creator>
    </item>
    <item>
      <title>Likelihood ratio tests in random graph models with increasing dimensions</title>
      <link>https://arxiv.org/abs/2311.05806</link>
      <description>arXiv:2311.05806v3 Announce Type: replace 
Abstract: We explore the Wilks phenomena in two random graph models: the $\beta$-model and the Bradley-Terry model. For two increasing dimensional null hypotheses, including a specified null $H_0: \beta_i=\beta_i^0$ for $i=1,\ldots, r$ and a homogenous null $H_0: \beta_1=\cdots=\beta_r$, we reveal high dimensional Wilks' phenomena that the normalized log-likelihood ratio statistic, $[2\{\ell(\widehat{\mathbf{\beta}}) - \ell(\widehat{\mathbf{\beta}}^0)\} - r]/(2r)^{1/2}$, converges in distribution to the standard normal distribution as $r$ goes to infinity. Here, $\ell( \mathbf{\beta})$ is the log-likelihood function on the model parameter $\mathbf{\beta}=(\beta_1, \ldots, \beta_n)^\top$, $\widehat{\mathbf{\beta}}$ is its maximum likelihood estimator (MLE) under the full parameter space, and $\widehat{\mathbf{\beta}}^0$ is the restricted MLE under the null parameter space. For the homogenous null with a fixed $r$, we establish Wilks-type theorems that $2\{\ell(\widehat{\mathbf{\beta}}) - \ell(\widehat{\mathbf{\beta}}^0)\}$ converges in distribution to a chi-square distribution with $r-1$ degrees of freedom, as the total number of parameters, $n$, goes to infinity. When testing the fixed dimensional specified null, we find that its asymptotic null distribution is a chi-square distribution in the $\beta$-model. However, unexpectedly, this is not true in the Bradley-Terry model. By developing several novel technical methods for asymptotic expansion, we explore Wilks type results in a principled manner; these principled methods should be applicable to a class of random graph models beyond the $\beta$-model and the Bradley-Terry model. Simulation studies and real network data applications further demonstrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05806v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting Yan, Yuanzhang Li, Jinfeng Xu, Yaning Yang, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Consistency of MLE in partially observed diffusion models on a torus</title>
      <link>https://arxiv.org/abs/2412.03380</link>
      <description>arXiv:2412.03380v4 Announce Type: replace 
Abstract: In this paper, we consider a general partially observed diffusion model with periodic coefficients and with non-degenerate diffusion component. The coefficients of such a model depend on an unknown (static and deterministic) parameter which needs to be estimated based on the observed component of the diffusion process. We show that, given enough regularity of the diffusion coefficients, a maximum likelihood estimator of the unknown parameter converges to the true parameter value as the sample size grows to infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03380v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ibrahim Ekren, Sergey Nadtochiy</dc:creator>
    </item>
    <item>
      <title>L-2 Regularized maximum likelihood for $\beta$-model in large and sparse networks</title>
      <link>https://arxiv.org/abs/2110.11856</link>
      <description>arXiv:2110.11856v5 Announce Type: replace-cross 
Abstract: The $\beta$-model is a powerful tool for modeling large and sparse networks driven by degree heterogeneity, where many network models become infeasible due to computational challenge and network sparsity. However, existing estimation algorithms for $\beta$-model do not scale up. Also, theoretical understandings remain limited to dense networks. This paper brings several significant improvements over existing results to address the urgent needs of practice. We propose a new $\ell_2$-penalized MLE algorithm that can comfortably handle sparse networks of millions of nodes with much-improved memory parsimony. We establish the first rate-optimal error bounds and high-dimensional asymptotic normality results for $\beta$-models, under much weaker network sparsity assumptions than best existing results.
  Application of our method to large COVID-19 network data sets discovered meaningful results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.11856v5</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Meijia Shao, Yu Zhang, Qiuping Wang, Yuan Zhang, Jing Luo, Ting Yan</dc:creator>
    </item>
    <item>
      <title>Correlation functions between singular values and eigenvalues</title>
      <link>https://arxiv.org/abs/2403.19157</link>
      <description>arXiv:2403.19157v4 Announce Type: replace-cross 
Abstract: Exploiting the explicit bijection between the density of singular values and the density of eigenvalues for bi-unitarily invariant complex random matrix ensembles of finite matrix size, we aim at finding the induced probability measure on $j$ eigenvalues and $k$ singular values that we coin $j,k$-point correlation measure. We find an expression for the $1,k$-point correlation measure which simplifies drastically when assuming that the singular values follow a polynomial ensemble, yielding a closed formula in terms of the kernel corresponding to the determinantal point process of the singular value statistics. These expressions simplify even further when the singular values are drawn from a P\'{o}lya ensemble and extend known results between the eigenvalue and singular value statistics of the corresponding bi-unitarily invariant ensemble.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19157v4</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Allard, Mario Kieburg</dc:creator>
    </item>
    <item>
      <title>On the robustness of semi-discrete optimal transport</title>
      <link>https://arxiv.org/abs/2410.19596</link>
      <description>arXiv:2410.19596v2 Announce Type: replace-cross 
Abstract: We derive the breakdown point for solutions of semi-discrete optimal transport problems, which characterizes the robustness of the multivariate quantiles based on optimal transport proposed in \cite{GS}. We do so under very mild assumptions: the absolutely continuous reference measure is only assumed to have a support that is \textcolor{mygreen}{convex}, whereas the target measure is a general discrete measure on a finite number, $n$ say, of atoms. The breakdown point depends on the target measure only through its probability weights (hence not on the location of the atoms) and involves the geometry of the reference measure through the \cite{Tuk1975} concept of halfspace depth. Remarkably, depending on this geometry, the breakdown point of the optimal transport median can be strictly smaller than the breakdown point of the univariate median or the breakdown point of the spatial median, namely~$\lceil n/2\rceil /2$. In the context of robust location estimation, our results provide a subtle insight on how to perform multivariate trimming when constructing trimmed means based on optimal transport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19596v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davy Paindaveine, Riccardo Passeggeri</dc:creator>
    </item>
    <item>
      <title>No-prior Bayesian inference reIMagined: probabilistic approximations of inferential models</title>
      <link>https://arxiv.org/abs/2503.19748</link>
      <description>arXiv:2503.19748v2 Announce Type: replace-cross 
Abstract: When prior information is lacking, the go-to strategy for probabilistic inference is to combine a "default prior" and the likelihood via Bayes's theorem. Objective Bayes, (generalized) fiducial inference, etc. fall under this umbrella. This construction is natural, but the corresponding posterior distributions generally only offer limited, approximately valid uncertainty quantification. The present paper takes a reimagined approach offering posterior distributions with stronger reliability properties. The proposed construction starts with an inferential model (IM), one that takes the mathematical form of a data-driven possibility measure and features exactly valid uncertainty quantification, and then returns a so-called inner probabilistic approximation thereof. This inner probabilistic approximation inherits many of the original IM's desirable properties, including credible sets with exact coverage and asymptotic efficiency. The approximation also agrees with the familiar Bayes/fiducial solution obtained in applications where the model has a group transformation structure. A Monte Carlo method for evaluating the probabilistic approximation is presented, along with numerical illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19748v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Sharp concentration of uniform generalization errors in binary linear classification</title>
      <link>https://arxiv.org/abs/2505.16713</link>
      <description>arXiv:2505.16713v2 Announce Type: replace-cross 
Abstract: We examine the concentration of uniform generalization errors around their expectation in binary linear classification problems via an isoperimetric argument. In particular, we establish Poincar\'{e} and log-Sobolev inequalities for the joint distribution of the output labels and the label-weighted input vectors, which we apply to derive concentration bounds. The derived concentration bounds are sharp up to moderate multiplicative constants by those under well-balanced labels. In asymptotic analysis, we also show that almost sure convergence of uniform generalization errors to their expectation occurs in very broad settings, such as proportionally high-dimensional regimes. Using this convergence, we establish uniform laws of large numbers under dimension-free conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16713v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Nakakita</dc:creator>
    </item>
    <item>
      <title>An introduction to Causal Modelling</title>
      <link>https://arxiv.org/abs/2506.16486</link>
      <description>arXiv:2506.16486v2 Announce Type: replace-cross 
Abstract: This tutorial provides a concise introduction to modern causal modeling by integrating potential outcomes and graphical methods. We motivate causal questions such as counterfactual reasoning under interventions and define binary treatments and potential outcomes. We discuss causal effect measures-including average treatment effects on the treated and on the untreated-and choices of effect scales for binary outcomes. We derive identification in randomized experiments under exchangeability and consistency, and extend to stratification and blocking designs. We present inverse probability weighting with propensity score estimation and robust inference via sandwich estimators. Finally, we introduce causal graphs, d-separation, the backdoor criterion, single-world intervention graphs, and structural equation models, showing how graphical and potential-outcome approaches complement each other. Emphasis is placed on clear notation, intuitive explanations, and practical examples for applied researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16486v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gauranga Kumar Baishya</dc:creator>
    </item>
  </channel>
</rss>
