<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 May 2025 01:43:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Snakemaker: Seamlessly transforming ad-hoc analyses into sustainable Snakemake workflows with generative AI</title>
      <link>https://arxiv.org/abs/2505.02841</link>
      <description>arXiv:2505.02841v1 Announce Type: new 
Abstract: Reproducibility and sustainability present significant challenges in bioinformatics software development, where rapidly evolving tools and complex workflows often result in short-lived or difficult-to-adapt pipelines. This paper introduces Snakemaker, a tool that leverages generative AI to facilitate researchers build sustainable data analysis pipelines by converting unstructured code into well-defined Snakemake workflows. Snakemaker non-invasively tracks the work performed in the terminal by the researcher, analyzes execution patterns, and generates Snakemake workflows that can be integrated into existing pipelines. Snakemaker also supports the transformation of monolithic Ipython Notebooks into modular Snakemake pipelines, resolving the global state of the notebook into discrete, file-based interactions between rules. An integrated chat assistant provides users with fine-grained control through natural language instructions. Snakemaker generates high-quality Snakemake workflows by adhering to the best practices, including Conda environment tracking, generic rule generation and loop unrolling. By lowering the barrier between prototype and production-quality code, Snakemaker addresses a critical gap in computational reproducibility for bioinformatics research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02841v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marco Masera, Alessandro Leone, Johannes K\"oster, Ivan Molineris</dc:creator>
    </item>
    <item>
      <title>Proceedings of the First International Workshop on Autonomous Systems Quality Assurance and Prediction with Digital Twins</title>
      <link>https://arxiv.org/abs/2505.02873</link>
      <description>arXiv:2505.02873v1 Announce Type: new 
Abstract: This volume contains the proceedings of the First International Workshop on Autonomous Systems Quality Assurance and Prediction with Digital Twins (ASQAP 2025), which was held in Hamilton, Canada, on May 4th, 2025, as a satellite event of ETAPS 2025. The aim of ASQAP 2025 is to gather experts from academia and industry to explore the potential of digital twin technology in supporting quality assurance in autonomous systems, including concepts such as specification, verification, validation, testing, analysis, and many others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02873v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.418</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 418, 2025</arxiv:journal_reference>
      <dc:creator>Marsha Chechik (University of Toronto), Arianna Fedeli (Gran Sasso Science Institute), Gianluca Filippone (Gran Sasso Science Institute), Federico Formica (McMaster University), Mirgita Frasheri (Aarhus University), Nico Hochgeschwender (University of Bremen), Lina Marsso (Polytechnique Montreal)</dc:creator>
    </item>
    <item>
      <title>SynQ: An Embedded DSL for Synchronous System Design with Quantitative Types</title>
      <link>https://arxiv.org/abs/2505.02883</link>
      <description>arXiv:2505.02883v1 Announce Type: new 
Abstract: System design automation aims to manage the design of embedded systems with ever-increasing complexity. To the success of system design automation, there is still a lack of systematic and formal design process because an entire design process, from a system's specification to its implementation, has to deal with inherent concerns about the systems' different aspects and, consequently, inherent semantic gaps. These gaps make it hard for a design process to be traceable or transparent. Particularly, guaranteeing the correctness of produced implementations becomes the main challenge for a system design process.
  SynQ (Synchronous system design with Quantitative types) is an embedded domain specification language (EDSL) targeting the design of systems obeying the perfect synchrony hypothesis. SynQ is based on a component-based design framework and, by design, facilitates semantic coherency by leveraging the quantitative type theory (QTT) and language embedding. SynQ enables a semantically coherent design process, including formal specification and verification, modelling, simulation and code generation. This paper presents SynQ and its underlying formalism and demonstrates its features and potential for semantically coherent system design through a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02883v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Chen, Ingo Sander</dc:creator>
    </item>
    <item>
      <title>The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models</title>
      <link>https://arxiv.org/abs/2505.02931</link>
      <description>arXiv:2505.02931v1 Announce Type: new 
Abstract: Automatic program repair (APR) aims to reduce the manual efforts required to identify and fix errors in source code. Before the rise of LLM-based agents, a common strategy was to increase the number of generated patches, sometimes to the thousands, to achieve better repair results on benchmarks. More recently, self-iterative capabilities enabled LLMs to refine patches over multiple rounds guided by feedback. However, literature often focuses on many iterations and disregards different numbers of outputs.
  We investigate an APR pipeline that balances these two approaches, the generation of multiple outputs and multiple rounds of iteration, while imposing a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs - DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR task. We further fine-tune each model on an APR dataset with three sizes (1K, 30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.
  Our results show that by using only a fraction (&lt;1%) of the fine-tuning dataset, we can achieve improvements of up to 78% in the number of plausible patches generated, challenging prior studies that reported limited gains using Full Fine-Tuning. However, we find that exceeding certain thresholds leads to diminishing outcomes, likely due to overfitting. Moreover, we show that base models greatly benefit from creating patches in an iterative fashion rather than generating them all at once. In addition, the benefit of iterative strategies becomes more pronounced in complex benchmarks. Even fine-tuned models, while benefiting less from iterations, still gain advantages, particularly on complex benchmarks. The research underscores the need for balanced APR strategies that combine multi-output generation and iterative refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02931v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Vallecillos Ruiz, Max Hort, Leon Moonen</dc:creator>
    </item>
    <item>
      <title>Can We Recycle Our Old Models? An Empirical Evaluation of Model Selection Mechanisms for AIOps Solutions</title>
      <link>https://arxiv.org/abs/2505.02961</link>
      <description>arXiv:2505.02961v1 Announce Type: new 
Abstract: AIOps (Artificial Intelligence for IT Operations) solutions leverage the tremendous amount of data produced during the operation of large-scale systems and machine learning models to assist software practitioners in their system operations. Existing AIOps solutions usually maintain AIOps models against concept drift through periodical retraining, despite leaving a pile of discarded historical models that may perform well on specific future data. Other prior works propose dynamically selecting models for prediction tasks from a set of candidate models to optimize the model performance. However, there is no prior work in the AIOps area that assesses the use of model selection mechanisms on historical models to improve model performance or robustness. To fill the gap, we evaluate several model selection mechanisms by assessing their capabilities in selecting the optimal AIOps models that were built in the past to make predictions for the target data. We performed a case study on three large-scale public operation datasets: two trace datasets from the cloud computing platforms of Google and Alibaba, and one disk stats dataset from the BackBlaze cloud storage data center. We observe that the model selection mechnisms utilizing temporal adjacency tend to have a better performance and can prevail the periodical retraining approach. Our findings also highlight a performance gap between existing model selection mechnisms and the theoretical upper bound which may motivate future researchers and practitioners in investigating more efficient and effective model selection mechanisms that fit in the context of AIOps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02961v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yingzhe Lyu, Hao Li, Heng Li, Ahmed E. Hassan</dc:creator>
    </item>
    <item>
      <title>Testing SSD Firmware with State Data-Aware Fuzzing: Accelerating Coverage in Nondeterministic I/O Environments</title>
      <link>https://arxiv.org/abs/2505.03062</link>
      <description>arXiv:2505.03062v1 Announce Type: new 
Abstract: Solid-State Drive (SSD) firmware manages complex internal states, including flash memory maintenance. Due to nondeterministic I/O operations, traditional testing methods struggle to rapidly achieve coverage of firmware code areas that require extensive I/O accumulation. To address this challenge, we propose a state data-aware fuzzing approach that leverages SSD firmware's internal state to guide input generation under nondeterministic I/O conditions and accelerate coverage discovery. Our experiments with an open-source SSD firmware emulator show that the proposed method achieves the same firmware test coverage as a state-of-the-art coverage-based fuzzer (AFL++) while requiring approximately 67% fewer commands, without reducing the number of crashes or hangs detected. Moreover, we extend our experiments by incorporating various I/O commands beyond basic write/read operations to reflect real user scenarios, and we confirm that our strategy remains effective even for multiple types of I/O tests. We further validate the effectiveness of state data-aware fuzzing for firmware testing under I/O environments and suggest that this approach can be extended to other storage firmware or threshold-based embedded systems in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03062v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gangho Yoon, Eunseok Lee</dc:creator>
    </item>
    <item>
      <title>An Empirical Study on the Impact of Gender Diversity on Code Quality in AI Systems</title>
      <link>https://arxiv.org/abs/2505.03082</link>
      <description>arXiv:2505.03082v1 Announce Type: new 
Abstract: The rapid advancement of AI systems necessitates high-quality, sustainable code to ensure reliability and mitigate risks such as bias and technical debt. However, the underrepresentation of women in software engineering raises concerns about homogeneity in AI development. Studying gender diversity in AI systems is crucial, as diverse perspectives are essential for improving system robustness, reducing bias, and enhancing overall code quality. While prior research has demonstrated the benefits of diversity in general software teams, its specific impact on the code quality of AI systems remains unexplored. This study addresses this gap by examining how gender diversity within AI teams influences project popularity, code quality, and individual contributions. Our study makes three key contributions. First, we analyzed the relationship between team diversity and repository popularity, revealing that diverse AI repositories not only differ significantly from non-diverse ones but also achieve higher popularity and greater community engagement. Second, we explored the effect of diversity on the overall code quality of AI systems and found that diverse repositories tend to have superior code quality compared to non-diverse ones. Finally, our analysis of individual contributions revealed that although female contributors contribute to a smaller proportion of the total code, their contributions demonstrate consistently higher quality than those of their male counterparts. These findings highlight the need to remove barriers to female participation in AI development, as greater diversity can improve the overall quality of AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03082v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shamse Tasnim Cynthia, Banani Roy</dc:creator>
    </item>
    <item>
      <title>DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral</title>
      <link>https://arxiv.org/abs/2505.03214</link>
      <description>arXiv:2505.03214v1 Announce Type: new 
Abstract: Acquiring structured data from domain-specific, image-based documents such as scanned reports is crucial for many downstream tasks but remains challenging due to document variability. Many of these documents exist as images rather than as machine-readable text, which requires human annotation to train automated extraction systems. We present DocSpiral, the first Human-in-the-Spiral assistive document annotation platform, designed to address the challenge of extracting structured information from domain-specific, image-based document collections. Our spiral design establishes an iterative cycle in which human annotations train models that progressively require less manual intervention. DocSpiral integrates document format normalization, comprehensive annotation interfaces, evaluation metrics dashboard, and API endpoints for the development of AI / ML models into a unified workflow. Experiments demonstrate that our framework reduces annotation time by at least 41\% while showing consistent performance gains across three iterations during model training. By making this annotation platform freely accessible, we aim to lower barriers to AI/ML models development in document processing, facilitating the adoption of large language models in image-based, document-intensive fields such as geoscience and healthcare. The system is freely available at: https://app.ai4wa.com. The demonstration video is available: https://app.ai4wa.com/docs/docspiral/demo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03214v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Sun, Sirui Li, Tingting Bi, Du Huynh, Mark Reynolds, Yuanyi Luo, Wei Liu</dc:creator>
    </item>
    <item>
      <title>Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models</title>
      <link>https://arxiv.org/abs/2505.03265</link>
      <description>arXiv:2505.03265v1 Announce Type: new 
Abstract: While modern Requirements Engineering (RE) heavily relies on natural language processing and Machine Learning (ML) techniques, their effectiveness is limited by the scarcity of high-quality datasets. This paper introduces Synthline, a Product Line (PL) approach that leverages Large Language Models to systematically generate synthetic RE data for classification-based use cases. Through an empirical evaluation conducted in the context of using ML for the identification of requirements specification defects, we investigated both the diversity of the generated data and its utility for training downstream models. Our analysis reveals that while synthetic datasets exhibit less diversity than real data, they are good enough to serve as viable training resources. Moreover, our evaluation shows that combining synthetic and real data leads to substantial performance improvements. Specifically, hybrid approaches achieve up to 85% improvement in precision and a 2x increase in recall compared to models trained exclusively on real data. These findings demonstrate the potential of PL-based synthetic data generation to address data scarcity in RE. We make both our implementation and generated datasets publicly available to support reproducibility and advancement in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03265v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdelkarim El-Hajjami, Camille Salinesi</dc:creator>
    </item>
    <item>
      <title>ATRAF-driven IMRaD Methodology: Tradeoff and Risk Analysis of Software Architectures Across Abstraction Levels</title>
      <link>https://arxiv.org/abs/2505.03624</link>
      <description>arXiv:2505.03624v1 Announce Type: new 
Abstract: Software architecture research relies on key architectural artifacts -- Software Architectures, Reference Architectures, and Architectural Frameworks -- that underpin the design and analysis of complex systems. Evaluating these artifacts is essential to assess tradeoffs and risks affecting quality attributes such as performance, modifiability, and security. Although methodologies like the Architecture Tradeoff Analysis Method (ATAM) support software architecture evaluation, their industrial focus misaligns with the IMRaD (Introduction, Methods, Results, Discussion) format prevalent in academic research, impeding transparency and reproducibility. Our prior work introduced the Architecture Tradeoff and Risk Analysis Framework (ATRAF), extending ATAM through three methods -- ATRAM, RATRAM, and AFTRAM, addressing all abstraction levels, using a unified, iterative four-phase spiral model. These phases -- Scenario and Requirements Gathering, Architectural Views and Scenario Realization, Attribute-Specific Analyses, and Sensitivity, Tradeoff, and Risk Analysis -- ensure traceability and coherence. This paper presents the ATRAF-driven IMRaD Methodology, a concise method to align ATRAF's phases with IMRaD sections. This methodology enhances the rigor, transparency, and accessibility of software architecture research, enabling systematic reporting of complex evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03624v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amine Ben Hassouna</dc:creator>
    </item>
    <item>
      <title>Moral Testing of Autonomous Driving Systems</title>
      <link>https://arxiv.org/abs/2505.03683</link>
      <description>arXiv:2505.03683v1 Announce Type: new 
Abstract: Autonomous Driving System (ADS) testing plays a crucial role in their development, with the current focus primarily on functional and safety testing. However, evaluating the non-functional morality of ADSs, particularly their decision-making capabilities in unavoidable collision scenarios, is equally important to ensure the systems' trustworthiness and public acceptance. Unfortunately, testing ADS morality is nearly impossible due to the absence of universal moral principles. To address this challenge, this paper first extracts a set of moral meta-principles derived from existing moral experiments and well-established social science theories, aiming to capture widely recognized and common-sense moral values for ADSs. These meta-principles are then formalized as quantitative moral metamorphic relations, which act as the test oracle. Furthermore, we propose a metamorphic testing framework to systematically identify potential moral issues. Finally, we illustrate the implementation of the framework and present typical violation cases using the VIRES VTD simulator and its built-in ADS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03683v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenbing Tang, Mingfei Cheng, Yuan Zhou, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Overcoming Obstacles: Challenges of Gender Inequality in Undergraduate ICT Programs</title>
      <link>https://arxiv.org/abs/2505.02857</link>
      <description>arXiv:2505.02857v1 Announce Type: cross 
Abstract: Context: Gender inequality is a widely discussed issue across various sectors, including Information Technology and Communication (ICT). In Brazil, women represent less than 18% of ICT students in higher education. Prior studies highlight gender-related barriers that discourage women from staying in ICT. However, they provide limited insights into their perceptions as undergraduate students and the factors influencing their participation and confidence. Goal: This study explores the perceptions of women undergraduate students in ICT regarding gender inequality. Method: A survey of 402 women from 18 Brazilian states enrolled in ICT courses was conducted using a mixed-method approach, combining quantitative and qualitative analyses. Results: Women students reported experiencing discriminatory practices from peers and professors, both inside and outside the classroom. Gender stereotypes were found to undermine their self-confidence and self-esteem, occasionally leading to course discontinuation. Conclusions: Factors such as lack of representation, inappropriate jokes, isolation, mistrust, and difficulty being heard contribute to harmful outcomes, including reduced participation and reluctance to take leadership roles. Addressing these issues is essential to creating a safe and respectful learning environment for all students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02857v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelica Pereira Souza, Anderson Uch\^oa, Edna Dias Canedo, Juliana Alves Pereira, Claudia Pinto Pereira, Larissa Rocha</dc:creator>
    </item>
    <item>
      <title>Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering</title>
      <link>https://arxiv.org/abs/2505.03096</link>
      <description>arXiv:2505.03096v1 Announce Type: cross 
Abstract: This study explores the application of chaos engineering to enhance the robustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in production-like environments under real-world conditions. LLM-MAS can potentially improve a wide range of tasks, from answering questions and generating content to automating customer support and improving decision-making processes. However, LLM-MAS in production or preproduction environments can be vulnerable to emergent errors or disruptions, such as hallucinations, agent failures, and agent communication failures. This study proposes a chaos engineering framework to proactively identify such vulnerabilities in LLM-MAS, assess and build resilience against them, and ensure reliable performance in critical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03096v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Owotogbe</dc:creator>
    </item>
    <item>
      <title>Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis</title>
      <link>https://arxiv.org/abs/2505.03165</link>
      <description>arXiv:2505.03165v1 Announce Type: cross 
Abstract: The field of deep learning has witnessed significant breakthroughs, spanning various applications, and fundamentally transforming current software capabilities. However, alongside these advancements, there have been increasing concerns about reproducing the results of these deep learning methods. This is significant because reproducibility is the foundation of reliability and validity in software development, particularly in the rapidly evolving domain of deep learning. The difficulty of reproducibility may arise due to several reasons, including having differences from the original execution environment, incompatible software libraries, proprietary data and source code, lack of transparency, and the stochastic nature in some software. A study conducted by the Nature journal reveals that more than 70% of researchers failed to reproduce other researchers experiments and over 50% failed to reproduce their own experiments. Irreproducibility of deep learning poses significant challenges for researchers and practitioners. To address these concerns, this paper presents a systematic approach at analyzing and improving the reproducibility of deep learning models by demonstrating these guidelines using a case study. We illustrate the patterns and anti-patterns involved with these guidelines for improving the reproducibility of deep learning models. These guidelines encompass establishing a methodology to replicate the original software environment, implementing end-to-end training and testing algorithms, disclosing architectural designs, and enhancing transparency in data processing and training pipelines. We also conduct a sensitivity analysis to understand the model performance across diverse conditions. By implementing these strategies, we aim to bridge the gap between research and practice, so that innovations in deep learning can be effectively reproduced and deployed within software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03165v1</guid>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikita Ravi, Abhinav Goel, James C. Davis, George K. Thiruvathukal</dc:creator>
    </item>
    <item>
      <title>RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation</title>
      <link>https://arxiv.org/abs/2505.03275</link>
      <description>arXiv:2505.03275v1 Announce Type: cross 
Abstract: Large language models (LLMs) struggle to effectively utilize a growing number of external tools, such as those defined by the Model Context Protocol (MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to identify the most relevant MCP(s) for a given query from an external index before engaging the LLM. Only the selected tool descriptions are passed to the model, drastically reducing prompt size and simplifying decision-making. Experiments, including an MCP stress test, demonstrate RAG-MCP significantly cuts prompt tokens (e.g., by over 50%) and more than triples tool selection accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables scalable and accurate tool integration for LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03275v1</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tiantian Gan, Qiyao Sun</dc:creator>
    </item>
    <item>
      <title>Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces</title>
      <link>https://arxiv.org/abs/2505.03295</link>
      <description>arXiv:2505.03295v1 Announce Type: cross 
Abstract: Modern automation systems increasingly rely on modular architectures, with capabilities and skills as one solution approach. Capabilities define the functions of resources in a machine-readable form and skills provide the concrete implementations that realize those capabilities. However, the development of a skill implementation conforming to a corresponding capability remains a time-consuming and challenging task. In this paper, we present a method that treats capabilities as contracts for skill implementations and leverages large language models to generate executable code based on natural language user input. A key feature of our approach is the integration of existing software libraries and interface technologies, enabling the generation of skill implementations across different target languages. We introduce a framework that allows users to incorporate their own libraries and resource interfaces into the code generation process through a retrieval-augmented generation architecture. The proposed method is evaluated using an autonomous mobile robot controlled via Python and ROS 2, demonstrating the feasibility and flexibility of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03295v1</guid>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Miguel Vieira da Silva, Aljosha K\"ocher, Nicolas K\"onig, Felix Gehlhoff, Alexander Fay</dc:creator>
    </item>
    <item>
      <title>Qimax: Efficient quantum simulation via GPU-accelerated extended stabilizer formalism</title>
      <link>https://arxiv.org/abs/2505.03307</link>
      <description>arXiv:2505.03307v1 Announce Type: cross 
Abstract: Simulating Clifford and near-Clifford circuits using the extended stabilizer formalism has become increasingly popular, particularly in quantum error correction. Compared to the state-vector approach, the extended stabilizer formalism can solve the same problems with fewer computational resources, as it operates on stabilizers rather than full state vectors. Most existing studies on near-Clifford circuits focus on balancing the trade-off between the number of ancilla qubits and simulation accuracy, often overlooking performance considerations. Furthermore, in the presence of high-rank stabilizers, performance is limited by the sequential property of the stabilizer formalism. In this work, we introduce a parallelized version of the extended stabilizer formalism, enabling efficient execution on multi-core devices such as GPU. Experimental results demonstrate that, in certain scenarios, our Python-based implementation outperforms state-of-the-art simulators such as Qiskit and Pennylane.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03307v1</guid>
      <category>quant-ph</category>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vu Tuan Hai, Bui Cao Doanh, Le Vu Trung Duong, Pham Hoai Luan, Yasuhiko Nakashima</dc:creator>
    </item>
    <item>
      <title>The Struggles of LLMs in Cross-lingual Code Clone Detection</title>
      <link>https://arxiv.org/abs/2408.04430</link>
      <description>arXiv:2408.04430v3 Announce Type: replace 
Abstract: With the involvement of multiple programming languages in modern software development, cross-lingual code clone detection has gained traction within the software engineering community. Numerous studies have explored this topic, proposing various promising approaches. Inspired by the significant advances in machine learning in recent years, particularly Large Language Models (LLMs), which have demonstrated their ability to tackle various tasks, this paper revisits cross-lingual code clone detection. We evaluate the performance of five (05) LLMs and eight prompts (08) for the identification of cross-lingual code clones. Additionally, we compare these results against two baseline methods. Finally, we evaluate a pre-trained embedding model to assess the effectiveness of the generated representations for classifying clone and non-clone pairs. The studies involving LLMs and Embedding models are evaluated using two widely used cross-lingual datasets, XLCoST and CodeNet. Our results show that LLMs can achieve high F1 scores, up to 0.99, for straightforward programming examples. However, they not only perform less well on programs associated with complex programming challenges but also do not necessarily understand the meaning of "code clones" in a cross-lingual setting. We show that embedding models used to represent code fragments from different programming languages in the same representation space enable the training of a basic classifier that outperforms all LLMs by ~1 and ~20 percentage points on the XLCoST and CodeNet datasets, respectively. This finding suggests that, despite the apparent capabilities of LLMs, embeddings provided by embedding models offer suitable representations to achieve state-of-the-art performance in cross-lingual code clone detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04430v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3715764</arxiv:DOI>
      <dc:creator>Micheline B\'en\'edicte Moumoula, Abdoul Kader Kabore, Jacques Klein, Tegawend\'e Bissyande</dc:creator>
    </item>
    <item>
      <title>DiffSpec: Differential Testing with LLMs using Natural Language Specifications and Code Artifacts</title>
      <link>https://arxiv.org/abs/2410.04249</link>
      <description>arXiv:2410.04249v3 Announce Type: replace 
Abstract: Differential testing can be an effective way to find bugs in software systems with multiple implementations that conform to the same specification, like compilers, network protocol parsers, or language runtimes. Specifications for such systems are often standardized in natural language documents, like Instruction Set Architecture (ISA) specifications or IETF RFC's. Large Language Models (LLMs) have demonstrated potential in both generating tests and handling large volumes of natural language text, making them well-suited for analyzing artifacts like specification documents, bug reports, and code implementations. In this work, we leverage natural language and code artifacts to guide LLMs to generate targeted tests that highlight meaningful behavioral differences between implementations, including those corresponding to bugs. We introduce DiffSpec, a framework for generating differential tests with LLMs using prompt chaining. We demonstrate DiffSpec's efficacy on two different (extensively tested) systems, eBPF runtimes and Wasm validators. Using DiffSpec, we generated 1901 differentiating tests, uncovering at least four distinct and confirmed bugs in eBPF, including a kernel memory leak, inconsistent behavior in jump instructions, undefined behavior when using the stack pointer, and tests with infinite loops that hang the verifier in ebpf-for-windows. We also found 299 differentiating tests in Wasm validators pointing to two confirmed and fixed bugs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04249v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikitha Rao, Elizabeth Gilbert, Harrison Green, Tahina Ramananandro, Nikhil Swamy, Claire Le Goues, Sarah Fakhoury</dc:creator>
    </item>
    <item>
      <title>Large Language Models as Robust Data Generators in Software Analytics: Are We There Yet?</title>
      <link>https://arxiv.org/abs/2411.10565</link>
      <description>arXiv:2411.10565v3 Announce Type: replace 
Abstract: Large Language Model (LLM)-generated data is increasingly used in software analytics, but it is unclear how this data compares to human-written data, particularly when models are exposed to adversarial scenarios. Adversarial attacks can compromise the reliability and security of software systems, so understanding how LLM-generated data performs under these conditions, compared to human-written data, which serves as the benchmark for model performance, can provide valuable insights into whether LLM-generated data offers similar robustness and effectiveness. To address this gap, we systematically evaluate and compare the quality of human-written and LLM-generated data for fine-tuning robust pre-trained models (PTMs) in the context of adversarial attacks. We evaluate the robustness of six widely used PTMs, fine-tuned on human-written and LLM-generated data, before and after adversarial attacks. This evaluation employs nine state-of-the-art (SOTA) adversarial attack techniques across three popular software analytics tasks: clone detection, code summarization, and sentiment analysis in code review discussions. Additionally, we analyze the quality of the generated adversarial examples using eleven similarity metrics. Our findings reveal that while PTMs fine-tuned on LLM-generated data perform competitively with those fine-tuned on human-written data, they exhibit less robustness against adversarial attacks in software analytics tasks. Our study underscores the need for further exploration into enhancing the quality of LLM-generated training data to develop models that are both high-performing and capable of withstanding adversarial attacks in software analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10565v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Md. Abdul Awal, Mrigank Rochan, Chanchal K. Roy</dc:creator>
    </item>
    <item>
      <title>Gender Disparities in Contributions, Leadership, and Collaboration: An Exploratory Study on Software Systems Research</title>
      <link>https://arxiv.org/abs/2412.15661</link>
      <description>arXiv:2412.15661v3 Announce Type: replace 
Abstract: Gender diversity enhances research by bringing diverse perspectives and innovative approaches. It ensures equitable solutions that address the needs of diverse populations. However, gender disparity persists in research where women remain underrepresented, which might limit diversity and innovation. Many even leave scientific careers as their contributions often go unnoticed and undervalued. Therefore, understanding gender-based contributions and collaboration dynamics is crucial to addressing this gap and creating a more inclusive research environment. In this study, we analyzed 2,000 articles published over the past decade in the Journal of Systems and Software (JSS). From these, we selected 384 articles that detailed authors' contributions and contained both female and male authors to investigate gender-based contributions. Our contributions are fourfold. First, we analyzed women's engagement in software systems research. Our analysis showed that only 32.74% of the total authors are women and female-led or supervised studies were fewer than those of men. Second, we investigated female authors' contributions across 14 major roles. Interestingly, we found that women contributed comparably to men in most roles, with more contributions in conceptualization, writing, and reviewing articles. Third, we explored the areas of software systems research and found that female authors are more actively involved in human-centric research domains. Finally, we analyzed gender-based collaboration dynamics. Our findings revealed that female supervisors tended to collaborate locally more often than national-level collaborations. Our study highlights that females' contributions to software systems research are comparable to those of men. Therefore, the barriers need to be addressed to enhance female participation and ensure equity and inclusivity in research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15661v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shamse Tasnim Cynthia, Saikat Mondal, Joy Krishan Das, Banani Roy</dc:creator>
    </item>
    <item>
      <title>Refining Fuzzed Crashing Inputs for Better Fault Diagnosis</title>
      <link>https://arxiv.org/abs/2505.02305</link>
      <description>arXiv:2505.02305v2 Announce Type: replace 
Abstract: We present DiffMin, a technique that refines a fuzzed crashing input to gain greater similarities to given passing inputs to help developers analyze the crashing input to identify the failure-inducing condition and locate buggy code for debugging. DiffMin iteratively applies edit actions to transform a fuzzed input while preserving the crash behavior. Our pilot study with the Magma benchmark demonstrates that DiffMin effectively minimizes the differences between crashing and passing inputs while enhancing the accuracy of spectrum-based fault localization, highlighting its potential as a valuable pre-debugging step after greybox fuzzing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02305v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kieun Kim (Chungbuk National University), Seongmin Lee (Max Planck Institute for Security and Privacy), Shin Hong (Chungbuk National University)</dc:creator>
    </item>
    <item>
      <title>English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports</title>
      <link>https://arxiv.org/abs/2502.14338</link>
      <description>arXiv:2502.14338v3 Announce Type: replace-cross 
Abstract: Accurate translation of bug reports is critical for efficient collaboration in global software development. In this study, we conduct the first comprehensive evaluation of machine translation (MT) performance on bug reports, analyzing the capabilities of DeepL, AWS Translate, and large language models such as ChatGPT, Claude, Gemini, LLaMA, and Mistral using data from the Visual Studio Code GitHub repository, specifically focusing on reports labeled with the english-please tag. To assess both translation quality and source language identification accuracy, we employ a range of MT evaluation metrics-including BLEU, BERTScore, COMET, METEOR, and ROUGE-alongside classification metrics such as accuracy, precision, recall, and F1-score. Our findings reveal that while ChatGPT (gpt-4o) excels in semantic and lexical translation quality, it does not lead in source language identification. Claude and Mistral achieve the highest F1-scores (0.7182 and 0.7142, respectively), and Gemini records the best precision (0.7414). AWS Translate shows the highest accuracy (0.4717) in identifying source languages. These results highlight that no single system dominates across all tasks, reinforcing the importance of task-specific evaluations. This study underscores the need for domain adaptation when translating technical content and provides actionable insights for integrating MT into bug-triaging workflows. The code and dataset for this paper are available at GitHub-https://github.com/av9ash/English-Please</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14338v3</guid>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avinash Patil, Aryan Jadon</dc:creator>
    </item>
  </channel>
</rss>
