<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.ed-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.ed-ph</link>
    <description>physics.ed-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.ed-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 04:05:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Q-BEAST: A Practical Course on Experimental Evaluation and Characterization of Quantum Computing Systems</title>
      <link>https://arxiv.org/abs/2508.14084</link>
      <description>arXiv:2508.14084v1 Announce Type: new 
Abstract: Quantum computing (QC) promises to be a transformative technology with impact on various application domains, such as optimization, cryptography, and material science. However, the technology has a sharp learning curve, and practical evaluation and characterization of quantum systems remains complex and challenging, particularly for students and newcomers from computer science to the field of quantum computing. To address this educational gap, we introduce Q-BEAST, a practical course designed to provide structured training in the experimental analysis of quantum computing systems. Q-BEAST offers a curriculum that combines foundational concepts in quantum computing with practical methodologies and use cases for benchmarking and performance evaluation on actual quantum systems. Through theoretical instruction and hands-on experimentation, students gain experience in assessing the advantages and limitations of real quantum technologies. With that, Q-BEAST supports the education of a future generation of quantum computing users and developers. Furthermore, it also explicitly promotes a deeper integration of High Performance Computing (HPC) and QC in research and education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14084v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.ET</category>
      <category>math.QA</category>
      <category>quant-ph</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minh Chung, Yaknan Gambo, Burak Mete, Xiao-Ting Michelle To, Florian Kr\"otz, Korbinian Staudacher, Martin Letras, Xiaolong Deng, Mounika Vavilala, Amir Raoofy, Jorge Echavarria, Luigi Iapichino, Laura Schulz, Josef Weidendorfer, Martin Schulz</dc:creator>
    </item>
    <item>
      <title>Reliable generation of isomorphic physics problems using ChatGPT with prompt-chaining and tool use</title>
      <link>https://arxiv.org/abs/2508.14755</link>
      <description>arXiv:2508.14755v1 Announce Type: new 
Abstract: We present a method for generating large numbers of isomorphic physics problems using ChatGPT through prompt chaining and tool use. This approach enables precise control over structural variations-such as numeric values and spatial relations-while supporting diverse contextual variations in the problem body. By utilizing the Python code interpreter, the method supports automatic solution validation and simple diagram generation, addressing key limitations in existing LLM-based methods. We generated two example isomorphic problem banks and compared the outcome against simpler prompt-based approaches. Results show that prompt-chaining produces significantly higher quality and more consistent outputs than simpler, non-chaining prompts. This work demonstrates a promising method for efficient problem creation accessible to the average instructor, which opens new possibilities for personalized adaptive testing and automated content development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14755v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.AI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhongzhou Chen</dc:creator>
    </item>
    <item>
      <title>Students' Perceptions to a Large Language Model's Generated Feedback and Scores of Argumentation Essays</title>
      <link>https://arxiv.org/abs/2508.14759</link>
      <description>arXiv:2508.14759v1 Announce Type: new 
Abstract: Students in introductory physics courses often rely on ineffective strategies, focusing on final answers rather than understanding underlying principles. Integrating scientific argumentation into problem-solving fosters critical thinking and links conceptual knowledge with practical application. By facilitating learners to articulate their scientific arguments for solving problems, and by providing real-time feedback on students' strategies, we aim to enable students to develop superior problem-solving skills. Providing timely, individualized feedback to students in large-enrollment physics courses remains a challenge. Recent advances in Artificial Intelligence (AI) offer promising solutions. This study investigates the potential of AI-generated feedback on students' written scientific arguments in an introductory physics class. Using Open AI's GPT-4o, we provided delayed feedback on student written scientific arguments and surveyed them about the perceived usefulness and accuracy of this feedback. Our findings offer insights into the viability of implementing real-time AI feedback to enhance students' problem-solving and metacognitive skills in large-enrollment classrooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14759v1</guid>
      <category>physics.ed-ph</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Winter Allen, Anand Shanker, N. Sanjay Rebello</dc:creator>
    </item>
    <item>
      <title>Dual-Role Dynamics in Prompting: Elementary Pre-service Teachers' AI Prompting Strategies for Representational Choices</title>
      <link>https://arxiv.org/abs/2508.14760</link>
      <description>arXiv:2508.14760v1 Announce Type: new 
Abstract: Pre-service teachers play a unique dual role as they straddle between the roles of students and future teachers. This dual role requires them to adopt both the learner's and the instructor's perspectives while engaging with pedagogical and content knowledge. The current study investigates how pre-service elementary teachers taking a physical science course prompt AI to generate representations that effectively communicate conceptual ideas to two distinct audiences. The context involves participants interacting with AI to generate appropriate representations that explain the concepts of wave velocity to their elementary students (while casting themselves as teachers) and the Ideal Gas Law to their English teachers (while casting themselves as students). Emergent coding of the AI prompts highlight that, when acting as teachers, participants were more explicit in specifying the target audience, predetermining the type of representation, and producing a broader variety of representations compared to when they acted as students. Implications of the observed 'exploratory' and 'prescriptive' prompting trends across the two roles on pre-service teachers' education and their professional development are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14760v1</guid>
      <category>physics.ed-ph</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Razan Hamed, Amogh Sirnoorkar, N. Sanjay Rebello</dc:creator>
    </item>
    <item>
      <title>Investigation of the Inter-Rater Reliability between Large Language Models and Human Raters in Qualitative Analysis</title>
      <link>https://arxiv.org/abs/2508.14764</link>
      <description>arXiv:2508.14764v1 Announce Type: new 
Abstract: Qualitative analysis is typically limited to small datasets because it is time-intensive. Moreover, a second human rater is required to ensure reliable findings. Artificial intelligence tools may replace human raters if we demonstrate high reliability compared to human ratings. We investigated the inter-rater reliability of state-of-the-art Large Language Models (LLMs), ChatGPT-4o and ChatGPT-4.5-preview, in rating audio transcripts coded manually. We explored prompts and hyperparameters to optimize model performance. The participants were 14 undergraduate student groups from a university in the midwestern United States who discussed problem-solving strategies for a project. We prompted an LLM to replicate manual coding, and calculated Cohen's Kappa for inter-rater reliability. After optimizing model hyperparameters and prompts, the results showed substantial agreement (${\kappa}&gt;0.6$) for three themes and moderate agreement on one. Our findings demonstrate the potential of GPT-4o and GPT-4.5 for efficient, scalable qualitative analysis in physics education and identify their limitations in rating domain-general constructs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14764v1</guid>
      <category>physics.ed-ph</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikhil Sanjay Borse, Ravishankar Chatta Subramaniam, N. Sanjay Rebello</dc:creator>
    </item>
    <item>
      <title>Analyzing Undergraduate Problem-Solving in Physics Through Interaction With an AI Chatbot</title>
      <link>https://arxiv.org/abs/2508.14778</link>
      <description>arXiv:2508.14778v1 Announce Type: new 
Abstract: Providing individualized scaffolding for physics problem solving at scale remains an instructional challenge. We investigate (1) students' perceptions of a Socratic Artificial Intelligence (AI) chatbot's impact on problem-solving skills and confidence and (2) how the specificity of students' questions during tutoring relates to performance. We deployed a custom Socratic AI chatbot in a large-enrollment introductory mechanics course at a Midwestern public university, logging full dialogue transcripts from 150 first-year STEM majors. Post-interaction surveys revealed median ratings of 4.0/5 for knowledge-based skills and 3.4/5 for overall effectiveness. Transcript analysis showed question specificity rose from approximately 10-15% in the first turn to 100% by the final turn, and specificity correlated positively with self reported expected course grade (Pearson r = 0.43). These findings demonstrate that AI-driven Socratic dialogue not only fosters expert-like reasoning but also generates fine-grained analytics for physics education research, establishing a scalable dual-purpose tool for instruction and learning analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14778v1</guid>
      <category>physics.ed-ph</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syed Furqan Abbas Hashmi, N. Sanjay Rebello</dc:creator>
    </item>
    <item>
      <title>Using an LLM to Investigate Students' Explanations on Conceptual Physics Questions</title>
      <link>https://arxiv.org/abs/2508.14823</link>
      <description>arXiv:2508.14823v1 Announce Type: new 
Abstract: Analyzing students' written solutions to physics questions is a major area in PER. However, gauging student understanding in college courses is bottlenecked by large class sizes, which limits assessments to a multiple-choice (MC) format for ease of grading. Although sufficient in quantifying scientifically correct conceptions, MC assessments do not uncover students' deeper ways of understanding physics. Large language models (LLMs) offer a promising approach for assessing students' written responses at scale. Our study used an LLM, validated by human graders, to classify students' written explanations to three questions on the Energy and Momentum Conceptual Survey as correct or incorrect, and organized students' incorrect explanations into emergent categories. We found that the LLM (GPT-4o) can fairly assess students' explanations, comparable to human graders (0-3% discrepancy). Furthermore, the categories of incorrect explanations were different from corresponding MC distractors, allowing for different and deeper conceptions to become accessible to educators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14823v1</guid>
      <category>physics.ed-ph</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sean Savage, N. Sanjay Rebello</dc:creator>
    </item>
    <item>
      <title>Using Large Language Models to Assign Partial Credit to Students' Explanations of Problem-Solving Process: Grade at Human Level Accuracy with Grading Confidence Index and Personalized Student-facing Feedback</title>
      <link>https://arxiv.org/abs/2412.06910</link>
      <description>arXiv:2412.06910v3 Announce Type: replace 
Abstract: This study examines the feasibility and potential advantages of using large language models, in particular GPT-4o, to perform partial credit grading of large numbers of student written responses to introductory level physics problems. Students were instructed to write down verbal explanations of their reasoning process when solving one conceptual and two numerical calculation problems on in class exams. The explanations were then graded according to a 3-item rubric with each item grades as binary (1 or 0). We first demonstrate that machine grading using GPT-4o with no examples nor reference answer can reliably agree with human graders on 70%-80% of all cases, which is equal to or higher than the level at which two human graders agree with each other. Two methods are essential for achieving this level of accuracy: 1. Adding explanation language to each rubric item that targets the errors of initial machine grading. 2. Running the grading process 5 times and taking the most frequent outcome. Next, we show that the variation in outcomes across 5 machine grading attempts as measured by the Shannon Entropy can serve as a grading confidence index, allowing a human instructor to identify ~40% of all potentially incorrect gradings by reviewing just 10 - 15% of all responses. Finally, we show that it is straightforward to use GPT-4o to write clear explanations of the partial credit grading outcomes. Those explanations can be used as feedback for students, which will allow students to understand their grades and raise different opinions when necessary. Almost all feedback messages generated were rated 3 or above on a 5-point scale by two experienced instructors. The entire grading and feedback generating process cost roughly $5 per 100 student answers, which shows immense promise for automating labor-intensive grading process by a combination of machine grading with human input and supervision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06910v3</guid>
      <category>physics.ed-ph</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevPhysEducRes.21.010126</arxiv:DOI>
      <arxiv:journal_reference>Physical Review Physics Education Research, 21(1), 010126 (2025)</arxiv:journal_reference>
      <dc:creator>Zhongzhou Chen, Tong Wan</dc:creator>
    </item>
  </channel>
</rss>
