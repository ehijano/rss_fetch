<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Dec 2024 02:51:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Network Structural Equation Models for Causal Mediation and Spillover Effects</title>
      <link>https://arxiv.org/abs/2412.05397</link>
      <description>arXiv:2412.05397v1 Announce Type: new 
Abstract: Social network interference induces spillover effects from neighbors' exposures, and the complexity of statistical analysis increases when mediators are involved with network interference. To address various technical challenges, we develop a theoretical framework employing a structural graphical modeling approach to investigate both mediation and interference effects within network data. Our framework enables us to capture the multifaceted mechanistic pathways through which neighboring units' exposures and mediators exert direct and indirect influences on an individual's outcome. We extend the exposure mapping paradigm in the context of a random-effects network structural equation models (REN-SEM), establishing its capacity to delineate spillover effects of interest. Our proposed methodology contributions include maximum likelihood estimation for REN-SEM and inference procedures with theoretical guarantees. Such guarantees encompass consistent asymptotic variance estimators, derived under a non-i.i.d. asymptotic theory. The robustness and practical utility of our methodology are demonstrated through simulation experiments and a real-world data analysis of the Twitch Gamers Network Dataset, underscoring its effectiveness in capturing the intricate dynamics of network-mediated exposure effects. This work is the first to provide a rigorous theoretical framework and analytic toolboxes to the mediation analysis of network data, including a robust assessment on the interplay of mediation and interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05397v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritoban Kundu, Peter X. K. Song</dc:creator>
    </item>
    <item>
      <title>The BPgWSP test: a Bayesian Weibull Shape Parameter signal detection test for adverse drug reactions</title>
      <link>https://arxiv.org/abs/2412.05463</link>
      <description>arXiv:2412.05463v1 Announce Type: new 
Abstract: We develop a Bayesian Power generalized Weibull shape parameter (PgWSP) test as statistical method for signal detection of possible drug-adverse event associations using electronic health records for pharmacovigilance. The Bayesian approach allows the incorporation of prior knowledge about the likely time of occurrence along time-to-event data. The test is based on the shape parameters of the Power generalized Weibull (PgW) distribution. When both shape parameters are equal to one, the PgW distribution reduces to an exponential distribution, i.e. a constant hazard function. This is interpreted as no temporal association between drug and adverse event. The Bayesian PgWSP test involves comparing a region of practical equivalence (ROPE) around one reflecting the null hypothesis with estimated credibility intervals reflecting the posterior means of the shape parameters. The decision to raise a signal is based on the outcomes of the ROPE test and the selected combination rule for these outcomes. The development of the test requires a simulation study for tuning of the ROPE and credibility intervals to optimize specifcity and sensitivity of the test. Samples are generated under various conditions, including differences in sample size, prevalence of adverse drug reactions (ADRs), and the proportion of adverse events. We explore prior assumptions reflecting the belief in the presence or absence of ADRs at different points in the observation period. Various types of ROPE, credibility intervals, and combination rules are assessed and optimal tuning parameters are identifed based on the area under the curve. The tuned Bayesian PgWSP test is illustrated in a case study in which the time-dependent correlation between the intake of bisphosphonates and four adverse events is investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05463v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Dyck, Odile Sauzet</dc:creator>
    </item>
    <item>
      <title>Optimizing Returns from Experimentation Programs</title>
      <link>https://arxiv.org/abs/2412.05508</link>
      <description>arXiv:2412.05508v1 Announce Type: new 
Abstract: Experimentation in online digital platforms is used to inform decision making. Specifically, the goal of many experiments is to optimize a metric of interest. Null hypothesis statistical testing can be ill-suited to this task, as it is indifferent to the magnitude of effect sizes and opportunity costs. Given access to a pool of related past experiments, we discuss how experimentation practice should change when the goal is optimization. We survey the literature on empirical Bayes analyses of A/B test portfolios, and single out the A/B Testing Problem (Azevedo et al., 2020) as a starting point, which treats experimentation as a constrained optimization problem. We show that the framework can be solved with dynamic programming and implemented by appropriately tuning $p$-value thresholds. Furthermore, we develop several extensions of the A/B Testing Problem and discuss the implications of these results on experimentation programs in industry. For example, under no-cost assumptions, firms should be testing many more ideas, reducing test allocation sizes, and relaxing $p$-value thresholds away from $p = 0.05$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05508v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy Sudijono, Simon Ejdemyr, Apoorva Lal, Martin Tingley</dc:creator>
    </item>
    <item>
      <title>A generalized Bayesian approach for high-dimensional robust regression with serially correlated errors and predictors</title>
      <link>https://arxiv.org/abs/2412.05673</link>
      <description>arXiv:2412.05673v1 Announce Type: new 
Abstract: This paper presents a loss-based generalized Bayesian methodology for high-dimensional robust regression with serially correlated errors and predictors. The proposed framework employs a novel scaled pseudo-Huber (SPH) loss function, which smooths the well-known Huber loss, achieving a balance between quadratic and absolute linear loss behaviors. This flexibility enables the framework to accommodate both thin-tailed and heavy-tailed data effectively. The generalized Bayesian approach constructs a working likelihood utilizing the SPH loss that facilitates efficient and stable estimation while providing rigorous estimation uncertainty quantification for all model parameters. Notably, this allows formal statistical inference without requiring ad hoc tuning parameter selection while adaptively addressing a wide range of tail behavior in the errors. By specifying appropriate prior distributions for the regression coefficients -- e.g., ridge priors for small or moderate-dimensional settings and spike-and-slab priors for high-dimensional settings -- the framework ensures principled inference. We establish rigorous theoretical guarantees for the accurate estimation of underlying model parameters and the correct selection of predictor variables under sparsity assumptions for a wide range of data generating setups. Extensive simulation studies demonstrate the superiority of our approach compared to traditional quadratic and absolute linear loss-based Bayesian regression methods, highlighting its flexibility and robustness in high-dimensional and challenging data contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05673v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Chakraborty, Kshitij Khare, George Michailidis</dc:creator>
    </item>
    <item>
      <title>Bootstrap Model Averaging</title>
      <link>https://arxiv.org/abs/2412.05687</link>
      <description>arXiv:2412.05687v1 Announce Type: new 
Abstract: Model averaging has gained significant attention in recent years due to its ability of fusing information from different models. The critical challenge in frequentist model averaging is the choice of weight vector. The bootstrap method, known for its favorable properties, presents a new solution. In this paper, we propose a bootstrap model averaging approach that selects the weights by minimizing a bootstrap criterion. Our weight selection criterion can also be interpreted as a bootstrap aggregating. We demonstrate that the resultant estimator is asymptotically optimal in the sense that it achieves the lowest possible squared error loss. Furthermore, we establish the convergence rate of bootstrap weights tending to the theoretically optimal weights. Additionally, we derive the limiting distribution for our proposed model averaging estimator. Through simulation studies and empirical applications, we show that our proposed method often has better performance than other commonly used model selection and model averaging methods, and bootstrap variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05687v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghui Song, Guohua Zou, Alan T. K. Wan</dc:creator>
    </item>
    <item>
      <title>PICS: A sequential approach to obtain optimal designs for non-linear models leveraging closed-form solutions for faster convergence</title>
      <link>https://arxiv.org/abs/2412.05744</link>
      <description>arXiv:2412.05744v1 Announce Type: new 
Abstract: D-Optimal designs for estimating parameters of response models are derived by maximizing the determinant of the Fisher information matrix. For non-linear models, the Fisher information matrix depends on the unknown parameter vector of interest, leading to a weird situation that in order to obtain the D-optimal design, one needs to have knowledge of the parameter to be estimated. One solution to this problem is to choose the design points sequentially, optimizing the D-optimality criterion using parameter estimates based on available data, followed by updating the parameter estimates using maximum likelihood estimation. On the other hand, there are many non-linear models for which closed-form results for D-optimal designs are available, but because such solutions involve the parameters to be estimated, they can only be used by substituting "guestimates" of parameters. In this paper, a hybrid sequential strategy called PICS (Plug into closed-form solution) is proposed that replaces the optimization of the objective function at every single step by a draw from the probability distribution induced by the known optimal design by plugging in the current estimates. Under regularity conditions, asymptotic normality of the sequence of estimators generated by this approach are established. Usefulness of this approach in terms of saving computational time and achieving greater efficiency of estimation compared to the standard sequential approach are demonstrated with simulations conducted from two different sets of models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05744v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suvrojit Ghosh, Koulik Khamaru, Tirthankar Dasgupta</dc:creator>
    </item>
    <item>
      <title>A Two-stage Joint Modeling Approach for Multiple Longitudinal Markers and Time-to-event Data</title>
      <link>https://arxiv.org/abs/2412.05765</link>
      <description>arXiv:2412.05765v1 Announce Type: new 
Abstract: Collecting multiple longitudinal measurements and time-to-event outcomes is a common practice in clinical and epidemiological studies, often focusing on exploring associations between them. Joint modeling is the standard analytical tool for such data, with several R packages available. However, as the number of longitudinal markers increases, the computational burden and convergence challenges make joint modeling increasingly impractical.
  This paper introduces a novel two-stage Bayesian approach to estimate joint models for multiple longitudinal measurements and time-to-event outcomes. The method builds on the standard two-stage framework but improves the initial stage by estimating a separate one-marker joint model for the event and each longitudinal marker, rather than relying on mixed models. These estimates are used to derive predictions of individual marker trajectories, avoiding biases from informative dropouts. In the second stage, a proportional hazards model is fitted, incorporating the predicted current values and slopes of the markers as time-dependent covariates. To address uncertainty in the first-stage predictions, a multiple imputation technique is employed when estimating the Cox model in the second stage.
  This two-stage method allows for the analysis of numerous longitudinal markers, which is often infeasible with traditional multi-marker joint modeling. The paper evaluates the approach through simulation studies and applies it to the PBC2 dataset and a real-world dementia dataset containing 17 longitudinal markers. An R package, TSJM, implementing the method is freely available on GitHub: https://github.com/tbaghfalaki/TSJM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05765v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taban Baghfalaki, Reza Hashemi, Catherine Helmer, Helene Jacqmin-Gadda</dc:creator>
    </item>
    <item>
      <title>Modeling time to failure using a temporal sequence of events</title>
      <link>https://arxiv.org/abs/2412.05836</link>
      <description>arXiv:2412.05836v1 Announce Type: new 
Abstract: In recent years, the requirement for real-time understanding of machine behavior has become an important objective in industrial sectors to reduce the cost of unscheduled downtime and to maximize production with expected quality. The vast majority of high-end machines are equipped with a number of sensors that can record event logs over time. In this paper, we consider an injection molding (IM) machine that manufactures plastic bottles for soft drink. We have analyzed the machine log data with a sequence of three type of events, ``running with alert'', ``running without alert'', and ``failure''. Failure event leads to downtime of the machine and necessitates maintenance. The sensors are capable of capturing the corresponding operational conditions of the machine as well as the defined states of events. This paper presents a new model to predict a) time to failure of the IM machine and b) identification of important sensors in the system that may explain the events which in-turn leads to failure. The proposed method is more efficient than the popular competitor and can help reduce the downtime costs by controlling operational parameters in advance to prevent failures from occurring too soon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05836v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandip K Pal, Arnab Koley, Pritam Ranjan, Debasis Kundu</dc:creator>
    </item>
    <item>
      <title>Fast QR updating methods for statistical applications</title>
      <link>https://arxiv.org/abs/2412.05905</link>
      <description>arXiv:2412.05905v1 Announce Type: new 
Abstract: This paper introduces fast R updating algorithms designed for statistical applications, including regression, filtering, and model selection, where data structures change frequently. Although traditional QR decomposition is essential for matrix operations, it becomes computationally intensive when dynamically updating the design matrix in statistical models. The proposed algorithms efficiently update the R matrix without recalculating Q, significantly reducing computational costs. These algorithms provide a scalable solution for high-dimensional regression models, enhancing the feasibility of large-scale statistical analyses and model selection in data-intensive fields. Comprehensive simulation studies and real-world data applications reveal that the methods significantly reduce computational time while preserving accuracy. An extensive discussion highlights the versatility of fast R updating algorithms, illustrating their benefits across a wide range of models and applications in statistics and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05905v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauro Bernardi, Claudio Busatto, Manuela Cattelan</dc:creator>
    </item>
    <item>
      <title>B-MASTER: Scalable Bayesian Multivariate Regression Analysis for Selecting Targeted Essential Regressors to Identify the Key Genera in Microbiome-Metabolite Relation Dynamics</title>
      <link>https://arxiv.org/abs/2412.05998</link>
      <description>arXiv:2412.05998v1 Announce Type: new 
Abstract: The gut microbiome significantly influences responses to cancer therapies, including immunotherapies, primarily through its impact on the metabolome. Despite some existing studies addressing the effects of specific microbial genera on individual metabolites, there is little to no prior work focused on identifying the key microbiome components at the genus level that shape the overall metabolome profile. To bridge this gap, we introduce B-MASTER (Bayesian Multivariate regression Analysis for Selecting Targeted Essential Regressors), a fully Bayesian framework incorporating an L1 penalty to promote sparsity in the coefficient matrix and an L2 penalty to shrink coefficients for non-major covariate components simultaneously, thereby isolating essential regressors. The method is complemented with a scalable Gibbs sampling algorithm, whose computational speed increases linearly with the number of parameters and remains largely unaffected by sample size and data-specific characteristics for models of fixed dimensions. Notably, B-MASTER achieves full posterior inference for models with up to four million parameters within a practical time-frame. Using this approach, we identify key microbial genera influencing the overall metabolite profile, conduct an in-depth analysis of their effects on the most abundant metabolites, and investigate metabolites differentially abundant in colorectal cancer patients. These results provide foundational insights into the impact of the microbiome at the genus level on metabolite profiles relevant to cancer, a relationship that remains largely unexplored in the existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05998v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Priyam Das, Tanujit Dey, Christine Peterson, Sounak Chakraborty</dc:creator>
    </item>
    <item>
      <title>Imputation Matters: A Deeper Look into an Overlooked Step in Longitudinal Health and Behavior Sensing Research</title>
      <link>https://arxiv.org/abs/2412.06018</link>
      <description>arXiv:2412.06018v1 Announce Type: new 
Abstract: Longitudinal passive sensing studies for health and behavior outcomes often have missing and incomplete data. Handling missing data effectively is thus a critical data processing and modeling step. Our formative interviews with researchers working in longitudinal health and behavior passive sensing revealed a recurring theme: most researchers consider imputation a low-priority step in their analysis and inference pipeline, opting to use simple and off-the-shelf imputation strategies without comprehensively evaluating its impact on study outcomes. Through this paper, we call attention to the importance of imputation. Using publicly available passive sensing datasets for depression, we show that prioritizing imputation can significantly impact the study outcomes -- with our proposed imputation strategies resulting in up to 31% improvement in AUROC to predict depression over the original imputation strategy. We conclude by discussing the challenges and opportunities with effective imputation in longitudinal sensing studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06018v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Akshat Choube, Rahul Majethia, Sohini Bhattacharya, Vedant Das Swain, Jiachen Li, Varun Mishra</dc:creator>
    </item>
    <item>
      <title>New Additive OCBA Procedures for Robust Ranking and Selection</title>
      <link>https://arxiv.org/abs/2412.06020</link>
      <description>arXiv:2412.06020v1 Announce Type: new 
Abstract: Robust ranking and selection (R&amp;S) is an important and challenging variation of conventional R&amp;S that seeks to select the best alternative among a finite set of alternatives. It captures the common input uncertainty in the simulation model by using an ambiguity set to include multiple possible input distributions and shifts to select the best alternative with the smallest worst-case mean performance over the ambiguity set. In this paper, we aim at developing new fixed-budget robust R&amp;S procedures to minimize the probability of incorrect selection (PICS) under a limited sampling budget. Inspired by an additive upper bound of the PICS, we derive a new asymptotically optimal solution to the budget allocation problem. Accordingly, we design a new sequential optimal computing budget allocation (OCBA) procedure to solve robust R&amp;S problems efficiently. We then conduct a comprehensive numerical study to verify the superiority of our robust OCBA procedure over existing ones. The numerical study also provides insights on the budget allocation behaviors that lead to enhanced efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06020v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Wan, Zaile Li, L. Jeff Hong</dc:creator>
    </item>
    <item>
      <title>A Generalized Mixture Cure Model Incorporating Known Cured Individuals</title>
      <link>https://arxiv.org/abs/2412.06027</link>
      <description>arXiv:2412.06027v1 Announce Type: new 
Abstract: The Mixture Cure (MC) models constitute an appropriate and easily interpretable method when studying a time-to-event variable in a population comprised of both susceptible and cured individuals. In literature, those models usually assume that the latter are unobservable. However, there are cases in which a cured individual may be identified. For example, when studying the distant metastasis during the lifetime or the miscarriage during pregnancy, individuals that have died without a metastasis or have given birth are certainly non-susceptible. The same also holds when studying the x-year overall survival or the death during hospital stay. Common MC models ignore this information and consider them all censored, thus yielding in risk of assigning low immune probabilities to cured individuals. In this study, we consider a MC model that incorporates known information on cured individuals, with the time to cure identification being either deterministic or stochastic. We use the expectation-maximization algorithm to derive the maximum likelihood estimators. Furthermore, we compare different strategies that account for cure information such as (1) assigning infinite times to event for known cured cases and adjusting the traditional model and (2) considering only the probability of cure identification but ignoring the time until that happens. Theoretical results and simulations demonstrate the value of the proposed model especially when the time to cure identification is stochastic, increasing precision and decreasing the mean squared error. On the other hand, the traditional models that ignore the known cured information perform well when the curation is achieved after a known cutoff point. Moreover, through simulations the comparisons of the different strategies are examined, as possible alternatives to the complete-information model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06027v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Karakatsoulis</dc:creator>
    </item>
    <item>
      <title>Bayesian Clustering Prior with Overlapping Indices for Effective Use of Multisource External Data</title>
      <link>https://arxiv.org/abs/2412.06098</link>
      <description>arXiv:2412.06098v1 Announce Type: new 
Abstract: The use of external data in clinical trials offers numerous advantages, such as reducing the number of patients, increasing study power, and shortening trial durations. In Bayesian inference, information in external data can be transferred into an informative prior for future borrowing (i.e., prior synthesis). However, multisource external data often exhibits heterogeneity, which can lead to information distortion during the prior synthesis. Clustering helps identifying the heterogeneity, enhancing the congruence between synthesized prior and external data, thereby preventing information distortion. Obtaining optimal clustering is challenging due to the trade-off between congruence with external data and robustness to future data. We introduce two overlapping indices: the overlapping clustering index (OCI) and the overlapping evidence index (OEI). Using these indices alongside a K-Means algorithm, the optimal clustering of external data can be identified by balancing the trade-off. Based on the clustering result, we propose a prior synthesis framework to effectively borrow information from multisource external data. By incorporating the (robust) meta-analytic predictive prior into this framework, we develop (robust) Bayesian clustering MAP priors. Simulation studies and real-data analysis demonstrate their superiority over commonly used priors in the presence of heterogeneity. Since the Bayesian clustering priors are constructed without needing data from the prospective study to be conducted, they can be applied to both study design and data analysis in clinical trials or experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06098v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuetao Lu, J. Jack Lee</dc:creator>
    </item>
    <item>
      <title>Randomized interventional effects for semicompeting risks, with application to allogeneic stem cell transplantation study</title>
      <link>https://arxiv.org/abs/2412.06114</link>
      <description>arXiv:2412.06114v1 Announce Type: new 
Abstract: In clinical studies, the risk of the terminal event can be modified by an intermediate event, resulting in semicompeting risks. To study the treatment effect on the terminal event mediated by the intermediate event, it is of interest to decompose the total effect into direct and indirect effects. Three typical frameworks for causal mediation analysis have been proposed: natural effects, separable effects and interventional effects. In this article, we extend the interventional approach to time-to-event data, and compare it with other frameworks. With no time-varying confounders, these three frameworks lead to the same identification formula. With time-varying confounders, the interventional effects framework outperforms the other two because it requires weaker assumptions and fewer restrictions on time-varying confounders for identification. We present the identification formula for interventional effects and discuss some variants of the identification assumptions. As an illustration, we study the effect of transplant modalities on death mediated by relapse in an allogeneic stem cell transplantation study to treat leukemia with a time-varying confounder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06114v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Deng, Rui Wang</dc:creator>
    </item>
    <item>
      <title>Sandwich regression for accurate and robust estimation in generalized linear multilevel and longitudinal models</title>
      <link>https://arxiv.org/abs/2412.06119</link>
      <description>arXiv:2412.06119v1 Announce Type: new 
Abstract: Generalized linear models are a popular tool in applied statistics, with their maximum likelihood estimators enjoying asymptotic Gaussianity and efficiency. As all models are wrong, it is desirable to understand these estimators' behaviours under model misspecification. We study semiparametric multilevel generalized linear models, where only the conditional mean of the response is taken to follow a specific parametric form. Pre-existing estimators from mixed effects models and generalized estimating equations require specificaiton of a conditional covariance, which when misspecified can result in inefficient estimates of fixed effects parameters. It is nevertheless often computationally attractive to consider a restricted, finite dimensional class of estimators, as these models naturally imply. We introduce sandwich regression, that selects the estimator of minimal variance within a parametric class of estimators over all distributions in the full semiparametric model. We demonstrate numerically on simulated and real data the attractive improvements our sandwich regression approach enjoys over classical mixed effects models and generalized estimating equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06119v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elliot H. Young, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>SID: A Novel Class of Nonparametric Tests of Independence for Censored Outcomes</title>
      <link>https://arxiv.org/abs/2412.06311</link>
      <description>arXiv:2412.06311v1 Announce Type: new 
Abstract: We propose a new class of metrics, called the survival independence divergence (SID), to test dependence between a right-censored outcome and covariates. A key technique for deriving the SIDs is to use a counting process strategy, which equivalently transforms the intractable independence test due to the presence of censoring into a test problem for complete observations. The SIDs are equal to zero if and only if the right-censored response and covariates are independent, and they are capable of detecting various types of nonlinear dependence. We propose empirical estimates of the SIDs and establish their asymptotic properties. We further develop a wild bootstrap method to estimate the critical values and show the consistency of the bootstrap tests. The numerical studies demonstrate that our SID-based tests are highly competitive with existing methods in a wide range of settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06311v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinhong Li, Jicai Liu, Jinhong You, Riquan Zhang</dc:creator>
    </item>
    <item>
      <title>Efficiency of nonparametric superiority tests based on restricted mean survival time versus the log-rank test under proportional hazards</title>
      <link>https://arxiv.org/abs/2412.06442</link>
      <description>arXiv:2412.06442v1 Announce Type: new 
Abstract: Background: For RCTs with time-to-event endpoints, proportional hazard (PH) models are typically used to estimate treatment effects and logrank tests are commonly used for hypothesis testing. There is growing support for replacing this approach with a model-free estimand and assumption-lean analysis method. One alternative is to base the analysis on the difference in restricted mean survival time (RMST) at a specific time, a single-number summary measure that can be defined without any restrictive assumptions on the outcome model. In a simple setting without covariates, an assumption-lean analysis can be achieved using nonparametric methods such as Kaplan Meier estimation. The main advantage of moving to a model-free summary measure and assumption-lean analysis is that the validity and interpretation of conclusions do not depend on the PH assumption. The potential disadvantage is that the nonparametric analysis may lose efficiency under PH. There is disagreement in recent literature on this issue. Methods: Asymptotic results and simulations are used to compare the efficiency of a log-rank test against a nonparametric analysis of the difference in RMST in a superiority trial under PH. Previous studies have separately examined the effect of event rates and the censoring distribution on relative efficiency. This investigation clarifies conflicting results from earlier research by exploring the joint effect of event rate and censoring distribution together. Several illustrative examples are provided. Results: In scenarios with high event rates and/or substantial censoring across a large proportion of the study window, and when both methods make use of the same amount of data, relative efficiency is close to unity. However, in cases with low event rates but when censoring is concentrated at the end of the study window, the PH analysis has a considerable efficiency advantage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06442v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Magirr, Craig Wang, Xinlei Deng, Tim Morris, Mark Baillie</dc:creator>
    </item>
    <item>
      <title>Partial identification of principal causal effects under violations of principal ignorability</title>
      <link>https://arxiv.org/abs/2412.06628</link>
      <description>arXiv:2412.06628v1 Announce Type: new 
Abstract: Principal stratification is a general framework for studying causal mechanisms involving post-treatment variables. When estimating principal causal effects, the principal ignorability assumption is commonly invoked, which we study in detail in this manuscript. Our first key contribution is studying a commonly used strategy of using parametric models to jointly model the outcome and principal strata without requiring the principal ignorability assumption. We show that even if the joint distribution of principal strata is known, this strategy necessarily leads to only partial identification of causal effects, even under very simple and correctly specified outcome models. While principal ignorability can lead to point identification in this setting, we discuss alternative, weaker assumptions and show how they lead to more informative partial identification regions. An additional contribution is that we provide theoretical support to strategies used in the literature for identifying association parameters that govern the joint distribution of principal strata. We prove that this is possible, but only if the principal ignorability assumption is violated. Additionally, due to partial identifiability of causal effects even when these association parameters are known, we show that these association parameters are only identifiable under strong parametric constraints. Lastly, we extend these results to more flexible semiparametric and nonparametric Bayesian models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06628v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minxuan Wu, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Testing Mutual Independence in Metric Spaces Using Distance Profiles</title>
      <link>https://arxiv.org/abs/2412.06766</link>
      <description>arXiv:2412.06766v1 Announce Type: new 
Abstract: This paper introduces a novel unified framework for testing mutual independence among a vector of random objects that may reside in different metric spaces, including some existing methodologies as special cases. The backbone of the proposed tests is the notion of joint distance profiles, which uniquely characterize the joint law of random objects under a mild condition on the joint law or on the metric spaces. Our test statistics measure the difference of the joint distance profiles of each data point with respect to the joint law and the product of marginal laws of the vector of random objects, where flexible data-adaptive weight profiles are incorporated for power enhancement. We derive the limiting distribution of the test statistics under the null hypothesis of mutual independence and show that the proposed tests with specific weight profiles are asymptotically distribution-free if the marginal distance profiles are continuous. We also establish the consistency of the tests under sequences of alternative hypotheses converging to the null. Furthermore, since the asymptotic tests with non-trivial weight profiles require the knowledge of the underlying data distribution, we adopt a permutation scheme to approximate the $p$-values and provide theoretical guarantees that the permutation-based tests control the type I error rate under the null and are consistent under the alternatives. We demonstrate the power of the proposed tests across various types of data objects through simulations and real data applications, where our tests are shown to have superior performance compared with popular existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06766v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaqing Chen, Paromita Dubey</dc:creator>
    </item>
    <item>
      <title>Ranking of Large Language Model with Nonparametric Prompts</title>
      <link>https://arxiv.org/abs/2412.05506</link>
      <description>arXiv:2412.05506v1 Announce Type: cross 
Abstract: We consider the inference for the ranking of large language models (LLMs). Alignment arises as a big challenge to mitigate hallucinations in the use of LLMs. Ranking LLMs has been shown as a well-performing tool to improve alignment based on the best-of-$N$ policy. In this paper, we propose a new inferential framework for testing hypotheses and constructing confidence intervals of the ranking of language models. We consider the widely adopted Bradley-Terry-Luce (BTL) model, where each item is assigned a positive preference score that determines its pairwise comparisons' outcomes. We further extend it into the contextual setting, where the score of each model varies with the prompt. We show the convergence rate of our estimator. By extending the current Gaussian multiplier bootstrap theory to accommodate the supremum of not identically distributed empirical processes, we construct the confidence interval for ranking and propose a valid testing procedure. We also introduce the confidence diagram as a global ranking property. We conduct numerical experiments to assess the performance of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05506v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zebin Wang, Yi Han, Ethan X. Fang, Lan Wang, Junwei Lu</dc:creator>
    </item>
    <item>
      <title>Exact distribution-free tests of spherical symmetry applicable to high dimensional data</title>
      <link>https://arxiv.org/abs/2412.05608</link>
      <description>arXiv:2412.05608v1 Announce Type: cross 
Abstract: We develop some graph-based tests for spherical symmetry of a multivariate distribution using a method based on data augmentation. These tests are constructed using a new notion of signs and ranks that are computed along a path obtained by optimizing an objective function based on pairwise dissimilarities among the observations in the augmented data set. The resulting tests based on these signs and ranks have the exact distribution-free property, and irrespective of the dimension of the data, the null distributions of the test statistics remain the same. These tests can be conveniently used for high-dimensional data, even when the dimension is much larger than the sample size. Under appropriate regularity conditions, we prove the consistency of these tests in high dimensional asymptotic regime, where the dimension grows to infinity while the sample size may or may not grow with the dimension. We also propose a generalization of our methods to take care of the situations, where the center of symmetry is not specified by the null hypothesis. Several simulated data sets and a real data set are analyzed to demonstrate the utility of the proposed tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05608v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilol Banerjee, Anil K. Ghosh</dc:creator>
    </item>
    <item>
      <title>Leveraging Black-box Models to Assess Feature Importance in Unconditional Distribution</title>
      <link>https://arxiv.org/abs/2412.05759</link>
      <description>arXiv:2412.05759v1 Announce Type: cross 
Abstract: Understanding how changes in explanatory features affect the unconditional distribution of the outcome is important in many applications. However, existing black-box predictive models are not readily suited for analyzing such questions. In this work, we develop an approximation method to compute the feature importance curves relevant to the unconditional distribution of outcomes, while leveraging the power of pre-trained black-box predictive models. The feature importance curves measure the changes across quantiles of outcome distribution given an external impact of change in the explanatory features. Through extensive numerical experiments and real data examples, we demonstrate that our approximation method produces sparse and faithful results, and is computationally efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05759v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Zhou, Chunlin Li</dc:creator>
    </item>
    <item>
      <title>Infinite Mixture Models for Improved Modeling of Across-Site Evolutionary Variation</title>
      <link>https://arxiv.org/abs/2412.06042</link>
      <description>arXiv:2412.06042v1 Announce Type: cross 
Abstract: Scientific studies in many areas of biology routinely employ evolutionary analyses based on the probabilistic inference of phylogenetic trees from molecular sequence data. Evolutionary processes that act at the molecular level are highly variable, and properly accounting for heterogeneity in evolutionary processes is crucial for more accurate phylogenetic inference. Nucleotide substitution rates and patterns are known to vary among sites in multiple sequence alignments, and such variation can be modeled by partitioning alignments into categories corresponding to different substitution models. Determining $\textit{a priori}$ appropriate partitions can be difficult, however, and better model fit can be achieved through flexible Bayesian infinite mixture models that simultaneously infer the number of partitions, the partition that each site belongs to, and the evolutionary parameters corresponding to each partition. Here, we consider several different types of infinite mixture models, including classic Dirichlet process mixtures, as well as novel approaches for modeling across-site evolutionary variation: hierarchical models for data with a natural group structure, and infinite hidden Markov models that account for spatial patterns in alignments. In analyses of several viral data sets, we find that different types of infinite mixture models emerge as the best choices in different scenarios. To enable these models to scale efficiently to large data sets, we adapt efficient Markov chain Monte Carlo algorithms and exploit opportunities for parallel computing. We implement this infinite mixture modeling framework in BEAST X, a widely-used software package for Bayesian phylogenetic inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06042v1</guid>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mandev S. Gill, Guy Baele, Marc A. Suchard, Philippe Lemey</dc:creator>
    </item>
    <item>
      <title>Ethnography and Machine Learning: Synergies and New Directions</title>
      <link>https://arxiv.org/abs/2412.06087</link>
      <description>arXiv:2412.06087v1 Announce Type: cross 
Abstract: Ethnography (social scientific methods that illuminate how people understand, navigate and shape the real world contexts in which they live their lives) and machine learning (computational techniques that use big data and statistical learning models to perform quantifiable tasks) are each core to contemporary social science. Yet these tools have remained largely separate in practice. This chapter draws on a growing body of scholarship that argues that ethnography and machine learning can be usefully combined, particularly for large comparative studies. Specifically, this paper (a) explains the value (and challenges) of using machine learning alongside qualitative field research for certain types of projects, (b) discusses recent methodological trends to this effect, (c) provides examples that illustrate workflow drawn from several large projects, and (d) concludes with a roadmap for enabling productive coevolution of field methods and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06087v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1093/oxfordhb/9780197653609.013.36</arxiv:DOI>
      <arxiv:journal_reference>Borch, and Pardo-Guerra (eds), The Oxford Handbook of the Sociology of Machine Learning (2024)</arxiv:journal_reference>
      <dc:creator>Zhuofan Li, Corey M. Abramson</dc:creator>
    </item>
    <item>
      <title>Optimal estimation in private distributed functional data analysis</title>
      <link>https://arxiv.org/abs/2412.06582</link>
      <description>arXiv:2412.06582v1 Announce Type: cross 
Abstract: We systematically investigate the preservation of differential privacy in functional data analysis, beginning with functional mean estimation and extending to varying coefficient model estimation. Our work introduces a distributed learning framework involving multiple servers, each responsible for collecting several sparsely observed functions. This hierarchical setup introduces a mixed notion of privacy. Within each function, user-level differential privacy is applied to $m$ discrete observations. At the server level, central differential privacy is deployed to account for the centralised nature of data collection. Across servers, only private information is exchanged, adhering to federated differential privacy constraints. To address this complex hierarchy, we employ minimax theory to reveal several fundamental phenomena: from sparse to dense functional data analysis, from user-level to central and federated differential privacy costs, and the intricate interplay between different regimes of functional data analysis and privacy preservation.
  To the best of our knowledge, this is the first study to rigorously examine functional data estimation under multiple privacy constraints. Our theoretical findings are complemented by efficient private algorithms and extensive numerical evidence, providing a comprehensive exploration of this challenging problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06582v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gengyu Xue, Zhenhua Lin, Yi Yu</dc:creator>
    </item>
    <item>
      <title>A unified quantile framework for nonlinear heterogeneous transcriptome-wide associations</title>
      <link>https://arxiv.org/abs/2207.12081</link>
      <description>arXiv:2207.12081v3 Announce Type: replace 
Abstract: Transcriptome-wide association studies (TWAS) are powerful tools for identifying gene-level associations by integrating genome-wide association studies and gene expression data. However, most TWAS methods focus on linear associations between genes and traits, ignoring the complex nonlinear relationships that may be present in biological systems. To address this limitation, we propose a novel framework, QTWAS, which integrates a quantile-based gene expression model into the TWAS model, allowing for the discovery of nonlinear and heterogeneous gene-trait associations. Via comprehensive simulations and applications to both continuous and binary traits, we demonstrate that the proposed model is more powerful than conventional TWAS in identifying gene-trait associations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.12081v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianying Wang, Iuliana Ionita-Laza, Ying Wei</dc:creator>
    </item>
    <item>
      <title>Quantile Fourier Transform, Quantile Series, and Nonparametric Estimation of Quantile Spectra</title>
      <link>https://arxiv.org/abs/2211.05844</link>
      <description>arXiv:2211.05844v2 Announce Type: replace 
Abstract: A nonparametric method is proposed for estimating the quantile spectra and cross-spectra introduced in Li (2012; 2014) as bivariate functions of frequency and quantile level. The method is based on the quantile discrete Fourier transform (QDFT) defined by trigonometric quantile regression and the quantile series (QSER) defined by the inverse Fourier transform of the QDFT. A nonparametric spectral estimator is constructed from the autocovariance function of the QSER using the lag-window (LW) approach. Smoothing techniques are also employed to reduce the statistical variability of the LW estimator across quantiles when the underlying spectrum varies smoothly with respect to the quantile level. The performance of the proposed estimation method is evaluated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.05844v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ta-Hsin Li</dc:creator>
    </item>
    <item>
      <title>A New Spatio-Temporal Model Exploiting Hamiltonian Equations</title>
      <link>https://arxiv.org/abs/2211.13478</link>
      <description>arXiv:2211.13478v3 Announce Type: replace 
Abstract: The solutions of Hamiltonian equations are known to describe the underlying phase space of a mechanical system. In this article, we propose a novel spatio-temporal model using a strategic modification of the Hamiltonian equations, incorporating appropriate stochasticity via Gaussian processes. The resultant spatio-temporal process, continuously varying with time, turns out to be nonparametric, non-stationary, non-separable, and non-Gaussian. Additionally, the lagged correlations converge to zero as the spatio-temporal lag goes to infinity. We investigate the theoretical properties of the new spatio-temporal process, including its continuity and smoothness properties. We derive methods for complete Bayesian inference using MCMC techniques in the Bayesian paradigm. The performance of our method has been compared with that of a non-stationary Gaussian process (GP) using two simulation studies, where our method shows a significant improvement over the non-stationary GP. Further, applying our new model to two real data sets revealed encouraging performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.13478v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satyaki Mazumder, Sayantan Banerjee, Sourabh Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Distribution free MMD tests for model selection with estimated parameters</title>
      <link>https://arxiv.org/abs/2305.07549</link>
      <description>arXiv:2305.07549v2 Announce Type: replace 
Abstract: There exist some testing procedures based on the maximum mean discrepancy (MMD) to address the challenge of model specification. However, they ignore the presence of estimated parameters in the case of composite null hypotheses. In this paper, we first illustrate the effect of parameter estimation in model specification tests based on the MMD. Second, we propose simple model specification and model selection tests in the case of models with estimated parameters. All our tests are asymptotically standard normal under the null, even when the true underlying distribution belongs to the competing parametric families. A simulation study and a real data analysis illustrate the performance of our tests in terms of power and level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07549v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Br\"uck, Jean-David Fermanian, Aleksey Min</dc:creator>
    </item>
    <item>
      <title>When Composite Likelihood Meets Stochastic Approximation</title>
      <link>https://arxiv.org/abs/2310.04165</link>
      <description>arXiv:2310.04165v2 Announce Type: replace 
Abstract: A composite likelihood is an inference function derived by multiplying a set of likelihood components. This approach provides a flexible framework for drawing inference when the likelihood function of a statistical model is computationally intractable. While composite likelihood has computational advantages, it can still be demanding when dealing with numerous likelihood components and a large sample size. This paper tackles this challenge by employing an approximation of the conventional composite likelihood estimator, which is derived from an optimization procedure relying on stochastic gradients. This novel estimator is shown to be asymptotically normally distributed around the true parameter. In particular, based on the relative divergent rate of the sample size and the number of iterations of the optimization, the variance of the limiting distribution is shown to compound for two sources of uncertainty: the sampling variability of the data and the optimization noise, with the latter depending on the sampling distribution used to construct the stochastic gradients. The advantages of the proposed framework are illustrated through simulation studies on two working examples: an Ising model for binary data and a gamma frailty model for count data. Finally, a real-data application is presented, showing its effectiveness in a large-scale mental health survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04165v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2436219</arxiv:DOI>
      <dc:creator>Giuseppe Alfonzetti, Ruggero Bellio, Yunxiao Chen, Irini Moustaki</dc:creator>
    </item>
    <item>
      <title>A flexible Bayesian g-formula for causal survival analyses with time-dependent confounding</title>
      <link>https://arxiv.org/abs/2402.02306</link>
      <description>arXiv:2402.02306v3 Announce Type: replace 
Abstract: In longitudinal observational studies with time-to-event outcomes, a common objective in causal analysis is to estimate the causal survival curve under hypothetical intervention scenarios. The g-formula is a useful tool for this analysis. To enhance the traditional parametric g-formula, we developed an alternative g-formula estimator, which incorporates the Bayesian Additive Regression Trees (BART) into the modeling of the time-evolving generative components, aiming to mitigate the bias due to model misspecification. We focus on binary time-varying treatments and introduce a general class of g-formulas for discrete survival data that can incorporate the longitudinal balancing scores. The minimum sufficient formulation of these longitudinal balancing scores is linked to the nature of treatment strategies, i.e., static or dynamic. For each type of treatment strategy, we provide posterior sampling algorithms. We conducted simulations to illustrate the empirical performance of the proposed method and demonstrate its practical utility using data from the Yale New Haven Health System's electronic health records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02306v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Liangyuan Hu, Fan Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Functional Graphical Models with Change-Point Detection</title>
      <link>https://arxiv.org/abs/2405.03041</link>
      <description>arXiv:2405.03041v2 Announce Type: replace 
Abstract: Functional data analysis, which models data as realizations of random functions over a continuum, has emerged as a useful tool for time series data. Often, the goal is to infer the dynamic connections (or time-varying conditional dependencies) among multiple functions or time series. For this task, a dynamic and Bayesian functional graphical model is introduced. The proposed modeling approach prioritizes the careful definition of an appropriate graph to identify both time-invariant and time-varying connectivity patterns. A novel block-structured sparsity prior is paired with a finite basis expansion, which together yield effective shrinkage and graph selection with efficient computations via a Gibbs sampling algorithm. Crucially, the model includes (one or more) graph changepoints, which are learned jointly with all model parameters and incorporate graph dynamics. Simulation studies demonstrate excellent graph selection capabilities, with significant improvements over competing methods. The proposed approach is applied to study of dynamic connectivity patterns of sea surface temperatures in the Pacific Ocean and reveals meaningful edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03041v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chunshan Liu, Daniel R. Kowal, James Doss-Gollin, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Semi-parametric least-area linear-circular regression through M\"{o}bius transformation</title>
      <link>https://arxiv.org/abs/2411.15822</link>
      <description>arXiv:2411.15822v2 Announce Type: replace 
Abstract: This paper introduces a novel regression model designed for angular response variables with linear predictors, utilizing a generalized M\"{o}bius transformation to define the regression curve. By mapping the real axis to the circle, the model effectively captures the relationship between linear and angular components. A key innovation is the introduction of an area-based loss function, inspired by the geometry of a curved torus, for efficient parameter estimation. The semi-parametric nature of the model eliminates the need for specific distributional assumptions about the angular error, enhancing its versatility. Extensive simulation studies, incorporating von Mises and wrapped Cauchy distributions, highlight the robustness of the framework. The model's practical utility is demonstrated through real-world data analysis of Bitcoin and Ethereum, showcasing its ability to derive meaningful insights from complex data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15822v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Surojit Biswas, Buddhananda Banerjee</dc:creator>
    </item>
    <item>
      <title>Piecewise-linear modeling of multivariate geometric extremes</title>
      <link>https://arxiv.org/abs/2412.05195</link>
      <description>arXiv:2412.05195v2 Announce Type: replace 
Abstract: A recent development in extreme value modeling uses the geometry of the dataset to perform inference on the multivariate tail. A key quantity in this inference is the gauge function, whose values define this geometry. Methodology proposed to date for capturing the gauge function either lacks flexibility due to parametric specifications, or relies on complex neural network specifications in dimensions greater than three. We propose a semiparametric gauge function that is piecewise-linear, making it simple to interpret and provides a good approximation for the true underlying gauge function. This linearity also makes optimization tasks computationally inexpensive. The piecewise-linear gauge function can be used to define both a radial and an angular model, allowing for the joint fitting of extremal pseudo-polar coordinates, a key aspect of this geometric framework. We further expand the toolkit for geometric extremal modeling through the estimation of high radial quantiles at given angular values via kernel density estimation. We apply the new methodology to air pollution data, which exhibits a complex extremal dependence structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05195v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ryan Campbell, Jennifer Wadsworth</dc:creator>
    </item>
    <item>
      <title>Residual permutation test for regression coefficient testing</title>
      <link>https://arxiv.org/abs/2211.16182</link>
      <description>arXiv:2211.16182v3 Announce Type: replace-cross 
Abstract: We consider the problem of testing whether a single coefficient is equal to zero in linear models when the dimension of covariates $p$ can be up to a constant fraction of sample size $n$. In this regime, an important topic is to propose tests with finite-population valid size control without requiring the noise to follow strong distributional assumptions. In this paper, we propose a new method, called residual permutation test (RPT), which is constructed by projecting the regression residuals onto the space orthogonal to the union of the column spaces of the original and permuted design matrices. RPT can be proved to achieve finite-population size validity under fixed design with just exchangeable noises, whenever $p &lt; n / 2$. Moreover, RPT is shown to be asymptotically powerful for heavy tailed noises with bounded $(1+t)$-th order moment when the true coefficient is at least of order $n^{-t/(1+t)}$ for $t \in [0,1]$. We further proved that this signal size requirement is essentially rate-optimal in the minimax sense. Numerical studies confirm that RPT performs well in a wide range of simulation settings with normal and heavy-tailed noise distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16182v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiyue Wen, Tengyao Wang, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>Statistical Tests for Replacing Human Decision Makers with Algorithms</title>
      <link>https://arxiv.org/abs/2306.11689</link>
      <description>arXiv:2306.11689v2 Announce Type: replace-cross 
Abstract: This paper proposes a statistical framework of using artificial intelligence to improve human decision making. The performance of each human decision maker is benchmarked against that of machine predictions. We replace the diagnoses made by a subset of the decision makers with the recommendation from the machine learning algorithm. We apply both a heuristic frequentist approach and a Bayesian posterior loss function approach to abnormal birth detection using a nationwide dataset of doctor diagnoses from prepregnancy checkups of reproductive age couples and pregnancy outcomes. We find that our algorithm on a test dataset results in a higher overall true positive rate and a lower false positive rate than the diagnoses made by doctors only.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11689v2</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Feng, Han Hong, Ke Tang, Jingyuan Wang</dc:creator>
    </item>
    <item>
      <title>One-step smoothing splines instrumental regression</title>
      <link>https://arxiv.org/abs/2307.14867</link>
      <description>arXiv:2307.14867v4 Announce Type: replace-cross 
Abstract: We extend nonparametric regression smoothing splines to a context where there is endogeneity and instrumental variables are available. Unlike popular existing estimators, the resulting estimator is one-step and relies on a unique regularization parameter. We derive rates of the convergence for the estimator and its first derivative, which are uniform in the support of the endogenous variable. We also address the issue of imposing monotonicity in estimation and extend the approach to a partly linear model. Simulations confirm the good performances of our estimator compared to two-step procedures. Our method yields economically sensible results when used to estimate Engel curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14867v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jad Beyhum, Elia Lapenta, Pascal Lavergne</dc:creator>
    </item>
    <item>
      <title>Estimating Effects of Long-Term Treatments</title>
      <link>https://arxiv.org/abs/2308.08152</link>
      <description>arXiv:2308.08152v2 Announce Type: replace-cross 
Abstract: Estimating the effects of long-term treatments through A/B testing is challenging. Treatments, such as updates to product functionalities, user interface designs, and recommendation algorithms, are intended to persist within the system for a long duration of time after their initial launches. However, due to the constraints of conducting long-term experiments, practitioners often rely on short-term experimental results to make product launch decisions. It remains open how to accurately estimate the effects of long-term treatments using short-term experimental data. To address this question, we introduce a longitudinal surrogate framework that decomposes the long-term effects into functions based on user attributes, short-term metrics, and treatment assignments. We outline identification assumptions, estimation strategies, inferential techniques, and validation methods under this framework. Empirically, we demonstrate that our approach outperforms existing solutions by using data from two real-world experiments, each involving more than a million users on WeChat, one of the world's largest social networking platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08152v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Huang (Jingjing), Chen Wang (Jingjing), Yuan Yuan (Jingjing), Jinglong Zhao (Jingjing),  Brocco (Jingjing),  Zhang</dc:creator>
    </item>
    <item>
      <title>Learning to Simulate: Generative Metamodeling via Quantile Regression</title>
      <link>https://arxiv.org/abs/2311.17797</link>
      <description>arXiv:2311.17797v2 Announce Type: replace-cross 
Abstract: Stochastic simulation models effectively capture complex system dynamics but are often too slow for real-time decision-making. Traditional metamodeling techniques learn relationships between simulator inputs and a single output summary statistic, such as the mean or median. These techniques enable real-time predictions without additional simulations. However, they require prior selection of one appropriate output summary statistic, limiting their flexibility in practical applications. We propose a new concept: generative metamodeling. It aims to construct a "fast simulator of the simulator," generating random outputs significantly faster than the original simulator while preserving approximately equal conditional distributions. Generative metamodels enable rapid generation of numerous random outputs upon input specification, facilitating immediate computation of any summary statistic for real-time decision-making. We introduce a new algorithm, quantile-regression-based generative metamodeling (QRGMM), and establish its distributional convergence and convergence rate. Extensive numerical experiments demonstrate QRGMM's efficacy compared to other state-of-the-art generative algorithms in practical real-time decision-making scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17797v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>L. Jeff Hong, Yanxi Hou, Qingkai Zhang, Xiaowei Zhang</dc:creator>
    </item>
    <item>
      <title>Comparative Study of Causal Discovery Methods for Cyclic Models with Hidden Confounders</title>
      <link>https://arxiv.org/abs/2401.13009</link>
      <description>arXiv:2401.13009v2 Announce Type: replace-cross 
Abstract: Nowadays, the need for causal discovery is ubiquitous. A better understanding of not just the stochastic dependencies between parts of a system, but also the actual cause-effect relations, is essential for all parts of science. Thus, the need for reliable methods to detect causal directions is growing constantly. In the last 50 years, many causal discovery algorithms have emerged, but most of them are applicable only under the assumption that the systems have no feedback loops and that they are causally sufficient, i.e. that there are no unmeasured subsystems that can affect multiple measured variables. This is unfortunate since those restrictions can often not be presumed in practice. Feedback is an integral feature of many processes, and real-world systems are rarely completely isolated and fully measured. Fortunately, in recent years, several techniques, that can cope with cyclic, causally insufficient systems, have been developed. And with multiple methods available, a practical application of those algorithms now requires knowledge of the respective strengths and weaknesses. Here, we focus on the problem of causal discovery for sparse linear models which are allowed to have cycles and hidden confounders. We have prepared a comprehensive and thorough comparative study of four causal discovery techniques: two versions of the LLC method [10] and two variants of the ASP-based algorithm [11]. The evaluation investigates the performance of those techniques for various experiments with multiple interventional setups and different dataset sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13009v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boris Lorbeer, Mustafa Mohsen</dc:creator>
    </item>
    <item>
      <title>Auditing Fairness under Unobserved Confounding</title>
      <link>https://arxiv.org/abs/2403.14713</link>
      <description>arXiv:2403.14713v3 Announce Type: replace-cross 
Abstract: Many definitions of fairness or inequity involve unobservable causal quantities that cannot be directly estimated without strong assumptions. For instance, it is particularly difficult to estimate notions of fairness that rely on hard-to-measure concepts such as risk (e.g., quantifying whether patients at the same risk level have equal probability of treatment, regardless of group membership). Such measurements of risk can be accurately obtained when no unobserved confounders have jointly influenced past decisions and outcomes. However, in the real world, this assumption rarely holds. In this paper, we show that, surprisingly, one can still compute meaningful bounds on treatment rates for high-risk individuals (i.e., conditional on their true, \textit{unobserved} negative outcome), even when entirely eliminating or relaxing the assumption that we observe all relevant risk factors used by decision makers. We use the fact that in many real-world settings (e.g., the release of a new treatment) we have data from prior to any allocation to derive unbiased estimates of risk. This result enables us to audit unfair outcomes of existing decision-making systems in a principled manner. We demonstrate the effectiveness of our framework with a real-world study of Paxlovid allocation, provably identifying that observed racial inequity cannot be explained by unobserved confounders of the same strength as important observed covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14713v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yewon Byun, Dylan Sam, Michael Oberst, Zachary C. Lipton, Bryan Wilder</dc:creator>
    </item>
    <item>
      <title>Covariate-Adjusted Functional Data Analysis for Structural Health Monitoring</title>
      <link>https://arxiv.org/abs/2408.02106</link>
      <description>arXiv:2408.02106v2 Announce Type: replace-cross 
Abstract: Structural Health Monitoring (SHM) is increasingly applied in civil engineering. One of its primary purposes is detecting and assessing changes in structure conditions to increase safety and reduce potential maintenance downtime. Recent advancements, especially in sensor technology, facilitate data measurements, collection, and process automation, leading to large data streams. We propose a function-on-function regression framework for (nonlinear) modeling the sensor data and adjusting for covariate-induced variation. Our approach is particularly suited for long-term monitoring when several months or years of training data are available. It combines highly flexible yet interpretable semi-parametric modeling with functional principal component analysis and uses the corresponding out-of-sample Phase-II scores for monitoring. The method proposed can also be described as a combination of an ``input-output'' and an ``output-only'' method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02106v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Wittenberg, Lizzie Neumann, Alexander Mendler, Jan Gertheiss</dc:creator>
    </item>
    <item>
      <title>On the Unknowable Limits to Prediction</title>
      <link>https://arxiv.org/abs/2411.19223</link>
      <description>arXiv:2411.19223v3 Announce Type: replace-cross 
Abstract: This short Correspondence critiques the classic dichotomization of prediction error into reducible and irreducible components, noting that certain types of error can be eliminated at differential speeds. We propose an improved analytical framework that better distinguishes epistemic from aleatoric uncertainty, emphasizing that predictability depends on information sets and cautioning against premature claims of unpredictability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19223v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiani Yan, Charles Rahal</dc:creator>
    </item>
  </channel>
</rss>
