<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Apr 2024 04:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Eigenvector distributions and optimal shrinkage estimators for large covariance and precision matrices</title>
      <link>https://arxiv.org/abs/2404.14751</link>
      <description>arXiv:2404.14751v1 Announce Type: new 
Abstract: This paper focuses on investigating Stein's invariant shrinkage estimators for large sample covariance matrices and precision matrices in high-dimensional settings. We consider models that have nearly arbitrary population covariance matrices, including those with potential spikes. By imposing mild technical assumptions, we establish the asymptotic limits of the shrinkers for a wide range of loss functions. A key contribution of this work, enabling the derivation of the limits of the shrinkers, is a novel result concerning the asymptotic distributions of the non-spiked eigenvectors of the sample covariance matrices, which can be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14751v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiucai Ding, Yun Li, Fan Yang</dc:creator>
    </item>
    <item>
      <title>An Introduction to Complex Random Tensors</title>
      <link>https://arxiv.org/abs/2404.15170</link>
      <description>arXiv:2404.15170v1 Announce Type: new 
Abstract: This work considers the notion of random tensors and reviews some fundamental concepts in statistics when applied to a tensor based data or signal. In several engineering fields such as Communications, Signal Processing, Machine learning, and Control systems, the concepts of linear algebra combined with random variables have been indispensable tools. With the evolution of these subjects to multi-domain communication systems, multi-way signal processing, high dimensional data analysis, and multi-linear systems theory, there is a need to bring in multi-linear algebra equipped with the notion of random tensors. Also, since several such application areas deal with complex-valued entities, it is imperative to study this subject from a complex random tensor perspective, which is the focus of this paper. Using tools from multi-linear algebra, we characterize statistical properties of complex random tensors, both proper and improper, study various correlation structures, and fundamentals of tensor valued random processes. Furthermore, the asymptotic distribution of various tensor eigenvalue and singular value definitions is also considered, which is used for the study of spiked real tensor models that deals with recovery of low rank tensor signals perturbed by noise. This paper aims to provide an overview of the state of the art in random tensor theory of both complex and real valued tensors, for the purpose of enabling its application in engineering and applied science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15170v1</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divyanshu Pandey, Alexis Decurninge, Harry Leib</dc:creator>
    </item>
    <item>
      <title>Hoeffding's inequality for continuous-time Markov chains</title>
      <link>https://arxiv.org/abs/2404.14888</link>
      <description>arXiv:2404.14888v1 Announce Type: cross 
Abstract: Hoeffding's inequality is a fundamental tool widely applied in probability theory, statistics, and machine learning. In this paper, we establish Hoeffding's inequalities specifically tailored for an irreducible and positive recurrent continuous-time Markov chain (CTMC) on a countable state space with the invariant probability distribution ${\pi}$ and an $\mathcal{L}^{2}(\pi)$-spectral gap ${\lambda}(Q)$. More precisely, for a function $g:E\to [a,b]$ with a mean $\pi(g)$, and given $t,\varepsilon&gt;0$, we derive the inequality \[ \mathbb{P}_{\pi}\left(\frac{1}{t} \int_{0}^{t} g\left(X_{s}\right)\mathrm{d}s-\pi (g) \geq \varepsilon \right) \leq \exp\left\{-\frac{{\lambda}(Q)t\varepsilon^2}{(b-a)^2} \right\}, \] which can be viewed as a generalization of Hoeffding's inequality for discrete-time Markov chains (DTMCs) presented in [J. Fan et al., J. Mach. Learn. Res., 22(2022), pp. 6185-6219] to the realm of CTMCs. The key analysis enabling the attainment of this inequality lies in the utilization of the techniques of skeleton chains and augmented truncation approximations. Furthermore, we also discuss Hoeffding's inequality for a jump process on a general state space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14888v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinpeng Liu, Yuanyuan Liu, Lin Zhou</dc:creator>
    </item>
    <item>
      <title>Fast and reliable confidence intervals for a variance component or proportion</title>
      <link>https://arxiv.org/abs/2404.15060</link>
      <description>arXiv:2404.15060v1 Announce Type: cross 
Abstract: We show that confidence intervals for a variance component or proportion, with asymptotically correct uniform coverage probability, can be obtained by inverting certain test-statistics based on the score for the restricted likelihood. The results apply in settings where the variance or proportion is near or at the boundary of the parameter set. Simulations indicate the proposed test-statistics are approximately pivotal and lead to confidence intervals with near-nominal coverage even in small samples. We illustrate our methods' application in spatially-resolved transcriptomics where we compute approximately 15,000 confidence intervals, used for gene ranking, in less than 4 minutes. In the settings we consider, the proposed method is between two and 28,000 times faster than popular alternatives, depending on how many confidence intervals are computed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15060v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiqiao Zhang, Karl Oskar Ekvall, Aaron J. Molstad</dc:creator>
    </item>
    <item>
      <title>GIST: Gibbs self-tuning for locally adaptive Hamiltonian Monte Carlo</title>
      <link>https://arxiv.org/abs/2404.15253</link>
      <description>arXiv:2404.15253v1 Announce Type: cross 
Abstract: We present a novel and flexible framework for localized tuning of Hamiltonian Monte Carlo samplers by sampling the algorithm's tuning parameters conditionally based on the position and momentum at each step. For adaptively sampling path lengths, we show that randomized Hamiltonian Monte Carlo, the No-U-Turn Sampler, and the Apogee-to-Apogee Path Sampler all fit within this unified framework as special cases. The framework is illustrated with a simple alternative to the No-U-Turn Sampler for locally adapting path lengths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15253v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Bob Carpenter, Milo Marsden</dc:creator>
    </item>
    <item>
      <title>Score matching for sub-Riemannian bridge sampling</title>
      <link>https://arxiv.org/abs/2404.15258</link>
      <description>arXiv:2404.15258v1 Announce Type: cross 
Abstract: Simulation of conditioned diffusion processes is an essential tool in inference for stochastic processes, data imputation, generative modelling, and geometric statistics. Whilst simulating diffusion bridge processes is already difficult on Euclidean spaces, when considering diffusion processes on Riemannian manifolds the geometry brings in further complications. In even higher generality, advancing from Riemannian to sub-Riemannian geometries introduces hypoellipticity, and the possibility of finding appropriate explicit approximations for the score of the diffusion process is removed. We handle these challenges and construct a method for bridge simulation on sub-Riemannian manifolds by demonstrating how recent progress in machine learning can be modified to allow for training of score approximators on sub-Riemannian manifolds. Since gradients dependent on the horizontal distribution, we generalise the usual notion of denoising loss to work with non-holonomic frames using a stochastic Taylor expansion, and we demonstrate the resulting scheme both explicitly on the Heisenberg group and more generally using adapted coordinates. We perform numerical experiments exemplifying samples from the bridge process on the Heisenberg group and the concentration of this process for small time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15258v1</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.DG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erlend Grong, Karen Habermann, Stefan Sommer</dc:creator>
    </item>
    <item>
      <title>A uniform kernel trick for high-dimensional two-sample problems</title>
      <link>https://arxiv.org/abs/2210.02171</link>
      <description>arXiv:2210.02171v3 Announce Type: replace 
Abstract: We use a suitable version of the so-called "kernel trick" to devise two-sample (homogeneity) tests, especially focussed on high-dimensional and functional data. Our proposal entails a simplification related to the important practical problem of selecting an appropriate kernel function. Specifically, we apply a uniform variant of the kernel trick which involves the supremum within a class of kernel-based distances. We obtain the asymptotic distribution (under the null and alternative hypotheses) of the test statistic. The proofs rely on empirical processes theory, combined with the delta method and Hadamard (directional) differentiability techniques, and functional Karhunen-Lo\`eve-type expansions of the underlying processes. This methodology has some advantages over other standard approaches in the literature. We also give some experimental insight into the performance of our proposal compared to the original kernel-based approach \cite{Gretton2007} and the test based on energy distances \cite{Szekely-Rizzo-2017}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.02171v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jmva.2024.105317</arxiv:DOI>
      <arxiv:journal_reference>Journal of Multivariate Analysis, 2024</arxiv:journal_reference>
      <dc:creator>Javier C\'arcamo, Antonio Cuevas, Luis-Alberto Rodr\'iguez</dc:creator>
    </item>
    <item>
      <title>Parameter estimation of stochastic SIR model driven by small L\'{e}vy noise with time-dependent periodic transmission</title>
      <link>https://arxiv.org/abs/2303.07983</link>
      <description>arXiv:2303.07983v2 Announce Type: replace 
Abstract: We investigate the parameter estimation and prediction of two forms of the stochastic SIR model driven by small L\'{e}vy noise with time-dependent periodic transmission. We present consistency and rate of convergence results for the least-squares estimators. We include simulation studies using the method of projected gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07983v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Terry Easlick, Wei Sun</dc:creator>
    </item>
    <item>
      <title>Wavelet-Based Density Estimation for Persistent Homology</title>
      <link>https://arxiv.org/abs/2305.08999</link>
      <description>arXiv:2305.08999v3 Announce Type: replace 
Abstract: Persistent homology is a central methodology in topological data analysis that has been successfully implemented in many fields and is becoming increasingly popular and relevant. The output of persistent homology is a persistence diagram -- a multiset of points supported on the upper half plane -- that is often used as a statistical summary of the topological features of data. In this paper, we study the random nature of persistent homology and estimate the density of expected persistence diagrams from observations using wavelets; we show that our wavelet-based estimator is optimal. Furthermore, we propose an estimator that offers a sparse representation of the expected persistence diagram that achieves near-optimality. We demonstrate the utility of our contributions in a machine learning task in the context of dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08999v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantin H\"aberle, Barbara Bravi, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>The ESPRIT algorithm under high noise: Optimal error scaling and noisy super-resolution</title>
      <link>https://arxiv.org/abs/2404.03885</link>
      <description>arXiv:2404.03885v2 Announce Type: replace-cross 
Abstract: Subspace-based signal processing techniques, such as the Estimation of Signal Parameters via Rotational Invariant Techniques (ESPRIT) algorithm, are popular methods for spectral estimation. These algorithms can achieve the so-called super-resolution scaling under low noise conditions, surpassing the well-known Nyquist limit. However, the performance of these algorithms under high-noise conditions is not as well understood. Existing state-of-the-art analysis indicates that ESPRIT and related algorithms can be resilient even for signals where each observation is corrupted by statistically independent, mean-zero noise of size $\mathcal{O}(1)$, but these analyses only show that the error $\epsilon$ decays at a slow rate $\epsilon=\mathcal{\tilde{O}}(n^{-1/2})$ with respect to the cutoff frequency $n$. In this work, we prove that under certain assumptions of bias and high noise, the ESPRIT algorithm can attain a significantly improved error scaling $\epsilon = \mathcal{\tilde{O}}(n^{-3/2})$, exhibiting noisy super-resolution scaling beyond the Nyquist limit. We further establish a theoretical lower bound and show that this scaling is optimal. Our analysis introduces novel matrix perturbation results, which could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03885v2</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyan Ding, Ethan N. Epperly, Lin Lin, Ruizhe Zhang</dc:creator>
    </item>
  </channel>
</rss>
