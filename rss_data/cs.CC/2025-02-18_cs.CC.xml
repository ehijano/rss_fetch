<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Feb 2025 03:01:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>MaxMin Separation Problems: FPT Algorithms for $st$-Separator and Odd Cycle Transversal</title>
      <link>https://arxiv.org/abs/2502.10449</link>
      <description>arXiv:2502.10449v1 Announce Type: new 
Abstract: In this paper, we study the parameterized complexity of the MaxMin versions of two fundamental separation problems: Maximum Minimal $st$-Separator and Maximum Minimal Odd Cycle Transversal (OCT), both parameterized by the solution size. In the Maximum Minimal $st$-Separator problem, given a graph $G$, two distinct vertices $s$ and $t$ and a positive integer $k$, the goal is to determine whether there exists a minimal $st$-separator in $G$ of size at least $k$. Similarly, the Maximum Minimal OCT problem seeks to determine if there exists a minimal set of vertices whose deletion results in a bipartite graph, and whose size is at least $k$. We demonstrate that both problems are fixed-parameter tractable parameterized by $k$. Our FPT algorithm for Maximum Minimal $st$-Separator answers the open question by Hanaka, Bodlaender, van der Zanden and Ono (TCS 2019).
  One unique insight from this work is the following. We use the meta-result of Lokshtanov, Ramanujan, Saurabh and Zehavi (ICALP 2018) that enables us to reduce our problems to highly unbreakable graphs. This is interesting, as an explicit use of the recursive understanding and randomized contractions framework of Chitnis, Cygan, Hajiaghayi, Pilipczuk and Pilipczuk (SICOMP 2016) to reduce to the highly unbreakable graphs setting (which is the result that Lokshtanov et al. tries to abstract out in their meta-theorem) does not seem obvious because certain ``extension'' variants of our problems are W[1]-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10449v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajinkya Gaikwad, Hitendra Kumar, Soumen Maity, Saket Saurabh, Roohani Sharma</dc:creator>
    </item>
    <item>
      <title>Algorithms and Hardness for Estimating Statistical Similarity</title>
      <link>https://arxiv.org/abs/2502.10527</link>
      <description>arXiv:2502.10527v1 Announce Type: cross 
Abstract: We study the problem of computing statistical similarity between probability distributions. For distributions $P$ and $Q$ over a finite sample space, their statistical similarity is defined as $S_{\mathrm{stat}}(P, Q) := \sum_{x} \min(P(x), Q(x))$. Statistical similarity is a basic measure of similarity between distributions, with several natural interpretations, and captures the Bayes error in prediction and hypothesis testing problems. Recent work has established that, somewhat surprisingly, even for the simple class of product distributions, exactly computing statistical similarity is $\#\mathsf{P}$-hard. This motivates the question of designing approximation algorithms for statistical similarity. Our primary contribution is a Fully Polynomial-Time deterministic Approximation Scheme (FPTAS) for estimating statistical similarity between two product distributions. To obtain this result, we introduce a new variant of the Knapsack problem, which we call the Masked Knapsack problem, and design an FPTAS to estimate the number of solutions of a multidimensional version of this problem. This new technical contribution could be of independent interest. Furthermore, we also establish a complementary hardness result. We show that it is $\mathsf{NP}$-hard to estimate statistical similarity when $P$ and $Q$ are Bayes net distributions of in-degree $2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10527v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S. Meel, Dimitrios Myrisiotis, A. Pavan, N. V. Vinodchandran</dc:creator>
    </item>
    <item>
      <title>Explaining Necessary Truths</title>
      <link>https://arxiv.org/abs/2502.11251</link>
      <description>arXiv:2502.11251v1 Announce Type: cross 
Abstract: Knowing the truth is rarely enough -- we also seek out reasons why the fact is true. While much is known about how we explain contingent truths, we understand less about how we explain facts, such as those in mathematics, that are true as a matter of logical necessity. We present a framework, based in computational complexity, where explanations for deductive truths co-emerge with discoveries of simplifying steps during the search process. When such structures are missing, we revert, in turn, to error-based reasons, where a (corrected) mistake can serve as fictitious, but explanatory, contingency-cause: not making the mistake serves as a reason why the truth takes the form it does. We simulate human subjects, using GPT-4o, presented with SAT puzzles of varying complexity and reasonableness, validating our theory and showing how its predictions can be tested in future human studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11251v1</guid>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>math.HO</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G\"ulce Karde\c{s}, Simon DeDeo</dc:creator>
    </item>
    <item>
      <title>On a tree-based variant of bandwidth and forbidding simple topological minors</title>
      <link>https://arxiv.org/abs/2502.11674</link>
      <description>arXiv:2502.11674v1 Announce Type: cross 
Abstract: We obtain structure theorems for graphs excluding a fan (a path with a universal vertex) or a dipole ($K_{2,k}$) as a topological minor. The corresponding decompositions can be computed in FPT linear time. This is motivated by the study of a graph parameter we call treebandwidth which extends the graph parameter bandwidth by replacing the linear layout by a rooted tree such that neighbours in the graph are in ancestor-descendant relation in the tree.
  We deduce an approximation algorithm for treebandwidth running in FPT linear time from our structure theorems. We complement this result with a precise characterisation of the parameterised complexity of computing the parameter exactly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11674v1</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Jacob, William Lochet, Christophe Paul</dc:creator>
    </item>
    <item>
      <title>Two-player Domino games</title>
      <link>https://arxiv.org/abs/2310.19759</link>
      <description>arXiv:2310.19759v2 Announce Type: replace 
Abstract: We introduce a 2-player game played on an infinite grid, initially empty, where each player in turn chooses a vertex and colours it. The first player aims to create some pattern from a target set, while the second player aims to prevent it. We study the problem of deciding which player wins, and prove that it is undecidable. We also consider a variant where the turn order is not alternating but given by a balanced word, and we characterise the decidable and undecidable cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19759v2</guid>
      <category>cs.CC</category>
      <category>cs.GT</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Hellouin de Menibus, R\'emi Pallen</dc:creator>
    </item>
    <item>
      <title>Hypercontractivity on HDX II: Symmetrization and q-Norms</title>
      <link>https://arxiv.org/abs/2408.16687</link>
      <description>arXiv:2408.16687v2 Announce Type: replace 
Abstract: Bourgain's symmetrization theorem is a powerful technique reducing boolean analysis on product spaces to the cube. It states that for any product $\Omega_i^{\otimes d}$, function $f: \Omega_i^{\otimes d} \to \mathbb{R}$, and $q &gt; 1$:
  $$||T_{\frac{1}{2}}f(x)||_q \leq ||\tilde{f}(r,x)||_{q} \leq ||T_{c_q}f(x)||_q$$ where $T_{\rho}f = \sum\limits \rho^Sf^{=S}$ is the noise operator and $\widetilde{f}(r,x) = \sum\limits r_Sf^{=S}(x)$ `symmetrizes' $f$ by convolving its Fourier components $\{f^{=S}\}_{S \subseteq [d]}$ with a random boolean string $r \in \{\pm 1\}^d$.
  In this work, we extend the symmetrization theorem to high dimensional expanders (HDX). Building on (O'Donnell and Zhao 2021), we show this implies nearly-sharp $(2{\to}q)$-hypercontractivity for partite HDX. This resolves the main open question of (Gur, Lifshitz, and Liu STOC 2022) and gives the first fully hypercontractive subsets $X \subset [n]^d$ of support $n\cdot\exp(\text{poly}(d))$, an exponential improvement over Bafna, Hopkins, Kaufman, and Lovett's $n\cdot\exp(\exp(d))$ bound (BHKL STOC 2022). Adapting (Bourgain JAMS 1999), we also give the first booster theorem for HDX, resolving a main open question of BHKL.
  Our proof is based on two elementary new ideas in the theory of high dimensional expansion. First we introduce `$q$-norm HDX', generalizing standard spectral notions to higher moments, and observe every spectral HDX is a $q$-norm HDX. Second, we introduce a simple method of coordinate-wise analysis on HDX which breaks high dimensional random walks into coordinate-wise components and allows each component to be analyzed as a $\textit{$1$-dimensional}$ operator locally within $X$. This allows for application of standard tricks such as the replacement method, greatly simplifying prior analytic techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16687v2</guid>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Hopkins</dc:creator>
    </item>
    <item>
      <title>The Computational Advantage of MIP* Vanishes in the Presence of Noise</title>
      <link>https://arxiv.org/abs/2312.04360</link>
      <description>arXiv:2312.04360v3 Announce Type: replace-cross 
Abstract: Quantum multiprover interactive proof systems with entanglement MIP* are much more powerful than its classical counterpart MIP (Babai et al. '91, Ji et al. '20): while MIP = NEXP, the quantum class MIP* is equal to RE, a class including the halting problem. This is because the provers in MIP* can share unbounded quantum entanglement. However, recent works of Qin and Yao '21 and '23 have shown that this advantage is significantly reduced if the provers' shared state contains noise. This paper attempts to exactly characterize the effect of noise on the computational power of quantum multiprover interactive proof systems. We investigate the quantum two-prover one-round interactive system MIP*[poly, O(1)], where the verifier sends polynomially many bits to the provers and the provers send back constantly many bits. We show noise completely destroys the computational advantage given by shared entanglement in this model. Specifically, we show that if the provers are allowed to share arbitrarily many noisy EPR states, where each EPR state is affected by an arbitrarily small constant amount of noise, the resulting complexity class is equivalent to NEXP = MIP. This improves significantly on the previous best-known bound of NEEEXP (nondeterministic triply exponential time) by Qin and Yao '21. We also show that this collapse in power is due to the noise, rather than the O(1) answer size, by showing that allowing for noiseless EPR states gives the class the full power of RE = MIP*[poly, poly]. Along the way, we develop two technical tools of independent interest. First, we give a new, deterministic tester for the positivity of an exponentially large matrix, provided it has a low-degree Fourier decomposition in terms of Pauli matrices. Secondly, we develop a new invariance principle for smooth matrix functions having bounded third-order Fr\'echet derivatives or which are Lipschitz continous.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04360v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yangjing Dong, Honghao Fu, Anand Natarajan, Minglong Qin, Haochen Xu, Penghui Yao</dc:creator>
    </item>
  </channel>
</rss>
