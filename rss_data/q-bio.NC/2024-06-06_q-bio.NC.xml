<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 01:48:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Neural Representations of Dynamic Visual Stimuli</title>
      <link>https://arxiv.org/abs/2406.02659</link>
      <description>arXiv:2406.02659v1 Announce Type: new 
Abstract: Humans experience the world through constantly changing visual stimuli, where scenes can shift and move, change in appearance, and vary in distance. The dynamic nature of visual perception is a fundamental aspect of our daily lives, yet the large majority of research on object and scene processing, particularly using fMRI, has focused on static stimuli. While studies of static image perception are attractive due to their computational simplicity, they impose a strong non-naturalistic constraint on our investigation of human vision. In contrast, dynamic visual stimuli offer a more ecologically-valid approach but present new challenges due to the interplay between spatial and temporal information, making it difficult to disentangle the representations of stable image features and motion. To overcome this limitation -- given dynamic inputs, we explicitly decouple the modeling of static image representations and motion representations in the human brain. Three results demonstrate the feasibility of this approach. First, we show that visual motion information as optical flow can be predicted (or decoded) from brain activity as measured by fMRI. Second, we show that this predicted motion can be used to realistically animate static images using a motion-conditioned video diffusion model (where the motion is driven by fMRI brain activity). Third, we show prediction in the reverse direction: existing video encoders can be fine-tuned to predict fMRI brain activity from video imagery, and can do so more effectively than image encoders. This foundational work offers a novel, extensible framework for interpreting how the human brain processes dynamic visual information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02659v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Yeung, Andrew F. Luo, Gabriel Sarch, Margaret M. Henderson, Deva Ramanan, Michael J. Tarr</dc:creator>
    </item>
    <item>
      <title>Vagus nerve stimulation: Laying the groundwork for predictive network-based computer models</title>
      <link>https://arxiv.org/abs/2406.02729</link>
      <description>arXiv:2406.02729v1 Announce Type: new 
Abstract: Vagus Nerve Stimulation (VNS) is an established palliative treatment for drug resistant epilepsy. While effective for many patients, its mechanism of action is incompletely understood. Predicting individuals' response, or optimum stimulation parameters, is challenging. Computational modelling has informed other problems in epilepsy but, to our knowledge, has not been applied to VNS.
  We started with an established, four-population neural mass model (NMM), capable of reproducing the seizure-like dynamics of a thalamocortical circuit. We extended this to include 18 further neural populations, representing nine other brain regions relevant to VNS, with connectivity based on existing literature. We modelled stimulated afferent vagal fibres as projecting to the nucleus tractus solitarius (NTS), which receives input from the vagus nerve in vivo.
  Bifurcation analysis of a deterministic version of the model showed higher background NTS input made the model monostable at a fixed point (FP), representing normal activity, while lower inputs produce bistability between the FP and a limit cycle (LC), representing the seizure state.
  Adding noise produced transitions between seizure and normal states. This stochastic model spent decreasing time in the seizure state with increasing background NTS input, until seizures were abolished, consistent with the deterministic model.
  Simulated VNS stimulation, modelled as a 30 Hz square wave, was summed with the background input to the NTS and was found to reduce total seizure duration in a dose-dependent manner, similar to expectations in vivo.
  We have successfully produced an in silico model of VNS in epilepsy, capturing behaviour seen in vivo. This may aid understanding therapeutic mechanisms of VNS in epilepsy and provides a starting point to (i) determine which patients might respond best to VNS, and (ii) optimise individuals' treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02729v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John F. Ingham, Frances Hutchings, Paolo Zuliani, Yujiang Wang, Sadegh Soudjani, Peter N. Taylor</dc:creator>
    </item>
    <item>
      <title>GET: A Generative EEG Transformer for Continuous Context-Based Neural Signals</title>
      <link>https://arxiv.org/abs/2406.03115</link>
      <description>arXiv:2406.03115v2 Announce Type: new 
Abstract: Generating continuous electroencephalography (EEG) signals through advanced artificial neural networks presents a novel opportunity to enhance brain-computer interface (BCI) technology. This capability has the potential to significantly enhance applications ranging from simulating dynamic brain activity and data augmentation to improving real-time epilepsy detection and BCI inference. By harnessing generative transformer neural networks, specifically designed for EEG signal generation, we can revolutionize the interpretation and interaction with neural data. Generative AI has demonstrated significant success across various domains, from natural language processing (NLP) and computer vision to content creation in visual arts and music. It distinguishes itself by using large-scale datasets to construct context windows during pre-training, a technique that has proven particularly effective in NLP, where models are fine-tuned for specific downstream tasks after extensive foundational training. However, the application of generative AI in the field of BCIs, particularly through the development of continuous, context-rich neural signal generators, has been limited. To address this, we introduce the Generative EEG Transformer (GET), a model leveraging transformer architecture tailored for EEG data. The GET model is pre-trained on diverse EEG datasets, including motor imagery and alpha wave datasets, enabling it to produce high-fidelity neural signals that maintain contextual integrity. Our empirical findings indicate that GET not only faithfully reproduces the frequency spectrum of the training data and input prompts but also robustly generates continuous neural signals. By adopting the successful training strategies of the NLP domain for BCIs, the GET sets a new standard for the development and application of neural signal generation technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03115v2</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omair Ali, Muhammad Saif-ur-Rehman, Marita Metzler, Tobias Glasmachers, Ioannis Iossifidis, Christian Klaes</dc:creator>
    </item>
    <item>
      <title>Population Transformer: Learning Population-level Representations of Intracranial Activity</title>
      <link>https://arxiv.org/abs/2406.03044</link>
      <description>arXiv:2406.03044v1 Announce Type: cross 
Abstract: We present a self-supervised framework that learns population-level codes for intracranial neural recordings at scale, unlocking the benefits of representation learning for a key neuroscience recording modality. The Population Transformer (PopT) lowers the amount of data required for decoding experiments, while increasing accuracy, even on never-before-seen subjects and tasks. We address two key challenges in developing PopT: sparse electrode distribution and varying electrode location across patients. PopT stacks on top of pretrained representations and enhances downstream tasks by enabling learned aggregation of multiple spatially-sparse data channels. Beyond decoding, we interpret the pretrained PopT and fine-tuned models to show how it can be used to provide neuroscience insights learned from massive amounts of data. We release a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability, and code is available at https://github.com/czlwang/PopulationTransformer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03044v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geeling Chau, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, Yisong Yue, Boris Katz, Andrei Barbu</dc:creator>
    </item>
    <item>
      <title>Spiking representation learning for associative memories</title>
      <link>https://arxiv.org/abs/2406.03054</link>
      <description>arXiv:2406.03054v1 Announce Type: cross 
Abstract: Networks of interconnected neurons communicating through spiking signals offer the bedrock of neural computations. Our brains spiking neural networks have the computational capacity to achieve complex pattern recognition and cognitive functions effortlessly. However, solving real-world problems with artificial spiking neural networks (SNNs) has proved to be difficult for a variety of reasons. Crucially, scaling SNNs to large networks and processing large-scale real-world datasets have been challenging, especially when compared to their non-spiking deep learning counterparts. The critical operation that is needed of SNNs is the ability to learn distributed representations from data and use these representations for perceptual, cognitive and memory operations. In this work, we introduce a novel SNN that performs unsupervised representation learning and associative memory operations leveraging Hebbian synaptic and activity-dependent structural plasticity coupled with neuron-units modelled as Poisson spike generators with sparse firing (~1 Hz mean and ~100 Hz maximum firing rate). Crucially, the architecture of our model derives from the neocortical columnar organization and combines feedforward projections for learning hidden representations and recurrent projections for forming associative memories. We evaluated the model on properties relevant for attractor-based associative memories such as pattern completion, perceptual rivalry, distortion resistance, and prototype extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03054v1</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Naresh Ravichandran, Anders Lansner, Pawel Herman</dc:creator>
    </item>
    <item>
      <title>Maximal information at the edge of stability in excitatory-inhibitory neural populations</title>
      <link>https://arxiv.org/abs/2406.03380</link>
      <description>arXiv:2406.03380v1 Announce Type: cross 
Abstract: Understanding how the complex connectivity structure of the brain shapes its information-processing capabilities is a long-standing question. Here, by focusing on a paradigmatic architecture, we study how the neural activity of excitatory and inhibitory populations encodes information on external signals. We show that information is maximized at the edge of stability, where excitation is balanced by inhibition. When the input switches among different stimuli, this maximum corresponds to the entropy of the external switching dynamics. By analyzing the case of a prolonged stimulus, we find that stronger inhibition is needed to maximize the instantaneous sensitivity, revealing an intrinsic trade-off between short-time responses and long-time accuracy. In agreement with recent experimental findings, our results open the avenue for a complete information-theoretic understanding of how and why inhibition strength should be tuned to optimize information-processing capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03380v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giacomo Barzon, Daniel Maria Busiello, Giorgio Nicoletti</dc:creator>
    </item>
  </channel>
</rss>
