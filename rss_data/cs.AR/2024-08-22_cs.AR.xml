<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Aug 2024 04:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Floating-Point Multiply-Add with Approximate Normalization for Low-Cost Matrix Engines</title>
      <link>https://arxiv.org/abs/2408.11997</link>
      <description>arXiv:2408.11997v1 Announce Type: new 
Abstract: The widespread adoption of machine learning algorithms necessitates hardware acceleration to ensure efficient performance. This acceleration relies on custom matrix engines that operate on full or reduced-precision floating-point arithmetic. However, conventional floating-point implementations can be power hungry. This paper proposes a method to improve the energy efficiency of the matrix engines used in machine learning algorithm acceleration. Our approach leverages approximate normalization within the floating-point multiply-add units as a means to reduce their hardware complexity, without sacrificing overall machine-learning model accuracy. Hardware synthesis results show that this technique reduces area and power consumption roughly by 16% and 13% on average for Bfloat16 format. Also, the error introduced in transformer model accuracy is 1% on average, for the most efficient configuration of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11997v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosmas Alexandridis, Christodoulos Peltekis, Dionysios Filippas, Giorgos Dimitrakopoulos</dc:creator>
    </item>
    <item>
      <title>Virgo: Cluster-level Matrix Unit Integration in GPUs for Scalability and Energy Efficiency</title>
      <link>https://arxiv.org/abs/2408.12073</link>
      <description>arXiv:2408.12073v1 Announce Type: new 
Abstract: Modern GPUs incorporate specialized matrix units such as Tensor Cores to accelerate GEMM operations central to deep learning workloads. However, existing matrix unit designs are tightly coupled to the SIMT core, limiting the size and energy efficiency of the operation due to capacity and bandwidth constraints from the register file. Such a limitation in scalability makes it difficult to simultaneously enhance compute throughput and improve energy efficiency in GPUs.
  To address this challenge, we propose Virgo, a new GPU microarchitecture that integrates dedicated matrix units at the SIMT core cluster level. By physically disaggregating the matrix unit from the SIMT core, Virgo eliminates scalability constraints imposed by the core microarchitecture. Consequently, Virgo increases the granularity of operations at the hardware which not only improves data reuse, but also reduces the number of instructions processed in the SIMT core. This reduction in instruction processing decreases energy consumption within the core pipeline, thereby improving the system-level energy efficiency. Our evaluations, implemented in synthesizable RTL, demonstrate that Virgo achieves up to 66.3% reduction in active power and 77.2% reduction in active energy consumption of the system-on-chip compared to the baseline core-coupled design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12073v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hansung Kim, Ruohan Yan, Joshua You, Tieliang Vamber Yang, Yakun Sophia Shao</dc:creator>
    </item>
    <item>
      <title>Exposing Shadow Branches</title>
      <link>https://arxiv.org/abs/2408.12592</link>
      <description>arXiv:2408.12592v1 Announce Type: new 
Abstract: Modern processors implement a decoupled front-end in the form of Fetch Directed Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is driven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and branch target tracking structures to speculatively fetch instructions into the Instruction Cache (L1I). As data center applications become more complex, their code footprints also grow, resulting in an increase in Branch Target Buffer (BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB miss, the BPU may not identify the current instruction as a branch to FDIP. This can prevent FDIP from prefetching or cause it to speculate down the wrong path, further polluting the L1I cache. We observe that the vast majority, 75%, of BTB-missing, unidentified branches are actually present in instruction cache lines that FDIP has previously fetched but, these missing branches have not yet been decoded and inserted into the BTB. This is because the instruction line is decoded from an entry point (which is the target of the previous taken branch) till an exit point (the taken branch). Branch instructions present in the ignored portion of the cache line we call them "Shadow Branches". Here we present Skeia, a novel shadow branch decoding technique that identifies and decodes unused bytes in cache lines fetched by FDIP, inserting them into a Shadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB, allowing FDIP to speculate despite a BTB miss. With a minimal storage state of 12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB) and ~2% versus adding an equal amount of state to the BTB across 16 front-end bound applications. Since many branches stored in the SBB are unique compared to those in a similarly sized BTB, we consistently observe greater performance gains with Skeia across all examined sizes until saturation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12592v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chrysanthos Pepi, Bhargav Reddy Godala, Krishnam Tibrewala, Gino Chacon, Paul V. Gratz, Daniel A. Jim\'enez, Gilles A. Pokam, David I. August</dc:creator>
    </item>
    <item>
      <title>TSB: Tiny Shared Block for Efficient DNN Deployment on NVCIM Accelerators</title>
      <link>https://arxiv.org/abs/2406.06544</link>
      <description>arXiv:2406.06544v2 Announce Type: replace 
Abstract: Compute-in-memory (CIM) accelerators using non-volatile memory (NVM) devices offer promising solutions for energy-efficient and low-latency Deep Neural Network (DNN) inference execution. However, practical deployment is often hindered by the challenge of dealing with the massive amount of model weight parameters impacted by the inherent device variations within non-volatile computing-in-memory (NVCIM) accelerators. This issue significantly offsets their advantages by increasing training overhead, the time and energy needed for mapping weights to device states, and diminishing inference accuracy. To mitigate these challenges, we propose the "Tiny Shared Block (TSB)" method, which integrates a small shared 1x1 convolution block into the DNN architecture. This block is designed to stabilize feature processing across the network, effectively reducing the impact of device variation. Extensive experimental results show that TSB achieves over 20x inference accuracy gap improvement, over 5x training speedup, and weights-to-device mapping cost reduction while requiring less than 0.4% of the original weights to be write-verified during programming, when compared with state-of-the-art baseline solutions. Our approach provides a practical and efficient solution for deploying robust DNN models on NVCIM accelerators, making it a valuable contribution to the field of energy-efficient AI hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06544v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Qin, Zheyu Yan, Zixuan Pan, Wujie Wen, Xiaobo Sharon Hu, Yiyu Shi</dc:creator>
    </item>
    <item>
      <title>Design and Implementation of a Takum Arithmetic Hardware Codec in VHDL</title>
      <link>https://arxiv.org/abs/2408.10594</link>
      <description>arXiv:2408.10594v2 Announce Type: replace 
Abstract: The takum machine number format has been recently proposed as an enhancement over the posit number format, which is considered a promising alternative to the IEEE 754 floating-point standard. Takums retain the useful posit properties, but feature a novel exponent coding scheme that yields more precision for small and large magnitude numbers. Takum's dynamic range is larger and bounded, as reflected in its name, derived from the Icelandic 'takmarka{\dh} umfang', meaning 'limited range'. Consequently, the selection of bit string lengths becomes determined solely by precision requirements and independent of dynamic range considerations. Takum is defined in both a logarithmic number system (LNS) format and a traditional floating-point format.
  This paper presents the design and implementation of a hardware codec for both the logarithmic and floating-point takum formats. The design primarily focuses on the codec, as both formats share a common internal arithmetic representation. Non-essential aspects of current posit designs, such as fused or pipelined architectures and the choice of floating-point IP cores, are thus omitted. The proposed takum codec, implemented in VHDL, demonstrates near-optimal scalability and performance on an FPGA, matching or exceeding state-of-the-art posit codecs in terms of both latency and LUT utilisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10594v2</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laslo Hunhold</dc:creator>
    </item>
  </channel>
</rss>
