<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Sep 2024 04:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Clustered Factor Analysis for Multivariate Spatial Data</title>
      <link>https://arxiv.org/abs/2409.07018</link>
      <description>arXiv:2409.07018v1 Announce Type: new 
Abstract: Factor analysis has been extensively used to reveal the dependence structures among multivariate variables, offering valuable insight in various fields. However, it cannot incorporate the spatial heterogeneity that is typically present in spatial data. To address this issue, we introduce an effective method specifically designed to discover the potential dependence structures in multivariate spatial data. Our approach assumes that spatial locations can be approximately divided into a finite number of clusters, with locations within the same cluster sharing similar dependence structures. By leveraging an iterative algorithm that combines spatial clustering with factor analysis, we simultaneously detect spatial clusters and estimate a unique factor model for each cluster. The proposed method is evaluated through comprehensive simulation studies, demonstrating its flexibility. In addition, we apply the proposed method to a dataset of railway station attributes in the Tokyo metropolitan area, highlighting its practical applicability and effectiveness in uncovering complex spatial dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07018v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanxiu Jin, Tomoya Wakayama, Renhe Jiang, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Integrating Multiple Data Sources with Interactions in Multi-Omics Using Cooperative Learning</title>
      <link>https://arxiv.org/abs/2409.07125</link>
      <description>arXiv:2409.07125v1 Announce Type: new 
Abstract: Modeling with multi-omics data presents multiple challenges such as the high-dimensionality of the problem ($p \gg n$), the presence of interactions between features, and the need for integration between multiple data sources. We establish an interaction model that allows for the inclusion of multiple sources of data from the integration of two existing methods, pliable lasso and cooperative learning. The integrated model is tested both on simulation studies and on real multi-omics datasets for predicting labor onset and cancer treatment response. The results show that the model is effective in modeling multi-source data in various scenarios where interactions are present, both in terms of prediction performance and selection of relevant variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07125v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo D'Alessandro, Theophilus Quachie Asenso, Manuela Zucknick</dc:creator>
    </item>
    <item>
      <title>Non-parametric estimation of transition intensities in interval censored Markov multi-state models without loops</title>
      <link>https://arxiv.org/abs/2409.07176</link>
      <description>arXiv:2409.07176v1 Announce Type: new 
Abstract: Panel data arises when transitions between different states are interval-censored in multi-state data. The analysis of such data using non-parametric multi-state models was not possible until recently, but is very desirable as it allows for more flexibility than its parametric counterparts. The single available result to date has some unique drawbacks. We propose a non-parametric estimator of the transition intensities for panel data using an Expectation Maximisation algorithm. The method allows for a mix of interval-censored and right-censored (exactly observed) transitions. A condition to check for the convergence of the algorithm to the non-parametric maximum likelihood estimator is given. A simulation study comparing the proposed estimator to a consistent estimator is performed, and shown to yield near identical estimates at smaller computational cost. A data set on the emergence of teeth in children is analysed. Code to perform the analyses is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07176v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Gomon, Hein Putter</dc:creator>
    </item>
    <item>
      <title>Extended-support beta regression for $[0, 1]$ responses</title>
      <link>https://arxiv.org/abs/2409.07233</link>
      <description>arXiv:2409.07233v1 Announce Type: new 
Abstract: We introduce the XBX regression model, a continuous mixture of extended-support beta regressions for modeling bounded responses with or without boundary observations. The core building block of the new model is the extended-support beta distribution, which is a censored version of a four-parameter beta distribution with the same exceedance on the left and right of $(0, 1)$. Hence, XBX regression is a direct extension of beta regression. We prove that both beta regression with dispersion effects and heteroscedastic normal regression with censoring at both $0$ and $1$ -- known as the heteroscedastic two-limit tobit model in the econometrics literature -- are special cases of the extended-support beta regression model, depending on whether a single extra parameter is zero or infinity, respectively. To overcome identifiability issues that may arise in estimating the extra parameter due to the similarity of the beta and normal distribution for certain parameter settings, we assume that the additional parameter has an exponential distribution with an unknown mean. The associated marginal likelihood can be conveniently and accurately approximated using a Gauss-Laguerre quadrature rule, resulting in efficient estimation and inference procedures. The new model is used to analyze investment decisions in a behavioral economics experiment, where the occurrence and extent of loss aversion is of interest. In contrast to standard approaches, XBX regression can simultaneously capture the probability of rational behavior as well as the mean amount of loss aversion. Moreover, the effectiveness of the new model is illustrated through extensive numerical comparisons with alternative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07233v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Kosmidis, Achim Zeileis</dc:creator>
    </item>
    <item>
      <title>Order selection in GARMA models for count time series: a Bayesian perspective</title>
      <link>https://arxiv.org/abs/2409.07263</link>
      <description>arXiv:2409.07263v1 Announce Type: new 
Abstract: Estimation in GARMA models has traditionally been carried out under the frequentist approach. To date, Bayesian approaches for such estimation have been relatively limited. In the context of GARMA models for count time series, Bayesian estimation achieves satisfactory results in terms of point estimation. Model selection in this context often relies on the use of information criteria. Despite its prominence in the literature, the use of information criteria for model selection in GARMA models for count time series have been shown to present poor performance in simulations, especially in terms of their ability to correctly identify models, even under large sample sizes. In this study, we study the problem of order selection in GARMA models for count time series, adopting a Bayesian perspective through the application of the Reversible Jump Markov Chain Monte Carlo approach. Monte Carlo simulation studies are conducted to assess the finite sample performance of the developed ideas, including point and interval inference, sensitivity analysis, effects of burn-in and thinning, as well as the choice of related priors and hyperparameters. Two real-data applications are presented, one considering automobile production in Brazil and the other considering bus exportation in Brazil before and after the COVID-19 pandemic, showcasing the method's capabilities and further exploring its flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07263v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Katerine Zuniga Lastra, Guilherme Pumi, Taiane Schaedler Prass</dc:creator>
    </item>
    <item>
      <title>Local Effects of Continuous Instruments without Positivity</title>
      <link>https://arxiv.org/abs/2409.07350</link>
      <description>arXiv:2409.07350v1 Announce Type: new 
Abstract: Instrumental variables have become a popular study design for the estimation of treatment effects in the presence of unobserved confounders. In the canonical instrumental variables design, the instrument is a binary variable, and most extant methods are tailored to this context. In many settings, however, the instrument is a continuous measure. Standard estimation methods can be applied with continuous instruments, but they require strong assumptions regarding functional form. Moreover, while some recent work has introduced more flexible approaches for continuous instruments, these methods require an assumption known as positivity that is unlikely to hold in many applications. We derive a novel family of causal estimands using a stochastic dynamic intervention framework that considers a range of intervention distributions that are absolutely continuous with respect to the observed distribution of the instrument. These estimands focus on a specific form of local effect but do not require a positivity assumption. Next, we develop doubly robust estimators for these estimands that allow for estimation of the nuisance functions via nonparametric estimators. We use empirical process theory and sample splitting to derive asymptotic properties of the proposed estimators under weak conditions. In addition, we derive methods for profiling the principal strata as well as a method for sensitivity analysis for assessing robustness to an underlying monotonicity assumption. We evaluate our methods via simulation and demonstrate their feasibility using an application on the effectiveness of surgery for specific emergency conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07350v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Prabrisha Rakshit, Alexander Levis, Luke Keele</dc:creator>
    </item>
    <item>
      <title>Multi-source Stable Variable Importance Measure via Adversarial Machine Learning</title>
      <link>https://arxiv.org/abs/2409.07380</link>
      <description>arXiv:2409.07380v1 Announce Type: new 
Abstract: As part of enhancing the interpretability of machine learning, it is of renewed interest to quantify and infer the predictive importance of certain exposure covariates. Modern scientific studies often collect data from multiple sources with distributional heterogeneity. Thus, measuring and inferring stable associations across multiple environments is crucial in reliable and generalizable decision-making. In this paper, we propose MIMAL, a novel statistical framework for Multi-source stable Importance Measure via Adversarial Learning. MIMAL measures the importance of some exposure variables by maximizing the worst-case predictive reward over the source mixture. Our framework allows various machine learning methods for confounding adjustment and exposure effect characterization. For inferential analysis, the asymptotic normality of our introduced statistic is established under a general machine learning framework that requires no stronger learning accuracy conditions than those for single source variable importance. Numerical studies with various types of data generation setups and machine learning implementation are conducted to justify the finite-sample performance of MIMAL. We also illustrate our method through a real-world study of Beijing air pollution in multiple locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07380v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zitao Wang, Nian Si, Zijian Guo, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Improve Sensitivity Analysis Synthesizing Randomized Clinical Trials With Limited Overlap</title>
      <link>https://arxiv.org/abs/2409.07391</link>
      <description>arXiv:2409.07391v1 Announce Type: new 
Abstract: To estimate the average treatment effect in real-world populations, observational studies are typically designed around real-world cohorts. However, even when study samples from these designs represent the population, unmeasured confounders can introduce bias. Sensitivity analysis is often used to estimate bounds for the average treatment effect without relying on the strict mathematical assumptions of other existing methods. This article introduces a new approach that improves sensitivity analysis in observational studies by incorporating randomized clinical trial data, even with limited overlap due to inclusion/exclusion criteria. Theoretical proof and simulations show that this method provides a tighter bound width than existing approaches. We also apply this method to both a trial dataset and a real-world drug effectiveness comparison dataset for practical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07391v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuan Jiang, Wenjie Hu, Shu Yang, Xinxing Lai, Xiaohua Zhou</dc:creator>
    </item>
    <item>
      <title>Local Sequential MCMC for Data Assimilation with Applications in Geoscience</title>
      <link>https://arxiv.org/abs/2409.07111</link>
      <description>arXiv:2409.07111v1 Announce Type: cross 
Abstract: This paper presents a new data assimilation (DA) scheme based on a sequential Markov Chain Monte Carlo (SMCMC) DA technique [Ruzayqat et al. 2024] which is provably convergent and has been recently used for filtering, particularly for high-dimensional non-linear, and potentially, non-Gaussian state-space models. Unlike particle filters, which can be considered exact methods and can be used for filtering non-linear, non-Gaussian models, SMCMC does not assign weights to the samples/particles, and therefore, the method does not suffer from the issue of weight-degeneracy when a relatively small number of samples is used. We design a localization approach within the SMCMC framework that focuses on regions where observations are located and restricts the transition densities included in the filtering distribution of the state to these regions. This results in immensely reducing the effective degrees of freedom and thus improving the efficiency. We test the new technique on high-dimensional ($d \sim 10^4 - 10^5$) linear Gaussian model and non-linear shallow water models with Gaussian noise with real and synthetic observations. For two of the numerical examples, the observations mimic the data generated by the Surface Water and Ocean Topography (SWOT) mission led by NASA, which is a swath of ocean height observations that changes location at every assimilation time step. We also use a set of ocean drifters' real observations in which the drifters are moving according the ocean kinematics and assumed to have uncertain locations at the time of assimilation. We show that when higher accuracy is required, the proposed algorithm is superior in terms of efficiency and accuracy over competing ensemble methods and the original SMCMC filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07111v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamza Ruzayqat, Omar Knio</dc:creator>
    </item>
    <item>
      <title>Dynamic Bayesian Networks, Elicitation and Data Embedding for Secure Environments</title>
      <link>https://arxiv.org/abs/2409.07389</link>
      <description>arXiv:2409.07389v1 Announce Type: cross 
Abstract: Serious crime modelling typically needs to be undertaken securely behind a firewall where police knowledge and capabilities can remain undisclosed. Data informing an ongoing incident is often sparse, with a large proportion of relevant data only coming to light after the incident culminates or after police intervene - by which point it is too late to make use of the data to aid real-time decision making for the incident in question. Much of the data that is available to police to support real-time decision making is highly confidential so cannot be shared with academics, and is therefore missing to them. In this paper, we describe the development of a formal protocol where a graphical model is used as a framework for securely translating a model designed by an academic team to a model for use by a police team. We then show, for the first time, how libraries of these models can be built and used for real-time decision support to circumvent the challenges of data missingness and tardiness seen in such a secure environment. The parallel development described by this protocol ensures that any sensitive information collected by police, and missing to academics, remains secured behind a firewall. The protocol nevertheless guides police so that they are able to combine the typically incomplete data streams that are open source with their more sensitive information in a formal and justifiable way. We illustrate the application of this protocol by describing how a new entry - a suspected vehicle attack - can be embedded into such a police library of criminal plots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07389v1</guid>
      <category>stat.AP</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kieran Drury, Jim Q. Smith</dc:creator>
    </item>
    <item>
      <title>Circular and Spherical Projected Cauchy Distributions: A Novel Framework for Circular and Directional Data Modeling</title>
      <link>https://arxiv.org/abs/2302.02468</link>
      <description>arXiv:2302.02468v4 Announce Type: replace 
Abstract: We introduce a novel family of projected distributions on the circle and the sphere, namely the circular and spherical projected Cauchy distributions, as promising alternatives for modelling circular and spherical data. The circular distribution encompasses the wrapped Cauchy distribution as a special case, while featuring a more convenient parameterisation. We also propose a generalised wrapped Cauchy distribution that includes an extra parameter, enhancing the fit of the distribution. In the spherical context, we impose two conditions on the scatter matrix of the Cauchy distribution, resulting in an elliptically symmetric distribution. Our projected distributions exhibit attractive properties, such as a closed-form normalising constant and straightforward random value generation. The distribution parameters can be estimated using maximum likelihood, and we assess their bias through numerical studies. Further, we compare our proposed distributions to existing models with real datasets, demonstrating equal or superior fitting both with and without covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.02468v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michail Tsagris, Omar Alzeley</dc:creator>
    </item>
    <item>
      <title>Robustifying likelihoods by optimistically re-weighting data</title>
      <link>https://arxiv.org/abs/2303.10525</link>
      <description>arXiv:2303.10525v2 Announce Type: replace 
Abstract: Likelihood-based inferences have been remarkably successful in wide-spanning application areas. However, even after due diligence in selecting a good model for the data at hand, there is inevitably some amount of model misspecification: outliers, data contamination or inappropriate parametric assumptions such as Gaussianity mean that most models are at best rough approximations of reality. A significant practical concern is that for certain inferences, even small amounts of model misspecification may have a substantial impact; a problem we refer to as brittleness. This article attempts to address the brittleness problem in likelihood-based inferences by choosing the most model friendly data generating process in a distance-based neighborhood of the empirical measure. This leads to a new Optimistically Weighted Likelihood (OWL), which robustifies the original likelihood by formally accounting for a small amount of model misspecification. Focusing on total variation (TV) neighborhoods, we study theoretical properties, develop estimation algorithms and illustrate the methodology in applications to mixture models and regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.10525v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miheer Dewaskar, Christopher Tosh, Jeremias Knoblauch, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Model-assisted analysis of covariance estimators for stepped wedge cluster randomized experiments</title>
      <link>https://arxiv.org/abs/2306.11267</link>
      <description>arXiv:2306.11267v4 Announce Type: replace 
Abstract: Stepped wedge cluster randomized experiments (SW-CREs) represent a class of unidirectional crossover designs. Although SW-CREs have become popular, definitions of estimands and robust methods to target estimands under the potential outcomes framework remain insufficient. To address this gap, we describe a class of estimands that explicitly acknowledge the multilevel data structure in SW-CREs and highlight three typical members of the estimand class that are interpretable. We then introduce four analysis of covariance (ANCOVA) working models to achieve estimand-aligned analyses with covariate adjustment. Each ANCOVA estimator is model-assisted, as its point estimator is consistent even when the working model is misspecified. Under the stepped wedge randomization scheme, we establish the finite population Central Limit Theorem for each estimator. We study the finite-sample operating characteristics of the ANCOVA estimators in simulations and illustrate their application by analyzing the Washington State Expedited Partner Therapy study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11267v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Fan Li</dc:creator>
    </item>
    <item>
      <title>On the uses and abuses of regression models: a call for reform of statistical practice and teaching</title>
      <link>https://arxiv.org/abs/2309.06668</link>
      <description>arXiv:2309.06668v3 Announce Type: replace 
Abstract: Regression methods dominate the practice of biostatistical analysis, but biostatistical training emphasises the details of regression models and methods ahead of the purposes for which such modelling might be useful. More broadly, statistics is widely understood to provide a body of techniques for "modelling data", underpinned by what we describe as the "true model myth": that the task of the statistician/data analyst is to build a model that closely approximates the true data generating process. By way of our own historical examples and a brief review of mainstream clinical research journals, we describe how this perspective has led to a range of problems in the application of regression methods, including misguided "adjustment" for covariates, misinterpretation of regression coefficients and the widespread fitting of regression models without a clear purpose. We then outline a new approach to the teaching and application of biostatistical methods, which situates them within a framework that first requires clear definition of the substantive research question at hand within one of three categories: descriptive, predictive, or causal. Within this approach, the development and application of (multivariable) regression models, as well as other advanced biostatistical methods, should proceed differently according to the type of question. Regression methods will no doubt remain central to statistical practice as they provide a powerful tool for representing variation in a response or outcome variable as a function of "input" variables, but their conceptualisation and usage should follow from the purpose at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06668v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John B. Carlin, Margarita Moreno-Betancur</dc:creator>
    </item>
    <item>
      <title>Bayesian Quantile Regression with Subset Selection: A Posterior Summarization Perspective</title>
      <link>https://arxiv.org/abs/2311.02043</link>
      <description>arXiv:2311.02043v3 Announce Type: replace 
Abstract: Quantile regression is a powerful tool in epidemiological studies where interest lies in inferring how different exposures affect specific percentiles of the distribution of a health or life outcome. Existing methods either estimate conditional quantiles separately for each quantile of interest or estimate the entire conditional distribution using semi- or non-parametric models. The former often produce inadequate models for real data and do not share information across quantiles, while the latter are characterized by complex and constrained models that can be difficult to interpret and computationally inefficient. Further, neither approach is well-suited for quantile-specific subset selection. Instead, we pose the fundamental problems of linear quantile estimation, uncertainty quantification, and subset selection from a Bayesian decision analysis perspective. For any Bayesian regression model, we derive optimal and interpretable linear estimates and uncertainty quantification for each model-based conditional quantile. Our approach introduces a quantile-focused squared error loss, which enables efficient, closed-form computing and maintains a close relationship with Wasserstein-based density estimation. In an extensive simulation study, our methods demonstrate substantial gains in quantile estimation accuracy, variable selection, and inference over frequentist and Bayesian competitors. We use these tools to identify and quantify the heterogeneous impacts of multiple social stressors and environmental exposures on educational outcomes across the full spectrum of low-, medium-, and high-achieving students in North Carolina.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02043v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Feldman, Daniel Kowal</dc:creator>
    </item>
    <item>
      <title>Copula Approximate Bayesian Computation Using Distribution Random Forests</title>
      <link>https://arxiv.org/abs/2402.18450</link>
      <description>arXiv:2402.18450v4 Announce Type: replace 
Abstract: This invited feature article introduces and provides an extensive simulation study of a new Approximate Bayesian Computation (ABC) framework for estimating the posterior distribution and the maximum likelihood estimate (MLE) of the parameters of models defined by intractable likelihoods, which unifies and extends previous ABC method. This framework, copulaABcdrf, aims to accurately estimate and describe the possibly skewed and high dimensional posterior distribution by a novel multivariate copula-based meta-\textit{t} distribution, based on univariate marginal posterior distributions which can be accurately estimated by Distribution Random Forests (drf), while performing automatic summary statistics (covariates) selection, and robust estimation of copula dependence parameters. The copulaABcdrf framework also provides a novel multivariate mode estimator to perform MLE and posterior mode estimation, and an optional step to perform model selection from a given set of models using posterior probabilities estimated by drf. The posterior distribution estimation accuracy of copulaABcdrf is illustrated and compared to standard ABC methods, through several simulation studies involving low- and high-dimensional models with computable posterior distributions, which are either unimodal, skewed, or multimodal; and exponential random graph and mechanistic network models, each defined by an intractable likelihood from which it is costly to simulate large network datasets. We also study a new solution to the simulation cost problem in ABC. The copulaABcdrf framework and standard ABC methods are further illustrated through analyses of large real-life networks. The results of the simulation and empirical studies, and their implications for future research, are summarized. Keywords: Bayesian analysis, Maximum Likelihood, Intractable likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18450v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Karabatsos</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: V. Non-asymptotic</title>
      <link>https://arxiv.org/abs/2403.18951</link>
      <description>arXiv:2403.18951v3 Announce Type: replace 
Abstract: Due to the complexity of order statistics, the finite sample behaviour of robust statistics is generally not analytically solvable. While the Monte Carlo method can provide approximate solutions, its convergence rate is typically very slow, making the computational cost to achieve the desired accuracy unaffordable for ordinary users. In this paper, we propose an approach analogous to the Fourier transformation to decompose the finite sample structure of the uniform distribution. By obtaining sets of sequences that are consistent with parametric distributions for the first four sample moments, we can approximate the finite sample behavior of other estimators with significantly reduced computational costs. This article reveals the underlying structure of randomness and presents a novel approach to integrate multiple assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18951v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>Uncertainty Learning for High-dimensional Mean-variance Portfolio</title>
      <link>https://arxiv.org/abs/2405.16989</link>
      <description>arXiv:2405.16989v2 Announce Type: replace 
Abstract: Robust estimation for modern portfolio selection on a large set of assets becomes more important due to large deviation of empirical inference on big data. We propose a distributionally robust methodology for high-dimensional mean-variance portfolio problem, aiming to select an optimal conservative portfolio allocation by taking distribution uncertainty into account. With the help of factor structure, we extend the distributionally robust mean-variance problem investigated by Blanchet et al. (2022, Management Science) to the high-dimensional scenario and transform it to a new penalized risk minimization problem. Furthermore, we propose a data-adaptive method to estimate the quantified uncertainty size, which is the radius around the empirical probability measured by the Wasserstein distance. Asymptotic consistency is derived for the estimation of the population parameters involved in selecting the uncertainty size and the selected portfolio return. Our Monte-Carlo simulation results show that the chosen uncertainty size and target return from the proposed procedure are very close to the corresponding oracle version, and the new portfolio strategy is of low risk. Finally, we conduct empirical studies based on S&amp;P index components to show the robust performance of our proposal in terms of risk controlling and return-risk balancing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16989v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ruike Wu, Yanrong Yang, Han Lin Shang, Huanjun Zhu</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Inference for Double/Debiased Machine Learning of Causal Parameters</title>
      <link>https://arxiv.org/abs/2408.09598</link>
      <description>arXiv:2408.09598v2 Announce Type: replace 
Abstract: Double (debiased) machine learning (DML) has seen widespread use in recent years for learning causal/structural parameters, in part due to its flexibility and adaptability to high-dimensional nuisance functions as well as its ability to avoid bias from regularization or overfitting. However, the classic double-debiased framework is only valid asymptotically for a predetermined sample size, thus lacking the flexibility of collecting more data if sharper inference is needed, or stopping data collection early if useful inferences can be made earlier than expected. This can be of particular concern in large scale experimental studies with huge financial costs or human lives at stake, as well as in observational studies where the length of confidence of intervals do not shrink to zero even with increasing sample size due to partial identifiability of a structural parameter. In this paper, we present time-uniform counterparts to the asymptotic DML results, enabling valid inference and confidence intervals for structural parameters to be constructed at any arbitrary (possibly data-dependent) stopping time. We provide conditions which are only slightly stronger than the standard DML conditions, but offer the stronger guarantee for anytime-valid inference. This facilitates the transformation of any existing DML method to provide anytime-valid guarantees with minimal modifications, making it highly adaptable and easy to use. We illustrate our procedure using two instances: a) local average treatment effect in online experiments with non-compliance, and b) partial identification of average treatment effect in observational studies with potential unmeasured confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09598v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinandan Dalal, Patrick Bl\"obaum, Shiva Kasiviswanathan, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A parameterization of anisotropic Gaussian fields with penalized complexity priors</title>
      <link>https://arxiv.org/abs/2409.02331</link>
      <description>arXiv:2409.02331v2 Announce Type: replace 
Abstract: Gaussian random fields (GFs) are fundamental tools in spatial modeling and can be represented flexibly and efficiently as solutions to stochastic partial differential equations (SPDEs). The SPDEs depend on specific parameters, which enforce various field behaviors and can be estimated using Bayesian inference. However, the likelihood typically only provides limited insights into the covariance structure under in-fill asymptotics. In response, it is essential to leverage priors to achieve appropriate, meaningful covariance structures in the posterior. This study introduces a smooth, invertible parameterization of the correlation length and diffusion matrix of an anisotropic GF and constructs penalized complexity (PC) priors for the model when the parameters are constant in space. The formulated prior is weakly informative, effectively penalizing complexity by pushing the correlation range toward infinity and the anisotropy to zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02331v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liam Llamazares-Elias, Jonas Latz, Finn Lindgren</dc:creator>
    </item>
    <item>
      <title>Efficient nonparametric estimators of discrimination measures with censored survival data</title>
      <link>https://arxiv.org/abs/2409.05632</link>
      <description>arXiv:2409.05632v2 Announce Type: replace 
Abstract: Discrimination measures such as the concordance index and the cumulative-dynamic time-dependent area under the ROC-curve (AUC) are widely used in the medical literature for evaluating the predictive accuracy of a scoring rule which relates a set of prognostic markers to the risk of experiencing a particular event. Often the scoring rule being evaluated in terms of discriminatory ability is the linear predictor of a survival regression model such as the Cox proportional hazards model. This has the undesirable feature that the scoring rule depends on the censoring distribution when the model is misspecified. In this work we focus on linear scoring rules where the coefficient vector is a nonparametric estimand defined in the setting where there is no censoring. We propose so-called debiased estimators of the aforementioned discrimination measures for this class of scoring rules. The proposed estimators make efficient use of the data and minimize bias by allowing for the use of data-adaptive methods for model fitting. Moreover, the estimators do not rely on correct specification of the censoring model to produce consistent estimation. We compare the estimators to existing methods in a simulation study, and we illustrate the method by an application to a brain cancer study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05632v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marie S. Breum, Torben Martinussen</dc:creator>
    </item>
    <item>
      <title>Graphical models for infinite measures with applications to extremes</title>
      <link>https://arxiv.org/abs/2211.15769</link>
      <description>arXiv:2211.15769v2 Announce Type: replace-cross 
Abstract: Conditional independence and graphical models are well studied for probability distributions on product spaces. We propose a new notion of conditional independence for any measure $\Lambda$ on the punctured Euclidean space $\mathbb R^d\setminus \{0\}$ that explodes at the origin. The importance of such measures stems from their connection to infinitely divisible and max-infinitely divisible distributions, where they appear as L\'evy measures and exponent measures, respectively. We characterize independence and conditional independence for $\Lambda$ in various ways through kernels and factorization of a modified density, including a Hammersley-Clifford type theorem for undirected graphical models. As opposed to the classical conditional independence, our notion is intimately connected to the support of the measure $\Lambda$. Our general theory unifies and extends recent approaches to graphical modeling in the fields of extreme value analysis and L\'evy processes. Our results for the corresponding undirected and directed graphical models lay the foundation for new statistical methodology in these areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.15769v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Engelke, Jevgenijs Ivanovs, Kirstin Strokorb</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: I. Mean</title>
      <link>https://arxiv.org/abs/2403.12110</link>
      <description>arXiv:2403.12110v5 Announce Type: replace-cross 
Abstract: As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12110v5</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>Towards Complete Causal Explanation with Expert Knowledge</title>
      <link>https://arxiv.org/abs/2407.07338</link>
      <description>arXiv:2407.07338v2 Announce Type: replace-cross 
Abstract: We study the problem of restricting a Markov equivalence class of maximal ancestral graphs (MAGs) to only those MAGs that contain certain edge marks, which we refer to as expert knowledge. Such a restriction of the Markov equivalence class can be uniquely represented by a restricted essential ancestral graph. Our contributions are several-fold. First, we prove certain properties for the entire Markov equivalence class including a conjecture from Ali et al. (2009). Second, we present several new sound graphical orientation rules for adding expert knowledge to an essential ancestral graph. We also show that some orientation rules of Zhang (2008b) are not needed for restricting the Markov equivalence class with expert knowledge. Third, we provide an algorithm for including this expert knowledge and show that in certain settings the output of our algorithm is a restricted essential ancestral graph. Finally, outside of the specified settings, we provide an algorithm for checking whether a graph is a restricted essential graph and discuss its runtime. This work can be seen as a generalization of Meek (1995) to settings which allow for latent confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07338v2</guid>
      <category>stat.ML</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aparajithan Venkateswaran, Emilija Perkovi\'c</dc:creator>
    </item>
  </channel>
</rss>
