<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Oct 2024 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Query Based Construction of Chronic Disease Datasets</title>
      <link>https://arxiv.org/abs/2410.13880</link>
      <description>arXiv:2410.13880v1 Announce Type: new 
Abstract: The RECONNECT project addresses the fragmentation of Ireland's public healthcare systems, aiming to enhance service planning and delivery for chronic disease management. By integrating complex systems within the Health Service Executive (HSE), it prioritizes data privacy while supporting future digital resource integration. The methodology encompasses structural integration through a Federated Database design to maintain system autonomy and privacy, semantic integration using a Record Linkage module to facilitate integration without individual identifiers, and the adoption of the HL7-FHIR framework for high interoperability with the national electronic health record (EHR) and the Integrated Information Service (IIS). This innovative approach features a unique architecture for loosely coupled systems and a robust privacy layer. A demonstration system has been implemented to utilize synthetic data from the Hospital Inpatient Enquiry (HIPE), Chronic Disease Management (CDM), Primary Care Reimbursement Service (PCRS) and Retina Screen systems for healthcare queries. Overall, RECONNECT aims to provide timely and effective care, enhance clinical decision-making, and empower policymakers with comprehensive population health insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13880v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vuong M. Ngo, Geetika Sood, Patricia Kearney, Fionnuala Donohue, Dongyun Nie, Mark Roantree</dc:creator>
    </item>
    <item>
      <title>Cartographier des trajectoires maritimes incertaines du XVIII \`eme si\`ecle</title>
      <link>https://arxiv.org/abs/2410.13884</link>
      <description>arXiv:2410.13884v1 Announce Type: new 
Abstract: This article presents how ship trajectories have been built from historical sources dealing with maritime trade in the 18th century. It first summarizes the method for building the routes, and qualifying the uncertainty level linked to each of its segments. Then, it details how the geometries of these segments connecting two successive stopovers were automatically calculated in order to draw a map with maritime paths only. The algorithm, programmed with PL/SQL language, is available under an open-source licence (https://gitlab.huma-num.fr/portic/porticapi). Finally, an online tool for querying and mapping these routes with their associated level of uncertainty is exposed (http://shiproutes.portic.fr). We show its usefulness for historians, in particular for the control of the validity of built ship trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13884v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Spatial Analysis and GEOmatics (SAGEO) 2023, Thierry Badard, Jacynthe Pouliot, Matthieu Noucher, Marl{\`e}ne Villanova-Oliver, Jun 2023, Qu{\'e}bec, Canada</arxiv:journal_reference>
      <dc:creator>Christine Plumejeaud-Perreau (Migrinter), Bernard Pradines (Migrinter)</dc:creator>
    </item>
    <item>
      <title>Lightweight Correlation-Aware Table Compression</title>
      <link>https://arxiv.org/abs/2410.14066</link>
      <description>arXiv:2410.14066v1 Announce Type: new 
Abstract: The growing adoption of data lakes for managing relational data necessitates efficient, open storage formats that provide high scan performance and competitive compression ratios. While existing formats achieve fast scans through lightweight encoding techniques, they have reached a plateau in terms of minimizing storage footprint. Recently, correlation-aware compression schemes have been shown to reduce file sizes further. Yet, current approaches either incur significant scan overheads or require manual specification of correlations, limiting their practicability. We present $\texttt{Virtual}$, a framework that integrates seamlessly with existing open formats to automatically leverage data correlations, achieving substantial compression gains while having minimal scan performance overhead. Experiments on $\texttt{data.gov}$ datasets show that $\texttt{Virtual}$ reduces file sizes by up to 40% compared to Apache Parquet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14066v1</guid>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mihail Stoian, Alexander van Renen, Jan Kobiolka, Ping-Lin Kuo, Josif Grabocka, Andreas Kipf</dc:creator>
    </item>
    <item>
      <title>Towards a Simple and Extensible Standard for Object-Centric Event Data (OCED) -- Core Model, Design Space, and Lessons Learned</title>
      <link>https://arxiv.org/abs/2410.14495</link>
      <description>arXiv:2410.14495v1 Announce Type: new 
Abstract: Process mining is shifting towards use cases that explicitly leverage the relations between data objects and events under the term of object-centric process mining. Realizing this shift and generally simplifying the exchange and transformation of data between source systems and process mining solutions requires a standardized data format for such object-centric event data (OCED). This report summarizes the activities and results for identifying requirements and challenges for a community-supported standard for OCED. (1) We present a proposal for a core model for object-centric event data that underlies all known use cases. (2) We detail the limitations of the core model wrt. a broad range of use cases and discuss how to overcome them through conventions, usage patterns, and extensions of OCED, exhausting the design-space for an OCED data model and the inherent trade-offs in representing object-centric event data. (3) These insights are backed by five independent OCED implementations which are presented alongside a series of lessons learned in academic and industrial case studies. The results of this report provide guidance to the community to start adopting and building new process mining use cases and solutions around the reliable concepts for object-centric event data, and to engage in a structured process for standardizing OCED based on the known OCED design space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14495v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dirk Fahland, Marco Montali, Julian Lebherz, Wil M. P. van der Aalst, Maarten van Asseldonk, Peter Blank, Lien Bosmans, Marcus Brenscheidt, Claudio di Ciccio, Andrea Delgado, Daniel Calegari, Jari Peeperkorn, Eric Verbeek, Lotte Vugs, Moe Thandar Wynn</dc:creator>
    </item>
    <item>
      <title>SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space Trajectory Similarity</title>
      <link>https://arxiv.org/abs/2410.14629</link>
      <description>arXiv:2410.14629v1 Announce Type: cross 
Abstract: Free-space trajectory similarity calculation, e.g., DTW, Hausdorff, and Frechet, often incur quadratic time complexity, thus learning-based methods have been proposed to accelerate the computation. The core idea is to train an encoder to transform trajectories into representation vectors and then compute vector similarity to approximate the ground truth. However, existing methods face dual challenges of effectiveness and efficiency: 1) they all utilize Euclidean distance to compute representation similarity, which leads to the severe curse of dimensionality issue -- reducing the distinguishability among representations and significantly affecting the accuracy of subsequent similarity search tasks; 2) most of them are trained in triplets manner and often necessitate additional information which downgrades the efficiency; 3) previous studies, while emphasizing the scalability in terms of efficiency, overlooked the deterioration of effectiveness when the dataset size grows. To cope with these issues, we propose a simple, yet accurate, fast, scalable model that only uses a single-layer vanilla transformer encoder as the feature extractor and employs tailored representation similarity functions to approximate various ground truth similarity measures. Extensive experiments demonstrate our model significantly mitigates the curse of dimensionality issue and outperforms the state-of-the-arts in effectiveness, efficiency, and scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14629v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuang Yang, Renhe Jiang, Xiaohang Xu, Chuan Xiao, Kaoru Sezaki</dc:creator>
    </item>
    <item>
      <title>KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks</title>
      <link>https://arxiv.org/abs/2410.06526</link>
      <description>arXiv:2410.06526v2 Announce Type: replace 
Abstract: In this paper, we introduce Knowledge-Orthogonal Reasoning (KOR), which minimizes the impact of domain-specific knowledge for a more accurate evaluation of models' reasoning abilities in out-of-distribution scenarios. Based on this concept, we propose the Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench), encompassing five task categories: Operation, Logic, Cipher, Puzzle, and Counterfactual. KOR-Bench emphasizes the effectiveness of models in applying new rule descriptions to solve novel rule-driven questions. O1-Preview and O1-Mini achieve accuracies of 72.88% and 70.16%, significantly outperforming Claude-3.5-Sonnet and GPT-4o, which score 58.96% and 58.00%, revealing considerable performance gaps and highlighting KOR-Bench's effectiveness. We conduct thorough analyses to identify bottlenecks in the Cipher task using Stepwise Prompting, discovering that two rounds of Self-Correction yield optimal results. Complex Task Processing evaluates model performance across three integrated tasks, while we also explore the impact of Tricks on the Puzzle task and visualize rule-focused attention to enhance our understanding of model behavior. KOR-Bench aims to enhance reasoning evaluation and support further research in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06526v2</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaijing Ma, Xinrun Du, Yunran Wang, Haoran Zhang, Zhoufutu Wen, Xingwei Qu, Jian Yang, Jiaheng Liu, Minghao Liu, Xiang Yue, Wenhao Huang, Ge Zhang</dc:creator>
    </item>
    <item>
      <title>Finding Logic Bugs in Spatial Database Engines via Affine Equivalent Inputs</title>
      <link>https://arxiv.org/abs/2410.12496</link>
      <description>arXiv:2410.12496v2 Announce Type: replace 
Abstract: Spatial Database Management Systems (SDBMSs) aim to store, manipulate, and retrieve spatial data. SDBMSs are employed in various modern applications, such as geographic information systems, computer-aided design tools, and location-based services. However, the presence of logic bugs in SDBMSs can lead to incorrect results, substantially undermining the reliability of these applications. Detecting logic bugs in SDBMSs is challenging due to the lack of ground truth for identifying incorrect results. In this paper, we propose an automated geometry-aware generator to generate high-quality SQL statements for SDBMSs and a novel concept named Affine Equivalent Inputs (AEI) to validate the results of SDBMSs. We implemented them as a tool named Spatter (Spatial DBMSs Tester) for finding logic bugs in four popular SDBMSs: PostGIS, DuckDB Spatial, MySQL, and SQL Server. Our testing campaign detected 34 previously unknown and unique bugs in these SDBMS, of which 30 have been confirmed, and 18 have been already fixed. Our testing efforts have been well appreciated by the developers. Experimental results demonstrate that the geometry-aware generator significantly outperforms a naive random-shape generator in detecting unique bugs, and AEI can identify 14 logic bugs in SDBMSs that were overlooked by previous methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12496v2</guid>
      <category>cs.DB</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3698810</arxiv:DOI>
      <dc:creator>Wenjing Deng, Qiuyang Mang, Chengyu Zhang, Manuel Rigger</dc:creator>
    </item>
    <item>
      <title>Constructive Interpolation and Concept-Based Beth Definability for Description Logics via Sequents</title>
      <link>https://arxiv.org/abs/2404.15840</link>
      <description>arXiv:2404.15840v3 Announce Type: replace-cross 
Abstract: We introduce a constructive method applicable to a large number of description logics (DLs) for establishing the concept-based Beth definability property (CBP) based on sequent systems. Using the highly expressive DL RIQ as a case study, we introduce novel sequent calculi for RIQ-ontologies and show how certain interpolants can be computed from sequent calculus proofs, which permit the extraction of explicit definitions of implicitly definable concepts. To the best of our knowledge, this is the first sequent-based approach to computing interpolants and definitions within the context of DLs, as well as the first proof that RIQ enjoys the CBP. Moreover, due to the modularity of our sequent systems, our results hold for restrictions of RIQ, and are applicable to other DLs by suitable modifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15840v3</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>math.LO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.24963/ijcai.2024/386</arxiv:DOI>
      <dc:creator>Tim S. Lyon, Jonas Karge</dc:creator>
    </item>
    <item>
      <title>QUIS: Question-guided Insights Generation for Automated Exploratory Data Analysis</title>
      <link>https://arxiv.org/abs/2410.10270</link>
      <description>arXiv:2410.10270v2 Announce Type: replace-cross 
Abstract: Discovering meaningful insights from a large dataset, known as Exploratory Data Analysis (EDA), is a challenging task that requires thorough exploration and analysis of the data. Automated Data Exploration (ADE) systems use goal-oriented methods with Large Language Models and Reinforcement Learning towards full automation. However, these methods require human involvement to anticipate goals that may limit insight extraction, while fully automated systems demand significant computational resources and retraining for new datasets. We introduce QUIS, a fully automated EDA system that operates in two stages: insight generation (ISGen) driven by question generation (QUGen). The QUGen module generates questions in iterations, refining them from previous iterations to enhance coverage without human intervention or manually curated examples. The ISGen module analyzes data to produce multiple relevant insights in response to each question, requiring no prior training and enabling QUIS to adapt to new datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10270v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhijit Manatkar, Ashlesha Akella, Parthivi Gupta, Krishnasuri Narayanam</dc:creator>
    </item>
  </channel>
</rss>
