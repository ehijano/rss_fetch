<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Jul 2024 01:41:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Real-time risk estimation for active road safety: Leveraging Waymo AV sensor data with hierarchical Bayesian extreme value models</title>
      <link>https://arxiv.org/abs/2407.16832</link>
      <description>arXiv:2407.16832v1 Announce Type: new 
Abstract: This study develops a real-time framework for estimating the risk of near-misses by using high-fidelity two-dimensional (2D) risk indicator time-to-collision (TTC), which is calculated from high-resolution data collected by autonomous vehicles (AVs). The framework utilizes extreme value theory (EVT) to derive near-miss risk based on observed TTC data. Most existing studies employ a generalized extreme value (GEV) distribution for specific sites and conflict types and often overlook individual vehicle dynamics heterogeneity. This framework is versatile across various highway geometries and can encompass vehicle dynamics and fidelity by incorporating covariates such as speed, acceleration, steering angle, and heading. This makes the risk estimation framework suitable for dynamic, real-world traffic environments. The dataset for this study is derived from Waymo perception data, encompassing six sites across three cities: San Francisco, Phoenix, and Los Angeles. Vehicle trajectory data were extracted from the dataset, and near-miss frequencies were calculated using high-fidelity 2D TTC. The crash risk was derived from observed near misses using four hierarchical Bayesian GEV models, explicitly focusing on conflicting pairs as block minima (BM), which revealed that crash risk varies across pairs.The proposed framework is efficient using a hierarchical Bayesian structure random parameter (HBSRP) model, offering superior statistical performance and flexibility by accounting for unobserved heterogeneity across sites. The study identifies and quantifies that the most hazardous conditions involve conflicting vehicle speeds and rapid acceleration and deceleration, significantly increasing crash risk in urban arterials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16832v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Anis, Sixu Li, Srinivas R. Geedipally, Yang Zhou, Dominique Lord</dc:creator>
    </item>
    <item>
      <title>Exploring Covid-19 Spatiotemporal Dynamics: Non-Euclidean Spatially Aware Functional Registration</title>
      <link>https://arxiv.org/abs/2407.17132</link>
      <description>arXiv:2407.17132v1 Announce Type: new 
Abstract: When it came to Covid-19, timing was everything. This paper considers the spatiotemporal dynamics of the Covid-19 pandemic via a developed methodology of non-Euclidean spatially aware functional registration. In particular, the daily SARS-CoV-2 incidence in each of 380 local authorities in the UK from March to June 2020 is analysed to understand the phase variation of the waves when considered as curves. This is achieved by adapting a traditional registration method (that of local variation analysis) to account for the clear spatial dependencies in the data. This adapted methodology is shown via simulation studies to perform substantially better for the estimation of the registration functions than the non-spatial alternative. Moreover, it is found that the driving time between locations represents the spatial dependency in the Covid-19 data better than geographical distance. However, since driving time is non-Euclidean, the traditional spatial frameworks break down; to solve this, a methodology inspired by multidimensional scaling is developed to approximate the driving times by a Euclidean distance which enables the established theory to be applied. Finally, the resulting estimates of the registration/warping processes are analysed by taking functionals to understand the qualitatively observable earliness/lateness and sharpness/flatness of the Covid-19 waves quantitatively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17132v1</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke A. Barratt (Statistical Laboratory, DPMMS, University of Cambridge, UK), John A. D. Aston (Statistical Laboratory, DPMMS, University of Cambridge, UK)</dc:creator>
    </item>
    <item>
      <title>Longitudinal Principal Manifold Estimation</title>
      <link>https://arxiv.org/abs/2407.17450</link>
      <description>arXiv:2407.17450v1 Announce Type: new 
Abstract: Longitudinal magnetic resonance imaging data is used to model trajectories of change in brain regions of interest to identify areas susceptible to atrophy in those with neurodegenerative conditions like Alzheimer's disease. Most methods for extracting brain regions are applied to scans from study participants independently, resulting in wide variability in shape and volume estimates of these regions over time in longitudinal studies. To address this problem, we propose a longitudinal principal manifold estimation method, which seeks to recover smooth, longitudinally meaningful manifold estimates of shapes over time. The proposed approach uses a smoothing spline to smooth over the coefficients of principal manifold embedding functions estimated at each time point. This mitigates the effects of random disturbances to the manifold between time points. Additionally, we propose a novel data augmentation approach to enable principal manifold estimation on self-intersecting manifolds. Simulation studies demonstrate performance improvements over naive applications of principal manifold estimation and principal curve/surface methods. The proposed method improves the estimation of surfaces of hippocampuses and thalamuses using data from participants of the Alzheimer's Disease Neuroimaging Initiative. An analysis of magnetic resonance imaging data from 236 individuals shows the advantages of our proposed methods that leverage regional longitudinal trends for segmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17450v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Zielinski, Kun Meng, Ani Eloyan</dc:creator>
    </item>
    <item>
      <title>A Bayesian modelling framework for health care resource use and costs in trial-based economic evaluations</title>
      <link>https://arxiv.org/abs/2407.17036</link>
      <description>arXiv:2407.17036v1 Announce Type: cross 
Abstract: Individual-level effectiveness and healthcare resource use (HRU) data are routinely collected in trial-based economic evaluations. While effectiveness is often expressed in terms of utility scores derived from some health-related quality of life instruments (e.g.~EQ-5D questionnaires), different types of HRU may be included. Costs are usually generated by applying unit prices to HRU data and statistical methods have been traditionally implemented to analyse costs and utilities or after combining them into aggregated variables (e.g. Quality-Adjusted Life Years). When outcome data are not fully observed, e.g. some patients drop out or only provided partial information, the validity of the results may be hindered both in terms of efficiency and bias. Often, partially-complete HRU data are handled using "ad-hoc" methods, implicitly relying on some assumptions (e.g. fill-in a zero) which are hard to justify beside the practical convenience of increasing the completion rate. We present a general Bayesian framework for the modelling of partially-observed HRUs which allows a flexible model specification to accommodate the typical complexities of the data and to quantify the impact of different types of uncertainty on the results. We show the benefits of using our approach using a motivating example and compare the results to those from traditional analyses focussed on the modelling of cost variables after adopting some ad-hoc imputation strategy for HRU data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17036v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Gabrio</dc:creator>
    </item>
    <item>
      <title>Effect of Austerity Measures on Infant Mortality: Evidence from Greece</title>
      <link>https://arxiv.org/abs/2407.17084</link>
      <description>arXiv:2407.17084v1 Announce Type: cross 
Abstract: This study examines the effect of fiscal austerity measures on infant mortality in Greece. Austerity measures were initiated by the tripartite committee and implemented between 2010 and 2017 to counteract deep fiscal deficit and large public debt. By comparing Greece with a plausible donor pool of OECD and Mediterranean member states in the period 1991-2020, we estimate a series of missing counterfactual scenarios to evaluate the infant mortality effects of large-scale reduction in spending on health care. A series of hybrid synthetic control and difference-in-differences estimates indicate a unique and pervasive increase in infant mortality after the implementation of austerity measures. Compared to a plausible OECD and Mediterranean counterfactual scenario, pro-cyclical austerity measures are associated with derailed and permanently increased infant mortality up to the present day. Our estimates suggest that compared to a plausible counterfactual scenario, the cumulative infant mortality cost of austerity policies exceeds 10,000 infant deaths or slightly less than 850 deaths for each year of the austerity policies. Notably, mortality increases are concentrated among boys. The estimated impacts survive a battery of rigorous robustness and placebo tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17084v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert J. Kolesar, Rok Spruk</dc:creator>
    </item>
    <item>
      <title>Enhanced SMC$^2$: Leveraging Gradient Information from Differentiable Particle Filters Within Langevin Proposals</title>
      <link>https://arxiv.org/abs/2407.17296</link>
      <description>arXiv:2407.17296v1 Announce Type: cross 
Abstract: Sequential Monte Carlo Squared (SMC$^2$) is a Bayesian method which can infer the states and parameters of non-linear, non-Gaussian state-space models. The standard random-walk proposal in SMC$^2$ faces challenges, particularly with high-dimensional parameter spaces. This study outlines a novel approach by harnessing first-order gradients derived from a Common Random Numbers - Particle Filter (CRN-PF) using PyTorch. The resulting gradients can be leveraged within a Langevin proposal without accept/reject. Including Langevin dynamics within the proposal can result in a higher effective sample size and more accurate parameter estimates when compared with the random-walk. The resulting algorithm is parallelized on distributed memory using Message Passing Interface (MPI) and runs in $\mathcal{O}(\log_2N)$ time complexity. Utilizing 64 computational cores we obtain a 51x speed-up when compared to a single core. A GitHub link is given which provides access to the code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17296v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Conor Rosato, Joshua Murphy, Alessandro Varsi, Paul Horridge, Simon Maskell</dc:creator>
    </item>
    <item>
      <title>Estimation of bid-ask spreads in the presence of serial dependence</title>
      <link>https://arxiv.org/abs/2407.17401</link>
      <description>arXiv:2407.17401v1 Announce Type: cross 
Abstract: Starting from a basic model in which the dynamic of the transaction prices is a geometric Brownian motion disrupted by a microstructure white noise, corresponding to the random alternation of bids and asks, we propose moment-based estimators along with their statistical properties. We then make the model more realistic by considering serial dependence: we assume a geometric fractional Brownian motion for the price, then an Ornstein-Uhlenbeck process for the microstructure noise. In these two cases of serial dependence, we propose again consistent and asymptotically normal estimators. All our estimators are compared on simulated data with existing approaches, such as Roll, Corwin-Schultz, Abdi-Ranaldo, or Ardia-Guidotti-Kroencke estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17401v1</guid>
      <category>q-fin.ST</category>
      <category>q-fin.MF</category>
      <category>q-fin.TR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xavier Brouty, Matthieu Garcin, Hugo Roccaro</dc:creator>
    </item>
    <item>
      <title>Logistic regression models for patient-level prediction based on massive observational data: Do we need all data?</title>
      <link>https://arxiv.org/abs/2008.07361</link>
      <description>arXiv:2008.07361v2 Announce Type: replace 
Abstract: Objective: Provide guidance on sample size considerations for developing predictive models by empirically establishing the adequate sample size, which balances the competing objectives of improving model performance and reducing model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on prediction performance and model complexity by generating learning curves for 81 prediction problems (23 outcomes predicted in a depression cohort, 58 outcomes predicted in a hypertension cohort) in three large observational health databases, requiring training of 17,248 prediction models. The adequate sample size was defined as the sample size for which the performance of a model equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001, 0.005, 0.01, and 0.02, respectively. The median reduction of the number of predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction in sample size and model complexity can be estimated for future prediction work. Though, if a researcher is willing to generate a learning curve a much larger reduction of the model complexity may be possible as suggested by a large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the available data was sufficient to produce a model close to the performance of one developed on the full data set, but with a substantially reduced model complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.07361v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijmedinf.2022.104762</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Medical Informatics, Volume 163, July 2022, Article number 104762</arxiv:journal_reference>
      <dc:creator>Luis H. John, Jan A. Kors, Jenna M. Reps, Patrick B. Ryan, Peter R. Rijnbeek</dc:creator>
    </item>
    <item>
      <title>Low-rank longitudinal factor regression</title>
      <link>https://arxiv.org/abs/2311.16470</link>
      <description>arXiv:2311.16470v2 Announce Type: replace 
Abstract: Developmental epidemiology commonly focuses on assessing the association between multiple early life exposures and childhood health. Statistical analyses of data from such studies focus on inferring the contributions of individual exposures, while also characterizing time-varying and interacting effects. Such inferences are made more challenging by correlations among exposures, nonlinearity, and the curse of dimensionality. Motivated by studying the effects of prenatal bisphenol A (BPA) and phthalate exposures on glucose metabolism in adolescence using data from the ELEMENT study, we propose a low-rank longitudinal factor regression (LowFR) model for tractable inference on flexible longitudinal exposure effects. LowFR handles highly-correlated exposures using a Bayesian dynamic factor model, which is fit jointly with a health outcome via a novel factor regression approach. The model collapses on simpler and intuitive submodels when appropriate, while expanding to allow considerable flexibility in time-varying and interaction effects when supported by the data. After demonstrating LowFR's effectiveness in simulations, we use it to analyze the ELEMENT data and find that diethyl and dibutyl phthalate metabolite levels in trimesters 1 and 2 are associated with altered glucose metabolism in adolescence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16470v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glenn Palmer, Amy H. Herring, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>An open-source framework for data-driven trajectory extraction from AIS data -- the $\alpha$-method</title>
      <link>https://arxiv.org/abs/2407.04402</link>
      <description>arXiv:2407.04402v2 Announce Type: replace 
Abstract: Ship trajectories from Automatic Identification System (AIS) messages are important in maritime safety, domain awareness, and algorithmic testing. Although the specifications for transmitting and receiving AIS messages are fixed, it is well known that technical inaccuracies and lacking seafarer compliance lead to severe data quality impairment. This paper proposes an adaptable, data-driven, $\alpha$-quantile-based framework for decoding, constructing, splitting, and assessing trajectories from raw AIS records to improve transparency in AIS data mining. Results indicate the proposed filtering algorithm robustly extracts clean, long, and uninterrupted trajectories for further processing. An open-source Python implementation of the framework is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04402v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Niklas Paulig, Ostap Okhrin</dc:creator>
    </item>
    <item>
      <title>The piranha problem: Large effects swimming in a small pond</title>
      <link>https://arxiv.org/abs/2105.13445</link>
      <description>arXiv:2105.13445v5 Announce Type: replace-cross 
Abstract: In some scientific fields, it is common to have certain variables of interest that are of particular importance and for which there are many studies indicating a relationship with different explanatory variables. In such cases, particularly those where no relationships are known among the explanatory variables, it is worth asking under what conditions it is possible for all such claimed effects to exist simultaneously. This paper addresses this question by reviewing some theorems from multivariate analysis showing that, unless the explanatory variables also have sizable dependencies with each other, it is impossible to have many such large effects. We discuss implications for the replication crisis in social science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.13445v5</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Tosh, Philip Greengard, Ben Goodrich, Andrew Gelman, Aki Vehtari, Daniel Hsu</dc:creator>
    </item>
  </channel>
</rss>
