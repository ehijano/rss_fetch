<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Jul 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Generalisation to unseen topologies: Towards control of biological neural network activity</title>
      <link>https://arxiv.org/abs/2407.12789</link>
      <description>arXiv:2407.12789v1 Announce Type: new 
Abstract: Novel imaging and neurostimulation techniques open doors for advancements in closed-loop control of activity in biological neural networks. This would allow for applications in the investigation of activity propagation, and for diagnosis and treatment of pathological behaviour. Due to the partially observable characteristics of activity propagation, through networks in which edges can not be observed, and the dynamic nature of neuronal systems, there is a need for adaptive, generalisable control. In this paper, we introduce an environment that procedurally generates neuronal networks with different topologies to investigate this generalisation problem. Additionally, an existing transformer-based architecture is adjusted to evaluate the generalisation performance of a deep RL agent in the presented partially observable environment. The agent demonstrates the capability to generalise control from a limited number of training networks to unseen test networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12789v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laurens Engwegen, Daan Brinks, Wendelin B\"ohmer</dc:creator>
    </item>
    <item>
      <title>Evaluating the evolution and inter-individual variability of infant functional module development from 0 to 5 years old</title>
      <link>https://arxiv.org/abs/2407.13118</link>
      <description>arXiv:2407.13118v1 Announce Type: new 
Abstract: The segregation and integration of infant brain networks undergo tremendous changes due to the rapid development of brain function and organization. Traditional methods for estimating brain modularity usually rely on group-averaged functional connectivity (FC), often overlooking individual variability. To address this, we introduce a novel approach utilizing Bayesian modeling to analyze the dynamic development of functional modules in infants over time. This method retains inter-individual variability and, in comparison to conventional group averaging techniques, more effectively detects modules, taking into account the stationarity of module evolution. Furthermore, we explore gender differences in module development under awake and sleep conditions by assessing modular similarities. Our results show that female infants demonstrate more distinct modular structures between these two conditions, possibly implying relative quiet and restful sleep compared with male infants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13118v1</guid>
      <category>q-bio.NC</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingbin Bian, Nizhuan Wang, Yuanning Li, Adeel Razi, Qian Wang, Han Zhang, Dinggang Shen, the UNC/UMN Baby Connectome Project Consortium</dc:creator>
    </item>
    <item>
      <title>Statistical thermodynamics of the human brain activity, the Hagedorn temperature and the Zipf law</title>
      <link>https://arxiv.org/abs/2407.13196</link>
      <description>arXiv:2407.13196v1 Announce Type: new 
Abstract: It is well established that the brain spontaneously traverses through a very large number of states. Nevertheless, despite its relevance to understanding brain function, a formal description of this phenomenon is still lacking. To this end, we introduce a machine learning based method allowing for the determination of the probabilities of all possible states at a given coarse-graining, from which all the thermodynamics can be derived. This is a challenge not unique to the brain, since similar problems are at the heart of the statistical mechanics of complex systems. This paper uncovers a linear scaling of the entropies and energies of the brain states, a behaviour first conjectured by Hagedorn to be typical at the limiting temperature in which ordinary matter disintegrates into quark matter. Equivalently, this establishes the existence of a Zipf law scaling underlying the appearance of a wide range of brain states. Based on our estimation of the density of states for large scale functional magnetic resonance imaging (fMRI) human brain recordings, we observe that the brain operates asymptotically at the Hagedorn temperature. The presented approach is not only relevant to brain function but should be applicable for a wide variety of complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13196v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dante R. Chialvo, Romuald A. Janik</dc:creator>
    </item>
    <item>
      <title>Topological Analysis of Seizure-Induced Changes in Brain Hierarchy Through Effective Connectivity</title>
      <link>https://arxiv.org/abs/2407.13514</link>
      <description>arXiv:2407.13514v1 Announce Type: new 
Abstract: Traditional Topological Data Analysis (TDA) methods, such as Persistent Homology (PH), rely on distance measures (e.g., cross-correlation, partial correlation, coherence, and partial coherence) that are symmetric by definition. While useful for studying topological patterns in functional brain connectivity, the main limitation of these methods is their inability to capture the directional dynamics - which is crucial for understanding effective brain connectivity. We propose the Causality-Based Topological Ranking (CBTR) method, which integrates Causal Inference (CI) to assess effective brain connectivity with Hodge Decomposition (HD) to rank brain regions based on their mutual influence. Our simulations confirm that the CBTR method accurately and consistently identifies hierarchical structures in multivariate time series data. Moreover, this method effectively identifies brain regions showing the most significant interaction changes with other regions during seizures using electroencephalogram (EEG) data. These results provide novel insights into the brain's hierarchical organization and illuminate the impact of seizures on its dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13514v1</guid>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anass B. El-Yaagoubi, Moo K. Chung, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Cannabis Impairment Monitoring Using Objective Eye Tracking Analytics</title>
      <link>https://arxiv.org/abs/2407.13701</link>
      <description>arXiv:2407.13701v1 Announce Type: cross 
Abstract: The continuing growth in cannabis legalization necessitates the development of rapid, objective methods for assessing impairment to ensure public and occupational safety. Traditional measurement techniques are subjective, time-consuming, and do not directly measure physical impairment. This study introduces objective metrics derived from eye-tracking analytics to address these limitations.
  We employed a head-mounted display to present 20 subjects with smooth pursuit performance, horizontal saccade, and simple reaction time tasks. Individual and group performance was compared before and after cannabis use. Results demonstrated significant changes in oculomotor control post-cannabis consumption, with smooth pursuit performance showing the most substantial signal.
  The objective eye-tracking data was used to develop supervised learning models, achieving a classification accuracy of 89% for distinguishing between sober and impaired states when normalized against baseline measures. Eye-tracking is the optimal candidate for a portable, rapid, and objective tool for assessing cannabis impairment, offering significant improvements over current subjective and indirect methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13701v1</guid>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jon Allen, Leah Brickson, Jan van Merkensteijn, Daniel Beeler, Jamshid Ghajar</dc:creator>
    </item>
    <item>
      <title>Spiking mode-based neural networks</title>
      <link>https://arxiv.org/abs/2310.14621</link>
      <description>arXiv:2310.14621v3 Announce Type: replace 
Abstract: Spiking neural networks play an important role in brain-like neuromorphic computations and in studying working mechanisms of neural circuits. One drawback of training a large scale spiking neural network is that updating all weights is quite expensive. Furthermore, after training, all information related to the computational task is hidden into the weight matrix, prohibiting us from a transparent understanding of circuit mechanisms. Therefore, in this work, we address these challenges by proposing a spiking mode-based training protocol, where the recurrent weight matrix is explained as a Hopfield-like multiplication of three matrices: input, output modes and a score matrix. The first advantage is that the weight is interpreted by input and output modes and their associated scores characterizing the importance of each decomposition term. The number of modes is thus adjustable, allowing more degrees of freedom for modeling the experimental data. This significantly reduces the training cost because of significantly reduced space complexity for learning. Training spiking networks is thus carried out in the mode-score space. The second advantage is that one can project the high dimensional neural activity (filtered spike train) in the state space onto the mode space which is typically of a low dimension, e.g., a few modes are sufficient to capture the shape of the underlying neural manifolds. We successfully apply our framework in two computational tasks -- digit classification and selective sensory integration tasks. Our method accelerate the training of spiking neural networks by a Hopfield-like decomposition, and moreover this training leads to low-dimensional attractor structures of high-dimensional neural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14621v3</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhanghan Lin, Haiping Huang</dc:creator>
    </item>
    <item>
      <title>Cracking the neural code for word recognition in convolutional neural networks</title>
      <link>https://arxiv.org/abs/2403.06159</link>
      <description>arXiv:2403.06159v2 Announce Type: replace-cross 
Abstract: Learning to read places a strong challenge on the visual system. Years of expertise lead to a remarkable capacity to separate highly similar letters and encode their relative positions, thus distinguishing words such as FORM and FROM, invariantly over a large range of sizes and absolute positions. How neural circuits achieve invariant word recognition remains unknown. Here, we address this issue by training deep neural network models to recognize written words and then analyzing how reading-specialized units emerge and operate across different layers of the network. With literacy, a small subset of units becomes specialized for word recognition in the learned script, similar to the "visual word form area" of the human brain. We show that these units are sensitive to specific letter identities and their distance from the blank space at the left or right of a word, thus acting as "space bigrams". These units specifically encode ordinal positions and operate by pooling across low and high-frequency detector units from early layers of the network. The proposed neural code provides a mechanistic insight into how information on letter identity and position is extracted and allow for invariant word recognition, and leads to predictions for reading behavior, error patterns, and the neurophysiology of reading.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06159v2</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.48550/arXiv.2403.06159</arxiv:DOI>
      <dc:creator>Aakash Agrawal, Stanislas Dehaene</dc:creator>
    </item>
    <item>
      <title>Fermi-Bose Machine achieves both generalization and adversarial robustness</title>
      <link>https://arxiv.org/abs/2404.13631</link>
      <description>arXiv:2404.13631v2 Announce Type: replace-cross 
Abstract: Distinct from human cognitive processing, deep neural networks trained by backpropagation can be easily fooled by adversarial examples. To design a semantically meaningful representation learning, we discard backpropagation, and instead, propose a local contrastive learning, where the representation for the inputs bearing the same label shrink (akin to boson) in hidden layers, while those of different labels repel (akin to fermion). This layer-wise learning is local in nature, being biological plausible. A statistical mechanics analysis shows that the target fermion-pair-distance is a key parameter. Moreover, the application of this local contrastive learning to MNIST benchmark dataset demonstrates that the adversarial vulnerability of standard perceptron can be greatly mitigated by tuning the target distance, i.e., controlling the geometric separation of prototype manifolds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13631v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingshan Xie, Yuchen Wang, Haiping Huang</dc:creator>
    </item>
  </channel>
</rss>
