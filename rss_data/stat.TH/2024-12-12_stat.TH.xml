<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Dec 2024 02:52:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On kernel mode estimation under RLT and WOD model</title>
      <link>https://arxiv.org/abs/2412.07874</link>
      <description>arXiv:2412.07874v1 Announce Type: new 
Abstract: Let $(X_N)_{N\geq 1}$ denote a sequence of real random variables and let $\vartheta$ be the mode of the random variable of interest $X$. In this paper, we study the kernel mode estimator (say) $\vartheta_n$ when the data are widely orthant dependent (WOD) and subject to Random Left Truncation (RLT) mechanism. We establish the uniform consistency rate of the density estimator (say) $f_n$ of the underlying density $f$ as well as the almost sure convergence rate of $\vartheta_n$. The performance of the estimators are illustrated via some simulation studies and applied on a real dataset of car brake pads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07874v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Kaber El Alem, Zohra Guessoum, Abdelkader Tatachak</dc:creator>
    </item>
    <item>
      <title>Fast Mixing of Data Augmentation Algorithms: Bayesian Probit, Logit, and Lasso Regression</title>
      <link>https://arxiv.org/abs/2412.07999</link>
      <description>arXiv:2412.07999v1 Announce Type: new 
Abstract: Despite the widespread use of the data augmentation (DA) algorithm, the theoretical understanding of its convergence behavior remains incomplete. We prove the first non-asymptotic polynomial upper bounds on mixing times of three important DA algorithms: DA algorithm for Bayesian Probit regression (Albert and Chib, 1993, ProbitDA), Bayesian Logit regression (Polson, Scott, and Windle, 2013, LogitDA), and Bayesian Lasso regression (Park and Casella, 2008, Rajaratnam et al., 2015, LassoDA). Concretely, we demonstrate that with $\eta$-warm start, parameter dimension $d$, and sample size $n$, the ProbitDA and LogitDA require $\mathcal{O}\left(nd\log \left(\frac{\log \eta}{\epsilon}\right)\right)$ steps to obtain samples with at most $\epsilon$ TV error, whereas the LassoDA requires $\mathcal{O}\left(d^2(d\log d +n \log n)^2 \log \left(\frac{\eta}{\epsilon}\right)\right)$ steps. The results are generally applicable to settings with large $n$ and large $d$, including settings with highly imbalanced response data in the Probit and Logit regression. The proofs are based on the Markov chain conductance and isoperimetric inequalities. Assuming that data are independently generated from either a bounded, sub-Gaussian, or log-concave distribution, we improve the guarantees for ProbitDA and LogitDA to $\tilde{\mathcal{O}}(n+d)$ with high probability, and compare it with the best known guarantees of Langevin Monte Carlo and Metropolis Adjusted Langevin Algorithm. We also discuss the mixing times of the three algorithms under feasible initialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07999v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Holden Lee, Kexin Zhang</dc:creator>
    </item>
    <item>
      <title>Statistical Convergence Rates of Optimal Transport Map Estimation between General Distributions</title>
      <link>https://arxiv.org/abs/2412.08064</link>
      <description>arXiv:2412.08064v1 Announce Type: new 
Abstract: This paper studies the convergence rates of optimal transport (OT) map estimators, a topic of growing interest in statistics, machine learning, and various scientific fields. Despite recent advancements, existing results rely on regularity assumptions that are very restrictive in practice and much stricter than those in Brenier's Theorem, including the compactness and convexity of the probability support and the bi-Lipschitz property of the OT maps. We aim to broaden the scope of OT map estimation and fill this gap between theory and practice. Given the strong convexity assumption on Brenier's potential, we first establish the non-asymptotic convergence rates for the original plug-in estimator without requiring restrictive assumptions on probability measures. Additionally, we introduce a sieve plug-in estimator and establish its convergence rates without the strong convexity assumption on Brenier's potential, enabling the widely used cases such as the rank functions of normal or t-distributions. We also establish new Poincar\'e-type inequalities, which are proved given sufficient conditions on the local boundedness of the probability density and mild topological conditions of the support, and these new inequalities enable us to achieve faster convergence rates for the Donsker function class. Moreover, we develop scalable algorithms to efficiently solve the OT map estimation using neural networks and present numerical experiments to demonstrate the effectiveness and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08064v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizhe Ding, Runze Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Fast pick-freeze estimation of Sobol' sensitivity maps using basis expansions</title>
      <link>https://arxiv.org/abs/2412.08224</link>
      <description>arXiv:2412.08224v1 Announce Type: new 
Abstract: Global sensitivity analysis (GSA) aims at quantifying the contribution of input variables over the variability of model outputs. In the frame of functional outputs, a common goal is to compute sensitivity maps (SM), i.e sensitivity indices at each output dimension (e.g. time step for time series, or pixels for spatial outputs). In specific settings, some works have shown that the computation of Sobol' SM can be speeded up by using basis expansions employed for dimension reduction. However, how to efficiently compute such SM in a general setting has not received too much attention in the GSA literature.In this work, we propose fast computations of Sobol' SM using a general basis expansion, with a focus on statistical estimation. First, we write a closed-form expression of SM in function of the matrix-valued Sobol' index of the vector of basis coefficients. Secondly, we consider pick-freeze (PF) estimators, which have nice statistical properties (in terms of asymptotical efficiency) for Sobol' indices of any order. We provide similar basis-derived formulas for the PF estimator of Sobol' SM in function of the matrix-valued PF estimator of the vector of basis coefficients. We give the computational cost, and show that, compared to a dimension-wise approach, the computational gain is substantial and allows to calculate both SM and their associated bootstrap confidence bounds in a reasonable time. Finally, we illustrate the whole methodology on an analytical test case and on an application in non-Newtonian hydraulics, modelling an idealized dam-break flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08224v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuri Sao (INSA Toulouse, UNESP), Olivier Roustant (INSA Toulouse, IMT, RT-UQ), Geraldo de Freitas Maciel (UNESP)</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the stationary density for Hawkes-diffusion systems with known and unknown intensity</title>
      <link>https://arxiv.org/abs/2412.08386</link>
      <description>arXiv:2412.08386v1 Announce Type: new 
Abstract: We investigate the nonparametric estimation problem of the density $\pi$, representing the stationary distribution of a two-dimensional system $\left(Z_t\right)_{t \in[0, T]}=\left(X_t, \lambda_t\right)_{t \in[0, T]}$. In this system, $X$ is a Hawkes-diffusion process, and $\lambda$ denotes the stochastic intensity of the Hawkes process driving the jumps of $X$. Based on the continuous observation of a path of $(X_t)$ over $[0, T]$, and initially assuming that $\lambda$ is known, we establish the convergence rate of a kernel estimator $\widehat\pi\left(x^*, y^*\right)$ of $\pi\left(x^*,y^*\right)$ as $T \rightarrow \infty$. Interestingly, this rate depends on the value of $y^*$ influenced by the baseline parameter of the Hawkes intensity process. From the rate of convergence of $\widehat\pi\left(x^*,y^*\right)$, we derive the rate of convergence for an estimator of the invariant density $\lambda$. Subsequently, we extend the study to the case where $\lambda$ is unknown, plugging an estimator of $\lambda$ in the kernel estimator and deducing new rates of convergence for the obtained estimator. The proofs establishing these convergence rates rely on probabilistic results that may hold independent interest. We introduce a Girsanov change of measure to transform the Hawkes process with intensity $\lambda$ into a Poisson process with constant intensity. To achieve this, we extend a bound for the exponential moments for the Hawkes process, originally established in the stationary case, to the non-stationary case. Lastly, we conduct a numerical study to illustrate the obtained rates of convergence of our estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08386v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Charlotte Dion-Blanc, Arnaud Gloter, Sarah Lemler</dc:creator>
    </item>
    <item>
      <title>On partial stochastic comparisons based on tail values at risk</title>
      <link>https://arxiv.org/abs/2412.08440</link>
      <description>arXiv:2412.08440v1 Announce Type: new 
Abstract: In risk theory, financial asset returns often follow heavy-tailed distributions. Investors and risk managers used to compare risk measures as the value at risk or tail value at risk in order over the whole confidence levels to avoid the exposure to to large risks. In this paper we analyze the comparison between tail values at risk from a confidence level and beyond which is a reasonable criterion when we are focused on large losses or simply we cannot give a complete ordering over all the confidence levels. A family of stochastic orders indexed by $p_0\in(0,1)$ is proposed. We study their properties and connections with other classical criteria as the increasing convex and tail convex orders and we rank some parametrical families of distributions. Finally, two applications with real datasets are given as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08440v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3390/math8071181</arxiv:DOI>
      <arxiv:journal_reference>Mathematics 2020, 8, 1181</arxiv:journal_reference>
      <dc:creator>Alfonso J. Bello, Julio Mulero, Miguel A. Sordo, Alfonso Su\'arez-Llorens</dc:creator>
    </item>
    <item>
      <title>New stochastic comparisons based on tail values at risk</title>
      <link>https://arxiv.org/abs/2412.08456</link>
      <description>arXiv:2412.08456v1 Announce Type: new 
Abstract: In this paper we provide a new criterion for the comparison of claims, when we have conditional claims arising in stop loss contracts or contracts with franchise deductible. These stochastic comparisons are made on the basis of the Tail Value at Risk (also known as conditional tail expectation), just for a fixed level and beyond. In particular, we explain the interest of comparing these quantities, study some preservation properties and, in addition, we provide sufficient conditions for its study. Finally we illustrate its usefulness with some examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08456v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/03610926.2020.1754857</arxiv:DOI>
      <arxiv:journal_reference>Communications in Statistics - Theory and Methods 2020 Volume 51, Issue 3, 767-788</arxiv:journal_reference>
      <dc:creator>F\'elix Belzunce, Alba M. Franco-Pereira, Julio Mulero</dc:creator>
    </item>
    <item>
      <title>Rethinking Mean Square Error: Why Information is a Superior Assessment of Estimators</title>
      <link>https://arxiv.org/abs/2412.08475</link>
      <description>arXiv:2412.08475v1 Announce Type: new 
Abstract: James-Stein (JS) estimators have been described as showing the inadequacy of maximum likelihood estimation when assessed using mean square error (MSE). We claim the problem is not with maximum likelihood (ML) but with MSE. When MSE is replaced with a measure $\Lambda$ of the information utilized by a statistic, likelihood based methods are superior. The information measure $\Lambda$ describes not just point estimators but extends to Fisher's view of estimation so that we not only reconsider how estimators are assessed but also how we define an estimator. Fisher information and his views on the role of parameters, interpretation of probability, and logic of statistical inference fit well with $\Lambda$ as measure of information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08475v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Vos</dc:creator>
    </item>
    <item>
      <title>Hypothesis Testing for High-Dimensional Matrix-Valued Data</title>
      <link>https://arxiv.org/abs/2412.07987</link>
      <description>arXiv:2412.07987v1 Announce Type: cross 
Abstract: This paper addresses hypothesis testing for the mean of matrix-valued data in high-dimensional settings. We investigate the minimum discrepancy test, originally proposed by Cragg (1997), which serves as a rank test for lower-dimensional matrices. We evaluate the performance of this test as the matrix dimensions increase proportionally with the sample size, and identify its limitations when matrix dimensions significantly exceed the sample size. To address these challenges, we propose a new test statistic tailored for high-dimensional matrix rank testing. The oracle version of this statistic is analyzed to highlight its theoretical properties. Additionally, we develop a novel approach for constructing a sparse singular value decomposition (SVD) estimator for singular vectors, providing a comprehensive examination of its theoretical aspects. Using the sparse SVD estimator, we explore the properties of the sample version of our proposed statistic. The paper concludes with simulation studies and two case studies involving surveillance video data, demonstrating the practical utility of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07987v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Cui, Danning Li, Runze Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Two-way Node Popularity Model for Directed and Bipartite Networks</title>
      <link>https://arxiv.org/abs/2412.08051</link>
      <description>arXiv:2412.08051v1 Announce Type: cross 
Abstract: There has been extensive research on community detection in directed and bipartite networks. However, these studies often fail to consider the popularity of nodes in different communities, which is a common phenomenon in real-world networks. To address this issue, we propose a new probabilistic framework called the Two-Way Node Popularity Model (TNPM). The TNPM also accommodates edges from different distributions within a general sub-Gaussian family. We introduce the Delete-One-Method (DOM) for model fitting and community structure identification, and provide a comprehensive theoretical analysis with novel technical skills dealing with sub-Gaussian generalization. Additionally, we propose the Two-Stage Divided Cosine Algorithm (TSDC) to handle large-scale networks more efficiently. Our proposed methods offer multi-folded advantages in terms of estimation accuracy and computational efficiency, as demonstrated through extensive numerical studies. We apply our methods to two real-world applications, uncovering interesting findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08051v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bing-Yi Jing, Ting Li, Jiangzhou Wang, Ya Wang</dc:creator>
    </item>
    <item>
      <title>Analysis of Age of Information for A Discrete-Time hybrid Dual-Queue System</title>
      <link>https://arxiv.org/abs/2412.08277</link>
      <description>arXiv:2412.08277v1 Announce Type: cross 
Abstract: Using multiple sensors to update the status process of interest is promising in improving the information freshness. The unordered arrival of status updates at the monitor end poses a significant challenge in analyzing the timeliness performance of parallel updating systems. This work investigates the age of information (AoI) of a discrete-time dual-sensor status updating system. Specifically, the status update is generated following the zero-waiting policy. The two sensors are modeled as a geometrically distributed service time queue and a deterministic service time queue in parallel. We derive the analytical expressions for the average AoI and peak AoI using the graphical analysis method. Moreover, the connection of average AoI between discrete-time and continuous-time systems is also explored. It is shown that the AoI result of the continuous-time system is a limit case of that of the corresponding discrete-time system. Hence, the AoI result of the discrete-time system is more general than the continuous one. Numerical results validate the effectiveness of our analysis and further show that randomness of service time contributes more AoI reduction than determinacy of service time in dual-queue systems in most cases, which is different from what is known about the single-queue system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08277v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengchuan Chen, Yi Qu, Nikolaos Pappas, Chaowei Tang, Min Wang, Tony Q. S. Quek</dc:creator>
    </item>
    <item>
      <title>Combinatorial Characterization of Exponential Families of Lumpable Stochastic Matrices</title>
      <link>https://arxiv.org/abs/2412.08400</link>
      <description>arXiv:2412.08400v1 Announce Type: cross 
Abstract: It is known that the set of lumpable Markov chains over a finite state space, with respect to a fixed lumping function, generally does not form an exponential family of stochastic matrices. In this work, we explore efficiently verifiable necessary and sufficient combinatorial conditions for families of lumpable transition matrices to form exponential families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08400v1</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shun Watanabe, Geoffrey Wolfer</dc:creator>
    </item>
    <item>
      <title>Heavy Tail Robust Estimation and Inference for Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2412.08458</link>
      <description>arXiv:2412.08458v1 Announce Type: cross 
Abstract: We study the probability tail properties of Inverse Probability Weighting (IPW) estimators of the Average Treatment Effect (ATE) when there is limited overlap between the covariate distributions of the treatment and control groups. Under unconfoundedness of treatment assignment conditional on covariates, such limited overlap is manifested in the propensity score for certain units being very close (but not equal) to 0 or 1. This renders IPW estimators possibly heavy tailed, and with a slower than sqrt(n) rate of convergence. Trimming or truncation is ultimately based on the covariates, ignoring important information about the inverse probability weighted random variable Z that identifies ATE by E[Z]= ATE. We propose a tail-trimmed IPW estimator whose performance is robust to limited overlap. In terms of the propensity score, which is generally unknown, we plug-in its parametric estimator in the infeasible Z, and then negligibly trim the resulting feasible Z adaptively by its large values. Trimming leads to bias if Z has an asymmetric distribution and an infinite variance, hence we estimate and remove the bias using important improvements on existing theory and methods. Our estimator sidesteps dimensionality, bias and poor correspondence properties associated with trimming by the covariates or propensity score. Monte Carlo experiments demonstrate that trimming by the covariates or the propensity score requires the removal of a substantial portion of the sample to render a low bias and close to normal estimator, while our estimator has low bias and mean-squared error, and is close to normal, based on the removal of very few sample extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08458v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan B. Hill, Saraswata Chaudhuri</dc:creator>
    </item>
    <item>
      <title>On the robustness of posterior means</title>
      <link>https://arxiv.org/abs/2303.08653</link>
      <description>arXiv:2303.08653v2 Announce Type: replace 
Abstract: Consider a normal location model $X \mid \theta \sim N(\theta, \sigma^2)$ with known $\sigma^2$. Suppose $\theta \sim G_0$, where the prior $G_0$ has zero mean and variance bounded by $V$. Let $G_1$ be a possibly misspecified prior with zero mean and variance bounded by $V$. We show that the squared error Bayes risk of the posterior mean under $G_1$ is bounded, subjected to an additional tail condition on $G_1$, uniformly over $G_0, G_1, \sigma^2 &gt; 0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.08653v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Optimal Rates for Functional Linear Regression with General Regularization</title>
      <link>https://arxiv.org/abs/2406.10005</link>
      <description>arXiv:2406.10005v2 Announce Type: replace 
Abstract: Functional linear regression is one of the fundamental and well-studied methods in functional data analysis. In this work, we investigate the functional linear regression model within the context of reproducing kernel Hilbert space by employing general spectral regularization to approximate the slope function with certain smoothness assumptions. We establish optimal convergence rates for estimation and prediction errors associated with the proposed method under a H\"{o}lder type source condition, which generalizes and sharpens all the known results in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10005v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naveen Gupta, S. Sivananthan, Bharath K. Sriperumbudur</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes When Estimation Precision Predicts Parameters</title>
      <link>https://arxiv.org/abs/2212.14444</link>
      <description>arXiv:2212.14444v5 Announce Type: replace-cross 
Abstract: Gaussian empirical Bayes methods usually maintain a precision independence assumption: The unknown parameters of interest are independent from the known standard errors of the estimates. This assumption is often theoretically questionable and empirically rejected. This paper proposes to model the conditional distribution of the parameter given the standard errors as a flexibly parametrized location-scale family of distributions, leading to a family of methods that we call CLOSE. The CLOSE framework unifies and generalizes several proposals under precision dependence. We argue that the most flexible member of the CLOSE family is a minimalist and computationally efficient default for accounting for precision dependence. We analyze this method and show that it is competitive in terms of the regret of subsequent decisions rules. Empirically, using CLOSE leads to sizable gains for selecting high-mobility Census tracts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14444v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Differentially Private Low-dimensional Synthetic Data from High-dimensional Datasets</title>
      <link>https://arxiv.org/abs/2305.17148</link>
      <description>arXiv:2305.17148v3 Announce Type: replace-cross 
Abstract: Differentially private synthetic data provide a powerful mechanism to enable data analysis while protecting sensitive information about individuals. However, when the data lie in a high-dimensional space, the accuracy of the synthetic data suffers from the curse of dimensionality. In this paper, we propose a differentially private algorithm to generate low-dimensional synthetic data efficiently from a high-dimensional dataset with a utility guarantee with respect to the Wasserstein distance. A key step of our algorithm is a private principal component analysis (PCA) procedure with a near-optimal accuracy bound that circumvents the curse of dimensionality. Unlike the standard perturbation analysis, our analysis of private PCA works without assuming the spectral gap for the covariance matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17148v3</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiyun He, Thomas Strohmer, Roman Vershynin, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Fast and Optimal Inference for Change Points in Piecewise Polynomials via Differencing</title>
      <link>https://arxiv.org/abs/2307.03639</link>
      <description>arXiv:2307.03639v3 Announce Type: replace-cross 
Abstract: We consider the problem of uncertainty quantification in change point regressions, where the signal can be piecewise polynomial of arbitrary but fixed degree. That is we seek disjoint intervals which, uniformly at a given confidence level, must each contain a change point location. We propose a procedure based on performing local tests at a number of scales and locations on a sparse grid, which adapts to the choice of grid in the sense that by choosing a sparser grid one explicitly pays a lower price for multiple testing. The procedure is fast as its computational complexity is always of the order $\mathcal{O} (n \log (n))$ where $n$ is the length of the data, and optimal in the sense that under certain mild conditions every change point is detected with high probability and the widths of the intervals returned match the mini-max localisation rates for the associated change point problem up to log factors. A detailed simulation study shows our procedure is competitive against state of the art algorithms for similar problems. Our procedure is implemented in the R package ChangePointInference which is available via https://github.com/gaviosha/ChangePointInference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03639v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shakeel Gavioli-Akilagun, Piotr Fryzlewicz</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment in Randomized Experiments Motivated by Higher-Order Influence Functions</title>
      <link>https://arxiv.org/abs/2411.08491</link>
      <description>arXiv:2411.08491v2 Announce Type: replace-cross 
Abstract: Higher-Order Influence Functions (HOIF), developed in a series of papers over the past twenty years, is a fundamental theoretical device for constructing rate-optimal causal-effect estimators from observational studies. However, the value of HOIF for analyzing well-conducted randomized controlled trials (RCTs) has not been explicitly explored. In the recent U.S. Food and Drug Administration (FDA) and European Medicines Agency (EMA) guidelines on the practice of covariate adjustment in analyzing RCTs, in addition to the simple, unadjusted difference-in-mean estimator, it was also recommended to report the estimator adjusting for baseline covariates via a simple parametric working model, such as a linear model. In this paper, we show that a HOIF-motivated estimator for the treatment-specific mean has significantly improved statistical properties compared to popular adjusted estimators in practice when the number of baseline covariates $p$ is relatively large compared to the sample size $n$. We also characterize the conditions under which the HOIF-motivated estimator improves upon the unadjusted one. Furthermore, we demonstrate that a novel debiased adjusted estimator proposed recently by Lu et al. is, in fact, another HOIF-motivated estimator in disguise. Numerical and empirical studies are conducted to corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08491v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihui Zhao, Xinbo Wang, Lin Liu, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>Dense ReLU Neural Networks for Temporal-spatial Model</title>
      <link>https://arxiv.org/abs/2411.09961</link>
      <description>arXiv:2411.09961v5 Announce Type: replace-cross 
Abstract: In this paper, we focus on fully connected deep neural networks utilizing the Rectified Linear Unit (ReLU) activation function for nonparametric estimation. We derive non-asymptotic bounds that lead to convergence rates, addressing both temporal and spatial dependence in the observed measurements. By accounting for dependencies across time and space, our models better reflect the complexities of real-world data, enhancing both predictive performance and theoretical robustness. We also tackle the curse of dimensionality by modeling the data on a manifold, exploring the intrinsic dimensionality of high-dimensional data. We broaden existing theoretical findings of temporal-spatial analysis by applying them to neural networks in more general contexts and demonstrate that our proof techniques are effective for models with short-range dependence. Our empirical simulations across various synthetic response functions underscore the superior performance of our method, outperforming established approaches in the existing literature. These findings provide valuable insights into the strong capabilities of dense neural networks for temporal-spatial modeling across a broad range of function classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09961v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Zhang, Carlos Misael Madrid Padilla, Xiaokai Luo, Daren Wang, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
  </channel>
</rss>
