<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Mar 2024 04:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimal sub-Gaussian variance proxy for truncated Gaussian and exponential random variables</title>
      <link>https://arxiv.org/abs/2403.08628</link>
      <description>arXiv:2403.08628v1 Announce Type: new 
Abstract: This paper establishes the optimal sub-Gaussian variance proxy for truncated Gaussian and truncated exponential random variables. The proofs rely on first characterizing the optimal variance proxy as the unique solution to a set of two equations and then observing that for these two truncated distributions, one may find explicit solutions to this set of equations. Moreover, we establish the conditions under which the optimal variance proxy coincides with the variance, thereby characterizing the strict sub-Gaussianity of the truncated random variables. Specifically, we demonstrate that truncated Gaussian variables exhibit strict sub-Gaussian behavior if and only if they are symmetric, meaning their truncation is symmetric with respect to the mean. Conversely, truncated exponential variables are shown to never exhibit strict sub-Gaussian properties. These findings contribute to the understanding of these prevalent probability distributions in statistics and machine learning, providing a valuable foundation for improved and optimal modeling and decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08628v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Barreto, Olivier Marchal, Julyan Arbel</dc:creator>
    </item>
    <item>
      <title>A general framework for cyclic and fine-tuned causal models and their compatibility with space-time</title>
      <link>https://arxiv.org/abs/2109.12128</link>
      <description>arXiv:2109.12128v3 Announce Type: cross 
Abstract: Causal modelling is a tool for generating causal explanations of observed correlations and has led to a deeper understanding of correlations in quantum networks. Existing frameworks for quantum causality tend to focus on acyclic causal structures that are not fine-tuned i.e., where causal connections between variables necessarily create correlations between them. However, fine-tuned causal models (which permit causation without correlation) play a crucial role in cryptography, and cyclic causal models can be used to model physical processes involving feedback and may also be relevant in exotic solutions of general relativity. Here we develop a causal modelling framework capable of dealing with these general scenarios. The key feature of our framework is that it allows operational and relativistic notions of causality to be independently defined and for connections between them to be established in a theory-independent manner. The framework first gives an operational way to study causation that allows for cyclic, fine-tuned and non-classical causal influences. We then consider how a causal model can be embedded in a space-time structure (modelled as a partial order) and propose a compatibility condition for ensuring that the embedded causal model does not allow signalling outside the space-time future. We identify several distinct classes of causal loops that can arise in our framework, showing that compatibility with a space-time can rule out only some of them. We discuss conditions for preventing superluminal signalling within arbitrary (and possibly cyclic) causal structures and consider models of causation in post-quantum theories admitting so-called jamming correlations. Finally, this work introduces the concept of a "higher-order affects relation", which is useful for causal discovery in fined-tuned causal models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.12128v3</guid>
      <category>quant-ph</category>
      <category>gr-qc</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevA.106.032204</arxiv:DOI>
      <arxiv:journal_reference>Physical Review A, 106, 032204 (2022)</arxiv:journal_reference>
      <dc:creator>V. Vilasini, Roger Colbeck</dc:creator>
    </item>
    <item>
      <title>Impossibility of superluminal signalling in Minkowski space-time does not rule out causal loops</title>
      <link>https://arxiv.org/abs/2206.12887</link>
      <description>arXiv:2206.12887v2 Announce Type: cross 
Abstract: Causality is fundamental to science, but it appears in several different forms. One is relativistic causality, which is tied to a space-time structure and forbids signalling outside the future. A second is an operational notion of causation that considers the flow of information between physical systems and interventions on them. In [Vilasini and Colbeck, Phys. Rev. A. 106, 032204 (2022)], we propose a framework for characterising when a causal model can coexist with relativistic principles such as no superluminal signalling, while allowing for cyclic and non-classical causal influences and the possibility of causation without signalling. In a theory without superluminal causation, both superluminal signalling and causal loops are not possible in Minkowski space-time. Here we demonstrate that if we only forbid superluminal signalling, superluminal causation remains possible and show the mathematical possibility of causal loops that can be embedded in a Minkowski space-time without leading to superluminal signalling. The existence of such loops in the given space-time could in principle be operationally verified using interventions. This establishes that the physical principle of no superluminal signalling is not by itself sufficient to rule out causal loops between Minkowski space-time events. Interestingly, the conditions required to rule out causal loops in a space-time depend on the dimension. Whether such loops are possible in three spatial dimensions remains an important open question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.12887v2</guid>
      <category>quant-ph</category>
      <category>gr-qc</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevLett.129.110401</arxiv:DOI>
      <arxiv:journal_reference>Physical Review Letters, 129, 110401 (2022)</arxiv:journal_reference>
      <dc:creator>V. Vilasini, Roger Colbeck</dc:creator>
    </item>
    <item>
      <title>Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime</title>
      <link>https://arxiv.org/abs/2403.08160</link>
      <description>arXiv:2403.08160v1 Announce Type: cross 
Abstract: Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data. It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities. This leaves open the question of understanding the impact of parametrization on the performance of these models. How does model complexity and generalization depend on the number of parameters $p$? How should we choose $p$ relative to the sample size $n$ to achieve optimal test error?
  In this paper, we investigate the example of random feature ridge regression (RFRR). This model can be seen either as a finite-rank approximation to kernel ridge regression (KRR), or as a simplified model for neural networks trained in the so-called lazy regime. We consider covariates uniformly distributed on the $d$-dimensional sphere and compute sharp asymptotics for the RFRR test error in the high-dimensional polynomial scaling, where $p,n,d \to \infty$ while $p/ d^{\kappa_1}$ and $n / d^{\kappa_2}$ stay constant, for all $\kappa_1 , \kappa_2 \in \mathbb{R}_{&gt;0}$. These asymptotics precisely characterize the impact of the number of random features and regularization parameter on the test performance. In particular, RFRR exhibits an intuitive trade-off between approximation and generalization power. For $n = o(p)$, the sample size $n$ is the bottleneck and RFRR achieves the same performance as KRR (which is equivalent to taking $p = \infty$). On the other hand, if $p = o(n)$, the number of random features $p$ is the limiting factor and RFRR test error matches the approximation error of the random feature model class (akin to taking $n = \infty$). Finally, a double descent appears at $n= p$, a phenomenon that was previously only characterized in the linear scaling $\kappa_1 = \kappa_2 = 1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08160v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Hu, Yue M. Lu, Theodor Misiakiewicz</dc:creator>
    </item>
    <item>
      <title>On the large deviation principle for Metropolis-Hastings Markov Chains: the Lyapunov function condition and examples</title>
      <link>https://arxiv.org/abs/2403.08691</link>
      <description>arXiv:2403.08691v1 Announce Type: cross 
Abstract: With an aim to analyse the performance of Markov chain Monte Carlo (MCMC) methods, in our recent work we derive a large deviation principle (LDP) for the empirical measures of Metropolis-Hastings (MH) chains on a continuous state space. One of the (sufficient) assumptions for the LDP involves the existence of a particular type of Lyapunov function, and it was left as an open question whether or not such a function exists for specific choices of MH samplers. In this paper we analyse the properties of such Lyapunov functions and investigate their existence for some of the most popular choices of MCMC samplers built on MH dynamics: Independent Metropolis Hastings, Random Walk Metropolis, and the Metropolis-adjusted Langevin algorithm. We establish under what conditions such a Lyapunov function exists, and from this obtain LDPs for some instances of the MCMC algorithms under consideration. To the best of our knowledge, these are the first large deviation results for empirical measures associated with Metropolis-Hastings chains for specific choices of proposal and target distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08691v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federica Milinanni, Pierre Nyquist</dc:creator>
    </item>
    <item>
      <title>On the existence of Firth's modified estimates in logistic regression models</title>
      <link>https://arxiv.org/abs/2304.07484</link>
      <description>arXiv:2304.07484v2 Announce Type: replace 
Abstract: In logistic regression modeling, Firth's modified estimator is widely used to address the issue of data separation, which results in the nonexistence of the maximum likelihood estimate. Firth's modified estimator can be formulated as a penalized maximum likelihood estimator in which Jeffreys' prior is adopted as the penalty term. Despite its widespread use in practice, the formal verification of the corresponding estimate's existence has not been established. In this study, we establish the existence theorem of Firth's modified estimate in binomial logistic regression models, assuming only the full column rankness of the design matrix. We also discuss multinomial logistic regression models. Unlike the binomial regression case, we show through an example that the Jeffreys-prior penalty term does not necessarily diverge to negative infinity as the parameter diverges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07484v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mitsunori Ogawa, Yui Tomo</dc:creator>
    </item>
    <item>
      <title>Bayesian Analysis for Over-parameterized Linear Model without Sparsity</title>
      <link>https://arxiv.org/abs/2305.15754</link>
      <description>arXiv:2305.15754v2 Announce Type: replace 
Abstract: In the field of high-dimensional Bayesian statistics, a plethora of methodologies have been developed, including various prior distributions that result in parameter sparsity. However, such priors exhibit limitations in handling the spectral eigenvector structure of data, rendering estimations less effective for analyzing the over-parameterized models (high-dimensional linear models that do not assume sparsity) developed in recent years. This study introduces a Bayesian approach that employs a prior distribution dependent on the eigenvectors of data covariance matrices without inducing parameter sparsity. We also provide contraction rates of the derived posterior estimation and develop a truncated Gaussian approximation of the posterior distribution. The former demonstrates the efficiency of posterior estimation, whereas the latter facilitates the uncertainty quantification of parameters via a Bernstein--von Mises-type approach. These findings suggest that Bayesian methods capable of handling data spectra and estimating non-sparse high-dimensional parameters are feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.15754v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Increasing dimension asymptotics for two-way crossed mixed effect models</title>
      <link>https://arxiv.org/abs/2401.06446</link>
      <description>arXiv:2401.06446v2 Announce Type: replace 
Abstract: This paper presents asymptotic results for the maximum likelihood and restricted maximum likelihood (REML) estimators within a two-way crossed mixed effect model as the sizes of the rows, columns, and cells tend to infinity. Under very mild conditions which do not require the assumption of normality, the estimators are proven to be asymptotically normal, possessing a structured covariance matrix. The growth rate for the number of rows, columns, and cells is unrestricted, whether considered pairwise or collectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06446v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Lyu, S. A. Sisson, A. H. Welsh</dc:creator>
    </item>
    <item>
      <title>Cyclical Long Memory: Decoupling, Modulation, and Modeling</title>
      <link>https://arxiv.org/abs/2403.07170</link>
      <description>arXiv:2403.07170v2 Announce Type: replace 
Abstract: A new model for general cyclical long memory is introduced, by means of random modulation of certain bivariate long memory time series. This construction essentially decouples the two key features of cyclical long memory: quasi-periodicity and long-term persistence. It further allows for a general cyclical phase in cyclical long memory time series. Several choices for suitable bivariate long memory series are discussed, including a parametric fractionally integrated vector ARMA model. The parametric models introduced in this work have explicit autocovariance functions that can be used readily in simulation, estimation, and other tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07170v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefanos Kechagias, Vladas Pipiras, Pavlos Zoubouloglou</dc:creator>
    </item>
    <item>
      <title>A path-norm toolkit for modern networks: consequences, promises and challenges</title>
      <link>https://arxiv.org/abs/2310.01225</link>
      <description>arXiv:2310.01225v4 Announce Type: replace-cross 
Abstract: This work introduces the first toolkit around path-norms that fully encompasses general DAG ReLU networks with biases, skip connections and any operation based on the extraction of order statistics: max pooling, GroupSort etc. This toolkit notably allows us to establish generalization bounds for modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on layered fully-connected networks compared to the product of operator norms, another complexity measure most commonly used.
  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01225v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Gonon, Nicolas Brisebarre, Elisa Riccietti, R\'emi Gribonval</dc:creator>
    </item>
  </channel>
</rss>
