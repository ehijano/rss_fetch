<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 04:03:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Impact of Packetization on Network Calculus Analysis</title>
      <link>https://arxiv.org/abs/2509.17028</link>
      <description>arXiv:2509.17028v1 Announce Type: cross 
Abstract: For packet-switched networks, when the packetization effect is overlooked, network calculus analysis can produce faulty results. To exemplify, network calculus analysis is applied in this paper to two basic systems that are fundamental or default settings in Time-Sensitive Networking (TSN) and Deterministic Networking (DetNet). Through counterexamples, it is revealed that for the two fundamental settings, some widely adopted, network calculus-based service characterization results, known as service curves, which ignore packetization, are faulty. In addition, for performance bounds derived from the faulty service curves, it is shown that the validity of the bounds can be arguable. In particular, the output bound, backlog bound and concatenation service curve results are shown to be also faulty: counterexamples can be constructed. By factoring the packetization effect directly into the service models, corrected service curves and performance bounds are derived for the two basic systems. These results remind that special care is needed when applying network calculus analysis to packet-switched networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17028v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yming Jiang</dc:creator>
    </item>
    <item>
      <title>On the Design of Capacity-Achieving Distributions for Discrete-Time Poisson Channel with Low-Precision ADCs</title>
      <link>https://arxiv.org/abs/2509.17483</link>
      <description>arXiv:2509.17483v1 Announce Type: cross 
Abstract: This paper investigates the design of the capacity-achieving input distribution for the discrete-time Poisson channel (DTPC) under dark current effects with low-precision analog-to-digital converters (ADCs). This study introduces an efficient optimization algorithm that integrates the Newton-Raphson and Blahut-Arimoto (BA) methods to determine the capacity-achieving input distribution and the corresponding amplitudes of input mass points for the DTPC, subject to both peak and average power constraints. Additionally, the Karush-Kuhn-Tucker (KKT) conditions are established to provide necessary and sufficient conditions for the optimality of the obtained capacity-achieving distribution. Simulation results illustrate that the proposed algorithm attains $72\%$ and $83\%$ of the theoretical capacity at 5 dB for 1-bit and 2-bit quantized DTPC, respectively. Furthermore, for a finite-precision quantized DTPC (i.e., ${\log _2}K$ bits), the capacity can be achieved by a non-uniform discrete input distribution with support for $K$ mass points, under the given power constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17483v1</guid>
      <category>eess.SP</category>
      <category>cs.PF</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianqian Li, Lintao Li, Lixiang Liu, Lei Yang, Caihong Gong, Hua Li, Shiya Hao, Xiaoming Dai</dc:creator>
    </item>
    <item>
      <title>CEBench: A Benchmarking Toolkit for the Cost-Effectiveness of LLM Pipelines</title>
      <link>https://arxiv.org/abs/2407.12797</link>
      <description>arXiv:2407.12797v2 Announce Type: replace 
Abstract: Online Large Language Model (LLM) services such as ChatGPT and Claude 3 have transformed business operations and academic research by effortlessly enabling new opportunities. However, due to data-sharing restrictions, sectors such as healthcare and finance prefer to deploy local LLM applications using costly hardware resources. This scenario requires a balance between the effectiveness advantages of LLMs and significant financial burdens. Additionally, the rapid evolution of models increases the frequency and redundancy of benchmarking efforts. Existing benchmarking toolkits, which typically focus on effectiveness, often overlook economic considerations, making their findings less applicable to practical scenarios. To address these challenges, we introduce CEBench, an open-source toolkit specifically designed for multi-objective benchmarking that focuses on the critical trade-offs between expenditure and effectiveness required for LLM deployments. CEBench allows for easy modifications through configuration files, enabling stakeholders to effectively assess and optimize these trade-offs. This strategic capability supports crucial decision-making processes aimed at maximizing effectiveness while minimizing cost impacts. By streamlining the evaluation process and emphasizing cost-effectiveness, CEBench seeks to facilitate the development of economically viable AI solutions across various industries and research fields. The code and demonstration are available in https://github.com/amademicnoboday12/CEBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12797v2</guid>
      <category>cs.PF</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenbo Sun, Jiaqi Wang, Qiming Guo, Ziyu Li, Wenlu Wang, Rihan Hai</dc:creator>
    </item>
    <item>
      <title>Is Sparse Matrix Reordering Effective for Sparse Matrix-Vector Multiplication?</title>
      <link>https://arxiv.org/abs/2506.10356</link>
      <description>arXiv:2506.10356v2 Announce Type: replace-cross 
Abstract: This work evaluates the impact of sparse matrix reordering on the performance of sparse matrix-vector multiplication across different multicore CPU platforms. Reordering can significantly enhance performance by optimizing the non-zero element patterns to reduce total data movement and improve the load-balancing. We examine how these gains vary over different CPUs for different reordering strategies, focusing on both sequential and parallel execution. We address multiple aspects, including appropriate measurement methodology, comparison across different kinds of reordering strategies, consistency across machines, and impact of load imbalance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10356v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omid Asudeh, Sina Mahdipour Saravani, Gerald Sabin, Fabrice Rastello, P Sadayappan</dc:creator>
    </item>
    <item>
      <title>FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline</title>
      <link>https://arxiv.org/abs/2507.10367</link>
      <description>arXiv:2507.10367v2 Announce Type: replace-cross 
Abstract: Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\times$ throughput for small file read/write and up to 12.81$\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10367v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingwei Xu, Junbin Kang, Mingkai Dong, Mingyu Liu, Lu Zhang, Shaohong Guo, Ziyan Qiu, Mingzhen You, Ziyi Tian, Anqi Yu, Tianhong Ding, Xinwei Hu, Haibo Chen</dc:creator>
    </item>
  </channel>
</rss>
