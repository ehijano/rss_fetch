<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 May 2025 04:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Linking Potentially Misclassified Healthy Food Access to Diabetes Prevalence</title>
      <link>https://arxiv.org/abs/2505.01465</link>
      <description>arXiv:2505.01465v1 Announce Type: new 
Abstract: Access to healthy food is key to maintaining a healthy lifestyle and can be quantified by the distance to the nearest grocery store. However, calculating this distance forces a trade-off between cost and correctness. Accurate route-based distances following passable roads are cost-prohibitive, while simple straight-line distances ignoring infrastructure and natural barriers are accessible yet error-prone. Categorizing low-access neighborhoods based on these straight-line distances induces misclassification and introduces bias into standard regression models estimating the relationship between disease prevalence and access. Yet, fully observing the more accurate, route-based food access measure is often impossible, which induces a missing data problem. We combat bias and address missingness with a new maximum likelihood estimator for Poisson regression with a binary, misclassified exposure (access to healthy food within some threshold), where the misclassification may depend on additional error-free covariates. In simulations, we show the consequence of ignoring the misclassification (bias) and how the proposed estimator corrects for bias while preserving more statistical efficiency than the complete case analysis (i.e., deleting observations with missing data). Finally, we apply our estimator to model the relationship between census tract diabetes prevalence and access to healthy food in northwestern North Carolina.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01465v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashley E. Mullan, P. D. Anh Nguyen, Sarah C. Lotspeich</dc:creator>
    </item>
    <item>
      <title>Bayesian non-parametric survival estimation: stochastic hyperparameter sequences and distribution splicing</title>
      <link>https://arxiv.org/abs/2505.01604</link>
      <description>arXiv:2505.01604v1 Announce Type: new 
Abstract: A Bayesian non-parametric framework for studying time-to-event data is proposed, where the prior distribution is allowed to depend on an additional random source, and may update with the sample size. Such scenarios are natural, for instance, when considering empirical Bayes techniques or dynamic expert information. In this context, a natural stochastic class for studying the cumulative hazard function are conditionally inhomogeneous independent increment processes with non-decreasing sample paths, also known as mixed time-inhomogeneous subordinators or mixed non-decreasing additive processes.
  The asymptotic behaviour is studied by showing that Bayesian consistency and Bernstein--von~Mises theorems may be recovered under suitable conditions on the asymptotic negligibility of the stochastic prior sequences. The non-asymptotic behaviour of the posterior is also considered. Namely, upon conditioning, an efficient and exact simulation algorithm for the paths of the Beta L\'evy process is provided. As a natural application, it is shown how the model can provide an appropriate definition of non-parametric spliced models. Spliced models target data where an accurate global description of both the body and tail of the distribution is desirable. The Bayesian non-parametric nature of the proposed estimators can offer conceptual and numerical alternatives to their parametric counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01604v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Jorge Gonz\'alez C\'azares</dc:creator>
    </item>
    <item>
      <title>Unified and Simple Sample Size Calculations for Individual or Cluster Randomized Trials with Skewed or Ordinal Outcomes</title>
      <link>https://arxiv.org/abs/2505.01640</link>
      <description>arXiv:2505.01640v1 Announce Type: new 
Abstract: Sample size calculations can be challenging with skewed continuous outcomes in randomized controlled trials (RCTs). Standard t-test-based calculations may require data transformation, which may be difficult before data collection. Calculations based on individual and clustered Wilcoxon rank-sum tests have been proposed as alternatives, but these calculations assume no ties in continuous outcomes, and clustered Wilcoxon rank-sum tests perform poorly with heterogeneous cluster sizes. Recent work has shown that continuous outcomes can be analyzed in a robust manner using ordinal cumulative probability models. Analogously, sample size calculations for ordinal outcomes can be applied as a robust design strategy for continuous outcomes. We show that Whitehead's sample size calculations for independent ordinal outcomes can be easily extended to continuous outcomes. We extend these calculations to cluster RCTs using a design effect incorporating the rank intraclass correlation coefficient. Therefore, we provide a unifying and simple approach for designing individual and cluster RCTs that makes minimal assumptions on the distribution of the still-to-be-collected outcome. We conduct simulations to evaluate our approach's performance and illustrate its application in multiple RCTs: an individual RCT with skewed continuous outcomes, a cluster RCT with skewed continuous outcomes, and a non-inferiority cluster RCT with an irregularly distributed count outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01640v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengxin Tu, Chun Li, Caroline De Schacht, Carolyn M. Audet, Aminu Taura Abdullahi, Edwin Trevathan, Bryan E. Shepherd</dc:creator>
    </item>
    <item>
      <title>An unbiased estimator of a novel extended Gini index for gamma distributed populations</title>
      <link>https://arxiv.org/abs/2505.01659</link>
      <description>arXiv:2505.01659v1 Announce Type: new 
Abstract: In this paper, we introduce a novel flexible Gini index, referred to as the extended Gini index, which is defined through ordered differences between the $j$th and $k$th order statistics within subsamples of size $m$, for indices satisfying $1 \leqslant j \leqslant k \leqslant m$. We derive a closed-form expression for the expectation of the corresponding estimator under the gamma distribution and prove its unbiasedness, thereby extending prior findings by \cite{Deltas2003}, \cite{Baydil2025}, and \cite{Vila2025}. A Monte Carlo simulation illustrates the estimator's finite-sample unbiasedness. A real data set on gross domestic product (GDP) per capita is analyzed to illustrate the proposed measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01659v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Hettmansperger-Randles Estimator and its Applications</title>
      <link>https://arxiv.org/abs/2505.01669</link>
      <description>arXiv:2505.01669v1 Announce Type: new 
Abstract: The classic Hettmansperger-Randles Estimator has found extensive use in robust statistical inference. However, it cannot be directly applied to high-dimensional data. In this paper, we propose a high-dimensional Hettmansperger-Randles Estimator for the location parameter and scatter matrix of elliptical distributions in high-dimensional scenarios. Subsequently, we apply these estimators to two prominent problems: the one-sample location test problem and quadratic discriminant analysis. We discover that the corresponding new methods exhibit high effectiveness across a broad range of distributions. Both simulation studies and real-data applications further illustrate the superiority of the newly proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01669v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guowei Yan, Long Feng, Xiaoxu Zhang</dc:creator>
    </item>
    <item>
      <title>General Form Moment-based Estimator of Weibull, Gamma, and Log-normal Distributions</title>
      <link>https://arxiv.org/abs/2505.01911</link>
      <description>arXiv:2505.01911v1 Announce Type: new 
Abstract: This paper presents a unified and novel estimation framework for the Weibull, Gamma, and Log-normal distributions based on arbitrary-order moment pairs. Traditional estimation techniques, such as Maximum Likelihood Estimation (MLE) and the classical Method of Moments (MoM), are often restricted to fixed-order moment inputs and may require specific distributional assumptions or optimization procedures. In contrast, our general-form moment-based estimator allows the use of any two empirical moments, such as mean and variance, or higher-order combinations, to compute the underlying distribution parameters. For each distribution, we develop provably convergent numerical algorithms that guarantee unique solutions within a bounded parameter space and provide estimates within a user-defined error tolerance. The proposed framework generalizes existing estimation methods and offers greater flexibility and robustness for statistical modeling in diverse application domains. This is, to our knowledge, the first work that formalizes such a general estimation structure and provides theoretical guarantees across these three foundational distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01911v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kang Liu</dc:creator>
    </item>
    <item>
      <title>A copula-based rank histogram ensemble filter</title>
      <link>https://arxiv.org/abs/2505.01918</link>
      <description>arXiv:2505.01918v1 Announce Type: new 
Abstract: Serial ensemble filters implement triangular probability transport maps to reduce high-dimensional inference problems to sequences of state-by-state univariate inference problems. The univariate inference problems are solved by sampling posterior probability densities obtained by combining constructed prior densities with observational likelihoods according to Bayes' rule. Many serial filters in the literature focus on representing the marginal posterior densities of each state. However, rigorously capturing the conditional dependencies between the different univariate inferences is crucial to correctly sampling multidimensional posteriors. This work proposes a new serial ensemble filter, called the copula rank histogram filter (CoRHF), that seeks to capture the conditional dependency structure between variables via empirical copula estimates; these estimates are used to rigorously implement the triangular (state-by-state univariate) Bayesian inference. The success of the CoRHF is demonstrated on two-dimensional examples and the Lorenz '63 problem. A practical extension to the high-dimensional setting is developed by localizing the empirical copula estimation, and is demonstrated on the Lorenz '96 problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01918v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amit N. Subrahmanya, Julie Bessac, Andrey A. Popov, Adrian Sandu</dc:creator>
    </item>
    <item>
      <title>Association and Independence Test for Random Objects</title>
      <link>https://arxiv.org/abs/2505.01983</link>
      <description>arXiv:2505.01983v1 Announce Type: new 
Abstract: We develop a unified framework for testing independence and quantifying association between random objects that are located in general metric spaces. Special cases include functional and high-dimensional data as well as networks, covariance matrices and data on Riemannian manifolds, among other metric space-valued data. A key concept is the profile association, a measure based on distance profiles that intrinsically characterize the distributions of random objects in metric spaces. We rigorously establish a connection between the Hoeffding D statistic and the profile association and derive a permutation test with theoretical guarantees for consistency and power under alternatives to the null hypothesis of independence/no association. We extend this framework to the conditional setting, where the independence between random objects given a Euclidean predictor is of interest. In simulations across various metric spaces, the proposed profile independence test is found to outperform existing approaches. The practical utility of this framework is demonstrated with applications to brain connectivity networks derived from magnetic resonance imaging and age-at-death distributions for males and females obtained from human mortality data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01983v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Zhou, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Enhancing Data Completeness in Time Series: Imputation Strategies for Missing Data Using Significant Periodically Correlated Components</title>
      <link>https://arxiv.org/abs/2505.02008</link>
      <description>arXiv:2505.02008v1 Announce Type: new 
Abstract: Missing data is a pervasive issue in statistical analyses, affecting the reliability and validity of research across diverse scientific disciplines. Failure to adequately address missing data can lead to biased estimates and consequently flawed conclusions. In this study, we present a novel imputation method that leverages significant annual components identified through the Variable Bandpass Periodic Block Bootstrap (VBPBB) technique to improve the accuracy and integrity of imputed datasets. Our approach enhances the completeness of datasets by systematically incorporating periodic components into the imputation process, thereby preserving key statistical properties, including mean and variance. We conduct a comparative analysis of various imputation techniques, demonstrating that our VBPBB-enhanced approach consistently outperforms traditional methods in maintaining the statistical structure of the original dataset. The results of our study underscore the robustness and reliability of VBPBB-enhanced imputation, highlighting its potential for broader application in real-world datasets, particularly in fields such as healthcare, where data quality is critical. These findings provide a robust framework for improving the accuracy of imputed datasets, offering substantial implications for advancing research methodologies across scientific and analytical contexts. Our method not only impute missing data but also ensures that the imputed values align with underlying temporal patterns, thereby facilitating more accurate and reliable conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02008v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmaa Ahmad, Eric J Rose, Michael Roy, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Online Functional Principal Component Analysis on a Multidimensional Domain</title>
      <link>https://arxiv.org/abs/2505.02131</link>
      <description>arXiv:2505.02131v1 Announce Type: new 
Abstract: Multidimensional functional data streams arise in diverse scientific fields, yet their analysis poses significant challenges. We propose a novel online framework for functional principal component analysis that enables efficient and scalable modeling of such data. Our method represents functional principal components using tensor product splines, enforcing smoothness and orthonormality through a penalized framework on a Stiefel manifold. An efficient Riemannian stochastic gradient descent algorithm is developed, with extensions inspired by adaptive moment estimation and averaging techniques to accelerate convergence. Additionally, a dynamic tuning strategy for smoothing parameter selection is developed based on a rolling averaged block validation score that adapts to the streaming nature of the data. Extensive simulations and real-world applications demonstrate the flexibility and effectiveness of this framework for analyzing multidimensional functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02131v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muye Nanshan, Nan Zhang, Jiguo Cao</dc:creator>
    </item>
    <item>
      <title>A Robust Monotonic Single-Index Model for Skewed and Heavy-Tailed Data: A Deep Neural Network Approach Applied to Periodontal Studies</title>
      <link>https://arxiv.org/abs/2505.02153</link>
      <description>arXiv:2505.02153v1 Announce Type: new 
Abstract: Periodontal pocket depth is a widely used biomarker for diagnosing risk of periodontal disease. However, pocket depth typically exhibits skewness and heavy-tailedness, and its relationship with clinical risk factors is often nonlinear. Motivated by periodontal studies, this paper develops a robust single-index modal regression framework for analyzing skewed and heavy-tailed data. Our method has the following novel features: (1) a flexible two-piece scale Student-$t$ error distribution that generalizes both normal and two-piece scale normal distributions; (2) a deep neural network with guaranteed monotonicity constraints to estimate the unknown single-index function; and (3) theoretical guarantees, including model identifiability and a universal approximation theorem. Our single-index model combines the flexibility of neural networks and the two-piece scale Student-$t$ distribution, delivering robust mode-based estimation that is resistant to outliers, while retaining clinical interpretability through parametric index coefficients. We demonstrate the performance of our method through simulation studies and an application to periodontal disease data from the HealthPartners Institute of Minnesota. The proposed methodology is implemented in the \textsf{R} package \href{https://doi.org/10.32614/CRAN.package.DNNSIM}{\textsc{DNNSIM}}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02153v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qingyang Liu, Shijie Wang, Ray Bai, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Training-Set Conditionally Valid Prediction Sets with Right-Censored Data</title>
      <link>https://arxiv.org/abs/2505.02213</link>
      <description>arXiv:2505.02213v1 Announce Type: new 
Abstract: Uncertainty quantification of prediction models through prediction sets is increasingly popular and successful, but most existing methods rely on directly observing the outcome and do not appropriately handle censored outcomes, such as time-to-event outcomes. Recent works have introduced distribution-free conformal prediction methods that construct predictive intervals for right-censored outcomes with marginal coverage guarantees. However, these methods typically assume a restrictive Type I censoring framework, in which censoring times are all observed. In this paper, we leverage a semiparametric one-step estimation framework and propose a novel approach for constructing predictive lower bounds on survival times with training-set conditional validity under right-censoring, where censoring times may be unobserved when the survival time is observed. With slight modification, our method can also provide predictive lower bounds with marginal guarantees. Through extensive simulations and a real-world application dataset tracking users' active times on a mobile application, we demonstrate the effectiveness and practicality of our approach. Compared to existing methods, our technique shows superior efficiency and robustness to model misspecifications, representing a significant advancement in the toolbox for reliable machine learning for time-to-event outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02213v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenwen Si, Hongxiang Qiu</dc:creator>
    </item>
    <item>
      <title>Statistical methods for clustered competing risk data when the event types are only available in a training dataset</title>
      <link>https://arxiv.org/abs/2505.02217</link>
      <description>arXiv:2505.02217v1 Announce Type: new 
Abstract: We develop methods to analyze clustered competing risks data when the event types are only available in a training dataset and are missing in the main study. We propose to estimate the exposure effects through the cause-specific proportional hazards frailty model where random effects are introduced into the model to account for the within-cluster correlation. We propose a weighted penalized partial likelihood method where the weights represent the probabilities of the occurrence of events, and the weights can be obtained by fitting a classification model for the event types on the training dataset. Alternatively, we propose an imputation approach where the missing event types are imputed based on the predictions from the classification model. We derive the analytical variances, and evaluate the finite sample properties of our methods in an extensive simulation study. As an illustrative example, we apply our methods to estimate the associations between tinnitus and metabolic, sensory and metabolic+sensory hearing loss in the Conservation of Hearing Study Audiology Assessment Arm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02217v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujie Wu, Molin Wang</dc:creator>
    </item>
    <item>
      <title>Statistical method for pooling categorical biomarkers from multi-center matched/nested case-control studies</title>
      <link>https://arxiv.org/abs/2505.02220</link>
      <description>arXiv:2505.02220v1 Announce Type: new 
Abstract: Pooled analyses that aggregate data from multiple studies are becoming increasingly common in collaborative epidemiologic research in order to increase the size and diversity of the study population. However, biomarker measurements from different studies are subject to systematic measurement errors and directly pooling them for analyses may lead to biased estimates of the regression parameters. Therefore, study-specific calibration processes must be incorporated in the statistical analyses to address between-study/assay/laboratory variability in the biomarker measurements. We propose a likelihood-based method to evaluate biomarker-disease relationships for categorical biomarkers in matched/nested case-control studies. To account for the additional uncertainties from the calibration processes, we propose a sandwich variance estimator to obtain valid asymptotic variances of the estimated regression parameters. Extensive simulation studies with varying sample sizes and biomarker-disease associations are used to evaluate the finite sample performance of our proposed methods. As an illustration, we apply the methods to a vitamin D pooling project of colorectal cancer to evaluate the effect of categorical vitamin D levels on colorectal cancer risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02220v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujie Wu, Xiao Wu, Mitchell H. Gail, Regina G. Ziegler, Stephanie A. Smith-Warner, Molin Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Federated Cause-of-Death Classification and Quantification Under Distribution Shift</title>
      <link>https://arxiv.org/abs/2505.02257</link>
      <description>arXiv:2505.02257v1 Announce Type: new 
Abstract: In regions lacking medically certified causes of death, verbal autopsy (VA) is a critical and widely used tool to ascertain the cause of death through interviews with caregivers. Data collected by VAs are often analyzed using probabilistic algorithms. The performance of these algorithms often degrades due to distributional shift across populations. Most existing VA algorithms rely on centralized training, requiring full access to training data for joint modeling. This is often infeasible due to privacy and logistical constraints. In this paper, we propose a novel Bayesian Federated Learning (BFL) framework that avoids data sharing across multiple training sources. Our method enables reliable individual-level cause-of-death classification and population-level quantification of cause-specific mortality fractions (CSMFs), in a target domain with limited or no local labeled data. The proposed framework is modular, computationally efficient, and compatible with a wide range of existing VA algorithms as candidate models, facilitating flexible deployment in real-world mortality surveillance systems. We validate the performance of BFL through extensive experiments on two real-world VA datasets under varying levels of distribution shift. Our results show that BFL significantly outperforms the base models built on a single domain and achieves comparable or better performance compared to joint modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02257v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Zhu, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for cluster-randomized trials with multivariate outcomes subject to both truncation by death and missingness</title>
      <link>https://arxiv.org/abs/2505.02310</link>
      <description>arXiv:2505.02310v1 Announce Type: new 
Abstract: Cluster-randomized trials (CRTs) on fragile populations frequently encounter complex attrition problems where the reasons for missing outcomes can be heterogeneous, with participants who are known alive, known to have died, or with unknown survival status, and with complex and distinct missing data mechanisms for each group. Although existing methods have been developed to address death truncation in CRTs, no existing methods can jointly accommodate participants who drop out for reasons unrelated to mortality or serious illnesses, or those with an unknown survival status. This paper proposes a Bayesian framework for estimating survivor average causal effects in CRTs while accounting for different types of missingness. Our approach uses a multivariate outcome that jointly estimates the causal effects, and in the posterior estimates, we distinguish the individual-level and the cluster-level survivor average causal effect. We perform simulation studies to evaluate the performance of our model and found low bias and high coverage on key parameters across several different scenarios. We use data from a geriatric CRT to illustrate the use of our model. Although our illustration focuses on the case of a bivariate continuous outcome, our model is straightforwardly extended to accommodate more than two endpoints as well as other types of endpoints (e.g., binary). Thus, this work provides a general modeling framework for handling complex missingness in CRTs and can be applied to a wide range of settings with aging and palliative care populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02310v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangyu Tong, Chenxi Li, Eric Velazquez, Michael O. Harhay, Fan Li</dc:creator>
    </item>
    <item>
      <title>Sampling-based federated inference for M-estimators with non-smooth objective functions</title>
      <link>https://arxiv.org/abs/2505.02356</link>
      <description>arXiv:2505.02356v1 Announce Type: new 
Abstract: We propose a novel sampling-based federated learning framework for statistical inference on M-estimators with non-smooth objective functions, which frequently arise in modern statistical applications such as quantile regression and AUC maximization. Classical inference methods for such estimators are often computationally intensive or require nonparametric estimation of nuisance quantities. Our approach circumvents these challenges by leveraging Markov Chain Monte Carlo (MCMC) sampling and a second-stage perturbation scheme to efficiently estimate both the parameter of interest and its variance. In the presence of multiple sites with data-sharing constraints, we introduce an adaptive strategy to borrow information from potentially heterogeneous source sites without transferring individual-level data. This strategy selects source sites based on a dissimilarity measure and constructs an optimally weighted estimator using a lasso regularization. The resulting estimator has an oracle property, i.e., it achieves the optimal asymptotical efficiency by borrowing information from eligible sites while guarding against negative transfer. We establish consistency and asymptotic normality of our proposed estimators and validate the method through extensive simulations and a real-data application on type 2 diabetes. Our results demonstrate substantial gains in inference precision and underscore the importance of inclusive, data-adaptive analysis frameworks in federated learning settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02356v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiudi Li, Lu Tian, Tianxi Cai</dc:creator>
    </item>
    <item>
      <title>Attractor-Based Coevolving Dot Product Random Graph Model</title>
      <link>https://arxiv.org/abs/2505.02675</link>
      <description>arXiv:2505.02675v1 Announce Type: new 
Abstract: We introduce the attractor-based coevolving dot product random graph model (ABCDPRGM) to analyze time-series network data manifesting polarizing or flocking behavior. Graphs are generated based on latent positions under the random dot product graph regime. We assign group membership to each node. When evolving through time, the latent position of each node will change based on its current position and two attractors, which are defined to be the centers of the latent positions of all of its neighbors who share its group membership or who have different group membership than it. Parameters are assigned to the attractors to quantify the amount of influence that the attractors have on the trajectory of the latent position of each node. We developed estimators for the parameters, demonstrated their consistency, and established convergence rates under specific assumptions. Through the ABCDPRGM, we provided a novel framework for quantifying and understanding the underlying forces influencing the polarizing or flocking behaviors in dynamic network data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02675v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiwen Yang, Daniel L. Sussman</dc:creator>
    </item>
    <item>
      <title>Debiased inference in error-in-variable problems with non-Gaussian measurement error</title>
      <link>https://arxiv.org/abs/2505.02754</link>
      <description>arXiv:2505.02754v1 Announce Type: new 
Abstract: We consider drawing statistical inferences based on data subject to non-Gaussian measurement error. Unlike most existing methods developed under the assumption of Gaussian measurement error, the proposed strategy exploits hypercomplex numbers to reduce bias in naive estimation that ignores non-Gaussian measurement error. We apply this new method to several widely applicable parametric regression models with error-prone covariates, and kernel density estimation using error-contaminated data. The efficacy of this method in bias reduction is demonstrated in simulation studies and a real-life application in sports analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02754v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas W. Woolsey, Xianzheng Huang</dc:creator>
    </item>
    <item>
      <title>Latent Variable Estimation in Bayesian Black-Litterman Models</title>
      <link>https://arxiv.org/abs/2505.02185</link>
      <description>arXiv:2505.02185v1 Announce Type: cross 
Abstract: We revisit the Bayesian Black-Litterman (BL) portfolio model and remove its reliance on subjective investor views. Classical BL requires an investor "view": a forecast vector $q$ and its uncertainty matrix $\Omega$ that describe how much a chosen portfolio should outperform the market. Our key idea is to treat $(q,\Omega)$ as latent variables and learn them from market data within a single Bayesian network. Consequently, the resulting posterior estimation admits closed-form expression, enabling fast inference and stable portfolio weights. Building on these, we propose two mechanisms to capture how features interact with returns: shared-latent parametrization and feature-influenced views; both recover classical BL and Markowitz portfolios as special cases. Empirically, on 30-year Dow-Jones and 20-year sector-ETF data, we improve Sharpe ratios by 50% and cut turnover by 55% relative to Markowitz and the index baselines. This work turns BL into a fully data-driven, view-free, and coherent Bayesian framework for portfolio optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02185v1</guid>
      <category>q-fin.PM</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Y. L. Lin, Jerry Yao-Chieh Hu, Paul W. Chiou, Peter Lin</dc:creator>
    </item>
    <item>
      <title>Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview</title>
      <link>https://arxiv.org/abs/2505.02609</link>
      <description>arXiv:2505.02609v1 Announce Type: cross 
Abstract: Artificial intelligence is used at various stages of the recruitment process to automatically select the best candidate for a position, with companies guaranteeing unbiased recruitment. However, the algorithms used are either trained by humans or are based on learning from past experiences that were biased. In this article, we propose to generate data mimicking external (discrimination) and internal biases (self-censorship) in order to train five classic algorithms and to study the extent to which they do or do not find the best candidates according to objective criteria. In addition, we study the influence of the anonymisation of files on the quality of predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02609v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuyu Wang, Ang\'elique Saillet, Philom\`ene Le Gall, Alain Lacroux, Christelle Martin-Lacroux, Vincent Brault</dc:creator>
    </item>
    <item>
      <title>Hierarchical random measures without tables</title>
      <link>https://arxiv.org/abs/2505.02653</link>
      <description>arXiv:2505.02653v1 Announce Type: cross 
Abstract: The hierarchical Dirichlet process is the cornerstone of Bayesian nonparametric multilevel models. Its generative model can be described through a set of latent variables, commonly referred to as tables within the popular restaurant franchise metaphor. The latent tables simplify the expression of the posterior and allow for the implementation of a Gibbs sampling algorithm to approximately draw samples from it. However, managing their assignments can become computationally expensive, especially as the size of the dataset and of the number of levels increase. In this work, we identify a prior for the concentration parameter of the hierarchical Dirichlet process that (i) induces a quasi-conjugate posterior distribution, and (ii) removes the need of tables, bringing to more interpretable expressions for the posterior, with both a faster and an exact algorithm to sample from it. Remarkably, this construction extends beyond the Dirichlet process, leading to a new framework for defining normalized hierarchical random measures and a new class of algorithms to sample from their posteriors. The key analytical tool is the independence of multivariate increments, that is, their representation as completely random vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02653v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marta Catalano, Claudio Del Sole</dc:creator>
    </item>
    <item>
      <title>Inference on model parameters with many L-moments</title>
      <link>https://arxiv.org/abs/2210.04146</link>
      <description>arXiv:2210.04146v4 Announce Type: replace 
Abstract: This paper studies parameter estimation using L-moments, an alternative to traditional moments with attractive statistical properties. The estimation of model parameters by matching sample L-moments is known to outperform maximum likelihood estimation (MLE) in small samples from popular distributions. The choice of the number of L-moments used in estimation remains ad-hoc, though: researchers typically set the number of L-moments equal to the number of parameters, which is inefficient in larger samples. In this paper, we show that, by properly choosing the number of L-moments and weighting these accordingly, one is able to construct an estimator that outperforms MLE in finite samples, and yet retains asymptotic efficiency. We do so by introducing a generalised method of L-moments estimator and deriving its properties in an asymptotic framework where the number of L-moments varies with sample size. We then propose methods to automatically select the number of L-moments in a sample. Monte Carlo evidence shows our approach can provide mean-squared-error improvements over MLE in smaller samples, whilst working as well as it in larger samples. We consider extensions of our approach to the estimation of conditional models and a class semiparametric models. We apply the latter to study expenditure patterns in a ridesharing platform in Brazil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.04146v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luis Alvarez, Chang Chiann, Pedro Morettin</dc:creator>
    </item>
    <item>
      <title>A Universal Nonparametric Framework for Difference-in-Differences Analyses</title>
      <link>https://arxiv.org/abs/2212.13641</link>
      <description>arXiv:2212.13641v5 Announce Type: replace 
Abstract: Difference-in-differences (DiD) is a popular method to evaluate treatment effects of real-world policy interventions. Several approaches have previously developed under alternative identifying assumptions in settings where pre- and post-treatment outcome measurements are available. However, these approaches suffer from several limitations, either (i) they only apply to continuous outcomes and the average treatment effect on the treated, or (ii) they depend on the scale of the outcome, or (iii) they assume the absence of unmeasured confounding given pre-treatment covariate and outcome measurements, or (iv) they lack semiparametric efficiency theory. In this paper, we develop a new framework for causal identification and inference in DiD settings that satisfies (i)-(iv), making it universally applicable, unlike existing DiD methods. Key to our framework is an odds ratio equi-confounding (OREC) assumption, which states that the generalized odds ratio relating treatment and treatment-free potential outcome is stable across pre- and post-treatment periods. Notably, the framework recovers the standard DiD model under a certain simple location-shift model, but readily generalizes to nonlinear scales. Under the OREC assumption, we establish nonparametric identification for any potential treatment effect on the treated in view, which in principle would be identifiable under the stronger assumption of no unmeasured confounding. Moreover, we develop a consistent, asymptotically linear, and semiparametric efficient estimator of treatment effects on the treated by leveraging recent learning theory. We illustrate our framework through simulation studies and two real-world applications using Zika virus outbreak data and traffic safety data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13641v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chan Park, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Translating predictive distributions into informative priors</title>
      <link>https://arxiv.org/abs/2303.08528</link>
      <description>arXiv:2303.08528v4 Announce Type: replace 
Abstract: When complex Bayesian models exhibit implausible behaviour, one solution is to assemble available information into an informative prior. Challenges arise as prior information is often only available for the observable quantity, or some model-derived marginal quantity, rather than directly pertaining to the (usually latent) parameters in our model. We propose a method for translating available prior information, in the form of an elicited distribution for the observable or model-derived marginal quantity, into an informative joint prior. Our approach proceeds given a parametric class of prior distributions with as yet undetermined hyperparameters, and minimises the difference between the supplied elicited distribution and corresponding prior predictive distribution. We employ a global, multi-stage Bayesian optimisation procedure to locate optimal values for the hyperparameters. Three examples illustrate our approach: a cure-fraction survival model, where censoring implies that the observable quantity is _a priori_ a mixed discrete/continuous quantity; a setting in which prior information pertains to $R^{2}$ -- a model-derived quantity; and a nonlinear regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.08528v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew A. Manderson, Robert J. B. Goudie</dc:creator>
    </item>
    <item>
      <title>Wasserstein complexity penalization priors: a new class of penalizing complexity priors</title>
      <link>https://arxiv.org/abs/2312.04481</link>
      <description>arXiv:2312.04481v2 Announce Type: replace 
Abstract: Penalizing complexity (PC) priors provide a principled framework for reducing model complexity by penalizing the Kullback--Leibler Divergence (KLD) between a ``simple'' base model and a more complex model. However, constructing priors by penalizing the KLD becomes impossible in many cases because the KLD is infinite, and alternative principles often lose interpretability in terms of KLD. We propose a new class of priors, the Wasserstein complexity penalization (WCP) priors, which replace the KLD with the Wasserstein distance in the PC prior framework. WCP priors avoid the issue of infinite model distances and retain interpretability by adhering to adjusted principles. Additionally, we introduce the concept of base measures, removing the parameter dependency on the base model, and extend the framework to joint WCP priors for multiple parameters. These priors can be constructed analytically and we have both analytical and numerical implementations in R programming language. We demonstrate their use in previous PC prior applications and as well as new multivariate settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04481v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Bolin, Alexandre B. Simas, Zhen Xiong</dc:creator>
    </item>
    <item>
      <title>Skew-elliptical copula based mixed models for non-Gaussian longitudinal data with application to an HIV-AIDS study</title>
      <link>https://arxiv.org/abs/2402.00651</link>
      <description>arXiv:2402.00651v3 Announce Type: replace 
Abstract: This study was sparked by an extensive longitudinal dataset focusing on HIV CD4 T$^+$ cell counts from Livingstone district, Zambia. Analysis of the corresponding histogram plots reveals an absence of symmetry in the marginal distributions, while pairwise scatter plots uncover non-elliptical dependence patterns. Traditional linear mixed models designed for longitudinal data fail to capture these complexities adequately. Therefore, it appears prudent to explore a broader framework for modeling such data. In this article, we delve into generalized linear mixed models (GLMM) for the marginals (e.g., the Gamma mixed model), and we address the temporal dependency of repeated measurements by utilizing copulas associated with skew-elliptical distributions (such as the skew-normal/skew-$t$). Our proposed class of copula-based mixed models simultaneously accommodates asymmetry, between-subject variability, and non-standard temporal dependence, thus offering extensions to the standard linear mixed model based on multivariate normality. We estimate the model parameters using the IFM (inference function of margins) method and outline the process of obtaining standard errors for parameter estimates. Through extensive simulation studies covering skewed and symmetric marginal distributions and various copula choices, we assess the finite sample performance of our approach. Finally, we apply these models to the HIV dataset and present our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00651v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay, Kalyan Das, Sumitra Purkayastha</dc:creator>
    </item>
    <item>
      <title>Quantile Least Squares: A Flexible Approach for Robust Estimation and Validation of Location-Scale Families</title>
      <link>https://arxiv.org/abs/2402.07837</link>
      <description>arXiv:2402.07837v2 Announce Type: replace 
Abstract: In this paper, the problem of robust estimation and validation of location-scale families is revisited. The proposed methods exploit the joint asymptotic normality of sample quantiles (of i.i.d random variables) to construct the ordinary and generalized least squares estimators of location and scale parameters. These quantile least squares (QLS) estimators are easy to compute because they have explicit expressions, their robustness is achieved by excluding extreme quantiles from the least-squares estimation, and efficiency is boosted by using as many non-extreme quantiles as practically relevant. The influence functions of the QLS estimators are specified and plotted for several location-scale families. They closely resemble the shapes of some well-known influence functions yet those shapes emerge automatically (i.e., do not need to be specified). The joint asymptotic normality of the proposed estimators is established, and their finite-sample properties are explored using simulations. Also, computational costs of these estimators, as well as those of MLE, are evaluated for sample sizes n = 10^6, 10^7, 10^8, 10^9. For model validation, two goodness-of-fit tests are constructed and their performance is studied using simulations and real data. In particular, for the daily stock returns of Google over the last four years, both tests strongly support the logistic distribution assumption and reject other bell-shaped competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07837v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Adjieteh, Vytaras Brazauskas</dc:creator>
    </item>
    <item>
      <title>A Bayesian workflow for securitizing casualty insurance risk</title>
      <link>https://arxiv.org/abs/2407.14666</link>
      <description>arXiv:2407.14666v2 Announce Type: replace 
Abstract: Casualty insurance-linked securities (ILS) are appealing to investors because the underlying insurance claims, which are directly related to resulting security performance, are uncorrelated with most other asset classes. Conversely, casualty ILS are appealing to insurers as an efficient capital management tool. However, securitizing casualty insurance risk is non-trivial, as it requires forecasting loss ratios for pools of insurance policies that have not yet been written, in addition to estimating how the underlying losses will develop over time within future accident years. In this paper, we lay out a Bayesian workflow that tackles these complexities by using: (1) theoretically informed time-series and state-space models to capture how loss ratios develop and change over time; (2) historic industry data to inform prior distributions of models fit to individual programs; (3) stacking to combine loss ratio predictions from candidate models, and (4) both prior predictive simulations and simulation-based calibration to aid model specification. Using historic Schedule P filings, we then show how our proposed Bayesian workflow can be used to assess and compare models across a variety of key model performance metrics evaluated on future accident year losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14666v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Haines, Conor Goold, J. Mark Shoun</dc:creator>
    </item>
    <item>
      <title>Nonparametric Monitoring of Spatial Dependence</title>
      <link>https://arxiv.org/abs/2408.17022</link>
      <description>arXiv:2408.17022v2 Announce Type: replace 
Abstract: In process monitoring, it is common for measurements to be taken regularly or randomly from different spatial locations in two or three dimensions. While there are nonparametric methods for process monitoring with such spatial data to detect changes in the mean, there is a gap in the literature for nonparametric control charting methods developed to monitor spatial dependence. This study considers streams of regular, rectangular data sets using spatial ordinal patterns (SOPs) as a nonparametric method to detect spatial dependencies. We propose novel SOP control charts, which are distribution-free and do not require prior Phase-I analysis. To uncover higher-order dependencies, we develop a new class of statistics that combines SOPs with the Box-Pierce approach. An extensive simulation study demonstrates the superiority and effectiveness of our proposed charts over traditional parametric approaches, particularly when the spatial dependence is nonlinear or bilateral or when the spatial data contains outliers. The proposed SOP control charts are illustrated using real-world datasets to detect (i) heavy rainfall in Germany, (ii) war-related fires in (eastern) Ukraine, and (iii) manufacturing defects in textile production. This wide range of applications and findings demonstrates the broad utility of the proposed nonparametric control charts. In addition, all methods in this study are provided as a publicly available \texttt{Julia} package on \href{https://github.com/AdaemmerP/OrdinalPatterns.jl}{GitHub} for further implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17022v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Ad\"ammer, Philipp Wittenberg, Christian H. Wei{\ss}, Murat Caner Testik</dc:creator>
    </item>
    <item>
      <title>A Latent Variable Model with Change-Points and Its Application to Time Pressure Effects in Educational Assessment</title>
      <link>https://arxiv.org/abs/2410.22300</link>
      <description>arXiv:2410.22300v3 Announce Type: replace 
Abstract: Educational assessments are valuable tools for measuring student knowledge and skills, but their validity can be compromised when test takers exhibit changes in response behavior due to factors such as time pressure. To address this issue, we introduce a novel latent factor model with change-points for item response data, designed to detect and account for individual-level shifts in response patterns during testing. This model extends traditional Item Response Theory (IRT) by incorporating person-specific change-points, which enables simultaneous estimation of item parameters, person latent traits, and the location of behavioral changes. We evaluate the proposed model through extensive simulation studies, which demonstrate its ability to accurately recover item parameters, change-point locations, and individual ability estimates under various conditions. Our findings show that accounting for change-points significantly reduces bias in ability estimates, particularly for respondents affected by time pressure. Application of the model to two real-world educational testing datasets reveals distinct patterns of change-point occurrence between high-stakes and lower-stakes tests, providing insights into how test-taking behavior evolves during the tests. This approach offers a more nuanced understanding of test-taking dynamics, with important implications for test design, scoring, and interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22300v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Wallin, Yunxiao Chen, Yi-Hsuan Lee, Xiaoou Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Controlled FDR Variable Selection via Knockoffs</title>
      <link>https://arxiv.org/abs/2411.03304</link>
      <description>arXiv:2411.03304v2 Announce Type: replace 
Abstract: In many research fields, researchers aim to identify significant associations between a set of explanatory variables and a response while controlling the false discovery rate (FDR). The Knockoff filter has been recently proposed in the frequentist paradigm to introduce controlled noise in a model by cleverly constructing copies of the predictors as auxiliary variables. In this paper, we develop a fully Bayesian generalization of the classical model-X knockoff filter for normally distributed covariates. In our approach we consider a joint model of the covariates and the response variables, and incorporate the conditional independence structure of the covariates into the prior distribution of the auxiliary knockoff variables. We further incorporate the estimation of a graphical model among the covariates,leading to improved knockoffs generation and estimation of the covariate effects on the response. We use a modified spike-and-slab prior on the regression coefficients, which avoids the increase of the model dimension as typical in the classical knockoff filter. Our model performs variable selection using an upper bound on the posterior probability of non-inclusion. We show how our construction leads to valid model-X knockoffs and demonstrate that the proposed variable selection procedure leads to controlling the Bayesian FDR at an arbitrary level, in finite samples, if the distribution of the covariates is fully known, and asymptotically if estimated as in the proposed model. We use simulated data to demonstrate that our proposal increases the stability of the selection with respect to classical knockoff methods. With respect to Bayesian variable selection methods, we show that our selection procedure achieves comparable or better performances, while maintaining control over the FDR. Finally, we show the usefulness of the proposed model with an application to real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03304v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lorenzo Focardi-Olmi, Anna Gottard, Michele Guindani, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Test of Bias Acceptance (BTBA): Examining Its Performance in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2411.18481</link>
      <description>arXiv:2411.18481v3 Announce Type: replace 
Abstract: We introduce Bhirkuti's Test of Bias Acceptance (BTBA), a standardized framework for evaluating estimator bias in Monte Carlo simulation studies. BTBA uses a simulation-specific standardized score (Z*) and a decision matrix to assess bias acceptability based on the mean and variance of Z* distributions. Under ideal conditions, Z* values should approximate a standard normal distribution (Z-distribution) with a mean near zero and variance near one in the context of simulation research. Systematic deviations from these patterns such as shifted means or inflated variances indicate bias or estimator instability in simulation-based research. BTBA visualizes these patterns using ridgeline density plots, which reveal distributional features such as central tendency, spread, skewness, and outliers. Demonstrated in a latent growth modeling context, BTBA offers a reproducible and interpretable method for diagnosing bias across varying simulation conditions. By addressing key limitations of traditional relative bias (RB) metrics, BTBA provides a theoretically grounded, distribution-aware, transparent, and replicable alternative for evaluating estimator quality, particularly in psychometric modeling, structural equation modeling, and missing data research. Through this framework, we aim to enhance methodological decision-making by integrating statistical reasoning with comprehensive visualization techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18481v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>Improving exponential-family random graph models for bipartite networks</title>
      <link>https://arxiv.org/abs/2502.01892</link>
      <description>arXiv:2502.01892v2 Announce Type: replace 
Abstract: Bipartite graphs, representing two-mode networks, arise in many research fields. These networks have two disjoint node sets representing distinct entity types, for example persons and groups, with edges representing associations between the two entity types. In bipartite graphs, the smallest possible cycle is a cycle of length four, and hence four-cycles are the smallest structure to model closure in such networks. Exponential-family random graph models (ERGMs) are a widely used model for social, and other, networks, including specifically bipartite networks. Existing ERGM terms to model four-cycles in bipartite networks, however, are relatively rarely used. In this work we demonstrate some problems with these existing terms to model four-cycles, and define new ERGM terms to help overcome these problems. The position of the new terms in the ERGM dependence hierarchy, and their interpretation, is discussed. The new terms are demonstrated in simulation experiments, and their application illustrated on a canonical example of an empirical two-mode network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01892v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Stivala, Peng Wang, Alessandro Lomi</dc:creator>
    </item>
    <item>
      <title>Optimal Change Point Detection and Inference in the Spectral Density of General Time Series Models</title>
      <link>https://arxiv.org/abs/2503.23211</link>
      <description>arXiv:2503.23211v2 Announce Type: replace 
Abstract: This paper addresses the problem of detecting change points in the spectral density of time series, motivated by EEG analysis of seizure patients. Seizures disrupt coherence and functional connectivity, necessitating precise detection. Departing from traditional parametric approaches, we utilize the Wold decomposition, representing general time series as autoregressive processes with infinite lags, which are truncated and estimated around the change point. Our detection procedure employs an initial estimator that systematically searches across time points. We examine the localization error and its dependence on time series properties and sample size. To enhance accuracy, we introduce an optimal rate method with an asymptotic distribution, facilitating the construction of confidence intervals. The proposed method effectively identifies seizure onset in EEG data and extends to event detection in video data. Comprehensive numerical experiments demonstrate its superior performance compared to existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23211v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepideh Mosaferi, Abolfazl Safikhani, Peiliang Bai</dc:creator>
    </item>
    <item>
      <title>Testing for dice control at craps</title>
      <link>https://arxiv.org/abs/2504.13158</link>
      <description>arXiv:2504.13158v2 Announce Type: replace 
Abstract: Dice control involves "setting" the dice and then throwing them carefully, in the hope of influencing the outcomes and gaining an advantage at craps. How does one test for this ability? To specify the alternative hypothesis, we need a statistical model of dice control. Two have been suggested in the gambling literature, namely the Smith-Scott model and the Wong-Shackleford model. Both models are parameterized by $\theta\in[0,1]$, which measures the shooter's level of control. We propose and compare four test statistics: (a) the sample proportion of 7s; (b) the sample proportion of pass-line wins; (c) the sample mean of hand-length observations; and (d) the likelihood ratio statistic for a hand-length sample. We want to test $H_0:\theta = 0$ (no control) versus $H_1:\theta &gt; 0$ (some control). We also want to test $H_0:\theta\le\theta_0$ versus $H_1:\theta&gt;\theta_0$, where $\theta_0$ is the "break-even point." For the tests considered we estimate the power, either by normal approximation or by simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13158v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stewart N. Ethier</dc:creator>
    </item>
    <item>
      <title>Selecting Optimal Candidate Profiles in Adversarial Environments Using Conjoint Analysis and Machine Learning</title>
      <link>https://arxiv.org/abs/2504.19043</link>
      <description>arXiv:2504.19043v2 Announce Type: replace 
Abstract: Conjoint analysis, an application of factorial experimental design, is a popular tool in social science research for studying multidimensional preferences. In such political analysis experiments, respondents are often asked to choose between two hypothetical political candidates with randomly selected features, which can include partisanship, policy positions, gender, and race. We consider the problem of identifying optimal candidate profiles. Because the number of unique feature combinations far exceeds the total number of observations in a typical conjoint experiment, it is impossible to determine the optimal profile exactly. To address this identification challenge, we derive an optimal stochastic intervention that represents a probability distribution of various attributes aimed at achieving the most favorable average outcome. We first consider an environment where one political party optimizes their candidate selection. We then move to the more realistic case where two political parties optimize their own candidate selection simultaneously and in opposition to each other. We apply the proposed methodology to an existing candidate choice conjoint experiment concerning vote choice for US president. We find that, in contrast to the non-adversarial approach, expected outcomes in the adversarial regime fall within range of historical electoral outcomes, with optimal strategies suggested by the method more likely to match the actual observed candidates compared to strategies derived from a non-adversarial approach. These findings highlight that incorporating adversarial dynamics into conjoint analysis can provide more realistic insights into strategic analysis while opening up novel ways to test the empirical implications of institutional design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19043v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Connor T. Jerzak, Priyanshi Chandra, Rishi Hazra</dc:creator>
    </item>
    <item>
      <title>Distributed Reconstruction from Compressive Measurements: Nonconvexity and Heterogeneity</title>
      <link>https://arxiv.org/abs/2504.19919</link>
      <description>arXiv:2504.19919v2 Announce Type: replace 
Abstract: The compressive sensing (CS) and 1-bit CS demonstrate superior efficiency in signal acquisition and resource conservation, while 1-bit CS achieves maximum resource efficiency through sign-only measurements. With the emergence of massive data, the distributed signal aggregation under CS and 1-bit CS measurements introduces many challenges, including nonconvexity and heterogeneity. The nonconvexity originates from the unidentifiability of signal magnitude under finite-precision measurements. The heterogeneity arises from the signal and noisy measurement on each node. To address these challenges, we propose a framework with a squared cosine similarity penalty. We address nonconvexity by an novel invex relaxation formulation to ensure the uniqueness of the global optimality. For heterogeneous signals and noisy measurements, the proposed estimate adaptively debiases through correction guided by similarity and signal-to-noise ratio (SNR) information. Our method achieves a high probability minimax-optimal convergence rate under sufficient node counts and similarity conditions, improving from $O\{(p\log{p}/n_j)^{1/2}\}$ to $O\{(p\log{p}/N)^{1/2}+p^{1/2}/n_j\}$, with signal dimension $p$, local and total sizes $n_j$ and $N$. Extensive simulations validate the method's effectiveness and performance gains in reconstructing heterogeneous signals from 1-bit CS measurements. The proposed framework maintains applicability to CS measurements while reducing communication overhead in distributed setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19919v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erbo Li, Qi Qin, Yifan Sun, Liping Zhu</dc:creator>
    </item>
    <item>
      <title>Grouped Orthogonal Arrays and Their Applications</title>
      <link>https://arxiv.org/abs/2505.00536</link>
      <description>arXiv:2505.00536v2 Announce Type: replace 
Abstract: In computer experiments, it has become a standard practice to select the inputs that spread out as uniformly as possible over the design space. The resulting designs are called space-filling designs and they are undoubtedly desirable choices when there is no prior knowledge on how the input variables affect the response and the objective of experiments is global fitting. When there is some prior knowledge on the underlying true function of the system or what statistical models are more appropriate, a natural question is, are there more suitable designs than vanilla space-filling designs? In this article, we provide an answer for the cases where there are no interactions between the factors from disjoint groups of variables. In other words, we consider the design issue when the underlying functional form of the system or the statistical model to be used is additive where each component depends on one group of variables from a set of disjoint groups. For such cases, we recommend using {\em grouped orthogonal arrays.} Several construction methods are provided and many designs are tabulated for practical use. Compared with existing techniques in the literature, our construction methods can generate many more designs with flexible run sizes and better within-group projection properties for any prime power number of levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00536v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Statistica Sinica, 2025</arxiv:journal_reference>
      <dc:creator>Guangzhou Chen, Yuanzhen He, C. Devon Lin, Fasheng Sun</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference under Random Potential Outcomes via Riesz Representation</title>
      <link>https://arxiv.org/abs/2505.01324</link>
      <description>arXiv:2505.01324v2 Announce Type: replace 
Abstract: We introduce a general framework for design-based causal inference that accommodates stochastic potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function $\tilde{y}_i(z, \omega)$, where $\omega$ denotes latent randomness external to the treatment assignment. Building on recent work that connects design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This allows us to derive unbiased and consistent estimators, even when potential outcomes exhibit random variation. The framework retains the key advantage of design-based analysis, namely, the use of a known randomisation scheme for identification, while enabling inference in settings with inherent stochasticity. We establish large-sample properties under local dependence, provide a variance estimator compatible with sparse dependency structures, and illustrate the method through a simulation. Our results unify design-based reasoning with random-outcome modelling, broadening the applicability of causal inference in complex experimental environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01324v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukai Yang</dc:creator>
    </item>
    <item>
      <title>Weight-calibrated estimation for factor models of high-dimensional time series</title>
      <link>https://arxiv.org/abs/2505.01357</link>
      <description>arXiv:2505.01357v2 Announce Type: replace 
Abstract: The factor modeling for high-dimensional time series is powerful in discovering latent common components for dimension reduction and information extraction. Most available estimation methods can be divided into two categories: the covariance-based under asymptotically-identifiable assumption and the autocovariance-based with white idiosyncratic noise. This paper follows the autocovariance-based framework and develops a novel weight-calibrated method to improve the estimation performance. It adopts a linear projection to tackle high-dimensionality, and employs a reduced-rank autoregression formulation. The asymptotic theory of the proposed method is established, relaxing the assumption on white noise. Additionally, we make the first attempt in the literature by providing a systematic theoretical comparison among the covariance-based, the standard autocovariance-based, and our proposed weight-calibrated autocovariance-based methods in the presence of factors with different strengths. Extensive simulations are conducted to showcase the superior finite-sample performance of our proposed method, as well as to validate the newly established theory. The superiority of our proposal is further illustrated through the analysis of one financial and one macroeconomic data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01357v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinghao Qiao, Zihan Wang, Qiwei Yao, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Bounding Treatment Effects by Pooling Limited Information across Observations</title>
      <link>https://arxiv.org/abs/2111.05243</link>
      <description>arXiv:2111.05243v5 Announce Type: replace-cross 
Abstract: We provide novel bounds on average treatment effects (on the treated) that are valid under an unconfoundedness assumption. Our bounds are designed to be robust in challenging situations, for example, when the conditioning variables take on a large number of different values in the observed sample, or when the overlap condition is violated. This robustness is achieved by only using limited "pooling" of information across observations. Namely, the bounds are constructed as sample averages over functions of the observed outcomes such that the contribution of each outcome only depends on the treatment status of a limited number of observations. No information pooling across observations leads to so-called "Manski bounds", while unlimited information pooling leads to standard inverse propensity score weighting. We explore the intermediate range between these two extremes and provide corresponding inference methods. We show in Monte Carlo experiments and through two empirical application that our bounds are indeed robust and informative in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.05243v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sokbae Lee, Martin Weidner</dc:creator>
    </item>
    <item>
      <title>Large covariance matrix estimation via penalized log-det heuristics</title>
      <link>https://arxiv.org/abs/2209.04867</link>
      <description>arXiv:2209.04867v2 Announce Type: replace-cross 
Abstract: This paper provides a comprehensive estimation framework for large covariance matrices via a log-det heuristics augmented by a nuclear norm plus $\ell_{1}$-norm penalty. We develop the model framework, which includes high-dimensional approximate factor models with a sparse residual covariance. We prove that the aforementioned log-det heuristics is locally convex with a Lipschitz-continuous gradient, so that a proximal gradient algorithm may be stated to numerically solve the problem while controlling the threshold parameters. The proposed optimization strategy recovers in a single step both the covariance matrix components and the latent rank and the residual sparsity pattern with high probability, and performs systematically not worse than the corresponding estimators employing Frobenius loss in place of the log-det heuristics. The error bounds for the ensuing low rank and sparse covariance matrix estimators are established, and the identifiability conditions for the latent geometric manifolds are provided, improving existing literature. The validity of outlined results is highlighted by an exhaustive simulation study and a financial data example involving Euro Area banks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04867v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enrico Bernardi, Matteo Farn\`e</dc:creator>
    </item>
    <item>
      <title>Residual permutation test for regression coefficient testing</title>
      <link>https://arxiv.org/abs/2211.16182</link>
      <description>arXiv:2211.16182v4 Announce Type: replace-cross 
Abstract: We consider the problem of testing whether a single coefficient is equal to zero in linear models when the dimension of covariates $p$ can be up to a constant fraction of sample size $n$. In this regime, an important topic is to propose tests with finite-sample valid size control without requiring the noise to follow strong distributional assumptions. In this paper, we propose a new method, called residual permutation test (RPT), which is constructed by projecting the regression residuals onto the space orthogonal to the union of the column spaces of the original and permuted design matrices. RPT can be proved to achieve finite-population size validity under fixed design with just exchangeable noises, whenever $p &lt; n / 2$. Moreover, RPT is shown to be asymptotically powerful for heavy tailed noises with bounded $(1+t)$-th order moment when the true coefficient is at least of order $n^{-t/(1+t)}$ for $t \in [0,1]$. We further proved that this signal size requirement is essentially rate-optimal in the minimax sense. Numerical studies confirm that RPT performs well in a wide range of simulation settings with normal and heavy-tailed noise distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16182v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOS2479</arxiv:DOI>
      <arxiv:journal_reference>The Annals of Statistics 53.2 (2025): 724-748</arxiv:journal_reference>
      <dc:creator>Kaiyue Wen, Tengyao Wang, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Analysis for Over-parameterized Linear Model via Effective Spectra</title>
      <link>https://arxiv.org/abs/2305.15754</link>
      <description>arXiv:2305.15754v3 Announce Type: replace-cross 
Abstract: In high-dimensional Bayesian statistics, various methods have been developed, including prior distributions that induce parameter sparsity to handle many parameters. Yet, these approaches often overlook the rich spectral structure of the covariate matrix, which can be crucial when true signals are not sparse. To address this gap, we introduce a data-adaptive Gaussian prior whose covariance is aligned with the leading eigenvectors of the sample covariance. This prior design targets the data's intrinsic complexity rather than its ambient dimension by concentrating the parameter search along principal data directions. We establish contraction rates of the corresponding posterior distribution, which reveal how the mass in the spectrum affects the prediction error bounds. Furthermore, we derive a truncated Gaussian approximation to the posterior (i.e., a Bernstein-von Mises-type result), which allows for uncertainty quantification with a reduced computational burden. Our findings demonstrate that Bayesian methods leveraging spectral information of the data are effective for estimation in non-sparse, high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.15754v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2402.14264</link>
      <description>arXiv:2402.14264v3 Announce Type: replace-cross 
Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, the statistical optimality of these methods has still remained an open area of investigation, especially in regimes where these methods do not achieve parametric rates. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), as well as weighted variants of the former, which arise in policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14264v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Dynamic Local Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2405.01463</link>
      <description>arXiv:2405.01463v3 Announce Type: replace-cross 
Abstract: We consider Dynamic Treatment Regimes (DTRs) with One Sided Noncompliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may not comply with encouragements based on unobserved confounders. For settings with binary treatments and encouragements, we provide nonparametric identification, estimation, and inference for Dynamic Local Average Treatment Effects (LATEs), which are expected values of multiple time period treatment effect contrasts for the respective complier subpopulations. Under One Sided Noncompliance and sequential extensions of the assumptions in Imbens and Angrist (1994), we show that one can identify Dynamic LATEs that correspond to treating at single time steps. In Staggered Adoption settings, we show that the assumptions are sufficient to identify Dynamic LATEs for treating in multiple time periods. Moreover, this result extends to any setting where the effect of a treatment in one period is uncorrelated with the compliance event in a subsequent period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01463v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ravi B. Sojitra, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Granger Causality in High-Dimensional Networks of Time Series</title>
      <link>https://arxiv.org/abs/2406.02360</link>
      <description>arXiv:2406.02360v4 Announce Type: replace-cross 
Abstract: A novel approach is developed for discovering directed connectivity between specified pairs of nodes in a high-dimensional network (HDN) of brain signals. To accurately identify causal connectivity for such specified objectives, it is necessary to properly address the influence of all other nodes within the network. The proposed procedure herein starts with the estimation of a low-dimensional representation of the other nodes in the network utilizing (frequency-domain-based) spectral dynamic principal component analysis (sDPCA). The resulting scores can then be removed from the nodes of interest, thus eliminating the confounding effect of other nodes within the HDN. Accordingly, causal interactions can be dissected between nodes that are isolated from the effects of the network. Extensive simulations have demonstrated the effectiveness of this approach as a tool for causality analysis in complex time series networks. The proposed methodology has also been shown to be applicable to multichannel EEG networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02360v4</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing with e-values</title>
      <link>https://arxiv.org/abs/2410.23614</link>
      <description>arXiv:2410.23614v4 Announce Type: replace-cross 
Abstract: This book is written to offer a humble, but unified, treatment of e-values in hypothesis testing. It is organized into three parts: Fundamental Concepts, Core Ideas, and Advanced Topics. The first part includes four chapters that introduce the basic concepts. The second part includes five chapters of core ideas such as universal inference, log-optimality, e-processes, operations on e-values, and e-values in multiple testing. The third part contains seven chapters of advanced topics. The book collates important results from a variety of modern papers on e-values and related concepts, and also contains many results not published elsewhere. It offers a coherent and comprehensive picture on a fast-growing research area, and is ready to use as the basis of a graduate course in statistics and related fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23614v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaditya Ramdas, Ruodu Wang</dc:creator>
    </item>
  </channel>
</rss>
