<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 May 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Digital Evolution: Novo Nordisk's Shift to Ontology-Based Data Management</title>
      <link>https://arxiv.org/abs/2405.05413</link>
      <description>arXiv:2405.05413v1 Announce Type: new 
Abstract: Biomedical data is growing exponentially, and managing it is increasingly challenging. While Findable, Accessible, Interoperable and Reusable (FAIR) data principles provide guidance, their adoption has proven difficult, especially in larger enterprises like pharmaceutical companies. In this manuscript, we describe how we leverage an Ontology-Based Data Management (OBDM) strategy for digital transformation in Novo Nordisk Research &amp; Early Development. Here, we include both our technical blueprint and our approach for organizational change management. We further discuss how such an OBDM ecosystem plays a pivotal role in the organizations digital aspirations for data federation and discovery fuelled by artificial intelligence. Our aim for this paper is to share the lessons learned in order to foster dialogue with parties navigating similar waters while collectively advancing the efforts in the fields of data management, semantics and data driven drug discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05413v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shawn Zheng Kai Tan, Shounak Baksi, Thomas Gad Bjerrgaard, Preethi Elangovan, Thrishna Kuttikattu Gopalakrishnan, Darko Hric, Joffrey Joumaa, Beidi Li, Kashif Rabbani, Santhosh Kannan Venkatesan, Joshua Daniel Valdez, Saritha Vettikunnel Kuriakose</dc:creator>
    </item>
    <item>
      <title>How Good Are Multi-dimensional Learned Indices? An Experimental Survey</title>
      <link>https://arxiv.org/abs/2405.05536</link>
      <description>arXiv:2405.05536v1 Announce Type: new 
Abstract: Efficient indexing is fundamental for multi-dimensional data management and analytics. An emerging tendency is to directly learn the storage layout of multi-dimensional data by simple machine learning models, yielding the concept of Learned Index. Compared with the conventional indices used for decades (e.g., kd-tree and R-tree variants), learned indices are empirically shown to be both space- and time-efficient on modern architectures. However, there lacks a comprehensive evaluation of existing multi-dimensional learned indices under a unified benchmark, which makes it difficult to decide the suitable index for specific data and queries and further prevents the deployment of learned indices in real application scenarios. In this paper, we present the first in-depth empirical study to answer the question of how good multi-dimensional learned indices are. Six recently published indices are evaluated under a unified experimental configuration including index implementation, datasets, query workloads, and evaluation metrics. We thoroughly investigate the evaluation results and discuss the findings that may provide insights for future learned index design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05536v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyu Liu, Maocheng Li, Yuxiang Zeng, Yanyan Shen, Lei Chen</dc:creator>
    </item>
    <item>
      <title>Efficient Algorithms for Top-k Stabbing Queries on Weighted Interval Data (Full Version)</title>
      <link>https://arxiv.org/abs/2405.05601</link>
      <description>arXiv:2405.05601v1 Announce Type: new 
Abstract: Intervals have been generated in many applications (e.g., temporal databases), and they are often associated with weights, such as prices. This paper addresses the problem of processing top-k weighted stabbing queries on interval data. Given a set of weighted intervals, a query value, and a result size k, this problem finds the k intervals that are stabbed by the query value and have the largest weights. Although this problem finds practical applications (e.g., purchase, vehicle, and cryptocurrency analysis), it has not been well studied. A state-of-the-art algorithm for this problem incurs O(nlogk) time, where n is the number of intervals, so it is not scalable to large n. We solve this inefficiency issue and propose an algorithm that runs in O(sqrt(n)logn + k) time. Furthermore, we propose an O(logn + k) algorithm to further accelerate the search efficiency. Experiments on two real large datasets demonstrate that our algorithms are faster than existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05601v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daichi Amagata, Junya Yamada, Yuchen Ji, Takahiro Hara</dc:creator>
    </item>
    <item>
      <title>Big Data, Big Decisions Choosing the Right Database</title>
      <link>https://arxiv.org/abs/2405.02506</link>
      <description>arXiv:2405.02506v2 Announce Type: replace 
Abstract: In the burgeoning era of big data, selecting the optimal database solution has become a critical decision for organizations across every industry. Big data demands a powerful database solution. Traditionally, SQL Database, Database ruled, offering a structured approach familiar to many organizations. However, big data's complexity and unstructured nature challenge SQL Database's limitations. Enter NoSQL Database: flexible and scalable, making them ideal for big data's ever-changing nature. We'll explore the key differences between SQL and NoSQL Database. Performance-wise, SQL Database shines for structured queries. Its standardized language (SQL) ensures data consistency and complex analysis. But for big data's unstructured formats, this rigidity becomes a hurdle. NoSQL offers a welcome contrast. Its flexible schema allows for diverse data formats and evolving structures, perfect for undefined or frequently changing data models. Additionally, NoSQL boasts superior horizontal scalability, distributing data across multiple servers for cost-effective growth. Understanding these key differentiators empowers organizations to choose the optimal database for their big data needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02506v2</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.48550/arXiv.2405.02506</arxiv:DOI>
      <dc:creator>Mohamed Hassan</dc:creator>
    </item>
    <item>
      <title>Delta Tensor: Efficient Vector and Tensor Storage in Delta Lake</title>
      <link>https://arxiv.org/abs/2405.03708</link>
      <description>arXiv:2405.03708v2 Announce Type: replace-cross 
Abstract: The exponential growth of artificial intelligence (AI) and machine learning (ML) applications has necessitated the development of efficient storage solutions for vector and tensor data. This paper presents a novel approach for tensor storage in a Lakehouse architecture using Delta Lake. By adopting the multidimensional array storage strategy from array databases and sparse encoding methods to Delta Lake tables, experiments show that this approach has demonstrated notable improvements in both space and time efficiencies when compared to traditional serialization of tensors. These results provide valuable insights for the development and implementation of optimized vector and tensor storage solutions in data-intensive applications, contributing to the evolution of efficient data management practices in AI and ML domains in cloud-native environments</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03708v2</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhiwei Bao, Liu Liao-Liao, Zhiyu Wu, Yifan Zhou, Dan Fan, Michal Aibin, Yvonne Coady, Andrew Brownsword</dc:creator>
    </item>
  </channel>
</rss>
