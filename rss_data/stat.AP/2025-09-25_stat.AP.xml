<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Sep 2025 04:01:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Incorporating LLM Embeddings for Variation Across the Human Genome</title>
      <link>https://arxiv.org/abs/2509.20702</link>
      <description>arXiv:2509.20702v1 Announce Type: new 
Abstract: Recent advances in large language model (LLM) embeddings have enabled powerful representations for biological data, but most applications to date focus only on gene-level information. We present one of the first systematic frameworks to generate variant-level embeddings across the entire human genome. Using curated annotations from FAVOR, ClinVar, and the GWAS Catalog, we constructed semantic text descriptions for 8.9 billion possible variants and generated embeddings at three scales: 1.5 million HapMap3+MEGA variants, ~90 million imputed UK Biobank variants, and ~9 billion all possible variants. Embeddings were produced with both OpenAI's text-embedding-3-large and the open-source Qwen3-Embedding-0.6B models. Baseline experiments demonstrate high predictive accuracy for variant properties, validating the embeddings as structured representations of genomic variation. We outline two downstream applications: embedding-informed hypothesis testing by extending the Frequentist And Bayesian framework to genome-wide association studies, and embedding-augmented genetic risk prediction that enhances standard polygenic risk scores. These resources, publicly available on Hugging Face, provide a foundation for advancing large-scale genomic discovery and precision medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20702v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>q-bio.GN</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongqian Niu, Jordan Bryan, Xihao Li, Didong Li</dc:creator>
    </item>
    <item>
      <title>Statistical Learning of Trade Credit Insurance Network Data with Applications to Ratemaking and Reserving</title>
      <link>https://arxiv.org/abs/2509.20803</link>
      <description>arXiv:2509.20803v1 Announce Type: new 
Abstract: Trade credit insurance (TCI) is a specialized line of property and casualty insurance, protecting businesses against financial losses due to buyer's insolvency. Predictive modeling for TCI claims poses formidable challenges due to the data's complexity, yet remains underexplored in the literature. Leveraging six years of detailed TCI data from an Asian TCI insurer, we develop a bivariate, network-augmented Generalized Linear Mixed Model (GLMM) to jointly model claim probability and reporting time gaps. Our model integrates extended-order degree centrality and random effects at the business and policy levels, adjusted for data incompleteness, to capture claim histories, reporting time gaps, and network relationships specific to TCI data. To implement a feasible workaround for the high-dimensional integrations required by individual random effects, we propose a scalable Stochastic Expectation-Maximization (SEM) algorithm. Data analysis using this TCI dataset demonstrates that our model significantly outperforms benchmark models in both model fit and predictive accuracy, highlighting the effectiveness of our approach for improved ratemaking and reserving in TCI. Supplementary materials for this article are available as an online supplement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20803v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Woongchae Yoo, Spark C. Tseung, Tsz Chai Fung</dc:creator>
    </item>
    <item>
      <title>Detecting gene-environment interactions to guide personalized intervention: boosting distributional regression for polygenic scores</title>
      <link>https://arxiv.org/abs/2509.20850</link>
      <description>arXiv:2509.20850v1 Announce Type: new 
Abstract: Polygenic risk scores can be used to model the individual genetic liability for human traits. Current methods primarily focus on modeling the mean of a phenotype neglecting the variance. However, genetic variants associated with phenotypic variance can provide important insights to gene-environment interaction studies. To overcome this, we propose snpboostlss, a cyclical gradient boosting algorithm for a Gaussian location-scale model to jointly derive sparse polygenic models for both the mean and the variance of a quantitative phenotype. To improve computational efficiency on high-dimensional and large-scale genotype data (large n and large p), we only consider a batch of most relevant variants in each boosting step. We investigate the effect of statins therapy (the environmental factor) on low-density lipoprotein in the UK Biobank cohort using the new snpboostlss algorithm. We are able to verify the interaction between statins usage and the polygenic risk scores for phenotypic variance in both cross sectional and longitudinal analyses. Particularly, following the spirit of target trial emulation, we observe that the treatment effect of statins is more substantial in people with higher polygenic risk scores for phenotypic variance, indicating gene-environment interaction. When applying to body mass index, the newly constructed polygenic risk scores for variance show significant interaction with physical activity and sedentary behavior. Therefore, the polygenic risk scores for phenotypic variance derived by snpboostlss have potential to identify individuals that could benefit more from environmental changes (e.g. medical intervention and lifestyle changes).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20850v1</guid>
      <category>stat.AP</category>
      <category>q-bio.GN</category>
      <category>q-bio.QM</category>
      <category>stat.CO</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiong Wu, Hannah Klinkhammer, Kiran Kunwar, Christian Staerk, Carlo Maj, Andreas Mayr</dc:creator>
    </item>
    <item>
      <title>A sub-hourly spatio-temporal statistical model for solar irradiance in Ireland using open-source data</title>
      <link>https://arxiv.org/abs/2509.21041</link>
      <description>arXiv:2509.21041v1 Announce Type: new 
Abstract: Accurate estimation of solar irradiance is essential for reliable modelling of solar photovoltaic (PV) power production. In Ireland's highly variable maritime climate, where ground-based measurement stations are sparsely distributed, selecting an appropriate solar irradiance dataset presents a significant challenge. This study introduces a novel Bayesian spatio-temporal modelling framework for predicting solar irradiance at hourly and sub-hourly (10-minute) resolutions across Ireland. Cross-validation demonstrates that our model is statistically robust across all temporal resolutions with hourly showing highest prediction precision whereas 10-minute resolution encounters higher errors but better uncertainty quantification. In separate evaluations, we compare our model against alternative data sources, including reanalysis datasets and nearest-station interpolation, and find that it consistently provides superior site-specific accuracy. At the hourly scale, our model outperforms ERA5 in agreement with ground-based observations. At the sub-hourly scale, 10-minute resolution estimates provide solar PV power outputs consistent with residential and industrial solar PV installations in Ireland. Beyond surpassing existing datasets, our model delivers full uncertainty quantification, scalability and the capacity for real-time implementation, offering a powerful tool for solar energy prediction and the estimation of losses due to overload clipping from inverter undersizing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21041v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maeve Upton, Eamonn Organ, Amanda Lenzi, James Sweeney</dc:creator>
    </item>
    <item>
      <title>Not All Accuracy Is Equal: Prioritizing Diversity in Infectious Disease Forecasting</title>
      <link>https://arxiv.org/abs/2509.21191</link>
      <description>arXiv:2509.21191v1 Announce Type: new 
Abstract: Ensemble forecasts have become a cornerstone of large-scale disease response, underpinning decision making at agencies such as the US Centers for Disease Control and Prevention (CDC). Their growing use reflects the goal of combining multiple models to improve accuracy and stability versus using a single model. However, recent experience shows these benefits are not guaranteed. During the COVID-19 pandemic, the CDC's multi-model forecasting ensemble outperformed the best single model by only 1%, and CDC flu forecasting ensembles have often ranked below multiple individual models.
  This raises a key question: why are ensembles underperforming? We posit that a central reason is that both model developers and ensemble builders typically focus on stand-alone accuracy. Models are fit to minimize their own forecasting error, and ensembles are often weighted according to those same scores. However, most epidemic forecasts are built from a small set of approaches and trained on the same surveillance data, leading to highly correlated errors. This redundancy limits the benefit of ensembling and may explain why large ensembles sometimes deliver only marginal gains.
  To realize the potential of ensembles, both modelers and ensemblers should prioritize models that contribute complementary information rather than replicating existing approaches. Ensembles built with this principle in mind move beyond size for its own sake toward true diversity, producing forecasts that are more robust and more valuable for epidemic preparedness and response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21191v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carson Dudley, Marisa Eisenberg</dc:creator>
    </item>
    <item>
      <title>Optimal Transport Based Hyperspectral Unmixing for Highly Mixed Observations</title>
      <link>https://arxiv.org/abs/2509.20417</link>
      <description>arXiv:2509.20417v1 Announce Type: cross 
Abstract: We propose a novel approach based on optimal transport (OT) for tackling the problem of highly mixed data in blind hyperspectral unmixing. Our method constrains the distribution of the estimated abundance matrix to resemble a targeted Dirichlet distribution more closely. The novelty lies in using OT to measure the discrepancy between the targeted and true abundance distributions, which we incorporate as a regularization term in our optimization problem. We demonstrate the efficiency of our method through a case study involving an unsupervised deep learning approach. Our experiments show that the proposed approach allows for a better estimation of the endmembers in the presence of highly mixed data, while displaying robustness to the choice of target abundance distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20417v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/WHISPERS65427.2024.10876524</arxiv:DOI>
      <arxiv:journal_reference>2024 14th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)</arxiv:journal_reference>
      <dc:creator>D. Doutsas, B. Figliuzzi</dc:creator>
    </item>
    <item>
      <title>An Interpretable Single-Index Mixed-Effects Model for Non-Gaussian National Survey Data</title>
      <link>https://arxiv.org/abs/2509.20638</link>
      <description>arXiv:2509.20638v1 Announce Type: cross 
Abstract: This manuscript presents an innovative statistical model to quantify periodontal disease in the context of complex medical data. A mixed-effects model incorporating skewed random effects and heavy-tailed residuals is introduced, ensuring robust handling of non-normal data distributions. The fixed effect is modeled as a combination of a slope parameter and a single index function, constrained to be monotonic increasing for meaningful interpretation. This approach captures different dimensions of periodontal disease progression by integrating Clinical Attachment Level (CAL) and Pocket Depth (PD) biomarkers within a unified analytical framework. A variable selection method based on the grouped horseshoe prior is employed, addressing the relatively high number of risk factors. Furthermore, survey weight information typically provided with large survey data is incorporated to ensure accurate inference. This comprehensive methodology significantly advances the statistical quantification of periodontal disease, offering a nuanced and precise assessment of risk factors and disease progression. The proposed methodology is implemented in the \textsf{R} package \href{https://cran.r-project.org/package=MSIMST}{\textsc{MSIMST}}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20638v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyang Liu, Debdeep Pati, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Online Sequential Leveraging Sampling Method for Streaming Autoregressive Time Series with Application to Seismic Data</title>
      <link>https://arxiv.org/abs/2509.20698</link>
      <description>arXiv:2509.20698v1 Announce Type: cross 
Abstract: Seismic data contain complex temporal information that arrives at high speed and has a large, even potentially unbounded volume. The explosion of temporally correlated streaming data from advanced seismic sensors poses analytical challenges due to its sheer volume and real-time nature. Sampling, or data reduction, is a natural yet powerful tool for handling large streaming data while balancing estimation accuracy and computational cost. Currently, data reduction methods and their statistical properties for streaming data, especially streaming autoregressive time series, are not well-studied in the literature. In this article, we propose an online leverage-based sequential data reduction algorithm for streaming autoregressive time series with application to seismic data. The proposed Sequential Leveraging Sampling (SLS) method selects only one consecutively recorded block from the data stream for inference. While the starting point of the SLS block is chosen using a random mechanism based on streaming leverage scores of data, the block size is determined by a sequential stopping rule. The SLS block offers efficient sample usage, as evidenced by our results confirming asymptotic normality for the normalized least squares estimator in both linear and nonlinear autoregressive settings. The SLS method is applied to two seismic datasets: the 2023 Turkey-Syria earthquake doublet data on the macroseismic scale and the Oklahoma seismic data on the microseismic scale. We demonstrate the ability of the SLS method to efficiently identify seismic events and elucidate their intricate temporal dependence structure. Simulation studies are presented to evaluate the empirical performance of the SLS method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20698v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Xie, T. N. Sriram, Wei Biao Wu, Ping Ma</dc:creator>
    </item>
    <item>
      <title>Modi linear failure rate distribution with application to survival time data</title>
      <link>https://arxiv.org/abs/2509.20831</link>
      <description>arXiv:2509.20831v1 Announce Type: cross 
Abstract: A new lifetime model, named the Modi linear failure rate distribution, is suggested. This flexible model is capable of accommodating a wide range of hazard rate shapes, including decreasing, increasing, bathtub, upside-down bathtub, and modified bathtub forms, making it particularly suitable for modeling diverse survival and reliability data. Our proposed model contains the Modi exponential distribution and the Modi Rayleigh distribution as sub-models. Numerous mathematical and reliability properties are derived, including the $r^{th}$ moment, moment generating function, $r^{th}$ conditional moment, quantile function, order statistics, mean deviations, R\'{e}nyi entropy, and reliability function. The method of maximum likelihood is employed to estimate the model parameters. Monte Carlo simulations are presented to examine how these estimators perform. The superior fit of our newly introduced model is proved through two real-world survival data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20831v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lazhar Benkhelifa</dc:creator>
    </item>
    <item>
      <title>Detecting disease progression from animal movement using hidden Markov models</title>
      <link>https://arxiv.org/abs/2509.21132</link>
      <description>arXiv:2509.21132v1 Announce Type: cross 
Abstract: Understanding disease dynamics is crucial for managing wildlife populations and assessing spillover risk to domestic animals and humans, but infection data on free-ranging animals are difficult to obtain. Because pathogen and parasite infections can alter host movement, infection status may be inferred from animal trajectories. We present a hidden Markov model (HMM) framework that links observed movement behaviors to unobserved infection states, consistent with epidemiological compartmental models (e.g., susceptible, infected, recovered, dead). Using movement data from 84 reintroduced scimitar-horned oryx (Oryx dammah), 38 confirmed dead in the field and 6 sampled for disease testing, we demonstrate how HMMs can incorporate epidemiological structure through (1) constrained transition probabilities (e.g., to preclude or allow recovery), (2) covariate effects on transmission, and (3) hierarchically structured HMMs (HHMMs) for multi-scale transitions. Comparing veterinary diagnostic reports with model outputs, we found that HMMs with epidemiological constraints successfully identified infection-associated reductions in movement, whereas unconstrained models failed to capture disease progression. Simulations further showed that constrained HMMs accurately classified susceptible, infected, and recovered states. By illustrating flexible formulations and a workflow for model selection, we provide a transferable approach for detecting infection from movement data. This framework can enhance wildlife disease surveillance, guide population management, and improve understanding of disease dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21132v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dongmin Kim, Th\'eo Michelot, Katherine Mertes, Jared A. Stabach, John Fieberg</dc:creator>
    </item>
    <item>
      <title>A Diagnostic to Find and Help Combat Stochastic Positivity Issues -- with a Focus on Continuous Treatments</title>
      <link>https://arxiv.org/abs/2502.11820</link>
      <description>arXiv:2502.11820v3 Announce Type: replace 
Abstract: The positivity assumption is central in the identification of a causal effect, and especially the stochastic variant is an issue many applied researchers face, yet is rarely discussed, especially in conjunction with continuous treatments or Modified Treatment Policies. One common recommendation for dealing with a violation is to change the estimand. However, an applied researcher is faced with two problems: First, how can she tell whether there is a stochastic positivity violation given her estimand of interest, preferably without having to estimate a model first? Second, if she finds a problem with stochastic positivity, how should she change her estimand in order to arrive at an estimand which does not face the same issues? We suggest a novel diagnostic which allows the researcher to answer both questions by providing insights into how well an estimation for a certain estimand can be made for each observation using the data at hand. We provide a simulation study on the general behaviour of different Modified Treatment Policies (MTPs) at different levels of stochastic positivity violations and show how the diagnostic helps understand where bias is to be expected. We illustrate the application of our proposed diagnostic in a pharmacoepidemiological study based on data from CHAPAS-3, a trial comparing different treatment regimens for children living with HIV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11820v3</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katharina Ring, Michael Schomaker</dc:creator>
    </item>
    <item>
      <title>Inferring Piece Value in Chess and Chess Variants</title>
      <link>https://arxiv.org/abs/2509.04691</link>
      <description>arXiv:2509.04691v2 Announce Type: replace 
Abstract: We use logistic regression to estimate the value of the pieces in standard chess and several chess variants, namely Chess 960, Atomic chess, Antichess, and Horde chess. We perform our regressions on several years of data from Lichess, the free and open-source internet chess server. We use the published player ratings to control for the confounding effect of differential player skill. We adjust for the attenuation bias in regressions due to the noise in observed ratings. We find that major piece values, relative to the value of a pawn, are fairly consistent with historical valuation systems. However we find slightly higher value to bishops than knights. We find that piece values are smaller, in absolute value, in Atomic and Antichess than standard chess. We also present approximate values of the pieces to equalize odds when players of varying skill face off. We briefly consider self-play experiments using the Stockfish engine, which give a contrasting view of piece value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04691v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Steven Pav</dc:creator>
    </item>
    <item>
      <title>Contextual Combinatorial Bandits with Changing Action Sets via Gaussian Processes</title>
      <link>https://arxiv.org/abs/2110.02248</link>
      <description>arXiv:2110.02248v3 Announce Type: replace-cross 
Abstract: We consider a contextual bandit problem with a combinatorial action set and time-varying base arm availability. At the beginning of each round, the agent observes the set of available base arms and their contexts and then selects an action that is a feasible subset of the set of available base arms to maximize its cumulative reward in the long run. We assume that the mean outcomes of base arms are samples from a Gaussian Process (GP) indexed by the context set ${\cal X}$, and the expected reward is Lipschitz continuous in expected base arm outcomes. For this setup, we propose an algorithm called Optimistic Combinatorial Learning and Optimization with Kernel Upper Confidence Bounds (O'CLOK-UCB) and prove that it incurs $\tilde{O}(\sqrt{\lambda^*(K)KT\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)} )$ regret with high probability, where $\gamma_{KT}(\cup_{t\leq T}\mathcal{X}_t)$ is the maximum information gain associated with the sets of base arm contexts $\mathcal{X}_t$ that appeared in the first $T$ rounds, $K$ is the maximum cardinality of any feasible action over all rounds, and $\lambda^*(K)$ is the maximum eigenvalue of all covariance matrices of selected actions up to time $T$, which is a function of $K$. To dramatically speed up the algorithm, we also propose a variant of O'CLOK-UCB that uses sparse GPs. Finally, we experimentally show that both algorithms exploit inter-base arm outcome correlation and vastly outperform the previous state-of-the-art UCB-based algorithms in realistic setups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.02248v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andi Nika, Sepehr Elahi, Cem Tekin</dc:creator>
    </item>
    <item>
      <title>Modeling temporal hypergraphs</title>
      <link>https://arxiv.org/abs/2506.01408</link>
      <description>arXiv:2506.01408v2 Announce Type: replace-cross 
Abstract: Networks representing social, biological, technological or other systems are often characterized by higher-order interaction involving any number of nodes. Temporal hypergraphs are given by ordered sequences of hyperedges representing sets of nodes interacting at given points in time. In this paper we discuss how a recently proposed model family for time-stamped hyperedges - relational hyperevent models (RHEM) - can be employed to define tailored null distributions for temporal hypergraphs and to test and control for complex dependencies in hypergraph dynamics. RHEM can be specified with a given vector of temporal hyperedge statistics - functions that quantify the structural position of hyperedges in the history of previous hyperedges - and equate expected values of these statistics with their empirically observed values. This allows, for instance, to analyze the overrepresentation or underrepresentation of temporal hyperedge configurations in a model that reproduces the observed distributions of possibly complex sub-configurations, including but going beyond node degrees. Concrete examples include, but are not limited to, preferential attachment, repetition of subsets of any given size, triadic closure, homophily, and degree assortativity for subsets of any order.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01408v2</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\"urgen Lerner, Marian-Gabriel H\^ancean, Matjaz Perc</dc:creator>
    </item>
    <item>
      <title>Two-Phase Treatment with Noncompliance: Identifying the Cumulative Average Treatment Effect via Multisite Instrumental Variables</title>
      <link>https://arxiv.org/abs/2506.03104</link>
      <description>arXiv:2506.03104v2 Announce Type: replace-cross 
Abstract: When evaluating a two-phase intervention, the cumulative average treatment effect (ATE) is often the primary causal estimand of interest. However, some individuals who do not respond well to the Phase I treatment may subsequently display noncompliant behaviours. At the same time, exposure to the Phase I treatment is expected to directly influence an individual's potential outcomes, thereby violating the exclusion restriction. Building on an instrumental variable (IV) strategy for multisite trials, we clarify the conditions under which the cumulative ATE of a two-phase treatment can be identified by employing the random assignment of the Phase I treatment as the instrument. Our strategy relaxes both the conventional exclusion restriction and sequential ignorability assumptions. We assess the performance of the new strategy through simulation studies. Additionally, we reanalyze data from the Tennessee class size study, in which students and teachers were randomly assigned to either small or regular class types in kindergarten (Phase I) with noncompliance emerging in Grade 1 (Phase II). Applying our new strategy, we estimate the cumulative ATE of receiving two consecutive years of instruction in a small versus regular class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03104v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guanglei Hong, Xu Qin, Zhengyan Xu, Fan Yang</dc:creator>
    </item>
    <item>
      <title>CopulaSMOTE: A Copula-Based Oversampling Approach for Imbalanced Classification in Diabetes Prediction</title>
      <link>https://arxiv.org/abs/2506.17326</link>
      <description>arXiv:2506.17326v2 Announce Type: replace-cross 
Abstract: Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people are affected by it. Early detection can significantly lower this risk. Despite significant advancements in machine learning for identifying diabetic cases, results can still be influenced by the imbalanced nature of the data. To address this challenge, our study considered copula-based data augmentation, which preserves the dependency structure when generating data for the minority class and integrates it with machine learning (ML) techniques. We selected the Pima Indian dataset and generated data using A2 copula, then applied five machine learning algorithms: logistic regression, random forest, gradient boosting, extreme gradient boosting, and Multilayer Perceptron. Overall, our findings show that Random Forest with A2 copula oversampling (theta = 10) achieved the best performance, with improvements of 5.3% in accuracy, 9.5% in precision, 5.7% in recall, 7.6% in F1-score, and 1.1% in AUC compared to the standard SMOTE method. Furthermore, we statistically validated our results using the McNemar's test. This research represents the first known use of A2 copulas for data augmentation and serves as an alternative to the SMOTE technique, highlighting the efficacy of copulas as a statistical method in machine learning applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17326v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agnideep Aich, Md Monzur Murshed, Sameera Hewage, Amanda Mayeaux</dc:creator>
    </item>
    <item>
      <title>Clustering methods for Categorical Time Series and Sequences : A scoping review</title>
      <link>https://arxiv.org/abs/2509.07885</link>
      <description>arXiv:2509.07885v3 Announce Type: replace-cross 
Abstract: Objective: To provide an overview of clustering methods for categorical time series (CTS), a data structure commonly found in epidemiology, sociology, biology, and marketing, and to support method selection in regards to data characteristics.
  Methods: We searched PubMed, Web of Science, and Google Scholar, from inception up to November 2024 to identify articles that propose and evaluate clustering techniques for CTS. Methods were classified according to three major families -- distance-based, feature-based, and model-based -- and assessed on their ability to handle data challenges such as variable sequence length, multivariate data, continuous time, missing data, time-invariant covariates, and large data volumes.
  Results: Out of 14607 studies, we included 124 articles describing 129 methods, spanning domains such as artificial intelligence, social sciences, and epidemiology. Distance-based methods, particularly those using Optimal Matching, were most prevalent, with 56 methods. We identified 28 model-based methods, which demonstrated superior flexibility for handling complex data structures such as multivariate data, continuous time and time-invariant covariates. We also recorded 45 feature-based approaches, which were on average more scalable but less flexible. A searchable Web application was developed to facilitate method selection based on dataset characteristics ( https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/ )
  Discussion: While distance-based methods dominate, model-based approaches offer the richest modeling potential but are less scalable. Feature-based methods favor performance over flexibility, with limited support for complex data structures.
  Conclusion: This review highlights methodological diversity and gaps in CTS clustering. The proposed typology aims to guide researchers in selecting methods for their specific use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07885v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ottavio Khalifa, Viet-Thi Tran, Alan Balendran, Fran\c{c}ois Petit</dc:creator>
    </item>
  </channel>
</rss>
