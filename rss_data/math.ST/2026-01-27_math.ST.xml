<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Jan 2026 05:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Tighter confidence intervals for quantiles of heterogeneous data</title>
      <link>https://arxiv.org/abs/2601.17302</link>
      <description>arXiv:2601.17302v1 Announce Type: new 
Abstract: It is well known that the asymptotic variance of sample quantiles can be reduced under heterogeneity relative to the i.i.d. setting. However, asymptotically correct confidence intervals for quantiles are not yet available. We propose a novel, consistent estimator of the reduced asymptotic variance arising when quantiles are computed from groups of observations, leading to asymptotically correct confidence intervals. Simulation studies show that our confidence intervals are substantially shorter than those in the i.i.d. case and attain nearly correct coverage across a wide range of heterogeneous settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17302v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John H. J. Einmahl, Yi He</dc:creator>
    </item>
    <item>
      <title>A new stochastic dominance criterion for dependent random variables with applications</title>
      <link>https://arxiv.org/abs/2601.17511</link>
      <description>arXiv:2601.17511v1 Announce Type: new 
Abstract: In this paper we develop a new tool for the comparison of paired data based on a new criterion of stochastic dominance that takes into account the dependence structure of the random variables under comparison. This new procedure provides a more detailed comparison of dependent random variables and overcomes some difficulties of standard techniques like Student's t and Wilcoxon-Mann-Whitney tests for non normal data. This tool provides an alternative to the usual stochastic dominance criterion which only considers the marginal distributions in the comparison. We show how this new tool can be fruitfully used for the comparison of paired asset returns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17511v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.insmatheco.2022.12.002</arxiv:DOI>
      <arxiv:journal_reference>Insurance: Mathematics and Economics (2023), 108, 165-176</arxiv:journal_reference>
      <dc:creator>F. Belzunce, C. Mart\'inez-Riquelme</dc:creator>
    </item>
    <item>
      <title>Comparisons of policies based on relevation and replacement by a new one unit in reliability</title>
      <link>https://arxiv.org/abs/2601.17518</link>
      <description>arXiv:2601.17518v1 Announce Type: new 
Abstract: The purpose of this paper is to study the role of the relevation transform, where a failed unit is replaced by a used unit with the same age as the failed one, as an alternative to the policy based on the replacement by a new one. In particular, we compare the stochastic processes arising from a policy based on the replacement of a failed unit by a new one and from the one in which the unit is being continuously subjected to a relevation policy. The comparisons depend on the aging properties of the units under repair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17518v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11749-020-00710-6</arxiv:DOI>
      <arxiv:journal_reference>TEST (2021), 30, 211-227</arxiv:journal_reference>
      <dc:creator> Belzunce,  F.,  Mart\'inez-Riquelme,  C.,  Mercader, J. A.,  Ruiz, J. M</dc:creator>
    </item>
    <item>
      <title>Directional footrule-coefficients</title>
      <link>https://arxiv.org/abs/2601.17565</link>
      <description>arXiv:2601.17565v1 Announce Type: new 
Abstract: Rank-based dependence measures such as Spearman's footrule are robust and invariant, but they often fail to capture directional or asymmetric dependence in multivariate settings. This paper introduces a new family of directional Spearman's footrule coefficients for multivariate data, defined within the copula framework to clearly separate marginal behavior from dependence structure. We establish their main theoretical properties, showing full consistency with the classical footrule, including behavior under independence and extreme dependence, as well as symmetry and reflection properties. Nonparametric rank-based estimators are proposed and their asymptotic consistency is discussed. Explicit expressions for several known families of copulas illustrate the ability of the proposed coefficients to detect directional dependence patterns undetected by classical measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17565v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Enrique de Amo, David Garc\'ia-Fern\'andez, Manuel \'Ubeda-Flores</dc:creator>
    </item>
    <item>
      <title>Event history analysis with time-dependent covariates via landmarking supermodels and boosted trees</title>
      <link>https://arxiv.org/abs/2601.17605</link>
      <description>arXiv:2601.17605v1 Announce Type: new 
Abstract: We propose a nonparametric method for dynamic prediction in event history analysis with high-dimensional, time-dependent covariates. The approach estimates future conditional hazards by combining landmarking supermodels with gradient boosted trees. Unlike joint modeling or Cox landmarking models, the proposed estimator flexibly captures interactions and nonlinear effects without imposing restrictive parametric assumptions or requiring the covariate process to be Markovian. We formulate the approach as a sieve M-estimator and establish weak consistency. Computationally, the problem reduces to a Poisson regression, allowing implementation via standard gradient boosting software. A key theoretical advantage is that the method avoids the temporal inconsistencies that arise in landmark Cox models. Simulation studies demonstrate that the method performs well in a variety of settings, and its practical value is illustrated through an analysis of primary biliary cirrhosis data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17605v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Lunding Sandqvist</dc:creator>
    </item>
    <item>
      <title>On the contraction rate of the posterior distribution for nonlinear PDE parameter identification</title>
      <link>https://arxiv.org/abs/2601.17805</link>
      <description>arXiv:2601.17805v1 Announce Type: new 
Abstract: In this work, we investigate the estimation of a parameter $f$ in PDEs using Bayesian procedures, and focus on posterior distributions constructed using Gaussian process priors, and its variational approximation. We establish contraction rates for the posterior distribution and the variational approximation in the regime of low-regularity parameters. The main novelty of the study lies in relaxing the condition that the ground truth parameter must lie in the reproducing kernel Hilbert space of the Gaussian process prior, which is commonly imposed in existing studies on posterior contraction rate analysis [14,40,44]. The analysis relies on a delicate approximation argument that suitably balances various error sources. We illustrate the general theory on three nonlinear inverse problems for PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17805v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuxin Fan, Bangti Jin</dc:creator>
    </item>
    <item>
      <title>The Hellinger Bounds on the Kullback-Leibler Divergence and the Bernstein Norm</title>
      <link>https://arxiv.org/abs/2601.17860</link>
      <description>arXiv:2601.17860v1 Announce Type: new 
Abstract: The Kullback-Leibler divergence, the Kullback-Leibler variation, and the Bernstein "norm" are used to quantify discrepancies among probability distributions in likelihood models such as nonparametric maximum likelihood and nonparametric Bayes. They are closely related to the Hellinger distance, which is often easier to work with. Consequently, it is of interest to characterize conditions under which the Hellinger distance serves as an upper bound for these measures. This article characterizes a necessary and sufficient condition for each of the discrepancy measures to be bounded by the Hellinger distance. It accommodates unbounded likelihood ratios and generalizes all previously known results. We then apply it to relax the regularity condition for the sieve maximum likelihood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17860v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Kaji</dc:creator>
    </item>
    <item>
      <title>Asymptotic properties of multivariate Sz\'{a}sz-Mirakyan distribution estimators on the nonnegative orthant</title>
      <link>https://arxiv.org/abs/2601.18178</link>
      <description>arXiv:2601.18178v1 Announce Type: new 
Abstract: The asymptotic properties of multivariate Sz\'{a}sz-Mirakyan estimators for distribution functions supported on the nonnegative orthant are investigated. Explicit bias and variance expansions are derived on compact subsets of the interior, yielding sharp mean squared error characterizations and optimal smoothing rates. The analysis shows that the proposed Poisson smoothing yields a non-negligible variance reduction relative to the empirical distribution function, leading to asymptotic efficiency gains that can be quantified through local and global deficiency measures. The behavior of the estimator near the boundary of its support is examined separately. Under a boundary-layer scaling that preserves nondegenerate Poisson smoothing as the evaluation point approaches the boundary of $[0,\infty)^d$, bias and variance expansions are obtained that differ fundamentally from those in the interior region. In particular, the variance reduction mechanism disappears at leading order, implying that no asymptotically optimal smoothing parameter exists in the boundary regime. Central limit theorems and almost sure uniform consistency are also established. Together, these results provide a unified asymptotic theory for multivariate Sz\'{a}sz-Mirakyan distribution estimation and clarify the distinct roles of smoothing in the interior and boundary regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18178v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanjie Lyu, Fr\'ed\'eric Ouimet, Cindy Feng</dc:creator>
    </item>
    <item>
      <title>Nonparametric inference for spot volatility in pure-jump semimartingales</title>
      <link>https://arxiv.org/abs/2601.18371</link>
      <description>arXiv:2601.18371v1 Announce Type: new 
Abstract: We provide a comprehensive analysis of spot volatility inference in pure-jump semimartingales under two asymptotic settings: fixed-$k$, where each local window uses a fixed number of observations, and large-$k$, where this number grows with sampling frequency. For both active- and possibly inactive-jump settings, we derive generally nonstandard, typically non-Gaussian limit distributions and establish valid inference, including when the jump-activity index is consistently estimated. Simulations show that fixed-$k$ asymptotics offer markedly better finite-sample accuracy, underscoring their practical advantage for nonparametric spot volatility inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18371v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengxin Yan, Dachuan Chen, Jia Li</dc:creator>
    </item>
    <item>
      <title>Function estimation in the empirical Bayes setting</title>
      <link>https://arxiv.org/abs/2601.18689</link>
      <description>arXiv:2601.18689v1 Announce Type: new 
Abstract: We study function estimation in the empirical Bayes setting for Poisson and normal means. Specifically, given observations $Y_i\sim f(\cdot; \theta_i)$ with latent parameters $\theta_i\sim \pi$, the goal is to estimate $\mathbb{E}_{\pi}[\ell(\theta)|X = x]$. This task lies between classical deconvolution (recovering the full prior $\pi$), and standard empirical Bayes mean estimation. While the minimax risk for estimating $\pi$ in the Wasserstein distance is known to decay only logarithmically, we show that estimating certain smooth functions admits dramatically faster rates. In particular, for polynomial functions of degree $k$ in the Poisson model, we establish a tight bound of $\Theta(\frac{1}{n}(\frac{\log n}{\log \log n})^{k+1})$ and $\Theta(\frac{1}{n}(\log n)^{2k+1})$ for bounded and subexponential priors, respectively, attainable by estimators mimicking those that achieve optimal regret for the mean estimation problem (Robbins, mininum distance, ERM). Our analysis identifies the approximation-theoretic origin of this improvement: smooth functions can be well-approximated by low-degree polynomials, whereas Lipschitz functions require dense polynomial approximations, incurring a $\frac{1}{k}$ loss for degree $k$ polynomial approximation. The results reveal a sharp hierarchy in the difficulty of empirical Bayes problems: ranging from slow, logarithmic deconvolution to near-parametric convergence for smooth posterior functionals, and establish new connections between nonparametric empirical Bayes theory, polynomial approximation, and statistical inverse problems. Finally, we complement our analysis with a lower bound of $\Omega(\frac 1n (\frac{\log n}{\log \log n})^{k+1})$ (bounded priors) and $\Omega(\frac 1n (\log n)^{k + 1})$ (subgaussian priors) for the normal means model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18689v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Kang, Yury Polyanskiy, Anzo Teh</dc:creator>
    </item>
    <item>
      <title>Equality between two general ridge estimators and applications in several linear models</title>
      <link>https://arxiv.org/abs/2601.18770</link>
      <description>arXiv:2601.18770v1 Announce Type: new 
Abstract: General ridge estimators are widely used in the general linear model because they possess desirable properties such as linear sufficiency and linear admissibility. However, when the covariance matrix of the error term is partially unknown, estimation typically requires a two-step procedure. This paper derives conditions under which the general ridge estimator based on the covariance matrix coincides with the one that does not depend on it. In particular, we provide practically verifiable conditions for several linear models, including Rao's mixed-effects model, a seemingly unrelated regression model, first-order spatial autoregressive and spatial moving average models, and serial correlation models. These results enable the use of a covariance-free general ridge estimator, thereby simplifying the two-step estimation procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18770v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hirai Mukasa</dc:creator>
    </item>
    <item>
      <title>Optimal Design under Interference, Homophily, and Robustness Trade-offs</title>
      <link>https://arxiv.org/abs/2601.17145</link>
      <description>arXiv:2601.17145v1 Announce Type: cross 
Abstract: To minimize the mean squared error (MSE) in global average treatment effect (GATE) estimation under network interference, a popular approach is to use a cluster-randomized design. However, in the presence of homophily, which is common in social networks, cluster randomization can instead increase the MSE. We develop a novel potential outcomes model that accounts for interference, homophily, and heterogeneous variation. In this setting, we establish a framework for optimizing designs for worst-case MSE under the Horvitz-Thompson estimator. This leads to an optimization problem over the covariance matrices of the treatment assignment, trading off interference, homophily, and robustness. We frame and solve this problem using two complementary approaches. The first involves formulating a semidefinite program (SDP) and employing Gaussian rounding, in the spirit of the Goemans-Williamson approximation algorithm for MAXCUT. The second is an adaptation of the Gram-Schmidt Walk, a vector-balancing algorithm which has recently received much attention. Finally, we evaluate the performance of our designs through various experiments on simulated network data and a real village network dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17145v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vydhourie Thiyageswaran, Alex Kokot, Jennifer Brennan, Marina Meila, Christina Lee Yu, Maryam Fazel</dc:creator>
    </item>
    <item>
      <title>Sufficient conditions for some transform orders based on the quantile density ratio</title>
      <link>https://arxiv.org/abs/2601.17501</link>
      <description>arXiv:2601.17501v1 Announce Type: cross 
Abstract: In this paper we focus on providing sufficient conditions for some transform orders for which the quantile densities ratio is non-monotone and, therefore, the convex transform order does not hold. These results are interesting for comparing random variables with a non-explicit expression of their quantile functions or they are computationally complex. In addition, the main results are applied to compare two Tukey generalized distributed random variables and to establish new relationships among non-monotone and positive aging notions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17501v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11009-019-09740-6</arxiv:DOI>
      <arxiv:journal_reference>Methodology and Computing in Applied Probability (2021), 23, 29-52</arxiv:journal_reference>
      <dc:creator>A. Arriaza, F. Belzunce, C. Mart\'inez-Riquelme</dc:creator>
    </item>
    <item>
      <title>Exact Recovery in the Geometric Hidden Community Model</title>
      <link>https://arxiv.org/abs/2601.17591</link>
      <description>arXiv:2601.17591v1 Announce Type: cross 
Abstract: Hidden community problems, such as community detection in the Stochastic Block Model (SBM), submatrix localization, and $\mathbb{Z}_2$ synchronization, have received considerable attention in the probability, statistics, and information-theory literature. Motivated by transitive behavior in social networks, which tend to exhibit high triangle density, recent works have considered hidden community models in spatially-embedded networks. In particular, Baccelli and Sankararaman proposed the Geometric SBM, a spatially-embedded analogue of the standard SBM with dramatically more triangles. In this paper, we consider the problem of exact recovery for the Geometric Hidden Community Model (GHCM) of Gaudio, Guan, Niu, and Wei, which generalizes the Geometric SBM to allow for arbitrary pairwise observation distributions. Under mild technical assumptions, we find the information-theoretic threshold for exact recovery in the ``distance-dependent'' GHCM, which allows the pairwise distributions to depend on distance as well as community labels, thus completing the picture of exact recovery in spatially-embedded hidden community models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17591v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Gaudio, Andrew Jin</dc:creator>
    </item>
    <item>
      <title>A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization</title>
      <link>https://arxiv.org/abs/2601.17646</link>
      <description>arXiv:2601.17646v1 Announce Type: cross 
Abstract: Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlev\'e-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17646v1</guid>
      <category>cs.LG</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karim Bounja, Lahcen Laayouni, Abdeljalil Sakat</dc:creator>
    </item>
    <item>
      <title>Functional Large Deviations for Wide Deep Neural Networks with Gaussian Initialization and Lipschitz Activations</title>
      <link>https://arxiv.org/abs/2601.18276</link>
      <description>arXiv:2601.18276v1 Announce Type: cross 
Abstract: We establish a functional large deviation principle for fully connected multi-layer perceptrons with i.i.d. Gaussian weights (LeCun initialization) and general Lipschitz activation functions, including therefore the popular case of ReLU. The large deviation principle holds for the entire network output process on any compact input set. The proof combines exponential tightness for recursively defined processes, finite-dimensional large deviations, and the Dawson-G\"artner theorem, extending existing results beyond finite input sets and less general activations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18276v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Claudio Macci, Barbara Pacchiarotti, Katerina Papagiannouli, Giovanni Luca Torrisi, Dario Trevisan</dc:creator>
    </item>
    <item>
      <title>A necessary and sufficient condition for convergence in distribution of the P-P process in $L^1[0,1]$</title>
      <link>https://arxiv.org/abs/2601.18390</link>
      <description>arXiv:2601.18390v1 Announce Type: cross 
Abstract: We establish that the procentile-procentile (P-P) process constructed from a random sample of pairs converges in distribution in $L^1[0,1]$ if and only if the P-P curve is absolutely continuous. If the P-P process converges in distribution then it may be approximated using the bootstrap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18390v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendan K. Beare, Tetsuya Kaji</dc:creator>
    </item>
    <item>
      <title>Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data</title>
      <link>https://arxiv.org/abs/2601.18728</link>
      <description>arXiv:2601.18728v1 Announce Type: cross 
Abstract: Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18728v1</guid>
      <category>cs.LG</category>
      <category>math.DG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Willem Diepeveen, Oscar Leong</dc:creator>
    </item>
    <item>
      <title>Semiparametric Efficient Empirical Higher Order Influence Function Estimators</title>
      <link>https://arxiv.org/abs/1705.07577</link>
      <description>arXiv:1705.07577v5 Announce Type: replace 
Abstract: Robins et al. (2008, 2017) applied the theory of higher order influence functions (HOIFs) to derive an estimator of the mean $\psi$ of an outcome Y in a missing data model with Y missing at random conditional on a vector X of continuous covariates; their estimator, in contrast to previous estimators, is semiparametric efficient under the minimal conditions of Robins et al. (2009b), together with an additional (non-minimal) smoothness condition on the density g of X, because the Robins et al. (2008, 2017) estimator depends on a nonparametric estimate of g. In this paper, we introduce a new HOIF estimator that has the same asymptotic properties as the original one, but does not impose any smoothness requirement on g. This is important for two reasons. First, one rarely has the knowledge about the properties of g. Second, even when g is smooth, if the dimension of X is even moderate, accurate nonparametric estimation of its density is not feasible at the sample sizes often encountered in applications. In fact, to the best of our knowledge, this new HOIF estimator remains the only semiparametric efficient estimator of $\psi$ under minimal conditions, despite the rapidly growing literature on causal effect estimation. We also show that our estimator can be generalized to the entire class of functionals considered by Robins et al. (2008) which include the average effect of a treatment on a response Y when a vector X suffices to control confounding and the expected conditional variance of a response Y given a vector X. Simulation experiments are also conducted, which demonstrate that our new estimator outperforms those of Robins et al. (2008, 2017) in finite samples, when g is not very smooth.</description>
      <guid isPermaLink="false">oai:arXiv.org:1705.07577v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Liu, Rajarshi Mukherjee, Whitney K. Newey, James M. Robins</dc:creator>
    </item>
    <item>
      <title>Contrastive independent component analysis</title>
      <link>https://arxiv.org/abs/2407.02357</link>
      <description>arXiv:2407.02357v3 Announce Type: replace 
Abstract: In recent years, there has been growing interest in jointly analyzing a foreground dataset, representing an experimental group, and a background dataset, representing a control group. The goal of such contrastive investigations is to identify salient features in the experimental group relative to the control. Independent component analysis (ICA) is a powerful tool for learning independent patterns in a dataset. We generalize it to contrastive ICA (cICA). For this purpose, we devise a new linear algebra based tensor decomposition algorithm, which is more expressive but just as efficient and identifiable as other linear algebra based algorithms. We establish the identifiability of cICA and demonstrate its performance in finding patterns and visualizing data, using synthetic, semi-synthetic, and real-world datasets, comparing the approach to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02357v3</guid>
      <category>math.ST</category>
      <category>math.AG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kexin Wang, Aida Maraj, Anna Seigal</dc:creator>
    </item>
    <item>
      <title>ARMAr-LASSO: Mitigating the Impact of Predictor Serial Correlation on the LASSO</title>
      <link>https://arxiv.org/abs/2408.09288</link>
      <description>arXiv:2408.09288v3 Announce Type: replace 
Abstract: We explore estimation and forecast accuracy for sparse linear models, focusing on scenarios where both predictors and errors carry serial correlations. We establish a clear link between predictor serial correlation and the performance of the LASSO, showing that even orthogonal or weakly correlated stationary AR processes can lead to significant spurious correlations due to their serial correlations. To address this challenge, we propose a novel approach named ARMAr-LASSO ({\em ARMA residuals LASSO}), which applies the LASSO to predictors that have been pre-whitened with ARMA filters and lags of dependent variable. We derive both asymptotic results and oracle inequalities for the ARMAr-LASSO, demonstrating that it effectively reduces estimation errors while also providing an effective forecasting and feature selection strategy. Our findings are supported by extensive simulations and an application to real-world macroeconomic data, which highlight the superior performance of the ARMAr-LASSO for handling sparse linear models in the context of time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09288v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Tonini (L'EMbeDS and Institute of Economics, Sant'Anna School of Advanced Studies, Pisa), Francesca Chiaromonte (L'EMbeDS and Institute of Economics, Sant'Anna School of Advanced Studies, Pisa, Dept. of Statistics, The Pennsylvania State University), Alessandro Giovannelli (University of L'Aquila)</dc:creator>
    </item>
    <item>
      <title>A necessary and sufficient condition for convergence in distribution of the quantile process in $L^1(0,1)$</title>
      <link>https://arxiv.org/abs/2502.01254</link>
      <description>arXiv:2502.01254v3 Announce Type: replace 
Abstract: We establish a necessary and sufficient condition for the quantile process based on iid sampling to converge in distribution in $L^1(0,1)$. The condition is that the quantile function is locally absolutely continuous and satisfies a slight strengthening of square integrability. If the quantile process converges in distribution then it may be approximated using the bootstrap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01254v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendan K. Beare, Tetsuya Kaji</dc:creator>
    </item>
    <item>
      <title>Implicit vs. explicit regularization for high-dimensional gradient descent</title>
      <link>https://arxiv.org/abs/2502.10578</link>
      <description>arXiv:2502.10578v2 Announce Type: replace 
Abstract: In this paper we investigate the generalization error of gradient descent (GD) applied to an $\ell_2$-regularized OLS objective function in the linear model. Based on our analysis we develop new methodology for computationally tractable and statistically efficient linear prediction in a high-dimensional and massive data scenario (large-$n$, large-$p$). Our results are based on the surprising observation that the generalization error of optimally tuned regularized gradient descent approaches that of an optimal benchmark procedure $monotonically$ in the iteration number $t$. On the other hand standard GD for OLS (without explicit regularization) can achieve the benchmark only in degenerate cases. This shows that (optimal) explicit regularization can be nearly statistically efficient (for large $t$) whereas implicit regularization by (optimal) early stopping can not.
  To complete our methodology, we provide a fully data driven and computationally tractable choice of the $\ell_2$ regularization parameter $\lambda$ that is computationally cheaper than cross-validation. On this way, we follow and extend ideas of Dicker (2014) to the non-gaussian case, which requires new results on high-dimensional sample covariance matrices that might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10578v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Stark, Lukas Steinberger</dc:creator>
    </item>
    <item>
      <title>Minimum Variance Designs With Constrained Maximum Bias</title>
      <link>https://arxiv.org/abs/2512.21806</link>
      <description>arXiv:2512.21806v4 Announce Type: replace 
Abstract: Designs which are minimax in the presence of model misspecifications have been constructed so as to minimize the maximum, over classes of alternate response models, of the integrated mean squared error of the predicted values. This mean squared error decomposes into a term arising solely from variation, and a bias term arising from the model errors. Here we consider the problem of designing so as to minimize the variance of the predictors, subject to a bound on the maximum (over model misspecifications) bias. We consider as well designing so as to minimize the maximum bias, subject to a bound on the variance. We show that solutions to both problems are given by the minimax designs, with appropriately chosen values of their tuning constants. Conversely, any minimax design solves each problem for an appropriate choice of the bound on the maximum bias or on the variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21806v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas P. Wiens</dc:creator>
    </item>
    <item>
      <title>A Generative Approach to Quasi-Random Sampling from Copulas via Space-Filling Designs</title>
      <link>https://arxiv.org/abs/2403.05281</link>
      <description>arXiv:2403.05281v2 Announce Type: replace-cross 
Abstract: Exploring the dependence between covariates across distributions is crucial for many applications. Copulas serve as a powerful tool for modeling joint variable dependencies and have been effectively applied in various practical contexts due to their intuitive properties. However, existing computational methods lack the capability for feasible inference and sampling of any copula, preventing their widespread use. This paper introduces an innovative quasi-random sampling approach for copulas, utilizing generative adversarial networks (GANs) and space-filling designs. The proposed framework constructs a direct mapping from low-dimensional uniform distributions to high-dimensional copula structures using GANs, and generates quasi-random samples for any copula structure from points set of space-filling designs. In the high-dimensional situations with limited data, the proposed approach significantly enhances sampling accuracy and computational efficiency compared to existing methods. Additionally, we develop convergence rate theory for quasi-Monte Carlo estimators, providing rigorous upper bounds for bias and variance. Both simulated experiments and practical implementations, particularly in risk management, validate the proposed method and showcase its superiority over existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05281v2</guid>
      <category>stat.ML</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sumin Wang, Chenxian Huang, Yongdao Zhou, Min-Qian Liu</dc:creator>
    </item>
    <item>
      <title>Highly Multivariate Large-scale Spatial Stochastic Processes -- A Cross-Markov Random Field Approach</title>
      <link>https://arxiv.org/abs/2408.10396</link>
      <description>arXiv:2408.10396v3 Announce Type: replace-cross 
Abstract: Key challenges in the analysis of highly multivariate large-scale spatial stochastic processes, where both the number of components (p) and spatial locations (n) can be large, include achieving maximal sparsity in the joint precision matrix, ensuring efficient computational cost for its generation, accommodating asymmetric cross-covariance in the joint covariance matrix, and delivering scientific interpretability. We propose a cross-MRF model class, consisting of a mixed spatial graphical model framework and cross-MRF theory, to collectively address these challenges in one unified framework across two modelling stages. The first stage exploits scientifically informed conditional independence (CI) among p component fields and allows for a step-wise parallel generation of joint covariance and precision matrix, enabling a simultaneous accommodation of asymmetric cross-covariance in joint covariance matrix and sparsity in joint precision matrix. The second stage extends the first-stage CI to doubly CI among both p and n and unearths the cross-MRF via an extended Hammersley-Clifford theorem for multivariate spatial stochastic processes. This results in the sparsest possible representation of the joint precision matrix and ensures its lowest generation complexity. We demonstrate with 1D simulated comparative studies and 2D real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10396v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoqing Chen, Peter Diggle, James V. Zidek, Gavin Shaddick</dc:creator>
    </item>
    <item>
      <title>Bias-Aware Conformal Prediction for Metric-Based Imaging Pipelines</title>
      <link>https://arxiv.org/abs/2410.05263</link>
      <description>arXiv:2410.05263v2 Announce Type: replace-cross 
Abstract: Reliable confidence measures of metrics derived from medical imaging reconstruction pipelines would improve the standard of decision-making in many clinical workflows. Conformal Prediction (CP) provides a robust framework for producing calibrated prediction intervals, but standard CP formulations face a critical challenge in the imaging pipeline: common mismatches between image reconstruction objectives and downstream metrics can introduce systematic prediction deviations from ground truth values, known as bias. These biases in turn compromise the efficiency of prediction intervals, which is a problem that has been unexplored in the CP literature. In this study, we formalize the behavior of symmetric (where bounds expand equally in both directions) and asymmetric (where bounds expand unequally) formulations for common non-conformity scores in CP in the presence of bias, and argue that this measurable bias must inform the choice of CP formulation. We theoretically and empirically demonstrate that symmetric intervals are inflated by a factor of two times the magnitude of bias while asymmetric intervals remain unaffected by bias, and provide conditions under which each formulation produces tighter intervals. We empirically validated our theoretical analyses on sparse-view CT reconstruction for downstream radiotherapy planning. Our work enables users of medical imaging pipelines to proactively select optimal CP formulations, thereby improving interval length efficiency for critical downstream metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05263v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matt Y. Cheung, Tucker J. Netherton, Laurence E. Court, Ashok Veeraraghavan, Guha Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Doubly Robust and Efficient Calibration of Prediction Sets for Right-Censored Time-to-Event Outcomes</title>
      <link>https://arxiv.org/abs/2501.04615</link>
      <description>arXiv:2501.04615v3 Announce Type: replace-cross 
Abstract: Our objective is to construct well-calibrated prediction sets for a time-to-event outcome subject to right-censoring with guaranteed coverage. Inspired by modern conformal inference, our approach avoids the need for a well-specified parametric or semiparametric survival model. Unlike existing conformal methods for survival data, which assume Type-I censoring with fully observed censoring times, we consider the more common right-censoring setting in which only the censoring time or only the event time is observed, whichever comes first. Under a standard conditional independence censoring condition, we propose and analyze several lower prediction bounds for the survival time of a future observation, including inverse-probability-of-censoring weighting, and its augmented version based on the semiparametric efficient influence function for the relevant marginal quantile of the outcome accounting for dependent censoring. We formally establish asymptotic coverage guarantees of the proposed methods, and demonstrate both theoretically and through empirical experiments, that the augmented approach substantially improves efficiency over all other proposed methods. Specifically, its coverage error bound is doubly robust, and therefore of second order, thus ensuring that it is asymptotically negligible relative to the coverage error of the other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04615v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rebecca Farina, Eric J. Tchetgen Tchetgen, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>The Likelihood Correspondence</title>
      <link>https://arxiv.org/abs/2503.02536</link>
      <description>arXiv:2503.02536v3 Announce Type: replace-cross 
Abstract: An arrangement of hypersurfaces in projective space is strict normal crossing (SNC) if and only if its Euler discriminant is nonzero. We study the critical loci of arbitrary Laurent monomials in the equations of the smooth hypersurfaces. The family of these loci forms an irreducible variety in the product of two projective spaces, known in algebraic statistics as the likelihood correspondence and in particle physics as the scattering correspondence. We establish an explicit determinantal representation for the minimal generators of the bihomogeneous prime ideal that defines this variety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02536v3</guid>
      <category>math.AC</category>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Kahle, Hal Schenck, Bernd Sturmfels, Maximilian Wiesmann</dc:creator>
    </item>
    <item>
      <title>A Computational Transition for Detecting Multivariate Shuffled Linear Regression by Low-Degree Polynomials</title>
      <link>https://arxiv.org/abs/2504.03097</link>
      <description>arXiv:2504.03097v2 Announce Type: replace-cross 
Abstract: In this paper, we study the problem of multivariate shuffled linear regression, where the correspondence between predictors and responses in a linear model is obfuscated by a latent permutation. Specifically, we investigate the model $Y=\tfrac{1}{\sqrt{1+\sigma^2}}(\Pi_* X Q_* + \sigma Z)$, where $X$ is an $n*d$ standard Gaussian design matrix, $Z$ is an $n*m$ Gaussian noise matrix, $\Pi_*$ is an unknown $n*n$ permutation matrix, and $Q_*$ is an unknown $d*m$ on the Grassmanian manifold satisfying $Q_*^{\top} Q_* = \mathbb I_m$.
  Consider the hypothesis testing problem of distinguishing this model from the case where $X$ and $Y$ are independent Gaussian random matrices of sizes $n*d$ and $n*m$, respectively. Our results reveal a phase transition phenomenon in the performance of low-degree polynomial algorithms for this task. (1) When $m=o(d)$, we show that all degree-$D$ polynomials fail to distinguish these two models even when $\sigma=0$, provided with $D^4=o\big( \tfrac{d}{m} \big)$. (2) When $m=d$ and $\sigma=\omega(1)$, we show that all degree-$D$ polynomials fail to distinguish these two models provided with $D=o(\sigma)$. (3) When $m=d$ and $\sigma=o(1)$, we show that there exists a constant-degree polynomial that strongly distinguish these two models. These results establish a smooth transition in the effectiveness of low-degree polynomial algorithms for this problem, highlighting the interplay between the dimensions $m$ and $d$, the noise level $\sigma$, and the computational complexity of the testing task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03097v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Distributed Poisson multi-Bernoulli filtering via generalised covariance intersection</title>
      <link>https://arxiv.org/abs/2506.18397</link>
      <description>arXiv:2506.18397v2 Announce Type: replace-cross 
Abstract: This paper presents the distributed Poisson multi-Bernoulli (PMB) filter based on the generalised covariance intersection (GCI) fusion rule for distributed multi-object filtering. Since the exact GCI fusion of two PMB densities is intractable, we derive a principled approximation. Specifically, we approximate the power of a PMB density as an unnormalised PMB density, which corresponds to an upper bound of the PMB density. Then, the GCI fusion rule corresponds to the normalised product of two unnormalised PMB densities. We show that the result is a Poisson multi-Bernoulli mixture (PMBM), which can be expressed in closed form. Future prediction and update steps in each filter preserve the PMBM form, which can be projected back to a PMB density before the next fusion step. Experimental results show the benefits of this approach compared to other distributed multi-object filters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18397v2</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TSP.2026.3651805</arxiv:DOI>
      <arxiv:journal_reference>\'A. F. Garc\'ia-Fern\'andez and G. Battistelli, "Distributed Poisson Multi-Bernoulli Filtering via Generalized Covariance Intersection," in IEEE Transactions on Signal Processing, vol. 74, pp. 246-257, 2026</arxiv:journal_reference>
      <dc:creator>\'Angel F. Garc\'ia-Fern\'andez, Giorgio Battistelli</dc:creator>
    </item>
    <item>
      <title>Efficient Difference-in-Differences Estimation when Outcomes are Missing at Random</title>
      <link>https://arxiv.org/abs/2509.25009</link>
      <description>arXiv:2509.25009v2 Announce Type: replace-cross 
Abstract: The Difference-in-Differences (DiD) method is a fundamental tool for causal inference, yet its application is often complicated by missing data. Although recent work has developed robust DiD estimators for complex settings like staggered treatment adoption, these methods typically assume complete data and fail to address the critical challenge of outcomes that are missing at random (MAR) -- a common problem that invalidates standard estimators. We develop a rigorous framework, rooted in semiparametric theory, for identifying and efficiently estimating the Average Treatment Effect on the Treated (ATT) when either pre- or post-treatment (or both) outcomes are missing at random. We first establish nonparametric identification of the ATT under two minimal sets of sufficient conditions. For each, we derive the semiparametric efficiency bound, which provides a formal benchmark for asymptotic optimality. We then propose novel estimators that are asymptotically efficient, achieving this theoretical bound. A key feature of our estimators is their multiple robustness, which ensures consistency even if some nuisance function models are misspecified. We validate the properties of our estimators and showcase their broad applicability through an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25009v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Testa, Edward H. Kennedy, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>Set-valued data analysis for interlaboratory comparisons</title>
      <link>https://arxiv.org/abs/2510.23170</link>
      <description>arXiv:2510.23170v2 Announce Type: replace-cross 
Abstract: This article introduces tools to analyze set-valued data statistically. The tools were initially developed to analyze results from an interlaboratory comparison made by the Electromagnetic Compatibility Working Group of Eurolab France, where the goal was to select a consensual set of injection points on an electrical device. Families based on the Hamming-distance from a consensus set are introduced and Fisher's noncentral hypergeometric distribution is proposed to model the number of deviations. A Bayesian approach is used and two types of techniques are proposed for the inference. Hierarchical models are also considered to quantify a possible within-laboratory effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23170v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'ebastien Petit (LNE), S\'ebastien Marmin (LNE), Nicolas Fischer (LNE)</dc:creator>
    </item>
    <item>
      <title>Provable test-time adaptivity and distributional robustness of in-context learning</title>
      <link>https://arxiv.org/abs/2510.23254</link>
      <description>arXiv:2510.23254v2 Announce Type: replace-cross 
Abstract: We study in-context learning problems where a Transformer is pretrained on tasks drawn from a mixture distribution $\pi=\sum_{\alpha\in\mathcal{A}} \lambda_{\alpha} \pi_{\alpha}$, called the pretraining prior, in which each mixture component $\pi_{\alpha}$ is a distribution on tasks of a specific difficulty level indexed by $\alpha$. Our goal is to understand the performance of the pretrained Transformer when evaluated on a different test distribution $\mu$, consisting of tasks of fixed difficulty $\beta\in\mathcal{A}$, and with potential distribution shift relative to $\pi_\beta$, subject to the chi-squared divergence $\chi^2(\mu,\pi_{\beta})$ being at most $\kappa$. In particular, we consider nonparametric regression problems with random smoothness, and multi-index models with random smoothness as well as random effective dimension. We prove that a large Transformer pretrained on sufficient data achieves the optimal rate of convergence corresponding to the difficulty level $\beta$, uniformly over test distributions $\mu$ in the chi-squared divergence ball. Thus, the pretrained Transformer is able to achieve faster rates of convergence on easier tasks and is robust to distribution shift at test time. Finally, we prove that even if an estimator had access to the test distribution $\mu$, the convergence rate of its expected risk over $\mu$ could not be faster than that of our pretrained Transformers, thereby providing a more appropriate optimality guarantee than minimax lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23254v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Ma, Tengyao Wang, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Approximate full conformal prediction in an RKHS</title>
      <link>https://arxiv.org/abs/2601.13102</link>
      <description>arXiv:2601.13102v2 Announce Type: replace-cross 
Abstract: Full conformal prediction is a framework that implicitly formulates distribution-free confidence prediction regions for a wide range of estimators. However, a classical limitation of the full conformal framework is the computation of the confidence prediction regions, which is usually impossible since it requires training infinitely many estimators (for real-valued prediction for instance). The main purpose of the present work is to describe a generic strategy for designing a tight approximation to the full conformal prediction region that can be efficiently computed. Along with this approximate confidence region, a theoretical quantification of the tightness of this approximation is developed, depending on the smoothness assumptions on the loss and score functions. The new notion of thickness is introduced for quantifying the discrepancy between the approximate confidence region and the full conformal one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13102v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davidson Lova Razafindrakoto, Alain Celisse, J\'er\^ome Lacaille</dc:creator>
    </item>
    <item>
      <title>Entropy-Wasserstein regularization, defective local concentration and a cutoff criterion beyond non-negative curvature</title>
      <link>https://arxiv.org/abs/2601.13259</link>
      <description>arXiv:2601.13259v2 Announce Type: replace-cross 
Abstract: Notions of positive curvature have been shown to imply many remarkable properties for Markov processes, in terms, e.g., of regularization effects, functional inequalities, mixing time bounds and, more recently, the cutoff phenomenon. In this work, we are interested in a relaxed variant of Ollivier's coarse Ricci curvature, where a Markov kernel $P$ satisfies only a weaker Wasserstein bound $W_p(\mu P, \nu P) \leq K W_p(\mu,\nu)+M$ for constants $M\ge 0, K\in [0,1], p \ge 1$. Under appropriate additional assumptions on the one-step transition measures $\delta_x P$, we establish (i) a form of local concentration, given by a defective Talagrand inequality, and (ii) an entropy-transport regularization effect. We consider as illustrative examples the Langevin dynamics and the Proximal Sampler when the target measure is a log-Lipschitz perturbation of a log-concave measure. As an application of the above results, we derive criteria for the occurrence of the cutoff phenomenon in some negatively curved settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13259v2</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Pedrotti</dc:creator>
    </item>
    <item>
      <title>Additive-Functional Approach to Transport in Periodic and Tilted Periodic Potentials</title>
      <link>https://arxiv.org/abs/2601.13561</link>
      <description>arXiv:2601.13561v2 Announce Type: replace-cross 
Abstract: In this Letter, we clarify the physical origin of effective transport in periodic and tilted periodic systems. When Brownian dynamics is examined on the scale of a single period, the particle displacement admits a natural separation into a bounded part associated with recurrent motion within the periodic landscape, and an unbounded stochastic part that grows in time and carries the net transport. We show that effective drift and diffusion are governed entirely by this unbounded component, while local potential-induced fluctuations contribute only bounded corrections. Treating the displacement as an additive functional of the stochastic dynamics provides a rigorous formulation of this separation and leads to a corrector-martingale representation at the trajectory level. Within this framework, classical results-including the Lifson-Jackson formula for unbiased periodic systems and the Stratonovich expressions for tilted periodic potentials-follow as direct consequences of the same underlying structure. The same perspective extends naturally to higher-dimensional periodic environments, recovering the standard homogenized transport tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13561v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sang Yang, Zhixin Peng</dc:creator>
    </item>
  </channel>
</rss>
