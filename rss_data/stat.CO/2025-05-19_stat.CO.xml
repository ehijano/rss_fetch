<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 May 2025 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>JaxSGMC: Modular stochastic gradient MCMC in JAX</title>
      <link>https://arxiv.org/abs/2505.11190</link>
      <description>arXiv:2505.11190v1 Announce Type: new 
Abstract: We present JaxSGMC, an application-agnostic library for stochastic gradient Markov chain Monte Carlo (SG-MCMC) in JAX. SG-MCMC schemes are uncertainty quantification (UQ) methods that scale to large datasets and high-dimensional models, enabling trustworthy neural network predictions via Bayesian deep learning. JaxSGMC implements several state-of-the-art SG-MCMC samplers to promote UQ in deep learning by reducing the barriers of entry for switching from stochastic optimization to SG-MCMC sampling. Additionally, JaxSGMC allows users to build custom samplers from standard SG-MCMC building blocks. Due to this modular structure, we anticipate that JaxSGMC will accelerate research into novel SG-MCMC schemes and facilitate their application across a broad range of domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11190v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.softx.2024.101722</arxiv:DOI>
      <arxiv:journal_reference>SoftwareX, Volume 26, 2024, 101722, ISSN 2352-7110</arxiv:journal_reference>
      <dc:creator>Stephan Thaler, Paul Fuchs, Ana Cukarska, Julija Zavadlav</dc:creator>
    </item>
    <item>
      <title>Maximum likelihood discretization of the transport equation</title>
      <link>https://arxiv.org/abs/2505.10713</link>
      <description>arXiv:2505.10713v1 Announce Type: cross 
Abstract: The transport of positive quantities underlies countless physical processes, including fluid, gas, and plasma dynamics. Discretizing the associated partial differential equations with Galerkin methods can result in spurious nonpositivity of solutions. We observe that these methods amount to performing statistical inference using the method of moments (MoM) and that the loss of positivity arises from MoM's susceptibility to producing estimates inconsistent with the observed data. We overcome this problem by replacing MoM with maximum likelihood estimation, introducing $\textit{maximum likelihood discretization} $(MLD). In the continuous limit, MLD simplifies to the Fisher-Rao Galerkin (FRG) semidiscretization, which replaces the $L^2$ inner product in Galerkin projection with the Fisher-Rao metric of probability distributions. We show empirically that FRG preserves positivity. We prove rigorously that it yields error bounds in the Kullback-Leibler divergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10713v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brook Eyob, Florian Sch\"afer</dc:creator>
    </item>
    <item>
      <title>STRIDE: Sparse Techniques for Regression in Deep Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.11355</link>
      <description>arXiv:2505.11355v1 Announce Type: cross 
Abstract: Gaussian processes (GPs) have gained popularity as flexible machine learning models for regression and function approximation with an in-built method for uncertainty quantification. However, GPs suffer when the amount of training data is large or when the underlying function contains multi-scale features that are difficult to represent by a stationary kernel. To address the former, training of GPs with large-scale data is often performed through inducing point approximations (also known as sparse GP regression (GPR)), where the size of the covariance matrices in GPR is reduced considerably through a greedy search on the data set. To aid the latter, deep GPs have gained traction as hierarchical models that resolve multi-scale features by combining multiple GPs. Posterior inference in deep GPs requires a sampling or, more usual, a variational approximation. Variational approximations lead to large-scale stochastic, non-convex optimisation problems and the resulting approximation tends to represent uncertainty incorrectly. In this work, we combine variational learning with MCMC to develop a particle-based expectation-maximisation method to simultaneously find inducing points within the large-scale data (variationally) and accurately train the GPs (sampling-based). The result is a highly efficient and accurate methodology for deep GP training on large-scale data. We test our method on standard benchmark problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11355v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Urbainczyk, Aretha L. Teckentrup, Jonas Latz</dc:creator>
    </item>
    <item>
      <title>Adaptive Stereographic MCMC</title>
      <link>https://arxiv.org/abs/2408.11780</link>
      <description>arXiv:2408.11780v2 Announce Type: replace 
Abstract: In order to tackle the problem of sampling from heavy tailed, high dimensional distributions via Markov Chain Monte Carlo (MCMC) methods, Yang, Latuszy\'nski, and Roberts (2022) (arXiv:2205.12112) introduces the stereographic projection as a tool to compactify $\mathbb{R}^d$ and transform the problem into sampling from a density on the unit sphere $\mathbb{S}^d$. However, the improvement in algorithmic efficiency, as well as the computational cost of the implementation, are still significantly impacted by the parameters used in this transformation.
  To address this, we introduce adaptive versions three stereographic MCMC algorithms - the Stereographic Random Walk (SRW), the Stereographic Slice Sampler (SSS), and the Stereographic Bouncy Particle Sampler (SBPS) - which automatically update the parameters of the algorithms as the run progresses. The adaptive setup allows to better exploit the power of the stereographic projection, even when the target distribution is neither centered nor homogeneous. Unlike Hamiltonian Monte Carlo (HMC) and other off-the-shelf MCMC samplers, the resulting algorithms are robust to starting far from the mean in heavy-tailed, high-dimensional settings. To prove convergence properties, we develop a novel framework for the analysis of adaptive MCMC algorithms over collections of simultaneously uniformly ergodic Markov operators, which is applicable to continuous-time processes, such as SBPS. This framework allows us to obtain $\mathcal{L}^2$ and almost sure convergence results, and a CLT for our adaptive stereographic algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11780v2</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cameron Bell, Krzystof {\L}atuszy\'nski, Gareth O. Roberts</dc:creator>
    </item>
    <item>
      <title>Pre-Training Estimators for Structural Models: Application to Consumer Search</title>
      <link>https://arxiv.org/abs/2505.00526</link>
      <description>arXiv:2505.00526v2 Announce Type: replace-cross 
Abstract: We explore pretraining estimators for structural econometric models. The estimator is "pretrained" in the sense that the bulk of the computational cost and researcher effort occur during the construction of the estimator. Subsequent applications of the estimator to different datasets require little computational cost or researcher effort. The estimation leverages a neural net to recognize the structural model's parameter from data patterns. As an initial trial, this paper builds a pretrained estimator for a sequential search model that is known to be difficult to estimate. We evaluate the pretrained estimator on 12 real datasets. The estimation takes seconds to run and shows high accuracy. We provide the estimator at pnnehome.github.io. More generally, pretrained, off-the-shelf estimators can make structural models more accessible to researchers and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00526v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanhao 'Max' Wei, Zhenling Jiang</dc:creator>
    </item>
  </channel>
</rss>
