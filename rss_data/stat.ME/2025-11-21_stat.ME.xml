<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Nov 2025 05:02:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Models with Accelerated Failure Conditionals</title>
      <link>https://arxiv.org/abs/2511.15769</link>
      <description>arXiv:2511.15769v1 Announce Type: new 
Abstract: Arnold and Arvanitis (2020) introduced a novel bivariate conditionally specified distribution, a distribution in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This novel conditioning regime was achieved through the use of survival functions, and the approach was termed the accelerated failure conditionals model. In their work, the conditioning framework was constructed using the exponential distribution. Although further generalization was proposed, challenges emerged in deriving the necessary and sufficient conditions for valid joint survival functions. The present study achieves such generalization, extending the conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, moving beyond distributional families whose marginal densities are non-increasing. The resulting models are fully specified through closed-form expressions for their moments, with simulations implemented using either a copula-based procedure or the Metropolis-Hastings algorithm. Empirical applications to two datasets, each featuring variables which are unimodal and skewed, demonstrate that the models with flexible, non-monotonic marginal densities yield a superior fit relative to those models with marginal densities restricted to monotonically decaying forms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15769v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared N. Lakhani</dc:creator>
    </item>
    <item>
      <title>Multidimensional scaling of two-mode three-way asymmetric dissimilarities: finding archetypal profiles and clustering</title>
      <link>https://arxiv.org/abs/2511.15813</link>
      <description>arXiv:2511.15813v1 Announce Type: new 
Abstract: Multidimensional scaling visualizes dissimilarities among objects and reduces data dimensionality. While many methods address symmetric proximity data, asymmetric and especially three-way proximity data (capturing relationships across multiple occasions) remain underexplored. Recent developments, such as the h-plot, enable the analysis of asymmetric and non-reflexive relationships by embedding dissimilarities in a Euclidean space, allowing further techniques like archetypoid analysis to identify representative extreme profiles. However, no existing methods extract archetypal profiles from three-way asymmetric proximity data. This work extends the h-plot methodology to three-way proximity data under both symmetric and asymmetric, conditional and unconditional frameworks. The proposed approach offers several advantages: intuitive interpretability through a unified Euclidean representation; an explicit, eigenvector-based analytical solution free from local minima; scale invariance under linear transformations; computational efficiency for large matrices; and a straightforward goodness-of-fit evaluation. Furthermore, it enables the identification of archetypal profiles and clustering structures for three-way asymmetric proximities. Its performance is compared with existing models for multidimensional scaling and clustering, and illustrated through a financial application. All data and code are provided to facilitate reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15813v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleix Alcacer, Rafael Benitez, Vicente J. Bolos, Irene Epifanio</dc:creator>
    </item>
    <item>
      <title>Causal Inference on Sequential Treatments via Tensor Completion</title>
      <link>https://arxiv.org/abs/2511.15866</link>
      <description>arXiv:2511.15866v1 Announce Type: new 
Abstract: Marginal Structural Models (MSMs) are popular for causal inference of sequential treatments in longitudinal observational studies, which however are sensitive to model misspecification. To achieve flexible modeling, we envision the potential outcomes to form a three-dimensional tensor indexed by subject, time, and treatment regime and propose a tensorized history-restricted MSM (HRMSM). The semi-parametric tensor factor model allows us to leverage the underlying low-rank structure of the potential outcomes tensor and exploit the pre-treatment covariate information to recover the counterfactual outcomes. We incorporate the inverse probability of treatment weighting in the loss function for tensor completion to adjust for time-varying confounding. Theoretically, a non-asymptotic upper bound on the Frobenius norm error for the proposed estimator is provided. Empirically, simulation studies show that the proposed tensor completion approach outperforms the parametric HRMSM and existing matrix/tensor completion methods. Finally, we illustrate the practical utility of the proposed approach to study the effect of ventilation on organ dysfunction from the Medical Information Mart for Intensive Care database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15866v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chenyin Gao, Han Chen, Anru R. Zhang, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Bayesian semiparametric modelling of biomarker variability in joint models</title>
      <link>https://arxiv.org/abs/2511.15882</link>
      <description>arXiv:2511.15882v1 Announce Type: new 
Abstract: There is growing interest in the role of within-individual variability (WIV) in biomarker trajectories for assessing disease risk and progression. A trajectory-based definition that has attracted recent attention characterises WIV as the curvature-based roughness of the latent biomarker trajectory (TB-WIV). To rigorously evaluate the association between TB-WIV and clinical outcomes and to perform dynamic risk prediction, joint models for longitudinal and time-to-event data (JM) are necessary. However, specifying the longitudinal trajectory is critical in this framework and poses methodological challenges. In this work, we investigate three Bayesian semiparametric approaches for longitudinal modelling and TB-WIV estimation within the JM framework to improve stability and accuracy over existing approaches. Two key methods are newly introduced: one based on Bayesian penalised splines (P-splines) and another on functional principal component analysis (FPCA). Using extensive simulation studies, we compare their performance under two important TB-WIV definitions against established approaches. Our results demonstrate overall inferential and predictive advantages of the proposed P-spline and FPCA-based approaches while also providing insights that guide method choice and interpretation of inference results. The proposed approaches are applied to data from the UK Cystic Fibrosis Registry, where, for the first time, we identify a significant positive association between lung function TB-WIV and mortality risk in patients with cystic fibrosis and demonstrate improved predictive performance for survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15882v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sida Chen, Jessica K. Barrett, Marco Palma, Jianxin Pan, Brian D. M. Tom</dc:creator>
    </item>
    <item>
      <title>Cross-Balancing for Data-Informed Design and Efficient Analysis of Observational Studies</title>
      <link>https://arxiv.org/abs/2511.15896</link>
      <description>arXiv:2511.15896v1 Announce Type: new 
Abstract: Causal inference starts with a simple idea: compare groups that differ by treatment, not much else. Traditionally, similar groups are constructed using only observed covariates; however, it remains a long-standing challenge to incorporate available outcome data into the study design while preserving valid inference. In this paper, we study the general problem of covariate adjustment, effect estimation, and statistical inference when balancing features are constructed or selected with the aid of outcome information from the data. We propose cross-balancing, a method that uses sample splitting to separate the error in feature construction from the error in weight estimation. Our framework addresses two cases: one where the features are learned functions and one where they are selected from a potentially high-dimensional dictionary. In both cases, we establish mild and general conditions under which cross-balancing produces consistent, asymptotically normal, and efficient estimators. In the learned-function case, cross-balancing achieves finite-sample bias reduction relative to plug-in-type estimators, and is multiply robust when the learned features converge at slow rates. In the variable-selection case, cross-balancing only requires a product condition on how well the selected variables approximate true functions. We illustrate cross-balancing in extensive simulations and an observational study, showing that careful use of outcome information can substantially improve both estimation and inference while maintaining interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15896v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Jin, Jos\'e Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Bayesian Semiparametric Causal Inference: Targeted Doubly Robust Estimation of Treatment Effects</title>
      <link>https://arxiv.org/abs/2511.15904</link>
      <description>arXiv:2511.15904v1 Announce Type: new 
Abstract: We propose a semiparametric Bayesian methodology for estimating the average treatment effect (ATE) within the potential outcomes framework using observational data with high-dimensional nuisance parameters. Our method introduces a Bayesian debiasing procedure that corrects for bias arising from nuisance estimation and employs a targeted modeling strategy based on summary statistics rather than the full data. These summary statistics are identified in a debiased manner, enabling the estimation of nuisance bias via weighted observables and facilitating hierarchical learning of the ATE. By combining debiasing with sample splitting, our approach separates nuisance estimation from inference on the target parameter, reducing sensitivity to nuisance model specification. We establish that, under mild conditions, the marginal posterior for the ATE satisfies a Bernstein-von Mises theorem when both nuisance models are correctly specified and remains consistent and robust when only one is correct, achieving Bayesian double robustness. This ensures asymptotic efficiency and frequentist validity. Extensive simulations confirm the theoretical results, demonstrating accurate point estimation and credible intervals with nominal coverage, even in high-dimensional settings. The proposed framework can also be extended to other causal estimands, and its key principles offer a general foundation for advancing Bayesian semiparametric inference more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15904v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G\"ozde Sert, Abhishek Chakrabortty, Anirban Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Small Area Estimation Methods for Multivariate Health and Demographic Outcomes using Complex Survey Data</title>
      <link>https://arxiv.org/abs/2511.15917</link>
      <description>arXiv:2511.15917v1 Announce Type: new 
Abstract: Improving health in the most disadvantaged populations requires reliable estimates of health and demographic indicators to inform policy and interventions. Low- and middle-income countries with the largest burden of disease and disability tend to have the least comprehensive data, relying primarily on household surveys. Subnational estimates are increasingly used to inform targeted interventions and health policies. Producing reliable estimates from these data at fine geographical scales requires statistical modeling, and small area estimation models are commonly used in this context. Although most current methods model univariate outcomes, improved estimates may be attained by borrowing strength across related outcomes via multivariate modeling. In this paper, we develop classes of area- and unit-level multivariate shared component models using complex survey data. This framework jointly models multiple outcomes to improve accuracy of estimates compared to separately fitting univariate models. We conduct simulation studies to validate the methodology and use the proposed approach on survey data from Kenya in 2014; first, to jointly model height-for-age and weight-for-age in children, and second, to model three categories of contraceptive use in women. These models produce improved estimates compared to univariate and naive multivariate modeling approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15917v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin E Schumacher, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Sequential Testing for Assessing the Incremental Value of Biomarkers Under Biorepository Specimen Constraints with Robustness to Model Misspecification</title>
      <link>https://arxiv.org/abs/2511.15918</link>
      <description>arXiv:2511.15918v1 Announce Type: new 
Abstract: In cancer biomarker development, a key objective is to evaluate whether a new biomarker, when combined with an established one, improves early cancer detection compared to using the established biomarker alone. Incremental value is often quantified by changes at specific points on the ROC curve, such as an increase in sensitivity at a fixed specificity, which is especially relevant in early cancer detection. Our research is motivated by the Early Detection Research Network (EDRN) biorepository studies, which aim to validate multiple cancer biomarkers across laboratories using specimens obtained from a centralized biorepository, under the constraint of limited specimen availability. To address this challenge, we propose a two-stage group sequential hypothesis testing framework for assessing incremental effects, allowing early stopping for futility or efficacy to conserve valuable samples. Our asymptotic results are derived under a logistic working model and remain valid even under model misspecification, ensuring robustness and broad applicability. We further integrate a rotating group membership design to facilitate validation of multiple candidate biomarkers across laboratories. Through extensive simulations, we demonstrate valid type I error control and efficient utilization of specimens. Finally, we apply our method to data from a multicenter EDRN pancreatic cancer reference set study and show how the proposed approach identifies promising candidate biomarkers that provide incremental performance when combined with CA19-9, while enabling efficient evaluation of a large number of such candidates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15918v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Indrila Ganguly, Ying Huang</dc:creator>
    </item>
    <item>
      <title>Robust Estimation under Outcome Dependent Right Censoring in Huntington Disease: Estimators for Low and High Censoring Rates</title>
      <link>https://arxiv.org/abs/2511.15929</link>
      <description>arXiv:2511.15929v1 Announce Type: new 
Abstract: Across health applications, researchers model outcomes as a function of time to an event, but the event time is right-censored for participants who exit the study or otherwise do not experience the event during follow-up. When censoring depends on the outcome-as in neurodegenerative disease studies where dropout is potentially related to disease severity-standard regression estimators produce biased estimates. We develop three consistent estimators for this outcome-dependent censoring setting: two augmented inverse probability weighted (AIPW) estimators and one maximum likelihood estimator (MLE). We establish their asymptotic properties and derive their robust sandwich variance estimators that account for nuisance parameter estimation. A key contribution is demonstrating that the choice of estimator to use depends on the censoring rate-the MLE performs best under low censoring rates, while the AIPW estimators yield lower bias and a higher nominal coverage under high censoring rates. We apply our estimators to Huntington disease data to characterize health decline leading up to mild cognitive impairment onset. The AIPW estimator with robustness matrix provided clinically-backed estimates with improved precision over inverse probability weighting, while MLE exhibited bias. Our results provide practical guidance for estimator selection based on censoring rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15929v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesus E. Vazquez, Yanyuan Ma, Karen Marder, Tanya P. Garcia</dc:creator>
    </item>
    <item>
      <title>A Simple and Robust Multi-Fidelity Data Fusion Method for Effective Modeling of Citizen-Science Air Pollution Data</title>
      <link>https://arxiv.org/abs/2511.15942</link>
      <description>arXiv:2511.15942v1 Announce Type: new 
Abstract: We propose a robust multi-fidelity Gaussian process for integrating sparse, high-quality reference monitors with dense but noisy citizen-science sensors. The approach replaces the Gaussian log-likelihood in the high-fidelity channel with a global Huber loss applied to precision-weighted residuals, yielding bounded influence on all parameters, including the cross-fidelity coupling, while retaining the flexibility of co-kriging. We establish attenuation and unbounded influence of the Gaussian maximum likelihood estimator under low-fidelity contamination and derive explicit finite bounds for the proposed estimator that clarify how whitening and mean-shift sensitivity determine robustness. Monte Carlo experiments with controlled contamination show that the robust estimator maintains stable MAE and RMSE as anomaly magnitude and frequency increase, whereas the Gaussian MLE deteriorates rapidly. In an empirical study of PM2.5 concentrations in Hamburg, combining UBA monitors with openSenseMap data, the method consistently improves cross-validated predictive accuracy and yields coherent uncertainty maps without relying on auxiliary covariates. The framework remains computationally scalable through diagonal or low-rank whitening and is fully reproducible with publicly available code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15942v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camilla Andreozzi, Pietro Colombo, Philipp Otto</dc:creator>
    </item>
    <item>
      <title>Evaluating Variance Estimates with Relative Efficiency</title>
      <link>https://arxiv.org/abs/2511.15961</link>
      <description>arXiv:2511.15961v1 Announce Type: new 
Abstract: Experimentation platforms in industry must often deal with customer trust issues. Platforms must prove the validity of their claims as well as catch issues that arise. As a central quantity estimated by experimentation platforms, the validity of confidence intervals is of particular concern. To ensure confidence intervals are reliable, we must understand and diagnose when our variance estimates are biased or noisy, or when the confidence intervals may be incorrect.
  A common method for this is A/A testing, in which both the control and test arms receive the same treatment. One can then test if the empirical false positive rate (FPR) deviates substantially from the target FPR over many tests. However, this approach turns each A/A test into a simple binary random variable. It is an inefficient estimate of the FPR as it throws away information about the magnitude of each experiment result. We show how to empirically evaluate the effectiveness of statistics that monitor the variance estimates that partly dictate a platform's statistical reliability. We also show that statistics other than empirical FPR are more effective at detecting issues. In particular, we propose a $t^2$-statistic that is more sample efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15961v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kedar Karhadkar, Jack Klys, Daniel Ting, Artem Vorozhtsov, Houssam Nassif</dc:creator>
    </item>
    <item>
      <title>Possibilistic Instrumental Variable Regression</title>
      <link>https://arxiv.org/abs/2511.16029</link>
      <description>arXiv:2511.16029v1 Announce Type: new 
Abstract: Instrumental variable regression is a common approach for causal inference in the presence of unobserved confounding. However, identifying valid instruments is often difficult in practice. In this paper, we propose a novel method based on possibility theory that performs posterior inference on the treatment effect, conditional on a user-specified set of potential violations of the exogeneity assumption. Our method can provide informative results even when only a single, potentially invalid, instrument is available, offering a natural and principled framework for sensitivity analysis. Simulation experiments and a real-data application indicate strong performance of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16029v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gregor Steiner, Jeremie Houssineau, Mark F. J. Steel</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification in Bayesian Clustering</title>
      <link>https://arxiv.org/abs/2511.16040</link>
      <description>arXiv:2511.16040v1 Announce Type: new 
Abstract: Bayesian clustering methods have the widely touted advantage of providing a probabilistic characterization of uncertainty in clustering through the posterior distribution. An amazing variety of priors and likelihoods have been proposed for clustering in a broad array of settings. There is also a rich literature on Markov chain Monte Carlo (MCMC) algorithms for sampling from posterior clustering distributions. However, there is relatively little work on summarizing the posterior uncertainty. The complexity of the partition space corresponding to different clusterings makes this problem challenging. We propose a post-processing procedure for any Bayesian clustering model with posterior samples that generates a credible set that is easy to use, fast to compute, and intuitive to interpret. We also provide new measures of clustering uncertainty and show how to compute cluster-specific parameter estimates and credible regions that accumulate a desired posterior probability without having to condition on a partition estimate or employ label-switching techniques. We illustrate our procedure through several applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16040v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garritt L. Page, Andr\'es F. Barrientos, David B. Dahl, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Estimation of the Coefficient of Variation of Weibull Distribution under Type-I Progressively Interval Censoring: A Simulation-based Approach</title>
      <link>https://arxiv.org/abs/2511.16102</link>
      <description>arXiv:2511.16102v1 Announce Type: new 
Abstract: Measures of relative variability, such as the Pearson's coefficient of variation (CV$_p$), give much insight into the spread of lifetime distributions, like the Weibull distribution. The estimation of the Weibull CV$_p$ in modern statistics has traditionally been prioritized only when complete data is available. In this article, we estimate the Weibull CV$_p$ and its second-order alternative, denoted as CV$_k$, under type-I progressively interval censoring, which is a typical scenario in survival analysis and reliability theory. Point estimates are obtained using the methods of maximum likelihood, least squares, and the Bayesian approach with MCMC simulation. A nonlinear least squares method is proposed for estimating the CV$_p$ and CV$_k$. We also perform interval estimation of the CV$_p$ and CV$_k$ using the asymptotic confidence intervals, bootstrap intervals through the least squares estimates, and the highest posterior density intervals. A comprehensive Monte Carlo simulation study is carried out to understand and compare the performance of the estimators. The proposed least squares and the Bayesian methods produce better point estimates for the CV$_p$. The highest posterior density intervals outperform other interval estimates in many cases. The methodologies are also applied to a real dataset to demonstrate the performance of the estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16102v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bankitdor M Nongrum, Adarsha Kumar Jena</dc:creator>
    </item>
    <item>
      <title>Targeted Parameter Estimation for Robust Empirical Bayes Ranking</title>
      <link>https://arxiv.org/abs/2511.16530</link>
      <description>arXiv:2511.16530v1 Announce Type: new 
Abstract: Ordering the expected outcomes across a collection of clusters after performing a covariate adjustment commonly arises in many applied settings, such as healthcare provider evaluation. Regression parameters in such covariate adjustment models are frequently estimated by maximum likelihood or through other criteria that do not directly evaluate the quality of the rankings resulting from using a particular set of parameter estimates. In this article, we propose both a novel empirical Bayes ranking procedure and an associated estimation approach for finding the regression parameters of the covariate adjustment model. By building our ranking approach around estimating approximate percentiles of the covariate-adjusted cluster-level means, we are able to develop manageable expressions for the expected ranking squared-error loss associated with any choice of the covariate-adjustment model parameters, and we harness this to generate a novel unbiased estimator for this expected loss. Minimization of this unbiased estimator directly leads to a novel ranking procedure that is often more robust than conventional empirical Bayes ranking methods. Through a series of simulation studies, we show that our approach consistently delivers improved ranking squared-error performance relative to competing methods, such as posterior expected ranks and ranking the components of the best linear unbiased predictor. Estimating rankings using our method is illustrated with an example from a longitudinal study evaluating test scores across a large group of schools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16530v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas C. Henderson, Nicholas Hartman</dc:creator>
    </item>
    <item>
      <title>A flexible quantile mixed-effects model for censored outcomes</title>
      <link>https://arxiv.org/abs/2511.16589</link>
      <description>arXiv:2511.16589v1 Announce Type: new 
Abstract: We introduce a Bayesian quantile mixed-effects model for censored longitudinal outcomes based on the skew exponential power (SEP) error distribution. The SEP family separates tail behavior and skewness from the targeted quantile and includes the skew Laplace (SL) distribution as a special case. We derive analytic likelihood contributions for left, right, and interval censoring under the SEP model, so censored observations are handled within a single parametric framework without numerical integration in the likelihood. In simulation studies with varying censoring patterns and skewness profiles, the SEP-based quantile mixed-effects model maintains near-nominal bias and credible interval coverage for regression coefficients. In contrast, the SL-based model can exhibit bias and undercoverage when the data's skewness conflicts with the skewness implied by the target quantile. In an HIV-1 RNA viral load case study with left censoring at the assay limit, bridge-sampled marginal likelihoods and simulation-based residual diagnostics favor the SEP specification across quantiles and yield more stable estimates of treatment-specific viral load trajectories than the SL benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16589v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divan A. Burger, Sean van der Merwe, Emmanuel Lesaffre</dc:creator>
    </item>
    <item>
      <title>Confidence Sets for the Emergence, Collapse, and Recovery Dates of a Bubble</title>
      <link>https://arxiv.org/abs/2511.16172</link>
      <description>arXiv:2511.16172v1 Announce Type: cross 
Abstract: We propose constructing confidence sets for the emergence, collapse, and recovery dates of a bubble by inverting tests for the location of the break date. We examine both likelihood ratio-type tests and the Elliott-Muller-type (2007) tests for detecting break locations. The limiting distributions of these tests are derived under the null hypothesis, and their asymptotic consistency under the alternative is established. Finite-sample properties are evaluated through Monte Carlo simulations. The results indicate that combining different types of tests effectively controls the empirical coverage rate while maintaining a reasonably small length of the confidence set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16172v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eiji Kurozumi, Anton Skrobotov</dc:creator>
    </item>
    <item>
      <title>Causal Synthetic Data Generation in Recruitment</title>
      <link>https://arxiv.org/abs/2511.16204</link>
      <description>arXiv:2511.16204v1 Announce Type: cross 
Abstract: The importance of Synthetic Data Generation (SDG) has increased significantly in domains where data quality is poor or access is limited due to privacy and regulatory constraints. One such domain is recruitment, where publicly available datasets are scarce due to the sensitive nature of information typically found in curricula vitae, such as gender, disability status, or age. %
This lack of accessible, representative data presents a significant obstacle to the development of fair and transparent machine learning models, particularly ranking algorithms that require large volumes of data to effectively learn how to recommend candidates. In the absence of such data, these models are prone to poor generalisation and may fail to perform reliably in real-world scenarios. %
Recent advances in Causal Generative Models (CGMs) offer a promising solution. CGMs enable the generation of synthetic datasets that preserve the underlying causal relationships within the data, providing greater control over fairness and interpretability in the data generation process. %
In this study, we present a specialised SDG method involving two CGMs: one modelling job offers and the other modelling curricula. Each model is structured according to a causal graph informed by domain expertise. We use these models to generate synthetic datasets and evaluate the fairness of candidate rankings under controlled scenarios that introduce specific biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16204v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Iommi, Antonio Mastropietro, Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri</dc:creator>
    </item>
    <item>
      <title>Toward Valid Generative Clinical Trial Data with Survival Endpoints</title>
      <link>https://arxiv.org/abs/2511.16551</link>
      <description>arXiv:2511.16551v1 Announce Type: cross 
Abstract: Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16551v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Machine Learning for Health (ML4H) 2025</arxiv:journal_reference>
      <dc:creator>Perrine Chassat, Van Tuan Nguyen, Lucas Ducrot, Emilie Lanoy, Agathe Guilloux</dc:creator>
    </item>
    <item>
      <title>Control Variates for MCMC</title>
      <link>https://arxiv.org/abs/2402.07349</link>
      <description>arXiv:2402.07349v2 Announce Type: replace 
Abstract: This chapter describes several control variate methods for improving estimates of expectations from MCMC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07349v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leah South, Matthew Sutton</dc:creator>
    </item>
    <item>
      <title>Addressing the Influence of Unmeasured Confounding in Observational Studies with Time-to-Event Outcomes: A Semiparametric Sensitivity Analysis Approach</title>
      <link>https://arxiv.org/abs/2403.02539</link>
      <description>arXiv:2403.02539v2 Announce Type: replace 
Abstract: In this paper, we develop a semiparametric sensitivity analysis approach designed to address unmeasured confounding in observational studies with time-to-event outcomes. We target estimation of the marginal distributions of potential outcomes under competing exposures using influence function-based techniques. We derive the non-parametric influence function for uncensored data and map the uncensored data influence function to the observed data influence function. Our methodology is motivated by and applied to an observational study evaluating the effectiveness of radical prostatectomy (RP) versus external beam radiotherapy with androgen deprivation (EBRT+AD) for the treatment of prostate cancer. We also present a realistic simulation study demonstrating the finite-sample properties of our estimation procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02539v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linda Amoafo, Shiyao Xu, Elizabeth Platz, Daniel Scharfstein</dc:creator>
    </item>
    <item>
      <title>The conditional saddlepoint approximation for fast and accurate large-scale hypothesis testing</title>
      <link>https://arxiv.org/abs/2407.08911</link>
      <description>arXiv:2407.08911v4 Announce Type: replace 
Abstract: Large-scale testing in modern applications such as genomics often entails a trade-off between accuracy and speed: multiplicity corrections push cutoffs deep into the tails, where normal approximations can fail, while resampling is accurate but computationally expensive for large datasets. To resolve this impasse in the context of conditional independence testing, we introduce spaCRT, a closed-form saddlepoint approximation (SPA) for the distilled conditional randomization test (dCRT) that retains the statistical accuracy of dCRT's resampling while avoiding its computational cost. We prove that spaCRT's relative approximation error vanishes asymptotically by establishing a general theorem on the relative error of conditional SPAs. Because dCRT uses a plug-in nuisance regression, we specialize our guarantees to common choices: low-dimensional generalized linear model (GLM), high-dimensional GLM lasso, and kernel ridge regression. Our general theorem is, to our knowledge, the first rigorous technical tool for analyzing SPAs for resampling tests, which had previously been justified only heuristically. It extends beyond spaCRT, as we exemplify by justifying an SPA for the classical sign-flipping location test. Empirically, spaCRT matches dCRT's statistical performance by approximating its p-values with median error 1-12% across settings while delivering a 250x speedup on a single-cell CRISPR screen dataset with 85,000 hypotheses. Building on dCRT's versatility, spaCRT and its open-source R package enable fast and accurate large-scale testing across diverse applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08911v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Niu, Jyotishka Ray Choudhury, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Assessing Spatial Disparities: A Bayesian Linear Regression Approach</title>
      <link>https://arxiv.org/abs/2407.19171</link>
      <description>arXiv:2407.19171v5 Announce Type: replace 
Abstract: Epidemiological investigations of regionally aggregated spatial data often involve detecting spatial health disparities among neighboring regions on a map of disease mortality or incidence rates. Analyzing such data introduces spatial dependence among health outcomes and seeks to report statistically significant spatial disparities by delineating boundaries that separate neighboring regions with disparate health outcomes. However, there are statistical challenges to appropriately define what constitutes a spatial disparity and to construct robust probabilistic inferences for spatial disparities. We enrich the familiar Bayesian linear regression framework to introduce spatial autoregression and offer model-based detection of spatial disparities. We derive exploitable analytical tractability that considerably accelerates computation. Simulation experiments conducted on a county map of the entire United States demonstrate the effectiveness of our method, and we apply our method to a data set from the Institute of Health Metrics and Evaluation (IHME) on age-standardized US county-level estimates of lung cancer mortality rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19171v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Lin Wu, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Nonparametric tests of treatment effect homogeneity for policy-makers</title>
      <link>https://arxiv.org/abs/2410.00985</link>
      <description>arXiv:2410.00985v3 Announce Type: replace 
Abstract: Recent work has focused on nonparametric estimation of conditional treatment effects, but inference has remained relatively unexplored. We propose a class of nonparametric tests for both quantitative and qualitative treatment effect heterogeneity. The tests can incorporate a variety of structured assumptions on the conditional average treatment effect, allow for both continuous and discrete covariates, and do not require sample splitting to obtain a tractable asymptotic null distribution. Furthermore, we show how the tests are tailored to detect alternatives where the population impact of adopting a personalized decision rule differs from using a rule that discards covariates. The proposal is thus relevant for guiding treatment policies. The utility of the proposal is borne out in simulation studies and a re-analysis of an AIDS clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00985v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Dukes, Mats J. Stensrud, Riccardo Brioschi, Aaron Hudson</dc:creator>
    </item>
    <item>
      <title>A Componentwise Estimation Procedure for Multivariate Location and Scatter: Robustness, Efficiency and Scalability</title>
      <link>https://arxiv.org/abs/2410.21166</link>
      <description>arXiv:2410.21166v2 Announce Type: replace 
Abstract: Covariance matrix estimation is an important problem in multivariate data analysis, both from theoretical as well as applied points of view. Many simple and popular covariance matrix estimators are known to be severely affected by model misspecification and the presence of outliers in the data; on the other hand robust estimators with reasonably high efficiency are often computationally challenging for modern large and complex datasets. In this work, we propose a new, simple, robust and highly efficient method for estimation of the location vector and the scatter matrix for elliptically symmetric distributions. The proposed estimation procedure is designed in the spirit of the minimum density power divergence (DPD) estimation approach with appropriate modifications which makes our proposal (sequential minimum DPD estimation) computationally very economical and scalable to large as well as higher dimensional datasets. Consistency and asymptotic normality of the proposed sequential estimators of the multivariate location and scatter are established along with asymptotic positive definiteness of the estimated scatter matrix. Robustness of our estimators are studied by means of influence functions. All theoretical results are illustrated further under multivariate normality. A large-scale simulation study is presented to assess finite sample performances and scalability of our method in comparison to the usual maximum likelihood estimator (MLE), the ordinary minimum DPD estimator (MDPDE) and other popular non-parametric methods. The applicability of our method is further illustrated with a real dataset on credit card transactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21166v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Chakraborty, Ayanendranath Basu, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Modular Jump Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.15557</link>
      <description>arXiv:2505.15557v3 Announce Type: replace 
Abstract: Gaussian processes (GPs) furnish accurate nonlinear predictions with well-calibrated uncertainty. However, the typical GP setup has a built-in stationarity assumption, making it ill-suited for modeling data from processes with sudden changes, or "jumps" in the output variable. The "jump GP" (JGP) was developed for modeling data from such processes, combining local GPs and latent "level" variables under a joint inferential framework. But joint modeling can be fraught with difficulty. We aim to simplify by suggesting a more modular setup, eschewing joint inference but retaining the main JGP themes: (a) learning optimal neighborhood sizes that locally respect manifolds of discontinuity; and (b) a new cluster-based (latent) feature to capture regions of distinct output levels on both sides of the manifold. We show that each of (a) and (b) separately leads to dramatic improvements when modeling processes with jumps. In tandem (but without requiring joint inference) that benefit is compounded, as illustrated on real and synthetic benchmark examples from the recent literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15557v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna R. Flowers, Christopher T. Franck, Micka\"el Binois, Chiwoo Park, Robert B. Gramacy</dc:creator>
    </item>
    <item>
      <title>Selection of functional predictors and smooth coefficient estimation for scalar-on-function regression models</title>
      <link>https://arxiv.org/abs/2506.17773</link>
      <description>arXiv:2506.17773v2 Announce Type: replace 
Abstract: In the framework of scalar-on-function regression models, in which several functional variables are employed to predict a scalar response, we propose a methodology for selecting relevant functional predictors while simultaneously providing accurate smooth (or, more generally, regular) estimates of the functional coefficients. We suppose that the functional predictors belong to a real separable Hilbert space, while the functional coefficients belong to a specific subspace of this Hilbert space. Such a subspace can be a Reproducing Kernel Hilbert Space (RKHS) to ensure the desired regularity characteristics, such as smoothness or periodicity, for the coefficient estimates. Our procedure, called SOFIA (Scalar-On-Function Integrated Adaptive Lasso), is based on an adaptive penalized least squares algorithm that leverages functional subgradients to efficiently solve the minimization problem. We demonstrate that the proposed method satisfies the functional oracle property, even when the number of predictors exceeds the sample size. SOFIA's effectiveness in variable selection and coefficient estimation is evaluated through extensive simulation studies and a real-data application to GDP growth prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17773v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hedayat Fathi, Marzia A. Cremona, Federico Severino</dc:creator>
    </item>
    <item>
      <title>A Non-Parametric Sensitivity Analysis for Bounding Bias in Hybrid Control Trials</title>
      <link>https://arxiv.org/abs/2507.18876</link>
      <description>arXiv:2507.18876v2 Announce Type: replace 
Abstract: We study hybrid control trials (HCTs), in which a randomized controlled trial (RCT) is augmented with external control patients. Existing approaches for HCTs typically assume conditional exchangeability of the concurrent and external controls to identify trial-specific effects. When violated, this can induce substantial unquantified bias, which in turn limits the acceptability of HCTs in regulatory settings. We treat violations of mean exchangeability as omitted variable bias and develop a non-parametric sensitivity analysis that (i) applies to the efficient, doubly robust HCT estimator of the trial-specific ATE, and (ii) delivers sharp bounds on the bias induced by unmeasured covariates. Building on recent work in double machine learning, our approach characterizes the maximal bias in terms of two partial R-squared sensitivity parameters: the additional explanatory power that unmeasured confounders could have for the outcome regression and for trial participation. For any given choice of these parameters, we construct valid confidence bounds for bias-adjusted treatment effects and visualize critical causal gaps via contour plots and robustness values that show how strong unmeasured confounding would need to be to overturn nominally significant HCT findings. Through simulations, we show that the method (i) reliably upper-bounds true bias, (ii) restores type I error control in settings where na\"ive HCT analysis is anti-conservative, and (iii) can still deliver meaningful power gains and RCT sample-size reductions even under moderate violations of mean exchangeability. We illustrate the approach in a phase III trial on diabetes, supplemented with external controls. We discuss practical guidelines for designing and evaluating HCTs, including external-data selection, sample-size allocation, and interpretation of sensitivity contours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18876v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alissa Gordon, Emilie H{\o}jbjerre-Frandsen, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>Analytics of Adaptive Online Testing in Practice Over a Decade</title>
      <link>https://arxiv.org/abs/2508.08643</link>
      <description>arXiv:2508.08643v2 Announce Type: replace 
Abstract: Adaptive online testing efficiently assesses examinee proficiency by dynamically adjusting the difficulty of test items based on their performance. To achieve this, items are selected so that their difficulty closely matches the test taker's estimated ability at each stage of the test. This alignment implies that the probability of a correct answer tends toward 0.5. However, in practical settings, this probability may not converge to 0.5 unless the test comprises a sufficiently large number of items. This could give the impression that the adaptive mechanism is not functioning properly. Nevertheless, even when the number of items is small, such as 5 or 7, the adaptive nature of the system can still be observed by examining the relationship between item difficulty and the mean estimated ability of examinees for the corresponding item. Considering that each item typically requires about 2-5 minutes to solve, this number of items appears to be a practical choice that avoids excessive fatigue for test-takers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08643v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hideo Hirose</dc:creator>
    </item>
    <item>
      <title>How many patients could we save with LLM priors?</title>
      <link>https://arxiv.org/abs/2509.04250</link>
      <description>arXiv:2509.04250v2 Announce Type: replace 
Abstract: Imagine a world where clinical trials need far fewer patients to achieve the same statistical power, thanks to the knowledge encoded in large language models (LLMs). We present a novel framework for hierarchical Bayesian modeling of adverse events in multi-center clinical trials, leveraging LLM-informed prior distributions. Unlike data augmentation approaches that generate synthetic data points, our methodology directly obtains parametric priors from the model. Our approach systematically elicits informative priors for hyperparameters in hierarchical Bayesian models using a pre-trained LLM, enabling the incorporation of external clinical expertise directly into Bayesian safety modeling. Through comprehensive temperature sensitivity analysis and rigorous cross-validation on real-world clinical trial data, we demonstrate that LLM-derived priors consistently improve predictive performance compared to traditional meta-analytical approaches. This methodology paves the way for more efficient and expert-informed clinical trial design, enabling substantial reductions in the number of patients required to achieve robust safety assessment and with the potential to transform drug safety monitoring and regulatory decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04250v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shota Arai, David Selby, Andrew Vargo, Sebastian Vollmer</dc:creator>
    </item>
    <item>
      <title>On Assessing Overall Survival (OS) in Oncology Studies</title>
      <link>https://arxiv.org/abs/2510.07122</link>
      <description>arXiv:2510.07122v2 Announce Type: replace 
Abstract: In assessing Overall Survival (OS) in oncology studies, it is essential for the efficacy measure to be Logic-respecting, for otherwise patients may be incorrectly targeted. This paper explains, while Time Ratio (TR) is Logic-respecting, Hazard Ratio (HR) is not Logic-respecting. With Time Ratio (TR) being recommended, a smooth transitioning strategy is suggested. The conclusion states: Logicality requires, and Subgroup Mixable Estimation (SME) delivers, an efficacy assessment for the overall population within the range of minimum and maximum efficacy in the subgroups, no matter how outcome is measured, whichever logic-respecting efficacy measure is chosen, the same efficacy assessment regardless of how subgroups are stratified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07122v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jason C. Hsu</dc:creator>
    </item>
    <item>
      <title>On Focusing Statistical Power for Searches and Measurements in Particle Physics</title>
      <link>https://arxiv.org/abs/2507.17831</link>
      <description>arXiv:2507.17831v2 Announce Type: replace-cross 
Abstract: Particle physics experiments rely on the (generalised) likelihood ratio test (LRT) for searches and measurements, which consist of composite hypothesis tests. However, this test is not guaranteed to be optimal, as the Neyman-Pearson lemma pertains only to simple hypothesis tests. Any choice of test statistic thus implicitly determines how statistical power varies across the parameter space. An improvement in the core statistical testing methodology for general settings with composite tests would have widespread ramifications across experiments. We discuss an alternate test statistic that provides the data analyzer an ability to focus the power of the test on physics-motivated regions of the parameter space. We demonstrate the improvement from this technique compared to the LRT on a Higgs $\rightarrow\tau\tau$ dataset simulated by the ATLAS experiment and a dark matter dataset inspired by the LZ experiment. We also employ machine learning to efficiently perform the Neyman construction, which is essential to ensure statistically valid confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17831v2</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Carzon, Aishik Ghosh, Rafael Izbicki, Ann Lee, Luca Masserano, Daniel Whiteson</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks</title>
      <link>https://arxiv.org/abs/2511.14455</link>
      <description>arXiv:2511.14455v2 Announce Type: replace-cross 
Abstract: We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14455v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicola Rares Franco, Lorenzo Tedesco</dc:creator>
    </item>
  </channel>
</rss>
