<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 04:01:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Comment on "Deep Regression Learning with Optimal Loss Function"</title>
      <link>https://arxiv.org/abs/2509.03702</link>
      <description>arXiv:2509.03702v1 Announce Type: new 
Abstract: OpenReview benefits the peer-review system by promoting transparency, openness, and collaboration. By making reviews, comments, and author responses publicly accessible, the platform encourages constructive feedback, reduces bias, and allows the research community to engage directly in the review process. This level of openness fosters higher-quality reviews, greater accountability, and continuous improvement in scholarly communication. In the statistics community, such a transparent and open review system has not traditionally existed. This lack of transparency has contributed to significant variation in the quality of published papers, even in leading journals, with some containing substantial errors in both proofs and numerical analyses. To illustrate this issue, this note examines several results from Wang, Zhou and Lin (2025) [arXiv:2309.12872; https://doi.org/10.1080/01621459.2024.2412364] and highlights potential errors in their proofs, some of which are strikingly obvious. This raises a critical question: how important are mathematical proofs in statistical journals, and how should they be rigorously verified? Addressing this question is essential not only for maintaining academic rigor but also for fostering the right attitudes toward scholarship and quality assurance in the field. A plausible approach would be for arXiv to provide an anonymous discussion section, allowing readers-whether anonymous or not-to post comments, while also giving authors the opportunity to respond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03702v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen Li</dc:creator>
    </item>
    <item>
      <title>Simulation-based Inference via Langevin Dynamics with Score Matching</title>
      <link>https://arxiv.org/abs/2509.03853</link>
      <description>arXiv:2509.03853v1 Announce Type: new 
Abstract: Simulation-based inference (SBI) enables Bayesian analysis when the likelihood is intractable but model simulations are available. Recent advances in statistics and machine learning, including Approximate Bayesian Computation and deep generative models, have expanded the applicability of SBI, yet these methods often face challenges in moderate to high-dimensional parameter spaces. Motivated by the success of gradient-based Monte Carlo methods in Bayesian sampling, we propose a novel SBI method that integrates score matching with Langevin dynamics to explore complex posterior landscapes more efficiently in such settings. Our approach introduces tailored score-matching procedures for SBI, including a localization scheme that reduces simulation costs and an architectural regularization that embeds the statistical structure of log-likelihood scores to improve score-matching accuracy. We provide theoretical analysis of the method and illustrate its practical benefits on benchmark tasks and on more challenging problems in moderate to high dimensions, where it performs favorably compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03853v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyu Jiang, Yuexi Wang, Yun Yang</dc:creator>
    </item>
    <item>
      <title>Spatial disaggregation of time series</title>
      <link>https://arxiv.org/abs/2509.04065</link>
      <description>arXiv:2509.04065v1 Announce Type: new 
Abstract: Spatiotemporal modeling of economic aggregates is increasingly relevant in regional science due to the presence of both spatial spillovers and temporal dynamics. Traditional temporal disaggregation methods, such as Chow-Lin, often ignore spatial dependence, potentially losing important regional information. We propose a novel methodology for spatiotemporal disaggregation, integrating spatial autoregressive models, benchmarking restrictions, and auxiliary covariates. The approach accommodates partially observed regional data through an anchoring mechanism, ensuring consistency with known aggregates while reducing prediction variance. We establish identifiability and asymptotic normality of the estimator under general conditions, including non-Gaussian and heteroskedastic residuals. Extensive simulations confirm the method's robustness across a wide range of spatial autocorrelations and covariate informativeness. The methodology is illustrated by disaggregating Spanish GDP into 17 autonomous communities from 2002 to 2023, using auxiliary indicators and principal component analysis for dimensionality reduction. This framework extends classical temporal disaggregation to the spatial domain, providing accurate regional estimates while accounting for spatial spillovers and irregular data availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04065v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Tobar, A. Mir, R. Alberich, I. Garcia, M. Mir\'o, NA. Cruz</dc:creator>
    </item>
    <item>
      <title>Bayesian Stacking via Proper Scoring Rule Optimization using a Gibbs Posterior</title>
      <link>https://arxiv.org/abs/2509.04203</link>
      <description>arXiv:2509.04203v1 Announce Type: new 
Abstract: In collaborative forecast projects, the combining of multiple probabilistic forecasts into an ensemble is standard practice, with linear pooling being a common combination method. The weighting scheme of a linear pool should be tailored to the specific research question, and weight selection is often performed via optimizing a proper scoring rule. This is known as optimal linear pooling. Besides optimal linear pooling, Bayesian predictive synthesis has emerged as a model probability updating scheme which is more flexible than standard Bayesian model averaging and which provides a Bayesian solution to selecting model weights for a linear pool. In many problems, equally weighted linear pool forecasts often outperform forecasts constructed using sophisticated weight selection methods. Thus regularization to an equal weighting of forecasts may be a valuable addition to any weight selection method. In this manuscript, we introduce an optimal linear pool based on a Gibbs posterior over stacked model weights optimized over a proper scoring rule. The Gibbs posterior extends stacking into a Bayesian framework by allowing for optimal weight solutions to be influenced by a prior distribution, and it also provides uncertainty quantification of weights in the form of a probability distribution. We compare ensemble forecast performance with model averaging methods and equal weighted models in simulation studies and in a real data example from the 2023-24 US Centers for Disease Control FluSight competition. In both the simulation studies and the FluSight analysis, the stacked Gibbs posterior produces ensemble forecasts which often outperform the ensembles of other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04203v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Spencer Wadsworth, Jarad Niemi</dc:creator>
    </item>
    <item>
      <title>Conformalized Multiple Testing under Unknown Null Distribution with Symmetric Errors</title>
      <link>https://arxiv.org/abs/2509.04231</link>
      <description>arXiv:2509.04231v1 Announce Type: new 
Abstract: This article addresses a fundamental concern, first raised by Efron (2004), regarding the selection of null distributions in large-scale multiple testing. In modern data-intensive applications involving thousands or even millions of hypotheses, the theoretical null distribution of the test statistics often deviates from the true underlying null distribution, severely compromising the false discovery rate (FDR) analysis. We propose a conformalized empirical Bayes method using self-calibrated empirical null samples (SENS) for both one-sample and two-sample multiple testing problems. The new framework not only sidesteps the use of potentially erroneous theoretical null distributions, which is common in conventional practice, but also mitigates the impact of estimation errors in the unknown null distribution on the validity of FDR control, a challenge frequently encountered in the empirical Bayes FDR literature. In contrast to the empirical Bayes approaches (cf. Efron, 2004; Jin and Cai, 2007; Sun and Cai, 2007) that rely on Gaussian assumptions for the null models, SENS imposes only a weak condition on the symmetry of the error distribution, and leverages conformal tools to achieve FDR control in finite samples. Moreover, SENS incorporates structural insights from empirical Bayes into inference, exhibiting higher power compared to frequentist model-free methods. We conduct an in-depth analysis to establish a novel optimality theory for SENS under Efron's two-group model and demonstrate its superiority over existing empirical Bayes FDR methods and recent model-free FDR methods through numerical experiments on both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04231v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Tian, Zinan Zhao, Wenguang Sun</dc:creator>
    </item>
    <item>
      <title>How many patients could we save with LLM priors?</title>
      <link>https://arxiv.org/abs/2509.04250</link>
      <description>arXiv:2509.04250v1 Announce Type: new 
Abstract: Imagine a world where clinical trials need far fewer patients to achieve the same statistical power, thanks to the knowledge encoded in large language models (LLMs). We present a novel framework for hierarchical Bayesian modeling of adverse events in multi-center clinical trials, leveraging LLM-informed prior distributions. Unlike data augmentation approaches that generate synthetic data points, our methodology directly obtains parametric priors from the model. Our approach systematically elicits informative priors for hyperparameters in hierarchical Bayesian models using a pre-trained LLM, enabling the incorporation of external clinical expertise directly into Bayesian safety modeling. Through comprehensive temperature sensitivity analysis and rigorous cross-validation on real-world clinical trial data, we demonstrate that LLM-derived priors consistently improve predictive performance compared to traditional meta-analytical approaches. This methodology paves the way for more efficient and expert-informed clinical trial design, enabling substantial reductions in the number of patients required to achieve robust safety assessment and with the potential to transform drug safety monitoring and regulatory decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04250v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shota Arai, David Selby, Andrew Vargo, Sebastian Vollmer</dc:creator>
    </item>
    <item>
      <title>We Have It Covered: A Resampling-based Method for Uplift Model Comparison</title>
      <link>https://arxiv.org/abs/2509.04315</link>
      <description>arXiv:2509.04315v1 Announce Type: new 
Abstract: Uplift models play a critical role in modern marketing applications to help understand the incremental benefits of interventions and identify optimal targeting strategies. A variety of techniques exist for building uplift models, and it is essential to understand the model differences in the context of intended applications. The uplift curve is a widely adopted tool for assessing uplift model performance on the selection universe when observations are available for the entire population. However, when it is uneconomical or infeasible to select the entire population, it becomes difficult or even impossible to estimate the uplift curve without appropriate sampling design. To the best of our knowledge, no prior work has addressed uncertainty quantification of uplift curve estimates, which is essential for model comparisons. We propose a two-step sampling procedure and a resampling-based approach to compare uplift models with uncertainty quantification, examine the proposed method via simulations and real data applications, and conclude with a discussion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04315v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Liu, Chaoyu Yuan</dc:creator>
    </item>
    <item>
      <title>Selecting the Best Arm in One-Shot Multi-Arm RCTs: The Asymptotic Minimax-Regret Decision Framework for the Best-Population Selection Problem</title>
      <link>https://arxiv.org/abs/2509.03796</link>
      <description>arXiv:2509.03796v1 Announce Type: cross 
Abstract: We develop a frequentist decision-theoretic framework for selecting the best arm in one-shot, multi-arm randomized controlled trials (RCTs). Our approach characterizes the minimax-regret (MMR) optimal decision rule for any location-family reward distribution with full support. We show that the MMR rule is deterministic, unique, and computationally tractable, as it can be derived by solving the dual problem with nature's least-favorable prior. We then specialize to the case of multivariate normal (MVN) rewards with an arbitrary covariance matrix, and establish the local asymptotic minimaxity of a plug-in version of the rule when only estimated means and covariances are available. This asymptotic MMR (AMMR) procedure maps a covariance-matrix estimate directly into decision boundaries, allowing straightforward implementation in practice. Our analysis highlights a sharp contrast between two-arm and multi-arm designs. With two arms, the empirical success rule ("pick-the-winner") remains MMR-optimal, regardless of the arm-specific variances. By contrast, with three or more arms and heterogeneous variances, the empirical success rule is no longer optimal: the MMR decision boundaries become nonlinear and systematically penalize high-variance arms, requiring stronger evidence to select them. This result underscores that variance plays no role in optimal two-arm comparisons, but it matters critically when more than two options are on the table. Our multi-arm AMMR framework extends classical decision theory to multi-arm RCTs, offering a rigorous foundation and a practical tool for comparing multiple policies simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03796v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joonhwi Joo</dc:creator>
    </item>
    <item>
      <title>Counterfactual simulations for large scale systems with burnout variables</title>
      <link>https://arxiv.org/abs/2509.04038</link>
      <description>arXiv:2509.04038v1 Announce Type: cross 
Abstract: We consider large-scale systems influenced by burnout variables - state variables that start active, shape dynamics, and irreversibly deactivate once certain conditions are met. Simulating what-if scenarios in such systems is computationally demanding, as alternative trajectories often require sequential processing, which does not scale very well. This challenge arises in settings like online advertising, because of campaigns budgets, complicating counterfactual analysis despite rich data availability. We introduce a new type of algorithms based on what we refer to as uncertainty relaxation, that enables efficient parallel computation, significantly improving scalability for counterfactual estimation in systems with burnout variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04038v1</guid>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Heymann</dc:creator>
    </item>
    <item>
      <title>History matching for functional data and its application to tsunami warnings in the Indian Ocean</title>
      <link>https://arxiv.org/abs/2509.04342</link>
      <description>arXiv:2509.04342v1 Announce Type: cross 
Abstract: Traditional History Matching (HM) identifies implausible regions of the input parameter space by comparing scalar outputs of a computer model to observations. It offers higher computational efficiency than Bayesian calibration, making it suitable for high-dimensional problems. However, in real physical systems, outputs are often functional, such as time series or spatial fields, and conventional HM cannot fully exploit such information. We propose a novel method, Functional History Matching (FHM), which extends HM to handle functional data. FHM incorporates the Outer Product Emulator, an extension of the Gaussian Process emulator designed for time series, to enhance computational efficiency. FHM also leverages Random Projection to extract dynamic features from infinite-dimensional data, including derivatives. FHM supports uncertainty quantification essential for decision-making and naturally accommodates model discrepancies. To demonstrate its practical effectiveness, we apply FHM to a synthetic tsunami forecasting scenario in the Indian Ocean, assuming a realistic event in the Makran subduction zone. Wave elevation time series from offshore buoy data are used to predict wave elevations over the Indian coastline. Our results show that FHM significantly outperforms scalar-based HM in accuracy. FHM enables reliable forecasting from functional data within feasible computational constraints, offering a robust framework for early warning systems and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04342v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryuichi Kanai, Nicol\'as Hern\'andez, Devaraj Gopinathan, Serge Guillas</dc:creator>
    </item>
    <item>
      <title>Estimation of High-Dimensional Markov-Switching VAR Models with an Approximate EM Algorithm</title>
      <link>https://arxiv.org/abs/2210.07456</link>
      <description>arXiv:2210.07456v3 Announce Type: replace 
Abstract: Regime shifts in high-dimensional time series arise naturally in many applications, from neuroimaging to finance. This problem has received considerable attention in low-dimensional settings, with both Bayesian and frequentist methods used extensively for parameter estimation. The EM algorithm is a particularly popular strategy for parameter estimation in low-dimensional settings, although the statistical properties of the resulting estimates have not been well understood. Furthermore, its extension to high-dimensional time series has proved challenging. To overcome these challenges, in this paper we propose an approximate EM algorithm for Markov-switching VAR models that leads to efficient computation and also facilitates the investigation of asymptotic properties of the resulting parameter estimates. We establish the consistency of the proposed EM algorithm in high dimensions and investigate its performance via simulation studies. We also demonstrate the algorithm by analyzing a brain electroencephalography (EEG) dataset recorded on a patient experiencing epileptic seizure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.07456v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiudi Li, Abolfazl Safikhani, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Bootstrapping the Cross-Validation Estimate</title>
      <link>https://arxiv.org/abs/2307.00260</link>
      <description>arXiv:2307.00260v2 Announce Type: replace 
Abstract: Cross-validation is a widely used technique for evaluating the performance of prediction models, ranging from simple binary classification to complex precision medicine strategies. It helps correct for optimism bias in error estimates, which can be significant for models built using complex statistical learning algorithms. However, since the cross-validation estimate is a random value dependent on observed data, it is essential to accurately quantify the uncertainty associated with the estimate. This is especially important when comparing the performance of two models using cross-validation, as one must determine whether differences in estimated error are due to chance. Although various methods have been developed to make inferences on cross-validation estimates, they often have many limitations, such as requiring stringent model assumptions. This paper proposes a fast bootstrap method that quickly estimates the standard error of the cross-validation estimate and produces valid confidence intervals for a population parameter measuring average model performance. Our method overcomes the computational challenges inherent in bootstrapping a cross-validation estimate by estimating the variance component within a random-effects model. It is also as flexible as the cross-validation procedure itself. To showcase the effectiveness of our approach, we conducted comprehensive simulations and real-data analysis across two applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00260v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryan Cai, Yuanhui Luo, Xinzhou Guo, Fabio Pellegrini, Menglan Pang, Carl de Moor, Changyu Shen, Vivek Charu, Lu Tian</dc:creator>
    </item>
    <item>
      <title>The joint survival super learner: A super learner for right-censored data</title>
      <link>https://arxiv.org/abs/2405.17259</link>
      <description>arXiv:2405.17259v2 Announce Type: replace 
Abstract: Risk prediction models are widely used to guide real-world decision-making in areas such as healthcare and economics, and they also play a key role in estimating nuisance parameters in semiparametric inference. The super learner is a machine learning framework that combines a library of prediction algorithms into a meta-learner using cross-validated loss. In the context of right-censored data, careful consideration must be given to both the choice of loss function and the estimation of expected loss. Moreover, estimators such as inverse probability of censoring weighting require accurate modeling and an estimator of the censoring distribution. We propose a novel approach to super learning for survival analysis that jointly evaluates candidate learners for both the event-time distribution and the censoring distribution. Our method imposes no restrictions on the algorithms included in the library, accommodates competing risks, and does not rely on a single pre-specified estimator of the censoring distribution. We establish a finite-sample bound on the average price we pay for using cross-validation, and show that this price vanishes asymptotically, up to poly-logarithmic terms, provided that the size of the library does not grow faster than at a polynomial rate in the sample size. We demonstrate the practical utility of our method using prostate cancer data and compare it to existing super learner algorithms for survival analysis using synthesized data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17259v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anders Munch, Thomas A. Gerds</dc:creator>
    </item>
    <item>
      <title>Perturbation-Robust Predictive Modeling of Social Effects by Network Subspace Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2410.01163</link>
      <description>arXiv:2410.01163v5 Announce Type: replace 
Abstract: Network-linked data, where multivariate observations are interconnected by a network, are becoming increasingly prevalent in fields such as sociology and biology. These data often exhibit inherent noise and complex relational structures, complicating conventional modeling and statistical inference. Motivated by empirical challenges in analyzing such data sets, this paper introduces a family of network subspace generalized linear models designed for analyzing noisy, network-linked data. We propose a model inference method based on subspace-constrained maximum likelihood, which emphasizes flexibility in capturing network effects and provides a robust inference framework against network perturbations. We establish the asymptotic distributions of the estimators under network perturbations, demonstrating the method's accuracy through extensive simulations involving random network models and deep-learning-based embedding algorithms. The proposed methodology is applied to a comprehensive analysis of a large-scale study on school conflicts, where it identifies significant social effects, offering meaningful and interpretable insights into student behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01163v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianxiang Wang, Can M. Le, Tianxi Li</dc:creator>
    </item>
    <item>
      <title>dsld: A Socially Relevant Tool for Teaching Statistics</title>
      <link>https://arxiv.org/abs/2411.04228</link>
      <description>arXiv:2411.04228v3 Announce Type: replace 
Abstract: The growing influence of data science in statistics education requires tools that make key concepts accessible through real-world applications. We introduce "Data Science Looks At Discrimination" (dsld), an R package that provides a comprehensive set of analytical and graphical methods for examining issues of discrimination involving attributes such as race, gender, and age. By positioning fairness analysis as a teaching tool, the package enables instructors to demonstrate confounder effects, model bias, and related topics through applied examples. An accompanying 80-page Quarto book guides students and legal professionals in understanding these principles and applying them to real data. We describe the implementation of the package functions and illustrate their use with examples. Python interfaces are also available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04228v3</guid>
      <category>stat.ME</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aditya Mittal, Taha Abdullah, Arjun Ashok, Brandon Zarate Estrada, Shubhada Martha, Billy Ouattara, Jonathan Tran, Norman Matloff</dc:creator>
    </item>
    <item>
      <title>Difference-in-differences under network dependency and interference</title>
      <link>https://arxiv.org/abs/2502.03414</link>
      <description>arXiv:2502.03414v2 Announce Type: replace 
Abstract: Differences-in-differences (DiD) is a causal inference method for observational longitudinal data that assumes parallel expected potential outcome trajectories between treatment groups under the counterfactual scenario where all units receive a specific treatment. In this paper DiD is extended to allow for (i) network dependency, where outcomes, treatments, and covariates may exhibit between-unit correlation, (ii) interference, where treatments can affect outcomes in neighboring units, and (iii) network effect heterogeneity, where effects can vary based on a unit's position in the network. The causal estimand of interest is the network averaged expected exposure effect among units with a specific exposure level, where a unit's exposure is a function of its own treatment and its neighbors' treatments. Under a conditional parallel trends assumption and suitable network dependency conditions, a doubly robust estimator allowing for data-adaptive nuisance function estimation is proposed and shown to be consistent and asymptotically normal. The proposed methods are evaluated in simulations and applied to study the effects of adopting emission control technologies in coal power plants on county-level mortality due to cardiovascular disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03414v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Jetsupphasuk, Didong Li, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Gold after Randomized Sand: Model-X Split Knockoffs for Controlled Transformation Selection</title>
      <link>https://arxiv.org/abs/2507.01732</link>
      <description>arXiv:2507.01732v2 Announce Type: replace 
Abstract: Controlling the False Discovery Rate (FDR) is critical for reproducible variable selection, especially given the prevalence of complex predictive modeling. The recent Split Knockoff method, an extension of the canonical Knockoffs framework, offers finite-sample FDR control for selecting sparse transformations but is limited to linear models with fixed designs. Extending this framework to random designs, which would accommodate a much broader range of models, is challenged by the fundamental difficulty of reconciling a random covariate design with a deterministic linear transformation. To bridge this gap, we introduce Model-X Split Knockoffs. Our method achieves robust FDR control for transformation selection in random designs by introducing a novel auxiliary randomized design. This key innovation effectively mediates the interaction between the random design and the deterministic transformation, enabling the construction of valid knockoffs. Like the classical Model-X framework, our approach provides provable, finite-sample FDR control under known or accurately estimated covariate distributions, regardless of the response's conditional distribution. Importantly, it guarantees at least the same, and often superior, selection power as standard Model-X Knockoffs when both are applicable. Empirical studies, including simulations and real-world applications to Alzheimer's disease imaging and university ranking analysis, demonstrate robust FDR control and improved statistical power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01732v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Cao, Hangyu Lin, Xinwei Sun, Yuan Yao</dc:creator>
    </item>
    <item>
      <title>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</title>
      <link>https://arxiv.org/abs/2505.18102</link>
      <description>arXiv:2505.18102v5 Announce Type: replace-cross 
Abstract: Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18102v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takashi Ishida, Thanawat Lodkaew, Ikko Yamane</dc:creator>
    </item>
    <item>
      <title>Design-Based and Network Sampling-Based Uncertainties in Network Experiments</title>
      <link>https://arxiv.org/abs/2506.22989</link>
      <description>arXiv:2506.22989v2 Announce Type: replace-cross 
Abstract: Ordinary least squares (OLS) estimators are widely used in network experiments to estimate spillover effects. We study the causal interpretation of, and inference for the OLS estimator under both design-based uncertainty from random treatment assignment and sampling-based uncertainty in network links. We show that correlations among regressors that capture the exposure to neighbors' treatments can induce contamination bias, preventing OLS from aggregating heterogeneous spillover effects for a clear causal interpretation. We derive the OLS estimator's asymptotic distribution and propose a network-robust variance estimator. Simulations and an empirical application demonstrate that contamination bias can be substantial, leading to inflated spillover estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22989v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kensuke Sakamoto, Yuya Shimizu</dc:creator>
    </item>
  </channel>
</rss>
