<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 02:42:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Physics Informed Neural Network using Finite Difference Method</title>
      <link>https://arxiv.org/abs/2602.21590</link>
      <description>arXiv:2602.21590v1 Announce Type: new 
Abstract: In recent engineering applications using deep learning, physics-informed neural network (PINN) is a new development as it can exploit the underlying physics of engineering systems. The novelty of PINN lies in the use of partial differential equations (PDE) for the loss function. Most PINNs are implemented using automatic differentiation (AD) for training the PDE loss functions. A lesser well-known study is the use of finite difference method (FDM) as an alternative. Unlike an AD based PINN, an immediate benefit of using a FDM based PINN is low implementation cost. In this paper, we propose the use of finite difference method for estimating the PDE loss functions in PINN. Our work is inspired by computational analysis in electromagnetic systems that traditionally solve Laplace's equation using successive over-relaxation. In the case of Laplace's equation, our PINN approach can be seen as taking the Laplacian filter response of the neural network output as the loss function. Thus, the implementation of PINN can be very simple. In our experiments, we tested PINN on Laplace's equation and Burger's equation. We showed that using FDM, PINN consistently outperforms non-PINN based deep learning. When comparing to AD based PINNs, we showed that our method is faster to compute as well as on par in terms of error reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21590v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kart Leong Lim, Rahul Dutta, Mihai Rotaru</dc:creator>
    </item>
    <item>
      <title>Inverse prediction of capacitor multiphysics dynamic parameters using deep generative model</title>
      <link>https://arxiv.org/abs/2602.21606</link>
      <description>arXiv:2602.21606v1 Announce Type: new 
Abstract: Finite element simulations are run by package design engineers to model design structures. The process is irreversible meaning every minute structural adjustment requires a fresh input parameter run. In this paper, the problem of modeling changing (small) design structures through varying input parameters is known as inverse prediction. We demonstrate inverse prediction on the electrostatics field of an air-filled capacitor dataset where the structural change is affected by a dynamic parameter to the boundary condition. Using recent AI such as deep generative model, we outperformed best baseline on inverse prediction both visually and in terms of quantitative measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21606v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kart-Leong Lim, Rahul Dutta, Mihai Rotaru</dc:creator>
    </item>
    <item>
      <title>Intrusive and Non-Intrusive Model Order Reduction for Airborne Contaminant Transport: Comparative Analysis and Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2602.21996</link>
      <description>arXiv:2602.21996v1 Announce Type: new 
Abstract: Numerical simulations of contaminant dispersion, as after a gas leakage incident on a chemical plant, can provide valuable insights for both emergency response and preparedness. Simulation approaches combine incompressible Navier-Stokes (INS) equations with advection-diffusion (AD) processes to model wind and concentration field. However, the computational cost of such high-fidelity simulations increases rapidly for complex geometries like urban environments, making them unfeasible in time-critical or multi-query "what-if" scenarios. Therefore, this study focuses on the application of model order reduction (MOR) techniques enabling fast yet accurate predictions. To this end, a thorough comparison of intrusive and non-intrusive MOR methods is performed for the computationally more demanding parametric INS problem with varying wind velocities. Based on these insights, a non-intrusive reduced-order model (ROM) is constructed accounting for both wind velocity and direction. The study is conducted on a two-dimensional domain derived from real-world building footprints, preserving key features for analyzing the dispersion of, for instance, denser contaminants. The resulting ROM enables faster than real-time predictions of spatio-temporal contaminant dispersion from an instantaneous source under varying wind conditions. This capability allows assessing wind measurement uncertainties through a Monte Carlo analysis. To demonstrate the practical applicability, an interactive dashboard provides intuitive access to simulation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21996v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lisa K\"uhn, Jacopo Bonari, Max von Danwitz, Alexander Popp</dc:creator>
    </item>
    <item>
      <title>Signed network models for dimensionality reduction of portfolio optimization</title>
      <link>https://arxiv.org/abs/2602.21362</link>
      <description>arXiv:2602.21362v1 Announce Type: cross 
Abstract: In this paper, we develop a time-series-based signed network model for dimensionality reduction in portfolio optimization, grounded in Markowitz's portfolio theory and extended to incorporate higher-order moments of asset return distributions. Unlike traditional correlation-based approaches, we construct a complete signed graph for each trading day within a specified time window, where the sign of an edge between a pair of assets is determined by the relative behavior of their log returns with respect to their mean returns. Within this framework, we introduce a combinatorial interpretation of higher-order moments, showing that maximizing skewness and minimizing kurtosis correspond to maximizing balanced triangles and balanced 4-cliques with specific signed edge configurations respectively. We establish that the latter leads to an NP-hard combinatorial optimization problem, while the former is naturally guaranteed by the structural properties of the signed graph model. Based on this interpretation, we propose a dimensionality reduction method using a combinatorial formulation of the mean-variance optimization problem through a combinatorial hedge score metric for assets. The proposed framework is validated through extensive backtesting on 199 S\&amp;P 500 assets over a 16-year period (2006 - 2021), demonstrating the effectiveness of reduced asset universes for portfolio construction using both Markowitz optimization and equally weighted strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21362v1</guid>
      <category>math.CO</category>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bibhas Adhikari</dc:creator>
    </item>
    <item>
      <title>VCDF: A Validated Consensus-Driven Framework for Time Series Causal Discovery</title>
      <link>https://arxiv.org/abs/2602.21381</link>
      <description>arXiv:2602.21381v1 Announce Type: cross 
Abstract: Time series causal discovery is essential for understanding dynamic systems, yet many existing methods remain sensitive to noise, non-stationarity, and sampling variability. We propose the Validated Consensus-Driven Framework (VCDF), a simple and method-agnostic layer that improves robustness by evaluating the stability of causal relations across blocked temporal subsets. VCDF requires no modification to base algorithms and can be applied to methods such as VAR-LiNGAM and PCMCI. Experiments on synthetic datasets show that VCDF improves VAR-LiNGAM by approximately 0.08-0.12 in both window and summary F1 scores across diverse data characteristics, with gains most pronounced for moderate-to-long sequences. The framework also benefits from longer sequences, yielding up to 0.18 absolute improvement on time series of length 1000 and above. Evaluations on simulated fMRI data and IT-monitoring scenarios further demonstrate enhanced stability and structural accuracy under realistic noise conditions. VCDF provides an effective reliability layer for time series causal discovery without altering underlying modeling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21381v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gene Yu, Ce Guo, Wayne Luk</dc:creator>
    </item>
    <item>
      <title>An index of effective number of variables for uncertainty and reliability analysis in model selection problems</title>
      <link>https://arxiv.org/abs/2602.21403</link>
      <description>arXiv:2602.21403v1 Announce Type: cross 
Abstract: An index of an effective number of variables (ENV) is introduced for model selection in nested models. This is the case, for instance, when we have to decide the order of a polynomial function or the number of bases in a nonlinear regression, choose the number of clusters in a clustering problem, or the number of features in a variable selection application (to name few examples). It is inspired by the idea of the maximum area under the curve (AUC). The interpretation of the ENV index is identical to the effective sample size (ESS) indices concerning a set of samples. The ENV index improves {drawbacks of} the elbow detectors described in the literature and introduces different confidence measures of the proposed solution. These novel measures can be also employed jointly with the use of different information criteria, such as the well-known AIC and BIC, or any other model selection procedures. Comparisons with classical and recent schemes are provided in different experiments involving real datasets. Related Matlab code is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21403v1</guid>
      <category>stat.ME</category>
      <category>cs.CE</category>
      <category>eess.SP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.sigpro.2024.109735</arxiv:DOI>
      <arxiv:journal_reference>Signal Processing, Volume 227, Pages 1-9, 2025. Num. 109735</arxiv:journal_reference>
      <dc:creator>Luca Martino, Eduardo Morgado, Roberto San Mill\'an-Castillo</dc:creator>
    </item>
    <item>
      <title>ABM-UDE: Developing Surrogates for Epidemic Agent-Based Models via Scientific Machine Learning</title>
      <link>https://arxiv.org/abs/2602.21588</link>
      <description>arXiv:2602.21588v1 Announce Type: cross 
Abstract: Agent-based epidemic models (ABMs) encode behavioral and policy heterogeneity but are too slow for nightly hospital planning. We develop county-ready surrogates that learn directly from exascale ABM trajectories using Universal Differential Equations (UDEs): mechanistic SEIR-family ODEs with a neural-parameterized contact rate $\kappa_\phi(u,t)$ (no additive residual). Our contributions are threefold: we adapt multiple shooting and an observer-based prediction-error method (PEM) to stabilize identification of neural-augmented epidemiological dynamics across intervention-driven regime shifts; we enforce positivity and mass conservation and show the learned contact-rate parameterization yields a well-posed vector field; and we quantify accuracy, calibration, and compute against ABM ensembles and UDE baselines. On a representative ExaEpi scenario, PEM-UDE reduces mean MSE by 77% relative to single-shooting UDE (3.00 vs. 13.14) and by 20% relative to MS-UDE (3.75). Reliability improves in parallel: empirical coverage of ABM $10$-$90$% and $25$-$75$% bands rises from 0.68/0.43 (UDE) and 0.79/0.55 (MS-UDE) to 0.86/0.61 with PEM-UDE and 0.94/0.69 with MS+PEM-UDE, indicating calibrated uncertainty rather than overconfident fits. Inference runs in seconds on commodity CPUs (20-35 s per $\sim$90-day forecast), enabling nightly ''what-if'' sweeps on a laptop. Relative to a $\sim$100 CPU-hour ABM reference run, this yields $\sim10^{4}\times$ lower wall-clock per scenario. This closes the realism-cadence gap, supports threshold-aware decision-making (e.g., maintaining ICU occupancy $&lt;75$%), preserves mechanistic interpretability, and enables calibrated, risk-aware scenario planning on standard institutional hardware. Beyond epidemics, the ABM$\to$UDE recipe provides a portable path to distill agent-based simulators into fast, trustworthy surrogates for other scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21588v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sharv Murgai, Utkarsh Utkarsh, Kyle C. Nguyen, Alan Edelman, Erin C. S. Acquesta, Christopher Vincent Rackauckas</dc:creator>
    </item>
    <item>
      <title>Deep Clustering based Boundary-Decoder Net for Inter and Intra Layer Stress Prediction of Heterogeneous Integrated IC Chip</title>
      <link>https://arxiv.org/abs/2602.21601</link>
      <description>arXiv:2602.21601v1 Announce Type: cross 
Abstract: High stress occurs when 3D heterogeneous IC packages are subjected to thermal cycling at extreme temperatures. Stress mainly occurs at the interface between different materials. We investigate stress image using latent space representation which is based on using deep generative model (DGM). However, most DGM approaches are unsupervised, meaning they resort to image pairing (input and output) to train DGM. Instead, we rely on a recent boundary-decoder (BD) net, which uses boundary condition and image pairing for stress modeling. The boundary net maps material parameters to the latent space co-shared by its image counterpart. Because such a setup is dimensionally wise ill-posed, we further couple BD net with deep clustering. To access the performance of our proposed method, we simulate an IC chip dataset comprising of 1825 stress images. We compare our new approach using variants of BD net as well as a baseline approach. We show that our approach is able to outperform all the comparison in terms of train and test error reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21601v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kart Leong Lim, Ji Lin</dc:creator>
    </item>
    <item>
      <title>MolFORM: Preference-Aligned Multimodal Flow Matching for Structure-Based Drug Design</title>
      <link>https://arxiv.org/abs/2507.05503</link>
      <description>arXiv:2507.05503v3 Announce Type: replace 
Abstract: Structure-based drug design (SBDD) aims to efficiently discover high-affinity ligands within vast chemical spaces. However, current generative models struggle with objective misalignment and rigid sampling budgets. We present MolFORM, a fast multi-modal flow matching framework for discrete atom types and continuous coordinates. Crucially, to bridge the gap between generative capability and biochemical objectives, we introduce two distinct post-training strategies: (1) Direct Preference Optimization (DPO), which performs offline alignment using ranked preference pairs; and (2) an online reinforcement learning paradigm that optimizes the generative flow directly on the forward process. Both strategies effectively navigate the chemical space toward high-affinity regions. MolFORM achieves state-of-the-art results on the CrossDocked2020 benchmark (Vina Score -7.60, Diversity 0.75), demonstrating that incorporating preference alignment mechanisms-whether via offline optimization or online reinforcement-is crucial for steering generative models toward high-affinity binding regions. The source code for MolFORM is publicly available at https://github.com/daiheng-zhang/SBDD-MolFORM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05503v3</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daiheng Zhang, Zhao Zhang</dc:creator>
    </item>
    <item>
      <title>Error bounded compression for weather and climate applications</title>
      <link>https://arxiv.org/abs/2510.22265</link>
      <description>arXiv:2510.22265v2 Announce Type: replace 
Abstract: As the resolution of weather and climate simulations increases, the amount of data produced is growing rapidly from hundreds of terabytes to tens of petabytes. The huge size becomes a limiting factor for broader adoption, and its fast growth rate will soon exhaust all the available storage devices. To address these issues, we present EBCC (Error Bounded Climate-data Compressor). It follows a two-layer approach: a base compression layer using JPEG2000 to capture the bulk of the data with a high compression ratio, and a residual compression layer using wavelet transform and SPIHT encoding to efficiently eliminate long-tail extreme errors introduced by the base compression layer. It incorporates a feedback rate-control mechanism for both layers that adjusts compression ratios to achieve the specified maximum error target. We evaluate EBCC alongside other established compression methods on benchmarks related to weather and climate science including error statistics, a case study on primitive and derived variables near a hurricane, evaluation of the closure of the global energy budget, and a Lagrangian air parcel trajectory simulation. This is the first time that trajectory simulation is used to benchmark compression methods. Our method concentrates most errors near zero, while others tend to distribute errors uniformly within the error bound. EBCC outperforms other methods in the benchmarks at relative error targets ranging from 0.1% to 10% and achieves compression ratios from 15x to more than 300x. In the energy budget closure and Lagrangian trajectory benchmarks, it can achieve more than 100x compression while keeping errors within natural variability derived from ERA5 uncertainty members. This verifies the effectiveness of EBCC in creating heavily compressed weather and climate datasets suitable for downstream applications. The source code of EBCC is available in github.com/spcl/EBCC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22265v2</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Langwen Huang, Luigi Fusco, Florian Scheidl, Jan Zibell, Michael Armand Sprenger, Sebastian Schemm, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery</title>
      <link>https://arxiv.org/abs/2602.13769</link>
      <description>arXiv:2602.13769v2 Announce Type: replace-cross 
Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We introduce a hierarchical optimization-inspired reflection system in which short-term reflections act as verbal gradients, long-term reflections as verbal momentum, and memory compression as semantic weight decay, collectively forming a principled mechanism for governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. All code and experimental data are publicly available at https://github.com/qiliuchn/OR-Agent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13769v2</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.NE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Liu, Ruochen Hao, Can Li, Wanjing Ma</dc:creator>
    </item>
    <item>
      <title>WeirNet: A Large-Scale 3D CFD Benchmark for Geometric Surrogate Modeling of Piano Key Weirs</title>
      <link>https://arxiv.org/abs/2602.20714</link>
      <description>arXiv:2602.20714v2 Announce Type: replace-cross 
Abstract: Reliable prediction of hydraulic performance is challenging for Piano Key Weir (PKW) design because discharge capacity depends on three-dimensional geometry and operating conditions. Surrogate models can accelerate hydraulic-structure design, but progress is limited by scarce large, well-documented datasets that jointly capture geometric variation, operating conditions, and functional performance. This study presents WeirNet, a large 3D CFD benchmark dataset for geometric surrogate modeling of PKWs. WeirNet contains 3,794 parametric, feasibility-constrained rectangular and trapezoidal PKW geometries, each scheduled at 19 discharge conditions using a consistent free-surface OpenFOAM workflow, resulting in 71,387 completed simulations that form the benchmark and with complete discharge coefficient labels. The dataset is released as multiple modalities compact parametric descriptors, watertight surface meshes and high-resolution point clouds together with standardized tasks and in-distribution and out-of-distribution splits. Representative surrogate families are benchmarked for discharge coefficient prediction. Tree-based regressors on parametric descriptors achieve the best overall accuracy, while point- and mesh-based models remain competitive and offer parameterization-agnostic inference. All surrogates evaluate in milliseconds per sample, providing orders-of-magnitude speedups over CFD runtimes. Out-of-distribution results identify geometry shift as the dominant failure mode compared to unseen discharge values, and data-efficiency experiments show diminishing returns beyond roughly 60% of the training data. By publicly releasing the dataset together with simulation setups and evaluation pipelines, WeirNet establishes a reproducible framework for data-driven hydraulic modeling and enables faster exploration of PKW designs during the early stages of hydraulic planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20714v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lisa L\"uddecke, Michael Hohmann, Sebastian Eilermann, Jan Tillmann-Mumm, Pezhman Pourabdollah, Mario Oertel, Oliver Niggemann</dc:creator>
    </item>
  </channel>
</rss>
