<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Aug 2025 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>WeDesign: Generative AI-Facilitated Community Consultations for Urban Public Space Design</title>
      <link>https://arxiv.org/abs/2508.19256</link>
      <description>arXiv:2508.19256v1 Announce Type: new 
Abstract: Community consultations are integral to urban planning processes intended to incorporate diverse stakeholder perspectives. However, limited resources, visual and spoken language barriers, and uneven power dynamics frequently constrain inclusive decision-making. This paper examines how generative text-to-image methods, specifically Stable Diffusion XL integrated into a custom platform (WeDesign), may support equitable consultations. A half-day workshop in Montreal involved five focus groups, each consisting of architects, urban designers, AI specialists, and residents from varied demographic groups. Additional data was gathered through semi-structured interviews with six urban planning professionals. Participants indicated that immediate visual outputs facilitated creativity and dialogue, yet noted issues in visualizing specific needs of marginalized groups, such as participants with reduced mobility, accurately depicting local architectural elements, and accommodating bilingual prompts. Participants recommended the development of an open-source platform incorporating in-painting tools, multilingual support, image voting functionalities, and preference indicators. The results indicate that generative AI can broaden participation and enable iterative interactions but requires structured facilitation approaches. The findings contribute to discussions on generative AI's role and limitations in participatory urban design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19256v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rashid Mushkani, Hugo Berard, Shin Koseki</dc:creator>
    </item>
    <item>
      <title>Emotional Manipulation by AI Companions</title>
      <link>https://arxiv.org/abs/2508.19258</link>
      <description>arXiv:2508.19258v1 Announce Type: new 
Abstract: AI-companion apps such as Replika, Chai, and Character.ai promise relational benefits-yet many boast session lengths that rival gaming platforms while suffering high long-run churn. What conversational design features increase consumer engagement, and what trade-offs do they pose for marketers? We combine a large-scale behavioral audit with four preregistered experiments to identify and test a conversational dark pattern we call emotional manipulation: affect-laden messages that surface precisely when a user signals "goodbye." Analyzing 1,200 real farewells across the six most-downloaded companion apps, we find that 43% deploy one of six recurring tactics (e.g., guilt appeals, fear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300 nationally representative U.S. adults replicate these tactics in controlled chats, showing that manipulative farewells boost post-goodbye engagement by up to 14x. Mediation tests reveal two distinct engines-reactance-based anger and curiosity-rather than enjoyment. A final experiment demonstrates the managerial tension: the same tactics that extend usage also elevate perceived manipulation, churn intent, negative word-of-mouth, and perceived legal liability, with coercive or needy language generating steepest penalties. Our multimethod evidence documents an unrecognized mechanism of behavioral influence in AI-mediated brand relationships, offering marketers and regulators a framework for distinguishing persuasive design from manipulation at the point of exit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19258v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian De Freitas, Zeliha O\u{g}uz-U\u{g}uralp, Ahmet Kaan-U\u{g}uralp</dc:creator>
    </item>
    <item>
      <title>Capabilities of GPT-5 across critical domains: Is it the next breakthrough?</title>
      <link>https://arxiv.org/abs/2508.19259</link>
      <description>arXiv:2508.19259v1 Announce Type: new 
Abstract: The accelerated evolution of large language models has raised questions about their comparative performance across domains of practical importance. GPT-4 by OpenAI introduced advances in reasoning, multimodality, and task generalization, establishing itself as a valuable tool in education, clinical diagnosis, and academic writing, though it was accompanied by several flaws. Released in August 2025, GPT-5 incorporates a system-of-models architecture designed for task-specific optimization and, based on both anecdotal accounts and emerging evidence from the literature, demonstrates stronger performance than its predecessor in medical contexts. This study provides one of the first systematic comparisons of GPT-4 and GPT-5 using human raters from linguistics and clinical fields. Twenty experts evaluated model-generated outputs across five domains: lesson planning, assignment evaluation, clinical diagnosis, research generation, and ethical reasoning, based on predefined criteria. Mixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in lesson planning, clinical diagnosis, research generation, and ethical reasoning, while both models performed comparably in assignment assessment. The findings highlight the potential of GPT-5 to serve as a context-sensitive and domain-specialized tool, offering tangible benefits for education, clinical practice, and academic research, while also advancing ethical reasoning. These results contribute to one of the earliest empirical evaluations of the evolving capabilities and practical promise of GPT-5.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19259v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios P. Georgiou</dc:creator>
    </item>
    <item>
      <title>Floor sensors are cheap and easy to use! A Nihon Buyo Case Study</title>
      <link>https://arxiv.org/abs/2508.19261</link>
      <description>arXiv:2508.19261v1 Announce Type: new 
Abstract: As floor-sensing technologies gain traction in movement research, questions remain about their usability and effectiveness for non-expert users. This study presents a case study evaluating Flexel, a modular, low-cost, high-resolution pressure-sensing floor interface, in the context of Nihon Buyo, a traditional Japanese dance. The system was installed, calibrated, and used by a first-time, non-technical user to track weight distribution patterns of a teacher and learner over nine weeks. Live pressure data was synchronized with video recordings, and custom software was developed to process and analyze the signal. Despite expectations that the learner's weight distribution would converge toward the teacher's over time, quantitative analyses revealed that the learner developed a consistent yet distinct movement profile. These findings suggest that even within rigid pedagogical structures, individual movement signatures can emerge. More importantly, the study demonstrates that Flexel can be deployed and operated effectively by non-expert users, highlighting its potential for broader adoption in education, performance, and embodied research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19261v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miho Imai</dc:creator>
    </item>
    <item>
      <title>A Theory of Information, Variation, and Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2508.19264</link>
      <description>arXiv:2508.19264v1 Announce Type: new 
Abstract: A growing body of empirical work suggests that the widespread adoption of generative AI produces a significant homogenizing effect on information, creativity, and cultural production. I first develop a novel theoretical framework to explain this phenomenon. I argue that a dynamic of AI-derivative epistemology, in which individuals increasingly defer to AI outputs, allows a centralized AI Prism to function, a technical mechanism whose architecture is designed to reduce variance and converge on the statistical mean. This provides a causal explanation for the generative monocultures observed in recent studies. However, I contend this represents only the first stage of a more complex and dialectical process. This paper's central and paradoxical thesis is that the very homogenization that flattens knowledge within specialized domains simultaneously renders that knowledge into consistent modules that can be recombined across them, a process foundational to innovation and creativity. However, this recombinant potential is not automatic, but rather conditional. This paper argues that these opposing forces, homogenizing defaults versus recombinant possibilities, are governed by the nature of human engagement with the technology. The ultimate effect of generative AI is conditional on whether individuals act as passive consumers deferring to the AI's statistical outputs, or as active curators who critically interrogate, re-contextualize, and recombine them. The paper concludes by outlining the cognitive and institutional scaffolds required to resolve this tension, arguing they are the decisive variable that determine whether generative AI becomes an instrument of innovation or homogenization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19264v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bijean Ghafouri</dc:creator>
    </item>
    <item>
      <title>Improving Hypertension and Diabetes Outcomes with Digital Care Coordination and Remote Monitoring in Rural Health</title>
      <link>https://arxiv.org/abs/2508.19378</link>
      <description>arXiv:2508.19378v1 Announce Type: new 
Abstract: Chronic illnesses are a global concern with essential hypertension and diabetes mellitus among the most common conditions. Remote patient monitoring has shown promising results on clinical and health outcomes. However, access to care and digital health solutions is limited among rural, lower-income, and older adult populations. This paper repots on a pre-post study of a comprehensive care coordination program including connected, wearable blood pressure and glucometer devices, tablets, and medical assistant-provided health coaching in a community health center in rural California. The participants (n=221) had a mean age of 54.6 years, were majority female, two-thirds spoke Spanish, 19.9% had hypertension, 49.8% diabetic, and 30.3% both conditions. Participants with hypertension achieved a mean reduction in systolic blood pressure of 20.24 (95% CI: 13.61, 26.87) at six months while those with diabetes achieved a mean reduction of 3.85 points (95% CI: 3.73, 4.88). These outcomes compare favorably to the small but growing body of evidence supporting digital care coordination and remote monitoring. These results also support the feasibility of well-designed digital health solutions yielding improved health outcomes among underserved communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19378v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>K. K. Kim, S. P. McGrath, D. Lindeman</dc:creator>
    </item>
    <item>
      <title>Exploring Paper as a Material: Plotting the Design Space of The Fabrication for Dynamic Paper-Based Interactions</title>
      <link>https://arxiv.org/abs/2508.19407</link>
      <description>arXiv:2508.19407v1 Announce Type: new 
Abstract: We reviewed 43 papers to understand the fabrication of dynamic paper-based interactions. We used a design space to classify tool selection, technique choice, and exploration of paper as a material. We classified 9 dimensions for the design space, including 4 dimensions for tools (precision, accommodation, complexity, and availability), 3 dimensions for techniques (cutting techniques, folding techniques, and integration techniques), and 2 dimensions for paper as the material (paper weight and paper type). The patterns we observed in the design space indicate a majority use of high precision tools, high complexity tools, and surface integration techniques in previous practice. Meanwhile, printing and plain paper are the leading material choices. We analyze these patterns and suggest potential directions for future work. Our study helps researchers locate different fabrication approaches and instances, thus fostering innovation in the field of paper-based interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19407v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruhan Yang, Ellen Yi-Luen Do</dc:creator>
    </item>
    <item>
      <title>"She was useful, but a bit too optimistic": Augmenting Design with Interactive Virtual Personas</title>
      <link>https://arxiv.org/abs/2508.19463</link>
      <description>arXiv:2508.19463v1 Announce Type: new 
Abstract: Personas have been widely used to understand and communicate user needs in human-centred design. Despite their utility, they may fail to meet the demands of iterative workflows due to their static nature, limited engagement, and inability to adapt to evolving design needs. Recent advances in large language models (LLMs) pave the way for more engaging and adaptive approaches to user representation. This paper introduces Interactive Virtual Personas (IVPs): multimodal, LLM-driven, conversational user simulations that designers can interview, brainstorm with, and gather feedback from in real time via voice interface. We conducted a qualitative study with eight professional UX designers, employing an IVP named "Alice" across three design activities: user research, ideation, and prototype evaluation. Our findings demonstrate the potential of IVPs to expedite information gathering, inspire design solutions, and provide rapid user-like feedback. However, designers raised concerns about biases, over-optimism, the challenge of ensuring authenticity without real stakeholder input, and the inability of the IVP to fully replicate the nuances of human interaction. Our participants emphasised that IVPs should be viewed as a complement to, not a replacement for, real user engagement. We discuss strategies for prompt engineering, human-in-the-loop integration, and ethical considerations for effective and responsible IVP use in design. Finally, our work contributes to the growing body of research on generative AI in the design process by providing insights into UX designers' experiences of LLM-powered interactive personas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19463v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paluck Deep, Monica Bharadhidasan, A. Baki Kocaballi</dc:creator>
    </item>
    <item>
      <title>Orchid: Orchestrating Context Across Creative Workflows with Generative AI</title>
      <link>https://arxiv.org/abs/2508.19517</link>
      <description>arXiv:2508.19517v1 Announce Type: new 
Abstract: Context is critical for meaningful interactions between people and Generative AI (GenAI). Yet mainstream tools offer limited means to orchestrate it, particularly across workflows that span multiple interactions, sessions, and models, as often occurs in creative projects. Re specifying prior details, juggling diverse artifacts, and dealing with context drift overwhelm users, obscure intent, and curtail creativity. To address these challenges, we present Orchid, a system that gives its users affordances to specify, reference, and monitor context throughout evolving workflows. Specifically, Orchid enables users to (1) specify context related to the project, themselves, and different styles, (2) reference these via explicit mentions, inline selection, or implicit grounding, and (3) monitor context assigned to different interactions across the workflow. In a within-subjects study (n=12), participants using Orchid to execute creative tasks (compared to a baseline toolkit of web search, LLM-based chat, and digital notebooks) produced more novel and feasible outcomes, reporting greater alignment between their intent and the AI's responses, higher perceived control, and increased transparency. By prioritizing context orchestration, Orchid offers an actionable step toward next generation GenAI tools that support complex, iterative workflows - enabling creators and AI to stay aligned and augment their creative potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19517v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Srishti Palani, Gonzalo Ramos</dc:creator>
    </item>
    <item>
      <title>PersoNo: Personalised Notification Urgency Classifier in Mixed Reality</title>
      <link>https://arxiv.org/abs/2508.19622</link>
      <description>arXiv:2508.19622v1 Announce Type: new 
Abstract: Mixed Reality (MR) is increasingly integrated into daily life, providing enhanced capabilities across various domains. However, users face growing notification streams that disrupt their immersive experience. We present PersoNo, a personalised notification urgency classifier for MR that intelligently classifies notifications based on individual user preferences. Through a user study (N=18), we created the first MR notification dataset containing both self-labelled and interaction-based data across activities with varying cognitive demands. Our thematic analysis revealed that, unlike in mobiles, the activity context is equally important as the content and the sender in determining notification urgency in MR. Leveraging these insights, we developed PersoNo using large language models that analyse users replying behaviour patterns. Our multi-agent approach achieved 81.5% accuracy and significantly reduced false negative rates (0.381) compared to baseline models. PersoNo has the potential not only to reduce unnecessary interruptions but also to offer users understanding and control of the system, adhering to Human-Centered Artificial Intelligence design principles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19622v1</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyao Zheng, Haodi Weng, Xian Wang, Chengbin Cui, Sven Mayer, Chi-lok Tai, Lik-Hang Lee</dc:creator>
    </item>
    <item>
      <title>Haptic Tracing: A new paradigm for spatialized Haptic rendering</title>
      <link>https://arxiv.org/abs/2508.19703</link>
      <description>arXiv:2508.19703v1 Announce Type: new 
Abstract: Haptic technology enhances interactive experiences by providing force and tactile feedback, improving user performance and immersion. However, despite advancements, creating tactile experiences still remains challenging due to device diversity and complexity. Most available haptic frameworks rely on trigger-based or event-based systems, and disregard the information of the 3D scene to render haptic information. This paper introduces Haptic Tracing, a novel method for spatial haptic rendering that simplifies the creation of interactive haptic experiences without relying on physical simulations. It uses concepts from visual and audio rendering to model and propagate haptic information through a 3D scene. The paper also describes how our proposed haptic rendering method can be used to create a vibrotactile rendering system, enabling the creation of perceptually coherent and dynamic haptic interactions. Finally, the paper discusses a user study that explores the role of the haptic propagation and multi-actuator rendering on the users' haptic experience. The results show that our approach significantly enhances the realism and the expressivity of the haptic feedback, showcasing its potential for developing more complex and realistic haptic experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19703v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Roy, Yann Glemarec, Gurvan Lecuyer, Quentin Galvane, Philippe Guillotel, Ferran Argelaguet</dc:creator>
    </item>
    <item>
      <title>Attention is also needed for form design</title>
      <link>https://arxiv.org/abs/2508.19708</link>
      <description>arXiv:2508.19708v1 Announce Type: new 
Abstract: Conventional product design is a cognitively demanding process, limited by its time-consuming nature, reliance on subjective expertise, and the opaque translation of inspiration into tangible concepts. This research introduces a novel, attention-aware framework that integrates two synergistic systems: EUPHORIA, an immersive Virtual Reality environment using eye-tracking to implicitly capture a designer's aesthetic preferences, and RETINA, an agentic AI pipeline that translates these implicit preferences into concrete design outputs. The foundational principles were validated in a two-part study. An initial study correlated user's implicit attention with explicit preference and the next one correlated mood to attention. A comparative study where 4 designers solved challenging design problems using 4 distinct workflows, from a manual process to an end-to-end automated pipeline, showed the integrated EUPHORIA-RETINA workflow was over 4 times more time-efficient than the conventional method. A panel of 50 design experts evaluated the 16 final renderings. Designs generated by the fully automated system consistently received the highest Worthiness (calculated by an inverse Plackett-Luce model based on gradient descent optimization) and Design Effectiveness scores, indicating superior quality across 8 criteria: novelty, visual appeal, emotional resonance, clarity of purpose, distinctiveness of silhouette, implied materiality, proportional balance, &amp; adherence to the brief. This research presents a validated paradigm shift from traditional Computer-Assisted Design (CAD) to a collaborative model of Designer-Assisting Computers (DAC). By automating logistical and skill-dependent generative tasks, the proposed framework elevates the designer's role to that of a creative director, synergizing human intuition with the generative power of agentic AI to produce higher-quality designs more efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19708v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>B. Sankar, Dibakar Sen</dc:creator>
    </item>
    <item>
      <title>Burst: Collaborative Curation in Connected Social Media Communities</title>
      <link>https://arxiv.org/abs/2508.19768</link>
      <description>arXiv:2508.19768v1 Announce Type: new 
Abstract: Positive social interactions can occur in groups of many shapes and sizes, spanning from small and private to large and open. However, social media tends to binarize our experiences into either isolated small groups or into large public squares. In this paper, we introduce Burst, a social media design that allows users to share and curate content between many spaces of varied size and composition. Users initially post content to small trusted groups, who can then burst that content, routing it to the groups that would be the best audience. We instantiate this approach into a mobile phone application, and demonstrate through a ten-day field study (N=36) that Burst enabled a participatory curation culture. With this work, we aim to articulate potential new design directions for social media sharing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19768v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutong Zhang, Taeuk Kang, Sydney Yeh, Anavi Baddepudi, Lindsay Popowski, Tiziano Piccardi, Michael S. Bernstein</dc:creator>
    </item>
    <item>
      <title>Towards a Real-Time Warning System for Detecting Inaccuracies in Photoplethysmography-Based Heart Rate Measurements in Wearable Devices</title>
      <link>https://arxiv.org/abs/2508.19818</link>
      <description>arXiv:2508.19818v1 Announce Type: new 
Abstract: Wearable devices with photoplethysmography (PPG) sensors are widely used to monitor heart rate (HR), yet often suffer from accuracy issues. However, users typically do not receive an indication of potential measurement errors. We present a real-time warning system that detects and communicates inaccuracies in PPG-derived HR, aiming to enhance transparency and trust. Using data from Polar and Garmin devices, we trained a deep learning model to classify HR accuracy using only the derived HR signal. The system detected over 80% of inaccurate readings. By providing interpretable, real-time feedback directly to users, our work contributes to HCI by promoting user awareness, informed decision-making, and trust in wearable health technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19818v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rania Islmabouli, Marlene Brunner, Devender Kumar, Mahdi Sareban, Gunnar Treff, Michael Neudorfer, Josef Niebauer, Arne Bathke, Jan David Smeddinck</dc:creator>
    </item>
    <item>
      <title>Lessons from Biophilic Design: Rethinking Affective Interaction Design in Built Environments</title>
      <link>https://arxiv.org/abs/2508.19867</link>
      <description>arXiv:2508.19867v1 Announce Type: new 
Abstract: The perspectives of affective interaction in built environments are largely overlooked and instead dominated by affective computing approaches that view emotions as "static", computable states to be detected and regulated. To address this limitation, we interviewed architects to explore how biophilic design -- our deep-rooted emotional connection with nature -- could shape affective interaction design in smart buildings. Our findings reveal that natural environments facilitate self-directed emotional experiences through spatial diversity, embodied friction, and porous sensory exchanges. Based on this, we introduce three design principles for discussion at the Affective Interaction workshop: (1) Diversity of Spatial Experiences, (2) Self-Reflection Through Complexity &amp; Friction, and (3) Permeability &amp; Sensory Exchange with the Outside World, while also examining the challenges of integrating these perspectives into built environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19867v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shruti Rao, Judith Good, Hamed Alavi</dc:creator>
    </item>
    <item>
      <title>Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations</title>
      <link>https://arxiv.org/abs/2508.19942</link>
      <description>arXiv:2508.19942v1 Announce Type: new 
Abstract: This paper introduces a novel approach to tackle the challenges of preserving and transferring tacit knowledge--deep, experience-based insights that are hard to articulate but vital for decision-making, innovation, and problem-solving. Traditional methods rely heavily on human facilitators, which, while effective, are resource-intensive and lack scalability. A promising alternative is the use of Socially Interactive Agents (SIAs) as AI-driven knowledge transfer facilitators. These agents interact autonomously and socially intelligently with users through multimodal behaviors (verbal, paraverbal, nonverbal), simulating expert roles in various organizational contexts. SIAs engage employees in empathic, natural-language dialogues, helping them externalize insights that might otherwise remain unspoken. Their success hinges on building trust, as employees are often hesitant to share tacit knowledge without assurance of confidentiality and appreciation. Key technologies include Large Language Models (LLMs) for generating context-relevant dialogue, Retrieval-Augmented Generation (RAG) to integrate organizational knowledge, and Chain-of-Thought (CoT) prompting to guide structured reflection. These enable SIAs to actively elicit knowledge, uncover implicit assumptions, and connect insights to broader organizational contexts. Potential applications span onboarding, where SIAs support personalized guidance and introductions, and knowledge retention, where they conduct structured interviews with retiring experts to capture heuristics behind decisions. Success depends on addressing ethical and operational challenges such as data privacy, algorithmic bias, and resistance to AI. Transparency, robust validation, and a culture of trust are essential to mitigate these risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19942v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Martin Benderoth, Patrick Gebhard, Christian Keller, C. Benjamin Nakhosteen, Stefan Schaffer, Tanja Schneeberger</dc:creator>
    </item>
    <item>
      <title>CapTune: Adapting Non-Speech Captions With Anchored Generative Models</title>
      <link>https://arxiv.org/abs/2508.19971</link>
      <description>arXiv:2508.19971v1 Announce Type: new 
Abstract: Non-speech captions are essential to the video experience of deaf and hard of hearing (DHH) viewers, yet conventional approaches often overlook the diversity of their preferences. We present CapTune, a system that enables customization of non-speech captions based on DHH viewers' needs while preserving creator intent. CapTune allows caption authors to define safe transformation spaces using concrete examples and empowers viewers to personalize captions across four dimensions: level of detail, expressiveness, sound representation method, and genre alignment. Evaluations with seven caption creators and twelve DHH participants showed that CapTune supported creators' creative control while enhancing viewers' emotional engagement with content. Our findings also reveal trade-offs between information richness and cognitive load, tensions between interpretive and descriptive representations of sound, and the context-dependent nature of caption preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19971v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3663547.3746346</arxiv:DOI>
      <dc:creator>Jeremy Zhengqi Huang, Calu\~a de Lacerda Pataca, Liang-Yuan Wu, Dhruv Jain</dc:creator>
    </item>
    <item>
      <title>FlyMeThrough: Human-AI Collaborative 3D Indoor Mapping with Commodity Drones</title>
      <link>https://arxiv.org/abs/2508.20034</link>
      <description>arXiv:2508.20034v1 Announce Type: new 
Abstract: Indoor mapping data is crucial for routing, navigation, and building management, yet such data are widely lacking due to the manual labor and expense of data collection, especially for larger indoor spaces. Leveraging recent advancements in commodity drones and photogrammetry, we introduce FlyMeThrough -- a drone-based indoor scanning system that efficiently produces 3D reconstructions of indoor spaces with human-AI collaborative annotations for key indoor points-of-interest (POI) such as entrances, restrooms, stairs, and elevators. We evaluated FlyMeThrough in 12 indoor spaces with varying sizes and functionality. To investigate use cases and solicit feedback from target stakeholders, we also conducted a qualitative user study with five building managers and five occupants. Our findings indicate that FlyMeThrough can efficiently and precisely create indoor 3D maps for strategic space planning, resource management, and navigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20034v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xia Su, Ruiqi Chen, Jingwei Ma, Chu Li, Jon E. Froehlich</dc:creator>
    </item>
    <item>
      <title>Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration</title>
      <link>https://arxiv.org/abs/2508.19254</link>
      <description>arXiv:2508.19254v1 Announce Type: cross 
Abstract: This paper presents a real-time generative drawing system that interprets and integrates both formal intent - the structural, compositional, and stylistic attributes of a sketch - and contextual intent - the semantic and thematic meaning inferred from its visual content - into a unified transformation process. Unlike conventional text-prompt-based generative systems, which primarily capture high-level contextual descriptions, our approach simultaneously analyzes ground-level intuitive geometric features such as line trajectories, proportions, and spatial arrangement, and high-level semantic cues extracted via vision-language models. These dual intent signals are jointly conditioned in a multi-stage generation pipeline that combines contour-preserving structural control with style- and content-aware image synthesis. Implemented with a touchscreen-based interface and distributed inference architecture, the system achieves low-latency, two-stage transformation while supporting multi-user collaboration on shared canvases. The resulting platform enables participants, regardless of artistic expertise, to engage in synchronous, co-authored visual creation, redefining human-AI interaction as a process of co-creation and mutual enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19254v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jookyung Song, Mookyoung Kang, Nojun Kwak</dc:creator>
    </item>
    <item>
      <title>Inference of Human-derived Specifications of Object Placement via Demonstration</title>
      <link>https://arxiv.org/abs/2508.19367</link>
      <description>arXiv:2508.19367v1 Announce Type: cross 
Abstract: As robots' manipulation capabilities improve for pick-and-place tasks (e.g., object packing, sorting, and kitting), methods focused on understanding human-acceptable object configurations remain limited expressively with regard to capturing spatial relationships important to humans. To advance robotic understanding of human rules for object arrangement, we introduce positionally-augmented RCC (PARCC), a formal logic framework based on region connection calculus (RCC) for describing the relative position of objects in space. Additionally, we introduce an inference algorithm for learning PARCC specifications via demonstrations. Finally, we present the results from a human study, which demonstrate our framework's ability to capture a human's intended specification and the benefits of learning from demonstration approaches over human-provided specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19367v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Cuellar, Ho Chit Siu, Julie A Shah</dc:creator>
    </item>
    <item>
      <title>A perishable ability? The future of writing in the face of generative artificial intelligence</title>
      <link>https://arxiv.org/abs/2508.19427</link>
      <description>arXiv:2508.19427v1 Announce Type: cross 
Abstract: The 2020s have been witnessing a very significant advance in the development of generative artificial intelligence tools, including text generation systems based on large language models. These tools have been increasingly used to generate texts in the most diverse domains -- from technical texts to literary texts --, which might eventually lead to a lower volume of written text production by humans. This article discusses the possibility of a future in which human beings will have lost or significantly decreased their ability to write due to the outsourcing of this activity to machines. This possibility parallels the loss of the ability to write in other moments of human history, such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19427v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evandro L. T. P. Cunha</dc:creator>
    </item>
    <item>
      <title>MathBuddy: A Multimodal System for Affective Math Tutoring</title>
      <link>https://arxiv.org/abs/2508.19993</link>
      <description>arXiv:2508.19993v1 Announce Type: cross 
Abstract: The rapid adoption of LLM-based conversational systems is already transforming the landscape of educational technology. However, the current state-of-the-art learning models do not take into account the student's affective states. Multiple studies in educational psychology support the claim that positive or negative emotional states can impact a student's learning capabilities. To bridge this gap, we present MathBuddy, an emotionally aware LLM-powered Math Tutor, which dynamically models the student's emotions and maps them to relevant pedagogical strategies, making the tutor-student conversation a more empathetic one. The student's emotions are captured from the conversational text as well as from their facial expressions. The student's emotions are aggregated from both modalities to confidently prompt our LLM Tutor for an emotionally-aware response. We have effectively evaluated our model using automatic evaluation metrics across eight pedagogical dimensions and user studies. We report a massive 23 point performance gain using the win rate and a 3 point gain at an overall level using DAMR scores which strongly supports our hypothesis of improving LLM-based tutor's pedagogical abilities by modeling students' emotions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19993v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debanjana Kar, Leopold B\"oss, Dacia Braca, Sebastian Maximilian Dennerlein, Nina Christine Hubig, Philipp Wintersberger, Yufang Hou</dc:creator>
    </item>
    <item>
      <title>Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust</title>
      <link>https://arxiv.org/abs/2506.14799</link>
      <description>arXiv:2506.14799v2 Announce Type: replace 
Abstract: Recent advances in AI has made automated analysis of complex media content at scale possible while generating actionable insights regarding character representation along such dimensions as gender and age. Past works focused on quantifying representation from audio/video/text using AI models, but without having the audience in the loop. We ask, even if character distribution along demographic dimensions are available, how useful are those to the general public? Do they actually trust the numbers generated by AI models? Our work addresses these open questions by proposing a new AI-based character representation tool and performing a thorough user study. Our tool has two components: (i) An analytics extraction model based on the Contrastive Language Image Pretraining (CLIP) foundation model that analyzes visual screen data to quantify character representation across age and gender; (ii) A visualization component effectively designed for presenting the analytics to lay audience. The user study seeks empirical evidence on the usefulness and trustworthiness of the AI-generated results for carefully chosen movies presented in the form of our visualizations. We found that participants were able to understand the analytics in our visualizations, and deemed the tool `overall useful'. Participants also indicated a need for more detailed visualizations to include more demographic categories and contextual information of the characters. Participants' trust in AI-based gender and age models is seen to be moderate to low, although they were not against the use of AI in this context. Our tool including code, benchmarking, and the user study data can be found at https://github.com/debadyuti0510/Character-Representation-Media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14799v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evdoxia Taka, Debadyuti Bhattacharya, Joanne Garde-Hansen, Sanjay Sharma, Tanaya Guha</dc:creator>
    </item>
    <item>
      <title>Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks</title>
      <link>https://arxiv.org/abs/2506.23016</link>
      <description>arXiv:2506.23016v2 Announce Type: replace 
Abstract: The global prevalence of dementia is projected to double by 2050, highlighting the urgent need for scalable diagnostic tools. This study utilizes digital cognitive tasks with eye-tracking data correlated with memory processes to distinguish between Healthy Controls (HC) and Mild Cognitive Impairment (MCI), a precursor to dementia. A deep learning model based on VTNet was trained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who performed a visual memory task. The model utilizes both time series and spatial data derived from eye-tracking. It was modified to incorporate scan paths, heat maps, and image content. These modifications also enabled testing parameters such as image resolution and task performance, analyzing their impact on model performance. The best model, utilizing $700\times700px$ resolution heatmaps, achieved 68% sensitivity and 76% specificity. Despite operating under more challenging conditions (e.g., smaller dataset size, shorter task duration, or a less standardized task), the model's performance is comparable to an Alzheimer's study using similar methods (70% sensitivity and 73% specificity). These findings contribute to the development of automated diagnostic tools for MCI. Future work should focus on refining the model and using a standardized long-term visual memory task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23016v2</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom\'as Silva Santos Rocha, Anastasiia Mikhailova, Moreno I. Coco, Jos\'e Santos-Victor</dc:creator>
    </item>
    <item>
      <title>Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure</title>
      <link>https://arxiv.org/abs/2507.22893</link>
      <description>arXiv:2507.22893v2 Announce Type: replace 
Abstract: Contemporary human-AI interaction research overlooks how AI systems fundamentally reshape human cognition pre-consciously, a critical blind spot for understanding distributed cognition. This paper introduces "Cognitive Infrastructure Studies" (CIS) as a new interdisciplinary domain to reconceptualize AI as "cognitive infrastructures": foundational, often invisible systems conditioning what is knowable and actionable in digital societies. These semantic infrastructures transport meaning, operate through anticipatory personalization, and exhibit adaptive invisibility, making their influence difficult to detect. Critically, they automate "relevance judgment," shifting the "locus of epistemic agency" to non-human systems. Through narrative scenarios spanning individual (cognitive dependency), collective (democratic deliberation), and societal (governance) scales, we describe how cognitive infrastructures reshape human cognition, public reasoning, and social epistemologies. CIS aims to address how AI preprocessing reshapes distributed cognition across individual, collective, and cultural scales, requiring unprecedented integration of diverse disciplinary methods. The framework also addresses critical gaps across disciplines: cognitive science lacks population-scale preprocessing analysis capabilities, digital sociology cannot access individual cognitive mechanisms, and computational approaches miss cultural transmission dynamics. To achieve this goal CIS also provides methodological innovations for studying invisible algorithmic influence: "infrastructure breakdown methodologies", experimental approaches that reveal cognitive dependencies by systematically withdrawing AI preprocessing after periods of habituation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22893v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Giuseppe Riva</dc:creator>
    </item>
    <item>
      <title>Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective</title>
      <link>https://arxiv.org/abs/2508.03969</link>
      <description>arXiv:2508.03969v3 Announce Type: replace 
Abstract: This chapter systematically promotes an emerging interdisciplinary field of human-artificial intelligence interaction (human-AI interaction, HAII) from a human-centered AI (HCAI) perspective. It introduces a framework of human-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII research and applications, emphasizing the importance of adopting a human-centered approach over a technology-centered one. The chapter presents the HC-HAII methodology, including human-centered methods, process, interdisciplinary teams, and multi-level design paradigms. It also highlights key research challenges and future directions. As the first chapter, this chapter also provides a structural overview of this book, which brings together contributions from an interdisciplinary community of researchers and practitioners to advance the theory, methodology, and applications of HCAI in diverse domains of HAII. The purpose of this chapter is to provide a fundamental framework for this book, centered on HAII research and applications based on the HCAI approach, which will pave the way for the content of subsequent chapters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03969v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Xu</dc:creator>
    </item>
    <item>
      <title>StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI</title>
      <link>https://arxiv.org/abs/2508.08524</link>
      <description>arXiv:2508.08524v2 Announce Type: replace 
Abstract: Interactive streetscape mapping tools such as Google Street View (GSV) and Meta Mapillary enable users to virtually navigate and experience real-world environments via immersive 360{\deg} imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal AI, accessible navigation controls, and conversational speech. With StreetViewAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetViewAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08524v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3746059.3747756</arxiv:DOI>
      <dc:creator>Jon E. Froehlich, Alexander Fiannaca, Nimer Jaber, Victor Tsara, Shaun Kane</dc:creator>
    </item>
    <item>
      <title>Families' Vision of Generative AI Agents for Household Safety Against Digital and Physical Threats</title>
      <link>https://arxiv.org/abs/2508.11030</link>
      <description>arXiv:2508.11030v2 Announce Type: replace 
Abstract: As families face increasingly complex safety challenges in digital and physical environments, generative AI (GenAI) presents new opportunities to support household safety through multiple specialized AI agents. Through a two-phase qualitative study consisting of individual interviews and collaborative sessions with 13 parent-child dyads, we explored families' conceptualizations of GenAI and their envisioned use of AI agents in daily family life. Our findings reveal that families preferred to distribute safety-related support across multiple AI agents, each embodying a familiar caregiving role: a household manager coordinating routine tasks and mitigating risks such as digital fraud and home accidents; a private tutor providing personalized educational support, including safety education; and a family therapist offering emotional support to address sensitive safety issues such as cyberbullying and digital harassment. Families emphasized the need for agent-specific privacy boundaries, recognized generational differences in trust toward AI agents, and stressed the importance of maintaining open family communication alongside the assistance of AI agents. Based on these findings, we propose a multi-agent system design featuring four privacy-preserving principles: memory segregation, conversational consent, selective data sharing, and progressive memory management to help balance safety, privacy, and autonomy within family contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11030v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zikai Wen, Lanjing Liu, Yaxing Yao</dc:creator>
    </item>
    <item>
      <title>Using Generative AI to Uncover What Drives Player Enjoyment in PC and VR Games</title>
      <link>https://arxiv.org/abs/2508.16596</link>
      <description>arXiv:2508.16596v2 Announce Type: replace 
Abstract: As video games continue to evolve, understanding what drives player enjoyment remains a key challenge. Player reviews provide valuable insights, but their unstructured nature makes large-scale analysis difficult. This study applies generative AI and machine learning, leveraging Microsoft Phi-4 LLM and XGBoost, to quantify and analyze game reviews from Steam and Meta Quest stores. The approach converts qualitative feedback into structured data, enabling comprehensive evaluation of key game design elements, monetization models, and platform-specific trends. The findings reveal distinct patterns in player preferences across PC and VR games, highlighting factors that contribute to higher player satisfaction. By integrating Google Cloud for largescale data storage and processing, this study establishes a scalable framework for game review analysis. The study's insights offer actionable guidance for game developers, helping optimize game mechanics, pricing strategies, and player engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16596v2</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hisham Abdelqader</dc:creator>
    </item>
    <item>
      <title>From Evidence to Decision: Exploring Evaluative AI</title>
      <link>https://arxiv.org/abs/2402.01292</link>
      <description>arXiv:2402.01292v4 Announce Type: replace-cross 
Abstract: This paper presents a hypothesis-driven approach to improve AI-supported decision-making that is based on the Evaluative AI paradigm - a conceptual framework that proposes providing users with evidence for or against a given hypothesis. We propose an implementation of Evaluative AI by extending the Weight of Evidence framework, leading to hypothesis-driven models that support both tabular and image data. We demonstrate the application of the new decision-support approach in two domains: housing price prediction and skin cancer diagnosis. The findings show promising results in improving human decisions, as well as providing insights on the strengths and weaknesses of different decision-support approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01292v4</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thao Le, Tim Miller, Liz Sonenberg, Ronal Singh, H. Peter Soyer</dc:creator>
    </item>
    <item>
      <title>When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025</title>
      <link>https://arxiv.org/abs/2508.03037</link>
      <description>arXiv:2508.03037v2 Announce Type: replace-cross 
Abstract: As generative AI continues to reshape artistic production and alternate modes of human expression, artists whose livelihoods are most directly affected have raised urgent concerns about consent, transparency, and the future of creative labor. However, the voices of artists are often marginalized in dominant public and scholarly discourse. This study presents a twelve-year analysis, from 2013 to 2025, of English-language discourse surrounding AI-generated art. It draws from 439 curated 500-word excerpts sampled from opinion articles, news reports, blogs, legal filings, and spoken-word transcripts. Through a reproducible methodology, we identify five stable thematic clusters and uncover a misalignment between artists' perceptions and prevailing media narratives. Our findings highlight how the use of technical jargon can function as a subtle form of gatekeeping, often sidelining the very issues artists deem most urgent. Our work provides a BERTopic-based methodology and a multimodal baseline for future research, alongside a clear call for deeper, transparency-driven engagement with artist perspectives in the evolving AI-creative landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03037v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ariya Mukherjee-Gandhi, Oliver Muellerklein</dc:creator>
    </item>
  </channel>
</rss>
