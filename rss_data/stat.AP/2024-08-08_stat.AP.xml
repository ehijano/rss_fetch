<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Aug 2024 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>How reliable are remote sensing maps calibrated over large areas? A matter of scale?</title>
      <link>https://arxiv.org/abs/2408.03953</link>
      <description>arXiv:2408.03953v1 Announce Type: new 
Abstract: Remote sensing data are increasingly available and frequently used to produce forest attributes maps. The sampling strategy of the calibration plots may directly affect predictions and map qualities. The aim of this manuscript is to evaluate models transferability at different spatial scales according to the sampling efforts and the calibration domain of these models. Forest inventory plots from locals and regionals networks were used to calibrate randomForest (RF) models for stand basal area predictions. Auxiliary data from ALS flights and a Sentinel-2 image were used. Model transferability was assessed by comparing models developed over a given area and applied elsewhere. Performances were measured in terms of precision (RMSE and bias), coefficient of determination (R2) and the proportion of extrapolated predictions. Regional networks were also thinned to evaluate the effect of sampling efforts on models' performances. Local models showed large bias and extrapolation issues when applied elsewhere. Local issues of regional models were also observed, raising transferability and extrapolation concerns. An increase in sampling efforts was shown to reduce extrapolation issues. The outcoming results of this study underline the importance of considering models' validity domain while producing forest attribute maps, since their transferability is of crucial importance from a forest management perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03953v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrey Ramirez Luigui (UL, ONF), Jean-Pierre Renaud (ONF), C\'edric Vega (IGN)</dc:creator>
    </item>
    <item>
      <title>A Framework for Assessing Cumulative Exposure to Extreme Temperatures During Transit Trip</title>
      <link>https://arxiv.org/abs/2408.04081</link>
      <description>arXiv:2408.04081v1 Announce Type: new 
Abstract: The combined influence of urban heat islands, climate change, and extreme temperature events are increasingly impacting transit travelers, especially vulnerable populations such as older adults, people with disabilities, and those with chronic diseases. Previous studies have generally attempted to address this issue at either the micro- or macro-level, but each approach presents different limitations in modeling the impacts on transit trips. Other research proposes a meso-level approach to address some of these gaps, but the use of additive exposure calculation and spatial shortest path routing poses constraints meso-modeling accuracy. This study introduces HeatPath Analyzer, a framework to assess the exposure of transit riders to extreme temperatures, using TransitSim 4.0 to generate second-by-second spatio-temporal trip trajectories, the traveler activity profiles, and thermal comfort levels along the entire journey. The approach uses heat stress combines the standards proposed by the NWS and CDC to estimate cumulative exposure for transit riders, with specific parameters tailored to the elderly and people with disabilities. The framework assesses the influence of extreme heat and winter chill. A case study in Atlanta, GA, reveals that 10.2% of trips on an average summer weekday in 2019 were at risk of extreme heat. The results uncover exposure disparities across different transit trip mode segments, and across mitigation-based and adaptation-based strategies. While the mitigation-based strategy highlights high-exposure segments such as long ingress and egress, adaptation should be prioritized toward the middle or second half of the trip when a traveler is waiting for transit or transferring between routes. A comparison between the traditional additive approach and the dynamic approach presented also shows significant disparities, which, if overlooked, can mislead policy decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04081v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Huiying Fan, Hongyu Lu, Geyu Lyu, Angshuman Guin, Randall Guensler</dc:creator>
    </item>
    <item>
      <title>Real-time modelling of the SARS-CoV-2 pandemic in England 2020-2023: a challenging data integration</title>
      <link>https://arxiv.org/abs/2408.04178</link>
      <description>arXiv:2408.04178v1 Announce Type: new 
Abstract: A central pillar of the UK's response to the SARS-CoV-2 pandemic was the provision of up-to-the moment nowcasts and short term projections to monitor current trends in transmission and associated healthcare burden. Here we present a detailed deconstruction of one of the 'real-time' models that was key contributor to this response, focussing on the model adaptations required over three pandemic years characterised by the imposition of lockdowns, mass vaccination campaigns and the emergence of new pandemic strains. The Bayesian model integrates an array of surveillance and other data sources including a novel approach to incorporating prevalence estimates from an unprecedented large-scale household survey. We present a full range of estimates of the epidemic history and the changing severity of the infection, quantify the impact of the vaccination programme and deconstruct contributing factors to the reproduction number. We further investigate the sensitivity of model-derived insights to the availability and timeliness of prevalence data, identifying its importance to the production of robust estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04178v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul J Birrell, Joshua Blake, Joel Kandiah, Angelos Alexopoulos, Edwin van Leeuwen, Koen Pouwels, Sanmitra Ghosh, Colin Starr, Ann Sarah Walker, Thomas A House, Nigel Gay, Thomas Finnie, Nick Gent, Andr\'e Charlett, Daniela De Angelis</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Social Platforms Under Approximate Interference Networks</title>
      <link>https://arxiv.org/abs/2408.04441</link>
      <description>arXiv:2408.04441v1 Announce Type: new 
Abstract: Estimating the total treatment effect (TTE) of a new feature in social platforms is crucial for understanding its impact on user behavior. However, the presence of network interference, which arises from user interactions, often complicates this estimation process. Experimenters typically face challenges in fully capturing the intricate structure of this interference, leading to less reliable estimates. To address this issue, we propose a novel approach that leverages surrogate networks and the pseudo inverse estimator. Our contributions can be summarized as follows: (1) We introduce the surrogate network framework, which simulates the practical situation where experimenters build an approximation of the true interference network using observable data. (2) We investigate the performance of the pseudo inverse estimator within this framework, revealing a bias-variance trade-off introduced by the surrogate network. We demonstrate a tighter asymptotic variance bound compared to previous studies and propose an enhanced variance estimator outperforming the original estimator. (3) We apply the pseudo inverse estimator to a real experiment involving over 50 million users, demonstrating its effectiveness in detecting network interference when combined with the difference-in-means estimator. Our research aims to bridge the gap between theoretical literature and practical implementation, providing a solution for estimating TTE in the presence of network interference and unknown interference structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04441v1</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiming Jiang, Lu Deng, Yong Wang, He Wang</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences for Health Policy and Practice: A Review of Modern Methods</title>
      <link>https://arxiv.org/abs/2408.04617</link>
      <description>arXiv:2408.04617v1 Announce Type: new 
Abstract: Difference-in-differences (DiD) is the most popular observational causal inference method in health policy, employed to evaluate the real-world impact of policies and programs. To estimate treatment effects, DiD relies on the "parallel trends assumption", that on average treatment and comparison groups would have had parallel trajectories in the absence of an intervention. Historically, DiD has been considered broadly applicable and straightforward to implement, but recent years have seen rapid advancements in DiD methods. This paper reviews and synthesizes these innovations for medical and health policy researchers. We focus on four topics: (1) assessing the parallel trends assumption in health policy contexts; (2) relaxing the parallel trends assumption when appropriate; (3) employing estimators to account for staggered treatment timing; and (4) conducting robust inference for analyses in which normal-based clustered standard errors are inappropriate. For each, we explain challenges and common pitfalls in traditional DiD and modern methods available to address these issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04617v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuo Feng, Ishani Ganguli, Youjin Lee, John Poe, Andrew Ryan, Alyssa Bilinski</dc:creator>
    </item>
    <item>
      <title>BayesFBHborrow: An R Package for Bayesian borrowing for time-to-event data from a flexible baseline hazard</title>
      <link>https://arxiv.org/abs/2408.04327</link>
      <description>arXiv:2408.04327v1 Announce Type: cross 
Abstract: There is currently a focus on statistical methods which can use external trial information to help accelerate the discovery, development and delivery of medicine. Bayesian methods facilitate borrowing which is "dynamic" in the sense that the similarity of the data helps to determine how much information is used. We propose a Bayesian semiparameteric model, which allows the baseline hazard to take any form through an ensemble average. We introduce priors to smooth the posterior baseline hazard improving both model estimation and borrowing characteristics. A "lump-and-smear" borrowing prior accounts for non-exchangable historical data and helps reduce the maximum type I error in the presence of prior-data conflict. In this article, we present BayesFBHborrow, an R package, which enables the user to perform Bayesian borrowing with a historical control dataset in a semiparameteric time-to-event model. User-defined hyperparameters smooth an ensemble averaged posterior baseline hazard. The model offers the specification of lump-and-smear priors on the commensurability parameter where the associated hyperparameters can be chosen according to the users tolerance for difference between the log baseline hazards. We demonstrate the performance of our Bayesian flexible baseline hazard model on a simulated and real world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04327v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sophia Axillus, Alex Lewin, Darren Scott</dc:creator>
    </item>
    <item>
      <title>More on verification of probability forecasts for football outcomes: score decompositions, reliability, and discrimination analyses</title>
      <link>https://arxiv.org/abs/2106.14345</link>
      <description>arXiv:2106.14345v2 Announce Type: replace 
Abstract: Forecast of football outcomes in terms of Home Win, Draw and Away Win relies largely on ex ante probability elicitation of these events and ex post verification of them via computation of probability scoring rules (Brier, Ranked Probability, Logarithmic, Zero-One scores). Usually, appraisal of the quality of forecasting procedures is restricted to reporting mean score values. The purpose of this article is to propose additional tools of verification, such as score decompositions into several components of special interest. Graphical and numerical diagnoses of reliability and discrimination and kindred statistical methods are presented using different techniques of binning (fixed thresholds, quantiles, logistic and iso regression). These procedures are illustrated on probability forecasts for the outcomes of the UEFA Champions League (C1) at the end of the group stage based on typical Poisson regression models with reasonably good results in terms of reliability as compared to those obtained from bookmaker odds and whatever the technique used. Links with research in machine learning and different areas of application (meteorology, medicine) are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.14345v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Louis Foulley</dc:creator>
    </item>
    <item>
      <title>A Bayesian Inference Approach for Reducing Inter-Investigator Variability in Sampling-Based Land Cover Classification</title>
      <link>https://arxiv.org/abs/2403.15720</link>
      <description>arXiv:2403.15720v2 Announce Type: replace 
Abstract: Land cover classification plays a critical role in describing Earth's surface characteristics. However, these classifications can be affected by uncertainties introduced by variability in investigator interpretations. While land cover classification mapping is becoming easier due to the emergence of cloud geospatial platforms, such uncertainty is often overlooked. This study aimed to create a robust land cover classification map by reducing inter-investigator variability from independent investigators' maps using a Bayesian inference framework. In Saitama City, Japan, as a case study, 44 investigators used a point-based visual interpretation method via Google Earth Engine to collect stratified reference samples across four different land cover classes: Forest, Agriculture, Urban, and Water. These samples were then used to train a random forest classifier, resulting in the creation of individual classification maps. We quantified pixel-level discrepancies in these maps arising from inherent inter-investigator variability. To address them, we developed a Bayesian inference framework to produce a unified classification map. This framework updates the classification probability based on a Dirichlet distribution and yielded an overall accuracy of 0.851 for independent validation samples, an improvement over the average accuracy of 0.728 for the individual maps. We further improved the results by introducing K-Medoids to group the most reliable maps as the input for Bayesian inference, achieving an overall accuracy of 0.858, the highest among all approaches tested. Our approach also effectively reduced salt-and-pepper noise, which is often found in individual classification maps. This research underscores the intrinsic uncertainties present in land cover classification maps attributable to investigator variations and introduces a potential solution to mitigate these variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15720v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Narumasa Tsutsumida, Akira Kato</dc:creator>
    </item>
    <item>
      <title>Varying coefficients correlated velocity models in complex landscapes with boundaries applied to narwhal responses to noise exposure</title>
      <link>https://arxiv.org/abs/2408.03741</link>
      <description>arXiv:2408.03741v2 Announce Type: replace 
Abstract: Narwhals in the Arctic are increasingly exposed to human activities that can temporarily or permanently threaten their survival by modifying their behavior. We examine GPS data from a population of narwhals exposed to ship and seismic airgun noise during a controlled experiment in 2018 in the Scoresby Sound fjord system in Southeast Greenland. The fjord system has a complex shore line, restricting the behavioral response options for the narwhals to escape the threats. We propose a new continuous-time correlated velocity model with varying coefficients that includes spatial constraints on movement. To assess the sound exposure effect we compare a baseline model for the movement before exposure to a response model for the movement during exposure. Our model, applied to the narwhal data, suggests increased tortuosity of the trajectories as a consequence of the spatial constraints, and further indicates that sound exposure can disturb narwhal motion up to a couple of tens of kilometers. Specifically, we found an increase in velocity and a decrease in the movement persistence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03741v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Delporte, Susanne Ditlevsen, Adeline Samson</dc:creator>
    </item>
    <item>
      <title>Statistical visualisation for tidy and geospatial data in R via kernel smoothing methods in the eks package</title>
      <link>https://arxiv.org/abs/2203.01686</link>
      <description>arXiv:2203.01686v4 Announce Type: replace-cross 
Abstract: Kernel smoothers are essential tools for data analysis due to their ability to convey complex statistical information with concise graphical visualisations. Their inclusion in the base distribution and in the many user-contributed add-on packages of the R statistical analysis environment caters well to many practitioners. Though there remain some important gaps for specialised data, most notably for tidy and geospatial data. The proposed eks package fills in these gaps. In addition to kernel density estimation, this package also caters for more complex data analysis situations, such as density derivative estimation, density-based classification (supervised learning) and mean shift clustering (unsupervised learning). We illustrate with experimental data how to obtain and to interpret the statistical visualisations for these kernel smoothing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.01686v4</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarn Duong</dc:creator>
    </item>
    <item>
      <title>Statistical Inference and A/B Testing in Fisher Markets and Paced Auctions</title>
      <link>https://arxiv.org/abs/2406.15522</link>
      <description>arXiv:2406.15522v2 Announce Type: replace-cross 
Abstract: We initiate the study of statistical inference and A/B testing for two market equilibrium models: linear Fisher market (LFM) equilibrium and first-price pacing equilibrium (FPPE). LFM arises from fair resource allocation systems such as allocation of food to food banks and notification opportunities to different types of notifications. For LFM, we assume that the data observed is captured by the classical finite-dimensional Fisher market equilibrium, and its steady-state behavior is modeled by a continuous limit Fisher market. The second type of equilibrium we study, FPPE, arises from internet advertising where advertisers are constrained by budgets and advertising opportunities are sold via first-price auctions. For platforms that use pacing-based methods to smooth out the spending of advertisers, FPPE provides a hindsight-optimal configuration of the pacing method. We propose a statistical framework for the FPPE model, in which a continuous limit FPPE models the steady-state behavior of the auction platform, and a finite FPPE provides the data to estimate primitives of the limit FPPE. Both LFM and FPPE have an Eisenberg-Gale convex program characterization, the pillar upon which we derive our statistical theory. We start by deriving basic convergence results for the finite market to the limit market. We then derive asymptotic distributions, and construct confidence intervals. Furthermore, we establish the asymptotic local minimax optimality of estimation based on finite markets. We then show that the theory can be used for conducting statistically valid A/B testing on auction platforms. Synthetic and semi-synthetic experiments verify the validity and practicality of our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15522v2</guid>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer</dc:creator>
    </item>
  </channel>
</rss>
