<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Dec 2025 05:01:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sensitivity Analysis of the Consistency Assumption</title>
      <link>https://arxiv.org/abs/2512.21379</link>
      <description>arXiv:2512.21379v1 Announce Type: new 
Abstract: Sensitivity analysis informs causal inference by assessing the sensitivity of conclusions to departures from assumptions. The consistency assumption states that there are no hidden versions of treatment and that the outcome arising naturally equals the outcome arising from intervention. When reasoning about the possibility of consistency violations, it can be helpful to distinguish between covariates and versions of treatment. In the context of surgery, for example, genomic variables are covariates and the skill of a particular surgeon is a version of treatment. There may be hidden versions of treatment, and this paper addresses that concern with a new kind of sensitivity analysis. Whereas many methods for sensitivity analysis are focused on confounding by unmeasured covariates, the methodology of this paper is focused on confounding by hidden versions of treatment. In this paper, new mathematical notation is introduced to support the novel method, and example applications are described.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21379v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Knaeble, Qinyun Lin, Erich Kummerfeld, Kenneth A. Frank</dc:creator>
    </item>
    <item>
      <title>Standardized Descriptive Index for Measuring Deviation and Uncertainty in Psychometric Indicators</title>
      <link>https://arxiv.org/abs/2512.21399</link>
      <description>arXiv:2512.21399v1 Announce Type: new 
Abstract: The use of descriptive statistics in pilot testing procedures requires objective, standard diagnostic tools that are feasible for small sample sizes. While current psychometric practices report item-level statistics, they often report these raw descriptives separately rather than consolidating both mean and standard deviation into a single diagnostic tool to directly measure item quality. By leveraging the analytical properties of Cohen's d, this article repurposes its use in scale development as a standardized item deviation index. This measures the extent of an item's raw deviation relative to its scale midpoint while accounting for its own uncertainty. Analytical properties such as boundedness, scale invariance, and bias are explored to further understand how the index values behave, which will aid future efforts to establish empirical thresholds that characterize redundancy among formative indicators and consistency among reflective indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21399v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Dominique Dalipe Mu\~noz</dc:creator>
    </item>
    <item>
      <title>Adaptive Test for High Dimensional Quantile Regression</title>
      <link>https://arxiv.org/abs/2512.21541</link>
      <description>arXiv:2512.21541v1 Announce Type: new 
Abstract: Testing high-dimensional quantile regression coefficients is crucial, as tail quantiles often reveal more than the mean in many practical applications. Nevertheless, the sparsity pattern of the alternative hypothesis is typically unknown in practice, posing a major challenge. To address this, we propose an adaptive test that remains powerful across both sparse and dense alternatives.We first establish the asymptotic independence between the max-type test statistic proposed by \citet{tang2022conditional} and the sum-type test statistic introduced by \citet{chen2024hypothesis}. Building on this result, we propose a Cauchy combination test that effectively integrates the strengths of both statistics and achieves robust performance across a wide range of sparsity levels. Simulation studies and real data applications demonstrate that our proposed procedure outperforms existing methods in terms of both size control and power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21541v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ping Zhao, Zhenyu Liu, Dan Zhuang</dc:creator>
    </item>
    <item>
      <title>Cross-Semantic Transfer Learning for High-Dimensional Linear Regression</title>
      <link>https://arxiv.org/abs/2512.21689</link>
      <description>arXiv:2512.21689v1 Announce Type: new 
Abstract: Current transfer learning methods for high-dimensional linear regression assume feature alignment across domains, restricting their applicability to semantically matched features. In many real-world scenarios, however, distinct features in the target and source domains can play similar predictive roles, creating a form of cross-semantic similarity. To leverage this broader transferability, we propose the Cross-Semantic Transfer Learning (CSTL) framework. It captures potential relationships by comparing each target coefficient with all source coefficients through a weighted fusion penalty. The weights are derived from the derivative of the SCAD penalty, effectively approximating an ideal weighting scheme that preserves transferable signals while filtering out source-specific noise. For computational efficiency, we implement CSTL using the Alternating Direction Method of Multipliers (ADMM). Theoretically, we establish that under mild conditions, CSTL achieves the oracle estimator with overwhelming probability. Empirical results from simulations and a real-data application confirm that CSTL outperforms existing methods in both cross-semantic and partial signal similarity settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21689v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiancheng Jiang, Xuejun Jiang, Hongxia Jin</dc:creator>
    </item>
    <item>
      <title>Surrogate-Powered Inference: Regularization and Adaptivity</title>
      <link>https://arxiv.org/abs/2512.21826</link>
      <description>arXiv:2512.21826v1 Announce Type: new 
Abstract: High-quality labeled data are essential for reliable statistical inference, but are often limited by validation costs. While surrogate labels provide cost-effective alternatives, their noise can introduce non-negligible bias. To address this challenge, we propose the surrogate-powered inference (SPI) toolbox, a unified framework that leverages both the validity of high-quality labels and the abundance of surrogates to enable reliable statistical inference. SPI comprises three progressively enhanced versions. Base-SPI integrates validated labels and surrogates through augmentation to improve estimation efficiency. SPI+ incorporates regularized regression to safely handle multiple surrogates, preventing performance degradation due to error accumulation. SPI++ further optimizes efficiency under limited validation budgets through an adaptive, multiwave labeling procedure that prioritizes informative subjects for labeling. Compared to traditional methods, SPI substantially reduces the estimation error and increases the power in risk factor identification. These results demonstrate the value of SPI in improving the reproducibility. Theoretical guarantees and extensive simulation studies further illustrate the properties of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21826v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianmin Chen, Huiyuan Wang, Thomas Lumley, Xiaowu Dai, Yong Chen</dc:creator>
    </item>
    <item>
      <title>Targeted learning via probabilistic subpopulation matching</title>
      <link>https://arxiv.org/abs/2512.21840</link>
      <description>arXiv:2512.21840v1 Announce Type: new 
Abstract: In biomedical research, to obtain more accurate prediction results from a target study, leveraging information from multiple similar source studies is proved to be useful. However, in many biomedical applications based on real-world data, populations under consideration in different studies, e.g., clinical sites, can be heterogeneous, leading to challenges in properly borrowing information towards the target study. The state of art methods are typically based on study-level matching to identify source studies that are similar to the target study, whilst samples from source studies that significantly differ from the target study will all be dropped at the study level, which can lead to substantial loss of information. We consider a general situation where all studies are sampled from a super-population composed of distinct subpopulations, and propose a novel framework of targeted learning via subpopulation matching. In contrast to the existing study-level matching methods, measuring similarities between subpopulations can effectively decompose both within- and between-study heterogeneity, allowing incorporation of information from all source studies without dropping any samples as in the existing methods. We devise the proposed framework as a two-step procedure, where a finite mixture model is first fitted jointly across all studies to provide subject-wise probabilistic subpopulation information, followed by a step of within-subpopulation information transferring from source studies to the target study for each identified subpopulation. We establish the non-asymptotic properties of our estimator and demonstrate the ability of our method to improve prediction at the target study via simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21840v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaokang Liu, Jie Hu, Naimin Jing, Yang Ning, Cheng Yong Tang, Runze Li, Yong Chen</dc:creator>
    </item>
    <item>
      <title>A Communication-Efficient Distributed Algorithm for Learning with Heterogeneous and Structurally Incomplete Multi-Site Data</title>
      <link>https://arxiv.org/abs/2512.21879</link>
      <description>arXiv:2512.21879v1 Announce Type: new 
Abstract: In multicenter biomedical research, integrating data from multiple decentralized sites provides more robust and generalizable findings due to its larger sample size and the ability to account for the between-site heterogeneity. However, sharing individual-level data across sites is often difficult due to patient privacy concerns and regulatory restrictions. To overcome this challenge, many distributed algorithms, that fit a global model by only communicating aggregated information across sites, have been proposed. A major challenge in applying existing distributed algorithms to real-world data is that their validity often relies on the assumption that data across sites are independently and identically distributed, which is frequently violated in practice. In biomedical applications, data distributions across clinical sites can be heterogeneous. Additionally, the set of covariates available at each site may vary due to different data collection protocols. We propose a distributed inference framework for data integration in the presence of both distribution heterogeneity and data structural heterogeneity. By modeling heterogeneous and structurally missing data using density-tilted generalized method of moments, we developed a general aggregated data-based distributed algorithm that is communication-efficient and heterogeneity-aware. We establish the asymptotic properties of our estimator and demonstrate the validity of our method via simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21879v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaokang Liu, Yuchen Yang, Yifei Sun, Jiang Bian, Yanyuan Ma, Raymond J. Carroll, Yong Chen</dc:creator>
    </item>
    <item>
      <title>Modeling high dimensional point clouds with the spherical cluster model</title>
      <link>https://arxiv.org/abs/2512.21960</link>
      <description>arXiv:2512.21960v1 Announce Type: new 
Abstract: A parametric cluster model is a statistical model providing geometric insights onto the points defining a cluster. The {\em spherical cluster model} (SC) approximates a finite point set $P\subset \mathbb{R}^d$ by a sphere $S(c,r)$ as follows. Taking $r$ as a fraction $\eta\in(0,1)$ (hyper-parameter) of the std deviation of distances between the center $c$ and the data points, the cost of the SC model is the sum over all data points lying outside the sphere $S$ of their power distance with respect to $S$. The center $c$ of the SC model is the point minimizing this cost. Note that $\eta=0$ yields the celebrated center of mass used in KMeans clustering. We make three contributions.
  First, we show fitting a spherical cluster yields a strictly convex but not smooth combinatorial optimization problem. Second, we present an exact solver using the Clarke gradient on a suitable stratified cell complex defined from an arrangement of hyper-spheres. Finally, we present experiments on a variety of datasets ranging in dimension from $d=9$ to $d=10,000$, with two main observations. First, the exact algorithm is orders of magnitude faster than BFGS based heuristics for datasets of small/intermediate dimension and small values of $\eta$, and for high dimensional datasets (say $d&gt;100$) whatever the value of $\eta$. Second, the center of the SC model behave as a parameterized high-dimensional median.
  The SC model is of direct interest for high dimensional multivariate data analysis, and the application to the design of mixtures of SC will be reported in a companion paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21960v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Cazals, Antoine Commaret, Louis Goldenberg</dc:creator>
    </item>
    <item>
      <title>Prediction intervals for quantile autoregression</title>
      <link>https://arxiv.org/abs/2512.22018</link>
      <description>arXiv:2512.22018v1 Announce Type: new 
Abstract: This paper introduces new methods for constructing prediction intervals using quantile-based techniques. The procedures are developed for both classical (homoscedastic) autoregressive models and modern quantile autoregressive models. They combine quantile estimation with multiplier bootstrap schemes to approximate the sampling variability of coefficient estimates, together with bootstrap replications of future observations. We consider both percentile-based and predictive-root-based constructions. Theoretical results establish the validity and pertinence of the proposed methods. Simulation experiments evaluate their finite-sample performance and show that the proposed methods yield improved coverage properties and computational efficiency relative to existing approaches in the literature. The empirical usefulness of the methods is illustrated through applications to U.S. unemployment rate data and retail gasoline prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22018v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Silvia Novo, C\'esar S\'anchez-Sellero</dc:creator>
    </item>
    <item>
      <title>Exact inference via quasi-conjugacy in two-parameter Poisson-Dirichlet hidden Markov models</title>
      <link>https://arxiv.org/abs/2512.22098</link>
      <description>arXiv:2512.22098v1 Announce Type: new 
Abstract: We introduce a nonparametric model for time-evolving, unobserved probability distributions from discrete-time data consisting of unlabelled partitions. The latent process is a two-parameter Poisson-Dirichlet diffusion, and observations arise via exchangeable sampling. Applications include social and genetic data where only aggregate clustering summaries are observed. To address the intractable likelihood, we develop a tractable inferential framework that avoids label enumeration and direct simulation of the latent state. We exploit a duality between the diffusion and a pure-death process on partitions, together with coagulation operators that encode the effect of new data. These yield closed-form, recursive updates for forward and backward inference. We compute exact posterior distributions of the latent state at arbitrary times and predictive distributions of future or interpolated partitions. This enables online and offline inference and forecasting with full uncertainty quantification, bypassing MCMC and sequential Monte Carlo. Compared to particle filtering, our method achieves higher accuracy, lower variance, and substantial computational gains. We illustrate the methodology with synthetic experiments and a social network application, recovering interpretable patterns in time-varying heterozygosity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22098v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Dalla Pria, Matteo Ruggiero, Dario Span\`o</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences in the Presence of Unknown Interference</title>
      <link>https://arxiv.org/abs/2512.21176</link>
      <description>arXiv:2512.21176v1 Announce Type: cross 
Abstract: The stable unit treatment value (SUTVA) is a crucial assumption in the Difference-in-Differences (DiD) research design. It rules out hidden versions of treatment and any sort of interference and spillover effects across units. Even if this is a strong assumption, it has not received much attention from DiD practitioners and, in many cases, it is not even explicitly stated as an assumption, especially the no-interference assumption. In this technical note, we investigate what the DiD estimand identifies in the presence of unknown interference. We show that the DiD estimand identifies a contrast of causal effects, but it is not informative on any of these causal effects separately, without invoking further assumptions. Then, we explore different sets of assumptions under which the DiD estimand becomes informative about specific causal effects. We illustrate these results by revisiting the seminal paper on minimum wages and employment by Card and Krueger (1994).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21176v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabrizia Mealli, Javier Viviens</dc:creator>
    </item>
    <item>
      <title>Inference in the $p_0$ model for directed networks under local differential privacy</title>
      <link>https://arxiv.org/abs/2512.21700</link>
      <description>arXiv:2512.21700v1 Announce Type: cross 
Abstract: We explore the edge-flipping mechanism, a type of input perturbation, to release the directed graph under edge-local differential privacy. By using the noisy bi-degree sequence from the output graph, we construct the moment equations to estimate the unknown parameters in the $p_0$ model, which is an exponential family distribution with the bi-degree sequence as the natural sufficient statistic. We show that the resulting private estimator is asymptotically consistent and normally distributed under some conditions. In addition, we compare the performance of input and output perturbation mechanisms for releasing bi-degree sequences in terms of parameter estimation accuracy and privacy protection. Numerical studies demonstrate our theoretical findings and compare the performance of the private estimates obtained by different types of perturbation methods. We apply the proposed method to analyze the UC Irvine message network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21700v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xueying Sun, Ting Yan, Binyan Jiang</dc:creator>
    </item>
    <item>
      <title>Scalable Causal Structure Learning via Amortized Conditional Independence Testing</title>
      <link>https://arxiv.org/abs/2310.16626</link>
      <description>arXiv:2310.16626v3 Announce Type: replace 
Abstract: Controlling false positives (Type I errors) through statistical hypothesis testing is a foundation of modern scientific data analysis. Existing causal structure discovery algorithms either do not provide Type I error control or cannot scale to the size of modern scientific datasets. We consider a variant of the causal discovery problem with two sets of nodes, where the only edges of interest form a bipartite causal subgraph between the sets. We develop Scalable Causal Structure Learning (SCSL), a method for causal structure discovery on bipartite subgraphs that provides Type I error control. SCSL recasts the discovery problem as a simultaneous hypothesis testing problem and uses discrete optimization over the set of possible confounders to obtain an upper bound on the test statistic for each edge. Semi-synthetic simulations demonstrate that SCSL scales to handle graphs with hundreds of nodes while maintaining error control and good power. We demonstrate the practical applicability of the method by applying it to a cancer dataset to reveal connections between somatic gene mutations and metastases to different tissues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16626v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Leiner, Brian Manzo, Aaditya Ramdas, Wesley Tansey</dc:creator>
    </item>
    <item>
      <title>Principal stratification with U-statistics under principal ignorability</title>
      <link>https://arxiv.org/abs/2403.08927</link>
      <description>arXiv:2403.08927v5 Announce Type: replace 
Abstract: Principal stratification is a popular framework for causal inference in the presence of an intermediate outcome. While the principal average treatment effects are the standard target of inference, they may be insufficient when interest lies in the relative ordering of potential outcomes within a principal stratum. We introduce the principal generalized causal effect estimands to accommodate nonlinear contrast functions, providing robust, probability-scale summaries suitable for ordinal outcomes and win-loss comparisons with composite endpoints. Under principal ignorability, we expand the theoretical results in Jiang et al. (2022, JRSSB) to a broader class of causal estimands in the presence of a binary intermediate variable. We develop nonparametric identification results and derive efficient influence functions for the generalized causal estimands in principal stratification analyses. These efficient influence functions motivate multiply robust estimators and lay the ground for obtaining efficient debiased machine learning estimators via cross-fitting based on U-statistics. The proposed methods are illustrated through simulations and the analysis of a data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08927v5</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Fan Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Spatial-Temporal Non-Gaussian Data Using Predictive Stacking</title>
      <link>https://arxiv.org/abs/2406.04655</link>
      <description>arXiv:2406.04655v4 Announce Type: replace 
Abstract: Analysing non-Gaussian spatial-temporal data requires introducing spatial as well as temporal dependence in generalised linear models through the link function of an exponential family distribution. Unlike in Gaussian likelihoods, inference is considerably encumbered by the inability to analytically integrate out the random effects and reduce the dimension of the parameter space. Iterative estimation algorithms struggle to converge due to the presence of weakly identified parameters. We devise Bayesian inference using predictive stacking that assimilates inference from analytically tractable conditional posterior distributions. We achieve this by expanding upon the Diaconis-Ylvisaker family of conjugate priors and exploiting generalised conjugate multivariate (GCM) distribution theory for exponential families, which enables exact sampling from analytically available posterior distributions conditional upon some process parameters. Subsequently, we assimilate inference over a range of values of these parameters using Bayesian predictive stacking. We evaluate inferential performance on simulated data, compare with full Bayesian inference using Markov chain Monte Carlo (MCMC) and apply our method to analyse spatially-temporally referenced avian count data from the North American Breeding Bird Survey database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04655v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyakanti Pan, Lu Zhang, Jonathan R. Bradley, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Efficient estimation of average treatment effects with unmeasured confounding and proxies</title>
      <link>https://arxiv.org/abs/2501.02214</link>
      <description>arXiv:2501.02214v2 Announce Type: replace 
Abstract: Proximal causal inference provides a framework for estimating the average treatment effect (ATE) in the presence of unmeasured confounding by leveraging outcome and treatment proxies. Identification in this framework relies on the existence of a so-called bridge function. Standard approaches typically postulate a parametric specification for the bridge function, which is estimated in a first step and then plugged into an ATE estimator. However, this sequential procedure suffers from two potential sources of efficiency loss: (i) the difficulty of efficiently estimating a bridge function defined by an integral equation, and (ii) the failure to account for the correlation between the estimation steps. To overcome these limitations, we propose a novel approach that approximates the integral equation with increasing moment restrictions and jointly estimates the bridge function and the ATE. We show that, under suitable conditions, our estimator is efficient. Additionally, we provide a data-driven procedure for selecting the tuning parameter (i.e., the number of moment restrictions). Simulation studies reveal that the proposed method performs well in finite samples, and an application to the right heart catheterization dataset from the SUPPORT study demonstrates its practical value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02214v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5705/ss.202025.0104</arxiv:DOI>
      <arxiv:journal_reference>Statistica Sinica (2025)</arxiv:journal_reference>
      <dc:creator>Chunrong Ai, Jiawei Shan</dc:creator>
    </item>
    <item>
      <title>Asymptotic Analysis and Practical Evaluation of Jump Rate Estimators in Piecewise-Deterministic Markov Processes</title>
      <link>https://arxiv.org/abs/2502.14621</link>
      <description>arXiv:2502.14621v2 Announce Type: replace 
Abstract: Piecewise-deterministic Markov processes (PDMPs) offer a powerful stochastic modeling framework that combines deterministic trajectories with random perturbations at random times. Estimating their local characteristics (particularly the jump rate) is an important yet challenging task. In recent years, non-parametric methods for jump rate inference have been developed, but these approaches often rely on distinct theoretical frameworks, complicating direct comparisons. In this paper, we propose a unified framework to standardize and consolidate state-of-the-art approaches. We establish new results on consistency and asymptotic normality within this framework, enabling rigorous theoretical comparisons of convergence rates and asymptotic variances. Notably, we demonstrate that no single method uniformly outperforms the others, even within the same model. These theoretical insights are validated through numerical simulations using a representative PDMP application: the TCP model. Furthermore, we extend the comparison to real-world data, focusing on cell growth and division dynamics in Escherichia coli. This work enhances the theoretical understanding of PDMP inference while offering practical insights into the relative strengths and limitations of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14621v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Aza\"is, Solune Denis</dc:creator>
    </item>
    <item>
      <title>Borrowing Strength Across Exposures and Outcomes via Index Models for Multi-pollutant Mixtures</title>
      <link>https://arxiv.org/abs/2504.17195</link>
      <description>arXiv:2504.17195v2 Announce Type: replace 
Abstract: An important goal of environmental health research is to assess the health risks posed by mixtures of multiple environmental exposures. In these mixtures analyses, flexible models like Bayesian kernel machine regression and multiple index models are appealing because they allow for arbitrary non-linear exposure-outcome relationships. However, this flexibility comes at the cost of low power, particularly when exposures are highly correlated and the health effects are weak, as is typical in environmental health studies. We propose a multivariate index modelling strategy that borrows strength across exposures and outcomes by exploiting similar mixture component weights and exposure-response relationships. In the special case of distributed lag models, in which exposures are measured repeatedly over time, we jointly encourage co-clustering of lag profiles and exposure-response curves to more efficiently identify critical windows of vulnerability and characterize important exposure effects. We then extend the proposed approach to the multiple index model setting where the true index structure -- the number of indices and their composition -- is unknown, and introduce variable importance measures to quantify component contributions to mixture effects. Using time series data from the National Morbidity, Mortality and Air Pollution Study, we demonstrate the proposed methods by jointly modelling three mortality outcomes and two cumulative air pollution measurements with a maximum lag of 14 days.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17195v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glen McGee, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Selection of functional predictors and smooth coefficient estimation for scalar-on-function regression models</title>
      <link>https://arxiv.org/abs/2506.17773</link>
      <description>arXiv:2506.17773v3 Announce Type: replace 
Abstract: In the framework of scalar-on-function regression models, in which several functional variables are employed to predict a scalar response, we propose a methodology for selecting relevant functional predictors while simultaneously providing accurate smooth (or, more generally, regular) estimates of the functional coefficients. We suppose that the functional predictors belong to a real separable Hilbert space, while the functional coefficients belong to a specific subspace of this Hilbert space. Such a subspace can be a Reproducing Kernel Hilbert Space (RKHS) to ensure the desired regularity characteristics, such as smoothness or periodicity, for the coefficient estimates. Our procedure, called SOFIA (Scalar-On-Function Integrated Adaptive Lasso), is based on an adaptive penalized least squares algorithm that leverages functional subgradients to efficiently solve the minimization problem. We demonstrate that the proposed method satisfies the functional oracle property, even when the number of predictors exceeds the sample size. SOFIA's effectiveness in variable selection and coefficient estimation is evaluated through extensive simulation studies and a real-data application to GDP growth prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17773v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hedayat Fathi, Marzia A. Cremona, Federico Severino</dc:creator>
    </item>
    <item>
      <title>Distributional Evaluation of Generative Models via Relative Density Ratio</title>
      <link>https://arxiv.org/abs/2510.25507</link>
      <description>arXiv:2510.25507v2 Announce Type: replace 
Abstract: We propose a function-valued evaluation metric for generative models based on the relative density ratio (RDR) designed to characterize distributional differences between real and generated samples. As an evaluation metric, the RDR function preserves $\phi$-divergence between two distributions, enables sample-level evaluation that facilitates downstream investigations of feature-specific distributional differences, and has a bounded range that affords clear interpretability and numerical stability. Function estimation of the RDR is achieved efficiently through optimization on the variational form of $\phi$-divergence. We provide theoretical convergence rate guarantees for general estimators based on M-estimator theory, as well as the convergence rate of neural network-based estimators when the true ratio is in the anisotropic Besov space. We demonstrate the power of the proposed RDR-based evaluation through numerical experiments on MNIST, CelebA64, and the American Gut project microbiome data. We show that the estimated RDR enables not only effective overall comparison of competing generative models, but also a convenient way to reveal the underlying nature of goodness-of-fit. This enables one to assess support overlap, coverage, and fidelity while pinpointing regions of the sample space where generators concentrate and revealing the features that drive the most salient distributional differences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25507v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yuliang Xu, Yun Wei, Li Ma</dc:creator>
    </item>
    <item>
      <title>The Probabilistic Foundations of Surveillance Failure: From False Alerts to Structural Bias</title>
      <link>https://arxiv.org/abs/2511.12459</link>
      <description>arXiv:2511.12459v2 Announce Type: replace 
Abstract: For decades, forensic statisticians have debated whether searching large DNA databases undermines the evidential value of a match. Modern surveillance faces an exponentially harder problem: screening populations across thousands of attributes using threshold rules rather than exact matching. Intuition suggests that requiring many coincidental matches should make false alerts astronomically unlikely. This intuition fails.
  Consider a system that monitors 1,000 attributes, each with a 0.5 percent innocent match rate. Matching 15 pre-specified attributes has probability \(10^{-35}\), one in 30 decillion, effectively impossible. But operational systems require no such specificity. They might flag anyone who matches \emph{any} 15 of the 1,000. In a city of one million innocent people, this produces about 226 false alerts. A seemingly impossible event becomes all but guaranteed. This is not an implementation flaw but a mathematical consequence of high-dimensional screening.
  We identify fundamental probabilistic limits on screening reliability. Systems undergo sharp transitions from reliable to unreliable with small increases in data scale, a fragility worsened by data growth and correlations. As data accumulate and correlation collapses effective dimensionality, systems enter regimes where alerts lose evidential value even when individual coincidences remain vanishingly rare. This framework reframes the DNA database controversy as a shift between operational regimes. Unequal surveillance exposures magnify failure, making ``structural bias'' mathematically inevitable. These limits are structural: beyond a critical scale, failure cannot be prevented through threshold adjustment or algorithmic refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12459v2</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>math.PR</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/math14010049</arxiv:DOI>
      <arxiv:journal_reference>Mathematics, 14(1), 49 (2026)</arxiv:journal_reference>
      <dc:creator>Marco Pollanen</dc:creator>
    </item>
    <item>
      <title>Testing Conditional Independence via the Spectral Generalized Covariance Measure: Beyond Euclidean Data</title>
      <link>https://arxiv.org/abs/2511.15453</link>
      <description>arXiv:2511.15453v3 Announce Type: replace 
Abstract: We propose a conditional independence (CI) test based on a new measure, the \emph{spectral generalized covariance measure} (SGCM). The SGCM is constructed by approximating the basis expansion of the squared norm of the conditional cross-covariance operator, using data-dependent bases obtained via spectral decompositions of empirical covariance operators. This construction avoids direct estimation of conditional mean embeddings and reduces the problem to scalar-valued regressions, resulting in robust finite-sample size control. Theoretically, we derive the limiting distribution of the SGCM statistic, establish the validity of a wild bootstrap for inference, and obtain uniform asymptotic size control under doubly robust conditions. As an additional contribution, we show that exponential kernels induced by continuous semimetrics of negative type are characteristic on general Polish spaces -- with extensions to finite tensor products -- thereby providing a foundation for applying our test and other kernel methods to complex objects such as distribution-valued data and curves on metric spaces. Extensive simulations indicate that the SGCM-based CI test attains near-nominal size and exhibits power competitive with or superior to state-of-the-art alternatives across a range of challenging scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15453v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryunosuke Miyazaki, Yoshimasa Uematsu</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for Causal Explainability</title>
      <link>https://arxiv.org/abs/2512.20219</link>
      <description>arXiv:2512.20219v3 Announce Type: replace 
Abstract: Understanding how much each variable contributes to an outcome is a central question across disciplines. A causal view of explainability is favorable for its ability in uncovering underlying mechanisms and generalizing to new contexts. Based on a family of causal explainability quantities, we develop methods for their estimation and inference. In particular, we construct a one-step correction estimator using semi-parametric efficiency theory, which explicitly leverages the independence structure of variables to reduce the asymptotic variance. For a null hypothesis on the boundary, i.e., zero explainability, we show its equivalence to Fisher's sharp null, which motivates a randomization-based inference procedure. Finally, we illustrate the empirical efficacy of our approach through simulations as well as an immigration experiment dataset, where we investigate how features and their interactions shape public opinion toward admitting immigrants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20219v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weihan Zhang, Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2209.15224</link>
      <description>arXiv:2209.15224v5 Announce Type: replace-cross 
Abstract: Unsupervised learning has been widely used in many real-world applications. One of the simplest and most important unsupervised learning models is the Gaussian mixture model (GMM). In this work, we study the multi-task learning problem on GMMs, which aims to leverage potentially similar GMM parameter structures among tasks to obtain improved learning performance compared to single-task learning. We propose a multi-task GMM learning procedure based on the EM algorithm that effectively utilizes unknown similarities between related tasks and is robust against a fraction of outlier tasks from arbitrary distributions. The proposed procedure is shown to achieve the minimax optimal rate of convergence for both parameter estimation error and the excess mis-clustering error, in a wide range of regimes. Moreover, we generalize our approach to tackle the problem of transfer learning for GMMs, where similar theoretical results are derived. Additionally, iterative unsupervised multi-task and transfer learning methods may suffer from an initialization alignment problem, and two alignment algorithms are proposed to resolve the issue. Finally, we demonstrate the effectiveness of our methods through simulations and real data examples. To the best of our knowledge, this is the first work studying multi-task and transfer learning on GMMs with theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.15224v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Tian, Haolei Weng, Lucy Xia, Yang Feng</dc:creator>
    </item>
    <item>
      <title>A Causal Lens for Evaluating Faithfulness Metrics</title>
      <link>https://arxiv.org/abs/2502.18848</link>
      <description>arXiv:2502.18848v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's true reasoning faithfully. While several faithfulness metrics have been proposed, they are often evaluated in isolation, making principled comparisons between them difficult. We present Causal Diagnosticity, a testbed framework for evaluating faithfulness metrics for natural language explanations. We use the concept of diagnosticity, and employ model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate prominent faithfulness metrics, including post-hoc explanation and chain-of-thought methods. Diagnostic performance varies across tasks and models, with Filler Tokens performing best overall. Additionally, continuous metrics are generally more diagnostic than binary ones but can be sensitive to noise and model choice. Our results highlight the need for more robust faithfulness metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18848v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kerem Zaman, Shashank Srivastava</dc:creator>
    </item>
    <item>
      <title>A new machine learning framework for occupational accidents forecasting with safety inspections integration</title>
      <link>https://arxiv.org/abs/2507.00089</link>
      <description>arXiv:2507.00089v2 Announce Type: replace-cross 
Abstract: We propose a generic framework for short-term occupational accident forecasting that leverages safety inspections and models accident occurrences as binary time series. The approach generates daily predictions, which are then aggregated into weekly safety assessments to better inform decision making. To ensure the reliability and operational applicability of the forecasts, we apply a sliding-window cross-validation procedure specifically designed for time series data, combined with an evaluation based on aggregated period-level metrics. Several machine learning algorithms, including logistic regression, tree-based models, and neural networks, are trained and systematically compared within this framework. Unlike the other approaches, the long short-term memory (LSTM) network outperforms the other approaches and detects the upcoming high-risk periods with a balanced accuracy of 0.86, confirming the robustness of our methodology and demonstrating that a binary time series model can anticipate these critical periods based on safety inspections. The proposed methodology converts routine safety inspection data into clear weekly risk scores, detecting the periods when accidents are most likely. Decision-makers can integrate these scores into their planning tools to classify inspection priorities, schedule targeted interventions, and funnel resources to the sites or shifts classified as highest risk, stepping in before incidents occur and getting the greatest return on safety investments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00089v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aho Yapi, Pierre Latouche, Arnaud Guillin, Yan Bailly</dc:creator>
    </item>
    <item>
      <title>Optimal estimation for regression discontinuity design with binary outcomes</title>
      <link>https://arxiv.org/abs/2509.18857</link>
      <description>arXiv:2509.18857v2 Announce Type: replace-cross 
Abstract: We develop a finite-sample optimal estimator for regression discontinuity design when the outcomes are bounded, including binary outcomes as the leading case. Our estimator achieves minimax mean squared error among linear shrinkage estimators with nonnegative weights when the regression function lies in a Lipschitz class. Although the original minimax problem involves an iterative noncovex optimization problem, we show that our estimator is obtained by solving a convex optimization problem. A key advantage of the proposed estimator is that the Lipschitz constant is its only tuning parameter. We also propose a uniformly valid inference procedure without a large-sample approximation. In a simulation exercise for small samples, our estimator exhibits smaller mean squared errors and shorter confidence intervals than those of conventional large-sample techniques. In an empirical multi-cutoff design in which the sample size for each cutoff is small, our method yields informative confidence intervals, in contrast to the leading large-sample approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18857v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Ishihara, Masayuki Sawada, Kohei Yata</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Routing Protocol in Vehicular Opportunistic Networks: A Dynamic Cluster-based Routing Using Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2511.19026</link>
      <description>arXiv:2511.19026v2 Announce Type: replace-cross 
Abstract: Opportunistic Networks (OppNets) employ the Store-Carry-Forward (SCF) paradigm to maintain communication during intermittent connectivity. However, routing performance suffers due to dynamic topology changes, unpredictable contact patterns, and resource constraints including limited energy and buffer capacity. These challenges compromise delivery reliability, increase latency, and reduce node longevity in highly dynamic environments. This paper proposes Cluster-based Routing using Deep Reinforcement Learning (CR-DRL), an adaptive routing approach that integrates an Actor-Critic learning framework with a heuristic function. CR-DRL enables real-time optimal relay selection and dynamic cluster overlap adjustment to maintain connectivity while minimizing redundant transmissions and enhancing routing efficiency. Simulation results demonstrate significant improvements over state-of-the-art baselines. CR-DRL extends node lifetimes by up to 21%, overall energy use is reduced by 17%, and nodes remain active for 15% longer. Communication performance also improves, with up to 10% higher delivery ratio, 28.5% lower delay, 7% higher throughput, and data requiring 30% fewer transmission steps across the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19026v2</guid>
      <category>cs.NI</category>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meisam Sahrifi Sani, Saeid Iranmanesh, Raad Raad, Faisel Tubbal</dc:creator>
    </item>
    <item>
      <title>Treatment Effects with Correlated Spillovers: Bridging Discrete and Continuous Methods</title>
      <link>https://arxiv.org/abs/2512.12653</link>
      <description>arXiv:2512.12653v4 Announce Type: replace-cross 
Abstract: This paper develops a continuous functional framework for treatment effects propagating through geographic space and economic networks. We derive a master equation from three independent economic foundations -- heterogeneous agent aggregation, market equilibrium, and cost minimization -- establishing that the framework rests on fundamental principles rather than ad hoc specifications. The framework nests conventional econometric models -- autoregressive specifications, spatial autoregressive models, and network treatment effect models -- as special cases, providing a bridge between discrete and continuous methods. A key theoretical result shows that the spatial-network interaction coefficient equals the mutual information between geographic and network coordinates, providing a parameter-free measure of channel complementarity. The Feynman-Kac representation characterizes treatment effects as accumulated policy exposure along stochastic paths representing economic linkages, connecting the continuous framework to event study methodology. The no-spillover case emerges as a testable restriction, creating a one-sided risk profile where correct inference is maintained regardless of whether spillovers exist. Monte Carlo simulations confirm that conventional estimators exhibit 25-38% bias when spillovers are present, while our estimator maintains correct inference across all configurations including the no-spillover case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12653v4</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
  </channel>
</rss>
