<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Sep 2025 04:01:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Nonparametric functional data classification and bandwidth selection in the presence of MNAR class variables</title>
      <link>https://arxiv.org/abs/2509.08978</link>
      <description>arXiv:2509.08978v1 Announce Type: new 
Abstract: The problem of nonparametric functional data classification and bandwidth selection is considered when the response variable, also called the class label, might be missing but not at random (MNAR). This setup is broadly acknowledged to be more challenging than the simpler case of missing at random setup. With the focus on kernel methods, we develop nonparametric classification rules and also propose bandwidth selection procedures for the proposed estimators. To study the performance of the resulting classifiers, we look into the rates of convergence of the misclassification error of the proposed method to that of the theoretically optimal classifier. The final-sample performance of our classifiers is assessed via numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08978v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Majid Mojirsheibani</dc:creator>
    </item>
    <item>
      <title>Representation-Aware Distributionally Robust Optimization: A Knowledge Transfer Framework</title>
      <link>https://arxiv.org/abs/2509.09371</link>
      <description>arXiv:2509.09371v1 Announce Type: new 
Abstract: We propose REpresentation-Aware Distributionally Robust Estimation (READ), a novel framework for Wasserstein distributionally robust learning that accounts for predictive representations when guarding against distributional shifts. Unlike classical approaches that treat all feature perturbations equally, READ embeds a multidimensional alignment parameter into the transport cost, allowing the model to differentially discourage perturbations along directions associated with informative representations. This yields robustness to feature variation while preserving invariant structure. Our first contribution is a theoretical foundation: we show that seminorm regularizations for linear regression and binary classification arise as Wasserstein distributionally robust objectives, thereby providing tractable reformulations of READ and unifying a broad class of regularized estimators under the DRO lens. Second, we adopt a principled procedure for selecting the Wasserstein radius using the techniques of robust Wasserstein profile inference. This further enables the construction of valid, representation-aware confidence regions for model parameters with distinct geometric features. Finally, we analyze the geometry of READ estimators as the alignment parameters vary and propose an optimization algorithm to estimate the projection of the global optimum onto this solution surface. This procedure selects among equally robust estimators while optimally constructing a representation structure. We conclude by demonstrating the effectiveness of our framework through extensive simulations and a real-world study, providing a powerful robust estimation grounded in learning representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09371v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zitao Wang, Nian Si, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Unified Framework for Hybrid Aleatory and Epistemic Uncertainty Propagation via Decoupled Multi-Probability Density Evolution Method</title>
      <link>https://arxiv.org/abs/2509.09535</link>
      <description>arXiv:2509.09535v1 Announce Type: new 
Abstract: This paper presents a unified framework for uncertainty propagation in dynamical systems involving hybrid aleatory and epistemic uncertainties. The framework accommodates precise probabilistic, imprecise probabilistic, and non-probabilistic representations, including the distribution-free probability-box (p-box). A central aspect of the framework involves transforming the original uncertainty inputs into an augmented random space, yielding the primary challenge of determining the conditional probability density function (PDF) of the response quantity of interest given epistemic uncertainty parameters. The recently proposed decoupled multi-probability density evolution method (decoupled M-PDEM) is employed to numerically solve the conditional PDF for complex dynamical systems. Several numerical examples illustrate the applicability, efficiency, and accuracy of the proposed framework. These include a linear single-degree-of-freedom (SDOF) system subject to Gaussian white noise with its natural frequency modeled as a p-box, a 10-DOF hysteretic structure subject to imprecise seismic loads, and a crash box model with mixed random and interval system parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09535v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Luo, Meng-Ze Lyu, Matteo Broggi, Marko Thiele, Vasileios C. Fragkoulis, Michael Beer</dc:creator>
    </item>
    <item>
      <title>Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings</title>
      <link>https://arxiv.org/abs/2509.08920</link>
      <description>arXiv:2509.08920v1 Announce Type: cross 
Abstract: This research introduces a novel psychometric method for analyzing textual data using large language models. By leveraging contextual embeddings to create contextual scores, we transform textual data into response data suitable for psychometric analysis. Treating documents as individuals and words as items, this approach provides a natural psychometric interpretation under the assumption that certain keywords, whose contextual meanings vary significantly across documents, can effectively differentiate documents within a corpus. The modeling process comprises two stages: obtaining contextual scores and performing psychometric analysis. In the first stage, we utilize natural language processing techniques and encoder based transformer models to identify common keywords and generate contextual scores. In the second stage, we employ various types of factor analysis, including exploratory and bifactor models, to extract and define latent factors, determine factor correlations, and identify the most significant words associated with each factor. Applied to the Wiki STEM corpus, our experimental results demonstrate the method's potential to uncover latent knowledge dimensions and patterns within textual data. This approach not only enhances the psychometric analysis of textual data but also holds promise for applications in fields rich in textual information, such as education, psychology, and law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08920v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinsong Chen</dc:creator>
    </item>
    <item>
      <title>A Zero-Inflated Spatio-Temporal Model for Integrating Fishery-Dependent and Independent Data under Preferential Sampling</title>
      <link>https://arxiv.org/abs/2509.09336</link>
      <description>arXiv:2509.09336v1 Announce Type: cross 
Abstract: Sustainable management of marine ecosystems is vital for maintaining healthy fishery resources, and benefits from advanced scientific tools to accurately assess species distribution patterns. In fisheries science, two primary data sources are used: fishery-independent data (FID), collected through systematic surveys, and fishery-dependent data (FDD), obtained from commercial fishing activities. While these sources provide complementary information, their distinct sampling schemes - systematic for FID and preferential for FDD - pose significant integration challenges. This study introduces a novel spatio-temporal model that integrates FID and FDD, addressing challenges associated with zero-inflation and preferential sampling (PS) common in ecological data. The model employs a six-layer structure to differentiate between presence-absence and biomass observations, offering a robust framework for ecological studies affected by PS biases. Simulation results demonstrate the model's accuracy in parameter estimation across diverse PS scenarios and its ability to detect preferential signals. Application to the study of the distribution patterns of the European sardine populations along the southern Portuguese continental shelf illustrates the model's effectiveness in integrating diverse data sources and incorporating environmental and vessel-specific covariates. The model reveals spatio-temporal variability in sardine presence and biomass, providing actionable insights for fisheries management. Beyond ecology, this framework offers broad applicability to data integration challenges in other disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09336v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniela Silva, Raquel Menezes, Gon\c{c}alo Ara\'ujo, Ana Machado, Renato Rosa, Ana Moreno, Alexandra Silva, Susana Garrido</dc:creator>
    </item>
    <item>
      <title>Statistical modeling to adjust for time trends in adaptive platform trials utilizing non-concurrent controls</title>
      <link>https://arxiv.org/abs/2403.14348</link>
      <description>arXiv:2403.14348v2 Announce Type: replace 
Abstract: Utilizing non-concurrent control data (NCC) in the analysis of late-entering arms in platform trials has recently received considerable attention. While incorporating NCC can lead to increased power and lower sample sizes, it might introduce bias to the effect estimators if temporal drifts are present. Aiming to mitigate this potential bias, we propose various frequentist model-based approaches that leverage the NCC, while adjusting for time. One of the currently available models incorporates time as a categorical fixed effect, separating the trial duration into periods, defined as time intervals bounded by any arm entering or leaving the platform. In this work, we propose two extensions of this model. First, we consider an alternative definition of time by dividing the trial into fixed-length calendar time intervals. Second, we propose alternative model-based time adjustments. Specifically, we investigate adjusting for random effects and employing splines to model time with a polynomial function. We evaluate the performance of the proposed approaches in a simulation study and illustrate their use through a case study. We show that adjusting for time via a spline function controls the type I error in trials with a sufficiently smooth time trend pattern and may lead to power gains compared to the standard fixed effect model. However, the fixed effect model with period adjustment is the most robust model for arbitrary time trends, provided that the trend is equal across all arms. Especially, in trials with sudden changes in the time trend, the period-adjustment model is preferred if NCC are included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14348v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/bimj.70059</arxiv:DOI>
      <arxiv:journal_reference>Biometrical Journal (2025)</arxiv:journal_reference>
      <dc:creator>Pavla Krotka, Martin Posch, Mohamed Gewily, G\"unter H\"oglinger, Marta Bofill Roig</dc:creator>
    </item>
    <item>
      <title>Selective Randomization Inference for Adaptive Experiments</title>
      <link>https://arxiv.org/abs/2405.07026</link>
      <description>arXiv:2405.07026v3 Announce Type: replace 
Abstract: Adaptive experiments use preliminary analyses of the data to inform further course of action and are commonly used in many disciplines including medical and social sciences. Because the null hypothesis and experimental design are data-dependent, it has long been recognized that statistical inference for adaptive experiments is not straightforward. Most existing methods only apply to specific adaptive designs and rely on strong assumptions. In this work, we propose selective randomization inference as a general framework for analysing adaptive experiments. In a nutshell, our approach applies conditional post-selection inference to randomization tests. By using directed acyclic graphs to describe the data generating process, we derive a selective randomization p-value that controls the selective type-I error. As inference only relies on the randomness in the treatment assignment, no modelling assumptions or independent and identically distributed data are needed. We elaborate on conditions that render the proposed p-value computable and provide rejection sampling and MCMC algorithms to find a Monte Carlo approximation. Moreover, this article shows how to estimate and construct confidence intervals for a homogeneous treatment effect. Lastly, we demonstrate our method and compare it with other randomization tests using synthetic and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07026v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Freidling, Qingyuan Zhao, Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data</title>
      <link>https://arxiv.org/abs/2405.14492</link>
      <description>arXiv:2405.14492v4 Announce Type: replace 
Abstract: Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, full-scale approximations (FSAs) combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce computational costs in calculating likelihoods, gradients, and predictive distributions with FSAs. In particular, we introduce a novel preconditioner and show theoretically and empirically that it accelerates the conjugate gradient method's convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Furthermore, we introduce an accurate and fast way to calculate predictive variances using stochastic simulation and iterative methods. In addition, we show how our newly proposed FITC preconditioner can also be used in iterative methods for Vecchia approximations. In our experiments, it outperforms existing state-of-the-art preconditioners for Vecchia approximations. All methods are implemented in a free C++ software library with high-level Python and R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14492v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Gyger, Reinhard Furrer, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>Average Causal Effect Estimation in DAGs with Hidden Variables: Beyond Back-Door and Front-Door Criteria</title>
      <link>https://arxiv.org/abs/2409.03962</link>
      <description>arXiv:2409.03962v2 Announce Type: replace 
Abstract: The identification theory for causal effects in directed acyclic graphs (DAGs) with hidden variables is well established, but methods for estimating and inferring functionals that extend beyond the g-formula remain underdeveloped. Previous studies have introduced semiparametric estimators for such functionals in a broad class of DAGs with hidden variables. While these estimators exhibit desirable statistical properties such as double robustness in certain cases, they also face significant limitations. Notably, they encounter substantial computational challenges, particularly involving density estimation and numerical integration for continuous variables, and their estimates may fall outside the parameter space of the target estimand. Additionally, the asymptotic properties of these estimators is underexplored, especially when integrating flexible statistical and machine learning models for nuisance functional estimations. This paper addresses these challenges by introducing novel one-step corrected plug-in and targeted minimum loss-based estimators of causal effects for a class of hidden variable DAGs that go beyond classical back-door and front-door criteria (known as the treatment primal fixability criterion in prior literature). These estimators leverage data-adaptive machine learning algorithms to minimize modeling assumptions while ensuring key statistical properties including double robustness, efficiency, boundedness within the target parameter space, and asymptotic linearity under $L^2(P)$-rate conditions for nuisance functional estimates that yield root-n consistent causal effect estimates. To ensure our estimation methods are accessible in practice, we provide the flexCausal package in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03962v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Guo, Razieh Nabi</dc:creator>
    </item>
    <item>
      <title>Bayesian Transfer Learning for Artificially Intelligent Geospatial Systems: A Predictive Stacking Approach</title>
      <link>https://arxiv.org/abs/2410.09504</link>
      <description>arXiv:2410.09504v3 Announce Type: replace 
Abstract: Building artificially intelligent geospatial systems requires rapid delivery of spatial data analysis on massive scales with minimal human intervention. Depending upon their intended use, data analysis can also involve model assessment and uncertainty quantification. This article devises transfer learning frameworks for deployment in artificially intelligent systems, where a massive data set is split into smaller data sets that stream into the analytical framework to propagate learning and assimilate inference for the entire data set. Specifically, we introduce Bayesian predictive stacking for multivariate spatial data and demonstrate rapid and automated analysis of massive data sets. Furthermore, inference is delivered without human intervention without excessively demanding hardware settings. We illustrate the effectiveness of our approach through extensive simulation experiments and in producing inference from massive dataset on vegetation index that are indistinguishable from traditional (and more expensive) statistical approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09504v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Presicce, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Modular Jump Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.15557</link>
      <description>arXiv:2505.15557v2 Announce Type: replace 
Abstract: Gaussian processes (GPs) furnish accurate nonlinear predictions with well-calibrated uncertainty. However, the typical GP setup has a built-in stationarity assumption, making it ill-suited for modeling data from processes with sudden changes, or "jumps" in the output variable. The "jump GP" (JGP) was developed for modeling data from such processes, combining local GPs and latent "level" variables under a joint inferential framework. But joint modeling can be fraught with difficulty. We aim to simplify by suggesting a more modular setup, eschewing joint inference but retaining the main JGP themes: (a) learning optimal neighborhood sizes that locally respect manifolds of discontinuity; and (b) a new cluster-based (latent) feature to capture regions of distinct output levels on both sides of the manifold. We show that each of (a) and (b) separately leads to dramatic improvements when modeling processes with jumps. In tandem (but without requiring joint inference) that benefit is compounded, as illustrated on real and synthetic benchmark examples from the recent literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15557v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna R. Flowers, Christopher T. Franck, Micka\"el Binois, Chiwoo Park, Robert B. Gramacy</dc:creator>
    </item>
    <item>
      <title>Triply Robust Panel Estimators</title>
      <link>https://arxiv.org/abs/2508.21536</link>
      <description>arXiv:2508.21536v2 Announce Type: replace 
Abstract: This paper studies estimation of causal effects in a panel data setting. We introduce a new estimator, the Triply RObust Panel (TROP) estimator, that combines (i) a flexible model for the potential outcomes based on a low-rank factor structure on top of a two-way-fixed effect specification, with (ii) unit weights intended to upweight units similar to the treated units and (iii) time weights intended to upweight time periods close to the treated time periods. We study the performance of the estimator in a set of simulations designed to closely match several commonly studied real data sets. We find that there is substantial variation in the performance of the estimators across the settings considered. The proposed estimator outperforms two-way-fixed-effect/difference-in-differences, synthetic control, matrix completion and synthetic-difference-in-differences estimators. We investigate what features of the data generating process lead to this performance, and assess the relative importance of the three components of the proposed estimator. We have two recommendations. Our preferred strategy is that researchers use simulations closely matched to the data they are interested in, along the lines discussed in this paper, to investigate which estimators work well in their particular setting. A simpler approach is to use more robust estimators such as synthetic difference-in-differences or the new triply robust panel estimator which we find to substantially outperform two-way fixed effect estimators in many empirically relevant settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21536v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susan Athey, Guido Imbens, Zhaonan Qu, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models</title>
      <link>https://arxiv.org/abs/2508.19441</link>
      <description>arXiv:2508.19441v2 Announce Type: replace-cross 
Abstract: Partial differential equations (PDEs) underpin the modeling of many natural and engineered systems. It can be convenient to express such models as neural PDEs rather than using traditional numerical PDE solvers by replacing part or all of the PDE's governing equations with a neural network representation. Neural PDEs are often easier to differentiate, linearize, reduce, or use for uncertainty quantification than the original numerical solver. They are usually trained on solution trajectories obtained by long time integration of the PDE solver. Here we propose a more sample-efficient data-augmentation strategy for generating neural PDE training data from a computer model by space-filling sampling of local "stencil" states. This approach removes a large degree of spatiotemporal redundancy present in trajectory data and oversamples states that may be rarely visited but help the neural PDE generalize across the state space. We demonstrate that accurate neural PDE stencil operators can be learned from synthetic training data generated by the computational equivalent of 10 timesteps' worth of numerical simulation. Accuracy is further improved if we assume access to a single full-trajectory simulation from the computer model, which is typically available in practice. Across several PDE systems, we show that our data-augmented synthetic stencil data yield better trained neural stencil operators, with clear performance gains compared with naively sampled stencil data from simulation trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19441v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanket Jantre, Deepak Akhare, Xiaoning Qian, Nathan M. Urban</dc:creator>
    </item>
  </channel>
</rss>
