<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Deterministic complexity analysis of Hermitian eigenproblems</title>
      <link>https://arxiv.org/abs/2410.21550</link>
      <description>arXiv:2410.21550v1 Announce Type: new 
Abstract: In this work we revisit the arithmetic and bit complexity of Hermitian eigenproblems. We first provide an analysis for the divide-and-conquer tridiagonal eigensolver of Gu and Eisenstat [GE95] in the Real RAM model, when accelerated with the Fast Multipole Method. The analysis asserts the claimed nearly-$O(n^2)$ complexity to compute a full diagonalization of a symmetric tridiagonal matrix. Combined with the tridiagonal reduction algorithm of Sch\"onhage [Sch72], it implies that a Hermitian matrix can be diagonalized deterministically in $O(n^{\omega}\log(n)+n^2\mathrm{polylog}(n/\epsilon))$ arithmetic operations, where $\omega\lesssim 2.371$ is the square matrix multiplication exponent. This improves the classic deterministic $O(n^3)$ diagonalization algorithms, and derandomizes the $ O(n^{\omega}\log^2(n/\epsilon))$ algorithm of [BGVKS, FOCS '20]. Ultimately, this has a direct application to the SVD, which is widely used as a subroutine in advanced algorithms, but its complexity and approximation guarantees are often unspecified.
  In finite precision, we show that Sch\"onhage's algorithm is stable in floating point using $O(\log(n/\epsilon))$ bits. Combined with the (rational arithmetic) algorithm of Bini and Pan [BP91], it provides a deterministic algorithm to compute all the eigenvalues of a Hermitian matrix in $O\left(n^{\omega}F\left(\log(n/\epsilon)\right)+n^2\mathrm{polylog}(n/\epsilon)\right)$ bit operations, where $F(b)\in\widetilde{O}(b)$ is the bit complexity of a single floating point operation on $b$ bits. This improves the best known $\widetilde{O}(n^3)$ deterministic and $O\left( n^{\omega}\log^2(n/\epsilon)F\left(\log^4(n/\epsilon)\log(n)\right)\right)$ randomized complexities. We conclude with some other useful subroutines such as computing spectral gaps, condition numbers, and spectral projectors, and few open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21550v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleksandros Sobczyk</dc:creator>
    </item>
    <item>
      <title>Maximum Partial List H-Coloring on P_5-free graphs in polynomial time</title>
      <link>https://arxiv.org/abs/2410.21569</link>
      <description>arXiv:2410.21569v1 Announce Type: new 
Abstract: In this article we show that Maximum Partial List H-Coloring is polynomial-time solvable on P_5-free graphs for every fixed graph H. In particular, this implies that Maximum k-Colorable Subgraph is polynomial-time solvable on P_5-free graphs. This answers an open question from Agrawal, Lima, Lokshtanov, Saurabh &amp; Sharma [SODA 2024]. This also improves the $n^{\omega(G)}$-time algorithm for Maximum Partial H-Coloring by Chudnovsky, King, Pilipczuk, Rz\k{a}\.{z}ewski &amp; Spirkl [SIDMA 2021] to polynomial-time algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21569v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Lokshtanov, Pawe{\l} Rz\k{a}\.zewski, Saket Saurabh, Roohani Sharma, Meirav Zehavi</dc:creator>
    </item>
    <item>
      <title>Improved Spectral Density Estimation via Explicit and Implicit Deflation</title>
      <link>https://arxiv.org/abs/2410.21690</link>
      <description>arXiv:2410.21690v1 Announce Type: new 
Abstract: We study algorithms for approximating the spectral density of a symmetric matrix $A$ that is accessed through matrix-vector product queries. By combining a previously studied Chebyshev polynomial moment matching method with a deflation step that approximately projects off the largest magnitude eigendirections of $A$ before estimating the spectral density, we give an $\epsilon\cdot\sigma_\ell(A)$ error approximation to the spectral density in the Wasserstein-$1$ metric using $O(\ell\log n+ 1/\epsilon)$ matrix-vector products, where $\sigma_\ell(A)$ is the $\ell^{th}$ largest singular value of $A$. In the common case when $A$ exhibits fast singular value decay, our bound can be much stronger than prior work, which gives an error bound of $\epsilon \cdot ||A||_2$ using $O(1/\epsilon)$ matrix-vector products. We also show that it is nearly tight: any algorithm giving error $\epsilon \cdot \sigma_\ell(A)$ must use $\Omega(\ell+1/\epsilon)$ matrix-vector products.
  We further show that the popular Stochastic Lanczos Quadrature (SLQ) method matches the above bound, even though SLQ itself is parameter-free and performs no explicit deflation. This bound explains the strong practical performance of SLQ, and motivates a simple variant of SLQ that achieves an even tighter error bound. Our error bound for SLQ leverages an analysis that views it as an implicit polynomial moment matching method, along with recent results on low-rank approximation with single-vector Krylov methods. We use these results to show that the method can perform implicit deflation as part of moment matching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21690v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajarshi Bhattacharjee, Rajesh Jayaram, Cameron Musco, Christopher Musco, Archan Ray</dc:creator>
    </item>
    <item>
      <title>Edge Arrival Online Matching: The Power of Free Disposal on Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2410.21727</link>
      <description>arXiv:2410.21727v1 Announce Type: new 
Abstract: Online matching is a fundamental problem in the study of online algorithms. We study the problem under a very general arrival model: the edge arrival model. Free disposal is an important notion in the online matching literature, which allows the algorithm to dispose of the previously matched edges. Without free disposal, we cannot achieve any bounded ratio, even with randomized algorithms, when edges are weighted.
  Our paper focuses on clarifying the power of free disposal in both the unweighted and the weighted setting. As far as we know, it's still uncertain if free disposal can give us extra leverage to enhance the competitive ratio in the unweighted scenario, even in specific instances such as Growing Trees, where every new edge adds a new leaf to the graph. Our study serves as a valuable initial exploration of this open question. The results are listed as follows:
  1. With free disposal, we improve the competitive ratio for unweighted online matching on Growing Trees from $5/9$ to $2/3 \approx 0.66$, and show that the ratio is tight. For Forests, a more general setting where the underlying graph is a forest and edges may arrive in arbitrary order, we improve the competitive ratio from $5/9$ to $5/8 = 0.625$.
  2. Both the ratios of $2/3$ and $0.625$ show a separation to the upper bound of the competitive ratio without free disposal on Growing Trees ($0.5914$). Therefore, we demonstrate the additional power of free disposal for the unweighted setting for the first time, at least in the special setting of Growing Trees and Forests.
  3. We improve the competitive ratio for weighted online matching on Growing Trees from $1/3$ to $1/2$ using a very simple ordinal algorithm, and show that it is optimal among ordinal algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21727v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianle Jiang, Yuhao Zhang</dc:creator>
    </item>
    <item>
      <title>Beating Bellman's Algorithm for Subset Sum</title>
      <link>https://arxiv.org/abs/2410.21942</link>
      <description>arXiv:2410.21942v1 Announce Type: new 
Abstract: Bellman's algorithm for Subset Sum is one of the earliest and simplest examples of dynamic programming, dating back to 1957. For a given set of $n$ integers $X$ and a target $t$, it computes the set of subset sums $\mathcal S(X, t)$ (i.e., the set of integers $s \in [0\ldots t]$ for which there is a subset of $X$ summing to $s$) in time $O(|\mathcal S(X, t)| \cdot n)$. Since then, it has been an important question whether Bellman's seminal algorithm can be improved.
  This question is addressed in many recent works. And yet, while some algorithms improve upon Bellman's algorithm in specific parameter regimes, such as Bringmann's $\tilde O(t + n)$-time algorithm [SODA '17] and Bringmann and Nakos' $\tilde O(|\mathcal S(X, t)|^{4/3})$-time algorithm [STOC '20], none of the known algorithms beats Bellman's algorithm in all regimes. In particular, it remained open whether Subset Sum is in time $\tilde O(|\mathcal S(X, t)| \cdot n^{1-\epsilon})$ (for some $\epsilon &gt; 0$).
  In this work we positively resolve this question and design an algorithm that outperforms Bellman's algorithm in all regimes. Our algorithm runs in time $\tilde O(|\mathcal S(X, t)| \cdot \sqrt{n})$, thus improving the time complexity by a factor of nearly $\sqrt n$. Our key innovation is the use of a result from additive combinatorics, which has not been applied in an algorithmic context before and which we believe to be of further independent interest for algorithm design. To demonstrate the broader applicability of our approach, we extend our ideas to a variant of Subset Sum on vectors as well as to Unbounded Subset Sum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21942v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karl Bringmann, Nick Fischer, Vasileios Nakos</dc:creator>
    </item>
    <item>
      <title>Sumsets, 3SUM, Subset Sum: Now for Real!</title>
      <link>https://arxiv.org/abs/2410.21953</link>
      <description>arXiv:2410.21953v1 Announce Type: new 
Abstract: We study a broad class of algorithmic problems with an "additive flavor" such as computing sumsets, 3SUM, Subset Sum and geometric pattern matching. Our starting point is that these problems can often be solved efficiently for integers, owed to the rich available tool set including bit-tricks, linear hashing, and the Fast Fourier Transform. However, for real numbers these tools are not available, leading to significant gaps in the best-known running times for integer inputs versus for real inputs. In this work our goal is to close this gap.
  As our key contribution we design a new technique for computing real sumsets. It is based on a surprising blend of algebraic ideas (like Prony's method and coprime factorizations) with combinatorial tricks. We then apply our new algorithm to the aforementioned problems and successfully obtain, in all cases, equally fast algorithms for real inputs. Specifically, we replicate the running times of the following landmark results by randomized algorithms in the standard real RAM model:
  - Sumsets: Given two sets $A,B$, their sumset $A+B=\{a+b:a\in A,b\in B\}$ can be computed in time $\tilde O(|A+B|)$ [Cole, Hariharan; STOC'02].
  - Geometric pattern matching: Given two sets $A,B$, we can test whether there is some shift such that $A+s\subseteq B$ in time $\tilde O(|A|+|B|)$ [Cardoze, Schulman; FOCS'98].
  - 3SUM with preprocessing: We can preprocess three size-$n$ sets $A,B,C$ in time $\tilde O(n^2)$ such that upon query of sets $A'\subseteq A,B'\subseteq B,C'\subseteq C$, the 3SUM instance $(A',B',C')$ can be decided in time $\tilde O(n^{13/7})$ [Chan, Lewenstein; STOC'15].
  - Output-sensitive Subset Sum: Given a size-$n$ (multi-)set $X$ and a target $t$, we can compute the set of subset sums $\{\Sigma(X'):X'\subseteq X,\Sigma(X')\leq t\}$ in output-sensitive time $\tilde O(n+\mathrm{out}^{4/3})$ [Bringmann, Nakos; STOC'20].</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21953v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Fischer</dc:creator>
    </item>
    <item>
      <title>Ideal Membership Problem for Boolean Minority and Dual Discriminator</title>
      <link>https://arxiv.org/abs/2410.22102</link>
      <description>arXiv:2410.22102v1 Announce Type: new 
Abstract: We consider the polynomial Ideal Membership Problem (IMP) for ideals encoding combinatorial problems that are instances of CSPs over a finite language. In this paper, the input polynomial $f$ has degree at most $d=O(1)$ (we call this problem IMP$_d$). We bridge the gap in \cite{MonaldoMastrolilli2019} by proving that the IMP$_d$ for Boolean combinatorial ideals whose constraints are closed under the minority polymorphism can be solved in polynomial time. This completes the identification of the tractability for the Boolean IMP$_d$. We also prove that the proof of membership for the IMP$_d$ for problems constrained by the dual discriminator polymorphism over any finite domain can be found in polynomial time. Our results can be used in applications such as Nullstellensatz and Sum-of-Squares proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22102v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arpitha P. Bharathi, Monaldo Mastrolilli</dc:creator>
    </item>
    <item>
      <title>Faster two-dimensional pattern matching with $k$ mismatches</title>
      <link>https://arxiv.org/abs/2410.22109</link>
      <description>arXiv:2410.22109v1 Announce Type: new 
Abstract: The classical pattern matching asks for locating all occurrences of one string, called the pattern, in another, called the text, where a string is simply a sequence of characters. Due to the potential practical applications, it is desirable to seek approximate occurrences, for example by bounding the number of mismatches. This problem has been extensively studied, and by now we have a good understanding of the best possible time complexity as a function of $n$ (length of the text), $m$ (length of the pattern), and $k$ (number of mismatches). In particular, we know that for $k=\mathcal{O}(\sqrt{m})$, we can achieve quasi-linear time complexity [Gawrychowski and Uzna\'nski, ICALP 2018].
  We consider a natural generalisation of the approximate pattern matching problem to two-dimensional strings, which are simply square arrays of characters. The exact version of this problem has been extensively studied in the early 90s. While periodicity, which is the basic tool for one-dimensional pattern matching, admits a natural extension to two dimensions, it turns out to become significantly more challenging to work with, and it took some time until an alphabet-independent linear-time algorithm has been obtained by Galil and Park [SICOMP 1996].
  In the approximate two-dimensional pattern matching, we are given a pattern of size $m\times m$ and a text of size $n\times n$, and ask for all locations in the text where the pattern matches with at most $k$ mismatches. The asymptotically fastest algorithm for this algorithm works in $\mathcal{O}(kn^{2})$ time [Amir and Landau, TCS 1991]. We provide a new insight into two-dimensional periodicity to improve on these 30-years old bounds. Our algorithm works in $\tilde{\mathcal{O}}((m^{2}+mk^{5/4})n^{2}/m^{2})$ time, which is $\tilde{\mathcal{O}}(n^{2})$ for $k=\mathcal{O}(m^{4/5})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22109v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Ellert, Pawe{\l} Gawrychowski, Adam G\'orkiewicz, Tatiana Starikovskaya</dc:creator>
    </item>
    <item>
      <title>Testing Identity of Distributions under Kolmogorov Distance in Polylogarithmic Space</title>
      <link>https://arxiv.org/abs/2410.22123</link>
      <description>arXiv:2410.22123v1 Announce Type: new 
Abstract: Suppose we have a sample from a distribution $D$ and we want to test whether $D = D^*$ for a fixed distribution $D^*$. Specifically, we want to reject with constant probability, if the distance of $D$ from $D^*$ is $\geq \varepsilon$ in a given metric. In the case of continuous distributions, this has been studied thoroughly in the statistics literature. Namely, for the well-studied Kolmogorov metric a test is known that uses the optimal $O(1/\varepsilon^2)$ samples.
  However, this test naively uses also space $O(1/\varepsilon^2)$, and previous work improved this to $O(1/\varepsilon)$. In this paper, we show that much less space suffices -- we give an algorithm that uses space $O(\log^4 \varepsilon^{-1})$ in the streaming setting while also using an asymptotically optimal number of samples. This is in contrast with the standard total variation distance on discrete distributions for which such space reduction is known to be impossible. Finally, we state 9 related open problems that we hope will spark interest in this and related problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22123v1</guid>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Janos Lebeda, Jakub T\v{e}tek</dc:creator>
    </item>
    <item>
      <title>I/O complexity and pebble games with partial computations</title>
      <link>https://arxiv.org/abs/2410.22237</link>
      <description>arXiv:2410.22237v1 Announce Type: new 
Abstract: Optimizing data movements during program executions is essential for achieving high performance in modern computing systems. This has been classically modeled with the Red-Blue Pebble Game and its variants. In the existing models, it is typically assumed that the number of red pebbles, i.e., the size of the fast memory, is larger than the maximum in-degree in the computational graph (e.g. an arithmetic circuit). This assumption can be restrictive for many real applications, especially when dealing with "big data" in Machine Learning and Scientific Computing. In this work we study a generalization of the original Red-Blue Pebble Game to allow arbitrary in-degrees, that can be larger than the size of the fast memory. The objective is to minimize the I/O operations by allowing the computation of partial results in the fast memory. We show that this variant of the problem is NP-complete, even for the special case where the computational graph consists of a single level, and only two words fit in the fast memory. Approximation algorithms for a couple of special cases are also outlined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22237v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleksandros Sobczyk</dc:creator>
    </item>
    <item>
      <title>Approximately Counting Knapsack Solutions in Subquadratic Time</title>
      <link>https://arxiv.org/abs/2410.22267</link>
      <description>arXiv:2410.22267v1 Announce Type: new 
Abstract: We revisit the classic #Knapsack problem, which asks to count the Boolean points $(x_1,\dots,x_n)\in\{0,1\}^n$ in a given half-space $\sum_{i=1}^nW_ix_i\le T$. This #P-complete problem admits $(1\pm\epsilon)$-approximation. Before this work, [Dyer, STOC 2003]'s $\tilde{O}(n^{2.5}+n^2{\epsilon^{-2}})$-time randomized approximation scheme remains the fastest known in the natural regime of $\epsilon\ge 1/polylog(n)$. In this paper, we give a randomized $(1\pm\epsilon)$-approximation algorithm in $\tilde{O}(n^{1.5}{\epsilon^{-2}})$ time (in the standard word-RAM model), achieving the first sub-quadratic dependence on $n$. Such sub-quadratic running time is rare in the approximate counting literature in general, as a large class of algorithms naturally faces a quadratic-time barrier.
  Our algorithm follows Dyer's framework, which reduces #Knapsack to the task of sampling (and approximately counting) solutions in a randomly rounded instance with poly(n)-bounded integer weights. We refine Dyer's framework using the following ideas:
  - We decrease the sample complexity of Dyer's Monte Carlo method, by proving some structural lemmas for typical points near the input hyperplane via hitting-set arguments, and appropriately setting the rounding scale.
  - Instead of running a vanilla dynamic program on the rounded instance, we employ techniques from the growing field of pseudopolynomial-time Subset Sum algorithms, such as FFT, divide-and-conquer, and balls-into-bins hashing of [Bringmann, SODA 2017].
  We also need other ingredients, including a surprising application of the recent Bounded Monotone (max,+)-Convolution algorithm by [Chi-Duan-Xie-Zhang, STOC 2022] (adapted by [Bringmann-D\"urr-Polak, ESA 2024]), the notion of sum-approximation from [Gawrychowski-Markin-Weimann, ICALP 2018]'s #Knapsack approximation scheme, and a two-phase extension of Dyer's framework for handling tiny weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22267v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiming Feng, Ce Jin</dc:creator>
    </item>
    <item>
      <title>Learning the structure of any Hamiltonian from minimal assumptions</title>
      <link>https://arxiv.org/abs/2410.21635</link>
      <description>arXiv:2410.21635v1 Announce Type: cross 
Abstract: We study the problem of learning an unknown quantum many-body Hamiltonian $H$ from black-box queries to its time evolution $e^{-\mathrm{i} H t}$. Prior proposals for solving this task either impose some assumptions on $H$, such as its interaction structure or locality, or otherwise use an exponential amount of computational postprocessing. In this paper, we present efficient algorithms to learn any $n$-qubit Hamiltonian, assuming only a bound on the number of Hamiltonian terms, $m \leq \mathrm{poly}(n)$. Our algorithms do not need to know the terms in advance, nor are they restricted to local interactions. We consider two models of control over the time evolution: the first has access to time reversal ($t &lt; 0$), enabling an algorithm that outputs an $\epsilon$-accurate classical description of $H$ after querying its dynamics for a total of $\widetilde{O}(m/\epsilon)$ evolution time. The second access model is more conventional, allowing only forward-time evolutions; our algorithm requires $\widetilde{O}(\|H\|^3/\epsilon^4)$ evolution time in this setting. Central to our results is the recently introduced concept of a pseudo-Choi state of $H$. We extend the utility of this learning resource by showing how to use it to learn the Fourier spectrum of $H$, how to achieve nearly Heisenberg-limited scaling with it, and how to prepare it even under our more restricted access models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21635v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Zhao</dc:creator>
    </item>
    <item>
      <title>Classical Algorithms for Constant Approximation of the Ground State Energy of Local Hamiltonians</title>
      <link>https://arxiv.org/abs/2410.21833</link>
      <description>arXiv:2410.21833v1 Announce Type: cross 
Abstract: We construct classical algorithms computing an approximation of the ground state energy of an arbitrary $k$-local Hamiltonian acting on $n$ qubits.
  We first consider the setting where a good ``guiding state'' is available, which is the main setting where quantum algorithms are expected to achieve an exponential speedup over classical methods. We show that a constant approximation of the ground state energy can be computed classically in $\mathrm{poly}\left(1/\chi,n\right)$ time and $\mathrm{poly}(n)$ space, where $\chi$ denotes the overlap between the guiding state and the ground state (as in prior works in dequantization, we assume sample-and-query access to the guiding state). This gives a significant improvement over the recent classical algorithm by Gharibian and Le Gall (SICOMP 2023), and matches (up a to polynomial overhead) both the time and space complexities of quantum algorithms for constant approximation of the ground state energy. We also obtain classical algorithms for higher-precision approximation.
  For the setting where no guided state is given (i.e., the standard version of the local Hamiltonian problem), we obtain a classical algorithm computing a constant approximation of the ground state energy in $2^{O(n)}$ time and $\mathrm{poly}(n)$ space. To our knowledge, before this work it was unknown how to classically achieve these bounds simultaneously, even for constant approximation. We also discuss complexity-theoretic aspects of our results and their implications for the quantum PCP conjecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21833v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Le Gall</dc:creator>
    </item>
    <item>
      <title>On Eigenvector Approximation of Diagonalizable Random Matrices with Random Perturbations: Properties and Applications</title>
      <link>https://arxiv.org/abs/2410.21919</link>
      <description>arXiv:2410.21919v1 Announce Type: cross 
Abstract: We extend the result on the top eigenvalue of the i.i.d.\ matrix with fixed perturbations by Tao to random perturbations. In particular, we consider a setup that $\mathbf{M}=\mathbf{W}+\lambda\mathbf{u}\mathbf{u}^*$ with $\mathbf{W}$ drawn from a Ginibre Orthogonal Ensemble and the perturbation $\mathbf{u}$ drawn uniformly from $\mathcal{S}^{d-1}$. We provide several asymptotic properties about the eigenvalues and the top eigenvector of the random matrix, which can not be obtained trivially from the deterministic perturbation case.
  We also apply our results to extend the work of Max Simchowitz, which provides an optimal lower bound for approximating the eigenspace of a symmetric matrix. We present a \textit{query complexity} lower bound for approximating the eigenvector of any asymmetric but diagonalizable matrix $\mathbf{M}$ corresponding to the largest eigenvalue. We show that for every $\operatorname{gap}\in (0,1/2]$ and large enough dimension $d$, there exists a random matrix $\mathbf{M}$ with $\operatorname{gap}(\mathbf{M})=\Omega(\operatorname{gap})$, such that if a matrix-vector query product algorithm can identity a vector $\hat{\mathbf{v}}$ which satisfies $\left\|\hat{\mathbf{v}}-\mathbf{v}_1(\mathbf{M}) \right\|_2^2\le \operatorname{const}\times \operatorname{gap}$, it needs at least $\mathcal{O}\left(\frac{\log d}{\operatorname{gap}}\right)$ queries of matrix-vector products. In the inverse polynomial accuracy regime where $\epsilon \ge \frac{1}{\operatorname{poly}(d)}$, the complexity matches the upper bounds $\mathcal{O}\left(\frac{\log(d/\epsilon)}{\operatorname{gap}}\right)$, which can be obtained via the power method. As far as we know, it is the first lower bound for computing the eigenvector of an asymmetric matrix, which is far more complicated than in the symmetric case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21919v1</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Chen, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Random zero sets with local growth guarantees</title>
      <link>https://arxiv.org/abs/2410.21931</link>
      <description>arXiv:2410.21931v1 Announce Type: cross 
Abstract: We prove that if $(\mathcal{M},d)$ is an $n$-point metric space that embeds quasisymmetrically into a Hilbert space, then for every $\tau&gt;0$ there is a random subset $\mathcal{Z}$ of $\mathcal{M}$ such that for any pair of points $x,y\in \mathcal{M}$ with $d(x,y)\ge \tau$, the probability that both $x\in \mathcal{Z}$ and $d(y,\mathcal{Z})\ge \beta\tau/\sqrt{1+\log (|B(y,\kappa \beta \tau)|/|B(y,\beta \tau)|)}$ is $\Omega(1)$, where $\kappa&gt;1$ is a universal constant and $\beta&gt;0$ depends only on the modulus of the quasisymmetric embedding. The proof relies on a refinement of the Arora--Rao--Vazirani rounding technique. Among the applications of this result is that the largest possible Euclidean distortion of an $n$-point subset of $\ell_1$ is $\Theta(\sqrt{\log n})$, and the integrality gap of the Goemans--Linial semidefinite program for the Sparsest Cut problem on inputs of size $n$ is $\Theta(\sqrt{\log n})$. Multiple further applications are given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21931v1</guid>
      <category>math.MG</category>
      <category>cs.DS</category>
      <category>math.FA</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alan Chang, Assaf Naor, Kevin Ren</dc:creator>
    </item>
    <item>
      <title>A note on polynomial-time tolerant testing stabilizer states</title>
      <link>https://arxiv.org/abs/2410.22220</link>
      <description>arXiv:2410.22220v1 Announce Type: cross 
Abstract: We show an improved inverse theorem for the Gowers-$3$ norm of $n$-qubit quantum states $|\psi\rangle$ which states that: for every $\gamma\geq 0$, if the $\textsf{Gowers}(|\psi \rangle,3)^8 \geq \gamma$ then the stabilizer fidelity of $|\psi\rangle$ is at least $\gamma^C$ for some constant $C&gt;1$. This implies a constant-sample polynomial-time tolerant testing algorithm for stabilizer states which accepts if an unknown state is $\varepsilon_1$-close to a stabilizer state in fidelity and rejects when $|\psi\rangle$ is $\varepsilon_2 \leq \varepsilon_1^C$-far from all stabilizer states, promised one of them is the case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22220v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Srinivasan Arunachalam, Sergey Bravyi, Arkopal Dutt</dc:creator>
    </item>
    <item>
      <title>Online Dependent Rounding Schemes for Bipartite Matchings, with Applications</title>
      <link>https://arxiv.org/abs/2301.08680</link>
      <description>arXiv:2301.08680v3 Announce Type: replace 
Abstract: We introduce the abstract problem of rounding an unknown fractional bipartite $b$-matching $\bf{x}$ revealed online (e.g., output by an online fractional algorithm), exposed node-by-node on~one~side. The objective is to maximize the \emph{rounding ratio} of the output matching $M$, which is the minimum over all fractional $b$-matchings $\bf{x}$, and edges $e$, of the ratio $\Pr[e\in M]/x_e$. In analogy with the highly influential offline dependent rounding schemes of Gandhi et al.~(FOCS'02, JACM'06), we refer to such algorithms as \emph{online dependent rounding schemes} (ODRSes). This problem, with additional restrictions on the possible inputs $\bf{x}$, has played a key role in recent developments in online computing.
  We provide the first generic $b$-matching ODRSes that impose no restrictions on $\bf{x}$. Specifically, we provide ODRSes with rounding ratios of $0.646$ and $0.652$ for $b$-matchings and simple matchings, respectively. This breaks the natural barrier of $1-1/e$, prevalent for online matching problems, and numerous online problems more broadly. Using our ODRSes, we provide a number of algorithms with similar better-than-$(1-1/e)$ ratios for several problems in online edge coloring, stochastic optimization, and more.
  Our techniques, which have already found applications in several follow-up works (Patel and Wajc SODA'24, Blikstad et al.~SODA'25, Braverman et al.~SODA'25, and Aouad et al.~2024), include periodic use of \emph{offline} contention resolution schemes (in online algorithm design), grouping nodes, and a new scaling method which we call \emph{group discount and individual markup}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08680v3</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Joseph (Seffi),  Naor, Aravind Srinivasan, David Wajc</dc:creator>
    </item>
    <item>
      <title>Invariant subspaces and PCA in nearly matrix multiplication time</title>
      <link>https://arxiv.org/abs/2311.10459</link>
      <description>arXiv:2311.10459v4 Announce Type: replace 
Abstract: Approximating invariant subspaces of generalized eigenvalue problems (GEPs) is a fundamental computational problem at the core of machine learning and scientific computing. It is, for example, the root of Principal Component Analysis (PCA) for dimensionality reduction, data visualization, and noise filtering, and of Density Functional Theory (DFT), arguably the most popular method to calculate the electronic structure of materials. Given Hermitian $H,S\in\mathbb{C}^{n\times n}$, where $S$ is positive-definite, let $\Pi_k$ be the true spectral projector on the invariant subspace that is associated with the $k$ smallest (or largest) eigenvalues of the GEP $HC=SC\Lambda$, for some $k\in[n]$. We show that we can compute a matrix $\widetilde\Pi_k$ such that $\lVert\Pi_k-\widetilde\Pi_k\rVert_2\leq \epsilon$, in $O\left( n^{\omega+\eta}\mathrm{polylog}(n,\epsilon^{-1},\kappa(S),\mathrm{gap}_k^{-1}) \right)$ bit operations in the floating point model, for some $\epsilon\in(0,1)$, with probability $1-1/n$. Here, $\eta&gt;0$ is arbitrarily small, $\omega\lesssim 2.372$ is the matrix multiplication exponent, $\kappa(S)=\lVert S\rVert_2\lVert S^{-1}\rVert_2$, and $\mathrm{gap}_k$ is the gap between eigenvalues $k$ and $k+1$. To achieve such provable "forward-error" guarantees, our methods rely on a new $O(n^{\omega+\eta})$ stability analysis for the Cholesky factorization, and a smoothed analysis for computing spectral gaps, which can be of independent interest. Ultimately, we obtain new matrix multiplication-type bit complexity upper bounds for PCA problems, including classical PCA and (randomized) low-rank approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10459v4</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleksandros Sobczyk, Marko Mladenovi\'c, Mathieu Luisier</dc:creator>
    </item>
    <item>
      <title>Moderate Dimension Reduction for $k$-Center Clustering</title>
      <link>https://arxiv.org/abs/2312.01391</link>
      <description>arXiv:2312.01391v5 Announce Type: replace 
Abstract: The Johnson-Lindenstrauss (JL) Lemma introduced the concept of dimension reduction via a random linear map, which has become a fundamental technique in many computational settings. For a set of $n$ points in $\mathbb{R}^d$ and any fixed $\epsilon&gt;0$, it reduces the dimension $d$ to $O(\log n)$ while preserving, with high probability, all the pairwise Euclidean distances within factor $1+\epsilon$. Perhaps surprisingly, the target dimension can be lower if one only wishes to preserve the optimal value of a certain problem on the pointset, e.g., Euclidean max-cut or $k$-means. However, for some notorious problems, like diameter (aka furthest pair), dimension reduction via the JL map to below $O(\log n)$ does not preserve the optimal value within factor $1+\epsilon$.
  We propose to focus on another regime, of \emph{moderate dimension reduction}, where a problem's value is preserved within factor $\alpha&gt;1$ using target dimension $\tfrac{\log n}{poly(\alpha)}$. We establish the viability of this approach and show that the famous $k$-center problem is $\alpha$-approximated when reducing to dimension $O(\tfrac{\log n}{\alpha^2}+\log k)$. Along the way, we address the diameter problem via the special case $k=1$. Our result extends to several important variants of $k$-center (with outliers, capacities, or fairness constraints), and the bound improves further with the input's doubling dimension.
  While our $poly(\alpha)$-factor improvement in the dimension may seem small, it actually has significant implications for streaming algorithms, and easily yields an algorithm for $k$-center in dynamic geometric streams, that achieves $O(\alpha)$-approximation using space $poly(kdn^{1/\alpha^2})$. This is the first algorithm to beat $O(n)$ space in high dimension $d$, as all previous algorithms require space at least $\exp(d)$. Furthermore, it extends to the $k$-center variants mentioned above.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01391v5</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.SoCG.2024.64</arxiv:DOI>
      <dc:creator>Shaofeng H. -C. Jiang, Robert Krauthgamer, Shay Sapir</dc:creator>
    </item>
    <item>
      <title>On Sparse Covers of Minor Free Graphs, Low Dimensional Metric Embeddings, and other applications</title>
      <link>https://arxiv.org/abs/2401.14060</link>
      <description>arXiv:2401.14060v3 Announce Type: replace 
Abstract: Given a metric space $(X,d_X)$, a $(\beta,s,\Delta)$-sparse cover is a collection of clusters $\mathcal{C}\subseteq P(X)$ with diameter at most $\Delta$, such that for every point $x\in X$, the ball $B_X(x,\frac\Delta\beta)$ is fully contained in some cluster $C\in \mathcal{C}$, and $x$ belongs to at most $s$ clusters in $\mathcal{C}$. Our main contribution is to show that the shortest path metric of every $K_r$-minor free graphs admits $(O(r),O(r^2),\Delta)$-sparse cover, and for every $\epsilon&gt;0$, $(4+\epsilon,O(\frac1\epsilon)^r,\Delta)$-sparse cover (for arbitrary $\Delta&gt;0$). We then use this sparse cover to show that every $K_r$-minor free graph embeds into $\ell_\infty^{\tilde{O}(\frac1\epsilon)^{r+1}\cdot\log n}$ with distortion $3+\epsilon$ (resp. into $\ell_\infty^{\tilde{O}(r^2)\cdot\log n}$ with distortion $O(r)$). Further, among other applications, this sparse cover immediately implies an algorithm for the oblivious buy-at-bulk problem in fixed minor free graphs with the tight approximation factor $O(\log n)$ (previously nothing beyond general graphs was known).</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14060v3</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>math.CO</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnold Filtser</dc:creator>
    </item>
    <item>
      <title>Finding Maximum Common Contractions Between Phylogenetic Networks</title>
      <link>https://arxiv.org/abs/2405.16713</link>
      <description>arXiv:2405.16713v2 Announce Type: replace 
Abstract: In this paper, we lay the groundwork on the comparison of phylogenetic networks based on edge contractions and expansions as edit operations, as originally proposed by Robinson and Foulds to compare trees. We prove that these operations connect the space of all phylogenetic networks on the same set of leaves, even if we forbid contractions that create cycles. This allows to define an operational distance on this space, as the minimum number of contractions and expansions required to transform one network into another. We highlight the difference between this distance and the computation of the maximum common contraction between two networks. Given its ability to outline a common structure between them, which can provide valuable biological insights, we study the algorithmic aspects of the latter. We first prove that computing a maximum common contraction between two networks is NP-hard, even when the maximum degree, the size of the common contraction, or the number of leaves is bounded. We also provide lower bounds to the problem based on the Exponential-Time Hypothesis. Nonetheless, we do provide a polynomial-time algorithm for weakly-galled networks, a generalization of galled trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16713v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bertrand Marchand, Nadia Tahiri, Olivier Tremblay-Savard, Manuel Lafond</dc:creator>
    </item>
    <item>
      <title>Randomized Greedy Online Edge Coloring Succeeds for Dense and Randomly-Ordered Graphs</title>
      <link>https://arxiv.org/abs/2406.13000</link>
      <description>arXiv:2406.13000v2 Announce Type: replace 
Abstract: Vizing's theorem states that any graph of maximum degree $\Delta$ can be properly edge colored with at most $\Delta+1$ colors. In the online setting, it has been a matter of interest to find an algorithm that can properly edge color any graph on $n$ vertices with maximum degree $\Delta = \omega(\log n)$ using at most $(1+o(1))\Delta$ colors. Here we study the na\"{i}ve random greedy algorithm, which simply chooses a legal color uniformly at random for each edge upon arrival. We show that this algorithm can $(1+\epsilon)\Delta$-color the graph for arbitrary $\epsilon$ in two contexts: first, if the edges arrive in a uniformly random order, and second, if the edges arrive in an adversarial order but the graph is sufficiently dense, i.e., $n = O(\Delta)$. Prior to this work, the random greedy algorithm was only known to succeed in trees.
  Our second result is applicable even when the adversary is adaptive, and therefore implies the existence of a deterministic edge coloring algorithm which $(1+\epsilon)\Delta$ edge colors a dense graph. Prior to this, the best known deterministic algorithm for this problem was the simple greedy algorithm which utilized $2\Delta-1$ colors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13000v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aditi Dudeja, Rashmika Goswami, Michael Saks</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Fault Tolerance for Efficient Batch Matrix Multiplication via an Additive Combinatorics Lens</title>
      <link>https://arxiv.org/abs/2312.16460</link>
      <description>arXiv:2312.16460v2 Announce Type: replace-cross 
Abstract: Fault tolerance is a major concern in distributed computational settings. In the classic master-worker setting, a server (the master) needs to perform some heavy computation which it may distribute to $m$ other machines (workers) in order to speed up the time complexity. In this setting, it is crucial that the computation is made robust to failed workers, in order for the master to be able to retrieve the result of the joint computation despite failures. A prime complexity measure is thus the \emph{recovery threshold}, which is the number of workers that the master needs to wait for in order to derive the output. This is the counterpart to the number of failed workers that it can tolerate.
  In this paper, we address the fundamental and well-studied task of matrix multiplication. Specifically, our focus is on when the master needs to multiply a batch of $n$ pairs of matrices. Several coding techniques have been proven successful in reducing the recovery threshold for this task, and one approach that is also very efficient in terms of computation time is called \emph{Rook Codes}. The previously best known recovery threshold for batch matrix multiplication using Rook Codes is $O(n^{\log_2{3}})=O(n^{1.585})$.
  Our main contribution is a lower bound proof that says that any Rook Code for batch matrix multiplication must have a recovery threshold that is at least $\omega(n)$. Notably, we employ techniques from Additive Combinatorics in order to prove this, which may be of further interest. Moreover, we show a Rook Code that achieves a recovery threshold of $n^{1+o(1)}$, establishing a near-optimal answer to the fault tolerance of this coding scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16460v2</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keren Censor-Hillel, Yuka Machino, Pedro Soto</dc:creator>
    </item>
    <item>
      <title>Finding $d$-Cuts in Graphs of Bounded Diameter, Graphs of Bounded Radius and $H$-Free Graphs</title>
      <link>https://arxiv.org/abs/2404.11389</link>
      <description>arXiv:2404.11389v2 Announce Type: replace-cross 
Abstract: The $d$-Cut problem is to decide if a graph has an edge cut such that each vertex has at most $d$ neighbours at the opposite side of the cut. If $d=1$, we obtain the intensively studied Matching Cut problem. The $d$-Cut problem has been studied as well, but a systematic study for special graph classes was lacking. We initiate such a study and consider classes of bounded diameter, bounded radius and $H$-free graphs. We prove that for all $d\geq 2$, $d$-Cut is polynomial-time solvable for graphs of diameter $2$, $(P_3+P_4)$-free graphs and $P_5$-free graphs. These results extend known results for $d=1$. However, we also prove several NP-hardness results for $d$-Cut that contrast known polynomial-time results for $d=1$. Our results lead to full dichotomies for bounded diameter and bounded radius and to almost-complete dichotomies for $H$-free graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11389v2</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felicia Lucke, Ali Momeni, Dani\"el Paulusma, Siani Smith</dc:creator>
    </item>
    <item>
      <title>A Note on Approximating Weighted Nash Social Welfare with Additive Valuations</title>
      <link>https://arxiv.org/abs/2404.15607</link>
      <description>arXiv:2404.15607v2 Announce Type: replace-cross 
Abstract: We give the first $O(1)$-approximation for the weighted Nash Social Welfare problem with additive valuations. The approximation ratio we obtain is $e^{1/e} + \epsilon \approx 1.445 + \epsilon$, which matches the best known approximation ratio for the unweighted case \cite{BKV18}.
  Both our algorithm and analysis are simple. We solve a natural configuration LP for the problem, and obtain the allocation of items to agents using a randomized version of the Shmoys-Tardos rounding algorithm developed for unrelated machine scheduling problems. In the analysis, we show that the approximation ratio of the algorithm is at most the worst gap between the Nash social welfare of the optimum allocation and that of an EF1 allocation, for an unweighted Nash Social Welfare instance with identical additive valuations. This was shown to be at most $e^{1/e} \approx 1.445$ by Barman et al., leading to our approximation ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15607v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuda Feng, Shi Li</dc:creator>
    </item>
    <item>
      <title>Better Gaussian Mechanism using Correlated Noise</title>
      <link>https://arxiv.org/abs/2408.06853</link>
      <description>arXiv:2408.06853v2 Announce Type: replace-cross 
Abstract: We present a simple variant of the Gaussian mechanism for answering differentially private queries when the sensitivity space has a certain common structure. Our motivating problem is the fundamental task of answering $d$ counting queries under the add/remove neighboring relation. The standard Gaussian mechanism solves this task by adding noise distributed as a Gaussian with variance scaled by $d$ independently to each count. We show that adding a random variable distributed as a Gaussian with variance scaled by $(\sqrt{d} + 1)/4$ to all counts allows us to reduce the variance of the independent Gaussian noise samples to scale only with $(d + \sqrt{d})/4$. The total noise added to each counting query follows a Gaussian distribution with standard deviation scaled by $(\sqrt{d} + 1)/2$ rather than $\sqrt{d}$. The central idea of our mechanism is simple and the technique is flexible. We show that applying our technique to another problem gives similar improvements over the standard Gaussian mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06853v2</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Janos Lebeda</dc:creator>
    </item>
  </channel>
</rss>
