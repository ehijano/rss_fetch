<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Sep 2025 04:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?</title>
      <link>https://arxiv.org/abs/2509.21629</link>
      <description>arXiv:2509.21629v1 Announce Type: new 
Abstract: Program verification relies on loop invariants, yet automatically discovering strong invariants remains a long-standing challenge. We introduce a principled framework for evaluating LLMs on invariant synthesis. Our approach uses a verifier-based decision procedure with a formal soundness guarantee and assesses not only correctness but also the speedup that invariants provide in verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based verifiers against the traditional solver UAutomizer. While LLM-based verifiers represent a promising direction, they do not yet offer a significant advantage over UAutomizer. Model capability also proves critical, as shown by sharp differences in speedups across models, and our benchmark remains an open challenge for current LLMs. Finally, we show that supervised fine-tuning and Best-of-N sampling can improve performance: fine-tuning on 3589 instances raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%, and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21629v1</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjiang Wei, Tarun Suresh, Tianran Sun, Haoze Wu, Ke Wang, Alex Aiken</dc:creator>
    </item>
    <item>
      <title>Compiling by Proving: Language-Agnostic Automatic Optimization from Formal Semantics</title>
      <link>https://arxiv.org/abs/2509.21793</link>
      <description>arXiv:2509.21793v1 Announce Type: new 
Abstract: Verification proofs encode complete program behavior, yet we discard them after checking correctness. We present compiling by proving, a paradigm that transforms these proofs into optimized execution rules. By constructing All-Path Reachability Proofs through symbolic execution and compiling their graph structure, we consolidate many semantic rewrites into single rules while preserving correctness by construction. We implement this as a language-agnostic extension to the K framework. Evaluation demonstrates performance improvements across different compilation scopes: opcode-level optimizations show consistent speedups, while whole-program compilation achieves orders of magnitude greater performance gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21793v1</guid>
      <category>cs.PL</category>
      <category>cs.CL</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianhong Zhao, Everett Hildenbrandt, Juan Conejero, Yongwang Zhao</dc:creator>
    </item>
    <item>
      <title>Committing to the bit: Relational programming with semiring arrays and SAT solving</title>
      <link>https://arxiv.org/abs/2509.22614</link>
      <description>arXiv:2509.22614v1 Announce Type: new 
Abstract: We propose semiringKanren, a relational programming language where each relation expression denotes a semiring array. We formalize a type system that restricts the arrays to finite size. We then define a semantics that is parameterized by the semiring that the arrays draw their elements from. We compile semiringKanren types to bitstring representations. For the Boolean semiring, this compilation enables us to use an SAT solver to run semiringKanren programs efficiently. We compare the performance of semiringKanren and faster miniKanren for solving Sudoku puzzles. Our experiment shows that semiringKanren can be a more efficient variant of miniKanren.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22614v1</guid>
      <category>cs.PL</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitri Volkov, Yafei Yang, Chung-chieh Shan</dc:creator>
    </item>
    <item>
      <title>On Code-Induced Reasoning in LLMs</title>
      <link>https://arxiv.org/abs/2509.21499</link>
      <description>arXiv:2509.21499v1 Announce Type: cross 
Abstract: Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21499v1</guid>
      <category>cs.CL</category>
      <category>cs.PL</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul Waheed, Zhen Wu, Carolyn Ros\'e, Daphne Ippolito</dc:creator>
    </item>
    <item>
      <title>A Target-Agnostic Protocol-Independent Interface for the Transport Layer</title>
      <link>https://arxiv.org/abs/2509.21550</link>
      <description>arXiv:2509.21550v1 Announce Type: cross 
Abstract: Transport protocols are fundamental to network communications, continuously evolving to meet the demands of new applications, workloads, and network architectures while running in a wide range of execution environments (a.k.a targets). We argue that this diversity across protocols and targets calls for a high-level, target-agnostic programming abstraction for the transport layer. Specifically, we propose to specify transport protocols as high-level programs that take an event and flow state as input, and using constrained C-like constructs, produce the updated state along with target-agnostic instructions for key transport operations such as data reassembly, packet generation and scheduling, and timer manipulations.
  We show the benefits of our high-level transport programs by developing multiple transport protocols in our programming framework called TINF, developing two TINF- compliant backends, one in DPDK and one in Linux eXpress DataPath, and deploying TINF programs for multiple protocols across both backends. Inspired by the benefits unlocked by L2/L3 packet-processing languages like P4, we believe target-agnostic transport programs can reduce the development effort for transport protocols, enable automated analysis and formal verification of the transport layer, and further research in programmable targets for transport protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21550v1</guid>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <category>cs.PL</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pedro Mizuno, Kimiya Mohammadtaheri, Linfan Qian, Joshua Johnson, Danny Akbarzadeh, Chris Neely, Mario Baldi, Nacihket Kapre, Mina Tahmasbi Arashloo</dc:creator>
    </item>
    <item>
      <title>Guard Analysis and Safe Erasure Gradual Typing: a Type System for Elixir</title>
      <link>https://arxiv.org/abs/2408.14345</link>
      <description>arXiv:2408.14345v3 Announce Type: replace 
Abstract: We formalize a new type system for Elixir, a dynamically typed functional programming language of growing popularity that runs on the Erlang virtual machine. Our system combines gradual typing with semantic subtyping to enable precise, sound, and practical static type analysis, without requiring any changes to Elixir's compilation pipeline or runtime. Type soundness is ensured by leveraging runtime checks -- both implicit, from the Erlang VM, and explicit, via developer-written guards.
  Central to our approach are two key innovations: the notion of "strong functions", which can be assigned precise types even when applied to inputs that may fall outside their intended domain; and a fine-grained analysis of guards that enables accurate type refinement for case expressions and guarded function definitions. While type information is erased before execution and not used by the compiler, our "safe erasure" gradual typing strategy maintains soundness and expressiveness without compromising compatibility or performance. This work lays the theoretical foundation for Elixir's new type system, outlines its integration into recent versions of the language, and demonstrates its effectiveness on large-scale industrial codebases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14345v3</guid>
      <category>cs.PL</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Castagna, Guillaume Duboc</dc:creator>
    </item>
    <item>
      <title>A Verified Compiler for Quantum Simulation</title>
      <link>https://arxiv.org/abs/2509.18583</link>
      <description>arXiv:2509.18583v3 Announce Type: replace 
Abstract: Hamiltonian simulation is a central application of quantum computing, with significant potential in modeling physical systems and solving complex optimization problems. Existing compilers for such simulations typically focus on low-level representations based on Pauli operators, limiting programmability and offering no formal guarantees of correctness across the compilation pipeline. We introduce QBlue, a high-level, formally verified framework for compiling Hamiltonian simulations. QBlue is based on the formalism of second quantization, which provides a natural and expressive way to describe quantum particle systems using creation and annihilation operators. To ensure safety and correctness, QBlue includes a type system that tracks particle types and enforces Hermitian structure. The framework supports compilation to both digital and analog quantum circuits and captures multiple layers of semantics, from static constraints to dynamic evolution. All components of QBlue, including its language design, type system, and compilation correctness, are fully mechanized in the Rocq proof framework, making it the first end-to-end verified compiler for second-quantized Hamiltonian simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18583v3</guid>
      <category>cs.PL</category>
      <category>quant-ph</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liyi Li, Fenfen An, Federico Zahariev, Zhi Xiang Chong, Amr Sabry, Mark S. Gordon</dc:creator>
    </item>
    <item>
      <title>Efficient Symbolic Computation via Hash Consing</title>
      <link>https://arxiv.org/abs/2509.20534</link>
      <description>arXiv:2509.20534v2 Announce Type: replace 
Abstract: Symbolic computation systems suffer from memory inefficiencies due to redundant storage of structurally identical subexpressions, commonly known as expression swell, which degrades performance in both classical computer algebra and emerging AI-driven mathematical reasoning tools. In this paper, we present the first integration of hash consing into JuliaSymbolics, a high-performance symbolic toolkit in Julia, by employing a global weak-reference hash table that canonicalizes expressions and eliminates duplication. This approach reduces memory consumption and accelerates key operations such as differentiation, simplification, and code generation, while seamlessly integrating with Julia's metaprogramming and just-in-time compilation infrastructure. Benchmark evaluations across different computational domains reveal substantial improvements: symbolic computations are accelerated by up to 3.2 times, memory usage is reduced by up to 2 times, code generation is up to 5 times faster, function compilation up to 10 times faster, and numerical evaluation up to 100 times faster for larger models. While certain workloads with fewer duplicate unknown-variable expressions show more modest gains or even slight overhead in initial computation stages, downstream processing consistently benefits significantly. These findings underscore the importance of hash consing in scaling symbolic computation and pave the way for future work integrating hash consing with e-graphs for enhanced equivalence-aware expression sharing in AI-driven pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20534v2</guid>
      <category>cs.PL</category>
      <category>cs.SC</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bowen Zhu, Aayush Sabharwal, Songchen Tan, Yingbo Ma, Alan Edelman, Christopher Rackauckas</dc:creator>
    </item>
    <item>
      <title>SuperCoder: Assembly Program Superoptimization with Large Language Models</title>
      <link>https://arxiv.org/abs/2505.11480</link>
      <description>arXiv:2505.11480v2 Announce Type: replace-cross 
Abstract: Superoptimization is the task of transforming a program into a faster one while preserving its input-output behavior. In this work, we investigate whether large language models (LLMs) can serve as superoptimizers, generating assembly programs that outperform code already optimized by industry-standard compilers. We construct the first large-scale benchmark for this problem, consisting of 8,072 real-world assembly programs averaging 130 lines, in contrast to prior datasets restricted to 2-15 straight-line, loop-free programs. We evaluate 23 LLMs on this benchmark and find that the strongest baseline, Claude-opus-4, achieves a 51.5% test-passing rate and a 1.43x average speedup over gcc -O3. To further enhance performance, we fine-tune models with reinforcement learning, optimizing a reward function that integrates correctness and performance speedup. Starting from Qwen2.5-Coder-7B-Instruct (61.4% correctness, 1.10x speedup), the fine-tuned model SuperCoder attains 95.0% correctness and 1.46x average speedup. Our results demonstrate, for the first time, that LLMs can be applied as superoptimizers for assembly programs, establishing a foundation for future research in program performance optimization beyond compiler heuristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11480v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjiang Wei, Tarun Suresh, Huanmi Tan, Yinglun Xu, Gagandeep Singh, Ke Wang, Alex Aiken</dc:creator>
    </item>
    <item>
      <title>ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation</title>
      <link>https://arxiv.org/abs/2509.20380</link>
      <description>arXiv:2509.20380v2 Announce Type: replace-cross 
Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity of their hardware and parallel programming frameworks. Directive-based parallel programming standards like OpenACC simplify GPU programming to some extent by abstracting away low-level complexities, but a fair amount of expertise is still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically fine-tuned for generating expert OpenACC directives for data-parallel loops, along with the supervised fine-tuning dataset that was used to train them. The ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for testing. Experimental evaluations show a pronounced performance gap in generating correct OpenACC pragmas between base LLMs and our fine-tuned versions. On the held-out test set, base LLMs fail to consistently generate valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid pragmas with the correct directive type for $87\%$ of the data-parallel loops, and exact pragmas--including directives, clauses, clause order, and clause variables--for $50\%$ of the cases. Even when not exact, generated pragmas frequently incorporate the correct clauses in a different order than the ground-truth label, or include additional clauses that enable finer control over parallel execution, data movement, and concurrency, offering practical value beyond strict string-matching. By publicly releasing the code, models, and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU offloading of serially written programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20380v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Samyak Jhaveri, Vanessa Klotzmann, Crista Lopes</dc:creator>
    </item>
  </channel>
</rss>
