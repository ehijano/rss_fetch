<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CG updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CG</link>
    <description>cs.CG updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CG" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 May 2025 01:28:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Even Faster Algorithm for the Chamfer Distance</title>
      <link>https://arxiv.org/abs/2505.08957</link>
      <description>arXiv:2505.08957v1 Announce Type: new 
Abstract: For two d-dimensional point sets A, B of size up to n, the Chamfer distance from A to B is defined as CH(A,B) = \sum_{a \in A} \min_{b \in B} \|a-b\|. The Chamfer distance is a widely used measure for quantifying dissimilarity between sets of points, used in many machine learning and computer vision applications. A recent work of Bakshi et al, NeuriPS'23, gave the first near-linear time (1+eps)-approximate algorithm, with a running time of O(ndlog(n)/eps^2). In this paper we improve the running time further, to O(nd(loglog(n)+log(1/eps))/eps^2). When eps is a constant, this reduces the gap between the upper bound and the trivial Omega(dn) lower bound significantly, from O(log n) to O(loglog n).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08957v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Feng, Piotr Indyk</dc:creator>
    </item>
    <item>
      <title>Approximating the Directed Hausdorff Distance</title>
      <link>https://arxiv.org/abs/2505.09046</link>
      <description>arXiv:2505.09046v2 Announce Type: new 
Abstract: The Hausdorff distance is a metric commonly used to compute the set similarity of geometric sets.
  For sets containing a total of $n$ points, the exact distance can be computed na\"{i}vely in $O(n^2)$ time.
  In this paper, we show how to preprocess point sets individually so that the Hausdorff distance of any pair can then be approximated in linear time.
  We assume that the metric is doubling.
  The preprocessing time for each set is $O(n\log \Delta)$ where $\Delta$ is the ratio of the largest to smallest pairwise distances of the input.
  In theory, this can be reduced to $O(n\log n)$ time using a much more complicated algorithm.
  We compute $(1+\varepsilon)$-approximate Hausdorff distance in $(2 + \frac{1}{\varepsilon})^{O(d)}n$ time in a metric space with doubling dimension $d$.
  The $k$-partial Hausdorff distance ignores $k$ outliers to increase stability.
  Additionally, we give a linear-time algorithm to compute directed $k$-partial Hausdorff distance for all values of $k$ at once with no change to the preprocessing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09046v2</guid>
      <category>cs.CG</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver A. Chubet, Parth M. Parikh, Donald R. Sheehy, Siddharth S. Sheth</dc:creator>
    </item>
  </channel>
</rss>
