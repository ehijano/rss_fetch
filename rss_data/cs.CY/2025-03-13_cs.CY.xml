<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Mar 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Prioritizing Computing Research to Empower and Protect Vulnerable Populations</title>
      <link>https://arxiv.org/abs/2503.09612</link>
      <description>arXiv:2503.09612v1 Announce Type: new 
Abstract: Technology can pose signicant risks to a wide array of vulnerable populations. However, by addressing the challenges and opportunities in technology design, research, and deployment, we can create systems that benet everyone, fostering a society where even the most vulnerable are empowered and supported.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09612v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pamela Wisniewski (Vanderbilt University), Katie Siek (Indiana University Bloomington), Kevin Butler (University of Florida), Gabrielle Allen (University of Wyoming), Weisong Shi (University of Delaware), Manish Parashar (University of Utah)</dc:creator>
    </item>
    <item>
      <title>Empowering the Future Workforce: Prioritizing Education for the AI-Accelerated Job Market</title>
      <link>https://arxiv.org/abs/2503.09613</link>
      <description>arXiv:2503.09613v1 Announce Type: new 
Abstract: AI's rapid integration into the workplace demands new approaches to workforce education and training and broader AI literacy across disciplines. Coordinated action from government, industry, and educational institutions is necessary to ensure workers can adapt to accelerating technological change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09613v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lisa Amini (IBM Research), Henry F. Korth (Lehigh University), Nita Patel (Otis), Evan Peck (University of Colorado Boulder), Ben Zorn (Microsoft)</dc:creator>
    </item>
    <item>
      <title>Reversing the Computing Research Workforce Shortfall: Bolstering Domestic Student Pathways to PhDs</title>
      <link>https://arxiv.org/abs/2503.09614</link>
      <description>arXiv:2503.09614v1 Announce Type: new 
Abstract: To sustain innovation and safeguard national security, the U.S. must strengthen domestic pathways to computing PhDs by engaging talented undergraduates early - before they are committed to industry - with research experiences, mentorship, and financial support for graduate studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09614v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susanne Hambrusch (Purdue University), Lori Pollock (University of Delaware), Mary Hall (University of Utah), Nancy M. Amato (University of Illinois Urbana-Champaign)</dc:creator>
    </item>
    <item>
      <title>Complementarity, Augmentation, or Substitutivity? The Impact of Generative Artificial Intelligence on the U.S. Federal Workforce</title>
      <link>https://arxiv.org/abs/2503.09637</link>
      <description>arXiv:2503.09637v1 Announce Type: new 
Abstract: This study investigates the near-future impacts of generative artificial intelligence (AI) technologies on occupational competencies across the U.S. federal workforce. We develop a multi-stage Retrieval-Augmented Generation system to leverage large language models for predictive AI modeling that projects shifts in required competencies and to identify vulnerable occupations on a knowledge-by-skill-by-ability basis across the federal government workforce. This study highlights policy recommendations essential for workforce planning in the era of AI. We integrate several sources of detailed data on occupational requirements across the federal government from both centralized and decentralized human resource sources, including from the U.S. Office of Personnel Management (OPM) and various federal agencies. While our preliminary findings suggest some significant shifts in required competencies and potential vulnerability of certain roles to AI-driven changes, we provide nuanced insights that support arguments against abrupt or generic approaches to strategic human capital planning around the development of generative AI. The study aims to inform strategic workforce planning and policy development within federal agencies and demonstrates how this approach can be replicated across other large employment institutions and labor markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09637v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William G. Resh, Yi Ming, Xinyao Xia, Michael Overton, Gul Nisa G\"urb\"uz, Brandon De Breuhl</dc:creator>
    </item>
    <item>
      <title>Advancing Education through Tutoring Systems: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2503.09748</link>
      <description>arXiv:2503.09748v1 Announce Type: new 
Abstract: This study systematically reviews the transformative role of Tutoring Systems, encompassing Intelligent Tutoring Systems (ITS) and Robot Tutoring Systems (RTS), in addressing global educational challenges through advanced technologies. As many students struggle with proficiency in core academic areas, Tutoring Systems emerge as promising solutions to bridge learning gaps by delivering personalized and adaptive instruction. ITS leverages artificial intelligence (AI) models, such as Bayesian Knowledge Tracing and Large Language Models, to provide precise cognitive support, while RTS enhances social and emotional engagement through human-like interactions. This systematic review, adhering to the PRISMA framework, analyzed 86 representative studies. We evaluated the pedagogical and technological advancements, engagement strategies, and ethical considerations surrounding these systems. Based on these parameters, Latent Class Analysis was conducted and identified three distinct categories: computer-based ITS, robot-based RTS, and multimodal systems integrating various interaction modes. The findings reveal significant advancements in AI techniques that enhance adaptability, engagement, and learning outcomes. However, challenges such as ethical concerns, scalability issues, and gaps in cognitive adaptability persist. The study highlights the complementary strengths of ITS and RTS, proposing integrated hybrid solutions to maximize educational benefits. Future research should focus on bridging gaps in scalability, addressing ethical considerations comprehensively, and advancing AI models to support diverse educational needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09748v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Liu, Ehsan Latif, Xiaoming Zhai</dc:creator>
    </item>
    <item>
      <title>Honey Trap or Romantic Utopia: A Case Study of Final Fantasy XIV Players PII Disclosure in Intimate Partner-Seeking Posts</title>
      <link>https://arxiv.org/abs/2503.09832</link>
      <description>arXiv:2503.09832v1 Announce Type: new 
Abstract: Massively multiplayer online games (MMOGs) can foster social interaction and relationship formation, but they pose specific privacy and safety challenges, especially in the context of mediating intimate interpersonal connections. To explore the potential risks, we conducted a case study on Final Fantasy XIV (FFXIV) players intimate partner seeking posts on social media. We analyzed 1,288 posts from a public Weibo account using Latent Dirichlet Allocation (LDA) topic modeling and thematic analysis. Our findings reveal that players disclose sensitive personal information and share vulnerabilities to establish trust but face difficulties in managing identity and privacy across multiple platforms. We also found that players expectations regarding intimate partner are diversified, and mismatch of expectations may leads to issues like privacy leakage or emotional exploitation. Based on our findings, we propose design implications for reducing privacy and safety risks and fostering healthier social interactions in virtual worlds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09832v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3719831</arxiv:DOI>
      <dc:creator>Yihao Zhou, Tanusree Sharma</dc:creator>
    </item>
    <item>
      <title>IT Students Career Confidence and Career Identity During COVID-19</title>
      <link>https://arxiv.org/abs/2503.09882</link>
      <description>arXiv:2503.09882v1 Announce Type: new 
Abstract: COVID-19 disrupted the professional preparation of university students, with less opportunity to engage in professional practice due to a reduced employment market. Little is known about how this period impacted upon the career confidence and career identity of university students. This research paper explores the career confidence and identity of university students in Information Technology (IT) prior and during the COVID-19 period. Using a survey method and quantitative analysis, ANOVA and Kruskal-Wallis tests with different sensitivity and variance standards were used during analysis to present mean and mean rank of data collected during 2018, 2019, 2020 and 2021. 1349 IT students from an Australian University reported their career confidence. The results indicate IT students' career confidence maintained during the period. In 2021, the results indicate increased career commitment of IT students showing higher professional expectations to work in IT along with greater self-awareness regarding their professional development needs. Even with increased career confidence as observed in this study, supporting university students to explore their career options and build upon their career identity, and more broadly their employability, remains an important activity for universities to curate in their graduates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09882v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sophie McKenzie</dc:creator>
    </item>
    <item>
      <title>Sovereignty in the digital era: the quest for continuous access to dependable technological capabilities</title>
      <link>https://arxiv.org/abs/2503.10140</link>
      <description>arXiv:2503.10140v1 Announce Type: new 
Abstract: In an era where economies and societies are deeply integrated into cyberspace, achieving a robust level of digital sovereignty has become an essential goal for nations aiming to preserve their security and strategic political autonomy, particularly during turbulent geopolitical times marked by complex global supply chains of critical technologies that ties systemic rivals. Digital sovereignty is a multifaceted, interdisciplinary, and dynamic pursuit that fundamentally relies on a nation's ability to have continuous access to dependable technological capabilities (CTCs) for storing, transferring, and processing domestically produced data. This paper identifies how access continuity or technological dependability could be threatened by several malicious actions from cyberattacks, supply chain tamperings, political or economic actions. By examining different approaches adopted by countries like the United States, China, and the European Union, we highlight different strategies to get access to CTCs depending on their political, economic and institutional nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10140v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/MSEC.2024.3500192</arxiv:DOI>
      <arxiv:journal_reference>IEEE Secur. Priv. 23(1): 91-96 (2025)</arxiv:journal_reference>
      <dc:creator>Roberto Baldoni, Giuseppe Di Luna</dc:creator>
    </item>
    <item>
      <title>Social Media Harm Abatement: Mechanisms for Transparent Public Health Assessment</title>
      <link>https://arxiv.org/abs/2503.10458</link>
      <description>arXiv:2503.10458v1 Announce Type: new 
Abstract: Social media platforms have been accused of causing a range of harms, resulting in dozens of lawsuits across jurisdictions. These lawsuits are situated within the context of a long history of American product safety litigation, suggesting opportunities for remediation outside of financial compensation. Anticipating that at least some of these cases may be successful and/or lead to settlements, this article outlines an implementable mechanism for an abatement and/or settlement plan capable of mitigating abuse. The paper describes the requirements of such a mechanism, implications for privacy and oversight, and tradeoffs that such a procedure would entail. The mechanism is framed to operate at the intersection of legal procedure, standards for transparent public health assessment, and the practical requirements of modern technology products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10458v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Lubin, Yuning Liu, Amanda Yarnell, S. Bryn Austin, Zachary J. Ward, Ravi Iyer, Jonathan Stray, Matthew Lawrence, Alissa Cooper, Peter Chapman</dc:creator>
    </item>
    <item>
      <title>Short-term AI literacy intervention does not reduce over-reliance on incorrect ChatGPT recommendations</title>
      <link>https://arxiv.org/abs/2503.10556</link>
      <description>arXiv:2503.10556v1 Announce Type: new 
Abstract: In this study, we examined whether a short-form AI literacy intervention could reduce the adoption of incorrect recommendations from large language models. High school seniors were randomly assigned to either a control or an intervention group, which received an educational text explaining ChatGPT's working mechanism, limitations, and proper use. Participants solved math puzzles with the help of ChatGPT's recommendations, which were incorrect in half of the cases. Results showed that students adopted incorrect suggestions 52.1% of the time, indicating widespread over-reliance. The educational intervention did not significantly reduce over-reliance. Instead, it led to an increase in ignoring ChatGPT's correct recommendations. We conclude that the usage of ChatGPT is associated with over-reliance and it is not trivial to increase AI literacy to counter over-reliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10556v1</guid>
      <category>cs.CY</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Brett Puppart, Jaan Aru</dc:creator>
    </item>
    <item>
      <title>Evaluating the Effectiveness of XAI Techniques for Encoder-Based Language Models</title>
      <link>https://arxiv.org/abs/2501.15374</link>
      <description>arXiv:2501.15374v1 Announce Type: cross 
Abstract: The black-box nature of large language models (LLMs) necessitates the development of eXplainable AI (XAI) techniques for transparency and trustworthiness. However, evaluating these techniques remains a challenge. This study presents a general evaluation framework using four key metrics: Human-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. We assess the effectiveness of six explainability techniques from five different XAI categories model simplification (LIME), perturbation-based methods (SHAP), gradient-based approaches (InputXGradient, Grad-CAM), Layer-wise Relevance Propagation (LRP), and attention mechanisms-based explainability methods (Attention Mechanism Visualization, AMV) across five encoder-based language models: TinyBERT, BERTbase, BERTlarge, XLM-R large, and DeBERTa-xlarge, using the IMDB Movie Reviews and Tweet Sentiment Extraction (TSE) datasets. Our findings show that the model simplification-based XAI method (LIME) consistently outperforms across multiple metrics and models, significantly excelling in HA with a score of 0.9685 on DeBERTa-xlarge, robustness, and consistency as the complexity of large language models increases. AMV demonstrates the best Robustness, with scores as low as 0.0020. It also excels in Consistency, achieving near-perfect scores of 0.9999 across all models. Regarding Contrastivity, LRP performs the best, particularly on more complex models, with scores up to 0.9371.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15374v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.knosys.2025.113042</arxiv:DOI>
      <arxiv:journal_reference>310(2025)113042</arxiv:journal_reference>
      <dc:creator>Melkamu Abay Mersha, Mesay Gemeda Yigezu, Jugal Kalita</dc:creator>
    </item>
    <item>
      <title>Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy</title>
      <link>https://arxiv.org/abs/2503.09639</link>
      <description>arXiv:2503.09639v1 Announce Type: cross 
Abstract: Can we simulate a sandbox society with generative agents to model human behavior, thereby reducing the over-reliance on real human trials for assessing public policies? In this work, we investigate the feasibility of simulating health-related decision-making, using vaccine hesitancy, defined as the delay in acceptance or refusal of vaccines despite the availability of vaccination services (MacDonald, 2015), as a case study. To this end, we introduce the VacSim framework with 100 generative agents powered by Large Language Models (LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1) instantiate a population of agents with demographics based on census data; 2) connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) design and evaluate various public health interventions aimed at mitigating vaccine hesitancy. To align with real-world results, we also introduce simulation warmup and attitude modulation to adjust agents' attitudes. We propose a series of evaluations to assess the reliability of various LLM simulations. Experiments indicate that models like Llama and Qwen can simulate aspects of human behavior but also highlight real-world alignment challenges, such as inconsistent responses with demographic profiles. This early exploration of LLM-driven simulations is not meant to serve as definitive policy guidance; instead, it serves as a call for action to examine social simulation for policy development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09639v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abe Bohan Hou, Hongru Du, Yichen Wang, Jingyu Zhang, Zixiao Wang, Paul Pu Liang, Daniel Khashabi, Lauren Gardner, Tianxing He</dc:creator>
    </item>
    <item>
      <title>A Survey on Trustworthy LLM Agents: Threats and Countermeasures</title>
      <link>https://arxiv.org/abs/2503.09648</link>
      <description>arXiv:2503.09648v1 Announce Type: cross 
Abstract: With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. This evolution stems from empowering LLMs with additional modules such as memory, tools, environment, and even other agents. However, this advancement has also introduced more complex issues of trustworthiness, which previous research focused solely on LLMs could not cover. In this survey, we propose the TrustAgent framework, a comprehensive study on the trustworthiness of agents, characterized by modular taxonomy, multi-dimensional connotations, and technical implementation. By thoroughly investigating and summarizing newly emerged attacks, defenses, and evaluation methods for agents and MAS, we extend the concept of Trustworthy LLM to the emerging paradigm of Trustworthy Agent. In TrustAgent, we begin by deconstructing and introducing various components of the Agent and MAS. Then, we categorize their trustworthiness into intrinsic (brain, memory, and tool) and extrinsic (user, agent, and environment) aspects. Subsequently, we delineate the multifaceted meanings of trustworthiness and elaborate on the implementation techniques of existing research related to these internal and external modules. Finally, we present our insights and outlook on this domain, aiming to provide guidance for future endeavors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09648v1</guid>
      <category>cs.MA</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang, Junyuan Mao, Linsey Pang, Tianlong Chen, Kun Wang, Xinfeng Li, Yongfeng Zhang, Bo An, Qingsong Wen</dc:creator>
    </item>
    <item>
      <title>Data Traceability for Privacy Alignment</title>
      <link>https://arxiv.org/abs/2503.09823</link>
      <description>arXiv:2503.09823v1 Announce Type: cross 
Abstract: This paper offers a new privacy approach for the growing ecosystem of services--ranging from open banking to healthcare--dependent on sensitive personal data sharing between individuals and third-parties. While these services offer significant benefits, individuals want control over their data, transparency regarding how their data is used, and accountability from third-parties for misuse. However, existing legal and technical mechanisms are inadequate for supporting these needs. A comprehensive approach to the modern privacy challenges of accountable third-party data sharing requires a closer alignment of technical system architecture and legal institutional design. In order to achieve this privacy alignment, we extend traditional security threat modeling and analysis to encompass a broader range of privacy notions than has been typically considered. In particular, we introduce the concept of covert-accountability, which addresses adversaries that may act dishonestly but face potential identification and legal consequences. As a concrete instance of this design approach, we present the OTrace protocol, designed to provide traceable, accountable, consumer-control in third-party data sharing ecosystems. OTrace empowers consumers with the knowledge of where their data is, who has it, what it is being used for, and whom it is being shared with. By applying our alignment framework to OTrace, we demonstrate that OTrace's technical affordances can provide more confident, scalable regulatory oversight when combined with complementary legal mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09823v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Liao, Shreya Thipireddy, Daniel Weitzner</dc:creator>
    </item>
    <item>
      <title>Towards an Inclusive Digital Society: Digital Accessibility Framework for Visually Impaired Citizens in Swiss Public Administration</title>
      <link>https://arxiv.org/abs/2503.09824</link>
      <description>arXiv:2503.09824v1 Announce Type: cross 
Abstract: As we progress toward Society 5.0's vision of a human-centered digital society, ensuring digital accessibility becomes increasingly critical, particularly for citizens with visual impairments and other disabilities. This paper examines the implementation challenges of accessible digital public services within Swiss public administration. Through Design Science Research, we investigate the gap between accessibility legislation and practical implementation, analyzing how current standards translate into real-world usability. Our research reveals significant barriers including resource constraints, fragmented policy enforcement, and limited technical expertise. To address these challenges, we present the Inclusive Public Administration Framework, which integrates Web Content Accessibility Guidelines with the HERMES project management methodology. This framework provides a structured approach to embedding accessibility considerations throughout digital service development. Our findings contribute to the discourse on digital inclusion in Society 5.0 by providing actionable strategies for implementing accessible public services. As we move towards a more integrated human-machine society, ensuring digital accessibility for visually impaired citizens is crucial for building an equitable and inclusive digital future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09824v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabina Werren, Hermann Grieder, Christopher Scherb</dc:creator>
    </item>
    <item>
      <title>Using Causal Inference to Explore Government Policy Impact on Computer Usage</title>
      <link>https://arxiv.org/abs/2503.09957</link>
      <description>arXiv:2503.09957v1 Announce Type: cross 
Abstract: We explore the causal relationship between COVID-19 lockdown policies and changes in personal computer usage. In particular, we examine how lockdown policies affected average daily computer usage, as well as how it affected usage patterns of different groups of users. This is done through a merging of the Oxford Policy public data set, which describes the timeline of implementation of COVID policies across the world, and a collection of Intel's Data Collection and Analytics (DCA) telemetry data, which includes millions of computer usage records and updates daily. Through difference-in-difference, synthetic control, and change-point detection algorithms, we identify causal links between the increase in intensity (watts) and time (hours) of computer usage and the implementation of work from home policy. We also show an interesting trend in the individual's computer usage affected by the policy. We also conclude that computer usage behaviors are much less predictable during reduction in COVID lockdown policies than during increases in COVID lockdown policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09957v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingjia Zhu, Lechuan Wang, Julien Sebot, Bijan Arbab, Babak Salimi, Alexander Cloninger</dc:creator>
    </item>
    <item>
      <title>Understanding and Supporting Peer Review Using AI-reframed Positive Summary</title>
      <link>https://arxiv.org/abs/2503.10264</link>
      <description>arXiv:2503.10264v1 Announce Type: cross 
Abstract: While peer review enhances writing and research quality, harsh feedback can frustrate and demotivate authors. Hence, it is essential to explore how critiques should be delivered to motivate authors and enable them to keep iterating their work. In this study, we explored the impact of appending an automatically generated positive summary to the peer reviews of a writing task, alongside varying levels of overall evaluations (high vs. low), on authors' feedback reception, revision outcomes, and motivation to revise. Through a 2x2 online experiment with 137 participants, we found that adding an AI-reframed positive summary to otherwise harsh feedback increased authors' critique acceptance, whereas low overall evaluations of their work led to increased revision efforts. We discuss the implications of using AI in peer feedback, focusing on how AI-driven critiques can influence critique acceptance and support research communities in fostering productive and friendly peer feedback practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10264v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chi-Lan Yang, Alarith Uhde, Naomi Yamashita, Hideaki Kuzuoka</dc:creator>
    </item>
    <item>
      <title>The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory</title>
      <link>https://arxiv.org/abs/2503.10533</link>
      <description>arXiv:2503.10533v1 Announce Type: cross 
Abstract: High-quality test items are essential for educational assessments, particularly within Item Response Theory (IRT). Traditional validation methods rely on resource-intensive pilot testing to estimate item difficulty and discrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a domain-general approach for evaluating test items based on textual features. However, their relationship to IRT parameters remains underexplored. To address this gap, we conducted a study involving over 7,000 multiple-choice questions across various STEM subjects (e.g., math and biology). Using an automated approach, we annotated each question with a 19-criteria IWF rubric and studied relationships to data-driven IRT parameters. Our analysis revealed statistically significant links between the number of IWFs and IRT difficulty and discrimination parameters, particularly in life and physical science domains. We further observed how specific IWF criteria can impact item quality more and less severely (e.g., negative wording vs. implausible distractors). Overall, while IWFs are useful for predicting IRT parameters--particularly for screening low-difficulty MCQs--they cannot replace traditional data-driven validation methods. Our findings highlight the need for further research on domain-general evaluation rubrics and algorithms that understand domain-specific content for robust item validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10533v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robin Schmucker, Steven Moore</dc:creator>
    </item>
    <item>
      <title>Older adults' safety and security online: A post-pandemic exploration of attitudes and behaviors</title>
      <link>https://arxiv.org/abs/2403.09208</link>
      <description>arXiv:2403.09208v2 Announce Type: replace 
Abstract: Older adults' growing use of the internet and related technologies, further accelerated by the COVID-19 pandemic, has prompted not only a critical examination of their behaviors and attitudes about online threats but also a greater understanding of the roles of specific characteristics within this population group. Based on survey data and using descriptive and inferential statistics, this empirical study delves into this matter. The behaviors and attitudes of a group of older adults aged 60 years and older (n=275) regarding different dimensions of online safety and cybersecurity are investigated. The results show that older adults report a discernible degree of concern about the security of their personal information. Despite the varied precautions taken, most of them do not know where to report online threats. What is more, regarding key demographics, the study found some significant differences in terms of gender and age group, but not disability status. This implies that older adults do not seem to constitute a homogeneous group when it comes to attitudes and behaviors regarding safety and security online. The study concludes that support systems should include older adults in the development of protective measures and acknowledge their diversity. The implications of the results are discussed and some directions for future research are proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09208v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.34624/jdmi.v7i17.37825</arxiv:DOI>
      <arxiv:journal_reference>2024 Journal of Digital Media &amp; Interaction</arxiv:journal_reference>
      <dc:creator>Edgar Pacheco</dc:creator>
    </item>
    <item>
      <title>Revealing and Reducing Gender Biases in Vision and Language Assistants (VLAs)</title>
      <link>https://arxiv.org/abs/2410.19314</link>
      <description>arXiv:2410.19314v2 Announce Type: replace 
Abstract: Pre-trained large language models (LLMs) have been reliably integrated with visual input for multimodal tasks. The widespread adoption of instruction-tuned image-to-text vision-language assistants (VLAs) like LLaVA and InternVL necessitates evaluating gender biases. We study gender bias in 22 popular open-source VLAs with respect to personality traits, skills, and occupations. Our results show that VLAs replicate human biases likely present in the data, such as real-world occupational imbalances. Similarly, they tend to attribute more skills and positive personality traits to women than to men, and we see a consistent tendency to associate negative personality traits with men. To eliminate the gender bias in these models, we find that fine-tuning-based debiasing methods achieve the best trade-off between debiasing and retaining performance on downstream tasks. We argue for pre-deploying gender bias assessment in VLAs and motivate further development of debiasing strategies to ensure equitable societal outcomes. Code is available at https://github.com/ExplainableML/vla-gender-bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19314v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leander Girrbach, Stephan Alaniz, Yiran Huang, Trevor Darrell, Zeynep Akata</dc:creator>
    </item>
    <item>
      <title>Exploring near-optimal energy systems with stakeholders: a novel approach for participatory modelling</title>
      <link>https://arxiv.org/abs/2501.05280</link>
      <description>arXiv:2501.05280v2 Announce Type: replace 
Abstract: Involving people in energy systems planning can increase the legitimacy and socio-political feasibility of energy transitions. Participatory research in energy modelling offers the opportunity to engage with stakeholders in a comprehensive way, but is limited by how results can be generated and presented without imposing assumptions and discrete scenarios on the participants. To this end, we present a methodology and a framework, based on near-optimal modelling results, that can incorporate stakeholders in a holistic and engaging way. We confront stakeholders with a continuum of modelling-based energy system designs via an interactive interface allowing them to choose essentially any combination of components that meet the system requirements. Together with information on the implications of different technologies, it is possible to assess how participants prioritise different aspects in energy systems planning while also facilitating learning in an engaging and stimulating way. We showcase the methodology for the remote Arctic settlement of Longyearbyen and illustrate how participants deviate consistently from the cost optimum. At the same time, they manage to balance different priorities such as emissions, costs, and system vulnerability leading to a better understanding of the complexity and intertwined nature of decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05280v2</guid>
      <category>cs.CY</category>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oskar V{\aa}ger\"o, Koen van Greevenbroek, Aleksander Grochowicz, Maximilian Roithner</dc:creator>
    </item>
    <item>
      <title>The erasure of intensive livestock farming in text-to-image generative AI</title>
      <link>https://arxiv.org/abs/2502.19771</link>
      <description>arXiv:2502.19771v2 Announce Type: replace 
Abstract: Generative AI (e.g., ChatGPT) is increasingly integrated into people's daily lives. While it is known that AI perpetuates biases against marginalized human groups, their impact on non-human animals remains understudied. We found that ChatGPT's text-to-image model (DALL-E 3) introduces a strong bias toward romanticizing livestock farming as dairy cows on pasture and pigs rooting in mud. This bias remained when we requested realistic depictions and was only mitigated when the automatic prompt revision was inhibited. Most farmed animal in industrialized countries are reared indoors with limited space per animal, which fail to resonate with societal values. Inhibiting prompt revision resulted in images that more closely reflected modern farming practices; for example, cows housed indoors accessing feed through metal headlocks, and pigs behind metal railings on concrete floors in indoor facilities. While OpenAI introduced prompt revision to mitigate bias, in the case of farmed animal production systems, it paradoxically introduces a strong bias towards unrealistic farming practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19771v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kehan Sheng, Frank A. M. Tuyttens, Marina A. G. von Keyserlingk</dc:creator>
    </item>
    <item>
      <title>AGI, Governments, and Free Societies</title>
      <link>https://arxiv.org/abs/2503.05710</link>
      <description>arXiv:2503.05710v2 Announce Type: replace 
Abstract: This paper examines how artificial general intelligence (AGI) could fundamentally reshape the delicate balance between state capacity and individual liberty that sustains free societies. Building on Acemoglu and Robinson's 'narrow corridor' framework, we argue that AGI poses distinct risks of pushing societies toward either a 'despotic Leviathan' through enhanced state surveillance and control, or an 'absent Leviathan' through the erosion of state legitimacy relative to AGI-empowered non-state actors. Drawing on public administration theory and recent advances in AI capabilities, we analyze how these dynamics could unfold through three key channels: the automation of discretionary decision-making within agencies, the evolution of bureaucratic structures toward system-level architectures, and the transformation of democratic feedback mechanisms. Our analysis reveals specific failure modes that could destabilize liberal institutions. Enhanced state capacity through AGI could enable unprecedented surveillance and control, potentially entrenching authoritarian practices. Conversely, rapid diffusion of AGI capabilities to non-state actors could undermine state legitimacy and governability. We examine how these risks manifest differently at the micro level of individual bureaucratic decisions, the meso level of organizational structure, and the macro level of democratic processes. To preserve the narrow corridor of liberty, we propose a governance framework emphasizing robust technical safeguards, hybrid institutional designs that maintain meaningful human oversight, and adaptive regulatory mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05710v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Justin B. Bullock, Samuel Hammond, Seb Krier</dc:creator>
    </item>
    <item>
      <title>Securing External Deeper-than-black-box GPAI Evaluations</title>
      <link>https://arxiv.org/abs/2503.07496</link>
      <description>arXiv:2503.07496v2 Announce Type: replace 
Abstract: This paper examines the critical challenges and potential solutions for conducting secure and effective external evaluations of general-purpose AI (GPAI) models. With the exponential growth in size, capability, reach and accompanying risk of these models, ensuring accountability, safety, and public trust requires frameworks that go beyond traditional black-box methods. The discussion begins with an analysis of the need for deeper-than-black-box evaluations (Section I), emphasizing the importance of understanding model internals to uncover latent risks and ensure compliance with ethical and regulatory standards. Building on this foundation, Section II addresses the security considerations of remote evaluations, outlining the threat landscape, technical solutions, and safeguards necessary to protect both evaluators and proprietary model data. Finally, Section III synthesizes these insights into actionable recommendations and future directions, aiming to establish a robust, scalable, and transparent framework for external assessments in GPAI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07496v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Tlaie, Jimmy Farrell</dc:creator>
    </item>
    <item>
      <title>The Algorithmic State Architecture (ASA): An Integrated Framework for AI-Enabled Government</title>
      <link>https://arxiv.org/abs/2503.08725</link>
      <description>arXiv:2503.08725v2 Announce Type: replace 
Abstract: As artificial intelligence transforms public sector operations, governments struggle to integrate technological innovations into coherent systems for effective service delivery. This paper introduces the Algorithmic State Architecture (ASA), a novel four-layer framework conceptualising how Digital Public Infrastructure, Data-for-Policy, Algorithmic Government/Governance, and GovTech interact as an integrated system in AI-enabled states. Unlike approaches that treat these as parallel developments, ASA positions them as interdependent layers with specific enabling relationships and feedback mechanisms. Through comparative analysis of implementations in Estonia, Singapore, India, and the UK, we demonstrate how foundational digital infrastructure enables systematic data collection, which powers algorithmic decision-making processes, ultimately manifesting in user-facing services. Our analysis reveals that successful implementations require balanced development across all layers, with particular attention to integration mechanisms between them. The framework contributes to both theory and practice by bridging previously disconnected domains of digital government research, identifying critical dependencies that influence implementation success, and providing a structured approach for analysing the maturity and development pathways of AI-enabled government systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08725v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zeynep Engin, Jon Crowcroft, David Hand, Philip Treleaven</dc:creator>
    </item>
    <item>
      <title>Neural embedding of beliefs reveals the role of relative dissonance in human decision-making</title>
      <link>https://arxiv.org/abs/2408.07237</link>
      <description>arXiv:2408.07237v2 Announce Type: replace-cross 
Abstract: Beliefs form the foundation of human cognition and decision-making, guiding our actions and social connections. A model encapsulating beliefs and their interrelationships is crucial for understanding their influence on our actions. However, research on belief interplay has often been limited to beliefs related to specific issues and relied heavily on surveys. We propose a method to study the nuanced interplay between thousands of beliefs by leveraging an online user debate data and mapping beliefs onto a neural embedding space constructed using a fine-tuned large language model (LLM). This belief space captures the interconnectedness and polarization of diverse beliefs across social issues. Our findings show that positions within this belief space predict new beliefs of individuals and estimate cognitive dissonance based on the distance between existing and new beliefs. This study demonstrates how LLMs, combined with collective online records of human beliefs, can offer insights into the fundamental principles that govern human decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07237v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Byunghwee Lee, Rachith Aiyappa, Yong-Yeol Ahn, Haewoon Kwak, Jisun An</dc:creator>
    </item>
  </channel>
</rss>
