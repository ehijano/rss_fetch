<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Aug 2025 01:41:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Individualized Treatment Effects in Advanced Prostate Cancer: A Causal-Survival Modeling Approach to Risk-Guided Therapy</title>
      <link>https://arxiv.org/abs/2508.16894</link>
      <description>arXiv:2508.16894v1 Announce Type: new 
Abstract: We conducted a proof-of-concept evaluation of individualized treatment effect (ITE) estimation using survival data from a randomized trial of 475 men with advanced prostate cancer treated with high- versus low-dose diethylstilbestrol (DES). A Weibull accelerated failure time (AFT) model with interaction terms for treatment-by-age and treatment-by-log tumor size was used to capture subgroup-specific treatment effects. The estimated main effect of high-dose DES indicated a time ratio of 0.582 (95% CI: [0.306, 1.110]), reflecting reduced survival at the reference levels of age and tumor size. However, interaction-adjusted ITEs revealed marked effect modification: younger patients (e.g., age 50 years) had over fourfold expected survival gains (time ratio 4.09), whereas older patients (e.g., age 80 years) experienced reduced benefit (time ratio 0.71). Similarly, patients with larger tumors (log size $\sim$4.25, $\sim$70 $cm^2$) derived a stronger benefit (time ratio 1.89) than those with smaller tumors. To evaluate the reliability of these individualized estimates, both the delta method and bootstrap resampling were applied for uncertainty quantification, producing closely aligned intervals across the risk spectrum. This analysis illustrates how parametric survival models with clinically motivated interactions and robust inference procedures can yield interpretable patient-level treatment effect estimates, even in moderately sized oncology trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16894v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>J. T. Korley</dc:creator>
    </item>
    <item>
      <title>On the relationship between the Wasserstein distance and differences in life expectancy at birth</title>
      <link>https://arxiv.org/abs/2508.17235</link>
      <description>arXiv:2508.17235v1 Announce Type: new 
Abstract: The Wasserstein distance is a metric for assessing distributional differences. The measure originates in optimal transport theory and can be interpreted as the minimal cost of transforming one distribution into another. In this paper, the Wasserstein distance is applied to life table age-at-death distributions. The main finding is that, under certain conditions, the Wasserstein distance between two age-at-death distributions equals the corresponding gap in life expectancy at birth ($e_0$). More specifically, the paper shows mathematically and empirically that this equivalence holds whenever the survivorship functions do not cross. For example, this applies when comparing mortality between women and men from 1990 to 2020 using data from the Human Mortality Database. In such cases, the gap in $e_0$ reflects not only a difference in mean ages at death but can also be interpreted directly as a measure of distributional difference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17235v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Sauerberg</dc:creator>
    </item>
    <item>
      <title>Gaussian Process Modeling with Genotype x Environment Kernels for Wheat Performance Prediction</title>
      <link>https://arxiv.org/abs/2508.17730</link>
      <description>arXiv:2508.17730v1 Announce Type: new 
Abstract: Optimizing wheat variety selection for high performance in different environmental conditions is critical for reliable food production and stable incomes for growers. We employ a statistical machine learning framework utilizing Gaussian Process (GP) models to capture the effects of genetic and environmental factors on wheat yield and protein content. In doing so, selecting suitable covariance kernels to account for the distinct characteristics of the information is essential. The GP approach is closely related to linear mixed-effect models for genotype x environment predictions, where random additive and interaction effects are modeled with covariance structures. However, while commonly used linear mixed effect models in plant breeding rely on Euclidean-based kernels, we also test kernels specifically designed for strings and time series. The resulting GP models are capable of competitively predicting outcomes for (1) new environmental conditions, and (2) new varieties, even in scenarios with little to no previous data for the new conditions or variety. While we focus on a wheat test case using a novel dataset collected in Switzerland, the GP approach presented here can be applied and extended to a wide range of agricultural applications and beyond, paving the way for improved decision-making and data acquisition strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17730v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lea Friedli, Tim Steinert, Nathalie Wuyts, Fabian Guignard, Lilia Levy H\"aner, Didier Pellet, Juan M. Herrera, David Ginsbourger</dc:creator>
    </item>
    <item>
      <title>Seeing Isn't Believing: Addressing the Societal Impact of Deepfakes in Low-Tech Environments</title>
      <link>https://arxiv.org/abs/2508.16618</link>
      <description>arXiv:2508.16618v1 Announce Type: cross 
Abstract: Deepfakes, AI-generated multimedia content that mimics real media, are becoming increasingly prevalent, posing significant risks to political stability, social trust, and economic well-being, especially in developing societies with limited media literacy and technological infrastructure. This work aims to understand how these technologies are perceived and impact resource-limited communities. We conducted a survey to assess public awareness, perceptions, and experiences with deepfakes, leading to the development of a comprehensive framework for prevention, detection, and mitigation in tech-limited environments. Our findings reveal critical knowledge gaps and a lack of effective detection tools, emphasizing the need for targeted education and accessible verification solutions. This work offers actionable insights to support vulnerable populations and calls for further interdisciplinary efforts to tackle deepfake challenges globally, particularly in the Global South.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16618v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.MM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Azmine Toushik Wasi, Rahatun Nesa Priti, Mahir Absar Khan, Abdur Rahman, Mst Rafia Islam</dc:creator>
    </item>
    <item>
      <title>Alternative statistical inference for the first normalized incomplete moment</title>
      <link>https://arxiv.org/abs/2508.17145</link>
      <description>arXiv:2508.17145v1 Announce Type: cross 
Abstract: This paper re-examines the first normalized incomplete moment, a well-established measure of inequality with wide applications in economic and social sciences. Despite the popularity of the measure itself, existing statistical inference appears to lag behind the needs of modern-age analytics. To fill this gap, we propose an alternative solution that is intuitive, computationally efficient, mathematically equivalent to the existing solutions for "standard" cases, and easily adaptable to "non-standard" ones. The theoretical and practical advantages of the proposed methodology are demonstrated via both simulated and real-life examples. In particular, we discover that a common practice in industry can lead to highly non-trivial challenges for trustworthy statistical inference, or misleading decision making altogether.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17145v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiannan Lu, Peng Ding, Anqi Zhao</dc:creator>
    </item>
    <item>
      <title>Unit-Modified Weibull Distribution and Quantile Regression Model</title>
      <link>https://arxiv.org/abs/2508.17359</link>
      <description>arXiv:2508.17359v1 Announce Type: cross 
Abstract: The Sustainable Development Goals (SDGs) of the United Nations consist of 17 general objectives, subdivided into 169 targets to be achieved by 2030. Several SDG indices and indicators require continuous analysis and evaluation, and most of these indices are supported in the unit interval (0,1). To incorporate the flexibility of the modified Weibull (MW) distribution in doubly constrained datasets, the first objective of this work is to propose a new unit probability distribution based on the MW distribution. For this, a transformation of the MW distribution is applied, through which the unit modified Weibull (UMW) distribution is obtained. The second objective of this work is to introduce a quantile regression model for random variables with UMW distribution, reparameterized in terms of the quantiles of the distribution. Maximum likelihood estimators (MLEs) are used to estimate the model parameters. Monte Carlo simulations are performed to evaluate the MLE properties of the model parameters in finite sample sizes. The introduced methods are used for modeling some sustainability indicators related to the SDGs, also considering the reading skills of dyslexic children, which are indirectly associated with SDG 4 (Quality Education) and SDG 3 (Health and Well-Being).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17359v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jo\~ao In\'acio Scrimini, Cleber Bisognin, Renata Rojas Guerra, F\'abio M. Bayer</dc:creator>
    </item>
    <item>
      <title>Mutual Information Surprise: Rethinking Unexpectedness in Autonomous Systems</title>
      <link>https://arxiv.org/abs/2508.17403</link>
      <description>arXiv:2508.17403v1 Announce Type: cross 
Abstract: Recent breakthroughs in autonomous experimentation have demonstrated remarkable physical capabilities, yet their cognitive control remains limited--often relying on static heuristics or classical optimization. A core limitation is the absence of a principled mechanism to detect and adapt to the unexpectedness. While traditional surprise measures--such as Shannon or Bayesian Surprise--offer momentary detection of deviation, they fail to capture whether a system is truly learning and adapting. In this work, we introduce Mutual Information Surprise (MIS), a new framework that redefines surprise not as anomaly detection, but as a signal of epistemic growth. MIS quantifies the impact of new observations on mutual information, enabling autonomous systems to reflect on their learning progression. We develop a statistical test sequence to detect meaningful shifts in estimated mutual information and propose a mutual information surprise reaction policy (MISRP) that dynamically governs system behavior through sampling adjustment and process forking. Empirical evaluations--on both synthetic domains and a dynamic pollution map estimation task--show that MISRP-governed strategies significantly outperform classical surprise-based approaches in stability, responsiveness, and predictive accuracy. By shifting surprise from reactive to reflective, MIS offers a path toward more self-aware and adaptive autonomous systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17403v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinsong Wang, Xiao Liu, Quan Zeng, Yu Ding</dc:creator>
    </item>
    <item>
      <title>Tracking Temporal Evolution of Topological Features in Image Data</title>
      <link>https://arxiv.org/abs/2508.17530</link>
      <description>arXiv:2508.17530v1 Announce Type: cross 
Abstract: Topological Data Analysis (TDA) can be used to detect and characterize holes in an image, such as zero-dimensional holes (connected components) or one-dimensional holes (loops). However, there is currently no widely accepted statistical framework for modeling spatiotemporal dependence in the evolution of topological features, such as holes, within a time series of images. We propose a hypothesis testing framework to identify statistically significant topological features of images in space and time, simultaneously. This addition of time may induce higher-dimensional topological features which can be used to establish temporal connections between the lower-dimensional features at each point in time. The temporal evolution of these lower-dimensional features is then represented on a zigzag persistence diagram, as a topological summary statistic focused on time dynamics. We demonstrate that the method effectively captures the emergence and progression of topological features in a study of a series of images of a wounded cell as it repairs. The proposed method outperforms a current approach in a simulation study that includes features of the wound healing process. Since, the wounded cell images exhibit nonlinear, dynamic, spatial, and temporal structures during single-cell repair, they provide a good application for this method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17530v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susan Glenn, Jessi Cisewski-Kehe, Jun Zhu, William M Bement</dc:creator>
    </item>
    <item>
      <title>An Efficient Image Denoising Method Integrating Multi-resolution Local Clustering and Adaptive Smoothing</title>
      <link>https://arxiv.org/abs/2407.20210</link>
      <description>arXiv:2407.20210v3 Announce Type: replace 
Abstract: The importance of developing efficient image denoising methods is immense especially for modern applications such as image comparisons, image monitoring, medical image diagnostics, and so forth. Available methods in the vast literature on image denoising can address certain issues in image denoising, but no one single method can solve all such issues. For example, jump regression based methods can preserve linear edges well, but cannot preserve many other fine details of an image. On the other hand, local clustering based methods can preserve fine edge structures, but cannot perform well in presence of heavy noise. The proposed method uses various shapes and sizes of local neighborhood based on local information, and integrates this adaptive approach with the local clustering based smoothing. Theoretical justifications and numerical studies show that the proposed method indeed performs better than these two individual methods and outperforms many other state-of-the-art techniques as well. Such performance demonstrates vast potential of the applicability of the proposed method in many modern-day applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20210v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10260-025-00806-z</arxiv:DOI>
      <arxiv:journal_reference>An efficient image denoising method integrating multi-resolution local clustering and adaptive smoothing. Stat Methods Appl (2025)</arxiv:journal_reference>
      <dc:creator>Subhasish Basak, Partha Sarathi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Forecasting Extreme Day and Night Heat in Paris</title>
      <link>https://arxiv.org/abs/2508.12886</link>
      <description>arXiv:2508.12886v2 Announce Type: replace 
Abstract: As a demonstration of concept, quantile gradient boosting is used to forecast diurnal and nocturnal Q(.90) air temperatures for Paris, France during late the spring and summer months of 2020. The data are provided by the Paris-Montsouris weather station. Q(.90) values are estimated because the 90th percentile requires that the temperatures be relatively rare and extreme. Predictors include seven routinely collected indicators of weather conditions, lagged by 14 days; the temperature forecasts are produced two weeks in advance. Conformal prediction regions capture forecasting uncertainty with provably valid properties. For both diurnal and nocturnal temperatures, forecasting accuracy is promising, and sound measures of uncertainty are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12886v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard Berk</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Longitudinal Data under Unknown Interference</title>
      <link>https://arxiv.org/abs/2106.15074</link>
      <description>arXiv:2106.15074v5 Announce Type: replace-cross 
Abstract: In longitudinal studies where units are embedded in space or a social network, interference may arise, meaning that a unit's outcome can depend on treatment histories of others. The presence of interference poses significant challenges for causal inference, particularly when the interference structure -- how a unit's outcome responds to others' influences -- is complex, heterogeneous, and unknown to researchers. This paper develops a general framework for identifying and estimating both direct and spillover effects of treatment histories under minimal assumptions about the interference structure. We introduce a class of causal estimands that capture the effects of treatment histories at any specified proximity level and show that they can be represented by a modified marginal structural model. Under sequential exchangeability, these estimands are identifiable and can be estimated using inverse probability weighting. We derive conditions for consistency and asymptotic normality of the estimators and provide procedures for constructing asymptotically conservative confidence intervals. The method's utility is demonstrated through applications in both social science and biomedical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.15074v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Wang, Michael Jetsupphasuk</dc:creator>
    </item>
    <item>
      <title>Flexible and Probabilistic Topology Tracking with Partial Optimal Transport</title>
      <link>https://arxiv.org/abs/2302.02895</link>
      <description>arXiv:2302.02895v4 Announce Type: replace-cross 
Abstract: In this paper, we present a flexible and probabilistic framework for tracking topological features in time-varying scalar fields using merge trees and partial optimal transport. Merge trees are topological descriptors that record the evolution of connected components in the sublevel sets of scalar fields. We present a new technique for modeling and comparing merge trees using tools from partial optimal transport. In particular, we model a merge tree as a measure network, that is, a network equipped with a probability distribution, and define a notion of distance on the space of merge trees inspired by partial optimal transport. Such a distance offers a new and flexible perspective for encoding intrinsic and extrinsic information in the comparative measures of merge trees. More importantly, it gives rise to a partial matching between topological features in time-varying data, thus enabling flexible topology tracking for scientific simulations. Furthermore, such partial matching may be interpreted as probabilistic coupling between features at adjacent time steps, which gives rise to probabilistic tracking graphs. We derive a stability result for our distance and provide numerous experiments indicating the efficacy of our framework in extracting meaningful feature tracks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.02895v4</guid>
      <category>cs.CG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2025.3561300</arxiv:DOI>
      <dc:creator>Mingzhe Li, Xinyuan Yan, Lin Yan, Tom Needham, Bei Wang</dc:creator>
    </item>
    <item>
      <title>Utilizing Multiple Testing for Grouping in Singular Spectrum Analysis</title>
      <link>https://arxiv.org/abs/2401.01665</link>
      <description>arXiv:2401.01665v3 Announce Type: replace-cross 
Abstract: A key step in separating signal from noise in time series by means of singular spectrum analysis (SSA) is grouping. We present a multiple testing method for the grouping step in SSA. As separability criterion, we utilize the weighted correlation between the signal and the noise component of the (reconstructed) time series, and we test whether this weighted correlation is equal to zero. This test has to be performed for several possible groupings, resulting in a multiple test problem. The null distributions of the corresponding test statistics are approximated by a wild bootstrap procedure. The performance of our proposed method is assessed in a simulation study, and we illustrate its practical application with an analysis of real world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01665v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maryam Movahedifar, Friederike Preusse, Anna Vesely, Thorsten Dickhaus</dc:creator>
    </item>
    <item>
      <title>Probabilistic Classification of Near-Surface Shallow-Water Sediments using A Portable Free-Fall Penetrometer</title>
      <link>https://arxiv.org/abs/2410.00225</link>
      <description>arXiv:2410.00225v2 Announce Type: replace-cross 
Abstract: The geotechnical evaluation of seabed sediments is important for engineering projects and naval applications, offering valuable insights into sediment properties, behavior, and strength. Obtaining high-quality seabed samples can be a challenging task, making in situ testing an essential part of site characterization. Free-fall penetrometers (FFPs) are robust tools for rapidly profiling seabed surface sediments, even in energetic nearshore or estuarine conditions and shallow as well as deep depths. Although methods for interpretation of traditional offshore cone penetration testing (CPT) data are well-established, their adaptation to FFP data is still an area of research. This study introduces an innovative approach that utilizes machine learning algorithms to create a sediment behavior classification system based on portable free- fall penetrometer (PFFP) data. The proposed model leverages PFFP measurements obtained from multiple locations, such as Sequim Bay (Washington), the Potomac River, and the York River (Virginia). The results show 91.1% accuracy in the class prediction, with the classes representing cohesionless sediment with little to no plasticity (Class 1), cohesionless sediment with some plasticity (Class 2), cohesive sediment with low plasticity (Class 3), and cohesive sediment with high plasticity (Class 4). The model prediction not only predicts classes but also yields an estimate of inherent uncertainty associated with the prediction, which can provide valuable insight into different sediment behaviors. Lower uncertainties are more common, but they can increase significantly depending on variations in sediment composition, environmental conditions, and operational techniques. By quantifying uncertainty, the model offers a more comprehensive and informed approach to sediment classification</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00225v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1061/JGGEFK.GTENG-13486</arxiv:DOI>
      <arxiv:journal_reference>journal = {Journal of Geotechnical and Geoenvironmental Engineering}, volume = {151}, number = {11}, pages = {04025128}, year = {2025}</arxiv:journal_reference>
      <dc:creator>Md Rejwanur Rahman, Adrian Rodriguez-Marek, Nina Stark, Grace Massey, Carl Friedrichs, Kelly M. Dorgan</dc:creator>
    </item>
    <item>
      <title>Nonlocal Prior Mixture-Based Bayesian Wavelet Regression with Application to Noisy Imaging and Audio Data</title>
      <link>https://arxiv.org/abs/2501.18134</link>
      <description>arXiv:2501.18134v3 Announce Type: replace-cross 
Abstract: We propose a novel Bayesian wavelet regression approach using a three-component spike-and-slab prior for wavelet coefficients, combining a point mass at zero, a moment (MOM) prior, and an inverse moment (IMOM) prior. This flexible prior supports small and large coefficients differently, offering advantages for highly dispersed data where wavelet coefficients span multiple scales. The IMOM prior's heavy tails capture large coefficients, while the MOM prior is better suited for smaller non-zero coefficients. Further, our method introduces innovative hyperparameter specifications for mixture probabilities and scale parameters, including generalized logit, hyperbolic secant, and generalized normal decay for probabilities, and double exponential decay for scaling. Hyperparameters are estimated via an empirical Bayes approach, enabling posterior inference tailored to the data. Extensive simulations demonstrate significant performance gains over two-component wavelet methods. Applications to electroencephalography and noisy audio data illustrate the method's utility in capturing complex signal characteristics. We implement our method in an R package, NLPwavelet (&gt;= 1.1).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18134v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/math13162642</arxiv:DOI>
      <arxiv:journal_reference>Sanyal, N. (2025). Nonlocal Prior Mixture-Based Bayesian Wavelet Regression with Application to Noisy Imaging and Audio Data. Mathematics, 13(16), 2642</arxiv:journal_reference>
      <dc:creator>Nilotpal Sanyal</dc:creator>
    </item>
    <item>
      <title>Neural Posterior Estimation for Cataloging Astronomical Images with Spatially Varying Backgrounds and Point Spread Functions</title>
      <link>https://arxiv.org/abs/2503.00156</link>
      <description>arXiv:2503.00156v2 Announce Type: replace-cross 
Abstract: Neural posterior estimation (NPE), a type of amortized variational inference, is a computationally efficient means of constructing probabilistic catalogs of light sources from astronomical images. To date, NPE has not been used to perform inference in models with spatially varying covariates. However, ground-based astronomical images have spatially varying sky backgrounds and point spread functions (PSFs), and accounting for this variation is essential for constructing accurate catalogs of imaged light sources. In this work, we introduce a method of performing NPE with spatially varying backgrounds and PSFs. In this method, we generate synthetic catalogs and semi-synthetic images for these catalogs using randomly sampled PSF and background estimates from existing surveys. Using this data, we train a neural network, which takes an astronomical image and representations of its background and PSF as input, to output a probabilistic catalog. Our experiments with Sloan Digital Sky Survey data demonstrate the effectiveness of NPE in the presence of spatially varying backgrounds and PSFs for light source detection, star/galaxy separation, and flux measurement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00156v2</guid>
      <category>astro-ph.IM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3847/1538-3881/adef32</arxiv:DOI>
      <dc:creator>Aakash Patel, Tianqing Zhang, Camille Avestruz, Jeffrey Regier, the LSST Dark Energy Science Collaboration</dc:creator>
    </item>
    <item>
      <title>An Approximate Maximum Likelihood Estimator for Discretely Observed Linear Birth-and-Death Processes</title>
      <link>https://arxiv.org/abs/2508.07527</link>
      <description>arXiv:2508.07527v2 Announce Type: replace-cross 
Abstract: Linear birth-and-death processes (LBDPs) are foundational stochastic models in population dynamics, evolutionary biology, and hematopoiesis. Estimating parameters from discretely observed data is computationally demanding due to irregular sampling, noise, and missing values. We propose a novel approximate maximum likelihood estimator (MLE) for LBDPs based on a Gaussian approximation to transition probabilities. The approach transforms estimation into a univariate optimization problem, achieving substantial computational gains without sacrificing accuracy.
  Through simulations, we show that the approximate MLE outperforms Gaussian and saddlepoint-based estimators in speed and precision under realistic noise and sparsity. Applied to longitudinal clonal hematopoiesis data, the method produces biologically meaningful growth estimates even with noisy, compositional input. Unlike Gaussian and saddlepoint approximations, our estimator is invariant to data scaling, making it ideal for real-world applications such as variant allele frequency analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07527v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaochen Long, Marek Kimmel</dc:creator>
    </item>
  </channel>
</rss>
