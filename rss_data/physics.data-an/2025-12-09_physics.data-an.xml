<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Dec 2025 02:48:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Automating High Energy Physics Data Analysis with LLM-Powered Agents</title>
      <link>https://arxiv.org/abs/2512.07785</link>
      <description>arXiv:2512.07785v1 Announce Type: new 
Abstract: We present a proof-of-principle study demonstrating the use of large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. Using the Higgs boson diphoton cross-section measurement as a case study with ATLAS Open Data, we design a hybrid system that combines an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this architecture, the workflow manager enforces reproducibility and determinism, while the agent autonomously generates, executes, and iteratively corrects analysis code in response to user instructions. We define quantitative evaluation metrics including success rate, error distribution, costs per specific task, and average number of API calls, to assess agent performance across multi-stage workflows. To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. While the workflow manager ensures deterministic execution of all analysis steps, the final outputs still show stochastic variation. Although we set the temperature to zero, other sampling parameters (e.g., top-p, top-k) remained at their defaults, and some reasoning-oriented models internally adjust these settings. Consequently, the models do not produce fully deterministic results. This study establishes the first LLM-agent-driven automated data-analysis framework in HEP, enabling systematic benchmarking of model capabilities, stability, and limitations in real-world scientific computing environments. The baseline code used in this work is available at https://huggingface.co/HWresearch/LLM4HEP. This work was accepted as a poster at the Machine Learning and the Physical Sciences (ML4PS) workshop at NeurIPS 2025. The initial submission was made on August 30, 2025.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07785v1</guid>
      <category>physics.data-an</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Gendreau-Distler, Joshua Ho, Dongwon Kim, Luc Tomas Le Pottier, Haichen Wang, Chengxi Yang</dc:creator>
    </item>
    <item>
      <title>Quantifying the irregularity of a time series</title>
      <link>https://arxiv.org/abs/2512.05975</link>
      <description>arXiv:2512.05975v1 Announce Type: cross 
Abstract: We introduce circulance, a scalar measure for classifying time series of dynamical systems. Circulance captures the extent of temporal regularity or irregularity that is encoded in the topology of a directed ordinal pattern transition network derived from a time series. We demonstrate numerically that circulance sensitively and robustly positions time series of canonical model systems, representative of preset dynamical regimes, along a continuous spectrum from regularity to randomness. Analyzing empirical data from long-term observations of high-dimensional, complex systems -- human brain and the Sun -- reveals that circulance aids in elucidating different dynamical regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05975v1</guid>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Potratzki, Manuel Adams, Timo Br\"ohl, Klaus Lehnertz</dc:creator>
    </item>
    <item>
      <title>Generalized tension metrics for multiple cosmological datasets</title>
      <link>https://arxiv.org/abs/2512.06086</link>
      <description>arXiv:2512.06086v1 Announce Type: cross 
Abstract: We introduce a novel estimator to quantify statistical tensions among multiple cosmological datasets simultaneously. This estimator generalizes the Difference-in-Means statistic, $Q_{\rm DM}$, to the multi-dataset regime. Our framework enables the detection of dominant tension directions in the shared parameter space. It further provides a geometric interpretation of the tension for the two- and three-dataset cases in two dimensions. According to this approach, the previously reported increase in tension between DESI and Planck from $1.9\sigma$ (DR1) to $2.3\sigma$(DR2) is reinterpreted as a more modest shift from $1.18\sigma^{\rm eff}$ (DR1) to $1.45\sigma^{\rm eff}$ (DR2). These new tools may also prove valuable across research fields where dataset discrepancies arise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06086v1</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mat\'ias Leizerovich, Susana J. Landau, Claudia G. Sc\'occola</dc:creator>
    </item>
    <item>
      <title>Detrended cross-correlations and their random matrix limit: an example from the cryptocurrency market</title>
      <link>https://arxiv.org/abs/2512.06473</link>
      <description>arXiv:2512.06473v1 Announce Type: cross 
Abstract: Correlations in complex systems are often obscured by nonstationarity, long-range memory, and heavy-tailed fluctuations, which limit the usefulness of traditional covariance-based analyses. To address these challenges, we construct scale and fluctuation-dependent correlation matrices using the multifractal detrended cross-correlation coefficient $\rho_r$ that selectively emphasizes fluctuations of different amplitudes. We examine the spectral properties of these detrended correlation matrices and compare them to the spectral properties of the matrices calculated in the same way from synthetic Gaussian and $q$Gaussian signals. Our results show that detrending, heavy tails, and the fluctuation-order parameter $r$ jointly produce spectra, which substantially depart from the random case even under absence of cross-correlations in time series. Applying this framework to one-minute returns of 140 major cryptocurrencies from 2021-2024 reveals robust collective modes, including a dominant market factor and several sectoral components whose strength depends on the analyzed scale and fluctuation order. After filtering out the market mode, the empirical eigenvalue bulk aligns closely with the limit of random detrended cross-correlations, enabling clear identification of structurally significant outliers. Overall, the study provides a refined spectral baseline for detrended cross-correlations and offers a promising tool for distinguishing genuine interdependencies from noise in complex, nonstationary, heavy-tailed systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06473v1</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/e27121236</arxiv:DOI>
      <arxiv:journal_reference>Entropy 2025, 27(12), 1236</arxiv:journal_reference>
      <dc:creator>Stanis{\l}aw Dro\.zd\.z, Pawe{\l} Jarosz, Jaros{\l}aw Kwapie\'n, Maria Skupie\'n, Marcin W\k{a}torek</dc:creator>
    </item>
    <item>
      <title>Hierarchical geometric deep learning enables scalable analysis of molecular dynamics</title>
      <link>https://arxiv.org/abs/2512.06520</link>
      <description>arXiv:2512.06520v1 Announce Type: cross 
Abstract: Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06520v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihan Pengmei, Spencer C. Guo, Chatipat Lorpaiboon, Aaron R. Dinner</dc:creator>
    </item>
    <item>
      <title>Machine Learning-based Unfolding for Cross Section Measurements in the Presence of Nuisance Parameters</title>
      <link>https://arxiv.org/abs/2512.07074</link>
      <description>arXiv:2512.07074v1 Announce Type: cross 
Abstract: Statistically correcting measured cross sections for detector effects is an important step across many applications. In particle physics, this inverse problem is known as \textit{unfolding}. In cases with complex instruments, the distortions they introduce are often known only implicitly through simulations of the detector. Modern machine learning has enabled efficient simulation-based approaches for unfolding high-dimensional data. Among these, one of the first methods successfully deployed on experimental data is the \textsc{OmniFold} algorithm, a classifier-based Expectation-Maximization procedure. In practice, however, the forward model is only approximately specified, and the corresponding uncertainty is encoded through nuisance parameters. Building on the well-studied \textsc{OmniFold} algorithm, we show how to extend machine learning-based unfolding to incorporate nuisance parameters. Our new algorithm, called Profile \textsc{OmniFold}, is demonstrated using a Gaussian example as well as a particle physics case study using simulated data from the CMS Experiment at the Large Hadron Collider.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07074v1</guid>
      <category>stat.AP</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huanbiao Zhu, Krish Desai, Mikael Kuusela, Vinicius Mikuni, Benjamin Nachman, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Thermodynamic bounds on energy use in quasi-static Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2503.09980</link>
      <description>arXiv:2503.09980v3 Announce Type: replace-cross 
Abstract: The rapid growth of deep neural networks (DNNs) has brought increasing attention to their energy use during training and inference. Here, we establish the thermodynamic bounds on energy consumption in quasi-static analog DNNs by mapping modern feedforward architectures onto a physical free-energy functional. This framework provides a direct statistical-mechanical interpretation of quasi-static DNNs. As a result, inference can proceed in a thermodynamically reversible manner, with vanishing minimal energy cost, in contrast to the Landauer limit that constrains digital hardware. Importantly, inference corresponds to relaxation to a unique free-energy minimum with F_{\min}=0, allowing all constraints to be satisfied without residual stress. By comparison, training overconstrains the system: simultaneous clamping of inputs and outputs generates stresses that propagate backward through the architecture, reproducing the rules of backpropagation. Parameter annealing then relaxes these stresses, providing a purely physical route to learning without an explicit loss function. We further derive a universal lower bound on training energy, E&lt; 2NDkT, which scales with both the number of trainable parameters and the dataset size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09980v3</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexei V. Tkachenko</dc:creator>
    </item>
    <item>
      <title>Using Wavelet Decomposition to Determine the Dimension of Structures from Projected Images</title>
      <link>https://arxiv.org/abs/2503.23202</link>
      <description>arXiv:2503.23202v2 Announce Type: replace-cross 
Abstract: Mesoscale structures can often be described as fractional dimensional across a wide range of scales. We consider a $\gamma$ dimensional measure embedded in an $N$ dimensional space and discuss how to determine its dimension, both in $N$ dimensions and projected into $D$ dimensions.
  It is a highly non-trivial problem to decode the original geometry from lower dimensional projection of a high-dimensional measure. The projections are space-feeling, the popular box-counting techniques do not apply, and the Fourier methods are contaminated by aliasing effects. In the present paper we demonstrate that under the "Copernican hypothesis'' that we are not observing objects from a special direction, projection in a wavelet basis is remarkably simple: the wavelet power spectrum of a projected $\gamma$ dimensional measure is $P_j \propto 2^{-j\gamma}$. This holds regardless of the embedded dimension, $N$, and the projected dimension, $D$. This approach could have potentially broad applications in data sciences where a typically sparse matrix encodes lower dimensional information embedded in an extremely high dimensional field and often measured in projection to a low dimensional space.
  Here, we apply this method to JWST and Chandra observations of the nearby supernova Cas A. We find that the emissions can be represented by projections of mesoscale substructures with fractal dimensions varying from $\gamma = 1.7$ for the warm CO layer observed by JWST, up to $\gamma = 2.5$ for the hot X-ray emitting gas layer in the supernova remnant. The resulting power law indicates that the emission is coming from a fractal dimensional mesoscale structure likely produced by magneto-hydrodynamical instabilities in the expanding supernova shell.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23202v2</guid>
      <category>astro-ph.HE</category>
      <category>astro-ph.GA</category>
      <category>math.AP</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Svitlana Mayboroda, David N Spergel</dc:creator>
    </item>
    <item>
      <title>An exact multiple-time-step variational formulation for the committor and the transition rate</title>
      <link>https://arxiv.org/abs/2509.03539</link>
      <description>arXiv:2509.03539v2 Announce Type: replace-cross 
Abstract: For a transition between two stable states, the committor is the probability that the dynamics leads to one stable state before the other. It can be estimated from trajectory data by minimizing an expression for the transition rate that depends on a lag time. We show that an existing such expression is minimized by the exact committor only when the lag time is a single time step, resulting in a biased estimate in practical applications. We introduce an alternative expression that is minimized by the exact committor at any lag time. The key idea is that, when trajectories enter the stable states, the times that they enter (stopping times) must be used for estimating the committor and transition rate instead of the lag time. Numerical tests on benchmark systems demonstrate that our committor and transition rate estimates are much less sensitive to the choice of lag time. We show how further accuracy for the transition rate can be achieved by combining results from two lag times. We also relate the transition rate expression to a variational approach for kinetic statistics based on the mean-squared residual and discuss further numerical considerations with the aid of a decomposition of the error into dynamic modes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03539v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chatipat Lorpaiboon, Jonathan Weare, Aaron R. Dinner</dc:creator>
    </item>
    <item>
      <title>EnScale: Temporally-consistent multivariate generative downscaling via proper scoring rules</title>
      <link>https://arxiv.org/abs/2509.26258</link>
      <description>arXiv:2509.26258v2 Announce Type: replace-cross 
Abstract: The practical use of future climate projections from global circulation models (GCMs) is often limited by their coarse spatial resolution, requiring downscaling to generate high-resolution data. Regional climate models (RCMs) provide this refinement, but are computationally expensive. To address this issue, machine learning models can learn the downscaling function, mapping coarse GCM outputs to high-resolution fields. Among these, generative approaches aim to capture the full conditional distribution of RCM data given coarse-scale GCM data, which is characterized by large variability and thus challenging to model accurately. We introduce EnScale, a generative machine learning framework that emulates the full GCM-to-RCM map by training on multiple pairs of GCM and corresponding RCM data. It first adjusts large-scale mismatches between GCM and coarsened RCM data, followed by a super-resolution step to generate high-resolution fields. Both steps employ generative models optimized with the energy score, a proper scoring rule. Compared to state-of-the-art ML downscaling approaches, our setup reduces computational cost by about one order of magnitude. EnScale jointly emulates multiple variables -- temperature, precipitation, solar radiation, and wind -- spatially consistent over an area in Central Europe. In addition, we propose a variant EnScale-t that enables temporally consistent downscaling. We establish a comprehensive evaluation framework across various categories including calibration, spatial structure, extremes, and multivariate dependencies. Comparison with diverse benchmarks demonstrates EnScale's strong performance and computational efficiency. EnScale offers a promising approach for accurate and temporally consistent RCM emulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26258v2</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maybritt Schillinger, Maxim Samarin, Xinwei Shen, Reto Knutti, Nicolai Meinshausen</dc:creator>
    </item>
    <item>
      <title>A synchronization-free one-way ranging observable for detecting and characterizing coherent orbital-period systematics in GRACE-FO laser ranging data</title>
      <link>https://arxiv.org/abs/2511.14782</link>
      <description>arXiv:2511.14782v3 Announce Type: replace-cross 
Abstract: We present a synchronization-free differential observable for one-way inter-satellite laser ranging, designed to suppress first-order Doppler effects without requiring clock synchronization between spacecraft. The observable is constructed from successive pulse-interval differences, which isolate time-varying signatures while eliminating static and slowly varying biases. Applied to GRACE-FO Laser Ranging Interferometer (LRI) Level-1B data over four seasonal epochs in 2019, the method reveals a stable, spectrally narrow modulation at the orbital frequency. The amplitude and phase of the detected signature remain consistent across all datasets, demonstrating a deterministic, mission-internal origin. The detection is independently confirmed through synthetic-signal injection, shuffle-based significance testing, and cross-comparison with K-band ranging data. These results show that the proposed observable provides a sensitive diagnostic for identifying coherent orbital-period systematics that may remain hidden in conventional range-rate analysis. The method offers a pathway toward improved characterization of instrument and dynamical effects in current and future satellite gravity missions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14782v3</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <category>physics.ins-det</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. H. Wassegh</dc:creator>
    </item>
  </channel>
</rss>
