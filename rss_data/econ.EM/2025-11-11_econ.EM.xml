<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Nov 2025 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Cluster-robust inference with a single treated cluster using the t-test</title>
      <link>https://arxiv.org/abs/2511.05710</link>
      <description>arXiv:2511.05710v1 Announce Type: new 
Abstract: This paper considers inference when there is a single treated cluster and a fixed number of control clusters, a setting that is common in empirical work, especially in difference-in-differences designs. We use the t-statistic and develop suitable critical values to conduct valid inference under weak assumptions allowing for unknown dependence within clusters. In particular, our inference procedure does not involve variance estimation. It only requires specifying the relative heterogeneity between the variances from the treated cluster and some, but not necessarily all, control clusters. Our proposed test works for any significance level when there are at least two control clusters. When the variance of the treated cluster is bounded by those of all control clusters up to some prespecified scaling factor, the critical values for our t-statistic can be easily computed without any optimization for many conventional significance levels and numbers of clusters. In other cases, one-dimensional numerical optimization is needed and is often computationally efficient. We have also tabulated common critical values in the paper so researchers can use our test readily. We illustrate our method in simulations and empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05710v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chun Pong Lau, Xinran Li</dc:creator>
    </item>
    <item>
      <title>Optimally-Transported Generalized Method of Moments</title>
      <link>https://arxiv.org/abs/2511.05712</link>
      <description>arXiv:2511.05712v1 Announce Type: new 
Abstract: We propose a novel optimal transport-based version of the Generalized Method of Moment (GMM). Instead of handling overidentification by reweighting the data to satisfy the moment conditions (as in Generalized Empirical Likelihood methods), this method proceeds by allowing for errors in the variables of the least mean-square magnitude necessary to simultaneously satisfy all moment conditions. This approach, based on the notions of optimal transport and Wasserstein metric, aims to address the problem of assigning a logical interpretation to GMM results even when overidentification tests reject the null, a situation that cannot always be avoided in applications. We illustrate the method by revisiting Duranton, Morrow and Turner's (2014) study of the relationship between a city's exports and the extent of its transportation infrastructure. Our results corroborate theirs under weaker assumptions and provide insight into the error structure of the variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05712v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susanne Schennach, Vincent Starck</dc:creator>
    </item>
    <item>
      <title>The Exact Variance of the Average Treatment Effect Estimator in Cluster RCT</title>
      <link>https://arxiv.org/abs/2511.05801</link>
      <description>arXiv:2511.05801v1 Announce Type: new 
Abstract: In cluster randomized controlled trials (CRCT) with a finite populations, the exact design-based variance of the Horvitz-Thompson (HT) estimator for the average treatment effect (ATE) depends on the joint distribution of unobserved cluster-aggregated potential outcomes and is therefore not point-identifiable. We study a common two-stage sampling design-random sampling of clusters followed by sampling units within sampled clusters-with treatment assigned at the cluster level. First, we derive the exact (infeasible) design-based variance of the HT ATE estimator that accounts jointly for cluster- and unit-level sampling as well as random assignment. Second, extending Aronow et al (2014), we provide a sharp, attanable upper bound on that variance and propose a consistent estimator of the bound using only observed outcomes and known sampling/assignment probabilities. In simulations and an empirical application, confidence intervals based on our bound are valid and typically narrower than those based on cluster standard errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05801v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue Fang, Geert Ridder</dc:creator>
    </item>
    <item>
      <title>Synthetic Parallel Trends</title>
      <link>https://arxiv.org/abs/2511.05870</link>
      <description>arXiv:2511.05870v1 Announce Type: new 
Abstract: Popular empirical strategies for policy evaluation in the panel data literature -- including difference-in-differences (DID), synthetic control (SC) methods, and their variants -- rely on key identifying assumptions that can be expressed through a specific choice of weights $\omega$ relating pre-treatment trends to the counterfactual outcome. While each choice of $\omega$ may be defensible in empirical contexts that motivate a particular method, it relies on fundamentally untestable and often fragile assumptions. I develop an identification framework that allows for all weights satisfying a Synthetic Parallel Trends assumption: the treated unit's trend is parallel to a weighted combination of control units' trends for a general class of weights. The framework nests these existing methods as special cases and is by construction robust to violations of their respective assumptions. I construct a valid confidence set for the identified set of the treatment effect, which admits a linear programming representation with estimated coefficients and nuisance parameters that are profiled out. In simulations where the assumptions underlying DID or SC-based methods are violated, the proposed confidence set remains robust and attains nominal coverage, while existing methods suffer severe undercoverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05870v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiqi Liu</dc:creator>
    </item>
    <item>
      <title>Boundary Discontinuity Designs: Theory and Practice</title>
      <link>https://arxiv.org/abs/2511.06474</link>
      <description>arXiv:2511.06474v1 Announce Type: new 
Abstract: We review the literature on boundary discontinuity (BD) designs, a powerful non-experimental research methodology that identifies causal effects by exploiting a thresholding treatment assignment rule based on a bivariate score and a boundary curve. This methodology generalizes standard regression discontinuity designs based on a univariate score and scalar cutoff, and has specific challenges and features related to its multi-dimensional nature. We synthesize the empirical literature by systematically reviewing over $80$ empirical papers, tracing the method's application from its formative uses to its implementation in modern research. In addition to the empirical survey, we overview the latest methodological results on identification, estimation and inference for the analysis of BD designs, and offer recommendations for practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06474v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Unlocking the Regression Space</title>
      <link>https://arxiv.org/abs/2511.07183</link>
      <description>arXiv:2511.07183v1 Announce Type: new 
Abstract: This paper introduces and analyzes a framework that accommodates general heterogeneity in regression modeling. It demonstrates that regression models with fixed or time-varying parameters can be estimated using the OLS and time-varying OLS methods, respectively, across a broad class of regressors and noise processes not covered by existing theory. The proposed setting facilitates the development of asymptotic theory and the estimation of robust standard errors. The robust confidence interval estimators accommodate substantial heterogeneity in both regressors and noise. The resulting robust standard error estimates coincide with White's (1980) heteroskedasticity-consistent estimator but are applicable to a broader range of conditions, including models with missing data. They are computationally simple and perform well in Monte Carlo simulations. Their robustness, generality, and ease of implementation make them highly suitable for empirical applications. Finally, the paper provides a brief empirical illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07183v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liudas Giraitis, George Kapetanios, Yufei Li, Alexia Ventouri</dc:creator>
    </item>
    <item>
      <title>Estimating the Impact of the Bitcoin Halving on Its Price Using Synthetic Control</title>
      <link>https://arxiv.org/abs/2511.05512</link>
      <description>arXiv:2511.05512v1 Announce Type: cross 
Abstract: The third Bitcoin halving that took place in May 2020 cut down the mining reward from 12.5 to 6.25 BTC per block and thus slowed down the rate of issuance of new Bitcoins, making it more scarce. The fourth and most recent halving happened in April 2024, cutting the block reward further to 3.125 BTC. If the demand did not decrease simultaneously after these halvings, then the neoclassical economic theory posits that the price of Bitcoin should have increased due to the halving. But did it, in fact, increase for that reason, or is this a post hoc fallacy? This paper uses synthetic control to construct a weighted Bitcoin that is different from its counterpart in one aspect - it did not undergo halving. Comparing the price trajectory of the actual and the simulated Bitcoins, I find evidence of a positive effect of the 2024 Bitcoin halving on its price three months later. The magnitude of this effect is one fifth of the total percentage change in the price of Bitcoin during the study period - from April 2, 2023, to July 21, 2024 (17 months). The second part of the study fails to obtain a statistically significant and robust causal estimate of the effect of the 2020 Bitcoin halving on Bitcoin's price. This is the first paper analyzing the effect of halving causally, building on the existing body of correlational research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05512v1</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladislav Virtonen</dc:creator>
    </item>
    <item>
      <title>Multilevel non-linear interrupted time series analysis</title>
      <link>https://arxiv.org/abs/2511.05725</link>
      <description>arXiv:2511.05725v1 Announce Type: cross 
Abstract: Recent advances in interrupted time series analysis permit characterization of a typical non-linear interruption effect through use of generalized additive models. Concurrently, advances in latent time series modeling allow efficient Bayesian multilevel time series models. We propose to combine these concepts with a hierarchical model selection prior to characterize interruption effects with a multilevel structure, encouraging parsimony and partial pooling while incorporating meaningful variability in causal effects across subpopulations of interest, while allowing poststratification. These models are demonstrated with three applications: 1) the effect of the introduction of the prostate specific antigen test on prostate cancer diagnosis rates by race and age group, 2) the change in stroke or trans-ischemic attack hospitalization rates across Medicare beneficiaries by rurality in the months after the start of the COVID-19 pandemic, and 3) the effect of Medicaid expansion in Missouri on the proportion of inpatient hospitalizations discharged with Medicaid as a primary payer by key age groupings and sex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05725v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>RJ Waken, Fengxian Wang, Sarah A. Eisenstein, Tim McBride, Kim Johnson, Karen Joynt-Maddox</dc:creator>
    </item>
    <item>
      <title>Standard and comparative e-backtests for general risk measures</title>
      <link>https://arxiv.org/abs/2511.05840</link>
      <description>arXiv:2511.05840v1 Announce Type: cross 
Abstract: Backtesting risk measures is a unique and important problem for financial regulators to evaluate risk forecasts reported by financial institutions. As a natural extension to standard (or traditional) backtests, comparative backtests are introduced to evaluate different forecasts against regulatory standard models. Based on recently developed concepts of e-values and e-processes, we focus on how standard and comparative backtests can be manipulated in financial regulation by constructing e-processes. We design a model-free (non-parametric) method for standard backtests of identifiable risk measures and comparative backtests of elicitable risk measures. Our e-backtests are applicable to a wide range of common risk measures including the mean, the variance, the Value-at-Risk, the Expected Shortfall, and the expectile. Our results are illustrated by ample simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05840v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhanyi Jiao, Qiuqi Wang, Yimiao Zhao</dc:creator>
    </item>
    <item>
      <title>A sensitivity analysis for the average derivative effect</title>
      <link>https://arxiv.org/abs/2511.06243</link>
      <description>arXiv:2511.06243v1 Announce Type: cross 
Abstract: In observational studies, exposures are often continuous rather than binary or discrete. At the same time, sensitivity analysis is an important tool that can help determine the robustness of a causal conclusion to a certain level of unmeasured confounding, which can never be ruled out in an observational study. Sensitivity analysis approaches for continuous exposures have now been proposed for several causal estimands. In this article, we focus on the average derivative effect (ADE). We obtain closed-form bounds for the ADE under a sensitivity model that constrains the odds ratio (at any two dose levels) between the latent and observed generalized propensity score. We propose flexible, efficient estimators for the bounds, as well as point-wise and simultaneous (over the sensitivity parameter) confidence intervals. We examine the finite sample performance of the methods through simulations and illustrate the methods on a study assessing the effect of parental income on educational attainment and a study assessing the price elasticity of petrol.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06243v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Zhang</dc:creator>
    </item>
    <item>
      <title>Diffolio: A Diffusion Model for Multivariate Probabilistic Financial Time-Series Forecasting and Portfolio Construction</title>
      <link>https://arxiv.org/abs/2511.07014</link>
      <description>arXiv:2511.07014v1 Announce Type: cross 
Abstract: Probabilistic forecasting is crucial in multivariate financial time-series for constructing efficient portfolios that account for complex cross-sectional dependencies. In this paper, we propose Diffolio, a diffusion model designed for multivariate financial time-series forecasting and portfolio construction. Diffolio employs a denoising network with a hierarchical attention architecture, comprising both asset-level and market-level layers. Furthermore, to better reflect cross-sectional correlations, we introduce a correlation-guided regularizer informed by a stable estimate of the target correlation matrix. This structure effectively extracts salient features not only from historical returns but also from asset-specific and systematic covariates, significantly enhancing the performance of forecasts and portfolios. Experimental results on the daily excess returns of 12 industry portfolios show that Diffolio outperforms various probabilistic forecasting baselines in multivariate forecasting accuracy and portfolio performance. Moreover, in portfolio experiments, portfolios constructed from Diffolio's forecasts show consistently robust performance, thereby outperforming those from benchmarks by achieving higher Sharpe ratios for the mean-variance tangency portfolio and higher certainty equivalents for the growth-optimal portfolio. These results demonstrate the superiority of our proposed Diffolio in terms of not only statistical accuracy but also economic significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07014v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>q-fin.PM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>So-Yoon Cho, Jin-Young Kim, Kayoung Ban, Hyeng Keun Koo, Hyun-Gyoon Kim</dc:creator>
    </item>
    <item>
      <title>Identification-robust inference for the LATE with high-dimensional covariates</title>
      <link>https://arxiv.org/abs/2302.09756</link>
      <description>arXiv:2302.09756v5 Announce Type: replace 
Abstract: This paper presents an inference method for the local average treatment effect (LATE) in the presence of high-dimensional covariates, regardless of the strength of identification. We propose an orthogonalized Anderson-Rubin test statistic that maintains uniformly valid asymptotic size. We provide an easy-to-implement algorithm for inferring the high-dimensional LATE by inverting our test statistic and employing the double/debiased machine learning method. Simulation results show that our test achieves better size control under both weak identification and high dimensionality, outperforming conventional alternatives. Applying the proposed method to railroad and population data to study the effect of railroad access on urban population growth, we observe wider confidence intervals than those obtained using conventional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09756v5</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukun Ma</dc:creator>
    </item>
    <item>
      <title>Simple Estimation of Semiparametric Models with Measurement Errors</title>
      <link>https://arxiv.org/abs/2306.14311</link>
      <description>arXiv:2306.14311v4 Announce Type: replace 
Abstract: We develop a practical way of addressing the Errors-In-Variables (EIV) problem in the Generalized Method of Moments (GMM) framework. We focus on the settings in which the variability of the EIV is a fraction of that of the mismeasured variables, which is typical for empirical applications. For any initial set of moment conditions our approach provides a ``corrected'' set of moment conditions that are robust to the EIV. We show that the GMM estimator based on these moments is root-n-consistent, with the standard tests and confidence intervals providing valid inference. This is true even when the EIV are so large that naive estimators (that ignore the EIV problem) are heavily biased with their confidence intervals having 0% coverage. Our approach involves no nonparametric estimation, which is especially important for applications with many covariates and settings with multivariate EIV. In particular, the approach makes it easy to use instrumental variables to address EIV in nonlinear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14311v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirill S. Evdokimov, Andrei Zeleneev</dc:creator>
    </item>
    <item>
      <title>Invalid proxies and volatility changes</title>
      <link>https://arxiv.org/abs/2403.08753</link>
      <description>arXiv:2403.08753v4 Announce Type: replace 
Abstract: When in proxy-SVARs the covariance matrix of VAR disturbances is subject to exogenous, permanent breaks that cause IRFs to change across volatility regimes, even strong, exogenous external instruments yield inconsistent estimates of the dynamic causal effects. However, if these volatility shifts are properly incorporated into the analysis through (testable) "stability restrictions", we demonstrate that the target IRFs are point-identified and can be estimated consistently under a necessary and sufficient rank condition. If the shifts in volatility are sufficiently informative, standard asymptotic inference remains valid even with (i) local-to-zero covariance between the proxies and the instrumented structural shocks, and (ii) potential failures of instrument exogeneity. Intuitively, shifts in volatility act similarly to strong instruments that are correlated with both the target and non-target shocks. We illustrate the effectiveness of our approach by revisiting a seminal fiscal proxy-SVAR for the US economy. We detect a sharp change in the size of the tax multiplier when the narrative tax instrument is complemented with the decline in unconditional volatility observed during the transition from the Great Inflation to the Great Moderation. The narrative tax instrument contributes to identify the tax shock in both regimes, although our empirical analysis raises concerns about its "statistical" validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08753v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Angelini, Luca Fanelli, Luca Neri</dc:creator>
    </item>
    <item>
      <title>Leveraging Uniformization and Sparsity for Estimation and Computation of Continuous Time Dynamic Discrete Choice Games</title>
      <link>https://arxiv.org/abs/2407.14914</link>
      <description>arXiv:2407.14914v3 Announce Type: replace 
Abstract: Continuous-time empirical dynamic discrete choice games offer notable computational advantages over discrete-time models. This paper addresses remaining computational and econometric challenges to further improve both model solution and estimation. We establish convergence rates for value iteration and policy evaluation with fixed beliefs, and develop Newton-Kantorovich methods that exploit analytical Jacobians and sparse matrix structure. We apply uniformization both to derive a new representation of the value function that draws direct analogies to discrete-time models and to enable stable computation of the matrix exponential and its parameter derivatives for estimation with discrete-time snapshot data, a common but challenging data scenario. These methods provide a complete chain of analytical derivatives from the value function for a given equilibrium through the log-likelihood function, eliminating the need for numerical differentiation and improving finite-sample estimation accuracy and computational efficiency. Monte Carlo experiments demonstrate substantial gains in both statistical performance and computational efficiency, enabling researchers to estimate richer models of strategic interaction. While we focus on games, our methods extend to single-agent dynamic discrete choice and continuous-time Markov jump processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14914v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason R. Blevins</dc:creator>
    </item>
    <item>
      <title>Measuring the Euro Area Output Gap</title>
      <link>https://arxiv.org/abs/2505.05536</link>
      <description>arXiv:2505.05536v2 Announce Type: replace 
Abstract: We measure the Euro Area (EA) output gap and potential output using a non-stationary dynamic factor model estimated on a large dataset of macroeconomic and financial variables. Our results indicate that, between 2012 and 2024, the EA economy was consistently tighter than suggested by institutional estimates, implying that its weak growth reflects a potential output problem rather than a business-cycle one. Moreover, we find that the decline in trend inflation-rather than economic slack-kept core inflation below 2% before the pandemic, while demand forces explain at least 30% of the post-pandemic rise in core inflation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05536v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Claudio Lissona, Matteo Luciani</dc:creator>
    </item>
    <item>
      <title>Nonlinear Treatment Effects in Shift-Share Designs</title>
      <link>https://arxiv.org/abs/2507.21915</link>
      <description>arXiv:2507.21915v2 Announce Type: replace 
Abstract: We analyze heterogenous, nonlinear treatment effects in shift-share designs with exogenous shares. We employ a triangular model and correct for treatment endogeneity using a control function. Our tools identify four target parameters. Two of them capture the observable heterogeneity of treatment effects, while one summarizes this heterogeneity in a single measure. The last parameter analyzes counterfactual, policy-relevant treatment assignment mechanisms. We propose flexible parametric estimators for these parameters and apply them to reevaluate the impact of Chinese imports on U.S. manufacturing employment. Our results highlight substantial treatment effect heterogeneity, which is not captured by commonly used shift-share tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21915v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luigi Garzon, Vitor Possebom</dc:creator>
    </item>
    <item>
      <title>Correcting sample selection bias with categorical outcomes</title>
      <link>https://arxiv.org/abs/2510.05551</link>
      <description>arXiv:2510.05551v2 Announce Type: replace 
Abstract: In this paper, I propose a method for correcting sample selection bias when the outcome of interest is categorical, such as occupational choice, health status, or field of study. Classical approaches to sample selection rely on strong parametric distributional assumptions, which may be restrictive in practice. I develop a local representation that decomposes each joint probability into marginal probabilities and a category-specific association parameter that captures how selection differentially affects each outcome. Under some exclusion restrictions, I establish nonparametric point identification of the latent categorical distribution. Building on this identification result, I introduce a semiparametric multinomial logit model with sample selection, propose a computationally tractable two-step estimator, and derive its asymptotic properties. I illustrate the method by studying the determinants of healthcare utilization in C\^ote d'Ivoire.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05551v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onil Boussim</dc:creator>
    </item>
    <item>
      <title>Identifying treatment effects on categorical outcomes in IV models</title>
      <link>https://arxiv.org/abs/2510.10946</link>
      <description>arXiv:2510.10946v2 Announce Type: replace 
Abstract: This paper provides a nonparametric framework for causal inference with categorical outcomes under binary treatment and binary instrument settings. I decompose the observed joint probability of outcomes and treatment into marginal probabilities of potential outcomes and treatment, and association parameters that capture selection bias due to unobserved heterogeneity. Under a novel identifying assumption \emph{association similarity}, which requires the dependence between unobserved factors driving treatment and potential outcomes to be invariant across treatment states, I achieve point identification of the full distribution of potential outcomes. Recognizing that this assumption may be strong in some contexts, I propose two weaker alternatives: monotonic association, which restricts the direction of selection heterogeneity, and bounded association, which constrains its magnitude. These relaxed assumptions deliver sharp partial identification bounds that nest point identification as a special case and facilitate transparent sensitivity analysis. I illustrate the framework in an empirical application, estimating the causal effect of private health insurance on health outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10946v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onil Boussim</dc:creator>
    </item>
    <item>
      <title>Choosing What to Learn: Experimental Design when Combining Experimental with Observational Evidence</title>
      <link>https://arxiv.org/abs/2510.23434</link>
      <description>arXiv:2510.23434v2 Announce Type: replace 
Abstract: Experiments deliver credible but often localized effects, tied to specific sites, populations, or mechanisms. When such estimates are insufficient to extrapolate effects for broader policy questions, such as external validity and general-equilibrium (GE) effects, researchers combine trials with external evidence from reduced-form or structural observational estimates, or prior experiments. We develop a unified framework for designing experiments in this setting: the researcher selects which parameters (or moments) to identify experimentally from a feasible set (e.g., which treatment arms and/or individuals to include in the experiment), allocates sample size, and specifies how to weight experimental and observational estimators. Because observational inputs may be biased in ways unknown ex ante, we develop a minimax proportional regret objective that evaluates any candidate design relative to an oracle that knows the bias and jointly chooses the design and estimator. This yields a transparent bias-variance trade-off that requires no prespecified bias bound and depends only on information about the precision of the estimators and the estimand's sensitivity to the underlying parameters. We illustrate the framework by (i) designing small-scale cash transfer experiments aimed at estimating GE effects and (ii) optimizing site selection for microfinance interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23434v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Aristotelis Epanomeritakis, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Disentangling Structural Breaks in Factor Models for Macroeconomic Data</title>
      <link>https://arxiv.org/abs/2303.00178</link>
      <description>arXiv:2303.00178v3 Announce Type: replace-cross 
Abstract: We develop a projection-based decomposition to disentangle structural breaks in the factor variance and factor loadings. Our approach yields test statistics that can be compared against standard distributions commonly used in the structural break literature. Because standard methods for estimating factor models in macroeconomics normalize the factor variance, they do not distinguish between breaks of the factor variance and factor loadings. Applying our procedure to U.S. macroeconomic data, we find that the Great Moderation is more naturally accommodated as a break in the factor variance as opposed to a break in the factor loadings, in contrast to extant procedures which do not tell the two apart and thus interpret the Great Moderation as a structural break in the factor loadings. Through our projection-based decomposition, we estimate that the Great Moderation is associated with an over 70\% reduction in the total factor variance, highlighting the relevance of disentangling breaks in the factor structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.00178v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/07350015.2025.2583205</arxiv:DOI>
      <dc:creator>Bonsoo Koo, Benjamin Wong, Ze-Yu Zhong</dc:creator>
    </item>
    <item>
      <title>Policy Learning with Abstention</title>
      <link>https://arxiv.org/abs/2510.19672</link>
      <description>arXiv:2510.19672v2 Announce Type: replace-cross 
Abstract: Policy learning algorithms are widely used in areas such as personalized medicine and advertising to develop individualized treatment regimes. However, most methods force a decision even when predictions are uncertain, which is risky in high-stakes settings. We study policy learning with abstention, where a policy may defer to a safe default or an expert. When a policy abstains, it receives a small additive reward on top of the value of a random guess. We propose a two-stage learner that first identifies a set of near-optimal policies and then constructs an abstention rule from their disagreements. We establish fast O(1/n)-type regret guarantees when propensities are known, and extend these guarantees to the unknown-propensity case via a doubly robust (DR) objective. We further show that abstention is a versatile tool with direct applications to other core problems in policy learning: it yields improved guarantees under margin conditions without the common realizability assumption, connects to distributionally robust policy learning by hedging against small data shifts, and supports safe policy improvement by ensuring improvement over a baseline policy with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19672v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Sawarni, Jikai Jin, Justin Whitehouse, Vasilis Syrgkanis</dc:creator>
    </item>
  </channel>
</rss>
