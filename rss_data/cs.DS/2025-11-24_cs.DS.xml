<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Nov 2025 03:50:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Efficient Algorithms and Implementations for Extracting Maximum-Size $(k,\ell)$-Sparse Subgraphs</title>
      <link>https://arxiv.org/abs/2511.16877</link>
      <description>arXiv:2511.16877v1 Announce Type: new 
Abstract: A multigraph $G = (V, E)$ is $(k, \ell)$-sparse if every subset $X \subseteq V$ induces at most $\max\{k|X| - \ell, 0\}$ edges. Finding a maximum-size $(k, \ell)$-sparse subgraph is a classical problem in rigidity theory and combinatorial optimization, with known polynomial-time algorithms. This paper presents a highly efficient and flexible implementation of an augmenting path method, enhanced with a range of powerful practical heuristics that significantly reduce running time while preserving optimality. These heuristics $\unicode{x2013}$ including edge-ordering, node-ordering, two-phase strategies, and pseudoforest-based initialization $\unicode{x2013}$ steer the algorithm toward accepting more edges early in the execution and avoiding costly augmentations. A comprehensive experimental evaluation on both synthetic and real-world graphs demonstrates that our implementation outperforms existing tools by several orders of magnitude. We also propose an asymptotically faster algorithm for extracting an inclusion-wise maximal $(k,2k)$-sparse subgraph with the sparsity condition required only for node sets of size at least three, which is particularly relevant to 3D rigidity when $k = 3$. We provide a carefully engineered implementation, which is publicly available online and is proposed for inclusion in the LEMON graph library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16877v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P\'eter Madarasi</dc:creator>
    </item>
    <item>
      <title>Low-Sensitivity Matching via Sampling from Gibbs Distributions</title>
      <link>https://arxiv.org/abs/2511.16918</link>
      <description>arXiv:2511.16918v1 Announce Type: new 
Abstract: In this work, we study the maximum matching problem from the perspective of sensitivity. The sensitivity of an algorithm $A$ on a graph $G$ is defined as the maximum Wasserstein distance between the output distributions of $A$ on $G$ and on $G - e$, where $G - e$ is the graph obtained by deleting an edge $e$ from $G$. The maximum is taken over all edges $e$, and the underlying metric for the Wasserstein distance is the Hamming distance.
  We first show that for any $\varepsilon &gt; 0$, there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $\Delta^{O(1/\varepsilon)}$, where $\Delta$ is the maximum degree of the input graph. The algorithm is based on sampling from the Gibbs distribution over matchings and runs in time $O_{\varepsilon, \Delta}(m \log m)$, where $m$ is the number of edges in the graph. This result significantly improves the previously known sensitivity bounds.
  Next, we present significantly faster algorithms for planar and bipartite graphs as a function of $\varepsilon$ and $\Delta$, which run in time $\mathrm{poly}(n/\varepsilon)$. This improvement is achieved by designing a more efficient algorithm for sampling matchings from the Gibbs distribution in these graph classes, which improves upon the previous best in terms of running time.
  Finally, for general graphs with potentially unbounded maximum degree, we show that there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$, improving upon the previous best bound of $O(n^{1/(1+\varepsilon^2)})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16918v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuichi Yoshida, Zihan Zhang</dc:creator>
    </item>
    <item>
      <title>Merging RLBWTs adaptively</title>
      <link>https://arxiv.org/abs/2511.16953</link>
      <description>arXiv:2511.16953v1 Announce Type: new 
Abstract: We show how to merge run-length compressed Burrows-Wheeler Transforms (RLBWTs) quickly and in $O (R)$ space, where $R$ is the total number of runs in them, when a certain parameter is small. Specifically, we consider the boundaries in their combined extended Burrows-Wheeler Transform (eBWT) between blocks of characters from the same original RLBWT, and denote by $L$ the sum of the longest common prefix (LCP) values at those boundaries. We show how to merge the RLBWTs in $\tilde{O} (L + \sigma + R)$ time, where $\sigma$ is the alphabet size. We conjecture that $L$ tends to be small when the strings (or sets of strings) underlying the original RLBWTs are repetitive but dissimilar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16953v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Travis Gagie</dc:creator>
    </item>
    <item>
      <title>Triangle Detection in H-Free Graphs</title>
      <link>https://arxiv.org/abs/2511.17224</link>
      <description>arXiv:2511.17224v1 Announce Type: new 
Abstract: We initiate the study of combinatorial algorithms for Triangle Detection in $H$-free graphs. The goal is to decide if a graph that forbids a fixed pattern $H$ as a subgraph contains a triangle, using only "combinatorial" methods that notably exclude fast matrix multiplication. Our work aims to classify which patterns admit a subcubic speedup, working towards a dichotomy theorem. On the lower bound side, we show that if $H$ is not $3$-colorable or contains more than one triangle, the complexity of the problem remains unchanged, and no combinatorial speedup is likely possible. On the upper bound side, we develop an embedding approach that results in a strongly subcubic, combinatorial algorithm for a rich class of "embeddable" patterns. Specifically, for an embeddable pattern of size $k$, our algorithm runs in $\tilde O(n^{3-\frac{1}{2^{k-3}}})$ time, where $\tilde O(\cdot)$ hides poly-logarithmic factors. This algorithm also extends to listing all the triangles within the same time bound. We supplement this main result with two generalizations: 1) A generalization to patterns that are embeddable up to a single obstacle that arises from a triangle in the pattern. This completes our classification for small patterns, yielding a dichotomy theorem for all patterns of size up to eight. 2) An $H$-sensitive algorithm for embeddable patterns, which runs faster when the number of copies of $H$ is significantly smaller than the maximum possible $\Omega(n^k)$. Finally, we focus on the special case of odd cycles. We present specialized Triangle Detection algorithms that are very efficient: 1) A combinatorial algorithm for $C_{2k+1}$-free graphs that runs in $\tilde O(m+n^{1+2/k})$ time for every $k\geq2$, where $m$ is the number of edges in the graph. 2) A combinatorial $C_5$-sensitive algorithm that runs in $\tilde O(n^2+n^{4/3}t^{1/3})$ time, where $t$ is the number of $5$-cycles in the graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17224v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Abboud, Ron Safier, Nathan Wallheimer</dc:creator>
    </item>
    <item>
      <title>Spectral Clustering with Side Information</title>
      <link>https://arxiv.org/abs/2511.17326</link>
      <description>arXiv:2511.17326v1 Announce Type: new 
Abstract: In the graph clustering problem with a planted solution, the input is a graph on $n$ vertices partitioned into $k$ clusters, and the task is to infer the clusters from graph structure. A standard assumption is that clusters induce well-connected subgraphs (i.e. $\Omega(1)$-expanders), and form $\epsilon$-sparse cuts. Such a graph defines the clustering uniquely up to $\approx \epsilon$ misclassification rate, and efficient algorithms for achieving this rate are known. While this vanilla version of graph clustering is well studied, in practice, vertices of the graph are typically equipped with labels that provide additional information on cluster ids of the vertices. For example, each vertex could have a cluster label that is corrupted independently with probability $\delta$. Using only one of the two sources of information leads to misclassification rate $\min\{\epsilon, \delta\}$, but can they be combined to achieve a rate of $\approx \epsilon \delta$?
  In this paper, we give an affirmative answer to this question and present a sublinear-time algorithm in the number of vertices $n$. Our key algorithmic insight is a new observation on ``spectrally ambiguous'' vertices in a well-clusterable graph.
  While our sublinear-time classifier achieves the nearly optimal $\approx \widetilde O(\epsilon \delta)$ misclassification rate, the approximate clusters that it outputs do not necessarily induce expanders in the graph $G$. In our second result, we give a polynomial-time algorithm that reweights edges of the original $(k, \epsilon, \Omega(1))$-clusterable graph to transform it into a $(k, \widetilde O(\epsilon \delta), \Omega(1))$-clusterable one (for constant $k$), improving sparsity of cuts nearly optimally and preserving expansion properties of the communities - an algorithm for refining community structure of the input graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17326v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hendrik Fichtenberger, Michael Kapralov, Ekaterina Kochetkova, Silvio Lattanzi, Davide Mazzali, Weronika Wrzos-Kaminska</dc:creator>
    </item>
    <item>
      <title>Relative Error Streaming Quantiles with Seamless Mergeability via Adaptive Compactors</title>
      <link>https://arxiv.org/abs/2511.17396</link>
      <description>arXiv:2511.17396v1 Announce Type: new 
Abstract: Quantile summaries provide a scalable way to estimate the distribution of individual attributes in large datasets that are often distributed across multiple machines or generated by sensor networks. ReqSketch (arXiv:2004.01668) is currently the most space-efficient summary with two key properties: relative error guarantees, offering increasingly higher accuracy towards the distribution's tails, and mergeability, allowing distributed or parallel processing of datasets. Due to these features and its simple algorithm design, ReqSketch has been adopted in practice, via implementation in the Apache DataSketches library. However, the proof of mergeability in ReqSketch is overly complicated, requiring an intricate charging argument and complex variance analysis.
  In this paper, we provide a refined version of ReqSketch, by developing so-called adaptive compactors. This enables a significantly simplified proof of relative error guarantees in the most general mergeability setting, while retaining the original space bound, update time, and algorithmic simplicity. Moreover, the adaptivity of our sketch, together with the proof technique, yields near-optimal space bounds in specific scenarios - particularly when merging sketches of comparable size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17396v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom\'a\v{s} Domes, Pavel Vesel\'y</dc:creator>
    </item>
    <item>
      <title>Multi-variable Quantification of BDDs in External Memory using Nested Sweeping (Extended Paper)</title>
      <link>https://arxiv.org/abs/2408.14216</link>
      <description>arXiv:2408.14216v5 Announce Type: replace 
Abstract: Previous research on the Adiar BDD package has been successful at designing algorithms capable of handling large Binary Decision Diagrams (BDDs) stored in external memory. To do so, it uses consecutive sweeps through the BDDs to resolve computations. Yet, this approach has kept algorithms for multi-variable quantification, the relational product, and variable reordering out of its scope.
  In this work, we address this by introducing the nested sweeping framework. Here, multiple concurrent sweeps pass information between eachother to compute the result. We have implemented the framework in Adiar and used it to create a new external memory multi-variable quantification algorithm. Compared to conventional depth-first implementations, Adiar with nested sweeping is able to solve more instances of our benchmarks and/or solve them faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14216v5</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffan Christ S{\o}lvsten, Jaco van de Pol</dc:creator>
    </item>
    <item>
      <title>Subexponential and Parameterized Mixing Times of Glauber Dynamics on Independent Sets</title>
      <link>https://arxiv.org/abs/2504.18427</link>
      <description>arXiv:2504.18427v2 Announce Type: replace 
Abstract: Given a graph $G$, the hard-core model defines a probability distribution over its independent sets, assigning to each set of size $k$ a probability of $\frac{\lambda^k}{Z}$, where $\lambda&gt;0$ is a parameter known as the \emph{fugacity} and $Z$ is a normalization constant. The Glauber dynamics is a simple Markov chain that converges to this distribution and enables efficient sampling. Its \emph{mixing time}, the number of steps needed to approach the stationary distribution, has been widely studied across various graph classes, with most previous work emphasizing the dichotomy between polynomial and exponential mixing times, with a particular focus on sparse classes of graphs. Inspired by the modern fine-grained approach to computational complexity, we investigate subexponential mixing times of the Glauber dynamics on geometric intersection graphs, such as disk graphs. We further study parameterized mixing times with respect to two structural parameters that can remain small even in dense graphs: the tree-independence number and the path-independence number. Building on a result of Dyer, Greenhill, and M\"uller, we show that Glauber dynamics mixes in polynomial time on graphs of bounded path-independence number, and in quasi-polynomial time when the tree-independence number is bounded. Moreover, we prove that both bounds are tight via a conductance argument, thereby resolving a question raised in their work. This work provides a simple and efficient algorithm for sampling from the hard-core model. Unlike classical approaches that rely explicitly on geometric representations or on constructing decompositions such as tree decompositions or separator trees, our analysis only requires their existence to establish mixing time bounds-these structures are not used directly by the algorithm itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18427v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malory Marin</dc:creator>
    </item>
    <item>
      <title>Finding Colorings in One-Sided Expanders</title>
      <link>https://arxiv.org/abs/2508.02825</link>
      <description>arXiv:2508.02825v2 Announce Type: replace 
Abstract: We establish new algorithmic guarantees with matching hardness results for coloring and independent set problems in one-sided expanders and related classes of graphs. For example, given a $3$-colorable regular one-sided expander, we compute in polynomial time either an independent set of relative size at least $1/2-o(1)$ or a proper $3$-coloring for all but an $o(1)$ fraction of the vertices, where $o(1)$ stands for a function that tends to $0$ with the second largest eigenvalue of the normalized adjacency matrix. This result improves on recent seminal work of Bafna, Hsieh, and Kothari (STOC 2025) developing an algorithm that efficiently finds independent sets of relative size at least $0.01$ in such graphs. We also obtain an efficient $1.6667$-factor approximation algorithm for VERTEX COVER in sufficiently strong regular one-sided expanders, improving over a previous $(2-\epsilon)$-factor approximation in such graphs for an unspecified constant $\epsilon&gt;0$.
  We propose a new stratification of $k$-COLORING in terms of $k$-by-$k$ matrices akin to predicate sets for constraint satisfaction problems. We prove that whenever this matrix has repeated rows, the corresponding coloring problem is NP-hard for one-sided expanders under the Unique Games Conjecture. On the other hand, if this matrix has no repeated rows, our algorithms can solve the corresponding coloring problem on one-sided expanders in polynomial time.
  As starting point for our algorithmic results, we show a property of graph spectra that, to the best of our knowledge, has not been observed before: The number of negative eigenvalues smaller than $-\tau$ is at most $O(1/\tau^{4})$ times the number of eigenvalues larger than $\tau^{2}/2$. While this result allows us to bound the number of eigenvalues bounded away from $0$ in one-sided spectral expanders, this property alone is insufficient for our algorithmic results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02825v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rares-Darius Buhai, Yiding Hua, David Steurer, Andor V\'ari-Kakas</dc:creator>
    </item>
    <item>
      <title>Feature-aware manifold meshing and remeshing of point clouds and polyhedral surfaces with guaranteed smallest edge length</title>
      <link>https://arxiv.org/abs/2305.07570</link>
      <description>arXiv:2305.07570v2 Announce Type: replace-cross 
Abstract: Point clouds and polygonal meshes are widely used when modeling real-world scenarios. Here, point clouds arise, for instance, from acquisition processes applied in various surroundings, such as reverse engineering, rapid prototyping, or cultural preservation. Based on these raw data, polygonal meshes are created to, for example, run various simulations. For such applications, the utilized meshes must be of high quality. This paper presents an algorithm to derive triangle meshes from unstructured point clouds. The occurring edges have a close to uniform length and their lengths are bounded from below. Theoretical results guarantee the output to be manifold, provided suitable input and parameter choices. Further, the paper presents several experiments establishing that the algorithms can compete with widely used competitors in terms of quality of the output and timing and the output is stable under moderate levels of noise. Additionally, we expand the algorithm to detect and respect features on point clouds as well as to remesh polyhedral surfaces, possibly with features.
  Supplementary material, an extended preprint, a link to a previously published version of the article, utilized models, and implementation details are made available online: https://ms-math-computer.science/projects/guaranteed-smallest-edge-length-manifold-meshing.html</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07570v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611978001.1 10.1016/j.cad.2025.104010</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2024 International Meshing Roundtable (IMR); Special Issue of Computer Aided Geometric Design</arxiv:journal_reference>
      <dc:creator>Henriette Lipsch\"utz, Ulrich Reitebuch, Konrad Polthier, Martin Skrodzki</dc:creator>
    </item>
    <item>
      <title>Enumeration and updates for conjunctive linear algebra queries through expressibility</title>
      <link>https://arxiv.org/abs/2310.04118</link>
      <description>arXiv:2310.04118v4 Announce Type: replace-cross 
Abstract: Due to the importance of linear algebra and matrix operations in data analytics, there is significant interest in using relational query optimization and processing techniques for evaluating (sparse) linear algebra programs. In particular, in recent years close connections have been established between linear algebra programs and relational algebra that allow transferring optimization techniques of the latter to the former. In this paper, we ask ourselves which linear algebra programs in MATLANG correspond to the free-connex and q-hierarchical fragments of conjunctive first-order logic. Both fragments have desirable query processing properties: free-connex conjunctive queries support constant-delay enumeration after a linear-time preprocessing phase, and q-hierarchical conjunctive queries further allow constant-time updates. By characterizing the corresponding fragments of MATLANG, we hence identify the fragments of linear algebra programs that one can evaluate with constant-delay enumeration after linear-time preprocessing and with constant-time updates. To derive our results, we improve and generalize previous correspondences between MATLANG and relational algebra evaluated over semiring-annotated relations. In addition, we identify properties on semirings that allow to generalize the complexity bounds for free-connex and q-hierarchical conjunctive queries from Boolean annotations to general semirings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04118v4</guid>
      <category>cs.CC</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Mu\~noz, Cristian Riveros, Stijn Vansummeren</dc:creator>
    </item>
  </channel>
</rss>
