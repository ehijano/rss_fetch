<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Apr 2025 01:46:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Analyzing homogenous and heterogeneous multi-server queues via neural networks</title>
      <link>https://arxiv.org/abs/2504.01052</link>
      <description>arXiv:2504.01052v1 Announce Type: new 
Abstract: In this paper, we use a machine learning approach to predict the stationary distributions of the number of customers in a single-staiton multi server system. We consider two systems, the first is $c$ homogeneous servers, namely the $GI/GI/c$ queue. The second is a two-heterogeneous server system, namely the $GI/GI_i/2$ queue. We train a neural network for these queueing models, using the first four inter-arrival and service time moments. We demonstrate empirically that using the fifth moment and beyond does not increase accuracy.
  Compared to existing methods, we show that in terms of the stationary distribution and the mean value of the number of customers in a $GI/GI/c$ queue, we are state-of-the-art. Further, we are the only ones to predict the stationary distribution of the number of customers in the system in a $GI/GI_i/2$ queue. We conduct a thorough performance evaluation to assert that our model is accurate. In most cases, we demonstrate that our error is less than 5\%. Finally, we show that making inferences is very fast, where 5000 inferences can be made in parallel within a fraction of a second.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01052v1</guid>
      <category>cs.PF</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eliran Sherzer</dc:creator>
    </item>
    <item>
      <title>GigaAPI for GPU Parallelization</title>
      <link>https://arxiv.org/abs/2504.01266</link>
      <description>arXiv:2504.01266v1 Announce Type: cross 
Abstract: GigaAPI is a user-space API that simplifies multi-GPU programming, bridging the gap between the capabilities of parallel GPU systems and the ability of developers to harness their full potential. The API offers a comprehensive set of functionalities, including fundamental GPU operations, image processing, and complex GPU tasks, abstracting away the intricacies of low-level CUDA and C++ programming. GigaAPI's modular design aims to inspire future NVIDIA researchers to create a generalized, dynamic, extensible, and cross-GPU architecture-compatible API. Through experiments and simulations, we demonstrate the general efficiency gains achieved by leveraging GigaAPI's simplified multi-GPU programming model and showcase our learning experience through setup and other aspects, as we were interested in learning complex CUDA programming and parallelism. We hope that this contributes to the democratization of parallel GPU computing, enabling researchers and practitioners to unlock new possibilities across diverse domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01266v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Suvarna, O. Tehrani</dc:creator>
    </item>
    <item>
      <title>Should AI Optimize Your Code? A Comparative Study of Classical Optimizing Compilers Versus Current Large Language Models</title>
      <link>https://arxiv.org/abs/2406.12146</link>
      <description>arXiv:2406.12146v2 Announce Type: replace-cross 
Abstract: Traditional optimizing compilers have played an important role in adapting to the growing complexity of modern software systems. The need for efficient parallel programming in current architectures requires strong optimization techniques. The beginning of Large Language Models (LLMs) raises intriguing questions about the potential of these AI approaches to revolutionize code optimization methodologies. This work aims to answer an essential question for the compiler community: "Can AI-driven models revolutionize the way we approach code optimization?".
  To address this question, we present a comparative analysis between three classical optimizing compilers and two recent large language models, evaluating their respective abilities and limitations in optimizing code for maximum efficiency. In addition, we introduce a benchmark suite of challenging optimization patterns and an automatic mechanism for evaluating the performance and correctness of the code generated by LLMs. We used three different prompting strategies to evaluate the performance of the LLMs, Simple Instruction (IP), Detailed Instruction Prompting (DIP), and Chain of Thought (CoT).
  A key finding is that while LLMs have the potential to outperform current optimizing compilers, they often generate incorrect code on large code sizes, calling for automated verification methods. In addition, expressing a compiler strategy as part of the LLMs prompt substantially improves its overall performance. Our evaluation across three benchmark suites shows CodeLlama-70B as the superior LLM, capable of achieving speedups of up to x1.75. Additionally, CETUS is the best among the current optimizing compilers, achieving a maximum speedup of 1.67x. We also found substantial differences among the three prompting strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12146v2</guid>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miguel Romero Rosas, Miguel Torres Sanchez, Rudolf Eigenmann</dc:creator>
    </item>
  </channel>
</rss>
