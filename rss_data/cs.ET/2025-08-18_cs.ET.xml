<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 18 Aug 2025 04:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>RAG for Geoscience: What We Expect, Gaps and Opportunities</title>
      <link>https://arxiv.org/abs/2508.11246</link>
      <description>arXiv:2508.11246v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enhances language models by combining retrieval with generation. However, its current workflow remains largely text-centric, limiting its applicability in geoscience. Many geoscientific tasks are inherently evidence-hungry. Typical examples involve imputing missing observations using analog scenes, retrieving equations and parameters to calibrate models, geolocating field photos based on visual cues, or surfacing historical case studies to support policy analyses. A simple ``retrieve-then-generate'' pipeline is insufficient for these needs. We envision Geo-RAG, a next-generation paradigm that reimagines RAG as a modular retrieve $\rightarrow$ reason $\rightarrow$ generate $\rightarrow$ verify loop. Geo-RAG supports four core capabilities: (i) retrieval of multi-modal Earth data; (ii) reasoning under physical and domain constraints; (iii) generation of science-grade artifacts; and (iv) verification of generated hypotheses against numerical models, ground measurements, and expert assessments. This shift opens new opportunities for more trustworthy and transparent geoscience workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11246v1</guid>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Runlong Yu, Shiyuan Luo, Rahul Ghosh, Lingyao Li, Yiqun Xie, Xiaowei Jia</dc:creator>
    </item>
    <item>
      <title>Banking 2.0: The Stablecoin Banking Revolution -- How Digital Assets Are Reshaping Global Finance</title>
      <link>https://arxiv.org/abs/2508.11395</link>
      <description>arXiv:2508.11395v1 Announce Type: new 
Abstract: The global financial system stands at an inflection point. Stablecoins represent the most significant evolution in banking since the abandonment of the gold standard, positioned to enable "Banking 2.0" by seamlessly integrating cryptocurrency innovation with traditional finance infrastructure. This transformation rivals artificial intelligence as the next major disruptor in the financial sector. Modern fiat currencies derive value entirely from institutional trust rather than physical backing, creating vulnerabilities that stablecoins address through enhanced stability, reduced fraud risk, and unified global transactions that transcend national boundaries. Recent developments demonstrate accelerating institutional adoption: landmark U.S. legislation including the GENIUS Act of 2025, strategic industry pivots from major players like JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive "Pay with Crypto" service. Widespread stablecoin implementation addresses critical macroeconomic imbalances, particularly the inflation-productivity gap plaguing modern monetary systems, through more robust and diversified backing mechanisms. Furthermore, stablecoins facilitate deregulation and efficiency gains, paving the way for a more interconnected international financial system. This whitepaper comprehensively explores how stablecoins are poised to reshape banking, supported by real-world examples, current market data, and analysis of their transformative potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11395v1</guid>
      <category>cs.ET</category>
      <category>cs.CE</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin McNamara, Rhea Pritham Marpu</dc:creator>
    </item>
    <item>
      <title>Open Questions about Time and Self-reference in Living Systems</title>
      <link>https://arxiv.org/abs/2508.11423</link>
      <description>arXiv:2508.11423v1 Announce Type: new 
Abstract: Living systems exhibit a range of fundamental characteristics: they are active, self-referential, self-modifying systems. This paper explores how these characteristics create challenges for conventional scientific approaches and why they require new theoretical and formal frameworks. We introduce a distinction between 'natural time', the continuing present of physical processes, and 'representational time', with its framework of past, present and future that emerges with life itself. Representational time enables memory, learning and prediction, functions of living systems essential for their survival. Through examples from evolution, embryogenesis and metamorphosis we show how living systems navigate the apparent contradictions arising from self-reference as natural time unwinds self-referential loops into developmental spirals. Conventional mathematical and computational formalisms struggle to model self-referential and self-modifying systems without running into paradox. We identify promising new directions for modelling self-referential systems, including domain theory, co-algebra, genetic programming, and self-modifying algorithms. There are broad implications for biology, cognitive science and social sciences, because self-reference and self-modification are not problems to be avoided but core features of living systems that must be modelled to understand life's open-ended creativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11423v1</guid>
      <category>cs.ET</category>
      <category>q-bio.OT</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samson Abramsky, Wolfgang Banzhaf, Leo S. D. Caves, Michael Levin, Penousal Machado, Charles Ofria, Susan Stepney, Roger White</dc:creator>
    </item>
    <item>
      <title>CoMoNM: A Cost Modeling Framework for Compute-Near-Memory Systems</title>
      <link>https://arxiv.org/abs/2508.11451</link>
      <description>arXiv:2508.11451v1 Announce Type: new 
Abstract: Compute-Near-Memory (CNM) systems offer a promising approach to mitigate the von Neumann bottleneck by bringing computational units closer to data. However, optimizing for these architectures remains challenging due to their unique hardware and programming models. Existing CNM compilers often rely on manual programmer annotations for offloading and optimizations. Automating these decisions by exploring the optimization space, common in CPU/GPU systems, is difficult for CNMs as constructing and navigating the transformation space is tedious and time consuming. This is particularly the case during system-level design, where evaluation requires time-consuming simulations. To address this, we present CoMoNM, a generic cost modeling framework for CNM systems for execution time estimation in milliseconds. It takes a high-level, hardware-agnostic application representation, target system specifications, and a mapping specification as input and estimates the execution time for the given application on the target CNM system. We show how CoMoNM can be seamlessly integrated into state-of-the-art CNM compilers, providing improved offloading decisions. Evaluation on established benchmarks for CNM shows estimation errors within 7.80% and 2.99%, when compared to the real UPMEM CNM system and Samsung's HBM-PIM simulator. Notably, CoMoNM delivers estimates seven orders of magnitude faster compared to the UPMEM and HBM-PIM simulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11451v1</guid>
      <category>cs.ET</category>
      <category>cs.PL</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamid Farzaneh, Asif Ali Khan, Jeronimo Castrillon</dc:creator>
    </item>
    <item>
      <title>Insect-Wing Structured Microfluidic System for Reservoir Computing</title>
      <link>https://arxiv.org/abs/2508.10915</link>
      <description>arXiv:2508.10915v1 Announce Type: cross 
Abstract: As the demand for more efficient and adaptive computing grows, nature-inspired architectures offer promising alternatives to conventional electronic designs. Microfluidic platforms, drawing on biological forms and fluid dynamics, present a compelling foundation for low-power, high-resilience computing in environments where electronics are unsuitable. This study explores a hybrid reservoir computing system based on a dragonfly-wing inspired microfluidic chip, which encodes temporal input patterns as fluid interactions within the micro channel network.
  The system operates with three dye-based inlet channels and three camera-monitored detection areas, transforming discrete spatial patterns into dynamic color output signals. These reservoir output signals are then modified and passed to a simple and trainable readout layer for pattern classification. Using a combination of raw reservoir outputs and synthetically generated outputs, we evaluated system performance, system clarity, and data efficiency. The results demonstrate consistent classification accuracies up to $91\%$, even with coarse resolution and limited training data, highlighting the viability of the microfluidic reservoir computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10915v1</guid>
      <category>cs.NE</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Clouse (School of Computing, University of Nebraska-Lincoln, Lincoln, Nebraska, USA), Thomas Ramsey (Department of Mechanical and Materials Engineering, University of Nebraska-Lincoln, Lincoln, Nebraska, USA), Samitha Somathilaka (School of Computing, University of Nebraska-Lincoln, Lincoln, Nebraska, USA), Nicholas Kleinsasser (School of Computing, University of Nebraska-Lincoln, Lincoln, Nebraska, USA), Sangjin Ryu (Department of Mechanical and Materials Engineering, University of Nebraska-Lincoln, Lincoln, Nebraska, USA), Sasitharan Balasubramaniam (School of Computing, University of Nebraska-Lincoln, Lincoln, Nebraska, USA)</dc:creator>
    </item>
    <item>
      <title>Stories and Systems: Educational Interactive Storytelling to Teach Media Literacy and Systemic Thinking</title>
      <link>https://arxiv.org/abs/2508.11059</link>
      <description>arXiv:2508.11059v1 Announce Type: cross 
Abstract: This paper explores how Interactive Digital Narratives (IDNs) can support learners in developing the critical literacies needed to address complex societal challenges, so-called wicked problems, such as climate change, pandemics, and social inequality. While digital technologies offer broad access to narratives and data, they also contribute to misinformation and the oversimplification of interconnected issues. IDNs enable learners to navigate nonlinear, interactive stories, fostering deeper understanding and engagement. We introduce Systemic Learning IDNs: interactive narrative experiences explicitly designed to help learners explore and reflect on complex systems and interdependencies. To guide their creation and use, we propose the CLASS framework, a structured model that integrates systems thinking, design thinking, and storytelling. This transdisciplinary approach supports learners in developing curiosity, critical thinking, and collaborative problem-solving. Focusing on the classroom context, we apply CLASS to two cases, one commercial narrative simulation and one educational prototype, offering a comparative analysis and practical recommendations for future design and implementation. By combining narrative, systems mapping, and participatory design, this paper highlights how IDNs can become powerful tools for transformative, systems-oriented learning in an increasingly complex world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11059v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Roth, Rahmin Bender-Salazar, Breanne Pitt</dc:creator>
    </item>
    <item>
      <title>OpenCXD: An Open Real-Device-Guided Hybrid Evaluation Framework for CXL-SSDs</title>
      <link>https://arxiv.org/abs/2508.11477</link>
      <description>arXiv:2508.11477v1 Announce Type: cross 
Abstract: The advent of Compute Express Link (CXL) enables SSDs to participate in the memory hierarchy as large-capacity, byte-addressable memory devices. These CXL-enabled SSDs (CXL-SSDs) offer a promising new tier between DRAM and traditional storage, combining NAND flash density with memory-like access semantics. However, evaluating the performance of CXL-SSDs remains difficult due to the lack of hardware that natively supports the CXL.mem protocol on SSDs. As a result, most prior work relies on hybrid simulators combining CPU models augmented with CXL.mem semantics and SSD simulators that approximate internal flash behaviors. While effective for early-stage exploration, this approach cannot faithfully model firmware-level interactions and low-level storage dynamics critical to CXL-SSD performance. In this paper, we present OpenCXD, a real-device-guided hybrid evaluation framework that bridges the gap between simulation and hardware. OpenCXD integrates a cycle-accurate CXL.mem simulator on the host side with a physical OpenSSD platform running real firmware. This enables in-situ firmware execution triggered by simulated memory requests. Through these contributions, OpenCXD reflects device-level phenomena unobservable in simulation-only setups, providing critical insights for future firmware design tailored to CXL-SSDs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11477v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.OS</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunsun Chung, Junhyeok Park, Taewan Noh, Seonghoon Ahn, Kihwan Kim, Ming Zhao, Youngjae Kim</dc:creator>
    </item>
    <item>
      <title>DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring</title>
      <link>https://arxiv.org/abs/2508.11591</link>
      <description>arXiv:2508.11591v1 Announce Type: cross 
Abstract: Our study introduces a novel, low-cost, and reproducible framework for real-time, object-level structural assessment and geolocation of roadside vegetation and infrastructure with commonly available but underutilized dashboard camera (dashcam) video data. We developed an end-to-end pipeline that combines monocular depth estimation, depth error correction, and geometric triangulation to generate accurate spatial and structural data from street-level video streams from vehicle-mounted dashcams. Depth maps were first estimated using a state-of-the-art monocular depth model, then refined via a gradient-boosted regression framework to correct underestimations, particularly for distant objects. The depth correction model achieved strong predictive performance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly reducing bias beyond 15 m. Further, object locations were estimated using GPS-based triangulation, while object heights were calculated using pin hole camera geometry. Our method was evaluated under varying conditions of camera placement and vehicle speed. Low-speed vehicle with inside camera gave the highest accuracy, with mean geolocation error of 2.83 m, and mean absolute error (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To the best of our knowledge, it is the first framework to combine monocular depth modeling, triangulated GPS-based geolocation, and real-time structural assessment for urban vegetation and infrastructure using consumer-grade video data. Our approach complements conventional RS methods, such as LiDAR and image by offering a fast, real-time, and cost-effective solution for object-level monitoring of vegetation risks and infrastructure exposure, making it especially valuable for utility companies, and urban planners aiming for scalable and frequent assessments in dynamic urban environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11591v1</guid>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Durga Joshi (Department of Natural Resources and the Environment, Eversource Energy Center, University of Connecticut, Storrs, CT, USA), Chandi Witharana (Department of Natural Resources and the Environment, Eversource Energy Center, University of Connecticut, Storrs, CT, USA), Robert Fahey (Department of Natural Resources and the Environment, Eversource Energy Center, University of Connecticut, Storrs, CT, USA), Thomas Worthley (Department of Natural Resources and the Environment, Eversource Energy Center, University of Connecticut, Storrs, CT, USA), Zhe Zhu (Department of Natural Resources and the Environment, Eversource Energy Center, University of Connecticut, Storrs, CT, USA), Diego Cerrai (Department of Civil and Environmental Engineering, Eversource Energy Center, University of Connecticut, Storrs, CT, USA)</dc:creator>
    </item>
    <item>
      <title>Quantum Annealing for Enhanced Feature Selection in Single-Cell RNA Sequencing Data Analysis</title>
      <link>https://arxiv.org/abs/2408.08867</link>
      <description>arXiv:2408.08867v3 Announce Type: replace-cross 
Abstract: Feature selection is a machine learning technique for identifying relevant variables in classification and regression models. In single-cell RNA sequencing (scRNA-seq) data analysis, feature selection is used to identify relevant genes that are crucial for understanding cellular processes. Traditional feature selection methods often struggle with the complexity of scRNA-seq data and suffer from interpretation difficulties. Quantum annealing presents a promising alternative approach. In this study, we implement quantum annealing-empowered quadratic unconstrained binary optimization (QUBO) for feature selection in scRNA-seq data. Using data from a human cell differentiation system and an anticancer drug resistance study, we demonstrate that QUBO feature selection effectively identifies genes whose expression patterns reflect critical cell state transitions associated with differentiation and drug resistance development. Our findings indicate that quantum annealing-powered QUBO reveals complex gene expression patterns potentially missed by traditional methods, thereby enhancing scRNA-seq data analysis and interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08867v3</guid>
      <category>q-bio.GN</category>
      <category>cs.ET</category>
      <category>physics.bio-ph</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Selim Romero, Shreyan Gupta, Victoria Gatlin, Robert S. Chapkin, James J. Cai</dc:creator>
    </item>
    <item>
      <title>The Price of Cheaper Data: Measuring the Strategic Inefficiencies in the Post-EIP-4844 Ethereum Market</title>
      <link>https://arxiv.org/abs/2411.03892</link>
      <description>arXiv:2411.03892v3 Announce Type: replace-cross 
Abstract: High transaction fees on Ethereum have long hindered its scalability and user adoption. The recent Ethereum EIP-4844 upgrade aims to alleviate the scalability issue by introducing the blob, a new data structure for Layer-2 rollups. Instead of using expensive blockchain storage, blobs provide a cheaper, separate data layer with its own fee market, which drastically lowers data availability costs. This change, while lowering transaction costs, has created a new high-stakes economic game for block builders and rollups. However, the dynamics of this game remain poorly understood. In this paper, we conduct the first large-scale empirical analysis of the post-EIP-4844 ecosystem, leveraging a dataset of 319.5 million transactions, including 1.3 million blob-carrying type-3 transactions. Our analysis demonstrates clear evidence of a structural shift towards the utilization of cheap blobs over expensive transactions for Layer-2 data posting: while average block size grew 2.5 times, the space consumed by expensive transactions in the public mempool shrank from over 150 KB to just 30 KB. Yet, this scalability success masks widespread economic inefficiencies on both sides of the market. On the builder side, 29.48% of blocks containing blobs are constructed sub-optimally, earning less revenue than that could be achieved otherwise. On the rollup side, we identify that flawed submission strategies have led to $186.92$ ETH in direct losses and average inclusion delays of over 19 seconds. Moving beyond characterization, our work offers actionable solutions. For builders, we develop an optimal pricing model derived from a formal first-price auction framework, allowing builders to make provably profitable inclusion decisions. For rollups, we prove that batching multiple blobs into a single transaction is a dominant, utility-maximizing strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03892v3</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue Huang, Shuzheng Wang, Liang Du, Chuxuan Zeng, Ling Deng, Yuming Huang, Gareth Tyson, Jing Tang</dc:creator>
    </item>
  </channel>
</rss>
