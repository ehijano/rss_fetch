<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 02:20:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>TRUST: Transparent, Robust and Ultra-Sparse Trees</title>
      <link>https://arxiv.org/abs/2506.15791</link>
      <description>arXiv:2506.15791v1 Announce Type: new 
Abstract: Piecewise-constant regression trees remain popular for their interpretability, yet often lag behind black-box models like Random Forest in predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and Ultra-Sparse Trees), a novel regression tree model that combines the accuracy of Random Forests with the interpretability of shallow decision trees and sparse linear models. TRUST further enhances transparency by leveraging Large Language Models to generate tailored, user-friendly explanations. Extensive validation on synthetic and real-world benchmark datasets demonstrates that TRUST consistently outperforms other interpretable models -- including CART, Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy of Random Forest and offering substantial gains in both accuracy and interpretability over M5', a well-established model that is conceptually related.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15791v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Albert Dorador</dc:creator>
    </item>
    <item>
      <title>Network Modelling of Asynchronous Change-Points in Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2506.15801</link>
      <description>arXiv:2506.15801v1 Announce Type: new 
Abstract: This article introduces a novel Bayesian method for asynchronous change-point detection in multivariate time series. This method allows for change-points to occur earlier in some series (leading series) followed, after a short delay, by change-points in some other series (lagging series). Such dynamic dependence structure is common in fields such as seismology and neurology where a latent event such as an earthquake or seizure causes certain sensors to register change-points before others. We model these lead-lag dependencies via a latent directed graph and provide a hierarchical prior for learning the graph's structure and parameters. Posterior inference is made tractable by modifying particle MCMC methods designed for univariate change-point problems. We apply our method to both simulated and real datasets from the fields of seismology and neurology. In the simulated data, we find that our method outperforms competing Bayesian methods in settings where the change-point locations are dependent across series. In the real data applications we show that our model can also uncover interpretable network structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15801v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carson McKee, Maria Kalli</dc:creator>
    </item>
    <item>
      <title>Summary Statistics of Large-scale Model Outputs for Observation-corrected Outputs</title>
      <link>https://arxiv.org/abs/2506.15845</link>
      <description>arXiv:2506.15845v1 Announce Type: new 
Abstract: Physics-based models capture broad spatial and temporal dynamics, but often suffer from biases and numerical approximations, while observations capture localized variability but are sparse. Integrating these complementary data modalities is important to improving the accuracy and reliability of model outputs. Meanwhile, physics-based models typically generate large outputs that are challenging to manipulate. In this paper, we propose Sig-PCA, a space-time framework that integrates summary statistics from model outputs with localized observations via a neural network (NN). By leveraging reduced-order representations from physics-based models and integrating them with observational data, our approach corrects model outputs, while allowing to work with dimensionally-reduced quantities hence with smaller NNs. This framework highlights the synergy between observational data and statistical summaries of model outputs, and effectively combines multisource data by preserving essential statistical information. We demonstrate our approach on two datasets (surface temperature and surface wind) with different statistical properties and different ratios of model to observational data. Our method corrects model outputs to align closely with the observational data, specifically enabling to correct probability distributions and space-time correlation structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15845v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atlanta Chakraborty, Julie Bessac</dc:creator>
    </item>
    <item>
      <title>Sample size re-estimation in blinded hybrid-control design using inverse probability weighting</title>
      <link>https://arxiv.org/abs/2506.15913</link>
      <description>arXiv:2506.15913v1 Announce Type: new 
Abstract: With the increasing availability of data from historical studies and real-world data sources, hybrid control designs that incorporate external data into the evaluation of current studies are being increasingly adopted. In these designs, it is necessary to pre-specify during the planning phase the extent to which information will be borrowed from historical control data. However, if substantial differences in baseline covariate distributions between the current and historical studies are identified at the final analysis, the amount of effective borrowing may be limited, potentially resulting in lower actual power than originally targeted. In this paper, we propose two sample size re-estimation strategies that can be applied during the course of the blinded current study. Both strategies utilize inverse probability weighting (IPW) based on the probability of assignment to either the current or historical study. When large discrepancies in baseline covariates are detected, the proposed strategies adjust the sample size upward to prevent a loss of statistical power. The performance of the proposed strategies is evaluated through simulation studies, and their practical implementation is demonstrated using a case study based on two actual randomized clinical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15913v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Kojima, Shunichiro Orihara, Keisuke Hanada, Tomohiro Ohigashi</dc:creator>
    </item>
    <item>
      <title>Weighted Parameter Estimators of the Generalized Extreme Value Distribution in the Presence of Missing Observations</title>
      <link>https://arxiv.org/abs/2506.15964</link>
      <description>arXiv:2506.15964v1 Announce Type: new 
Abstract: Missing data occur in a variety of applications of extreme value analysis. In the block maxima approach to an extreme value analysis, missingness is often handled by either ignoring missing observations or dropping a block of observations from the analysis. However, in some cases, missingness may occur due to equipment failure during an extreme event, which can lead to bias in estimation. In this work, we propose weighted maximum likelihood and weighted moment-based estimators for the generalized extreme value distribution parameters to account for the presence of missing observations. We validate the procedures through an extensive simulation study and apply the estimation methods to data from multiple tidal gauges on the Eastern coast of Canada.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15964v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James H. McVittie, Orla A. Murphy</dc:creator>
    </item>
    <item>
      <title>Estimating Extreme Wave Surges in the Presence of Missing Data</title>
      <link>https://arxiv.org/abs/2506.15970</link>
      <description>arXiv:2506.15970v1 Announce Type: new 
Abstract: The block maxima approach, which consists of dividing a series of observations into equal sized blocks to extract the block maxima, is commonly used for identifying and modelling extreme events using the generalized extreme value (GEV) distribution. In the analysis of coastal wave surge levels, the underlying data which generate the block maxima typically have missing observations. Consequently, the observed block maxima may not correspond to the true block maxima yielding biased estimates of the GEV distribution parameters. Various parametric modelling procedures are proposed to account for the presence of missing observations under a block maxima framework. The performance of these estimators is compared through an extensive simulation study and illustrated by an analysis of extreme wave surges in Atlantic Canada.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15970v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James H. McVittie, Orla A. Murphy</dc:creator>
    </item>
    <item>
      <title>Leveraging Optimal Transport for Distributed Two-Sample Testing: An Integrated Transportation Distance-based Framework</title>
      <link>https://arxiv.org/abs/2506.16047</link>
      <description>arXiv:2506.16047v1 Announce Type: new 
Abstract: This paper introduces a novel framework for distributed two-sample testing using the Integrated Transportation Distance (ITD), an extension of the Optimal Transport distance. The approach addresses the challenges of detecting distributional changes in decentralized learning or federated learning environments, where data privacy and heterogeneity are significant concerns. We provide theoretical foundations for the ITD, including convergence properties and asymptotic behavior. A permutation test procedure is proposed for practical implementation in distributed settings, allowing for efficient computation while preserving data privacy. The framework's performance is demonstrated through theoretical power analysis and extensive simulations, showing robust Type I error control and high power across various distributions and dimensions. The results indicate that ITD effectively aggregates information across distributed clients, detecting subtle distributional shifts that might be missed when examining individual clients. This work contributes to the growing field of distributed statistical inference, offering a powerful tool for two-sample testing in modern, decentralized data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16047v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengqi Lin, Yan Chen</dc:creator>
    </item>
    <item>
      <title>Independent vector analysis -- an introduction for statisticians</title>
      <link>https://arxiv.org/abs/2506.16175</link>
      <description>arXiv:2506.16175v1 Announce Type: new 
Abstract: Blind source separation (BSS), particularly independent component analysis (ICA), has been widely used in various fields of science such as biomedical signal processing to recover latent source signals from the observed mixture. While ICA is typically applied to individual datasets, many real-world applications share underlying sources across datasets. Independent vector analysis (IVA) extends ICA to jointly analyze multiple datasets by exploiting statistical dependencies across them. While various IVA methods have been presented in signal processing literature, the statistical properties of methods remains largely unexplored. This article introduces the IVA model, numerous density models used in IVA, and various classical IVA methods to statistics community highlighting the need for further theoretical developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16175v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miro Arvila, Klaus Nordhausen, Mika Sipil\"a, Sara Taskinen</dc:creator>
    </item>
    <item>
      <title>Diversifying Conformal Selections</title>
      <link>https://arxiv.org/abs/2506.16229</link>
      <description>arXiv:2506.16229v1 Announce Type: new 
Abstract: When selecting from a list of potential candidates, it is important to ensure not only that the selected items are of high quality, but also that they are sufficiently dissimilar so as to both avoid redundancy and to capture a broader range of desirable properties. In drug discovery, scientists aim to select potent drugs from a library of unsynthesized candidates, but recognize that it is wasteful to repeatedly synthesize highly similar compounds. In job hiring, recruiters may wish to hire candidates who will perform well on the job, while also considering factors such as socioeconomic background, prior work experience, gender, or race. We study the problem of using any prediction model to construct a maximally diverse selection set of candidates while controlling the false discovery rate (FDR) in a model-free fashion. Our method, diversity-aware conformal selection (DACS), achieves this by designing a general optimization procedure to construct a diverse selection set subject to a simple constraint involving conformal e-values which depend on carefully chosen stopping times. The key idea of DACS is to use optimal stopping theory to adaptively choose the set of e-values which (approximately) maximizes the expected diversity measure. We give an example diversity metric for which our procedure can be run exactly and efficiently. We also develop a number of computational heuristics which greatly improve its running time for generic diversity metrics. We demonstrate the empirical performance of our method both in simulation and on job hiring and drug discovery datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16229v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yash Nair, Ying Jin, James Yang, Emmanuel Candes</dc:creator>
    </item>
    <item>
      <title>An introduction to Causal Modelling</title>
      <link>https://arxiv.org/abs/2506.16486</link>
      <description>arXiv:2506.16486v1 Announce Type: new 
Abstract: This tutorial provides a concise introduction to modern causal modeling by integrating potential outcomes and graphical methods. We motivate causal questions such as counterfactual reasoning under interventions and define binary treatments and potential outcomes. We discuss causal effect measures-including average treatment effects on the treated and on the untreated-and choices of effect scales for binary outcomes. We derive identification in randomized experiments under exchangeability and consistency, and extend to stratification and blocking designs. We present inverse probability weighting with propensity score estimation and robust inference via sandwich estimators. Finally, we introduce causal graphs, d-separation, the backdoor criterion, single-world intervention graphs, and structural equation models, showing how graphical and potential-outcome approaches complement each other. Emphasis is placed on clear notation, intuitive explanations, and practical examples for applied researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16486v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gauranga Kumar Baishya</dc:creator>
    </item>
    <item>
      <title>Bayesian Semiparametric Orthogonal Tucker Factorized Mixed Models for Multi-dimensional Longitudinal Functional Data</title>
      <link>https://arxiv.org/abs/2506.16668</link>
      <description>arXiv:2506.16668v1 Announce Type: new 
Abstract: We introduce a novel longitudinal mixed model for analyzing complex multidimensional functional data, addressing challenges such as high-resolution, structural complexities, and computational demands. Our approach integrates dimension reduction techniques, including basis function representation and Tucker tensor decomposition, to model complex functional (e.g., spatial and temporal) variations, group differences, and individual heterogeneity while drastically reducing model dimensions. The model accommodates multiplicative random effects whose marginalization yields a novel Tucker-decomposed covariance-tensor framework. To ensure scalability, we employ semi-orthogonal mode matrices implemented via a novel graph-Laplacian-based smoothness prior with low-rank approximation, leading to an efficient posterior sampling method. A cumulative shrinkage strategy promotes sparsity and enables semiautomated rank selection. We establish theoretical guarantees for posterior convergence and demonstrate the method's effectiveness through simulations, showing significant improvements over existing techniques. Applying the method to Alzheimer's Disease Neuroimaging Initiative (ADNI) neuroimaging data reveals novel insights into local brain changes associated with disease progression, highlighting the method's practical utility for studying cognitive decline and neurodegenerative conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16668v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Abhra Sarkar</dc:creator>
    </item>
    <item>
      <title>On Bayes factor functions</title>
      <link>https://arxiv.org/abs/2506.16674</link>
      <description>arXiv:2506.16674v1 Announce Type: new 
Abstract: We describe Bayes factors functions based on the sampling distributions of \emph{z}, \emph{t}, $\chi^2$, and \emph{F} statistics, using a class of inverse-moment prior distributions to define alternative hypotheses. These non-local alternative prior distributions are centered on standardized effects, which serve as indices for the Bayes factor function. We compare the conclusions drawn from resulting Bayes factor functions to those drawn from Bayes factors defined using local alternative prior specifications and examine their frequentist operating characteristics. Finally, an application of Bayes factor functions to replicated experimental designs in psychology is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16674v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptati Datta, Riana Guha, Rachael Shudde, Valen E. Johnson</dc:creator>
    </item>
    <item>
      <title>elicito: A Python Package for Expert Prior Elicitation</title>
      <link>https://arxiv.org/abs/2506.16830</link>
      <description>arXiv:2506.16830v1 Announce Type: new 
Abstract: Expert prior elicitation plays a critical role in Bayesian analysis by enabling the specification of prior distributions that reflect domain knowledge. However, expert knowledge often refers to observable quantities rather than directly to model parameters, posing a challenge for translating this information into usable priors. We present elicito, a Python package that implements a modular, simulation-based framework for expert prior elicitation. The framework supports both structural and predictive elicitation methods and allows for flexible customization of key components, including the generative model, the form of expert input, prior assumptions (parametric or nonparametric), and loss functions. By structuring the elicitation process into configurable modules, elicito offers transparency, reproducibility, and comparability across elicitation methods. We describe the methodological foundations of the package, its software architecture, and demonstrate its functionality through a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16830v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Florence Bockting, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Unveiling Complex Territorial Socio-Economic Dynamics: A Statistical Mechanics Approach</title>
      <link>https://arxiv.org/abs/2506.16872</link>
      <description>arXiv:2506.16872v1 Announce Type: new 
Abstract: This study proposes a novel approach based on the Ising model for analyzing the observed territorial configuration of a network of municipalities classified as being central hubs or peripheral areas. This is interpreted as being a reference of a system of interacting territorial binary units. The socio-economic structure of the municipalities is synthesized into interpretable composite indices, which are further aggregated by means of Principal Components Analysis in order to reduce dimensionality and construct a univariate external field compatible with the Ising framework. Monte Carlo simulations via parallel computing are conducted adopting a Simulated Annealing variant of the classic Metropolis-Hastings algorithm. This ensures an efficient local exploration of the configuration space in the neighbourhood of to the reference of the system. Model consistency is assessed both in terms of energy stability and the likelihood of these configurations. The comparison between observed configuration and simulated ones is crucial in the analysis of multivariate phenomena, concomitantly accounting for territorial interactions. Model uncertainty in estimating the probability of each municipality being a central hub or peripheral area is quantified by adopting the model-agnostic Conformal Prediction framework which yields adaptive intervals with guaranteed coverage. The innovative use of geographical maps of the prediction intervals renders this approach an effective tool. It combines statistical mechanics, multivariate analysis and uncertainty quantification, providing a robust and interpretable framework for modeling socio-economic territorial dynamics, with potential applications in Official Statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16872v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierpaolo Massoli</dc:creator>
    </item>
    <item>
      <title>Autoregressive Hypergraph</title>
      <link>https://arxiv.org/abs/2506.16966</link>
      <description>arXiv:2506.16966v1 Announce Type: new 
Abstract: Traditional graph representations are insufficient for modelling real-world phenomena involving multi-entity interactions, such as collaborative projects or protein complexes, necessitating the use of hypergraphs. While hypergraphs preserve the intrinsic nature of such complex relationships, existing models often overlook temporal evolution in relational data. To address this, we introduce a first-order autoregressive (i.e. AR(1)) model for dynamic non-uniform hypergraphs. This is the first dynamic hypergraph model with provable theoretical guarantees, explicitly defining the temporal evolution of hyperedge presence through transition probabilities that govern persistence and change dynamics. This framework provides closed-form expressions for key probabilistic properties and facilitates straightforward maximum-likelihood inference with uniform error bounds and asymptotic normality, along with a permutation-based diagnostic test. We also consider an AR(1) hypergraph stochastic block model (HSBM), where a novel Laplacian enables exact and efficient latent community recovery via a spectral clustering algorithm. Furthermore, we develop a likelihood-based change-point estimator for the HSBM to detect structural breaks within the time series. The efficacy and practical value of our methods are comprehensively demonstrated through extensive simulation studies and compelling applications to a primary school interaction data set and the Enron email corpus, revealing insightful community structures and significant temporal changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16966v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xianghe Zhu, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>A Semi-Parametric Torus-to-Torus Regression Model with Geometric Loss: Application to Cyclone Data</title>
      <link>https://arxiv.org/abs/2506.17014</link>
      <description>arXiv:2506.17014v1 Announce Type: new 
Abstract: This study demonstrates a novel application of torus-to-torus regression in cyclone data analysis, unleashing its potential for wider utilization in directional data modeling. This research, to our knowledge, establishes a mathematical framework for modeling the regression between bivariate angular predictors and bivariate angular responses for the first time in the literature. The proposed model makes use of generalized M\"{o}bius transformation and differential geometry for model building. A new loss function, derived from the intrinsic geometry of the torus, is introduced to facilitate effective semi-parametric estimation without requiring any specific distributional assumptions on the angular error. The prediction error is measured as an angular loss on the surface of the torus, and also the angular deflection along normal directions on the unit sphere transported from the torus. Additionally, a new visualization technique for circular data is introduced. The practical relevance of the model is illustrated through its application to wind and wave direction data from two major cyclonic events, Amphan and Biparjoy, that impacted the eastern and western coastlines of India, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17014v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Surojit Biswas, Buddhananda Banerjee</dc:creator>
    </item>
    <item>
      <title>Bayesian Joint Model of Multi-Sensor and Failure Event Data for Multi-Mode Failure Prediction</title>
      <link>https://arxiv.org/abs/2506.17036</link>
      <description>arXiv:2506.17036v1 Announce Type: new 
Abstract: Modern industrial systems are often subject to multiple failure modes, and their conditions are monitored by multiple sensors, generating multiple time-series signals. Additionally, time-to-failure data are commonly available. Accurately predicting a system's remaining useful life (RUL) requires effectively leveraging multi-sensor time-series data alongside multi-mode failure event data. In most existing models, failure modes and RUL prediction are performed independently, ignoring the inherent relationship between these two tasks. Some models integrate multiple failure modes and event prediction using black-box machine learning approaches, which lack statistical rigor and cannot characterize the inherent uncertainty in the model and data. This paper introduces a unified approach to jointly model the multi-sensor time-series data and failure time concerning multiple failure modes. This proposed model integrate a Cox proportional hazards model, a Convolved Multi-output Gaussian Process, and multinomial failure mode distributions in a hierarchical Bayesian framework with corresponding priors, enabling accurate prediction with robust uncertainty quantification. Posterior distributions are effectively obtained by Variational Bayes, and prediction is performed with Monte Carlo sampling. The advantages of the proposed model is validated through extensive numerical and case studies with jet-engine dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17036v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sina Aghaee Dabaghan Fard, Minhee Kim, Akash Deep, Jaesung Lee</dc:creator>
    </item>
    <item>
      <title>The fundamental problem of risk prediction for individuals: health AI, uncertainty, and personalized medicine</title>
      <link>https://arxiv.org/abs/2506.17141</link>
      <description>arXiv:2506.17141v1 Announce Type: new 
Abstract: Background: Clinical prediction models for a health condition are commonly evaluated regarding performance for a population, although decisions are made for individuals. The classic view relates uncertainty in risk estimates for individuals to sample size (estimation uncertainty) but uncertainty can also be caused by model uncertainty (variability in modeling choices) and applicability uncertainty (variability in measurement procedures and between populations). Methods: We used real and synthetic data for ovarian cancer diagnosis to train 59400 models with variations in estimation, model, and applicability uncertainty. We then used these models to estimate the probability of ovarian cancer in a fixed test set of 100 patients and evaluate the variability in individual estimates. Findings: We show empirically that estimation uncertainty can be strongly dominated by model uncertainty and applicability uncertainty, even for models that perform well at the population level. Estimation uncertainty decreased considerably with increasing training sample size, whereas model and applicability uncertainty remained large. Interpretation: Individual risk estimates are far more uncertain than often assumed. Model uncertainty and applicability uncertainty usually remain invisible when prediction models or algorithms are based on a single study. Predictive algorithms should inform, not dictate, care and support personalization through clinician-patient interaction rather than through inherently uncertain model outputs. Funding: This research is supported by Research Foundation Flanders (FWO) grants G097322N, G049312N, G0B4716N, and 12F3114N to BVC and/or DTi, KU Leuven internal grants C24M/20/064 and C24/15/037 to BVC and/or DT, ZoNMW VIDI grant 09150172310023 to LW. DT is a senior clinical investigator of FWO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17141v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lasai Barre\~nada, Ewout W Steyerberg, Dirk Timmerman, Doranne Thomassen, Laure Wynants, Ben Van Calster</dc:creator>
    </item>
    <item>
      <title>Profile monitoring of random functions with Gaussian process basis expansions</title>
      <link>https://arxiv.org/abs/2506.17153</link>
      <description>arXiv:2506.17153v1 Announce Type: new 
Abstract: We consider the problem of online profile monitoring of random functions that admit basis expansions possessing random coefficients for the purpose of out-of-control state detection. Our approach is applicable to a broad class of random functions which feature two sources of variation: additive error and random fluctuations through random coefficients in the basis representation of functions. We focus on a two-phase monitoring problem with a first stage consisting of learning the in-control process and the second stage leveraging the learned process for out-of-control state detection. The foundations of our method are derived under the assumption that the coefficients in the basis expansion are Gaussian random variables, which facilitates the development of scalable and effective monitoring methodology for the observed processes that makes weak functional assumptions on the underlying process. We demonstrate the potential of our method through simulation studies that highlight some of the nuances that emerge in profile monitoring problems with random functions, and through an application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17153v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takayuki Iguchi, Jonathan R. Stewart, Eric Chicken</dc:creator>
    </item>
    <item>
      <title>Regularized Targeted Maximum Likelihood Estimation in Highly Adaptive Lasso Implied Working Models</title>
      <link>https://arxiv.org/abs/2506.17214</link>
      <description>arXiv:2506.17214v1 Announce Type: new 
Abstract: We address the challenge of performing Targeted Maximum Likelihood Estimation (TMLE) after an initial Highly Adaptive Lasso (HAL) fit. Existing approaches that utilize the data-adaptive working model selected by HAL-such as the relaxed HAL update-can be simple and versatile but may become computationally unstable when the HAL basis expansions introduce collinearity. Undersmoothed HAL may fail to solve the efficient influence curve (EIC) at the desired level without overfitting, particularly in complex settings like survival-curve estimation. A full HAL-TMLE, which treats HAL as the initial estimator and then targets in the nonparametric or semiparametric model, typically demands costly iterative clever-covariate calculations in complex set-ups like survival analysis and longitudinal mediation analysis. To overcome these limitations, we propose two new HAL-TMLEs that operate within the finite-dimensional working model implied by HAL: Delta-method regHAL-TMLE and Projection-based regHAL-TMLE. We conduct extensive simulations to demonstrate the performance of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17214v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Li, Sky Qiu, Zeyi Wang, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs</title>
      <link>https://arxiv.org/abs/2506.15690</link>
      <description>arXiv:2506.15690v2 Announce Type: cross 
Abstract: The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15690v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Wang, Lingyou Pang, Akira Horiguchi, Carey E. Priebe</dc:creator>
    </item>
    <item>
      <title>Linear-Time Primitives for Algorithm Development in Graphical Causal Inference</title>
      <link>https://arxiv.org/abs/2506.15758</link>
      <description>arXiv:2506.15758v1 Announce Type: cross 
Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in graphical causal inference that isolates reachability as a reusable core operation. It builds on the insight that many causal reasoning tasks can be reduced to reachability in purpose-built state-space graphs that can be constructed on the fly during traversal. We formalize a rule table schema for specifying such algorithms and prove they run in linear time. We establish CIfly as a more efficient alternative to the common primitives moralization and latent projection, which we show are computationally equivalent to Boolean matrix multiplication. Our open-source Rust implementation parses rule table text files and runs the specified CIfly algorithms providing high-performance execution accessible from Python and R. We demonstrate CIfly's utility by re-implementing a range of established causal inference tasks within the framework and by developing new algorithms for instrumental variables. These contributions position CIfly as a flexible and scalable backbone for graphical causal inference, guiding algorithm development and enabling easy and efficient deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15758v1</guid>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcel Wien\"obst, Sebastian Weichwald, Leonard Henckel</dc:creator>
    </item>
    <item>
      <title>Bayesian Non-Negative Matrix Factorization with Correlated Mutation Type Probabilities for Mutational Signatures</title>
      <link>https://arxiv.org/abs/2506.15855</link>
      <description>arXiv:2506.15855v1 Announce Type: cross 
Abstract: Somatic mutations, or alterations in DNA of a somatic cell, are key markers of cancer. In recent years, mutational signature analysis has become a prominent field of study within cancer research, commonly with Nonnegative Matrix Factorization (NMF) and Bayesian NMF. However, current methods assume independence across mutation types in the signatures matrix. This paper expands upon current Bayesian NMF methodologies by proposing novel methods that account for the dependencies between the mutation types. First, we implement the Bayesian NMF specification with a Multivariate Truncated Normal prior on the signatures matrix in order to model the covariance structure using external information, in our case estimated from the COSMIC signatures database. This model converges in fewer iterations, using MCMC, when compared to a model with independent Truncated Normal priors on elements of the signatures matrix and results in improvements in accuracy, especially on small sample sizes. In addition, we develop a hierarchical model that allows the covariance structure of the signatures matrix to be discovered rather than specified upfront, giving the algorithm more flexibility. This flexibility for the algorithm to learn the dependence structure of the signatures allows a better understanding of biological interactions and how these change across different types of cancer. The code for this project is contributed to an open-source R software package. Our work lays the groundwork for future research to incorporate dependency structure across mutation types in the signatures matrix and is also applicable to any use of NMF beyond just single-base substitution (SBS) mutational signatures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15855v1</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iris Lang, Jenna Landy, Giovanni Parmigiani</dc:creator>
    </item>
    <item>
      <title>On Design of Representative Distributionally Robust Formulations for Evaluation of Tail Risk Measures</title>
      <link>https://arxiv.org/abs/2506.16230</link>
      <description>arXiv:2506.16230v1 Announce Type: cross 
Abstract: Conditional Value-at-Risk (CVaR) is a risk measure widely used to quantify the impact of extreme losses. Owing to the lack of representative samples CVaR is sensitive to the tails of the underlying distribution. In order to combat this sensitivity, Distributionally Robust Optimization (DRO), which evaluates the worst-case CVaR measure over a set of plausible data distributions is often deployed. Unfortunately, an improper choice of the DRO formulation can lead to a severe underestimation of tail risk. This paper aims at leveraging extreme value theory to arrive at a DRO formulation which leads to representative worst-case CVaR evaluations in that the above pitfall is avoided while simultaneously, the worst case evaluation is not a gross over-estimate of the true CVaR. We demonstrate theoretically that even when there is paucity of samples in the tail of the distribution, our formulation is readily implementable from data, only requiring calibration of a single scalar parameter. We showcase that our formulation can be easily extended to provide robustness to tail risk in multivariate applications as well as in the evaluation of other commonly used risk measures. Numerical illustrations on synthetic and real-world data showcase the practical utility of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16230v1</guid>
      <category>q-fin.RM</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anand Deo</dc:creator>
    </item>
    <item>
      <title>Simultaneous Inference for Multiple Proportions: A Multivariate Beta-Binomial Model</title>
      <link>https://arxiv.org/abs/1911.00098</link>
      <description>arXiv:1911.00098v5 Announce Type: replace 
Abstract: Statistical inference in high-dimensional settings is challenging when standard unregularized methods are employed. In this work, we focus on the case of multiple correlated proportions for which we develop a Bayesian inference framework. For this purpose, we construct an $m$-dimensional Beta distribution from a $2^m$-dimensional Dirichlet distribution, building on work by Olkin and Trikalinos (2015). This readily leads to a multivariate Beta-binomial model for which simple update rules from the common Dirichlet-multinomial model can be adopted. From the frequentist perspective, this approach amounts to adding pseudo-observations to the data and allows a joint shrinkage estimation of mean vector and covariance matrix. For higher dimensions ($m &gt; 10$), the extensive model based on $2^m$ parameters starts to become numerically infeasible. To counter this problem, we utilize a reduced parametrisation which has only $1 + m(m + 1)/2$ parameters describing first and second order moments. A copula model can then be used to approximate the (posterior) multivariate Beta distribution. A natural inference goal is the construction of multivariate credible regions. The properties of different credible regions are assessed in a simulation study in the context of investigating the accuracy of multiple binary classifiers. It is shown that the extensive and copula approach lead to a (Bayes) coverage probability very close to the target level. In this regard, they outperform credible regions based on a normal approximation of the posterior distribution, in particular for small sample sizes. Additionally, they always lead to credible regions which lie entirely in the parameter space which is not the case when the normal approximation is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:1911.00098v5</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Westphal</dc:creator>
    </item>
    <item>
      <title>Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison</title>
      <link>https://arxiv.org/abs/2008.10296</link>
      <description>arXiv:2008.10296v5 Announce Type: replace 
Abstract: It is useful to estimate the expected predictive performance of models planned to be used for prediction. We focus on leave-one-out cross-validation (LOO-CV), which has become a popular method for estimating predictive performance of Bayesian models. Given two models, we are interested in comparing the predictive performances and associated uncertainty, which can also be used to compute the probability of one model having better predictive performance than the other model. We study the properties of the Bayesian LOO-CV estimator and the related uncertainty quantification for the predictive performance difference, and analyse when a normal approximation of this uncertainty is well calibrated and whether taking into account higher moments could improve the approximation. We provide new results of the properties both theoretically in the linear regression case and empirically for hierarchical linear, latent linear, and spline models and discuss the challenges. We show that problematic cases include: comparing models with similar predictions, misspecified models, and small data. In these cases, there is a weak connection between the distributions of the LOO-CV estimator and its error. We show that that the problematic skewness of the error distribution for the difference, which occurs when the models make similar predictions, does not fade away when the data size grows to infinity in certain situations. Based on the results, we also provide some practical recommendations for the users of Bayesian LOO-CV for comparing predictive performance of models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.10296v5</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tuomas Sivula, M{\aa}ns Magnusson, Asael Alonzo Matamoros, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>A Robust Framework for Graph-based Two-Sample Tests Using Weights</title>
      <link>https://arxiv.org/abs/2307.12325</link>
      <description>arXiv:2307.12325v4 Announce Type: replace 
Abstract: Graph-based tests are a class of non-parametric two-sample tests useful for analyzing high-dimensional data. The test statistics are constructed from similarity graphs (such as K-minimum spanning tree), and consequently, their performance is sensitive to the structure of the graph. When the graph has problematic structures (for example, hubs), as is common for high-dimensional data, this can result in low power and unstable performance among existing graph-based tests. We address this challenge by proposing new test statistics that are robust to problematic structures of the graph and can provide reliable inferences. We employ an edge-weighting strategy using intrinsic characteristics of the graph that are computationally simple and efficient to obtain. The limiting null distribution of the robust test statistics is derived and shown to work well for finite sample sizes. Simulation studies and data analysis of Chicago taxi-trip travel patterns demonstrate the new tests' improved performance across a range of settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12325v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichuan Bai, Lynna Chu</dc:creator>
    </item>
    <item>
      <title>Conformal prediction for frequency-severity modeling</title>
      <link>https://arxiv.org/abs/2307.13124</link>
      <description>arXiv:2307.13124v4 Announce Type: replace 
Abstract: We present a model-agnostic framework for the construction of prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The framework effectiveness is showcased with simulated and real datasets using classical parametric models and contemporary machine learning methods. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction algorithm, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set in the conformal procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.13124v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helton Graziadei, Paulo C. Marques F., Eduardo F. L. de Melo, Rodrigo S. Targino</dc:creator>
    </item>
    <item>
      <title>Partial identification and unmeasured confounding with multiple treatments and multiple outcomes</title>
      <link>https://arxiv.org/abs/2311.12252</link>
      <description>arXiv:2311.12252v3 Announce Type: replace 
Abstract: Estimating the health effects of multiple air pollutants is a crucial problem in public health, but one that is difficult due to unmeasured confounding bias. Motivated by this issue, we develop a framework for partial identification of causal effects in the presence of unmeasured confounding in settings with multiple treatments and multiple outcomes. Under a factor confounding assumption, we show that joint partial identification regions for multiple estimands can be more informative than considering partial identification for individual estimands one at a time. We show how assumptions related to the strength of confounding or magnitude of plausible effect sizes for one estimand can reduce the partial identification regions for other estimands. As a special case of this result, we explore how negative control assumptions reduce partial identification regions and discuss conditions under which point identification can be obtained. We develop novel computational approaches to finding partial identification regions under a variety of these assumptions. We then estimate the causal effect of PM2.5 components on a variety of public health outcomes in the United States Medicare cohort, where we find that, in particular, the detrimental effect of black carbon is robust to the potential presence of unmeasured confounding bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12252v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suyeon Kang, Alexander Franks, Michelle Audirac, Danielle Braun, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Design of Bayesian A/B Tests Controlling False Discovery Rates and Power</title>
      <link>https://arxiv.org/abs/2312.10814</link>
      <description>arXiv:2312.10814v4 Announce Type: replace 
Abstract: Online controlled experiments (i.e., A/B tests) are a critical tool used by businesses with digital operations to optimize their products and services. These experiments routinely track information related to various business metrics, each of which summarizes a different aspect of how users interact with an online platform. Although multiple metrics are commonly tracked, this information is often not well utilized; multiple metrics are often aggregated into a single composite measure, losing valuable information, or strict family-wise error rate adjustments are imposed, leading to reduced power. In this paper, we propose an economical framework to design Bayesian A/B tests while controlling both power and the false discovery rate (FDR). Selecting optimal decision thresholds to control power and the FDR typically relies on intensive simulation at each sample size considered. Our framework efficiently recommends optimal sample sizes and decision thresholds for Bayesian A/B tests that satisfy criteria for the FDR and average power. Our approach is efficient because we leverage new theoretical results to obtain these recommendations using simulations conducted at only two sample sizes. Our methodology is illustrated using an example based on a real A/B test involving several metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10814v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Nathaniel T. Stevens</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to functional regression: theory and computation</title>
      <link>https://arxiv.org/abs/2312.14086</link>
      <description>arXiv:2312.14086v3 Announce Type: replace 
Abstract: We propose a novel Bayesian methodology for inference in functional linear and logistic regression models based on the theory of reproducing kernel Hilbert spaces (RKHS's). We introduce general models that build upon the RKHS generated by the covariance function of the underlying stochastic process, and whose formulation includes as particular cases all finite-dimensional models based on linear combinations of marginals of the process, which can collectively be seen as a dense subspace made of simple approximations. By imposing a suitable prior distribution on this dense functional space we can perform data-driven inference via standard Bayes methodology, estimating the posterior distribution through reversible jump Markov chain Monte Carlo methods. In this context, our contribution is two-fold. First, we derive theoretical results that guarantee strong posterior consistency and contraction at an optimal rate under mild conditions. Second, we show that several prediction strategies stemming from our Bayesian procedure are competitive against other usual alternatives in both simulations and real data sets, including a Bayesian-motivated variable selection method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14086v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jos\'e R. Berrendero, Antonio Co\'in, Antonio Cuevas</dc:creator>
    </item>
    <item>
      <title>Linear Model and Extensions</title>
      <link>https://arxiv.org/abs/2401.00649</link>
      <description>arXiv:2401.00649v2 Announce Type: replace 
Abstract: I developed the lecture notes based on my ``Linear Model'' course at the University of California, Berkeley over the past ten years. This book provides an intermediate-level introduction to the linear model. It balances rigorous proofs and heuristic arguments. This book provides R code to replicate all simulation studies and case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00649v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Ding</dc:creator>
    </item>
    <item>
      <title>On the PM2.5 -- Mortality Relationship: A Bayesian Model for Spatio-Temporal Confounding</title>
      <link>https://arxiv.org/abs/2405.16106</link>
      <description>arXiv:2405.16106v2 Announce Type: replace 
Abstract: Spatial confounding, often regarded as a major concern in epidemiological studies, relates to the difficulty of recovering the effect of an exposure on an outcome when these variables are associated with unobserved factors. This issue is particularly challenging in spatio-temporal analyses, where it has been less explored so far. To study the effects of air pollution on mortality in Italy, we argue that a model that simultaneously accounts for spatio-temporal confounding and for the non-linear form of the effect of interest is needed. To this end, we propose a Bayesian spatial dynamic generalized linear model, which allows for a non-linear association and for a decomposition of the exposure effect into two components. This decomposition accommodates associations with the outcome at fine and coarse temporal and spatial scales of variation. These features, when combined, allow reducing the spatio-temporal confounding bias and recovering the true shape of the association, as demonstrated through simulation studies. The results from the real-data application indicate that the exposure effect seems to have different magnitudes in different seasons, with peaks in the summer. We hypothesize that this could be due to possible interactions between the exposure variable with air temperature and unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16106v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlo Zaccardi, Pasquale Valentini, Luigi Ippoliti, Alexandra M. Schmidt</dc:creator>
    </item>
    <item>
      <title>Robust Maximum $L_q$-Likelihood Covariance Estimation for Replicated Spatial Data</title>
      <link>https://arxiv.org/abs/2407.17592</link>
      <description>arXiv:2407.17592v2 Announce Type: replace 
Abstract: Parameter estimation with the maximum $L_q$-likelihood estimator (ML$q$E) is an alternative to the maximum likelihood estimator (MLE) that considers the $q$-th power of the likelihood values for some $q&lt;1$. In this method, extreme values are down-weighted because of their lower likelihood values, which yields robust estimates. In this work, we study the properties of the ML$q$E for spatial data with replicates. We investigate the asymptotic properties of the ML$q$E for Gaussian random fields with a Mat\'ern covariance function, and carry out simulation studies to investigate the numerical performance of the ML$q$E. We show that it can provide more robust and stable estimation results when some of the replicates in the spatial data contain outliers. In addition, we develop a mechanism to find the optimal choice of the hyper-parameter $q$ for the ML$q$E. The robustness of our approach is further verified on a United States precipitation dataset. Compared with other robust methods for spatial data, our proposal is more intuitive and easier to understand, yet it performs well when dealing with datasets containing outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17592v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.52933/jdssv.v5i4.126</arxiv:DOI>
      <arxiv:journal_reference>Journal of Data Science, Statistics, and Visualisation, 5(4) (2025)</arxiv:journal_reference>
      <dc:creator>Sihan Chen, Joydeep Chowdhury, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>Local Level Dynamic Random Partition Models for Changepoint Detection</title>
      <link>https://arxiv.org/abs/2407.20085</link>
      <description>arXiv:2407.20085v2 Announce Type: replace 
Abstract: Motivated by an increasing demand for models that can effectively describe features of complex multivariate time series, e.g. from sensor data in biomechanics, motion analysis, and sports science, we introduce a novel state-space modeling framework where the state equation encodes the evolution of latent partitions of the data over time. Building on the principles of dynamic linear models, our approach develops a random partition model capable of linking data partitions to previous ones over time, using a straightforward Markov structure that accounts for temporal persistence and facilitates changepoint detection. The selection of changepoints involves multiple dependent decisions, and we address this time-dependence by adopting a non-marginal false discovery rate control. This leads to a simple decision rule that ensures more stringent control of the false discovery rate compared to approaches that do not consider dependence. The method is efficiently implemented using a Gibbs sampling algorithm, leading to a straightforward approach compared to existing methods for dependent random partition models. Additionally, we show how the proposed method can be adapted to handle multiview clustering scenarios. Simulation studies and the analysis of a human gesture phase dataset collected through various sensing technologies show the effectiveness of the method in dynamically clustering multivariate time series and detecting changepoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20085v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alice Giampino, Bernardo Nipoti, Marina Vannucci, Michele Guindani</dc:creator>
    </item>
    <item>
      <title>A Unified Principal Components Analysis for Stationary Functional Time Series</title>
      <link>https://arxiv.org/abs/2408.02343</link>
      <description>arXiv:2408.02343v3 Announce Type: replace 
Abstract: Functional time series (FTS) data have become increasingly available in real-world applications. Research on such data typically focuses on two objectives: curve reconstruction and forecasting, both of which require efficient dimension reduction. While functional principal component analysis (FPCA) serves as a standard tool, existing methods often fail to achieve simultaneous parsimony and optimality in dimension reduction, thereby restricting their practical implementation. To address this limitation, we propose a novel notion termed optimal functional filters, which unifies and enhances conventional FPCA methodologies. Specifically, we establish connections among diverse FPCA approaches through a dependence-adaptive representer for stationary FTS. Building on this theoretical foundation, we develop an estimation procedure for optimal functional filters that enables both dimension reduction and prediction within a Bayesian modeling framework. Theoretical properties are established for the proposed methodology, and comprehensive simulation studies validate its superiority over competing approaches. We further illustrate our method through an application to reconstructing and forecasting daily air pollutant concentration trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02343v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zerui Guo, Jianbin Tan, Hui Huang</dc:creator>
    </item>
    <item>
      <title>Adaptive partition Factor Analysis</title>
      <link>https://arxiv.org/abs/2410.18939</link>
      <description>arXiv:2410.18939v2 Announce Type: replace 
Abstract: Factor Analysis has traditionally been utilized across diverse disciplines to extrapolate latent traits that influence the behavior of multivariate observed variables. Historically, the focus has been on analyzing data from a single study, neglecting the potential study-specific variations present in data from multiple studies. Multi-study factor analysis has emerged as a recent methodological advancement that addresses this gap by distinguishing between latent traits shared across studies and study-specific components arising from artifactual or population-specific sources of variation. In this paper, we extend the current methodologies by introducing novel shrinkage priors for the latent factors, thereby accommodating a broader spectrum of scenarios -- from the absence of study-specific latent factors to models in which factors pertain only to small subgroups nested within or shared between the studies. For the proposed construction we provide conditions for identifiability of factor loadings and guidelines to perform straightforward posterior computation via Gibbs sampling. Through comprehensive simulation studies, we demonstrate that our proposed method exhibits competing performance across a variety of scenarios compared to existing methods, yet providing richer insights. The practical benefits of our approach are further illustrated through applications to bird species co-occurrence data and ovarian cancer gene expression data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18939v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Bortolato, Antonio Canale</dc:creator>
    </item>
    <item>
      <title>Two-sided conformalized survival analysis</title>
      <link>https://arxiv.org/abs/2410.24136</link>
      <description>arXiv:2410.24136v2 Announce Type: replace 
Abstract: This paper presents a conformal prediction procedure to generate two-sided or one-sided prediction intervals for survival times in the presence of right censoring. Specifically, the method provides two-sided predictive bounds for individuals deemed sufficiently similar to the uncensored population, while returning a lower predictive bound for others. The prediction intervals offer finite-sample coverage guarantees, requiring no distributional assumptions other than the sampled data points are independent and identically distributed. The performance and validity of the procedure is evaluated on both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24136v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Holmes, Ariane Marandon</dc:creator>
    </item>
    <item>
      <title>SMART-MC: Characterizing the Dynamics of Multiple Sclerosis Therapy Transitions Using a Covariate-Based Markov Model</title>
      <link>https://arxiv.org/abs/2412.03596</link>
      <description>arXiv:2412.03596v2 Announce Type: replace 
Abstract: Treatment switching is a common occurrence in the management of Multiple Sclerosis (MS), where patients transition across various disease-modifying therapies (DMTs) due to heterogeneous treatment responses, differences in disease progression, patient characteristics, and therapy-associated adverse effects. To investigate how patient-level covariates influence the likelihood of treatment transitions among DMTs, we adopt a Markovian framework, Sparse Matrix Estimation with Covariate-Based Transitions in Markov Chain Modeling (SMART-MC), in which the transition probabilities are modeled as functions of these covariates. Modeling real-world treatment transitions under this framework presents several challenges, including ensuring parameter identifiability and handling sparse transitions without overfitting. To address identifiability, we constrain each transition-specific covariate coefficient vectors to have a fixed L2 norm. Furthermore, our method automatically estimates transition probabilities for sparsely observed transitions as constants and enforces zero transition probabilities for transitions that are empirically unobserved. This approach mitigates the need for additional model complexity to handle sparsity while maintaining interpretability and efficiency. To optimize the multi-modal likelihood function, we develop a scalable, parallelized global optimization routine, which is validated through benchmark comparisons and supported by key theoretical properties. Our analysis uncovers meaningful patterns in DMT transitions, revealing variations across MS patient subgroups defined by age, race, and other clinical factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03596v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beomchang Kim, Zongqi Xia, Priyam Das</dc:creator>
    </item>
    <item>
      <title>A robust mixed-effects quantile regression model using generalized Laplace mixtures to handle outliers and skewness</title>
      <link>https://arxiv.org/abs/2504.14515</link>
      <description>arXiv:2504.14515v3 Announce Type: replace 
Abstract: Mixed-effects quantile regression models are widely used to capture heterogeneous responses in hierarchically structured data. The asymmetric Laplace (AL) distribution has traditionally served as the basis for quantile regression; however, its fixed skewness limits flexibility and renders it sensitive to outliers. In contrast, the generalized asymmetric Laplace (GAL) distribution enables more flexible modeling of skewness and heavy-tailed behavior, yet it remains vulnerable to extreme observations. In this paper, we extend the GAL distribution by introducing a contaminated GAL (cGAL) mixture model that incorporates a scale-inflated component to mitigate the impact of outliers without requiring explicit outlier identification or deletion. We apply this model within a Bayesian mixed-effects quantile regression framework to model HIV viral load decay over time. Our results demonstrate that the cGAL-based model more reliably captures the dynamics of HIV viral load decay, yielding more accurate parameter estimates compared to both AL and GAL approaches. Model diagnostics and comparison statistics confirm the cGAL model as the preferred choice. A simulation study further shows that the cGAL model is more robust to outliers than the GAL and exhibits favorable frequentist properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14515v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00180-025-01651-0</arxiv:DOI>
      <dc:creator>Divan A. Burger, Sean van der Merwe, Emmanuel Lesaffre</dc:creator>
    </item>
    <item>
      <title>A copula-based rank histogram ensemble filter</title>
      <link>https://arxiv.org/abs/2505.01918</link>
      <description>arXiv:2505.01918v2 Announce Type: replace 
Abstract: Serial ensemble filters implement triangular probability transport maps to reduce high-dimensional inference problems to sequences of state-by-state univariate inference problems. The univariate inference problems are solved by sampling posterior probability densities obtained by combining constructed prior densities with observational likelihoods according to Bayes' rule. Many serial filters in the literature focus on representing the marginal posterior densities of each state. However, rigorously capturing the conditional dependencies between the different univariate inferences is crucial to correctly sampling multidimensional posteriors. This work proposes a new serial ensemble filter, called the copula rank histogram filter (CoRHF), that seeks to capture the conditional dependency structure between variables via empirical copula estimates; these estimates are used to rigorously implement the triangular (state-by-state univariate) Bayesian inference. The success of the CoRHF is demonstrated on two-dimensional examples and the Lorenz '63 problem. A practical extension to the high-dimensional setting is developed by localizing the empirical copula estimation, and is demonstrated on the Lorenz '96 problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01918v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amit N. Subrahmanya, Julie Bessac, Andrey A. Popov, Adrian Sandu</dc:creator>
    </item>
    <item>
      <title>Assumption-robust Causal Inference</title>
      <link>https://arxiv.org/abs/2505.08729</link>
      <description>arXiv:2505.08729v2 Announce Type: replace 
Abstract: In observational causal inference, it is common to encounter multiple adjustment sets that appear equally plausible. It is often untestable which of these adjustment sets are valid to adjust for (i.e., satisfies ignorability). This discrepancy can pose practical challenges as it is typically unclear how to reconcile multiple, possibly conflicting estimates of the average treatment effect (ATE). A naive approach is to report the whole range (convex hull of the union) of the resulting confidence intervals. However, the width of this interval might not shrink to zero in large samples and can be unnecessarily wide in real applications. To address this issue, we propose a summary procedure that generates a single estimate, one confidence interval, and identifies a set of units for which the causal effect estimate remains valid, provided at least one adjustment set is valid. The width of our proposed confidence interval shrinks to zero with sample size at $n^{-1/2}$ rate, unlike the original range which is of constant order. Thus, our assumption-robust approach enables reliable causal inference on the ATE even in scenarios where most of the adjustment sets are invalid. Admittedly, this robustness comes at a cost:~our inferential guarantees apply to a target population close to, but different from, the one originally intended. We use synthetic and real-data examples to demonstrate that our proposed procedure provides substantially tighter confidence intervals for the ATE as compared to the whole range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08729v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Ghosh, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Non-null Shrinkage Regression and Subset Selection via the Fractional Ridge Regression</title>
      <link>https://arxiv.org/abs/2505.23925</link>
      <description>arXiv:2505.23925v2 Announce Type: replace 
Abstract: $\ell_p$-norm penalization, notably the Lasso, has become a standard technique, extending shrinkage regression to subset selection. Despite aiming for oracle properties and consistent estimation, existing Lasso-derived methods still rely on shrinkage toward a null model, necessitating careful tuning parameter selection and yielding monotone variable selection. This research introduces Fractional Ridge Regression, a novel generalization of the Lasso penalty that penalizes only a fraction of the coefficients. Critically, Fridge shrinks the model toward a non-null model of a prespecified target size, even under extreme regularization. By selectively penalizing coefficients associated with less important variables, Fridge aims to reduce bias, improve performance relative to the Lasso, and offer more intuitive model interpretation while retaining certain advantages of best subset selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23925v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sihyung Park (North Carolina State University), Leonard A. Stefanski (North Carolina State University)</dc:creator>
    </item>
    <item>
      <title>The Two Cultures of Prevalence Mapping: Small Area Estimation and Model-Based Geostatistics</title>
      <link>https://arxiv.org/abs/2110.09576</link>
      <description>arXiv:2110.09576v3 Announce Type: replace-cross 
Abstract: In low- and middle-income countries (LMICs), accurate estimates of subnational health and demographic indicators are critical for guiding policy and identifying disparities. Many indicators of interest are proportions of binary outcomes and the task of estimating these fractions is often called prevalence mapping. In LMICs, health and vital records data are limited, so prevalence mapping relies on data from household surveys with complex sampling designs. However, estimates are often desired at spatial resolutions at which data are insufficient. We review two families of approaches to prevalence mapping: small area estimation (SAE) methods (from the survey statistics literature) and model-based geostatistics (MBG) methods (from the spatial statistics literature). SAE models can be ``area-level" or ``unit-level" and commonly use area-specific random effects and rely upon high-quality covariate data from administrative sources. Unit-level models for binary responses are relatively underdeveloped. MBG approaches explicitly specify binary response models, incorporate continuous spatial random effects, and leverage alternative data sources, e.g., satellite imagery. SAE methods often address the design by incorporating sampling weights or modeling the sampling mechanism. Two delicate issues arise when using MBG methods. First, aggregating unit level predictions to create area-level summaries requires population-level information that is rarely available. Second, MBG approaches typically assume the sampling design is ignorable. We review both approaches, and argue that binary response models can be improved using insights from both the survey sampling and the spatial statistics literature. We highlight these issues using household survey data from the Zambia 2018 Demographic Health Survey to estimate subnational HIV prevalence for woman aged 15--49.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.09576v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jon Wakefield, Peter A. Gao, Geir-Arne Fuglstad, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>Assessing Omitted Variable Bias when the Controls are Endogenous</title>
      <link>https://arxiv.org/abs/2206.02303</link>
      <description>arXiv:2206.02303v5 Announce Type: replace-cross 
Abstract: Omitted variables are one of the most important threats to the identification of causal effects. Several widely used methods assess the impact of omitted variables on empirical conclusions by comparing measures of selection on observables with measures of selection on unobservables. The recent literature has discussed various limitations of these existing methods, however. This includes a companion paper of ours which explains issues that arise when the omitted variables are endogenous, meaning that they are correlated with the included controls. In the present paper, we develop a new approach to sensitivity analysis that avoids those limitations, while still allowing researchers to calibrate sensitivity parameters by comparing the magnitude of selection on observables with the magnitude of selection on unobservables as in previous methods. We illustrate our results in an empirical study of the effect of historical American frontier life on modern cultural beliefs. Finally, we implement these methods in the companion Stata module regsensitivity for easy use in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02303v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Diegert, Matthew A. Masten, Alexandre Poirier</dc:creator>
    </item>
    <item>
      <title>Measuring Evidence against Exchangeability and Group Invariance with E-values</title>
      <link>https://arxiv.org/abs/2310.01153</link>
      <description>arXiv:2310.01153v4 Announce Type: replace-cross 
Abstract: We study e-values for quantifying evidence against exchangeability and general invariance of a random variable under a compact group. We start by characterizing such e-values, and explaining how they nest traditional group invariance tests as a special case. We show they can be easily designed for an arbitrary test statistic, and computed through Monte Carlo sampling. We prove a result that characterizes optimal e-values for group invariance against optimality targets that satisfy a mild orbit-wise decomposition property. We apply this to design expected-utility-optimal e-values for group invariance, which include both Neyman-Pearson-optimal tests and log-optimal e-values. Moreover, we generalize the notion of rank- and sign-based testing to compact groups, by using a representative inversion kernel. In addition, we characterize e-processes for group invariance for arbitrary filtrations, and provide tools to construct them. We also describe test martingales under a natural filtration, which are simpler to construct. Peeking beyond compact groups, we encounter e-values and e-processes based on ergodic theorems. These nest e-processes based on de Finetti's theorem for testing exchangeability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01153v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning</dc:creator>
    </item>
    <item>
      <title>Adaptive Experimental Design for Policy Learning</title>
      <link>https://arxiv.org/abs/2401.03756</link>
      <description>arXiv:2401.03756v4 Announce Type: replace-cross 
Abstract: This study investigates the contextual best arm identification (BAI) problem, aiming to design an adaptive experiment to identify the best treatment arm conditioned on contextual information (covariates). We consider a decision-maker who assigns treatment arms to experimental units during an experiment and recommends the estimated best treatment arm based on the contexts at the end of the experiment. The decision-maker uses a policy for recommendations, which is a function that provides the estimated best treatment arm given the contexts. In our evaluation, we focus on the worst-case expected regret, a relative measure between the expected outcomes of an optimal policy and our proposed policy. We derive a lower bound for the expected simple regret and then propose a strategy called Adaptive Sampling-Policy Learning (PLAS). We prove that this strategy is minimax rate-optimal in the sense that its leading factor in the regret upper bound matches the lower bound as the number of experimental units increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03756v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato, Kyohei Okumura, Takuya Ishihara, Toru Kitagawa</dc:creator>
    </item>
    <item>
      <title>Deep learning joint extremes of metocean variables using the SPAR model</title>
      <link>https://arxiv.org/abs/2412.15808</link>
      <description>arXiv:2412.15808v2 Announce Type: replace-cross 
Abstract: This paper presents a novel deep learning framework for estimating multivariate joint extremes of metocean variables, based on the Semi-Parametric Angular-Radial (SPAR) model. When considered in polar coordinates, the problem of modelling multivariate extremes is transformed to one of modelling an angular density, and the tail of a univariate radial variable conditioned on angle. In the SPAR approach, the tail of the radial variable is modelled using a generalised Pareto (GP) distribution, providing a natural extension of univariate extreme value theory to the multivariate setting. In this work, we show how the method can be applied in higher dimensions, using a case study for five metocean variables: wind speed, wind direction, wave height, wave period, and wave direction. The angular variable is modelled using a kernel density method, while the parameters of the GP model are approximated using fully-connected deep neural networks. Our approach provides great flexibility in the dependence structures that can be represented, together with computationally efficient routines for training the model. Furthermore, the application of the method requires fewer assumptions about the underlying distribution(s) compared to existing approaches, and an asymptotically justified means for extrapolating outside the range of observations. Using various diagnostic plots, we show that the fitted models provide a good description of the joint extremes of the metocean variables considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15808v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ed Mackay, Callum Murphy-Barltrop, Jordan Richards, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title>
      <link>https://arxiv.org/abs/2501.01324</link>
      <description>arXiv:2501.01324v3 Announce Type: replace-cross 
Abstract: In this work, we develop a scalable approach for a flexible latent factor model for high-dimensional dynamical systems. Each latent factor process has its own correlation and variance parameters, and the orthogonal factor loading matrix can be either fixed or estimated. We utilize an orthogonal factor loading matrix that avoids computing the inversion of the posterior covariance matrix at each time of the Kalman filter, and derive closed-form expressions in an expectation-maximization algorithm for parameter estimation, which substantially reduces the computational complexity without approximation. Our study is motivated by inversely estimating slow slip events from geodetic data, such as continuous GPS measurements. Extensive simulated studies illustrate higher accuracy and scalability of our approach compared to alternatives. By applying our method to geodetic measurements in the Cascadia region, our estimated slip better agrees with independently measured seismic data of tremor events. The substantial acceleration from our method enables the use of massive noisy data for geological hazard quantification and other applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01324v3</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>An Axiomatic Approach to Comparing Sensitivity Parameters</title>
      <link>https://arxiv.org/abs/2504.21106</link>
      <description>arXiv:2504.21106v2 Announce Type: replace-cross 
Abstract: Many methods are available for assessing the importance of omitted variables. These methods typically make different, non-falsifiable assumptions. Hence the data alone cannot tell us which method is most appropriate. Since it is unreasonable to expect results to be robust against all possible robustness checks, researchers often use methods deemed "interpretable", a subjective criterion with no formal definition. In contrast, we develop the first formal, axiomatic framework for comparing and selecting among these methods. Our framework is analogous to the standard approach for comparing estimators based on their sampling distributions. We propose that sensitivity parameters be selected based on their covariate sampling distributions, a design distribution of parameter values induced by an assumption on how covariates are assigned to be observed or unobserved. Using this idea, we define a new concept of parameter consistency, and argue that a reasonable sensitivity parameter should be consistent. We prove that the literature's most popular approach is inconsistent, while several alternatives are consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21106v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Diegert, Matthew A. Masten, Alexandre Poirier</dc:creator>
    </item>
    <item>
      <title>Statistical Learning for Heterogeneous Treatment Effects: Pretraining, Prognosis, and Prediction</title>
      <link>https://arxiv.org/abs/2505.00310</link>
      <description>arXiv:2505.00310v2 Announce Type: replace-cross 
Abstract: Robust estimation of heterogeneous treatment effects is a fundamental challenge for optimal decision-making in domains ranging from personalized medicine to educational policy. In recent years, predictive machine learning has emerged as a valuable toolbox for causal estimation, enabling more flexible effect estimation. However, accurately estimating conditional average treatment effects (CATE) remains a major challenge, particularly in the presence of many covariates. In this article, we propose pretraining strategies that leverage a phenomenon in real-world applications: factors that are prognostic of the outcome are frequently also predictive of treatment effect heterogeneity. In medicine, for example, components of the same biological signaling pathways frequently influence both baseline risk and treatment response. Specifically, we demonstrate our approach within the R-learner framework, which estimates the CATE by solving individual prediction problems based on a residualized loss. We use this structure to incorporate side information and develop models that can exploit synergies between risk prediction and causal effect estimation. In settings where these synergies are present, this cross-task learning enables more accurate signal detection, yields lower estimation error, reduced false discovery rates, and higher power for detecting heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00310v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Schuessler, Erik Sverdrup, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>A new measure of dependence: Integrated $R^2$</title>
      <link>https://arxiv.org/abs/2505.18146</link>
      <description>arXiv:2505.18146v2 Announce Type: replace-cross 
Abstract: We propose a new measure of dependence that quantifies the degree to which a random variable $Y$ depends on a random vector $X$. This measure is zero if and only if $Y$ and $X$ are independent, and equals one if and only if $Y$ is a measurable function of $X$. We introduce a simple and interpretable estimator that is comparable in ease of computation to classical correlation coefficients such as Pearson's, Spearman's, or Chatterjee's. Building on this coefficient, we develop a model-free variable selection algorithm, feature ordering by dependence (FORD), inspired by FOCI. FORD requires no tuning parameters and is provably consistent under suitable sparsity assumptions. We demonstrate its effectiveness and improvements over FOCI through experiments on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18146v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mona Azadkia, Pouya Roudaki</dc:creator>
    </item>
  </channel>
</rss>
