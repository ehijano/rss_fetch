<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Feb 2025 03:01:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Implicit vs. explicit regularization for high-dimensional gradient descent</title>
      <link>https://arxiv.org/abs/2502.10578</link>
      <description>arXiv:2502.10578v1 Announce Type: new 
Abstract: In this paper we investigate the generalization error of gradient descent (GD) applied to an $\ell_2$-regularized OLS objective function in the linear model. Based on our analysis we develop new methodology for computationally tractable and statistically efficient linear prediction in a high-dimensional and massive data scenario (large-$n$, large-$p$). Our results are based on the surprising observation that the generalization error of optimally tuned regularized gradient descent approaches that of an optimal benchmark procedure $monotonically$ in the iteration number $m$. On the other hand standard GD for OLS (without explicit regularization) can achieve the benchmark only in degenerate cases. This shows that (optimal) explicit regularization can be nearly statistically efficient (for large $m$) whereas implicit regularization by (optimal) early stopping can not.
  To complete our methodology, we provide a fully data driven and computationally tractable choice of $\ell_2$ regularization parameter $\lambda$ that is computationally cheaper than cross-validation. On this way, we follow and extend ideas of Dicker (2014) to the non-gaussian case, which requires new results on high-dimensional sample covariance matrices that might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10578v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Stark, Lukas Steinberger</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of a Greedy Algorithm for Conditioning Gaussian Random Variables</title>
      <link>https://arxiv.org/abs/2502.10772</link>
      <description>arXiv:2502.10772v1 Announce Type: new 
Abstract: In the context of Gaussian conditioning, greedy algorithms iteratively select the most informative measurements, given an observed Gaussian random variable. However, the convergence analysis for conditioning Gaussian random variables remains an open problem. We adress this by introducing an operator $M$ that allows us to transfer convergence rates of the observed Gaussian random variable approximation onto the conditional Gaussian random variable. Furthermore we apply greedy methods from approximation theory to obtain convergence rates. These greedy methods have already demonstrated optimal convergence rates within the setting of kernel based function approximation. In this paper, we establish an upper bound on the convergence rates concerning the norm of the approximation error of the conditional covariance operator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10772v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Daniel Winkle, Ingo Steinwart, Bernard Haasdonk</dc:creator>
    </item>
    <item>
      <title>Spectral analysis of spatial-sign covariance matrices for heavy-tailed data with dependence</title>
      <link>https://arxiv.org/abs/2502.10943</link>
      <description>arXiv:2502.10943v1 Announce Type: new 
Abstract: This paper investigates the spectral properties of spatial-sign covariance matrices, a self-normalized version of sample covariance matrices, for data from $\alpha$-regularly varying populations with general covariance structures. By exploiting the elegant properties of self-normalized random variables, we establish the limiting spectral distribution and a central limit theorem for linear spectral statistics. We demonstrate that the Mar{\u{c}}enko-Pastur equation holds under the condition $\alpha \geq 2$, while the central limit theorem for linear spectral statistics is valid for $\alpha&gt;4$, which are shown to be nearly the weakest possible conditions for spatial-sign covariance matrices from heavy-tailed data in the presence of dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10943v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hantao Chen, Cheng Wang</dc:creator>
    </item>
    <item>
      <title>A robust and p-hacking-proof significance test under variance uncertainty</title>
      <link>https://arxiv.org/abs/2502.11038</link>
      <description>arXiv:2502.11038v1 Announce Type: new 
Abstract: P-hacking poses challenges to traditional hypothesis testing. In this paper, we propose a robust method for the one-sample significance test that can protect against p-hacking from sample manipulation. Precisely, assuming a sequential arrival of the data whose variance can be time-varying and for which only lower and upper bounds are assumed to exist with possibly unknown values, we use the modern theory of sublinear expectation to build a testing procedure which is robust under such variance uncertainty, and can protect the significance level against potential data manipulation by an experimenter. It is shown that our new method can effectively control the type I error while preserving a satisfactory power, yet a traditional rejection criterion performs poorly under such variance uncertainty. Our theoretical results are well confirmed by a detailed simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11038v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xifeng Li, Shuzhen Yang, Jianfeng Yao</dc:creator>
    </item>
    <item>
      <title>Consistency of heritability estimation from summary statistics in high-dimensional linear models</title>
      <link>https://arxiv.org/abs/2502.11144</link>
      <description>arXiv:2502.11144v1 Announce Type: new 
Abstract: In Genome-Wide Association Studies (GWAS), heritability is defined as the fraction of variance of an outcome explained by a large number of genetic predictors in a high-dimensional polygenic linear model. This work studies the asymptotic properties of the most common estimator of heritability from summary statistics called linkage disequilibrium score (LDSC) regression, together with a simpler and closely related estimator called GWAS heritability (GWASH). These estimators are analyzed in their basic versions and under various modifications used in practice including weighting and standardization. We show that, with some variations, two conditions which we call weak dependence (WD) and bounded-kurtosis effects (BKE) are sufficient for consistency of both the basic LDSC with fixed intercept and GWASH estimators, for both Gaussian and non-Gaussian predictors. For Gaussian predictors it is shown that these conditions are also necessary for consistency of GWASH (with truncation) and simulations suggest that necessity holds too when the predictors are non-Gaussian. We also show that, with properly truncated weights, weighting does not change the consistency results, but standardization of the predictors and outcome, as done in practice, introduces bias in both LDSC and GWASH if the two essential conditions are violated. Finally, we show that, when population stratification is present, all the estimators considered are biased, and the bias is not remedied by using the LDSC regression estimator with free intercept, as originally suggested by the authors of that estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11144v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Azriel, Samuel Davenport, Armin Schwartzman</dc:creator>
    </item>
    <item>
      <title>A statistical theory of overfitting for imbalanced classification</title>
      <link>https://arxiv.org/abs/2502.11323</link>
      <description>arXiv:2502.11323v1 Announce Type: new 
Abstract: Classification with imbalanced data is a common challenge in data analysis, where certain classes (minority classes) account for a small fraction of the training data compared with other classes (majority classes). Classical statistical theory based on large-sample asymptotics and finite-sample corrections is often ineffective for high-dimensional data, leaving many overfitting phenomena in empirical machine learning unexplained.
  In this paper, we develop a statistical theory for high-dimensional imbalanced classification by investigating support vector machines and logistic regression. We find that dimensionality induces truncation or skewing effects on the logit distribution, which we characterize via a variational problem under high-dimensional asymptotics. In particular, for linearly separable data generated from a two-component Gaussian mixture model, the logits from each class follow a normal distribution $\mathsf{N}(0,1)$ on the testing set, but asymptotically follow a rectified normal distribution $\max\{\kappa, \mathsf{N}(0,1)\}$ on the training set -- which is a pervasive phenomenon we verified on tabular data, image data, and text data. This phenomenon explains why the minority class is more severely affected by overfitting. Further, we show that margin rebalancing, which incorporates class sizes into the loss function, is crucial for mitigating the accuracy drop for the minority class. Our theory also provides insights into the effects of overfitting on calibration and other uncertain quantification measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11323v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyang Lyu, Kangjie Zhou, Yiqiao Zhong</dc:creator>
    </item>
    <item>
      <title>Moment Monotonicity of Weibull, Gamma and Log-normal Distributions</title>
      <link>https://arxiv.org/abs/2502.11366</link>
      <description>arXiv:2502.11366v1 Announce Type: new 
Abstract: This paper investigates the moment monotonicity property of Weibull, Gamma, and Log-normal distributions. We provide the first complete mathematical proofs for the monotonicity of the function $E(X^n)^{\frac{1}{n}}$ specific to these distributions. Through the derivations, we identify a key property: in many cases, one of the two parameters defining each distribution can be effectively canceled out. This finding opens up opportunities for improved parameter estimation of these random variables. Our results contribute to a deeper understanding of the behavior of these widely used distributions and offer valuable insights for applications in fields such as reliability engineering, econometrics, and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11366v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kang Liu</dc:creator>
    </item>
    <item>
      <title>T-calibration in semi-parametric models</title>
      <link>https://arxiv.org/abs/2502.11727</link>
      <description>arXiv:2502.11727v1 Announce Type: new 
Abstract: This note relates the calibration of models to the consistent loss functions for the target functional of the model. We demonstrate that a model is calibrated if and only if there is a parameter value that is optimal under all consistent loss functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11727v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anja M\"uhlemann, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Forecasting time series with constraints</title>
      <link>https://arxiv.org/abs/2502.10485</link>
      <description>arXiv:2502.10485v1 Announce Type: cross 
Abstract: Time series forecasting presents unique challenges that limit the effectiveness of traditional machine learning algorithms. To address these limitations, various approaches have incorporated linear constraints into learning algorithms, such as generalized additive models and hierarchical forecasting. In this paper, we propose a unified framework for integrating and combining linear constraints in time series forecasting. Within this framework, we show that the exact minimizer of the constrained empirical risk can be computed efficiently using linear algebra alone. This approach allows for highly scalable implementations optimized for GPUs. We validate the proposed methodology through extensive benchmarking on real-world tasks, including electricity demand forecasting and tourism forecasting, achieving state-of-the-art performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10485v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Doum\`eche (LPSM, EDF R&amp;D OSIRIS), Francis Bach (DI-ENS, SIERRA), \'Eloi Bedek (EDF R&amp;D OSIRIS), G\'erard Biau (LPSM, IUF), Claire Boyer (LMO, IUF), Yannig Goude (EDF R&amp;D OSIRIS, LMO)</dc:creator>
    </item>
    <item>
      <title>A Power Transform</title>
      <link>https://arxiv.org/abs/2502.10647</link>
      <description>arXiv:2502.10647v1 Announce Type: cross 
Abstract: Power transforms, such as the Box-Cox transform and Tukey's ladder of powers, are a fundamental tool in mathematics and statistics. These transforms are primarily used for normalizing and standardizing datasets, effectively by raising values to a power. In this work I present a novel power transform, and I show that it serves as a unifying framework for wide family of loss functions, kernel functions, probability distributions, bump functions, and neural network activation functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10647v1</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan T. Barron</dc:creator>
    </item>
    <item>
      <title>Maximal Inequalities for Separately Exchangeable Empirical Processes</title>
      <link>https://arxiv.org/abs/2502.11432</link>
      <description>arXiv:2502.11432v1 Announce Type: cross 
Abstract: This paper derives new maximal inequalities for empirical processes associated with separately exchangeable (SE) random arrays. For any fixed index dimension \(K\ge 1\), we establish a global maximal inequality that bounds the \(q\)-th moment, for any \(q\in[1,\infty)\), of the supremum of these processes. In addition, we obtain a refined local maximal inequality that controls the first absolute moment of the supremum. Both results are proved for a general pointwise measurable class of functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11432v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harold D. Chiang</dc:creator>
    </item>
    <item>
      <title>Low-Rank Thinning</title>
      <link>https://arxiv.org/abs/2502.12063</link>
      <description>arXiv:2502.12063v1 Announce Type: cross 
Abstract: The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12063v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Robust low-rank tensor regression via clipping and Huber loss</title>
      <link>https://arxiv.org/abs/2205.01582</link>
      <description>arXiv:2205.01582v2 Announce Type: replace 
Abstract: In this paper, we construct a parameter estimation framework under robust low-rank tensor regression based on the truncation method and Huber loss, and study robust low-rank tensor regression model under random noise with only finite second-order moment. Through the gradient descent method, our proposed Huber-type robust estimator is theoretically optimal in two aspects: (1) our statistical error rate is nearly the same as the optimal upper bound deduced by the traditional least squares method under sub-Gaussian error; (2) the sample complexity of recovering the tensor parameter is also optimal. Extensive numerical experiments show the robustness of our estimator, and the utilization of truncation and Huber loss is beneficial to improve the stability and statistical effectiveness of the proposed algorithm which is superior to the least squares method. Meanwhile, the phenomenon of phase transition in the convergence rate of the proposed robust estimator is confirmed through simulation. Furthermore, we apply this estimation technique to image compression, which demonstrates that our method is more effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.01582v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kangqiang Li, Bingqi Liu, Yang Yang, Junyang Yu, Li Wang</dc:creator>
    </item>
    <item>
      <title>Conditional Aalen--Johansen estimation</title>
      <link>https://arxiv.org/abs/2303.02119</link>
      <description>arXiv:2303.02119v3 Announce Type: replace 
Abstract: The conditional Aalen--Johansen estimator, a general-purpose non-parametric estimator of conditional state occupation probabilities, is introduced. The estimator is applicable for any finite-state jump process and supports conditioning on external as well as internal covariate information. The conditioning feature permits for a much more detailed analysis of the distributional characteristics of the process. The estimator reduces to the conditional Kaplan--Meier estimator in the special case of a survival model and also englobes other, more recent, landmark estimators when covariates are discrete. Strong uniform consistency and asymptotic normality are established under lax moment conditions on the multivariate counting process, allowing in particular for an unbounded number of transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02119v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Christian Furrer</dc:creator>
    </item>
    <item>
      <title>Multiple Testing under High-dimensional Dynamic Factor Model</title>
      <link>https://arxiv.org/abs/2303.07631</link>
      <description>arXiv:2303.07631v3 Announce Type: replace 
Abstract: Large-scale multiple testing under static factor models is commonly used to select skilled funds in financial market. However, static factor models are arguably too stringent as it ignores the serial correlation, which severely distorts error rate control in large-scale inference. In this manuscript, we propose a new multiple testing procedure under dynamic factor models that is robust to the nonlinear serial dependence (e.g., GARCH). The idea is to integrate a new sample-splitting strategy based on chronological order and a two-pass Fama-Macbeth regression to form a series of statistics with marginal symmetry properties and then to utilize the symmetry properties to obtain a data-driven threshold. We show that our procedure is able to control the false discovery rate (FDR) asymptotically under high-dimensional dynamic factor models. As a byproduct that is of independent interest, we establish a new exponential-type deviation inequality for the sum of random variables on a variety of functionals of linear and non-linear processes. Numerical results including a case study on hedge fund selection demonstrate the advantage of the proposed method over several state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07631v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinxin Yang, Lilun Du</dc:creator>
    </item>
    <item>
      <title>Censored extreme value estimation</title>
      <link>https://arxiv.org/abs/2312.10499</link>
      <description>arXiv:2312.10499v5 Announce Type: replace 
Abstract: A novel and comprehensive methodology designed to tackle the challenges posed by extreme values in the context of random censorship is introduced. The main focus is on the analysis of integrals based on the product-limit estimator of normalized upper order statistics, called extreme Kaplan--Meier integrals. These integrals allow for the transparent derivation of various important asymptotic distributional properties, offering an alternative approach to conventional plug-in estimation methods. Notably, this methodology demonstrates robustness and wide applicability among various tail regimes. A noteworthy by-product is the extension of generalized Hill-type estimators of extremes to encompass arbitrary tail behavior, which is of independent interest. The theoretical framework is applied to construct novel estimators for real-valued extreme value indices for right-censored data. Simulation studies confirm the asymptotic results and, in a competitor case, mostly show superiority in mean square error. An application to brain cancer data demonstrates that censoring effects are properly accounted for, even when focusing solely on tail classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10499v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Igor Rodionov</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian intensity estimation for covariate-driven inhomogeneous point processes</title>
      <link>https://arxiv.org/abs/2312.14073</link>
      <description>arXiv:2312.14073v3 Announce Type: replace 
Abstract: This work studies nonparametric Bayesian estimation of the intensity function of an inhomogeneous Poisson point process in the important case where the intensity depends on covariates, based on the observation of a single realisation of the point pattern over a large area. It is shown how the presence of covariates allows to borrow information from far away locations in the observation window, enabling consistent inference in the growing domain asymptotics. In particular, optimal posterior contraction rates under both global and point-wise loss functions are derived. The rates in global loss are obtained under conditions on the prior distribution resembling those in the well established theory of Bayesian nonparametrics, combined with concentration inequalities for functionals of stationary processes to control certain random covariate-dependent loss functions appearing in the analysis. The local rates are derived with an ad-hoc study that builds on recent advances in the theory of P\'olya tree priors, extended to the present multivariate setting with a novel construction that makes use of the random geometry induced by the covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14073v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Giordano, Alisa Kirichenko, Judith Rousseau</dc:creator>
    </item>
    <item>
      <title>The Rank of the Odd Normal Out</title>
      <link>https://arxiv.org/abs/2401.00952</link>
      <description>arXiv:2401.00952v2 Announce Type: replace 
Abstract: A century ago Thurstone wrote about using $\mathbf{X}\sim\mathcal{N}_{n}\left(\boldsymbol{\mu},\boldsymbol{\Sigma}\right)$ to model our preferences (1927a,b,c). Today latent normals frequently model random rankings. Despite this, for $R_{i}:=\#\left\{ 1\leq j\leq n:X_{j}\leq X_{i}\right\} $, little is known about $\left(R_{i_{0}},\ldots,R_{i_{m}}\right)$'s distribution in non-i.i.d. settings. We consider the simplest of such settings, namely that with $n+1$ independent normals, where $X_{0}\sim\mathcal{N}\left(\mu_{0},\sigma_{0}^{2}\right)$ and $X_{1},X_{2},\ldots,X_{n}\sim\mathcal{N}\left(\mu,\sigma^{2}\right)$. In this setting $\left(\left.R_{0}\right|X_{0}\right)\sim1+\mathrm{Binomial}\left(n,\Phi\left(\left(X_{0}-\mu\right)/\sigma\right)\right)$, and $\Phi\left(\left(X_{0}-\mu\right)/\sigma\right)$ is approximately beta-distributed. The beta distribution's conjugacy for the binomial implies that $R_{0}-1$ is roughly beta-binomial. We approximate the distribution of $\left(R_{i_{0}},\ldots,R_{i_{m}}\right)$, deriving $\mathbb{E}R_{i}$, $\mathrm{Var}\left(R_{i}\right)$, $\mathrm{Cov}\left(R_{i},R_{j}\right)$, and its limiting distributions as key parameters grow large or small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00952v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip T. Labo</dc:creator>
    </item>
    <item>
      <title>Combining exchangeable p-values</title>
      <link>https://arxiv.org/abs/2404.03484</link>
      <description>arXiv:2404.03484v4 Announce Type: replace 
Abstract: The problem of combining p-values is an old and fundamental one, and the classic assumption of independence is often violated or unverifiable in many applications. There are many well-known rules that can combine a set of arbitrarily dependent p-values (for the same hypothesis) into a single p-value. We show that essentially all these existing rules can be strictly improved when the p-values are exchangeable, or when external randomization is allowed (or both). For example, we derive randomized and/or exchangeable improvements of well known rules like ``twice the median'' and ``twice the average'', as well as geometric and harmonic means. Exchangeable p-values are often produced one at a time (for example, under repeated tests involving data splitting), and our rules can combine them sequentially as they are produced, stopping when the combined p-values stabilize. Our work also improves rules for combining arbitrarily dependent p-values, since the latter becomes exchangeable if they are presented to the analyst in a random order. The main technical advance is to show that all existing combination rules can be obtained by calibrating the p-values to e-values (using an $\alpha$-dependent calibrator), averaging those e-values, converting to a level-$\alpha$ test using Markov's inequality, and finally obtaining p-values by combining this family of tests; the improvements are delivered via recent randomized and exchangeable variants of Markov's inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03484v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Gasparin, Ruodu Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Manifold Learning with Sparse Regularised Optimal Transport</title>
      <link>https://arxiv.org/abs/2307.09816</link>
      <description>arXiv:2307.09816v2 Announce Type: replace-cross 
Abstract: Manifold learning is a central task in modern statistics and data science. Many datasets (cells, documents, images, molecules) can be represented as point clouds embedded in a high dimensional ambient space, however the degrees of freedom intrinsic to the data are usually far fewer than the number of ambient dimensions. The task of detecting a latent manifold along which the data are embedded is a prerequisite for a wide family of downstream analyses. Real-world datasets are subject to noisy observations and sampling, so that distilling information about the underlying manifold is a major challenge. We propose a method for manifold learning that utilises a symmetric version of optimal transport with a quadratic regularisation that constructs a sparse and adaptive affinity matrix, that can be interpreted as a generalisation of the bistochastic kernel normalisation.
  We prove that the resulting kernel is consistent with a Laplace-type operator in the continuous limit, establish robustness to heteroskedastic noise and exhibit these results in numerical experiments. We identify a highly efficient computational scheme for computing this optimal transport for discrete data and demonstrate that it outperforms competing methods in a set of examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.09816v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Zhang, Gilles Mordant, Tetsuya Matsumoto, Geoffrey Schiebinger</dc:creator>
    </item>
    <item>
      <title>On the Efficiency of Finely Stratified Experiments</title>
      <link>https://arxiv.org/abs/2307.15181</link>
      <description>arXiv:2307.15181v5 Announce Type: replace-cross 
Abstract: This paper studies the use of finely stratified designs for the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments. By a "finely stratified" design, we mean experiments in which units are divided into groups of a fixed size and a proportion within each group is assigned to a binary treatment uniformly at random. The class of parameters considered are those that can be expressed as the solution to a set of moment conditions constructed using a known function of the observed data. They include, among other things, average treatment effects, quantile treatment effects, and local average treatment effects as well as the counterparts to these quantities in experiments in which the unit is itself a cluster. In this setting, we establish three results. First, we show that under a finely stratified design, the na\"ive method of moments estimator achieves the same asymptotic variance as what could typically be attained under alternative treatment assignment mechanisms only through ex post covariate adjustment. Second, we argue that the na\"ive method of moments estimator under a finely stratified design is asymptotically efficient by deriving a lower bound on the asymptotic variance of regular estimators of the parameter of interest in the form of a convolution theorem. Finally, we establish conditions under which a "fast-balancing" property of finely stratified designs is in fact necessary for the na\"ive method of moments estimator to attain the efficiency bound. In this sense, finely stratified experiments are attractive because they lead to efficient estimators of treatment effect parameters "by design."</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15181v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Jizhou Liu, Azeem M. Shaikh, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Combining Evidence Across Filtrations</title>
      <link>https://arxiv.org/abs/2402.09698</link>
      <description>arXiv:2402.09698v3 Announce Type: replace-cross 
Abstract: In sequential anytime-valid inference, any admissible procedure must be based on e-processes: generalizations of test martingales that quantify the accumulated evidence against a composite null hypothesis at any stopping time. This paper proposes a method for combining e-processes constructed in different filtrations but for the same null. Although e-processes in the same filtration can be combined effortlessly (by averaging), e-processes in different filtrations cannot because their validity in a coarser filtration does not translate to a finer filtration. This issue arises in sequential tests of randomness and independence, as well as in the evaluation of sequential forecasters. We establish that a class of functions called adjusters can lift arbitrary e-processes across filtrations. The result yields a generally applicable "adjust-then-combine" procedure, which we demonstrate on the problem of testing randomness in real-world financial data. Furthermore, we prove a characterization theorem for adjusters that formalizes a sense in which using adjusters is necessary. There are two major implications. First, if we have a powerful e-process in a coarsened filtration, then we readily have a powerful e-process in the original filtration. Second, when we coarsen the filtration to construct an e-process, there is a logarithmic cost to recovering validity in the original filtration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09698v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yo Joong Choe, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Estimation of Integrated Volatility Functionals with Kernel Spot Volatility Estimators</title>
      <link>https://arxiv.org/abs/2407.09759</link>
      <description>arXiv:2407.09759v2 Announce Type: replace-cross 
Abstract: For a multidimensional It\^o semimartingale, we consider the problem of estimating integrated volatility functionals. Jacod and Rosenbaum (2013) studied a plug-in type of estimator based on a Riemann sum approximation of the integrated functional and a spot volatility estimator with a forward uniform kernel. Motivated by recent results that show that spot volatility estimators with general two-side kernels of unbounded support are more accurate, in this paper, an estimator using a general kernel spot volatility estimator as the plug-in is considered. A biased central limit theorem for estimating the integrated functional is established with an optimal convergence rate. Unbiased central limit theorems for estimators with proper de-biasing terms are also obtained both at the optimal convergence regime for the bandwidth and when applying undersmoothing. Our results show that one can significantly reduce the estimator's bias by adopting a general kernel instead of the standard uniform kernel. Our proposed bias-corrected estimators are found to maintain remarkable robustness against bandwidth selection in a variety of sampling frequencies and functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09759v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e E. Figueroa-L\'opez, Jincheng Pang, Bei Wu</dc:creator>
    </item>
    <item>
      <title>Provable Benefit of Annealed Langevin Monte Carlo for Non-log-concave Sampling</title>
      <link>https://arxiv.org/abs/2407.16936</link>
      <description>arXiv:2407.16936v2 Announce Type: replace-cross 
Abstract: We consider the outstanding problem of sampling from an unnormalized density that may be non-log-concave and multimodal. To enhance the performance of simple Markov chain Monte Carlo (MCMC) methods, techniques of annealing type have been widely used. However, quantitative theoretical guarantees of these techniques are under-explored. This study takes a first step toward providing a non-asymptotic analysis of annealed MCMC. Specifically, we establish, for the first time, an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\cal A}^2}{\varepsilon^6}\right)$ for the simple annealed Langevin Monte Carlo algorithm to achieve $\varepsilon^2$ accuracy in Kullback-Leibler divergence to the target distribution $\pi\propto{\rm e}^{-V}$ on $\mathbb{R}^d$ with $\beta$-smooth potential $V$. Here, ${\cal A}$ represents the action of a curve of probability measures interpolating the target distribution $\pi$ and a readily sampleable distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16936v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Guo, Molei Tao, Yongxin Chen</dc:creator>
    </item>
    <item>
      <title>Asymptotics for Random Quadratic Transportation Costs</title>
      <link>https://arxiv.org/abs/2409.08612</link>
      <description>arXiv:2409.08612v2 Announce Type: replace-cross 
Abstract: We establish the validity of asymptotic limits for the general transportation problem between random i.i.d. points and their common distribution, with respect to the squared Euclidean distance cost, in any dimension larger than three. Previous results were essentially limited to the two (or one) dimensional case, or to distributions whose absolutely continuous part is uniform.
  The proof relies upon recent advances in the stability theory of optimal transportation, combined with functional analytic techniques and some ideas from quantitative stochastic homogenization. The key tool we develop is a quantitative upper bound for the usual quadratic optimal transportation problem in terms of its boundary variant, where points can be freely transported along the boundary. The methods we use are applicable to more general random measures, including occupation measure of Brownian paths, and may open the door to further progress on challenging problems at the interface of analysis, probability, and discrete mathematics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08612v2</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Martin Huesmann, Michael Goldman, Dario Trevisan</dc:creator>
    </item>
    <item>
      <title>Functional Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2410.03619</link>
      <description>arXiv:2410.03619v4 Announce Type: replace-cross 
Abstract: Heterogeneous functional data commonly arise in time series and longitudinal studies. To uncover the statistical structures of such data, we propose Functional Singular Value Decomposition (FSVD), a unified framework encompassing various tasks for the analysis of functional data with potential heterogeneity. We establish the mathematical foundation of FSVD by proving its existence and providing its fundamental properties. We then develop an implementation approach for noisy and irregularly observed functional data based on a novel alternating minimization scheme and provide theoretical guarantees for its convergence and estimation accuracy. The FSVD framework also introduces the concepts of intrinsic basis functions and intrinsic basis vectors, representing two fundamental structural aspects of random functions. These concepts enable FSVD to provide new and improved solutions to tasks including functional principal component analysis, factor models, functional clustering, functional linear regression, and functional completion, while effectively handling heterogeneity and irregular temporal sampling. Through extensive simulations, we demonstrate that FSVD-based methods consistently outperform existing methods across these tasks. To showcase the value of FSVD in real-world datasets, we apply it to extract temporal patterns from a COVID-19 case count dataset and perform data completion on an electronic health record dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03619v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Precise Asymptotics and Refined Regret of Variance-Aware UCB</title>
      <link>https://arxiv.org/abs/2412.08843</link>
      <description>arXiv:2412.08843v2 Announce Type: replace-cross 
Abstract: In this paper, we study the behavior of the Upper Confidence Bound-Variance (UCB-V) algorithm for the Multi-Armed Bandit (MAB) problems, a variant of the canonical Upper Confidence Bound (UCB) algorithm that incorporates variance estimates into its decision-making process. More precisely, we provide an asymptotic characterization of the arm-pulling rates for UCB-V, extending recent results for the canonical UCB in Kalvit and Zeevi (2021) and Khamaru and Zhang (2024). In an interesting contrast to the canonical UCB, our analysis reveals that the behavior of UCB-V can exhibit instability, meaning that the arm-pulling rates may not always be asymptotically deterministic. Besides the asymptotic characterization, we also provide non-asymptotic bounds for the arm-pulling rates in the high probability regime, offering insights into the regret analysis. As an application of this high probability result, we establish that UCB-V can achieve a more refined regret bound, previously unknown even for more complicate and advanced variance-aware online decision-making algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08843v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Fan, Yuxuan Han, Jinchi Lv, Xiaocong Xu, Zhengyuan Zhou</dc:creator>
    </item>
    <item>
      <title>Estimating shared subspace with AJIVE: the power and limitation of multiple data matrices</title>
      <link>https://arxiv.org/abs/2501.09336</link>
      <description>arXiv:2501.09336v2 Announce Type: replace-cross 
Abstract: Integrative data analysis often requires disentangling joint and individual variations across multiple datasets, a challenge commonly addressed by the Joint and Individual Variation Explained (JIVE) model. While numerous methods have been developed to estimate the shared subspace under JIVE, the theoretical understanding of their performance remains limited, particularly in the context of multiple matrices and varying degrees of subspace misalignment. This paper bridges this gap by providing a systematic analysis of shared subspace estimation in multi-matrix settings.
  We focus on the Angle-based Joint and Individual Variation Explained (AJIVE) method, a two-stage spectral approach, and establish new performance guarantees that uncover its strengths and limitations. Specifically, we show that in high signal-to-noise ratio (SNR) regimes, AJIVE's estimation error decreases with the number of matrices, demonstrating the power of multi-matrix integration. Conversely, in low-SNR settings, AJIVE exhibits a non-diminishing error, highlighting fundamental limitations. To complement these results, we derive minimax lower bounds, showing that AJIVE achieves optimal rates in high-SNR regimes. Furthermore, we analyze an oracle-aided spectral estimator to demonstrate that the non-diminishing error in low-SNR scenarios is a fundamental barrier. Extensive numerical experiments corroborate our theoretical findings, providing insights into the interplay between SNR, the number of matrices, and subspace misalignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09336v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuepeng Yang, Cong Ma</dc:creator>
    </item>
    <item>
      <title>A New Proof for the Linear Filtering and Smoothing Equations, and Asymptotic Expansion of Nonlinear Filtering</title>
      <link>https://arxiv.org/abs/2501.16333</link>
      <description>arXiv:2501.16333v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a new approach to the linear filtering and smoothing problem and demonstrate its applicability to nonlinear filtering. For the linear case, our main theorem provides an explicit expression for the conditional distribution of the hidden process given the observations, leading to a novel derivation of the linear filtering and smoothing equations. Moreover, the theorem offers an efficient framework for computing the asymptotic expansion of nonlinear filtering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16333v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kurisaki</dc:creator>
    </item>
  </channel>
</rss>
