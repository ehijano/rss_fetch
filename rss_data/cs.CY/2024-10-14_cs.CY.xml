<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Oct 2024 03:31:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>New technologies and AI: envisioning future directions for UNSCR 1540</title>
      <link>https://arxiv.org/abs/2410.08216</link>
      <description>arXiv:2410.08216v1 Announce Type: new 
Abstract: This paper investigates the emerging challenges posed by the integration of Artificial Intelligence (AI) in the military domain, particularly within the context of United Nations Security Council Resolution 1540 (UNSCR 1540), which seeks to prevent the proliferation of weapons of mass destruction (WMDs). While the resolution initially focused on nuclear, chemical, and biological threats, the rapid advancement of AI introduces new complexities that were previously unanticipated. We critically analyze how AI can both exacerbate existing risks associated with WMDs (e.g., thorough the deployment of kamikaze drones and killer robots) and introduce novel threats (e.g., by exploiting Generative AI potentialities), thereby compromising international peace and security. The paper calls for an expansion of UNSCR 1540 to address the growing influence of AI technologies in the development, dissemination, and potential misuse of WMDs, urging the creation of a governance framework to mitigate these emerging risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08216v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Clara Punzi</dc:creator>
    </item>
    <item>
      <title>Digital Twins of Business Processes: A Research Manifesto</title>
      <link>https://arxiv.org/abs/2410.08219</link>
      <description>arXiv:2410.08219v1 Announce Type: new 
Abstract: Modern organizations necessitate continuous business processes improvement to maintain efficiency, adaptability, and competitiveness. In the last few years, the Internet of Things, via the deployment of sensors and actuators, has heavily been adopted in organizational and industrial settings to monitor and automatize physical processes influencing and enhancing how people and organizations work. Such advancements are now pushed forward by the rise of the Digital Twin paradigm applied to organizational processes. Advanced ways of managing and maintaining business processes come within reach as there is a Digital Twin of a business process - a virtual replica with real-time capabilities of a real process occurring in an organization. Combining business process models with real-time data and simulation capabilities promises to provide a new way to guide day-to-day organization activities. However, integrating Digital Twins and business processes is a non-trivial task, presenting numerous challenges and ambiguities. This manifesto paper aims to contribute to the current state of the art by clarifying the relationship between business processes and Digital Twins, identifying ongoing research and open challenges, thereby shedding light on and driving future exploration of this innovative interplay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08219v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fabrizio Fornari, Ivan Compagnucci, Massimo Callisto De Donato, Yannis Bertrand, Harry Herbert Beyel, Emilio Carri\'on, Marco Franceschetti, Wolfgang Groher, Joscha Gr\"uger, Emre Kilic, Agnes Koschmider, Francesco Leotta, Chiao-Yun Li, Giovanni Lugaresi, Lukas Malburg, Juergen Mangler, Massimo Mecella, Oscar Pastor, Uwe Riss, Ronny Seiger, Estefania Serral, Victoria Torres, Pedro Valderas</dc:creator>
    </item>
    <item>
      <title>Assessing Privacy Policies with AI: Ethical, Legal, and Technical Challenges</title>
      <link>https://arxiv.org/abs/2410.08381</link>
      <description>arXiv:2410.08381v1 Announce Type: new 
Abstract: The growing use of Machine Learning and Artificial Intelligence (AI), particularly Large Language Models (LLMs) like OpenAI's GPT series, leads to disruptive changes across organizations. At the same time, there is a growing concern about how organizations handle personal data. Thus, privacy policies are essential for transparency in data processing practices, enabling users to assess privacy risks. However, these policies are often long and complex. This might lead to user confusion and consent fatigue, where users accept data practices against their interests, and abusive or unfair practices might go unnoticed. LLMss can be used to assess privacy policies for users automatically. In this interdisciplinary work, we explore the challenges of this approach in three pillars, namely technical feasibility, ethical implications, and legal compatibility of using LLMs to assess privacy policies. Our findings aim to identify potential for future research, and to foster a discussion on the use of LLM technologies for enabling users to fulfil their important role as decision-makers in a constantly developing AI-driven digital economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08381v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Irem Aydin, Hermann Diebel-Fischer, Vincent Freiberger, Julia M\"oller-Klapperich, Erik Buchmann, Michael F\"arber, Anne Lauber-R\"onsberg, Birte Platow</dc:creator>
    </item>
    <item>
      <title>Can LLMs advance democratic values?</title>
      <link>https://arxiv.org/abs/2410.08418</link>
      <description>arXiv:2410.08418v1 Announce Type: new 
Abstract: LLMs are among the most advanced tools ever devised for analysing and generating linguistic content. Democratic deliberation and decision-making involve, at several distinct stages, the production and analysis of language. So it is natural to ask whether our best tools for manipulating language might prove instrumental to one of our most important linguistic tasks. Researchers and practitioners have recently asked whether LLMs can support democratic deliberation by leveraging abilities to summarise content, as well as to aggregate opinion over summarised content, and indeed to represent voters by predicting their preferences over unseen choices. In this paper, we assess whether using LLMs to perform these and related functions really advances the democratic values that inspire these experiments. We suggest that the record is decidedly mixed. In the presence of background inequality of power and resources, as well as deep moral and political disagreement, we should be careful not to use LLMs in ways that automate non-instrumentally valuable components of the democratic process, or else threaten to supplant fair and transparent decision-making procedures that are necessary to reconcile competing interests and values. However, while we argue that LLMs should be kept well clear of formal democratic decision-making processes, we think that they can be put to good use in strengthening the informal public sphere: the arena that mediates between democratic governments and the polities that they serve, in which political communities seek information, form civic publics, and hold their leaders to account.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08418v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seth Lazar, Lorenzo Manuali</dc:creator>
    </item>
    <item>
      <title>"I Am the One and Only, Your Cyber BFF": Understanding the Impact of GenAI Requires Understanding the Impact of Anthropomorphic AI</title>
      <link>https://arxiv.org/abs/2410.08526</link>
      <description>arXiv:2410.08526v1 Announce Type: new 
Abstract: Many state-of-the-art generative AI (GenAI) systems are increasingly prone to anthropomorphic behaviors, i.e., to generating outputs that are perceived to be human-like. While this has led to scholars increasingly raising concerns about possible negative impacts such anthropomorphic AI systems can give rise to, anthropomorphism in AI development, deployment, and use remains vastly overlooked, understudied, and underspecified. In this perspective, we argue that we cannot thoroughly map the social impacts of generative AI without mapping the social impacts of anthropomorphic AI, and outline a call to action.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08526v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myra Cheng, Alicia DeVrio, Lisa Egede, Su Lin Blodgett, Alexandra Olteanu</dc:creator>
    </item>
    <item>
      <title>Design of Secure, Privacy-focused, and Accessible E-Payment Applications for Older Adults</title>
      <link>https://arxiv.org/abs/2410.08555</link>
      <description>arXiv:2410.08555v1 Announce Type: new 
Abstract: E-payments are essential for transactional convenience in today's digital economy and are becoming increasingly important for older adults, emphasizing the need for enhanced security, privacy, and usability. To address this, we conducted a survey-based study with 400 older adults aged 65 and above to evaluate a high-fidelity prototype of an e-payment mobile application, which included features such as multi-factor authentication (MFA) and QR code-based recipient addition. Based on our findings, we developed a tailored \b{eta} version of the application to meet the specific needs of this demographic. Notably, approximately 91% of participants preferred traditional knowledge-based and single-mode authentication compared to expert-recommended MFA. We concluded by providing recommendations aimed at developing inclusive e-payment solutions that address the security, privacy, and usability requirements of older adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08555v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:journal_reference>BuildSEC'24 Building a Secure &amp; Empowered Cyberspace; 19-21 December 2024, New Delhi, India</arxiv:journal_reference>
      <dc:creator>Sanchari Das</dc:creator>
    </item>
    <item>
      <title>A Social Context-aware Graph-based Multimodal Attentive Learning Framework for Disaster Content Classification during Emergencies</title>
      <link>https://arxiv.org/abs/2410.08814</link>
      <description>arXiv:2410.08814v1 Announce Type: new 
Abstract: In times of crisis, the prompt and precise classification of disaster-related information shared on social media platforms is crucial for effective disaster response and public safety. During such critical events, individuals use social media to communicate, sharing multimodal textual and visual content. However, due to the significant influx of unfiltered and diverse data, humanitarian organizations face challenges in leveraging this information efficiently. Existing methods for classifying disaster-related content often fail to model users' credibility, emotional context, and social interaction information, which are essential for accurate classification. To address this gap, we propose CrisisSpot, a method that utilizes a Graph-based Neural Network to capture complex relationships between textual and visual modalities, as well as Social Context Features to incorporate user-centric and content-centric information. We also introduce Inverted Dual Embedded Attention (IDEA), which captures both harmonious and contrasting patterns within the data to enhance multimodal interactions and provide richer insights. Additionally, we present TSEqD (Turkey-Syria Earthquake Dataset), a large annotated dataset for a single disaster event, containing 10,352 samples. Through extensive experiments, CrisisSpot demonstrated significant improvements, achieving an average F1-score gain of 9.45% and 5.01% compared to state-of-the-art methods on the publicly available CrisisMMD dataset and the TSEqD dataset, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08814v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.eswa.2024.125337</arxiv:DOI>
      <arxiv:journal_reference>journal={Expert Systems with Applications},pages={125337},year={2024},publisher={Elsevier}</arxiv:journal_reference>
      <dc:creator>Shahid Shafi Dar, Mohammad Zia Ur Rehman, Karan Bais, Mohammed Abdul Haseeb, Nagendra Kumara</dc:creator>
    </item>
    <item>
      <title>Wikimedia data for AI: a review of Wikimedia datasets for NLP tasks and AI-assisted editing</title>
      <link>https://arxiv.org/abs/2410.08918</link>
      <description>arXiv:2410.08918v1 Announce Type: new 
Abstract: Wikimedia content is used extensively by the AI community and within the language modeling community in particular. In this paper, we provide a review of the different ways in which Wikimedia data is curated to use in NLP tasks across pre-training, post-training, and model evaluations. We point to opportunities for greater use of Wikimedia content but also identify ways in which the language modeling community could better center the needs of Wikimedia editors. In particular, we call for incorporating additional sources of Wikimedia data, a greater focus on benchmarks for LLMs that encode Wikimedia principles, and greater multilingualism in Wikimedia-derived datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08918v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Johnson, Lucie-Aim\'ee Kaffee, Miriam Redi</dc:creator>
    </item>
    <item>
      <title>From Uncertainty to Innovation: Wearable Prototyping with ProtoBot</title>
      <link>https://arxiv.org/abs/2410.08340</link>
      <description>arXiv:2410.08340v1 Announce Type: cross 
Abstract: Despite AI advancements, individuals without software or hardware expertise still face barriers in designing wearable electronic devices due to the lack of code-free prototyping tools. To eliminate these barriers, we designed ProtoBot, leveraging large language models, and conducted a case study with four professionals from different disciplines through playful interaction. The study resulted in four unique wearable device concepts, with participants using Protobot to prototype selected components. From this experience, we learned that (1) uncertainty can be turned into a positive experience, (2) the ProtoBot should transform to reliably act as a guide, and (3) users need to adjust design parameters when interacting with the prototypes. Our work demonstrates, for the first time, the use of large language models in rapid prototyping of wearable electronics. We believe this approach will pioneer rapid prototyping without fear of uncertainties for people who want to develop both wearable prototypes and other products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08340v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.PL</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\.Ihsan Ozan Y{\i}ld{\i}r{\i}m, Cansu \c{C}etin Er, Ege Keskin, Murat Ku\c{s}cu, O\u{g}uzhan \"Ozcan</dc:creator>
    </item>
    <item>
      <title>Language model developers should report train-test overlap</title>
      <link>https://arxiv.org/abs/2410.08385</link>
      <description>arXiv:2410.08385v1 Announce Type: cross 
Abstract: Language models are extensively evaluated, but correctly interpreting evaluation results requires knowledge of train-test overlap which refers to the extent to which the language model is trained on the very data it is being tested on. The public currently lacks adequate information about train-test overlap: most models have no public train-test overlap statistics, and third parties cannot directly measure train-test overlap since they do not have access to the training data. To make this clear, we document the practices of 30 model developers, finding that just 9 developers report train-test overlap: 4 developers release training data under open-source licenses, enabling the community to directly measure train-test overlap, and 5 developers publish their train-test overlap methodology and statistics. By engaging with language model developers, we provide novel information about train-test overlap for three additional developers. Overall, we take the position that language model developers should publish train-test overlap statistics and/or training data whenever they report evaluation results on public test sets. We hope our work increases transparency into train-test overlap to increase the community-wide trust in model evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08385v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andy K Zhang, Kevin Klyman, Yifan Mai, Yoav Levine, Yian Zhang, Rishi Bommasani, Percy Liang</dc:creator>
    </item>
    <item>
      <title>What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias</title>
      <link>https://arxiv.org/abs/2410.08407</link>
      <description>arXiv:2410.08407v1 Announce Type: cross 
Abstract: Knowledge Distillation is a commonly used Deep Neural Network compression method, which often maintains overall generalization performance. However, we show that even for balanced image classification datasets, such as CIFAR-100, Tiny ImageNet and ImageNet, as many as 41% of the classes are statistically significantly affected by distillation when comparing class-wise accuracy (i.e. class bias) between a teacher/distilled student or distilled student/non-distilled student model. Changes in class bias are not necessarily an undesirable outcome when considered outside of the context of a model's usage. Using two common fairness metrics, Demographic Parity Difference (DPD) and Equalized Odds Difference (EOD) on models trained with the CelebA, Trifeature, and HateXplain datasets, our results suggest that increasing the distillation temperature improves the distilled student model's fairness -- for DPD, the distilled student even surpasses the fairness of the teacher model at high temperatures. This study highlights the uneven effects of Knowledge Distillation on certain classes and its potentially significant role in fairness, emphasizing that caution is warranted when using distilled models for sensitive application domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08407v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aida Mohammadshahi, Yani Ioannou</dc:creator>
    </item>
    <item>
      <title>SocialGaze: Improving the Integration of Human Social Norms in Large Language Models</title>
      <link>https://arxiv.org/abs/2410.08698</link>
      <description>arXiv:2410.08698v1 Announce Type: cross 
Abstract: While much research has explored enhancing the reasoning capabilities of large language models (LLMs) in the last few years, there is a gap in understanding the alignment of these models with social values and norms. We introduce the task of judging social acceptance. Social acceptance requires models to judge and rationalize the acceptability of people's actions in social situations. For example, is it socially acceptable for a neighbor to ask others in the community to keep their pets indoors at night? We find that LLMs' understanding of social acceptance is often misaligned with human consensus. To alleviate this, we introduce SocialGaze, a multi-step prompting framework, in which a language model verbalizes a social situation from multiple perspectives before forming a judgment. Our experiments demonstrate that the SocialGaze approach improves the alignment with human judgments by up to 11 F1 points with the GPT-3.5 model. We also identify biases and correlations in LLMs in assigning blame that is related to features such as the gender (males are significantly more likely to be judged unfairly) and age (LLMs are more aligned with humans for older narrators).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08698v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anvesh Rao Vijjini, Rakesh R. Menon, Jiayi Fu, Shashank Srivastava, Snigdha Chaturvedi</dc:creator>
    </item>
    <item>
      <title>Integrating Expert Judgment and Algorithmic Decision Making: An Indistinguishability Framework</title>
      <link>https://arxiv.org/abs/2410.08783</link>
      <description>arXiv:2410.08783v1 Announce Type: cross 
Abstract: We introduce a novel framework for human-AI collaboration in prediction and decision tasks. Our approach leverages human judgment to distinguish inputs which are algorithmically indistinguishable, or "look the same" to any feasible predictive algorithm. We argue that this framing clarifies the problem of human-AI collaboration in prediction and decision tasks, as experts often form judgments by drawing on information which is not encoded in an algorithm's training data. Algorithmic indistinguishability yields a natural test for assessing whether experts incorporate this kind of "side information", and further provides a simple but principled method for selectively incorporating human feedback into algorithmic predictions. We show that this method provably improves the performance of any feasible algorithmic predictor and precisely quantify this improvement. We demonstrate the utility of our framework in a case study of emergency room triage decisions, where we find that although algorithmic risk scores are highly competitive with physicians, there is strong evidence that physician judgments provide signal which could not be replicated by any predictive algorithm. This insight yields a range of natural decision rules which leverage the complementary strengths of human experts and predictive algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08783v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Alur, Loren Laine, Darrick K. Li, Dennis Shung, Manish Raghavan, Devavrat Shah</dc:creator>
    </item>
    <item>
      <title>A Block-Based Testing Framework for Scratch</title>
      <link>https://arxiv.org/abs/2410.08835</link>
      <description>arXiv:2410.08835v1 Announce Type: cross 
Abstract: Block-based programming environments like Scratch are widely used in introductory programming courses. They facilitate learning pivotal programming concepts by eliminating syntactical errors, but logical errors that break the desired program behaviour are nevertheless possible. Finding such errors requires testing, i.e., running the program and checking its behaviour. In many programming environments, this step can be automated by providing executable tests as code; in Scratch, testing can only be done manually by invoking events through user input and observing the rendered stage. While this is arguably sufficient for learners, the lack of automated testing may be inhibitive for teachers wishing to provide feedback on their students' solutions. In order to address this issue, we introduce a new category of blocks in Scratch that enables the creation of automated tests. With these blocks, students and teachers alike can create tests and receive feedback directly within the Scratch environment using familiar block-based programming logic. To facilitate the creation and to enable batch processing sets of student solutions, we extend the Scratch user interface with an accompanying test interface. We evaluated this testing framework with 28 teachers who created tests for a popular Scratch game and subsequently used these tests to assess and provide feedback on student implementations. An overall accuracy of 0.93 of teachers' tests compared to manually evaluating the functionality of 21 student solutions demonstrates that teachers are able to create and effectively use tests. A subsequent survey confirms that teachers consider the block-based test approach useful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08835v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patric Feldmeier, Gordon Fraser, Ute Heuer, Florian Oberm\"uller, Siegfried Steckenbiller</dc:creator>
    </item>
    <item>
      <title>The Dynamics of Social Conventions in LLM populations: Spontaneous Emergence, Collective Biases and Tipping Points</title>
      <link>https://arxiv.org/abs/2410.08948</link>
      <description>arXiv:2410.08948v1 Announce Type: cross 
Abstract: Social conventions are the foundation for social and economic life. As legions of AI agents increasingly interact with each other and with humans, their ability to form shared conventions will determine how effectively they will coordinate behaviors, integrate into society and influence it. Here, we investigate the dynamics of conventions within populations of Large Language Model (LLM) agents using simulated interactions. First, we show that globally accepted social conventions can spontaneously arise from local interactions between communicating LLMs. Second, we demonstrate how strong collective biases can emerge during this process, even when individual agents appear to be unbiased. Third, we examine how minority groups of committed LLMs can drive social change by establishing new social conventions. We show that once these minority groups reach a critical size, they can consistently overturn established behaviors. In all cases, contrasting the experimental results with predictions from a minimal multi-agent model allows us to isolate the specific role of LLM agents. Our results clarify how AI systems can autonomously develop norms without explicit programming and have implications for designing AI systems that align with human values and societal goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08948v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli</dc:creator>
    </item>
    <item>
      <title>Shaping Integrity: Why Generative Artificial Intelligence Does Not Have to Undermine Education</title>
      <link>https://arxiv.org/abs/2407.19088</link>
      <description>arXiv:2407.19088v2 Announce Type: replace 
Abstract: This paper examines the role of generative artificial intelligence (GAI) in promoting academic integrity within educational settings. It explores how AI can be ethically integrated into classrooms to enhance learning experiences, foster intrinsic motivation, and support voluntary behavior change among students. By analyzing established ethical frameworks and educational theories such as deontological ethics, consequentialism, constructivist learning, and Self-Determination Theory (SDT), the paper argues that GAI, when used responsibly, can enhance digital literacy, encourage genuine knowledge construction, and uphold ethical standards in education. This research highlights the potential of GAI to create enriching, personalized learning environments that prepare students to navigate the complexities of the modern world ethically and effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19088v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Myles Joshua Toledo Tan, Nicholle Mae Amor Tan Maravilla</dc:creator>
    </item>
    <item>
      <title>How Unique is Whose Web Browser? The role of demographics in browser fingerprinting among US users</title>
      <link>https://arxiv.org/abs/2410.06954</link>
      <description>arXiv:2410.06954v2 Announce Type: replace 
Abstract: Browser fingerprinting can be used to identify and track users across the Web, even without cookies, by collecting attributes from users' devices to create unique "fingerprints". This technique and resulting privacy risks have been studied for over a decade. Yet further research is limited because prior studies used data not publicly available. Additionally, data in prior studies lacked user demographics. Here we provide a first-of-its-kind dataset to enable further research. It includes browser attributes with users' demographics and survey responses, collected with informed consent from 8,400 US study participants. We use this dataset to demonstrate how fingerprinting risks differ across demographic groups. For example, we find lower income users are more at risk, and find that as users' age increases, they are both more likely to be concerned about fingerprinting and at real risk of fingerprinting. Furthermore, we demonstrate an overlooked risk: user demographics, such as gender, age, income level and race, can be inferred from browser attributes commonly used for fingerprinting, and we identify which browser attributes most contribute to this risk. Our data collection process also conducted an experiment to study what impacts users' likelihood to share browser data for open research, in order to inform future data collection efforts, with responses from 12,461 total participants. Female participants were significantly less likely to share their browser data, as were participants who were shown the browser data we asked to collect. Overall, we show the important role of user demographics in the ongoing work that intends to assess fingerprinting risks and improve user privacy, with findings to inform future privacy enhancing browser developments. The dataset and data collection tool we provide can be used to further study research questions not addressed in this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06954v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Berke, Enrico Bacis, Badih Ghazi, Pritish Kamath, Ravi Kumar, Robin Lassonde, Pasin Manurangsi, Umar Syed</dc:creator>
    </item>
    <item>
      <title>Accurately Classifying Out-Of-Distribution Data in Facial Recognition</title>
      <link>https://arxiv.org/abs/2404.03876</link>
      <description>arXiv:2404.03876v5 Announce Type: replace-cross 
Abstract: Standard classification theory assumes that the distribution of images in the test and training sets are identical. Unfortunately, real-life scenarios typically feature unseen data (``out-of-distribution data") which is different from data in the training distribution (``in-distribution"). This issue is most prevalent in social justice problems where data from under-represented groups may appear in the test data without representing an equal proportion of the training data. This may result in a model returning confidently wrong decisions and predictions. We are interested in the following question: Can the performance of a neural network improve on facial images of out-of-distribution data when it is trained simultaneously on multiple datasets of in-distribution data? We approach this problem by incorporating the Outlier Exposure model and investigate how the model's performance changes when other datasets of facial images were implemented. We observe that the accuracy and other metrics of the model can be increased by applying Outlier Exposure, incorporating a trainable weight parameter to increase the machine's emphasis on outlier images, and by re-weighting the importance of different class labels. We also experimented with whether sorting the images and determining outliers via image features would have more of an effect on the metrics than sorting by average pixel value, and found no conclusive results. Our goal was to make models not only more accurate but also more fair by scanning a more expanded range of images. Utilizing Python and the Pytorch package, we found models utilizing outlier exposure could result in more fair classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03876v5</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianluca Barone, Aashrit Cunchala, Rudy Nunez</dc:creator>
    </item>
    <item>
      <title>Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis</title>
      <link>https://arxiv.org/abs/2409.05292</link>
      <description>arXiv:2409.05292v4 Announce Type: replace-cross 
Abstract: The world is currently experiencing an outbreak of mpox, which has been declared a Public Health Emergency of International Concern by WHO. No prior work related to social media mining has focused on the development of a dataset of Instagram posts about the mpox outbreak. The work presented in this paper aims to address this research gap and makes two scientific contributions to this field. First, it presents a multilingual dataset of 60,127 Instagram posts about mpox, published between July 23, 2022, and September 5, 2024. The dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram posts about mpox in 52 languages. For each of these posts, the Post ID, Post Description, Date of publication, language, and translated version of the post (translation to English was performed using the Google Translate API) are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis, hate speech detection, and anxiety or stress detection were performed. This process included classifying each post into (i) one of the sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no anxiety/stress detected. These results are presented as separate attributes in the dataset. Second, this paper presents the results of performing sentiment analysis, hate speech analysis, and anxiety or stress analysis. The variation of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and 50.64%, respectively. In terms of hate speech detection, 95.75% of the posts did not contain hate and the remaining 4.25% of the posts contained hate. Finally, 72.05% of the posts did not indicate any anxiety/stress, and the remaining 27.95% of the posts represented some form of anxiety/stress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05292v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nirmalya Thakur</dc:creator>
    </item>
    <item>
      <title>Mobility-GCN: a human mobility-based graph convolutional network for tracking and analyzing the spatial dynamics of the synthetic opioid crisis in the USA, 2013-2020</title>
      <link>https://arxiv.org/abs/2409.09945</link>
      <description>arXiv:2409.09945v4 Announce Type: replace-cross 
Abstract: Synthetic opioids are the most common drugs involved in drug-involved overdose mortalities in the U.S. The Center for Disease Control and Prevention reported that in 2018, about 70% of all drug overdose deaths involved opioids and 67% of all opioid-involved deaths were accounted for by synthetic opioids. In this study, we investigated the spread of synthetic opioids between 2013 and 2020 in the U.S. We analyzed the relationship between the spatiotemporal pattern of synthetic opioid-involved deaths and another key opioid, heroin, and compared patterns of deaths involving these two types of drugs during this period. Spatial connections and human mobility between counties were incorporated into a graph convolutional neural network model to represent and analyze the spread of synthetic opioid-involved deaths in the context of previous heroin-involved death patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09945v4</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyue Xia, Kathleen Stewart</dc:creator>
    </item>
  </channel>
</rss>
