<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Apr 2024 04:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>DeepVARMA: A Hybrid Deep Learning and VARMA Model for Chemical Industry Index Forecasting</title>
      <link>https://arxiv.org/abs/2404.17615</link>
      <description>arXiv:2404.17615v1 Announce Type: new 
Abstract: Since the chemical industry index is one of the important indicators to measure the development of the chemical industry, forecasting it is critical for understanding the economic situation and trends of the industry. Taking the multivariable nonstationary series-synthetic material index as the main research object, this paper proposes a new prediction model: DeepVARMA, and its variants Deep-VARMA-re and DeepVARMA-en, which combine LSTM and VARMAX models. The new model firstly uses the deep learning model such as the LSTM remove the trends of the target time series and also learn the representation of endogenous variables, and then uses the VARMAX model to predict the detrended target time series with the embeddings of endogenous variables, and finally combines the trend learned by the LSTM and dependency learned by the VARMAX model to obtain the final predictive values. The experimental results show that (1) the new model achieves the best prediction accuracy by combining the LSTM encoding of the exogenous variables and the VARMAX model. (2) In multivariate non-stationary series prediction, DeepVARMA uses a phased processing strategy to show higher adaptability and accuracy compared to the traditional VARMA model as well as the machine learning models LSTM, RF and XGBoost. (3) Compared with smooth sequence prediction, the traditional VARMA and VARMAX models fluctuate more in predicting non-smooth sequences, while DeepVARMA shows more flexibility and robustness. This study provides more accurate tools and methods for future development and scientific decision-making in the chemical industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17615v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Hu Yang</dc:creator>
    </item>
    <item>
      <title>Manipulating a Continuous Instrumental Variable in an Observational Study of Premature Babies: Algorithm, Partial Identification Bounds, and Inference under Randomization and Biased Randomization Assumptions</title>
      <link>https://arxiv.org/abs/2404.17734</link>
      <description>arXiv:2404.17734v1 Announce Type: new 
Abstract: Regionalization of intensive care for premature babies refers to a triage system of mothers with high-risk pregnancies to hospitals of varied capabilities based on risks faced by infants. Due to the limited capacity of high-level hospitals, which are equipped with advanced expertise to provide critical care, understanding the effect of delivering premature babies at such hospitals on infant mortality for different subgroups of high-risk mothers could facilitate the design of an efficient perinatal regionalization system. Towards answering this question, Baiocchi et al. (2010) proposed to strengthen an excess-travel-time-based, continuous instrumental variable (IV) in an IV-based, matched-pair design by switching focus to a smaller cohort amenable to being paired with a larger separation in the IV dose. Three elements changed with the strengthened IV: the study cohort, compliance rate and latent complier subgroup. Here, we introduce a non-bipartite, template matching algorithm that embeds data into a target, pair-randomized encouragement trial which maintains fidelity to the original study cohort while strengthening the IV. We then study randomization-based and IV-dependent, biased-randomization-based inference of partial identification bounds for the sample average treatment effect (SATE) in an IV-based matched pair design, which deviates from the usual effect ratio estimand in that the SATE is agnostic to the IV and who is matched to whom, although a strengthened IV design could narrow the partial identification bounds. Based on our proposed strengthened-IV design, we found that delivering at a high-level NICU reduced preterm babies' mortality rate compared to a low-level NICU for $81,766 \times 2 = 163,532$ mothers and their preterm babies and the effect appeared to be minimal among non-black, low-risk mothers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17734v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhe Chen, Min Haeng Cho, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Likelihood Based Inference in Fully and Partially Observed Exponential Family Graphical Models with Intractable Normalizing Constants</title>
      <link>https://arxiv.org/abs/2404.17763</link>
      <description>arXiv:2404.17763v1 Announce Type: new 
Abstract: Probabilistic graphical models that encode an underlying Markov random field are fundamental building blocks of generative modeling to learn latent representations in modern multivariate data sets with complex dependency structures. Among these, the exponential family graphical models are especially popular, given their fairly well-understood statistical properties and computational scalability to high-dimensional data based on pseudo-likelihood methods. These models have been successfully applied in many fields, such as the Ising model in statistical physics and count graphical models in genomics. Another strand of models allows some nodes to be latent, so as to allow the marginal distribution of the observable nodes to depart from exponential family to capture more complex dependence. These approaches form the basis of generative models in artificial intelligence, such as the Boltzmann machines and their restricted versions. A fundamental barrier to likelihood-based (i.e., both maximum likelihood and fully Bayesian) inference in both fully and partially observed cases is the intractability of the likelihood. The usual workaround is via adopting pseudo-likelihood based approaches, following the pioneering work of Besag (1974). The goal of this paper is to demonstrate that full likelihood based analysis of these models is feasible in a computationally efficient manner. The chief innovation lies in using a technique of Geyer (1991) to estimate the intractable normalizing constant, as well as its gradient, for intractable graphical models. Extensive numerical results, supporting theory and comparisons with pseudo-likelihood based approaches demonstrate the applicability of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17763v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujie Chen, Anindya Bhadra, Antik Chakraborty</dc:creator>
    </item>
    <item>
      <title>PWEXP: An R Package Using Piecewise Exponential Model for Study Design and Event/Timeline Prediction</title>
      <link>https://arxiv.org/abs/2404.17772</link>
      <description>arXiv:2404.17772v1 Announce Type: new 
Abstract: Parametric assumptions such as exponential distribution are commonly used in clinical trial design and analysis. However, violation of distribution assumptions can introduce biases in sample size and power calculations. Piecewise exponential (PWE) hazard model partitions the hazard function into segments each with constant hazards and is easy for interpretation and computation. Due to its piecewise property, PWE can fit a wide range of survival curves and accurately predict the future number of events and analysis time in event-driven clinical trials, thus enabling more flexible and reliable study designs. Compared with other existing approaches, the PWE model provides a superior balance of flexibility and robustness in model fitting and prediction. The proposed PWEXP package is designed for estimating and predicting PWE hazard models for right-censored data. By utilizing well-established criteria such as AIC, BIC, and cross-validation log-likelihood, the PWEXP package chooses the optimal number of change-points and determines the optimal position of change-points. With its particular goodness-of-fit, the PWEXP provides accurate and robust hazard estimation, which can be used for reliable power calculation at study design and timeline prediction at study conduct. The package also offers visualization functions to facilitate the interpretation of survival curve fitting results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17772v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tianchen Xu, Rachael Wen</dc:creator>
    </item>
    <item>
      <title>A General Framework for Random Effects Models for Binary, Ordinal, Count Type and Continuous Dependent Variables Including Variable Selection</title>
      <link>https://arxiv.org/abs/2404.17792</link>
      <description>arXiv:2404.17792v1 Announce Type: new 
Abstract: A general random effects model is proposed that allows for continuous as well as discrete distributions of the responses. Responses can be unrestricted continuous, bounded continuous, binary, ordered categorical or given in the form of counts. The distribution of the responses is not restricted to exponential families, which is a severe restriction in generalized mixed models. Generalized mixed models use fixed distributions for responses, for example the Poisson distribution in count data, which has the disadvantage of not accounting for overdispersion. By using a response function and a thresholds function the proposed mixed thresholds model can account for a variety of alternative distributions that often show better fits than fixed distributions used within the generalized linear model framework. A particular strength of the model is that it provides a tool for joint modeling, responses may be of different types, some can be discrete, others continuous. In addition to introducing the mixed thresholds model parameter sparsity is addressed. Random effects models can contain a large number of parameters, in particular if effects have to be assumed as measurement-specific. Methods to obtain sparser representations are proposed and illustrated. The methods are shown to work in the thresholds model but could also be adapted to other modeling approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17792v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gerhard Tutz</dc:creator>
    </item>
    <item>
      <title>Thinking inside the bounds: Improved error distributions for indifference point data analysis and simulation via beta regression using common discounting functions</title>
      <link>https://arxiv.org/abs/2404.18000</link>
      <description>arXiv:2404.18000v1 Announce Type: new 
Abstract: Standard nonlinear regression is commonly used when modeling indifference points due to its ability to closely follow observed data, resulting in a good model fit. However, standard nonlinear regression currently lacks a reasonable distribution-based framework for indifference points, which limits its ability to adequately describe the inherent variability in the data. Software commonly assumes data follow a normal distribution with constant variance. However, typical indifference points do not follow a normal distribution or exhibit constant variance. To address these limitations, this paper introduces a class of nonlinear beta regression models that offers excellent fit to discounting data and enhances simulation-based approaches. This beta regression model can accommodate popular discounting functions. This work proposes three specific advances. First, our model automatically captures non-constant variance as a function of delay. Second, our model improves simulation-based approaches since it obeys the natural boundaries of observable data, unlike the ordinary assumption of normal residuals and constant variance. Finally, we introduce a scale-location-truncation trick that allows beta regression to accommodate observed values of zero and one. A comparison between beta regression and standard nonlinear regression reveals close agreement in the estimated discounting rate k obtained from both methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18000v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingang Kim, Mikhail N. Koffarnus, Christopher T Franck</dc:creator>
    </item>
    <item>
      <title>A General Causal Inference Framework for Cross-Sectional Observational Data</title>
      <link>https://arxiv.org/abs/2404.18197</link>
      <description>arXiv:2404.18197v1 Announce Type: new 
Abstract: Causal inference methods for observational data are highly regarded due to their wide applicability. While there are already numerous methods available for de-confounding bias, these methods generally assume that covariates consist solely of confounders or make naive assumptions about the covariates. Such assumptions face challenges in both theory and practice, particularly when dealing with high-dimensional covariates. Relaxing these naive assumptions and identifying the confounding covariates that truly require correction can effectively enhance the practical significance of these methods. Therefore, this paper proposes a General Causal Inference (GCI) framework specifically designed for cross-sectional observational data, which precisely identifies the key confounding covariates and provides corresponding identification algorithm. Specifically, based on progressive derivations of the Markov property on Directed Acyclic Graph, we conclude that the key confounding covariates are equivalent to the common root ancestors of the treatment and the outcome variable. Building upon this conclusion, the GCI framework is composed of a novel Ancestor Set Identification (ASI) algorithm and de-confounding inference methods. Firstly, the ASI algorithm is theoretically supported by the conditional independence properties and causal asymmetry between variables, enabling the identification of key confounding covariates. Subsequently, the identified confounding covariates are used in the de-confounding inference methods to obtain unbiased causal effect estimation, which can support informed decision-making. Extensive experiments on synthetic datasets demonstrate that the GCI framework can effectively identify the critical confounding covariates and significantly improve the precision, stability, and interpretability of causal inference in observational studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18197v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghe Zhao, Huiyan Sun</dc:creator>
    </item>
    <item>
      <title>A cautious approach to constraint-based causal model selection</title>
      <link>https://arxiv.org/abs/2404.18232</link>
      <description>arXiv:2404.18232v1 Announce Type: new 
Abstract: We study the data-driven selection of causal graphical models using constraint-based algorithms, which determine the existence or non-existence of edges (causal connections) in a graph based on testing a series of conditional independence hypotheses. In settings where the ultimate scientific goal is to use the selected graph to inform estimation of some causal effect of interest (e.g., by selecting a valid and sufficient set of adjustment variables), we argue that a "cautious" approach to graph selection should control the probability of falsely removing edges and prefer dense, rather than sparse, graphs. We propose a simple inversion of the usual conditional independence testing procedure: to remove an edge, test the null hypothesis of conditional association greater than some user-specified threshold, rather than the null of independence. This equivalence testing formulation to testing independence constraints leads to a procedure with desriable statistical properties and behaviors that better match the inferential goals of certain scientific studies, for example observational epidemiological studies that aim to estimate causal effects in the face of causal model uncertainty. We illustrate our approach on a data example from environmental epidemiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18232v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniel Malinsky</dc:creator>
    </item>
    <item>
      <title>Semiparametric causal mediation analysis in cluster-randomized experiments</title>
      <link>https://arxiv.org/abs/2404.18256</link>
      <description>arXiv:2404.18256v1 Announce Type: new 
Abstract: In cluster-randomized experiments, there is emerging interest in exploring the causal mechanism in which a cluster-level treatment affects the outcome through an intermediate outcome. Despite an extensive development of causal mediation methods in the past decade, only a few exceptions have been considered in assessing causal mediation in cluster-randomized studies, all of which depend on parametric model-based estimators. In this article, we develop the formal semiparametric efficiency theory to motivate several doubly-robust methods for addressing several mediation effect estimands corresponding to both the cluster-average and the individual-level treatment effects in cluster-randomized experiments--the natural indirect effect, natural direct effect, and spillover mediation effect. We derive the efficient influence function for each mediation effect, and carefully parameterize each efficient influence function to motivate practical strategies for operationalizing each estimator. We consider both parametric working models and data-adaptive machine learners to estimate the nuisance functions, and obtain semiparametric efficient causal mediation estimators in the latter case. Our methods are illustrated via extensive simulations and two completed cluster-randomized experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18256v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Cheng, Fan Li</dc:creator>
    </item>
    <item>
      <title>Out-of-distribution generalization under random, dense distributional shifts</title>
      <link>https://arxiv.org/abs/2404.18370</link>
      <description>arXiv:2404.18370v1 Announce Type: new 
Abstract: Many existing approaches for estimating parameters in settings with distributional shifts operate under an invariance assumption. For example, under covariate shift, it is assumed that p(y|x) remains invariant. We refer to such distribution shifts as sparse, since they may be substantial but affect only a part of the data generating system. In contrast, in various real-world settings, shifts might be dense. More specifically, these dense distributional shifts may arise through numerous small and random changes in the population and environment. First, we will discuss empirical evidence for such random dense distributional shifts and explain why commonly used models for distribution shifts-including adversarial approaches-may not be appropriate under these conditions. Then, we will develop tools to infer parameters and make predictions for partially observed, shifted distributions. Finally, we will apply the framework to several real-world data sets and discuss diagnostics to evaluate the fit of the distributional uncertainty model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18370v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujin Jeong, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Inference for the panel ARMA-GARCH model when both $N$ and $T$ are large</title>
      <link>https://arxiv.org/abs/2404.18377</link>
      <description>arXiv:2404.18377v1 Announce Type: new 
Abstract: We propose a panel ARMA-GARCH model to capture the dynamics of large panel data with $N$ individuals over $T$ time periods. For this model, we provide a two-step estimation procedure to estimate the ARMA parameters and GARCH parameters stepwisely. Under some regular conditions, we show that all of the proposed estimators are asymptotically normal with the convergence rate $(NT)^{-1/2}$, and they have the asymptotic biases when both $N$ and $T$ diverge to infinity at the same rate. Particularly, we find that the asymptotic biases result from the fixed effect, estimation effect, and unobservable initial values. To correct the biases, we further propose the bias-corrected version of estimators by using either the analytical asymptotics or jackknife method. Our asymptotic results are based on a new central limit theorem for the linear-quadratic form in the martingale difference sequence, when the weight matrix is uniformly bounded in row and column. Simulations and one real example are given to demonstrate the usefulness of our panel ARMA-GARCH model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18377v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bing Su, Ke Zhu</dc:creator>
    </item>
    <item>
      <title>Semiparametric mean and variance joint models with Laplace link functions for count time series</title>
      <link>https://arxiv.org/abs/2404.18421</link>
      <description>arXiv:2404.18421v1 Announce Type: new 
Abstract: Count time series data are frequently analyzed by modeling their conditional means and the conditional variance is often considered to be a deterministic function of the corresponding conditional mean and is not typically modeled independently. We propose a semiparametric mean and variance joint model, called random rounded count-valued generalized autoregressive conditional heteroskedastic (RRC-GARCH) model, to address this limitation. The RRC-GARCH model and its variations allow for the joint modeling of both the conditional mean and variance and offer a flexible framework for capturing various mean-variance structures (MVSs). One main feature of this model is its ability to accommodate negative values for regression coefficients and autocorrelation functions. The autocorrelation structure of the RRC-GARCH model using the proposed Laplace link functions with nonnegative regression coefficients is the same as that of an autoregressive moving-average (ARMA) process. For the new model, the stationarity and ergodicity are established and the consistency and asymptotic normality of the conditional least squares estimator are proved. Model selection criteria are proposed to evaluate the RRC-GARCH models. The performance of the RRC-GARCH model is assessed through analyses of both simulated and real data sets. The results indicate that the model can effectively capture the MVS of count time series data and generate accurate forecast means and variances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18421v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianqing Liu, Xiaohui Yuan</dc:creator>
    </item>
    <item>
      <title>Sequential model confidence</title>
      <link>https://arxiv.org/abs/2404.18678</link>
      <description>arXiv:2404.18678v1 Announce Type: new 
Abstract: In most prediction and estimation situations, scientists consider various statistical models for the same problem, and naturally want to select amongst the best. Hansen et al. (2011) provide a powerful solution to this problem by the so-called model confidence set, a subset of the original set of available models that contains the best models with a given level of confidence. Importantly, model confidence sets respect the underlying selection uncertainty by being flexible in size. However, they presuppose a fixed sample size which stands in contrast to the fact that model selection and forecast evaluation are inherently sequential tasks where we successively collect new data and where the decision to continue or conclude a study may depend on the previous outcomes. In this article, we extend model confidence sets sequentially over time by relying on sequential testing methods. Recently, e-processes and confidence sequences have been introduced as new, safe methods for assessing statistical evidence. Sequential model confidence sets allow to continuously monitor the models' performances and come with time-uniform, nonasymptotic coverage guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18678v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Arnold, Georgios Gavrilopoulos, Benedikt Schulz, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Two-way Homogeneity Pursuit for Quantile Network Vector Autoregression</title>
      <link>https://arxiv.org/abs/2404.18732</link>
      <description>arXiv:2404.18732v1 Announce Type: new 
Abstract: While the Vector Autoregression (VAR) model has received extensive attention for modelling complex time series, quantile VAR analysis remains relatively underexplored for high-dimensional time series data. To address this disparity, we introduce a two-way grouped network quantile (TGNQ) autoregression model for time series collected on large-scale networks, known for their significant heterogeneous and directional interactions among nodes. Our proposed model simultaneously conducts node clustering and model estimation to balance complexity and interpretability. To account for the directional influence among network nodes, each network node is assigned two latent group memberships that can be consistently estimated using our proposed estimation procedure. Theoretical analysis demonstrates the consistency of membership and parameter estimators even with an overspecified number of groups. With the correct group specification, estimated parameters are proven to be asymptotically normal, enabling valid statistical inferences. Moreover, we propose a quantile information criterion for consistently selecting the number of groups. Simulation studies show promising finite sample performance, and we apply the methodology to analyze connectedness and risk spillover effects among Chinese A-share stocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18732v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wenyang Liu, Ganggang Xu, Jianqing Fan, Xuening Zhu</dc:creator>
    </item>
    <item>
      <title>Semiparametric fiducial inference</title>
      <link>https://arxiv.org/abs/2404.18779</link>
      <description>arXiv:2404.18779v1 Announce Type: new 
Abstract: R. A. Fisher introduced the concept of fiducial as a potential replacement for the Bayesian posterior distribution in the 1930s. During the past century, fiducial approaches have been explored in various parametric and nonparametric settings. However, to the best of our knowledge, no fiducial inference has been developed in the realm of semiparametric statistics. In this paper, we propose a novel fiducial approach for semiparametric models. To streamline our presentation, we use the Cox proportional hazards model, which is the most popular model for the analysis of survival data, as a running example. Other models and extensions are also discussed. In our experiments, we find our method to perform well especially in situations when the maximum likelihood estimator fails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18779v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Cui, Jan Hannig, Paul Edlefsen</dc:creator>
    </item>
    <item>
      <title>Switching Models of Oscillatory Networks Greatly Improve Inference of Dynamic Functional Connectivity</title>
      <link>https://arxiv.org/abs/2404.18854</link>
      <description>arXiv:2404.18854v1 Announce Type: new 
Abstract: Functional brain networks can change rapidly as a function of stimuli or cognitive shifts. Tracking dynamic functional connectivity is particularly challenging as it requires estimating the structure of the network at each moment as well as how it is shifting through time. In this paper, we describe a general modeling framework and a set of specific models that provides substantially increased statistical power for estimating rhythmic dynamic networks, based on the assumption that for a particular experiment or task, the network state at any moment is chosen from a discrete set of possible network modes. Each model is comprised of three components: (1) a set of latent switching states that represent transitions between the expression of each network mode; (2) a set of latent oscillators, each characterized by an estimated mean oscillation frequency and an instantaneous phase and amplitude at each time point; and (3) an observation model that relates the observed activity at each electrode to a linear combination of the latent oscillators. We develop an expectation-maximization procedure to estimate the network structure for each switching state and the probability of each state being expressed at each moment. We conduct a set of simulation studies to illustrate the application of these models and quantify their statistical power, even in the face of model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18854v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wan-Chi Hsin, Uri T. Eden, Emily P. Stephen</dc:creator>
    </item>
    <item>
      <title>VT-MRF-SPF: Variable Target Markov Random Field Scalable Particle Filter</title>
      <link>https://arxiv.org/abs/2404.18857</link>
      <description>arXiv:2404.18857v1 Announce Type: new 
Abstract: Markov random fields (MRFs) are invaluable tools across diverse fields, and spatiotemporal MRFs (STMRFs) amplify their effectiveness by integrating spatial and temporal dimensions. However, modeling spatiotemporal data introduces additional hurdles, including dynamic spatial dimensions and partial observations, prevalent in scenarios like disease spread analysis and environmental monitoring. Tracking high-dimensional targets with complex spatiotemporal interactions over extended periods poses significant challenges in accuracy, efficiency, and computational feasibility. To tackle these obstacles, we introduce the variable target MRF scalable particle filter (VT-MRF-SPF), a fully online learning algorithm designed for high-dimensional target tracking over STMRFs with varying dimensions under partial observation. We rigorously guarantee algorithm performance, explicitly indicating overcoming the curse of dimensionality. Additionally, we provide practical guidelines for tuning graphical parameters, leading to superior performance in extensive examinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18857v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ning Ning</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction Sets for Populations of Graphs</title>
      <link>https://arxiv.org/abs/2404.18862</link>
      <description>arXiv:2404.18862v1 Announce Type: new 
Abstract: The analysis of data such as graphs has been gaining increasing attention in the past years. This is justified by the numerous applications in which they appear. Several methods are present to predict graphs, but much fewer to quantify the uncertainty of the prediction. The present work proposes an uncertainty quantification methodology for graphs, based on conformal prediction. The method works both for graphs with the same set of nodes (labelled graphs) and graphs with no clear correspondence between the set of nodes across the observed graphs (unlabelled graphs). The unlabelled case is dealt with the creation of prediction sets embedded in a quotient space. The proposed method does not rely on distributional assumptions, it achieves finite-sample validity, and it identifies interpretable prediction sets. To explore the features of this novel forecasting technique, we perform two simulation studies to show the methodology in both the labelled and the unlabelled case. We showcase the applicability of the method in analysing the performance of different teams during the FIFA 2018 football world championship via their player passing networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18862v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Calissano, Matteo Fontana, Gianluca Zeni, Simone Vantini</dc:creator>
    </item>
    <item>
      <title>Detecting critical treatment effect bias in small subgroups</title>
      <link>https://arxiv.org/abs/2404.18905</link>
      <description>arXiv:2404.18905v1 Announce Type: new 
Abstract: Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18905v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piersilvio De Bartolomeis, Javier Abad, Konstantin Donhauser, Fanny Yang</dc:creator>
    </item>
    <item>
      <title>Universal Cold RNA Phase Transitions</title>
      <link>https://arxiv.org/abs/2403.15352</link>
      <description>arXiv:2403.15352v1 Announce Type: cross 
Abstract: RNA's diversity of structures and functions impacts all life forms since primordia. We use calorimetric force spectroscopy to investigate RNA folding landscapes in previously unexplored low-temperature conditions. We find that Watson-Crick RNA hairpins, the most basic secondary structure elements, undergo a glass-like transition below $\mathbf{T_G\sim 20 ^{\circ}}$C where the heat capacity abruptly changes and the RNA folds into a diversity of misfolded structures. We hypothesize that an altered RNA biochemistry, determined by sequence-independent ribose-water interactions, outweighs sequence-dependent base pairing. The ubiquitous ribose-water interactions lead to universal RNA phase transitions below $\mathbf{T_G}$, such as maximum stability at $\mathbf{T_S\sim 5 ^{\circ}}$C where water density is maximum, and cold denaturation at $\mathbf{T_C\sim-50^{\circ}}$C. RNA cold biochemistry may have a profound impact on RNA function and evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15352v1</guid>
      <category>q-bio.BM</category>
      <category>physics.bio-ph</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Rissone, Aurelien Severino, Isabel Pastor, Felix Ritort</dc:creator>
    </item>
    <item>
      <title>Testing for similarity of dose response in multi-regional clinical trials</title>
      <link>https://arxiv.org/abs/2404.17682</link>
      <description>arXiv:2404.17682v1 Announce Type: cross 
Abstract: This paper addresses the problem of deciding whether the dose response relationships between subgroups and the full population in a multi-regional trial are similar to each other. Similarity is measured in terms of the maximal deviation between the dose response curves. We consider a parametric framework and develop two powerful bootstrap tests for the similarity between the dose response curves of one subgroup and the full population, and for the similarity between the dose response curves of several subgroups and the full population. We prove the validity of the tests, investigate the finite sample properties by means of a simulation study and finally illustrate the methodology in a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17682v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Holger Dette, Lukas Koletzko, Frank Bretz</dc:creator>
    </item>
    <item>
      <title>Neutral Pivoting: Strong Bias Correction for Shared Information</title>
      <link>https://arxiv.org/abs/2404.17737</link>
      <description>arXiv:2404.17737v1 Announce Type: cross 
Abstract: In the absence of historical data for use as forecasting inputs, decision makers often ask a panel of judges to predict the outcome of interest, leveraging the wisdom of the crowd (Surowiecki 2005). Even if the crowd is large and skilled, shared information can bias the simple mean of judges' estimates. Addressing the issue of bias, Palley and Soll (2019) introduces a novel approach called pivoting. Pivoting can take several forms, most notably the powerful and reliable minimal pivot. We build on the intuition of the minimal pivot and propose a more aggressive bias correction known as the neutral pivot. The neutral pivot achieves the largest bias correction of its class that both avoids the need to directly estimate crowd composition or skill and maintains a smaller expected squared error than the simple mean for all considered settings. Empirical assessments on real datasets confirm the effectiveness of the neutral pivot compared to current methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17737v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Rilling</dc:creator>
    </item>
    <item>
      <title>Conformal Ranked Retrieval</title>
      <link>https://arxiv.org/abs/2404.17769</link>
      <description>arXiv:2404.17769v1 Announce Type: cross 
Abstract: Given the wide adoption of ranked retrieval techniques in various information systems that significantly impact our daily lives, there is an increasing need to assess and address the uncertainty inherent in their predictions. This paper introduces a novel method using the conformal risk control framework to quantitatively measure and manage risks in the context of ranked retrieval problems. Our research focuses on a typical two-stage ranked retrieval problem, where the retrieval stage generates candidates for subsequent ranking. By carefully formulating the conformal risk for each stage, we have developed algorithms to effectively control these risks within their specified bounds. The efficacy of our proposed methods has been demonstrated through comprehensive experiments on three large-scale public datasets for ranked retrieval tasks, including the MSLR-WEB dataset, the Yahoo LTRC dataset and the MS MARCO dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17769v1</guid>
      <category>cs.IR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunpeng Xu, Wenge Guo, Zhi Wei</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Single-Index Models: Link Estimation and Marginal Inference</title>
      <link>https://arxiv.org/abs/2404.17812</link>
      <description>arXiv:2404.17812v1 Announce Type: cross 
Abstract: This study proposes a novel method for estimation and hypothesis testing in high-dimensional single-index models. We address a common scenario where the sample size and the dimension of regression coefficients are large and comparable. Unlike traditional approaches, which often overlook the estimation of the unknown link function, we introduce a new method for link function estimation. Leveraging the information from the estimated link function, we propose more efficient estimators that are better aligned with the underlying model. Furthermore, we rigorously establish the asymptotic normality of each coordinate of the estimator. This provides a valid construction of confidence intervals and $p$-values for any finite collection of coordinates. Numerical experiments validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17812v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazuma Sawaya, Yoshimasa Uematsu, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification for iterative algorithms in linear models with application to early stopping</title>
      <link>https://arxiv.org/abs/2404.17856</link>
      <description>arXiv:2404.17856v1 Announce Type: cross 
Abstract: This paper investigates the iterates $\hbb^1,\dots,\hbb^T$ obtained from iterative algorithms in high-dimensional linear regression problems, in the regime where the feature dimension $p$ is comparable with the sample size $n$, i.e., $p \asymp n$. The analysis and proposed estimators are applicable to Gradient Descent (GD), proximal GD and their accelerated variants such as Fast Iterative Soft-Thresholding (FISTA). The paper proposes novel estimators for the generalization error of the iterate $\hbb^t$ for any fixed iteration $t$ along the trajectory. These estimators are proved to be $\sqrt n$-consistent under Gaussian designs. Applications to early-stopping are provided: when the generalization error of the iterates is a U-shape function of the iteration $t$, the estimates allow to select from the data an iteration $\hat t$ that achieves the smallest generalization error along the trajectory. Additionally, we provide a technique for developing debiasing corrections and valid confidence intervals for the components of the true coefficient vector from the iterate $\hbb^t$ at any finite iteration $t$. Extensive simulations on synthetic data illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17856v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre C. Bellec, Kai Tan</dc:creator>
    </item>
    <item>
      <title>Sequential monitoring for explosive volatility regimes</title>
      <link>https://arxiv.org/abs/2404.17885</link>
      <description>arXiv:2404.17885v1 Announce Type: cross 
Abstract: In this paper, we develop two families of sequential monitoring procedure to (timely) detect changes in a GARCH(1,1) model. Whilst our methodologies can be applied for the general analysis of changepoints in GARCH(1,1) sequences, they are in particular designed to detect changes from stationarity to explosivity or vice versa, thus allowing to check for volatility bubbles. Our statistics can be applied irrespective of whether the historical sample is stationary or not, and indeed without prior knowledge of the regime of the observations before and after the break. In particular, we construct our detectors as the CUSUM process of the quasi-Fisher scores of the log likelihood function. In order to ensure timely detection, we then construct our boundary function (exceeding which would indicate a break) by including a weighting sequence which is designed to shorten the detection delay in the presence of a changepoint. We consider two types of weights: a lighter set of weights, which ensures timely detection in the presence of changes occurring early, but not too early after the end of the historical sample; and a heavier set of weights, called Renyi weights which is designed to ensure timely detection in the presence of changepoints occurring very early in the monitoring horizon. In both cases, we derive the limiting distribution of the detection delays, indicating the expected delay for each set of weights. Our theoretical results are validated via a comprehensive set of simulations, and an empirical application to daily returns of individual stocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17885v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lajos Horvath, Lorenzo Trapani, Shixuan Wang</dc:creator>
    </item>
    <item>
      <title>Randomization-based confidence intervals for the local average treatment effect</title>
      <link>https://arxiv.org/abs/2404.18786</link>
      <description>arXiv:2404.18786v1 Announce Type: cross 
Abstract: We consider the problem of generating confidence intervals in randomized experiments with noncompliance. We show that a refinement of a randomization-based procedure proposed by Imbens and Rosenbaum (2005) has desirable properties. Namely, we show that using a studentized Anderson-Rubin-type statistic as a test statistic yields confidence intervals that are finite-sample exact under treatment effect homogeneity, and remain asymptotically valid for the Local Average Treatment Effect when the treatment effect is heterogeneous. We provide a uniform analysis of this procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18786v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. M. Aronow, Haoge Chang, Patrick Lopatto</dc:creator>
    </item>
    <item>
      <title>Global Sensitivity and Domain-Selective Testing for Functional-Valued Responses: An Application to Climate Economy Models</title>
      <link>https://arxiv.org/abs/2006.13850</link>
      <description>arXiv:2006.13850v4 Announce Type: replace 
Abstract: Understanding the dynamics and evolution of climate change and associated uncertainties is key for designing robust policy actions. Computer models are key tools in this scientific effort, which have now reached a high level of sophistication and complexity. Model auditing is needed in order to better understand their results, and to deal with the fact that such models are increasingly opaque with respect to their inner workings. Current techniques such as Global Sensitivity Analysis (GSA) are limited to dealing either with multivariate outputs, stochastic ones, or finite-change inputs. This limits their applicability to time-varying variables such as future pathways of greenhouse gases. To provide additional semantics in the analysis of a model ensemble, we provide an extension of GSA methodologies tackling the case of stochastic functional outputs with finite change inputs. To deal with finite change inputs and functional outputs, we propose an extension of currently available GSA methodologies while we deal with the stochastic part by introducing a novel, domain-selective inferential technique for sensitivity indices. Our method is explored via a simulation study that shows its robustness and efficacy in detecting sensitivity patterns. We apply it to real world data, where its capabilities can provide to practitioners and policymakers additional information about the time dynamics of sensitivity patterns, as well as information about robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.13850v4</guid>
      <category>stat.ME</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Fontana, Massimo Tavoni, Simone Vantini</dc:creator>
    </item>
    <item>
      <title>Mahalanobis balancing: a multivariate perspective on approximate covariate balancing</title>
      <link>https://arxiv.org/abs/2204.13439</link>
      <description>arXiv:2204.13439v4 Announce Type: replace 
Abstract: In the past decade, various exact balancing-based weighting methods were introduced to the causal inference literature. Exact balancing alleviates the extreme weight and model misspecification issues that may incur when one implements inverse probability weighting. It eliminates covariate imbalance by imposing balancing constraints in an optimization problem. The optimization problem can nevertheless be infeasible when there is bad overlap between the covariate distributions in the treated and control groups or when the covariates are high-dimensional. Recently, approximate balancing was proposed as an alternative balancing framework, which resolves the feasibility issue by using inequality moment constraints instead. However, it can be difficult to select the threshold parameters when the number of constraints is large. Moreover, moment constraints may not fully capture the discrepancy of covariate distributions. In this paper, we propose Mahalanobis balancing, which approximately balances covariate distributions from a multivariate perspective. We use a quadratic constraint to control overall imbalance with a single threshold parameter, which can be tuned by a simple selection procedure. We show that the dual problem of Mahalanobis balancing is an l_2 norm-based regularized regression problem, and establish interesting connection to propensity score models. We further generalize Mahalanobis balancing to the high-dimensional scenario. We derive asymptotic properties and make extensive comparisons with existing balancing methods in the numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.13439v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yimin Dai, Ying Yan</dc:creator>
    </item>
    <item>
      <title>Tucker tensor factor models: matricization and mode-wise PCA estimation</title>
      <link>https://arxiv.org/abs/2206.02508</link>
      <description>arXiv:2206.02508v3 Announce Type: replace 
Abstract: High-dimensional, higher-order tensor data are gaining prominence in a variety of fields, including but not limited to computer vision and network analysis. Tensor factor models, induced from noisy versions of tensor decompositions or factorizations, are natural potent instruments to study a collection of tensor-variate objects that may be dependent or independent. However, it is still in the early stage of developing statistical inferential theories for the estimation of various low-rank structures, which are customary to play the role of signals of tensor factor models. In this paper, we attempt to ``decode" the estimation of a higher-order tensor factor model by leveraging tensor matricization. Specifically, we recast it into mode-wise traditional high-dimensional vector/fiber factor models, enabling the deployment of conventional principal components analysis (PCA) for estimation. Demonstrated by the Tucker tensor factor model (TuTFaM), which is induced from the noisy version of the widely-used Tucker decomposition, we summarize that estimations on signal components are essentially mode-wise PCA techniques, and the involvement of projection and iteration will enhance the signal-to-noise ratio to various extent. We establish the inferential theory of the proposed estimators, conduct rich simulation experiments, and illustrate how the proposed estimations can work in tensor reconstruction, and clustering for independent video and dependent economic datasets, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02508v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xu Zhang, Guodong Li, Catherine C. Liu, Jianhua Guo</dc:creator>
    </item>
    <item>
      <title>On the Stability of General Bayesian Inference</title>
      <link>https://arxiv.org/abs/2301.13701</link>
      <description>arXiv:2301.13701v2 Announce Type: replace 
Abstract: We study the stability of posterior predictive inferences to the specification of the likelihood model and perturbations of the data generating process. In modern big data analyses, useful broad structural judgements may be elicited from the decision-maker but a level of interpolation is required to arrive at a likelihood model. As a result, an often computationally convenient canonical form is used in place of the decision-maker's true beliefs. Equally, in practice, observational datasets often contain unforeseen heterogeneities and recording errors and therefore do not necessarily correspond to how the process was idealised by the decision-maker. Acknowledging such imprecisions, a faithful Bayesian analysis should ideally be stable across reasonable equivalence classes of such inputs. We are able to guarantee that traditional Bayesian updating provides stability across only a very strict class of likelihood models and data generating processes, requiring the decision-maker to elicit their beliefs and understand how the data was generated with an unreasonable degree of accuracy. On the other hand, a generalised Bayesian alternative using the $\beta$-divergence loss function is shown to be stable across practical and interpretable neighbourhoods, providing assurances that posterior inferences are not overly dependent on accidentally introduced spurious specifications or data collection errors. We illustrate this in linear regression, binary classification, and mixture modelling examples, showing that stable updating does not compromise the ability to learn about the data generating process. These stability results provide a compelling justification for using generalised Bayes to facilitate inference under simplified canonical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.13701v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Jewson, Jim Q. Smith, Chris Holmes</dc:creator>
    </item>
    <item>
      <title>On new omnibus tests of uniformity on the hypersphere</title>
      <link>https://arxiv.org/abs/2304.04519</link>
      <description>arXiv:2304.04519v5 Announce Type: replace 
Abstract: Two new omnibus tests of uniformity for data on the hypersphere are proposed. The new test statistics exploit closed-form expressions for orthogonal polynomials, feature tuning parameters, and are related to a ``smooth maximum'' function and the Poisson kernel. We obtain exact moments of the test statistics under uniformity and rotationally symmetric alternatives, and give their null asymptotic distributions. We consider approximate oracle tuning parameters that maximize the power of the tests against known generic alternatives and provide tests that estimate oracle parameters through cross-validated procedures while maintaining the significance level. Numerical experiments explore the effectiveness of null asymptotic distributions and the accuracy of inexpensive approximations of exact null distributions. A simulation study compares the powers of the new tests with other tests of the Sobolev class, showing the benefits of the former. The proposed tests are applied to the study of the (seemingly uniform) nursing times of wild polar bears.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.04519v5</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11749-023-00882-x</arxiv:DOI>
      <arxiv:journal_reference>Test, 32(4):1508-1529, 2023</arxiv:journal_reference>
      <dc:creator>Alberto Fern\'andez-de-Marcos, Eduardo Garc\'ia-Portugu\'es</dc:creator>
    </item>
    <item>
      <title>Testing for no effect in regression problems: a permutation approach</title>
      <link>https://arxiv.org/abs/2305.02685</link>
      <description>arXiv:2305.02685v3 Announce Type: replace 
Abstract: Often the question arises whether $Y$ can be predicted based on $X$ using a certain model. Especially for highly flexible models such as neural networks one may ask whether a seemingly good prediction is actually better than fitting pure noise or whether it has to be attributed to the flexibility of the model. This paper proposes a rigorous permutation test to assess whether the prediction is better than the prediction of pure noise. The test avoids any sample splitting and is based instead on generating new pairings of $(X_i,Y_j)$. It introduces a new formulation of the null hypothesis and rigorous justification for the test, which distinguishes it from previous literature. The theoretical findings are applied both to simulated data and to sensor data of tennis serves in an experimental context. The simulation study underscores how the available information affects the test. It shows that the less informative the predictors, the lower the probability of rejecting the null hypothesis of fitting pure noise and emphasizes that detecting weaker dependence between variables requires a sufficient sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02685v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha{\l} Ciszewski, Jakob S\"ohl, Ton Leenen, Bart van Trigt, Geurt Jongbloed</dc:creator>
    </item>
    <item>
      <title>Confidence Intervals for Error Rates in 1:1 Matching Tasks: Critical Statistical Analysis and Recommendations</title>
      <link>https://arxiv.org/abs/2306.01198</link>
      <description>arXiv:2306.01198v3 Announce Type: replace 
Abstract: Matching algorithms are commonly used to predict matches between items in a collection. For example, in 1:1 face verification, a matching algorithm predicts whether two face images depict the same person. Accurately assessing the uncertainty of the error rates of such algorithms can be challenging when data are dependent and error rates are low, two aspects that have been often overlooked in the literature. In this work, we review methods for constructing confidence intervals for error rates in 1:1 matching tasks. We derive and examine the statistical properties of these methods, demonstrating how coverage and interval width vary with sample size, error rates, and degree of data dependence on both analysis and experiments with synthetic and real-world datasets. Based on our findings, we provide recommendations for best practices for constructing confidence intervals for error rates in 1:1 matching tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01198v3</guid>
      <category>stat.ME</category>
      <category>cs.CV</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo Fogliato, Pratik Patil, Pietro Perona</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian Inference for Berkson and Classical Measurement Error Models</title>
      <link>https://arxiv.org/abs/2306.01468</link>
      <description>arXiv:2306.01468v2 Announce Type: replace 
Abstract: Measurement error occurs when a covariate influencing a response variable is corrupted by noise. This can lead to misleading inference outcomes, particularly in problems where accurately estimating the relationship between covariates and response variables is crucial, such as causal effect estimation. Existing methods for dealing with measurement error often rely on strong assumptions such as knowledge of the error distribution or its variance and availability of replicated measurements of the covariates. We propose a Bayesian Nonparametric Learning framework that is robust to mismeasured covariates, does not require the preceding assumptions, and can incorporate prior beliefs about the error distribution. This approach gives rise to a general framework that is suitable for both Classical and Berkson error models via the appropriate specification of the prior centering measure of a Dirichlet Process (DP). Moreover, it offers flexibility in the choice of loss function depending on the type of regression model. We provide bounds on the generalization error based on the Maximum Mean Discrepancy (MMD) loss which allows for generalization to non-Gaussian distributed errors and nonlinear covariate-response relationships. We showcase the effectiveness of the proposed framework versus prior art in real-world problems containing either Berkson or Classical measurement errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01468v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charita Dellaporta, Theodoros Damoulas</dc:creator>
    </item>
    <item>
      <title>Estimating the Sampling Distribution of Posterior Decision Summaries in Bayesian Clinical Trials</title>
      <link>https://arxiv.org/abs/2306.09151</link>
      <description>arXiv:2306.09151v2 Announce Type: replace 
Abstract: Bayesian inference and the use of posterior or posterior predictive probabilities for decision making have become increasingly popular in clinical trials. The current practice in Bayesian clinical trials relies on a hybrid Bayesian-frequentist approach where the design and decision criteria are assessed with respect to frequentist operating characteristics such as power and type I error rate conditioning on a given set of parameters. These operating characteristics are commonly obtained via simulation studies. The utility of Bayesian measures, such as ``assurance", that incorporate uncertainty about model parameters in estimating the probabilities of various decisions in trials has been demonstrated recently. However, the computational burden remains an obstacle toward wider use of such criteria. In this article, we propose methodology which utilizes large sample theory of the posterior distribution to define parametric models for the sampling distribution of the posterior summaries used for decision making. The parameters of these models are estimated using a small number of simulation scenarios, thereby refining these models to capture the sampling distribution for small to moderate sample size. The proposed approach toward the assessment of conditional and marginal operating characteristics and sample size determination can be considered as simulation-assisted rather than simulation-based. It enables formal incorporation of uncertainty about the trial assumptions via a design prior and significantly reduces the computational burden for the design of Bayesian trials in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09151v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shirin Golchi, James Willard</dc:creator>
    </item>
    <item>
      <title>A switching state-space transmission model for tracking epidemics and assessing interventions</title>
      <link>https://arxiv.org/abs/2307.16138</link>
      <description>arXiv:2307.16138v2 Announce Type: replace 
Abstract: The effective control of infectious diseases relies on accurate assessment of the impact of interventions, which is often hindered by the complex dynamics of the spread of disease. A Beta-Dirichlet switching state-space transmission model is proposed to track underlying dynamics of disease and evaluate the effectiveness of interventions simultaneously. As time evolves, the switching mechanism introduced in the susceptible-exposed-infected-recovered (SEIR) model is able to capture the timing and magnitude of changes in the transmission rate due to the effectiveness of control measures. The implementation of this model is based on a particle Markov Chain Monte Carlo algorithm, which can estimate the time evolution of SEIR states, switching states, and high-dimensional parameters efficiently. The efficacy of the proposed model and estimation procedure are demonstrated through simulation studies. With a real-world application to British Columbia's COVID-19 outbreak, the proposed switching state-space transmission model quantifies the reduction of transmission rate following interventions. The proposed model provides a promising tool to inform public health policies aimed at studying the underlying dynamics and evaluating the effectiveness of interventions during the spread of the disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16138v2</guid>
      <category>stat.ME</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingxue Feng, Liangliang Wang</dc:creator>
    </item>
    <item>
      <title>Fully Synthetic Data for Complex Surveys</title>
      <link>https://arxiv.org/abs/2309.09115</link>
      <description>arXiv:2309.09115v4 Announce Type: replace 
Abstract: When seeking to release public use files for confidential data, statistical agencies can generate fully synthetic data. We propose an approach for making fully synthetic data from surveys collected with complex sampling designs. Our approach adheres to the general strategy proposed by Rubin (1993). Specifically, we generate pseudo-populations by applying the weighted finite population Bayesian bootstrap to account for survey weights, take simple random samples from those pseudo-populations, estimate synthesis models using these simple random samples, and release simulated data drawn from the models as public use files. To facilitate variance estimation, we use the framework of multiple imputation with two data generation strategies. In the first, we generate multiple data sets from each simple random sample. In the second, we generate a single synthetic data set from each simple random sample. We present multiple imputation combining rules for each setting. We illustrate the repeated sampling properties of the combining rules via simulation studies, including comparisons with synthetic data generation based on pseudo-likelihood methods. We apply the proposed methods to a subset of data from the American Community Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.09115v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shirley Mathur, Yajuan Si, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Expressing and visualizing model uncertainty in Bayesian variable selection using Cartesian credible sets</title>
      <link>https://arxiv.org/abs/2402.12323</link>
      <description>arXiv:2402.12323v2 Announce Type: replace 
Abstract: Modern regression applications can involve hundreds or thousands of variables which motivates the use of variable selection methods. Bayesian variable selection defines a posterior distribution on the possible subsets of the variables (which are usually termed models) to express uncertainty about which variables are strongly linked to the response. This can be used to provide Bayesian model averaged predictions or inference, and to understand the relative importance of different variables. However, there has been little work on meaningful representations of this uncertainty beyond first order summaries. We introduce Cartesian credible sets to address this gap. The elements of these sets are formed by concatenating sub-models defined on each block of a partition of the variables. Investigating these sub-models allow us to understand whether the models in the Cartesian credible set always/never/sometimes include a particular variable or group of variables and provide a useful summary of model uncertainty. We introduce methods to find these sets that emphasize ease of understanding. The potential of the method is illustrated on regression problems with both small and large numbers of variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12323v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J. E. Griffin</dc:creator>
    </item>
    <item>
      <title>Regularised Spectral Estimation for High-Dimensional Point Processes</title>
      <link>https://arxiv.org/abs/2403.12908</link>
      <description>arXiv:2403.12908v2 Announce Type: replace 
Abstract: Advances in modern technology have enabled the simultaneous recording of neural spiking activity, which statistically can be represented by a multivariate point process. We characterise the second order structure of this process via the spectral density matrix, a frequency domain equivalent of the covariance matrix. In the context of neuronal analysis, statistics based on the spectral density matrix can be used to infer connectivity in the brain network between individual neurons. However, the high-dimensional nature of spike train data mean that it is often difficult, or at times impossible, to compute these statistics. In this work, we discuss the importance of regularisation-based methods for spectral estimation, and propose novel methodology for use in the point process setting. We establish asymptotic properties for our proposed estimators and evaluate their performance on synthetic data simulated from multivariate Hawkes processes. Finally, we apply our methodology to neuroscience spike train data in order to illustrate its ability to infer brain connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12908v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carla Pinkney, Carolina Euan, Alex Gibberd, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Neyman-Pearson Multi-class Classification via Cost-sensitive Learning</title>
      <link>https://arxiv.org/abs/2111.04597</link>
      <description>arXiv:2111.04597v3 Announce Type: replace-cross 
Abstract: Most existing classification methods aim to minimize the overall misclassification error rate. However, in applications such as loan default prediction, different types of errors can have varying consequences. To address this asymmetry issue, two popular paradigms have been developed: the Neyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous studies on the NP paradigm have primarily focused on the binary case, while the multi-class NP problem poses a greater challenge due to its unknown feasibility. In this work, we tackle the multi-class NP problem by establishing a connection with the CS problem via strong duality and propose two algorithms. We extend the concept of NP oracle inequalities, crucial in binary classifications, to NP oracle properties in the multi-class context. Our algorithms satisfy these NP oracle properties under certain conditions. Furthermore, we develop practical algorithms to assess the feasibility and strong duality in multi-class NP problems, which can offer practitioners the landscape of a multi-class NP problem with various target error levels. Simulations and real data studies validate the effectiveness of our algorithms. To our knowledge, this is the first study to address the multi-class NP problem with theoretical guarantees. The proposed algorithms have been implemented in the R package \texttt{npcs}, which is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.04597v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Tian, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation</title>
      <link>https://arxiv.org/abs/2211.01939</link>
      <description>arXiv:2211.01939v3 Announce Type: replace-cross 
Abstract: We study the problem of model selection in causal inference, specifically for conditional average treatment effect (CATE) estimation. Unlike machine learning, there is no perfect analogue of cross-validation for model selection as we do not observe the counterfactual potential outcomes. Towards this, a variety of surrogate metrics have been proposed for CATE model selection that use only observed data. However, we do not have a good understanding regarding their effectiveness due to limited comparisons in prior studies. We conduct an extensive empirical analysis to benchmark the surrogate model selection metrics introduced in the literature, as well as the novel ones introduced in this work. We ensure a fair comparison by tuning the hyperparameters associated with these metrics via AutoML, and provide more detailed trends by incorporating realistic datasets via generative modeling. Our analysis suggests novel model selection strategies based on careful hyperparameter selection of CATE estimators and causal ensembling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.01939v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divyat Mahajan, Ioannis Mitliagkas, Brady Neal, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Linear Estimators</title>
      <link>https://arxiv.org/abs/2309.06305</link>
      <description>arXiv:2309.06305v3 Announce Type: replace-cross 
Abstract: We propose a novel sensitivity analysis framework for linear estimators with identification failures that can be viewed as seeing the wrong outcome distribution. Our approach measures the degree of identification failure through the change in measure between the observed distribution and a hypothetical target distribution that would identify the causal parameter of interest. The framework yields a sensitivity analysis that generalizes existing bounds for Average Potential Outcome (APO), Regression Discontinuity (RD), and instrumental variables (IV) exclusion failure designs. Our partial identification results extend results from the APO context to allow even unbounded likelihood ratios. Our proposed sensitivity analysis consistently estimates sharp bounds under plausible conditions and estimates valid bounds under mild conditions. We find that our method performs well in simulations even when targeting a discontinuous and nearly infinite bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06305v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Dorn, Luther Yap</dc:creator>
    </item>
    <item>
      <title>Riemannian Laplace Approximation with the Fisher Metric</title>
      <link>https://arxiv.org/abs/2311.02766</link>
      <description>arXiv:2311.02766v5 Announce Type: replace-cross 
Abstract: Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the method, and demonstrating practical improvements in a range of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02766v5</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto Klami</dc:creator>
    </item>
    <item>
      <title>What Hides behind Unfairness? Exploring Dynamics Fairness in Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2404.10942</link>
      <description>arXiv:2404.10942v2 Announce Type: replace-cross 
Abstract: In sequential decision-making problems involving sensitive attributes like race and gender, reinforcement learning (RL) agents must carefully consider long-term fairness while maximizing returns. Recent works have proposed many different types of fairness notions, but how unfairness arises in RL problems remains unclear. In this paper, we address this gap in the literature by investigating the sources of inequality through a causal lens. We first analyse the causal relationships governing the data generation process and decompose the effect of sensitive attributes on long-term well-being into distinct components. We then introduce a novel notion called dynamics fairness, which explicitly captures the inequality stemming from environmental dynamics, distinguishing it from those induced by decision-making or inherited from the past. This notion requires evaluating the expected changes in the next state and the reward induced by changing the value of the sensitive attribute while holding everything else constant. To quantitatively evaluate this counterfactual concept, we derive identification formulas that allow us to obtain reliable estimations from data. Extensive experiments demonstrate the effectiveness of the proposed techniques in explaining, detecting, and reducing inequality in reinforcement learning. We publicly release code at https://github.com/familyld/InsightFair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10942v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang</dc:creator>
    </item>
  </channel>
</rss>
