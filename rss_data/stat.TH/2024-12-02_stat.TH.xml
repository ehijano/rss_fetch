<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Dec 2024 05:00:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Robust estimation of boundary using doubly stochastic scaling of Gaussian kernel</title>
      <link>https://arxiv.org/abs/2411.18942</link>
      <description>arXiv:2411.18942v1 Announce Type: new 
Abstract: This paper addresses the problem of detecting points on or near the boundary of a dataset sampled, potentially with noise, from a compact manifold with boundary. We extend recent advances in doubly stochastic scaling of the Gaussian heat kernel via Sinkhorn iterations to this setting. Our main contributions are: (a) deriving a characterization of the scaling factors for manifolds with boundary, (b) developing a boundary direction estimator, aimed at identifying boundary points, based on doubly stochastic kernel and local principal component analysis, and (c) demonstrating through simulations that the resulting estimates of the boundary points outperform the standard Gaussian kernel-based approach, particularly under noisy conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18942v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruv Kohli, Jesse He, Chester Holtz, Gal Mishne, Alexander Cloninger</dc:creator>
    </item>
    <item>
      <title>How to measure multidimensional variation?</title>
      <link>https://arxiv.org/abs/2411.19529</link>
      <description>arXiv:2411.19529v1 Announce Type: new 
Abstract: The coefficient of variation, which measures the variability of a distribution from its mean, is not uniquely defined in the multidimensional case, and so is the multidimensional Gini index, which measures the inequality of a distribution in terms of the mean differences among its observations. In this paper, we connect these two notions of sparsity, and propose a multidimensional coefficient of variation based on a multidimensional Gini index. We demonstrate that the proposed coefficient possesses the properties of the univariate coefficient of variation. We also show its connection with the Voinov-Nikulin coefficient of variation, and compare it with the other multivariate coefficients available in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19529v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gennaro Auricchio, Paolo Giudici, Giuseppe Toscani</dc:creator>
    </item>
    <item>
      <title>Linear methods for non-linear inverse problems</title>
      <link>https://arxiv.org/abs/2411.19797</link>
      <description>arXiv:2411.19797v1 Announce Type: new 
Abstract: We consider the recovery of an unknown function $f$ from a noisy observation of the solution $u_f$ to a partial differential equation that can be written in the form $\mathcal{L} u_f=c(f,u_f)$, for a differential operator $\mathcal{L}$ that is rich enough to recover $f$ from $\mathcal{L} u_f$. Examples include the time-independent Schr\"odinger equation $\Delta u_f = 2u_ff$, the heat equation with absorption term $(\partial_t -\Delta_x/2) u_f=fu_f$, and the Darcy problem $\nabla\cdot (f \nabla u_f) = h$. We transform this problem into the linear inverse problem of recovering $\mathcal{L} u_f$ under the Dirichlet boundary condition, and show that Bayesian methods with priors placed either on $u_f$ or $\mathcal{L} u_f$ for this problem yield optimal recovery rates not only for $u_f$, but also for $f$. We also derive frequentist coverage guarantees for the corresponding Bayesian credible sets. Adaptive priors are shown to yield adaptive contraction rates for $f$, thus eliminating the need to know the smoothness of this function. The results are illustrated by numerical experiments on synthetic data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19797v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geerten Koers, Botond Szabo, Aad van der Vaart</dc:creator>
    </item>
    <item>
      <title>Statistical inference of a ranked community in a directed graph</title>
      <link>https://arxiv.org/abs/2411.19885</link>
      <description>arXiv:2411.19885v1 Announce Type: new 
Abstract: We study the problem of detecting or recovering a planted ranked subgraph from a directed graph, an analog for directed graphs of the well-studied planted dense subgraph model. We suppose that, among a set of $n$ items, there is a subset $S$ of $k$ items having a latent ranking in the form of a permutation $\pi$ of $S$, and that we observe a fraction $p$ of pairwise orderings between elements of $\{1, \dots, n\}$ which agree with $\pi$ with probability $\frac{1}{2} + q$ between elements of $S$ and otherwise are uniformly random. Unlike in the planted dense subgraph and planted clique problems where the community $S$ is distinguished by its unusual density of edges, here the community is only distinguished by the unusual consistency of its pairwise orderings. We establish computational and statistical thresholds for both detecting and recovering such a ranked community. In the log-density setting where $k$, $p$, and $q$ all scale as powers of $n$, we establish the exact thresholds in the associated exponents at which detection and recovery become statistically and computationally feasible. These regimes include a rich variety of behaviors, exhibiting both statistical-computational and detection-recovery gaps. We also give finer-grained results for two extreme cases: (1) $p = 1$, $k = n$, and $q$ small, where a full tournament is observed that is weakly correlated with a global ranking, and (2) $p = 1$, $q = \frac{1}{2}$, and $k$ small, where a small "ordered clique" (totally ordered directed subgraph) is planted in a random tournament.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19885v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitriy Kunisky, Daniel A. Spielman, Alexander S. Wein, Xifan Yu</dc:creator>
    </item>
    <item>
      <title>Inference on Dynamic Spatial Autoregressive Models with Change Point Detection</title>
      <link>https://arxiv.org/abs/2411.18773</link>
      <description>arXiv:2411.18773v1 Announce Type: cross 
Abstract: We analyze a varying-coefficient dynamic spatial autoregressive model with spatial fixed effects. One salient feature of the model is the incorporation of multiple spatial weight matrices through their linear combinations with varying coefficients, which help solve the problem of choosing the most "correct" one for applied econometricians who often face the availability of multiple expert spatial weight matrices. We estimate and make inferences on the model coefficients and coefficients in basis expansions of the varying coefficients through penalized estimations, establishing the oracle properties of the estimators and the consistency of the overall estimated spatial weight matrix, which can be time-dependent. We further consider two applications of our model in change point detections in dynamic spatial autoregressive models, providing theoretical justifications in consistent change point locations estimation and practical implementations. Simulation experiments demonstrate the performance of our proposed methodology, and a real data analysis is also carried out.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18773v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Yudong Chen, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>Double Descent in Portfolio Optimization: Dance between Theoretical Sharpe Ratio and Estimation Accuracy</title>
      <link>https://arxiv.org/abs/2411.18830</link>
      <description>arXiv:2411.18830v1 Announce Type: cross 
Abstract: We study the relationship between model complexity and out-of-sample performance in the context of mean-variance portfolio optimization. Representing model complexity by the number of assets, we find that the performance of low-dimensional models initially improves with complexity but then declines due to overfitting. As model complexity becomes sufficiently high, the performance improves with complexity again, resulting in a double ascent Sharpe ratio curve similar to the double descent phenomenon observed in artificial intelligence. The underlying mechanisms involve an intricate interaction between the theoretical Sharpe ratio and estimation accuracy. In high-dimensional models, the theoretical Sharpe ratio approaches its upper limit, and the overfitting problem is reduced because there are more parameters than data restrictions, which allows us to choose well-behaved parameters based on inductive bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18830v1</guid>
      <category>q-fin.PM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonghe Lu, Yanrong Yang, Terry Zhang</dc:creator>
    </item>
    <item>
      <title>Transfer Learning for High-dimensional Quantile Regression with Distribution Shift</title>
      <link>https://arxiv.org/abs/2411.19933</link>
      <description>arXiv:2411.19933v1 Announce Type: cross 
Abstract: Information from related source studies can often enhance the findings of a target study. However, the distribution shift between target and source studies can severely impact the efficiency of knowledge transfer. In the high-dimensional regression setting, existing transfer approaches mainly focus on the parameter shift. In this paper, we focus on the high-dimensional quantile regression with knowledge transfer under three types of distribution shift: parameter shift, covariate shift, and residual shift. We propose a novel transferable set and a new transfer framework to address the above three discrepancies. Non-asymptotic estimation error bounds and source detection consistency are established to validate the availability and superiority of our method in the presence of distribution shift. Additionally, an orthogonal debiased approach is proposed for statistical inference with knowledge transfer, leading to sharper asymptotic results. Extensive simulation results as well as real data applications further demonstrate the effectiveness of our proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19933v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiqi Bai, Yijiao Zhang, Hanbo Yang, Zhongyi Zhu</dc:creator>
    </item>
    <item>
      <title>On deviation probabilities in non-parametric regression with heavy-tailed noise</title>
      <link>https://arxiv.org/abs/2301.10498</link>
      <description>arXiv:2301.10498v3 Announce Type: replace 
Abstract: This paper is devoted to the problem of determining the concentration bounds that are achievable in non-parametric regression. We consider the setting where features are supported on a bounded subset of $\mathbb{R}^d$, the regression function is Lipschitz, and the noise is only assumed to have a finite second moment. We first specify the fundamental limits of the problem by establishing a general lower bound on deviation probabilities, and then construct explicit estimators that achieve this bound. These estimators are obtained by applying the median-of-means principle to classical local averaging rules in non-parametric regression, including nearest neighbors and kernel procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.10498v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Ben-Hamou, Arnaud Guyader</dc:creator>
    </item>
    <item>
      <title>Asymptotic properties of adaptive designs through differentiability in quadratic mean</title>
      <link>https://arxiv.org/abs/2312.13387</link>
      <description>arXiv:2312.13387v4 Announce Type: replace 
Abstract: There exist multiple regression applications in engineering, industry and medicine where the outcomes follow an adaptive experimental design in which the next measurement depends on the previous observations, so that the observations are not conditionally independent given the covariates. In the existing literature on such adaptive designs, results asserting asymptotic normality of the maximum likelihood estimator require regularity conditions involving the second or third derivatives of the log-likelihood. Here we instead extend the theory of differentiability in quadratic mean (DQM) to the setting of adaptive designs, which requires strictly fewer regularity assumptions than the classical theory. In doing so, we discover a new DQM assumption, which we call summable differentiability in quadratic mean (S-DQM). As applications, we first verify asymptotic normality for a classical adaptive designs, namely the Bruceton design, before moving on to a complicated problem, namely a Markovian version of the Langlie design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13387v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dennis Christensen, Emil Aas Stoltenberg, Nils Lid Hjort</dc:creator>
    </item>
    <item>
      <title>Asymptotics for non-degenerate multivariate $U$-statistics with estimated nuisance parameters under the null and local alternative hypotheses</title>
      <link>https://arxiv.org/abs/2401.11272</link>
      <description>arXiv:2401.11272v3 Announce Type: replace 
Abstract: The large-sample behavior of non-degenerate multivariate $U$-statistics of arbitrary degree is investigated under the assumption that their kernel depends on parameters that can be estimated consistently. Mild regularity conditions are provided which guarantee that once properly normalized, such statistics are asymptotically multivariate Gaussian both under the null hypothesis and sequences of local alternatives. The work of Randles (1982, Ann. Statist.) is extended in three ways: the data and the kernel values can be multivariate rather than univariate, the limiting behavior under local alternatives is studied for the first time, and the effect of knowing some of the nuisance parameters is quantified. These results can be applied to a broad range of goodness-of-fit testing contexts, as shown in two specific examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11272v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Multivariate Analysis (2025)</arxiv:journal_reference>
      <dc:creator>Alain Desgagn\'e, Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Quantile processes and their applications in finite populations</title>
      <link>https://arxiv.org/abs/2407.21238</link>
      <description>arXiv:2407.21238v2 Announce Type: replace 
Abstract: The weak convergence of the quantile processes, which are constructed based on different estimators of the finite population quantiles, is shown under various well-known sampling designs based on a superpopulation model. The results related to the weak convergence of these quantile processes are applied to find asymptotic distributions of the smooth $L$-estimators and the estimators of smooth functions of finite population quantiles. Based on these asymptotic distributions, confidence intervals are constructed for several finite population parameters like the median, the $\alpha$-trimmed means, the interquartile range and the quantile based measure of skewness. Comparisons of various estimators are carried out based on their asymptotic distributions. We show that the use of the auxiliary information in the construction of the estimators sometimes has an adverse effect on the performances of the smooth $L$-estimators and the estimators of smooth functions of finite population quantiles under several sampling designs. Further, the performance of each of the above-mentioned estimators sometimes becomes worse under sampling designs, which use the auxiliary information, than their performances under simple random sampling without replacement (SRSWOR).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21238v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOS2432</arxiv:DOI>
      <arxiv:journal_reference>The Annals of Statistics, 52(5), 2194-2216 (2024)</arxiv:journal_reference>
      <dc:creator>Anurag Dey, Probal Chaudhuri</dc:creator>
    </item>
    <item>
      <title>Community detection for binary graphical models in high dimension</title>
      <link>https://arxiv.org/abs/2411.15627</link>
      <description>arXiv:2411.15627v2 Announce Type: replace 
Abstract: Let $N$ components be partitioned into two communities, denoted ${\cal P}_+$ and ${\cal P}_-$, possibly of different sizes. Assume that they are connected via a directed and weighted Erd\"os-R\'enyi random graph (DWER) with unknown parameter $ p \in (0, 1).$ The weights assigned to the existing connections are of mean-field type, scaling as $N^{-1}$. At each time unit, we observe the state of each component: either it sends some signal to its successors (in the directed graph) or remains silent otherwise. In this paper, we show that it is possible to find the communities ${\cal P}_+$ and ${\cal P}_-$ based only on the activity of the $N$ components observed over $T$ time units. More specifically, we propose a simple algorithm for which the probability of {\it exact recovery} converges to $1$ as long as $(N/T^{1/2})\log(NT) \to 0$, as $T$ and $N$ diverge. Interestingly, this simple algorithm does not require any prior knowledge on the other model parameters (e.g. the edge probability $p$). The key step in our analysis is to derive an asymptotic approximation of the one unit time-lagged covariance matrix associated to the states of the $N$ components, as $N$ diverges. This asymptotic approximation relies on the study of the behavior of the solutions of a matrix equation of Stein type satisfied by the simultaneous (0-lagged) covariance matrix associated to the states of the components. This study is challenging, specially because the simultaneous covariance matrix is random since it depends on the underlying DWER random graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15627v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Chevallier, Guilherme Ost</dc:creator>
    </item>
    <item>
      <title>No Free Lunch for Approximate MCMC</title>
      <link>https://arxiv.org/abs/2010.12514</link>
      <description>arXiv:2010.12514v2 Announce Type: replace-cross 
Abstract: It is widely known that the performance of Markov chain Monte Carlo (MCMC) can degrade quickly when targeting computationally expensive posterior distributions, such as when the sample size is large. This has motivated the search for MCMC variants that scale well to large datasets. One popular general approach has been to look at only a subsample of the data at every step. In this note, we point out that well-known MCMC convergence results often imply that these ``subsampling'' MCMC algorithms cannot greatly improve performance. We apply these abstract results to realistic statistical problems and proposed algorithms, and also discuss some design principles suggested by the results. Finally, we develop estimates for the singular values of random matrices bounds that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.12514v2</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James E. Johndrow, Natesh S. Pillai, Aaron Smith</dc:creator>
    </item>
    <item>
      <title>On Consistency of Signature Using Lasso</title>
      <link>https://arxiv.org/abs/2305.10413</link>
      <description>arXiv:2305.10413v4 Announce Type: replace-cross 
Abstract: Signatures are iterated path integrals of continuous and discrete-time processes, and their universal nonlinearity linearizes the problem of feature selection in time series data analysis. This paper studies the consistency of signature using Lasso regression, both theoretically and numerically. We establish conditions under which the Lasso regression is consistent both asymptotically and in finite sample. Furthermore, we show that the Lasso regression is more consistent with the It\^o signature for time series and processes that are closer to the Brownian motion and with weaker inter-dimensional correlations, while it is more consistent with the Stratonovich signature for mean-reverting time series and processes. We demonstrate that signature can be applied to learn nonlinear functions and option prices with high accuracy, and the performance depends on properties of the underlying process and the choice of the signature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10413v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Guo, Binnan Wang, Ruixun Zhang, Chaoyi Zhao</dc:creator>
    </item>
    <item>
      <title>A smoothed-Bayesian approach to frequency recovery from sketched data</title>
      <link>https://arxiv.org/abs/2309.15408</link>
      <description>arXiv:2309.15408v3 Announce Type: replace-cross 
Abstract: We provide a novel statistical perspective on a classical problem at the intersection of computer science and information theory: recovering the empirical frequency of a symbol in a large discrete dataset using only a compressed representation, or sketch, obtained via random hashing. Departing from traditional algorithmic approaches, recent works have proposed Bayesian nonparametric (BNP) methods that can provide more informative frequency estimates by leveraging modeling assumptions about the distribution of the sketched data. In this paper, we propose a smoothed-Bayesian method, inspired by existing BNP approaches but designed in a frequentist framework to overcome the computational limitations of the BNP approaches when dealing with large-scale data from realistic distributions, including those with power-law tail behaviors. For sketches obtained with a single hash function, our approach is supported by rigorous frequentist properties, including unbiasedness and optimality under a squared error loss function within an intuitive class of linear estimators. For sketches with multiple hash functions, we introduce an approach based on multi-view learning to construct computationally efficient frequency estimators. We validate our method on synthetic and real data, comparing its performance to that of existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15408v3</guid>
      <category>stat.ME</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Stefano Favaro, Matteo Sesia</dc:creator>
    </item>
    <item>
      <title>Statistical learning theory and Occam's razor: The core argument</title>
      <link>https://arxiv.org/abs/2312.13842</link>
      <description>arXiv:2312.13842v2 Announce Type: replace-cross 
Abstract: Statistical learning theory is often associated with the principle of Occam's razor, which recommends a simplicity preference in inductive inference. This paper distills the core argument for simplicity obtainable from statistical learning theory, built on the theory's central learning guarantee for the method of empirical risk minimization. This core "means-ends" argument is that a simpler hypothesis class or inductive model is better because it has better learning guarantees; however, these guarantees are model-relative and so the theoretical push towards simplicity is checked by our prior knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13842v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11023-024-09703-y</arxiv:DOI>
      <dc:creator>Tom F. Sterkenburg</dc:creator>
    </item>
    <item>
      <title>Evaluating Quantumness, Efficiency and Cost of Quantum Random Number Generators via Photon Statistics</title>
      <link>https://arxiv.org/abs/2405.14085</link>
      <description>arXiv:2405.14085v4 Announce Type: replace-cross 
Abstract: This work presents two significant contributions from the perspectives of QRNG manufacturers and users. For manufacturers, the conventional method of assessing the quantumness of single-photon-based QRNGs through mean and variance comparisons of photon counts is statistically unreliable due to finite sample sizes. Given the sub-Poissonian statistics of single photons, confirming the underlying distribution is crucial for validating a QRNG's quantumness. We propose a more efficient two-fold statistical approach to ensure the quantumness of optical sources with the desired confidence level. Additionally, we demonstrate that the output of QRNGs from exponential and uniform distributions exhibit similarity under device noise, deriving corresponding photon statistics and conditions for $\epsilon$-randomness.
  From the user's perspective, the fundamental parameters of a QRNG are quantumness, efficiency (random entropy and random number generation rate), and cost. Our analysis reveals that these parameters depend on three factors, namely, expected photon count per unit time, external reference cycle duration, and detection efficiency. A lower expected photon count enhances entropy but increases cost and decreases the generation rate. A shorter external reference cycle boosts entropy but must exceed a minimum threshold to minimize timing errors, with minor impacts on cost and rate. Lower detection efficiency enhances entropy and lowers cost but reduces the generation rate. Finally, to validate our results, we perform statistical tests like NIST, Dieharder, AIS-31, ENT etc. over the data simulated with different values of the above parameters. Our findings can empower manufacturers to customize QRNGs to meet user needs effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14085v4</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Goutam Paul, Nirupam Basak, Soumya Das</dc:creator>
    </item>
    <item>
      <title>On the Convergence Rates of Set Membership Estimation of Linear Systems with Disturbances Bounded by General Convex Sets</title>
      <link>https://arxiv.org/abs/2406.00574</link>
      <description>arXiv:2406.00574v2 Announce Type: replace-cross 
Abstract: This paper studies the uncertainty set estimation of system parameters of linear dynamical systems with bounded disturbances, which is motivated by robust (adaptive) constrained control. Departing from the confidence bounds of least square estimation from the machine-learning literature, this paper focuses on a method commonly used in (robust constrained) control literature: set membership estimation (SME). SME tends to enjoy better empirical performance than LSE's confidence bounds when the system disturbances are bounded. However, the theoretical guarantees of SME are not fully addressed even for i.i.d. bounded disturbances. In the literature, SME's convergence has been proved for general convex supports of the disturbances, but SME's convergence rate assumes a special type of disturbance support: $ \ell_\infty $ ball. The main contribution of this paper is relaxing the assumption on the disturbance support and establishing the convergence rates of SME for general convex supports, which closes the gap on the applicability of the convergence and convergence rates results. Numerical experiments on SME and LSE's confidence bounds are also provided for different disturbance supports.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00574v2</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haonan Xu, Yingying Li</dc:creator>
    </item>
    <item>
      <title>Stein transport for Bayesian inference</title>
      <link>https://arxiv.org/abs/2409.01464</link>
      <description>arXiv:2409.01464v2 Announce Type: replace-cross 
Abstract: We introduce $\textit{Stein transport}$, a novel methodology for Bayesian inference designed to efficiently push an ensemble of particles along a predefined curve of tempered probability distributions. The driving vector field is chosen from a reproducing kernel Hilbert space and can be derived either through a suitable kernel ridge regression formulation or as an infinitesimal optimal transport map in the Stein geometry. The update equations of Stein transport resemble those of Stein variational gradient descent (SVGD), but introduce a time-varying score function as well as specific weights attached to the particles. While SVGD relies on convergence in the long-time limit, Stein transport reaches its posterior approximation at finite time $t=1$. Studying the mean-field limit, we discuss the errors incurred by regularisation and finite-particle effects, and we connect Stein transport to birth-death dynamics and Fisher-Rao gradient flows. In a series of experiments, we show that in comparison to SVGD, Stein transport not only often reaches more accurate posterior approximations with a significantly reduced computational budget, but that it also effectively mitigates the variance collapse phenomenon commonly observed in SVGD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01464v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolas N\"usken</dc:creator>
    </item>
    <item>
      <title>Polynomial approximation of noisy functions</title>
      <link>https://arxiv.org/abs/2410.02317</link>
      <description>arXiv:2410.02317v2 Announce Type: replace-cross 
Abstract: Approximating a univariate function on the interval $[-1,1]$ with a polynomial is among the most classical problems in numerical analysis. When the function evaluations come with noise, a least-squares fit is known to reduce the effect of noise as more samples are taken. The generic algorithm for the least-squares problem requires $O(Nn^2)$ operations, where $N+1$ is the number of sample points and $n$ is the degree of the polynomial approximant. This algorithm is unstable when $n$ is large, for example $n\gg \sqrt{N}$ for equispaced sample points. In this study, we blend numerical analysis and statistics to introduce a stable and fast $O(N\log N)$ algorithm called NoisyChebtrunc based on the Chebyshev interpolation. It has the same error reduction effect as least-squares and the convergence is spectral until the error reaches $O(\sigma \sqrt{{n}/{N}})$, where $\sigma$ is the noise level, after which the error continues to decrease at the Monte-Carlo $O(1/\sqrt{N})$ rate. To determine the polynomial degree, NoisyChebtrunc employs a statistical criterion, namely Mallows' $C_p$. We analyze NoisyChebtrunc in terms of the variance and concentration in the infinity norm to the underlying noiseless function. These results show that with high probability the infinity-norm error is bounded by a small constant times $\sigma \sqrt{{n}/{N}}$, when the noise {is} independent and follows a subgaussian or subexponential distribution. We illustrate the performance of NoisyChebtrunc with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02317v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takeru Matsuda, Yuji Nakatsukasa</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2411.07978</link>
      <description>arXiv:2411.07978v2 Announce Type: replace-cross 
Abstract: This study introduces a doubly robust (DR) estimator for regression discontinuity (RD) designs. In RD designs, treatment effects are estimated in a quasi-experimental setting where treatment assignment depends on whether a running variable surpasses a predefined cutoff. A common approach in RD estimation is to apply nonparametric regression methods, such as local linear regression. In such an approach, the validity relies heavily on the consistency of nonparametric estimators and is limited by the nonparametric convergence rate, thereby preventing $\sqrt{n}$-consistency. To address these issues, we propose the DR-RD estimator, which combines two distinct estimators for the conditional expected outcomes. If either of these estimators is consistent, the treatment effect estimator remains consistent. Furthermore, due to the debiasing effect, our proposed estimator achieves $\sqrt{n}$-consistency if both regression estimators satisfy certain mild conditions, which also simplifies statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07978v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>To bootstrap or to rollout? An optimal and adaptive interpolation</title>
      <link>https://arxiv.org/abs/2411.09731</link>
      <description>arXiv:2411.09731v2 Announce Type: replace-cross 
Abstract: Bootstrapping and rollout are two fundamental principles for value function estimation in reinforcement learning (RL). We introduce a novel class of Bellman operators, called subgraph Bellman operators, that interpolate between bootstrapping and rollout methods. Our estimator, derived by solving the fixed point of the empirical subgraph Bellman operator, combines the strengths of the bootstrapping-based temporal difference (TD) estimator and the rollout-based Monte Carlo (MC) methods. Specifically, the error upper bound of our estimator approaches the optimal variance achieved by TD, with an additional term depending on the exit probability of a selected subset of the state space. At the same time, the estimator exhibits the finite-sample adaptivity of MC, with sample complexity depending only on the occupancy measure of this subset. We complement the upper bound with an information-theoretic lower bound, showing that the additional term is unavoidable given a reasonable sample size. Together, these results establish subgraph Bellman estimators as an optimal and adaptive framework for reconciling TD and MC methods in policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09731v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenlong Mou, Jian Qian</dc:creator>
    </item>
  </channel>
</rss>
