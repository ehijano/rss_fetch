<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Jun 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Sim2Real Approach for Identifying Task-Relevant Properties in Interpretable Machine Learning</title>
      <link>https://arxiv.org/abs/2406.00116</link>
      <description>arXiv:2406.00116v1 Announce Type: new 
Abstract: Existing user studies suggest that different tasks may require explanations with different properties. However, user studies are expensive. In this paper, we introduce a generalizable, cost-effective method for identifying task-relevant explanation properties in silico, which can guide the design of more expensive user studies. We use our approach to identify relevant proxies for three example tasks and validate our simulation with real user studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00116v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eura Nofshin, Esther Brown, Brian Lim, Weiwei Pan, Finale Doshi-Velez</dc:creator>
    </item>
    <item>
      <title>Disengagement From Games: Characterizing the Experience and Process of Exiting Play Sessions</title>
      <link>https://arxiv.org/abs/2406.00189</link>
      <description>arXiv:2406.00189v1 Announce Type: new 
Abstract: The games research community has developed substantial knowledge on designing engaging experiences that draw players in. Surprisingly, less is known about player \textit{dis}engagement, with existing work predominantly addressing disengagement from the perspective of problematic play, and research exploring player disengagement from a constructive designer perspective is lacking. In this paper, we address this gap and argue that disengagement from games should be constructively designed, allowing players to exit play sessions in a self-determined way. Following a two-phase research approach that combines an interview study (n=16) with a follow-up online survey (n=111), we systematically analyze player perspectives on exiting play sessions. Our work expands the existing notion of disengagement through a characterization of exit experiences, a lens on disengagement as a process, and points for reflection for the design of games that seek to address player disengagement in a constructive way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00189v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dmitry Alexandrovsky, Kathrin Gerling, Merlin Steven Opp, Christopher Benjamin Hahn, Max V. Birk, Meshaiel Alsheail</dc:creator>
    </item>
    <item>
      <title>Measuring eye-tracking accuracy and its impact on usability in apple vision pro</title>
      <link>https://arxiv.org/abs/2406.00255</link>
      <description>arXiv:2406.00255v1 Announce Type: new 
Abstract: With built-in eye-tracking cameras, the Apple Vision Pro (AVP) enables gaze-based interaction, eye image rendering on external screens, and iris recognition for device unlocking. One of the technological advancements of the AVP is its heavy reliance on gaze- and gesture-based interaction. However, limited information is available regarding the specifics of the eye-tracking device in the AVP, and raw gaze data is inaccessible to developers. This study evaluated the eye-tracking accuracy of the AVP, leveraging foveated rendering, and examined how tracking accuracy relates to user-reported usability. The results revealed an overall gaze error of 2.5{\deg} (or 61.95 pixels) within a tested field of view (FOV) of approximately 34{\deg} x 18{\deg}. As expected, the lowest gaze error was observed in the central FOV, with higher gaze errors in peripheral areas. The usability and learnability scores of the AVP, measured using the standard System Usability Scale (SUS), were 73 and 70, respectively. Importantly, no statistically reliable correlation between gaze error and usability scores was found. These results suggest that the eye-tracking accuracy of the AVP is comparable to other VR/AR headsets. While eye-tracking accuracy is critical for gaze-based interaction, it is not the sole determinant of user experience in AR/VR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00255v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zehao Huang, Gancheng Zhu, Xiaoting Duan, Rong Wang, Yongkai Li, Shuai Zhang, Zhiguo Wang</dc:creator>
    </item>
    <item>
      <title>The Odyssey Journey: Hemifacial Spasm Patients' Top-Tier Medical Resource Seeking in China from an Actor-Network Perspective</title>
      <link>https://arxiv.org/abs/2406.00337</link>
      <description>arXiv:2406.00337v1 Announce Type: new 
Abstract: Health information-seeking behaviors are critical for individuals managing illnesses, especially in cases like hemifacial spasm (HFS), a condition familiar to specialists but not to general practitioners and the broader public. The limited awareness of HFS often leads to scarce online resources for self-diagnosis and a heightened risk of misdiagnosis. In China, the imbalance in the doctor-to-patient ratio and HFS's low incidence exacerbate information and power asymmetries within doctor-patient relationship. While HCI and CSCW research predominantly focuses on more common chronic conditions, our study delves into HFS, aiming to deepen the understanding of HFS patients' health information-seeking journeys in China, as well as exploring how these patients utilize various stakeholders and online resources to overcome asymmetries in the doctor-patient relationship and access top-tier medical resources. Through interviews with three neurosurgeons and 12 HFS patients from both rural and urban areas, and applying Actor-Network Theory, we offer empirical insights into the interactions and workflows within the health information-seeking network. Our analysis identified five strategies HFS patients adopted to access top-tier medical resources. We also propose design opportunities for technology to aid patients in overcoming the challenges encountered during their health information-seeking journey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00337v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ka I Chan, Yuntao Wang, Siying Hu, Bo Hei, Zhicong Lu, Pei-Luen Patrick Rau, Yuanchun Shi</dc:creator>
    </item>
    <item>
      <title>Eery Space: Facilitating Virtual Meetings Through Remote Proxemics</title>
      <link>https://arxiv.org/abs/2406.00370</link>
      <description>arXiv:2406.00370v1 Announce Type: new 
Abstract: Virtual meetings have become increasingly common with modern video-conference and collaborative software. While they allow obvious savings in time and resources, current technologies add unproductive layers of protocol to the flow of communication between participants, rendering the interactions far from seamless. In this work we introduce Remote Proxemics, an extension of proxemics aimed at bringing the syntax of co-located proximal interactions to virtual meetings. We propose Eery Space, a shared virtual locus that results from merging multiple remote areas, where meeting participants' are located side-by-side as if they shared the same physical location. Eery Space promotes collaborative content creation and seamless mediation of communication channels based on virtual proximity. Results from user evaluation suggest that our approach is effective at enhancing mutual awareness between participants and sufficient to initiate proximal exchanges regardless of their geolocation, while promoting smooth interactions between local and remote people alike. These results happen even in the absence of visual avatars and other social devices such as eye contact, which are largely the focus of previous approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00370v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-319-22698-9_43</arxiv:DOI>
      <arxiv:journal_reference>INTERACT 2015. Lecture Notes in Computer Science(), vol 9298. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Maur\'icio Sousa, Daniel Mendes, Alfredo Ferreira, Jo\~ao Madeiras Pereira, Joaquim Jorge</dc:creator>
    </item>
    <item>
      <title>MI 2 MI: Training Dyad with Collaborative Brain-Computer Interface and Cooperative Motor Imagery Tasks for Better BCI Performance</title>
      <link>https://arxiv.org/abs/2406.00470</link>
      <description>arXiv:2406.00470v1 Announce Type: new 
Abstract: Collaborative brain-computer interface (cBCI) that conduct motor imagery (MI) among multiple users has the potential not only to improve overall BCI performance by integrating information from multiple users, but also to leverage individuals' performance in decision-making or control. However, existed research mostly focused on the brain signals changes through a single user, not noticing the possible interaction between users during the collaboration. In this work, we utilized cBCI and designed a cooperative four-classes MI task to train the dyad. A humanoid robot would stimulate the dyad to conduct both left/right hand and tongue/foot MI. Single user was asked to conduct single MI task before and after the cooperative MI task. The experiment results showed that our training could activate better performance (e.g., high quality of EEG /MI classification accuracy) for the single user than single MI task, and the single user also obtained better single MI performance after cooperative MI training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00470v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shiwei Cheng, Jialing Wang</dc:creator>
    </item>
    <item>
      <title>Exploring Child-Robot Interaction in Individual and Group settings in India</title>
      <link>https://arxiv.org/abs/2406.00724</link>
      <description>arXiv:2406.00724v1 Announce Type: new 
Abstract: This study evaluates the effectiveness of child-robot interactions with the HaKsh-E social robot in India, examining both individual and group interaction settings. The research centers on game-based interactions designed to teach hand hygiene to children aged 7-11. Utilizing video analysis, rubric assessments, and post-study questionnaires, the study gathered data from 36 participants. Findings indicate that children in both settings developed positive perceptions of the robot in terms of the robot's trustworthiness, closeness, and social support. The significant difference in the interaction level scores presented in the study suggests that group settings foster higher levels of interaction, potentially due to peer influence and collaborative dynamics. While both settings showed significant improvements in learning outcomes, the individual setting had more pronounced learning gains. This suggests that personal interactions with the robot might lead to deeper or more effective learning experiences. Consequently, this study concludes that individual interaction settings are more conducive for focused learning gains, while group settings enhance interaction and engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00724v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gayathri Manikutty, Sai Ankith Potapragada, Devasena Pasupuleti, Mahesh S. Unnithan, Arjun Venugopal, Pranav Prabha, Arunav H., Vyshnavi Anil Kumar, Rthuraj P. R., Rao R Bhavani</dc:creator>
    </item>
    <item>
      <title>Cheap and Easy Open-Ended Text Input for Interactive Emergent Narrative</title>
      <link>https://arxiv.org/abs/2406.00942</link>
      <description>arXiv:2406.00942v1 Announce Type: new 
Abstract: We present a demonstration of Play What I Mean (PWIM): a novel, AI-supported interaction technique for interactive emergent narrative (IEN) games and play experiences. By assisting players in translating high-level gameplay intents (expressed as short, unstructured text strings) into concrete game actions, PWIM aims to support open-ended player input while mitigating the overwhelm that players sometimes feel when confronting the large action spaces that characterize IEN gameplay. In matching player intents to game actions, PWIM makes use of an off-the-shelf sentence embedding model that is lightweight enough to run locally on a player's device, and wraps this model in a simple user interface that allows the player to work around occasional classification errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00942v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Max Kreminski</dc:creator>
    </item>
    <item>
      <title>Design of a High-Performance Tomographic Tactile Sensor by Manipulating the Detector Conductivity</title>
      <link>https://arxiv.org/abs/2406.00978</link>
      <description>arXiv:2406.00978v1 Announce Type: new 
Abstract: Recent advancements in soft robots, human-machine interfaces, and wearable electronics have led to an increased demand for high-performance soft tactile sensors. Tomographic tactile sensor based on resistive coupling is a novel contact pressure imaging method that allows the use of an arbitrary conductive material in a detector. However, the influence of material properties on the sensing performance remains unclear and the efficient and appropriate selection of materials is difficult. In this study, the relationship between the conductivity distribution of the material used as a detector and the sensing performance including sensitivity, force range, spatial resolution, and position accuracy is clarified to develop a high-performance tomographic tactile sensor. The performance maps reveal that a material with a conductivity of approximately 0.2 S/m can serve as an effective detector for touch interactions involving a force range of several Newtons. Additionally, incorporating gradient conductivity in the cross-section of the detector and multi-layer conductive porous media with anisotropic conductive bonding can help expand the design flexibility for enhanced performance. Based on these findings, various tomographic tactile sensors for soft grippers, tangible input interfaces, flexible touch displays, and wearable electronics are demonstrated by using a conductive porous media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00978v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TIE.2024.3384613</arxiv:DOI>
      <dc:creator>Shunsuke Yoshimoto, Koji Sakamoto, Rina Takeda, Akio Yamamoto</dc:creator>
    </item>
    <item>
      <title>Pseudo-Haptics Survey: Human-Computer Interaction in Extended Reality &amp; Teleoperation</title>
      <link>https://arxiv.org/abs/2406.01102</link>
      <description>arXiv:2406.01102v1 Announce Type: new 
Abstract: Pseudo-haptic techniques are becoming increasingly popular in human-computer interaction. They replicate haptic sensations by leveraging primarily visual feedback rather than mechanical actuators. These techniques bridge the gap between the real and virtual worlds by exploring the brain's ability to integrate visual and haptic information. One of the many advantages of pseudo-haptic techniques is that they are cost-effective, portable, and flexible. They eliminate the need for direct attachment of haptic devices to the body, which can be heavy and large and require a lot of power and maintenance. Recent research has focused on applying these techniques to extended reality and mid-air interactions. To better understand the potential of pseudo-haptic techniques, the authors developed a novel taxonomy encompassing tactile feedback, kinesthetic feedback, and combined categories in multimodal approaches, ground not covered by previous surveys. This survey highlights multimodal strategies and potential avenues for future studies, particularly regarding integrating these techniques into extended reality and collaborative virtual environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01102v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2024.3409449</arxiv:DOI>
      <arxiv:journal_reference>IEEE Access 2024 June 3</arxiv:journal_reference>
      <dc:creator>Rui Xavier, Jos\'e Lu\'is Silva, Rodrigo Ventura, Joaquim Jorge</dc:creator>
    </item>
    <item>
      <title>A Literature Review and Taxonomy of In-VR Questionnaire User Interfaces</title>
      <link>https://arxiv.org/abs/2406.01122</link>
      <description>arXiv:2406.01122v1 Announce Type: new 
Abstract: Previous research demonstrates that the interruption of immersive experiences may lead to a bias in the results of questionnaires. Thus, the traditional way of presenting questionnaires, paper-based or web-based, may not be compatible with evaluating VR experiences. Recent research has shown the positive impact of embedding questionnaires contextually into the virtual environment. However, a comprehensive overview of the available VR questionnaire solutions is currently missing. Furthermore, no clear taxonomy exists for these different solutions in the literature. To address this, we present a literature review of VR questionnaire user interfaces (UI) following PRISMA guidelines. Our search returned 1.109 initial results, which were screened for eligibility, resulting in a corpus of 25 papers. This paper contributes to HCI and games research with a literature review of embedded questionnaires in VR, discussing the advantages and disadvantages and introducing a taxonomy of in-VR questionnaire UIs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01122v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Saeed Safikhani, Lennart Nacke, Johanna Pirker</dc:creator>
    </item>
    <item>
      <title>The Application of Procedurally Generated Libraries in Immersive Virtual Reality</title>
      <link>https://arxiv.org/abs/2406.01128</link>
      <description>arXiv:2406.01128v1 Announce Type: new 
Abstract: While digital libraries offer essential benefits for the digital age such as constant availability and accessibility, they often fail in providing a similarly enjoyable browsing experience that benefits engagement and serendipitous exploration inherent to traditional library experiences. In this paper, we propose a virtual reality (VR) library environment that emulates a traditional library promoting an engaging browsing experience that benefits browsing enjoyment and serendipity. The implemented system uses procedural content generation to dynamically generate a library-like environment, wherein each room corresponds to a specific category of the digital library. To assess the usability of the implemented system, we conducted an A-B study comparing the VR environment with the web interface of Project Gutenberg. The results of the study suggest that while the physical demand for using the VR environment is higher, the system can significantly benefit the browsing experience and entice users curiosity. Furthermore, the study suggests that the system can increase user engagement and users generally felt that using the system was rewarding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01128v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Saeed Safikhani, Benedikt Gross, Johanna Pirker</dc:creator>
    </item>
    <item>
      <title>Transferring Domain Knowledge with (X)AI-Based Learning Systems</title>
      <link>https://arxiv.org/abs/2406.01329</link>
      <description>arXiv:2406.01329v1 Announce Type: new 
Abstract: In numerous high-stakes domains, training novices via conventional learning systems does not suffice. To impart tacit knowledge, experts' hands-on guidance is imperative. However, training novices by experts is costly and time-consuming, increasing the need for alternatives. Explainable artificial intelligence (XAI) has conventionally been used to make black-box artificial intelligence systems interpretable. In this work, we utilize XAI as an alternative: An (X)AI system is trained on experts' past decisions and is then employed to teach novices by providing examples coupled with explanations. In a study with 249 participants, we measure the effectiveness of such an approach for a classification task. We show that (X)AI-based learning systems are able to induce learning in novices and that their cognitive styles moderate learning. Thus, we take the first steps to reveal the impact of XAI on human learning and point AI developers to future options to tailor the design of (X)AI-based learning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01329v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Philipp Spitzer, Niklas K\"uhl, Marc Goutier, Manuel Kaschura, Gerhard Satzger</dc:creator>
    </item>
    <item>
      <title>Recover as It is Designed to Be: Recovering from Compatibility Mobile App Crashes by Reusing User Flows</title>
      <link>https://arxiv.org/abs/2406.01339</link>
      <description>arXiv:2406.01339v1 Announce Type: new 
Abstract: Android OS is severely fragmented by API updates and device vendors' OS customization, creating a market condition where vastly different OS versions coexist. This gives rise to compatibility crash problems where Android apps crash on certain Android versions but not on others. Although well-known, this problem is extremely challenging for app developers to overcome due to the sheer number of Android versions in the market that must be tested. We present RecoFlow, a framework for enabling app developers to automatically recover an app from a crash by programming user flows with our API and visual tools. RecoFlow tracks app feature usage with the user flows on user devices and recovers an app from a crash by replaying UI actions of the app feature disrupted by the crash. To prevent recurring compatibility crashes, RecoFlow executes a previously crashed app in compatibility mode that is enabled by our novel Android OS virtualization technique. Our evaluation with professional Android developers shows that our API and tools are easy to use and effective in recovering from compatibility crashes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01339v1</guid>
      <category>cs.HC</category>
      <category>cs.OS</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Donghwi Kim, Hyungjun Yoon, Chang Min Park, Sujin Han, Youngjin Kwon, Steven Y. Ko, Sung-Ju Lee</dc:creator>
    </item>
    <item>
      <title>Advancing Ear Biometrics: Enhancing Accuracy and Robustness through Deep Learning</title>
      <link>https://arxiv.org/abs/2406.00135</link>
      <description>arXiv:2406.00135v1 Announce Type: cross 
Abstract: Biometric identification is a reliable method to verify individuals based on their unique physical or behavioral traits, offering a secure alternative to traditional methods like passwords or PINs. This study focuses on ear biometric identification, exploiting its distinctive features for enhanced accuracy, reliability, and usability. While past studies typically investigate face recognition and fingerprint analysis, our research demonstrates the effectiveness of ear biometrics in overcoming limitations such as variations in facial expressions and lighting conditions. We utilized two datasets: AMI (700 images from 100 individuals) and EarNV1.0 (28,412 images from 164 individuals). To improve the accuracy and robustness of our ear biometric identification system, we applied various techniques including data preprocessing and augmentation. Our models achieved a testing accuracy of 99.35% on the AMI Dataset and 98.1% on the EarNV1.0 dataset, showcasing the effectiveness of our approach in precisely identifying individuals based on ear biometric characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00135v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youssef Mohamed, Zeyad Youssef, Ahmed Heakl, Ahmed Zaky</dc:creator>
    </item>
    <item>
      <title>Show, Don't Tell: Aligning Language Models with Demonstrated Feedback</title>
      <link>https://arxiv.org/abs/2406.00888</link>
      <description>arXiv:2406.00888v1 Announce Type: cross 
Abstract: Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number ($&lt;10$) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. We evaluate DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants ($N=16$). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00888v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Shaikh, Michelle Lam, Joey Hejna, Yijia Shao, Michael Bernstein, Diyi Yang</dc:creator>
    </item>
    <item>
      <title>Evaluating MEDIRL: A Replication and Ablation Study of Maximum Entropy Deep Inverse Reinforcement Learning for Human Social Navigation</title>
      <link>https://arxiv.org/abs/2406.00968</link>
      <description>arXiv:2406.00968v1 Announce Type: cross 
Abstract: In this study, we enhance the Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) framework, targeting its application in human robot interaction (HRI) for modeling pedestrian behavior in crowded environments. Our work is grounded in the pioneering research by Fahad, Chen, and Guo, and aims to elevate MEDIRL's efficacy in real world HRI settings. We replicated the original MEDIRL model and conducted detailed ablation studies, focusing on key model components like learning rates, state dimensions, and network layers. Our findings reveal the effectiveness of a two dimensional state representation over three dimensional approach, significantly improving model accuracy for pedestrian behavior prediction in HRI scenarios. These results not only demonstrate MEDIRL's enhanced performance but also offer valuable insights for future HRI system development, emphasizing the importance of model customization to specific environmental contexts. Our research contributes to advancing the field of socially intelligent navigation systems, promoting more intuitive and safer human robot interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00968v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vinay Gupta (Purdue University), Nihal Gunukula (Purdue University)</dc:creator>
    </item>
    <item>
      <title>Detection of Acetone as a Gas Biomarker for Diabetes Based on Gas Sensor Technology</title>
      <link>https://arxiv.org/abs/2406.00993</link>
      <description>arXiv:2406.00993v1 Announce Type: cross 
Abstract: With the continuous development and improvement of medical services, there is a growing demand for improving diabetes diagnosis. Exhaled breath analysis, characterized by its speed, convenience, and non-invasive nature, is leading the trend in diagnostic development. Studies have shown that the acetone levels in the breath of diabetes patients are higher than normal, making acetone a basis for diabetes breath analysis. This provides a more readily accepted method for early diabetes prevention and monitoring. Addressing issues such as the invasive nature, disease transmission risks, and complexity of diabetes testing, this study aims to design a diabetes gas biomarker acetone detection system centered around a sensor array using gas sensors and pattern recognition algorithms. The research covers sensor selection, sensor preparation, circuit design, data acquisition and processing, and detection model establishment to accurately identify acetone. Titanium dioxide was chosen as the nano gas-sensitive material to prepare the acetone gas sensor, with data collection conducted using STM32. Filtering was applied to process the raw sensor data, followed by feature extraction using principal component analysis. A recognition model based on support vector machine algorithm was used for qualitative identification of gas samples, while a recognition model based on backpropagation neural network was employed for quantitative detection of gas sample concentrations. Experimental results demonstrated recognition accuracies of 96% and 97.5% for acetone-ethanol and acetone-methanol mixed gases, and 90% for ternary acetone, ethanol, and methanol mixed gases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00993v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Wei, Tong Liu, Jipeng Huang, Xiaowei Li, Yurui Qi, Gangyin Luo</dc:creator>
    </item>
    <item>
      <title>Virtual avatar generation models as world navigators</title>
      <link>https://arxiv.org/abs/2406.01056</link>
      <description>arXiv:2406.01056v1 Announce Type: cross 
Abstract: We introduce SABR-CLIMB, a novel video model simulating human movement in rock climbing environments using a virtual avatar. Our diffusion transformer predicts the sample instead of noise in each diffusion step and ingests entire videos to output complete motion sequences. By leveraging a large proprietary dataset, NAV-22M, and substantial computational resources, we showcase a proof of concept for a system to train general-purpose virtual avatars for complex tasks in robotics, sports, and healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01056v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sai Mandava</dc:creator>
    </item>
    <item>
      <title>How Ethical Should AI Be? How AI Alignment Shapes the Risk Preferences of LLMs</title>
      <link>https://arxiv.org/abs/2406.01168</link>
      <description>arXiv:2406.01168v1 Announce Type: cross 
Abstract: This study explores the risk preferences of Large Language Models (LLMs) and how the process of aligning them with human ethical standards influences their economic decision-making. By analyzing 30 LLMs, we uncover a broad range of inherent risk profiles ranging from risk-averse to risk-seeking. We then explore how different types of AI alignment, a process that ensures models act according to human values and that focuses on harmlessness, helpfulness, and honesty, alter these base risk preferences. Alignment significantly shifts LLMs towards risk aversion, with models that incorporate all three ethical dimensions exhibiting the most conservative investment behavior. Replicating a prior study that used LLMs to predict corporate investments from company earnings call transcripts, we demonstrate that although some alignment can improve the accuracy of investment forecasts, excessive alignment results in overly cautious predictions. These findings suggest that deploying excessively aligned LLMs in financial decision-making could lead to severe underinvestment. We underline the need for a nuanced approach that carefully balances the degree of ethical alignment with the specific requirements of economic domains when leveraging LLMs within finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01168v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shumiao Ouyang, Hayong Yun, Xingjian Zheng</dc:creator>
    </item>
    <item>
      <title>Extraction of Maternal and fetal ECG in a non-invasive way from abdominal ECG recordings using modified Progressive FastICA Peel-off</title>
      <link>https://arxiv.org/abs/2406.01281</link>
      <description>arXiv:2406.01281v1 Announce Type: cross 
Abstract: The non-invasive abdominal electrocardiogram (AECG) gives a non-invasive way to monitor fetal well-being during pregnancy. Due to the overlap with maternal ECG (MECG) as well as potential noises from other sources, it is challenging to extract weak fetal ECG (FECG) using surface electrodes. Taking advantage of precise source separation capability of the FastICA approach combined with its constrained version specific to FECG, with weak source extraction capability warranted by the peel-off strategy and FECG waveform reconstruction ability ensured by singular value decomposition (SVD) method, a novel framework for FECG extraction from AECG recordings is presented in this paper. Specifically, a periodic constrained FastICA(pcFastICA) was developed to improve the precision of examining and correcting FECG source signals, based on the statistical characteristics of continuous and repetitive ECG emissions. Additionally, a successive judgement algorithm is designed to selected the optimal maternal and fetal ECG. The performance of the proposed method was examined on public datasets, synthetic data and clinical data, with an F1-scores for FECG extraction on ADFECG and NIFECGA dataset of 99.71% and 99.36%, on synthetic data with the highest noise level of 98.77%, on clinical data of 98.09%, which are all superior to other comparative methods. The results indicates that our proposed method has potential and effectiveness to separate weak FECG from multichannel AECG with high precision in high noise condition, which is of vital importance for ensuring the safety of both the fetus and the mother, as well as the advancement of artificial intelligent clinical monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01281v1</guid>
      <category>physics.med-ph</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Li (School of Microelectronics at University of Science and Technology of China, Hefei, Anhui, China), Xuanyu Luo (School of Microelectronics at University of Science and Technology of China, Hefei, Anhui, China), Haowen Zhao (School of Microelectronics at University of Science and Technology of China, Hefei, Anhui, China), Jiawen Cui (School of Microelectronics at University of Science and Technology of China, Hefei, Anhui, China), Yangfan She (School of Microelectronics at University of Science and Technology of China, Hefei, Anhui, China), Dongfang Li (School of Microelectronics at University of Science and Technology of China, Hefei, Anhui, China), Lai Jiang (The first affiliated hospital of University of Science and Technology of China, Hefei, Anhui, China), Xu Zhang (School of Microelectronics at University of Science and Technology of China, Hefei, Anhui, China)</dc:creator>
    </item>
    <item>
      <title>Extraction of Weak Surface Diaphragmatic Electromyogram Using Modified Progressive FastICA Peel-Off</title>
      <link>https://arxiv.org/abs/2406.01284</link>
      <description>arXiv:2406.01284v1 Announce Type: cross 
Abstract: Diaphragmatic electromyogram (EMGdi) is a crucial electrophysiological signal that contains information about human respiration. Although it is practical to record surface EMGdi (sEMGdi) noninvasively and conveniently by placing electrodes over chest skin, extraction of such weak sEMGdi from noise condition is a challenging task, limiting its clinical use compared with esophageal EMGdi. In this paper, a novel method is presented for extracting weak sEMGdi signal from high-noise condition based on fast independent component analysis (FastICA), constrained FastICA and a peel-off strategy. The constrained FastICA helps to extract and refine respiration-related sEMGdi signals, and the peel-off strategy ensures the complete extraction of weaker sEMGdi components. The method was validated using both synthetic signals and clinical signals. It was demonstrated that our method was able to extract sEMGdi signals efficiently with little distortion. It outperformed state-of-the-art comparison methods in terms of sufficiently high SIR and CORR at all noise levels when tested on synthetic data. Our method also achieved an accuracy of 95.06% and a F2-score of 96.73% for breath identification on clinical data. The study presents a valuable solution for noninvasive extraction of sEMGdi signals, providing a convenient and valuable way of ventilator synchrony. Our study also offers a significant potential in aiding respiratory rehabilitation and health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01284v1</guid>
      <category>physics.med-ph</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Li (School of Microelectronics at University of Science and Technology of China, Hefei, Anhui, China), Dongsheng Zhao (the First Affiliated Hospital of Anhui Medical University, Hefei, Anhui, China), Haowen Zhao (School of Microelectronics at University of Science and Technology of China, Hefei, Anhui, China), Xu Zhang (School of Microelectronics at University of Science and Technology of China, Hefei, Anhui, China), Min Shao (the First Affiliated Hospital of Anhui Medical University, Hefei, Anhui, China)</dc:creator>
    </item>
    <item>
      <title>Evidence for five types of fixation during a random saccade eye tracking task: Implications for the study of oculomotor fatigue</title>
      <link>https://arxiv.org/abs/2406.01496</link>
      <description>arXiv:2406.01496v1 Announce Type: cross 
Abstract: Our interest was to evaluate changes in fixation duration as a function of time-on-task (TOT) during a random saccade task. We employed a large, publicly available dataset. The frequency histogram of fixation durations was multimodal and modelled as a Gaussian mixture. We found five fixation types. The ``ideal'' response would be a single accurate saccade after each target movement, with a typical saccade latency of 200-250 msec, followed by a long fixation (&gt; 800 msec) until the next target jump. We found fixations like this, but they comprised only 10% of all fixations and were the first fixation after target movement only 23.4% of the time. More frequently (57.4% of the time), the first fixation after target movement was short (117.7 msec mean) and was commonly followed by a corrective saccade. Across the entire 100 sec of the task, median total fixation duration decreased. This decrease was approximated with a power law fit with R^2=0.94. A detailed examination of the frequency of each of our five fixation types over time on task (TOT) revealed that the three shortest duration fixation types became more and more frequent with TOT whereas the two longest fixations became less and less frequent. In all cases, the changes over TOT followed power law relationships, with R^2 values between 0.73 and 0.93. We concluded that, over the 100 second duration of our task, long fixations are common in the first 15 to 22 seconds but become less common after that. Short fixations are relatively uncommon in the first 15 to 22 seconds but become more and more common as the task progressed. Apparently. the ability to produce an ideal response, although somewhat likely in the first 22 seconds, rapidly declines. This might be related to a noted decline in saccade accuracy over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01496v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lee Friedman, Oleg V. Komogortsev</dc:creator>
    </item>
    <item>
      <title>Exploring the Landscape of Ubiquitous In-home Health Monitoring: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2306.12660</link>
      <description>arXiv:2306.12660v2 Announce Type: replace 
Abstract: Ubiquitous in-home health monitoring systems have become popular in recent years due to the rise of digital health technologies and the growing demand for remote health monitoring. These systems enable individuals to increase their independence by allowing them to monitor their health from the home and by allowing more control over their well-being. In this study, we perform a comprehensive survey on this topic by reviewing a large number of literature in the area. We investigate these systems from various aspects, namely sensing technologies, communication technologies, intelligent and computing systems, and application areas. Specifically, we provide an overview of in-home health monitoring systems and identify their main components. We then present each component and discuss its role within in-home health monitoring systems. In addition, we provide an overview of the practical use of ubiquitous technologies in the home for health monitoring. Finally, we identify the main challenges and limitations based on the existing literature and provide eight recommendations for potential future research directions toward the development of in-home health monitoring systems. We conclude that despite extensive research on various components needed for the development of effective in-home health monitoring systems, the development of effective in-home health monitoring systems still requires further investigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.12660v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farhad Pourpanah, Ali Etemad</dc:creator>
    </item>
    <item>
      <title>ID.8: Co-Creating Visual Stories with Generative AI</title>
      <link>https://arxiv.org/abs/2309.14228</link>
      <description>arXiv:2309.14228v2 Announce Type: replace 
Abstract: Storytelling is an integral part of human culture and significantly impacts cognitive and socio-emotional development and connection. Despite the importance of interactive visual storytelling, the process of creating such content requires specialized skills and is labor-intensive. This paper introduces ID.8, an open-source system designed for the co-creation of visual stories with generative AI. We focus on enabling an inclusive storytelling experience by simplifying the content creation process and allowing for customization. Our user evaluation confirms a generally positive user experience in domains such as enjoyment and exploration, while highlighting areas for improvement, particularly in immersiveness, alignment, and partnership between the user and the AI system. Overall, our findings indicate promising possibilities for empowering people to create visual stories with generative AI. This work contributes a novel content authoring system, ID.8, and insights into the challenges and potential of using generative AI for multimedia content creation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14228v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Nikhil Antony, Chien-Ming Huang</dc:creator>
    </item>
    <item>
      <title>Evolving Agents: Interactive Simulation of Dynamic and Diverse Human Personalities</title>
      <link>https://arxiv.org/abs/2404.02718</link>
      <description>arXiv:2404.02718v2 Announce Type: replace 
Abstract: Human-like Agents with diverse and dynamic personality could serve as an important design probe in the process of user-centered design, thereby enabling designers to enhance the user experience of interactive application.In this article, we introduce Evolving Agents, a novel agent architecture that consists of two systems: Personality and Behavior. The Personality system includes three modules: Cognition, Emotion and Character Growth. The Behavior system comprises two modules: Planning and Action. We also build a simulation platform that enables agents to interact with the environment and other agents. Evolving Agents can simulate the human personality evolution process. Compared to its initial state, agents' personality and behavior patterns undergo believable development after several days of simulation. Agents reflect on their behavior to reason and develop new personality traits. These traits, in turn, generate new behavior patterns, forming a feedback loop-like personality evolution.In our experiment, we utilized simulation platform with 10 agents for evaluation. During the evaluation, these agents experienced believable and inspirational personality evolution. Through ablation and control experiments, we demonstrated the outstanding effectiveness of agent personality evolution and all modules of our agent architecture contribute to creating believable human-like agents with diverse and dynamic personalities. We also demonstrated through workshops how Evolving Agents could inspire designers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02718v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiale Li, Jiayang Li, Jiahao Chen, Yifan Li, Shijie Wang, Hugo Zhou, Minjun Ye, Yunsheng Su</dc:creator>
    </item>
    <item>
      <title>Eye-tracking in Mixed Reality for Diagnosis of Neurodegenerative Diseases</title>
      <link>https://arxiv.org/abs/2404.12984</link>
      <description>arXiv:2404.12984v2 Announce Type: replace 
Abstract: Parkinson's disease ranks as the second most prevalent neurodegenerative disorder globally. This research aims to develop a system leveraging Mixed Reality capabilities for tracking and assessing eye movements. In this paper, we present a medical scenario and outline the development of an application designed to capture eye-tracking signals through Mixed Reality technology for the evaluation of neurodegenerative diseases. Additionally, we introduce a pipeline for extracting clinically relevant features from eye-gaze analysis, describing the capabilities of the proposed system from a medical perspective. The study involved a cohort of healthy control individuals and patients suffering from Parkinson's disease, showcasing the feasibility and potential of the proposed technology for non-intrusive monitoring of eye movement patterns for the diagnosis of neurodegenerative diseases.
  Clinical relevance - Developing a non-invasive biomarker for Parkinson's disease is urgently needed to accurately detect the disease's onset. This would allow for the timely introduction of neuroprotective treatment at the earliest stage and enable the continuous monitoring of intervention outcomes. The ability to detect subtle changes in eye movements allows for early diagnosis, offering a critical window for intervention before more pronounced symptoms emerge. Eye tracking provides objective and quantifiable biomarkers, ensuring reliable assessments of disease progression and cognitive function. The eye gaze analysis using Mixed Reality glasses is wireless, facilitating convenient assessments in both home and hospital settings. The approach offers the advantage of utilizing hardware that requires no additional specialized attachments, enabling examinations through personal eyewear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12984v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mateusz Daniol, Daria Hemmerling, Jakub Sikora, Pawel Jemiolo, Marek Wodzinski, Magdalena Wojcik-Pedziwiatr</dc:creator>
    </item>
    <item>
      <title>Tangible Scenography as a Holistic Design Method for Human-Robot Interaction</title>
      <link>https://arxiv.org/abs/2405.19449</link>
      <description>arXiv:2405.19449v2 Announce Type: replace 
Abstract: Traditional approaches to human-robot interaction design typically examine robot behaviors in controlled environments and narrow tasks. These methods are impractical for designing robots that interact with diverse user groups in complex human environments. Drawing from the field of theater, we present the construct of scenes -- individual environments consisting of specific people, objects, spatial arrangements, and social norms -- and tangible scenography, as a holistic design approach for human-robot interactions. We created a design tool, Tangible Scenography Kit (TaSK), with physical props to aid in design brainstorming. We conducted design sessions with eight professional designers to generate exploratory designs. Designers used tangible scenography and TaSK components to create multiple scenes with specific interaction goals, characterize each scene's social environment, and design scene-specific robot behaviors. From these sessions, we found that this method can encourage designers to think beyond a robot's narrow capabilities and consider how they can facilitate complex social interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19449v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3643834.3661530</arxiv:DOI>
      <dc:creator>Amy Koike, Bengisu Cagiltay, Bilge Mutlu</dc:creator>
    </item>
    <item>
      <title>Leveraging Expert Consistency to Improve Algorithmic Decision Support</title>
      <link>https://arxiv.org/abs/2101.09648</link>
      <description>arXiv:2101.09648v3 Announce Type: replace-cross 
Abstract: Machine learning (ML) is increasingly being used to support high-stakes decisions. However, there is frequently a construct gap: a gap between the construct of interest to the decision-making task and what is captured in proxies used as labels to train ML models. As a result, ML models may fail to capture important dimensions of decision criteria, hampering their utility for decision support. Thus, an essential step in the design of ML systems for decision support is selecting a target label among available proxies. In this work, we explore the use of historical expert decisions as a rich -- yet also imperfect -- source of information that can be combined with observed outcomes to narrow the construct gap. We argue that managers and system designers may be interested in learning from experts in instances where they exhibit consistency with each other, while learning from observed outcomes otherwise. We develop a methodology to enable this goal using information that is commonly available in organizational information systems. This involves two core steps. First, we propose an influence function-based methodology to estimate expert consistency indirectly when each case in the data is assessed by a single expert. Second, we introduce a label amalgamation approach that allows ML models to simultaneously learn from expert decisions and observed outcomes. Our empirical evaluation, using simulations in a clinical setting and real-world data from the child welfare domain, indicates that the proposed approach successfully narrows the construct gap, yielding better predictive performance than learning from either observed outcomes or expert decisions alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.09648v3</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria De-Arteaga, Vincent Jeanselme, Artur Dubrawski, Alexandra Chouldechova</dc:creator>
    </item>
    <item>
      <title>Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining</title>
      <link>https://arxiv.org/abs/2307.03887</link>
      <description>arXiv:2307.03887v3 Announce Type: replace-cross 
Abstract: In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the Prototypical Part Network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this architecture is able to produce visually interpretable classifications, it often learns to classify based on parts of the image that are not semantically meaningful. To address this problem, we propose the Reward Reweighing, Reselecting, and Retraining (R3) post-processing framework, which performs three additional corrective updates to a pretrained ProtoPNet in an offline and efficient manner. The first two steps involve learning a reward model based on collected human feedback and then aligning the prototypes with human preferences. The final step is retraining, which realigns the base features and the classifier layer of the original model with the updated prototypes. We find that our R3 framework consistently improves both the interpretability and the predictive accuracy of ProtoPNet and its variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03887v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron J. Li, Robin Netzorg, Zhihan Cheng, Zhuoqin Zhang, Bin Yu</dc:creator>
    </item>
    <item>
      <title>Learning to Defer in Content Moderation: The Human-AI Interplay</title>
      <link>https://arxiv.org/abs/2402.12237</link>
      <description>arXiv:2402.12237v3 Announce Type: replace-cross 
Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).
  In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to capture this human-AI interplay is via the framework of learning to defer, where the algorithm has the option to defer a classification task to humans for a fixed cost and immediately receive feedback. Our model contributes to this literature by introducing congestion in the human review system. Moreover, unlike work on online learning with delayed feedback where the delay in the feedback is exogenous to the algorithm's decisions, the delay in our model is endogenous to both the admission and the scheduling decisions.
  We propose a near-optimal learning algorithm that carefully balances the classification loss from a selectively sampled dataset, the idiosyncratic loss of non-reviewed posts, and the delay loss of having congestion in the human review system. To the best of our knowledge, this is the first result for online learning in contextual queueing systems and hence our analytical framework may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12237v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thodoris Lykouris, Wentao Weng</dc:creator>
    </item>
    <item>
      <title>An Iterative Associative Memory Model for Empathetic Response Generation</title>
      <link>https://arxiv.org/abs/2402.17959</link>
      <description>arXiv:2402.17959v2 Announce Type: replace-cross 
Abstract: Empathetic response generation aims to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17959v2</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhou Yang, Zhaochun Ren, Yufeng Wang, Chao Chen, Haizhou Sun, Xiaofei Zhu, Xiangwen Liao</dc:creator>
    </item>
    <item>
      <title>Embedding Privacy in Computational Social Science and Artificial Intelligence Research</title>
      <link>https://arxiv.org/abs/2404.11515</link>
      <description>arXiv:2404.11515v2 Announce Type: replace-cross 
Abstract: Privacy is a human right. It ensures that individuals are free to engage in discussions, participate in groups, and form relationships online or offline without fear of their data being inappropriately harvested, analyzed, or otherwise used to harm them. Preserving privacy has emerged as a critical factor in research, particularly in the computational social science (CSS), artificial intelligence (AI) and data science domains, given their reliance on individuals' data for novel insights. The increasing use of advanced computational models stands to exacerbate privacy concerns because, if inappropriately used, they can quickly infringe privacy rights and lead to adverse effects for individuals -- especially vulnerable groups -- and society. We have already witnessed a host of privacy issues emerge with the advent of large language models (LLMs), such as ChatGPT, which further demonstrate the importance of embedding privacy from the start. This article contributes to the field by discussing the role of privacy and the issues that researchers working in CSS, AI, data science and related domains are likely to face. It then presents several key considerations for researchers to ensure participant privacy is best preserved in their research design, data collection and use, analysis, and dissemination of research results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11515v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.36190/2024.18</arxiv:DOI>
      <dc:creator>Keenan Jones, Fatima Zahrah, Jason R. C. Nurse</dc:creator>
    </item>
    <item>
      <title>ARAIDA: Analogical Reasoning-Augmented Interactive Data Annotation</title>
      <link>https://arxiv.org/abs/2405.11912</link>
      <description>arXiv:2405.11912v2 Announce Type: replace-cross 
Abstract: Human annotation is a time-consuming task that requires a significant amount of effort. To address this issue, interactive data annotation utilizes an annotation model to provide suggestions for humans to approve or correct. However, annotation models trained with limited labeled data are prone to generating incorrect suggestions, leading to extra human correction effort. To tackle this challenge, we propose Araida, an analogical reasoning-based approach that enhances automatic annotation accuracy in the interactive data annotation setting and reduces the need for human corrections. Araida involves an error-aware integration strategy that dynamically coordinates an annotation model and a k-nearest neighbors (KNN) model, giving more importance to KNN's predictions when predictions from the annotation model are deemed inaccurate. Empirical studies demonstrate that Araida is adaptable to different annotation tasks and models. On average, it reduces human correction labor by 11.02% compared to vanilla interactive data annotation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11912v2</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Huang, Yiping Jin, Ilija Ilievski, Wenqiang Lei, Jiancheng Lv</dc:creator>
    </item>
    <item>
      <title>Human-Agent Cooperation in Games under Incomplete Information through Natural Language Communication</title>
      <link>https://arxiv.org/abs/2405.14173</link>
      <description>arXiv:2405.14173v3 Announce Type: replace-cross 
Abstract: Developing autonomous agents that can strategize and cooperate with humans under information asymmetry is challenging without effective communication in natural language. We introduce a shared-control game, where two players collectively control a token in alternating turns to achieve a common objective under incomplete information. We formulate a policy synthesis problem for an autonomous agent in this game with a human as the other player. To solve this problem, we propose a communication-based approach comprising a language module and a planning module. The language module translates natural language messages into and from a finite set of flags, a compact representation defined to capture player intents. The planning module leverages these flags to compute a policy using an asymmetric information-set Monte Carlo tree search with flag exchange algorithm we present. We evaluate the effectiveness of this approach in a testbed based on Gnomes at Night, a search-and-find maze board game. Results of human subject experiments show that communication narrows the information gap between players and enhances human-agent cooperation efficiency with fewer turns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14173v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shenghui Chen, Daniel Fried, Ufuk Topcu</dc:creator>
    </item>
    <item>
      <title>Product Design Using Generative Adversarial Network: Incorporating Consumer Preference and External Data</title>
      <link>https://arxiv.org/abs/2405.15929</link>
      <description>arXiv:2405.15929v2 Announce Type: replace-cross 
Abstract: The development of generative artificial intelligence (AI) enables large-scale product design automation. However, this automated process usually does not incorporate consumer preference information from the internal dataset of a company. Furthermore, external sources such as social media and user-generated content (UGC) websites often contain rich product design and consumer preference information, but such information is not utilized by companies when generating designs. We propose a semi-supervised deep generative framework that integrates consumer preferences and external data into the product design process, allowing companies to generate consumer-preferred designs in a cost-effective and scalable way. We train a predictor model to learn consumer preferences and use predicted popularity levels as additional input labels to guide the training procedure of a continuous conditional generative adversarial network (CcGAN). The CcGAN can be instructed to generate new designs with a certain popularity level, enabling companies to efficiently create consumer-preferred designs and save resources by avoiding the development and testing of unpopular designs. The framework also incorporates existing product designs and consumer preference information from external sources, which is particularly helpful for small or start-up companies that have limited internal data and face the "cold-start" problem. We apply the proposed framework to a real business setting by helping a large self-aided photography chain in China design new photo templates. We show that our proposed model performs well in terms of generating appealing template designs for the company.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15929v2</guid>
      <category>econ.GN</category>
      <category>cs.HC</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hui Li, Jian Ni, Fangzhu Yang</dc:creator>
    </item>
    <item>
      <title>Stratified Avatar Generation from Sparse Observations</title>
      <link>https://arxiv.org/abs/2405.20786</link>
      <description>arXiv:2405.20786v2 Announce Type: replace-cross 
Abstract: Estimating 3D full-body avatars from AR/VR devices is essential for creating immersive experiences in AR/VR applications. This task is challenging due to the limited input from Head Mounted Devices, which capture only sparse observations from the head and hands. Predicting the full-body avatars, particularly the lower body, from these sparse observations presents significant difficulties. In this paper, we are inspired by the inherent property of the kinematic tree defined in the Skinned Multi-Person Linear (SMPL) model, where the upper body and lower body share only one common ancestor node, bringing the potential of decoupled reconstruction. We propose a stratified approach to decouple the conventional full-body avatar reconstruction pipeline into two stages, with the reconstruction of the upper body first and a subsequent reconstruction of the lower body conditioned on the previous stage. To implement this straightforward idea, we leverage the latent diffusion model as a powerful probabilistic generator, and train it to follow the latent distribution of decoupled motions explored by a VQ-VAE encoder-decoder model. Extensive experiments on AMASS mocap dataset demonstrate our state-of-the-art performance in the reconstruction of full-body motions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20786v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Feng, Wenchao Ma, Quankai Gao, Xianwei Zheng, Nan Xue, Huijuan Xu</dc:creator>
    </item>
  </channel>
</rss>
