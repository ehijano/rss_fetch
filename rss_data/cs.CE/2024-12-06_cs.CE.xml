<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Dec 2024 05:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>ParetoFlow: Guided Flows in Multi-Objective Optimization</title>
      <link>https://arxiv.org/abs/2412.03718</link>
      <description>arXiv:2412.03718v1 Announce Type: new 
Abstract: In offline multi-objective optimization (MOO), we leverage an offline dataset of designs and their associated labels to simultaneously minimize multiple objectives. This setting more closely mirrors complex real-world problems compared to single-objective optimization. Recent works mainly employ evolutionary algorithms and Bayesian optimization, with limited attention given to the generative modeling capabilities inherent in such data. In this study, we explore generative modeling in offline MOO through flow matching, noted for its effectiveness and efficiency. We introduce ParetoFlow, specifically designed to guide flow sampling to approximate the Pareto front. Traditional predictor (classifier) guidance is inadequate for this purpose because it models only a single objective. In response, we propose a multi-objective predictor guidance module that assigns each sample a weight vector, representing a weighted distribution across multiple objective predictions. A local filtering scheme is introduced to address non-convex Pareto fronts. These weights uniformly cover the entire objective space, effectively directing sample generation towards the Pareto front. Since distributions with similar weights tend to generate similar samples, we introduce a neighboring evolution module to foster knowledge sharing among neighboring distributions. This module generates offspring from these distributions, and selects the most promising one for the next iteration. Our method achieves state-of-the-art performance across various tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03718v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ye Yuan, Can Chen, Christopher Pal, Xue Liu</dc:creator>
    </item>
    <item>
      <title>GenChaR: A Dataset for Stock Chart Captioning</title>
      <link>https://arxiv.org/abs/2412.04041</link>
      <description>arXiv:2412.04041v1 Announce Type: new 
Abstract: In this work, we introduce a new dataset GenChaR for an image captioning task around stock charts. The task aims to read market sentiment directly from depicted charts and generate descriptions, hopefully to provide comprehensible and useful insights for stock trading. Impressed by the success of large language models (LLMs), the study decides to pioneer itself by exploring the capabilities of large vision-language models (LVLMs) on the proposed task. This paper outlines the objectives of the stock captioning task, the dataset we built, and automatic evaluation with some representative general-purpose LVLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04041v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Le Qiu, Emmanuele Chersoni</dc:creator>
    </item>
    <item>
      <title>A Phase-Field-Micromechanics Study on the Microstructural Evolution during Viscous Sintering</title>
      <link>https://arxiv.org/abs/2412.04050</link>
      <description>arXiv:2412.04050v1 Announce Type: new 
Abstract: In the manufacturing process of high-performance particulate materials, viscous sintering plays a crucial role, particularly in fields such as polymer processing and additive manufacturing. The interactions between microscopic particles, their flow behavior, and the evolution of porosity during the viscous sintering process directly influence the material's density and mechanical properties. Therefore, developing efficient modeling techniques to simulate the viscous sintering process is essential for optimizing sintering technology. However, the large deformations and dynamic surface evolution inherent in the viscous sintering of particulate materials present challenges to traditional methods based on the sharp interface model. To address these challenges, we propose a thermodynamically consistent diffusion interface model, referred to as the phase-field-micromechanics model, to analyze the evolution of various physical quantities throughout the viscous sintering process. This model implicitly describes the evolution of particle morphology through an introduced phase-field variable. Through comparisons with analytical solutions and experimental data, we rigorously validate the correctness of the proposed model qualitatively and quantitatively under both isothermal and non-isothermal conditions. Using the proposed model, we explore the development of strain and stress during the sintering process, as well as the effects of particle size, shape and arrangement on the overall sintering behavior. The evolution of these characteristic indicators allows for a clear observation of the viscous sintering process, which is vital for understanding the mechanisms behind viscous sintering and for guiding industrial production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04050v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoxu Dai, Bo Qian, Arkadz Kirshtein, Qingcheng Yang</dc:creator>
    </item>
    <item>
      <title>Reinforced Symbolic Learning with Logical Constraints for Predicting Turbine Blade Fatigue Life</title>
      <link>https://arxiv.org/abs/2412.03580</link>
      <description>arXiv:2412.03580v1 Announce Type: cross 
Abstract: Accurate prediction of turbine blade fatigue life is essential for ensuring the safety and reliability of aircraft engines. A significant challenge in this domain is uncovering the intrinsic relationship between mechanical properties and fatigue life. This paper introduces Reinforced Symbolic Learning (RSL), a method that derives predictive formulas linking these properties to fatigue life. RSL incorporates logical constraints during symbolic optimization, ensuring that the generated formulas are both physically meaningful and interpretable. The optimization process is further enhanced using deep reinforcement learning, which efficiently guides the symbolic regression towards more accurate models. The proposed RSL method was evaluated on two turbine blade materials, GH4169 and TC4, to identify optimal fatigue life prediction models. When compared with six empirical formulas and five machine learning algorithms, RSL not only produces more interpretable formulas but also achieves superior or comparable predictive accuracy. Additionally, finite element simulations were conducted to assess mechanical properties at critical points on the blade, which were then used to predict fatigue life under various operating conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03580v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pei Li, Joo-Ho Choi, Dingyang Zhang, Shuyou Zhang, Yiming Zhang</dc:creator>
    </item>
    <item>
      <title>Transferring self-supervised pre-trained models for SHM data anomaly detection with scarce labeled data</title>
      <link>https://arxiv.org/abs/2412.03880</link>
      <description>arXiv:2412.03880v1 Announce Type: cross 
Abstract: Structural health monitoring (SHM) has experienced significant advancements in recent decades, accumulating massive monitoring data. Data anomalies inevitably exist in monitoring data, posing significant challenges to their effective utilization. Recently, deep learning has emerged as an efficient and effective approach for anomaly detection in bridge SHM. Despite its progress, many deep learning models require large amounts of labeled data for training. The process of labeling data, however, is labor-intensive, time-consuming, and often impractical for large-scale SHM datasets. To address these challenges, this work explores the use of self-supervised learning (SSL), an emerging paradigm that combines unsupervised pre-training and supervised fine-tuning. The SSL-based framework aims to learn from only a very small quantity of labeled data by fine-tuning, while making the best use of the vast amount of unlabeled SHM data by pre-training. Mainstream SSL methods are compared and validated on the SHM data of two in-service bridges. Comparative analysis demonstrates that SSL techniques boost data anomaly detection performance, achieving increased F1 scores compared to conventional supervised training, especially given a very limited amount of labeled data. This work manifests the effectiveness and superiority of SSL techniques on large-scale SHM data, providing an efficient tool for preliminary anomaly detection with scarce label information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03880v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingyuan Zhou, Xudong Jian, Ye Xia, Zhilu Lai</dc:creator>
    </item>
    <item>
      <title>A robust quantum nonlinear solver based on the asymptotic numerical method</title>
      <link>https://arxiv.org/abs/2412.03939</link>
      <description>arXiv:2412.03939v1 Announce Type: cross 
Abstract: Quantum computing offers a promising new avenue for advancing computational methods in science and engineering. In this work, we introduce the quantum asymptotic numerical method, a novel quantum nonlinear solver that combines Taylor series expansions with quantum linear solvers to efficiently address nonlinear problems. By linearizing nonlinear problems using the Taylor series, the method transforms them into sequences of linear equations solvable by quantum algorithms, thus extending the convergence region for solutions and simultaneously leveraging quantum computational advantages. Numerical tests on the quantum simulator Qiskit confirm the convergence and accuracy of the method in solving nonlinear problems. Additionally, we apply the proposed method to a beam buckling problem, demonstrating its robustness in handling strongly nonlinear problems and its potential advantages in quantum resource requirements. Furthermore, we perform experiments on a superconducting quantum processor from Quafu, successfully achieving up to 98% accuracy in the obtained nonlinear solution path. We believe this work contributes to the utility of quantum computing in scientific computing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03939v1</guid>
      <category>quant-ph</category>
      <category>cs.CE</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongchun Xu, Zengtao Kuang, Huang Qun, Yang Jie, Hamid Zahrouni, Michel Potier-Ferry, Huang Kaixuan, Zhang Jia-Chi, Fan Heng, Hu Heng</dc:creator>
    </item>
    <item>
      <title>DeepFEA: Deep Learning for Prediction of Transient Finite Element Analysis Solutions</title>
      <link>https://arxiv.org/abs/2412.04121</link>
      <description>arXiv:2412.04121v1 Announce Type: cross 
Abstract: Finite Element Analysis (FEA) is a powerful but computationally intensive method for simulating physical phenomena. Recent advancements in machine learning have led to surrogate models capable of accelerating FEA. Yet there are still limitations in developing surrogates of transient FEA models that can simultaneously predict the solutions for both nodes and elements with applicability on both the 2D and 3D domains. Motivated by this research gap, this study proposes DeepFEA, a deep learning-based framework that leverages a multilayer Convolutional Long Short-Term Memory (ConvLSTM) network branching into two parallel convolutional neural networks to predict the solutions for both nodes and elements of FEA models. The proposed network is optimized using a novel adaptive learning algorithm, called Node-Element Loss Optimization (NELO). NELO minimizes the error occurring at both branches of the network enabling the prediction of solutions for transient FEA simulations. The experimental evaluation of DeepFEA is performed on three datasets in the context of structural mechanics, generated to serve as publicly available reference datasets. The results show that DeepFEA can achieve less than 3% normalized mean and root mean squared error for 2D and 3D simulation scenarios, and inference times that are two orders of magnitude faster than FEA. In contrast, relevant state-of-the-art methods face challenges with multi-dimensional output and dynamic input prediction. Furthermore, DeepFEA's robustness was demonstrated in a real-life biomedical scenario, confirming its suitability for accurate and efficient predictions of FEA simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04121v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georgios Triantafyllou, Panagiotis G. Kalozoumis, George Dimas, Dimitris K. Iakovidis</dc:creator>
    </item>
    <item>
      <title>A survey on multi-fidelity surrogates for simulators with functional outputs: unified framework and benchmark</title>
      <link>https://arxiv.org/abs/2408.17075</link>
      <description>arXiv:2408.17075v3 Announce Type: replace 
Abstract: Multi-fidelity surrogate models combining dimensionality reduction and an intermediate surrogate in the reduced space allow a cost-effective emulation of simulators with functional outputs. The surrogate is an input-output mapping learned from a limited number of simulator evaluations. This computational efficiency makes surrogates commonly used for many-query tasks. Diverse methods for building them have been proposed in the literature, but they have only been partially compared.
  This paper introduces a unified framework encompassing the different surrogate families, followed by a methodological comparison and the exposition of practical considerations. More than a dozen of existing multi-fidelity surrogates have been implemented under the unified framework and evaluated on a set of benchmark problems. Based on the results, guidelines and recommendations are proposed regarding multi-fidelity surrogates with functional outputs.
  Our study shows that most multi-fidelity surrogates outperform their tested single-fidelity counterparts under the considered settings. But no particular surrogate is performing better on every test case. Therefore, the selection of a surrogate should consider the specific properties of the emulated functions, in particular the correlation between the low- and high-fidelity simulators, the size of the training set, the local nonlinear variations in the residual fields, and the size of the training datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17075v3</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Brunel, Mathieu Balesdent, Lo\"ic Brevault, Rodolphe Le Riche, Bruno Sudret</dc:creator>
    </item>
    <item>
      <title>Analyzing Challenges in Deployment of the SLSA Framework for Software Supply Chain Security</title>
      <link>https://arxiv.org/abs/2409.05014</link>
      <description>arXiv:2409.05014v2 Announce Type: replace 
Abstract: In 2023, Sonatype reported a 200\% increase in software supply chain attacks, including major build infrastructure attacks. To secure the software supply chain, practitioners can follow security framework guidance like the Supply-chain Levels for Software Artifacts (SLSA). However, recent surveys and industry summits have shown that despite growing interest, the adoption of SLSA is not widespread. To understand adoption challenges, \textit{the goal of this study is to aid framework authors and practitioners in improving the adoption and development of Supply-Chain Levels for Software Artifacts (SLSA) through a qualitative study of SLSA-related issues on GitHub}. We analyzed 1,523 SLSA-related issues extracted from 233 GitHub repositories. We conducted a topic-guided thematic analysis, leveraging the Latent Dirichlet Allocation (LDA) unsupervised machine learning algorithm, to explore the challenges of adopting SLSA and the strategies for overcoming these challenges. We identified four significant challenges and five suggested adoption strategies. The two main challenges reported are complex implementation and unclear communication, highlighting the difficulties in implementing and understanding the SLSA process across diverse ecosystems. The suggested strategies include streamlining provenance generation processes, improving the SLSA verification process, and providing specific and detailed documentation. Our findings indicate that some strategies can help mitigate multiple challenges, and some challenges need future research and tool enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05014v2</guid>
      <category>cs.CE</category>
      <category>cs.SE</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahzabin Tamanna, Sivana Hamer, Mindy Tran, Sascha Fahl, Yasemin Acar, Laurie Williams</dc:creator>
    </item>
    <item>
      <title>Spatial Weather, Socio-Economic and Political Risks in Probabilistic Load Forecasting</title>
      <link>https://arxiv.org/abs/2408.00507</link>
      <description>arXiv:2408.00507v2 Announce Type: replace-cross 
Abstract: Accurate forecasts of the impact of spatial weather and pan-European socio-economic and political risks on hourly electricity demand for the mid-term horizon are crucial for strategic decision-making amidst the inherent uncertainty. Most importantly, these forecasts are essential for the operational management of power plants, ensuring supply security and grid stability, and in guiding energy trading and investment decisions. The primary challenge for this forecasting task lies in disentangling the multifaceted drivers of load, which include national deterministic (daily, weekly, annual, and holiday patterns) and national stochastic weather and autoregressive effects. Additionally, transnational stochastic socio-economic and political effects add further complexity, in particular, due to their non-stationarity. To address this challenge, we present an interpretable probabilistic mid-term forecasting model for the hourly load that captures, besides all deterministic effects, the various uncertainties in load. This model recognizes transnational dependencies across 24 European countries, with multivariate modeled socio-economic and political states and cross-country dependent forecasting. Built from interpretable Generalized Additive Models (GAMs), the model enables an analysis of the transmission of each incorporated effect to the hour-specific load. Our findings highlight the vulnerability of countries reliant on electric heating under extreme weather scenarios. This emphasizes the need for high-resolution forecasting of weather effects on pan-European electricity consumption especially in anticipation of widespread electric heating adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00507v2</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monika Zimmermann, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Mixing It Up: The Cocktail Effect of Multi-Task Fine-Tuning on LLM Performance -- A Case Study in Finance</title>
      <link>https://arxiv.org/abs/2410.01109</link>
      <description>arXiv:2410.01109v2 Announce Type: replace-cross 
Abstract: The application of large language models (LLMs) in domain-specific contexts, including finance, has expanded rapidly. Domain-specific LLMs are typically evaluated based on their performance in various downstream tasks relevant to the domain. In this work, we present a detailed analysis of fine-tuning LLMs for such tasks. Somewhat counterintuitively, we find that in domain-specific cases, fine-tuning exclusively on the target task is not always the most effective strategy. Instead, multi-task finetuning - where models are trained on a cocktail of related tasks - can significantly enhance performance. We demonstrate how this approach enables a small model, such as Phi-3-Mini, to achieve state-of-the-art results, even surpassing the much larger GPT-4-o model on financial benchmarks. Our study involves a large-scale experiment, conducting over 200 training experiments using several widely adopted LLMs as baselines, and empirically confirms the benefits of multi-task fine-tuning. Additionally, we explore the use of general instruction data as a form of regularization, suggesting that it helps minimize performance degradation. We also investigate the inclusion of mathematical data, finding improvements in numerical reasoning that transfer effectively to financial tasks. Finally, we note that while fine-tuning for downstream tasks leads to targeted improvements in task performance, it does not necessarily result in broader gains in domain knowledge or complex domain reasoning abilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01109v2</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Meni Brief, Oded Ovadia, Gil Shenderovitz, Noga Ben Yoash, Rachel Lemberg, Eitam Sheetrit</dc:creator>
    </item>
  </channel>
</rss>
