<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Jul 2025 01:37:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the Testing of complete causal mediation and its applications</title>
      <link>https://arxiv.org/abs/2507.14246</link>
      <description>arXiv:2507.14246v1 Announce Type: new 
Abstract: The Complete Mediation Test (CMT) serves as a specialized approach of mediation analysis to assess whether an independent variable A, influences an outcome variable Y exclusively through a mediator M, without any direct effect. An application of CMT lies in Mendelian Randomization (MR) studies, where it can be used to investigate non-pleiotropy, that is, to test whether genetic variants impact a disease outcome solely through their effect on a target exposure variable. Traditionally, CMT has relied on two significance-based criteria and a proportion-based criterion with a heuristic threshold that has not been rigorously evaluated. In this paper, we explored the theoretical properties of conventional CMT, and proposed using standardized absolute proportion of mediation (SAPM) as a criterion for CMT. We, systematically assess the performance of various CMT criteria via simulation, and demonstrate their practical utility in the context of MR studies. Our results indicate that the offers the best performance. We also propose using different optimal thresholds depending on whether the mediator and outcome are continuous or binary. The SAPM with proper thresholds ensures that the indirect pathway meaningfully accounts for the effect of the exposure on the outcome, thereby strengthening the case for complete mediation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14246v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichin Tsai, Wan-Tzu Chang, Jia Jyun Sie, Cathy SJ Fann, Iebin Lian</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing for quantitative trait locus effects in both location and scale in genetic backcross studies</title>
      <link>https://arxiv.org/abs/2507.14253</link>
      <description>arXiv:2507.14253v1 Announce Type: new 
Abstract: Testing the existence of a quantitative trait locus (QTL) effect is an important task in QTL mapping studies. Most studies concentrate on the case where the phenotype distributions of different QTL groups follow normal distributions with the same unknown variance. In this paper we make a more general assumption that the phenotype distributions come from a location-scale distribution family. We derive the limiting distribution of the LRT for the existence of the QTL effect in both location and scale in genetic backcross studies. We further identify an explicit representation for this limiting distribution. As a complement, we study the limiting distribution of the LRT and its explicit representation for the existence of the QTL effect in the location only. The asymptotic properties of the LRTs under a local alternative are also investigated. Simulation studies are used to evaluate the asymptotic results, and a real-data example is included for illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14253v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1111/sjos.12442</arxiv:DOI>
      <arxiv:journal_reference>Scandinavian Journal of Statistics, 47, 1064-1089 (2020)</arxiv:journal_reference>
      <dc:creator>Guanfu Liu, Pengfei Li, Yukun Liu, Xiaolong Pu</dc:creator>
    </item>
    <item>
      <title>A Statistician's Overview of Mechanistic-Informed Modeling</title>
      <link>https://arxiv.org/abs/2507.14336</link>
      <description>arXiv:2507.14336v1 Announce Type: new 
Abstract: The recent success of deep neural network models with physical constraints (so-called, Physics-Informed Neural Networks, PINNs) has led to renewed interest in the incorporation of mechanistic information in predictive models. Statisticians and others have long been interested in this problem, which has led to several practical and innovative solutions dating back decades. In this overview, we focus on the problem of data-driven prediction and inference of dynamic spatio-temporal processes that include mechanistic information, such as would be available from partial differential equations, with a strong focus on the quantification of uncertainty associated with data, process, and parameters. We give a brief review of several paradigms and focus our attention on Bayesian implementations given they naturally accommodate uncertainty quantification. We then specify a general Bayesian hierarchical modeling framework for spatio-temporal data that can accommodate these mechanistic-informed process approaches. The advantage of this framework is its flexibility and generalizability. We illustrate the methodology via a simulation study in which a nonlinear Burgers' equation PDE is embedded via a Bayesian PINN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14336v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher K. Wikle, Joshua North, Giri Gopalan, Myungsoo Yoo</dc:creator>
    </item>
    <item>
      <title>Distributed Kaplan-Meier Analysis via the Influence Function with Application to COVID-19 and COVID-19 Vaccine Adverse Events</title>
      <link>https://arxiv.org/abs/2507.14351</link>
      <description>arXiv:2507.14351v1 Announce Type: new 
Abstract: During the COVID-19 pandemic, regulatory decision-making was hampered by a lack of timely and high-quality data on rare outcomes. Studying rare outcomes following infection and vaccination requires conducting multi-center observational studies, where sharing individual-level data is a privacy concern. In this paper, we conduct a multi-center observational study of thromboembolic events following COVID-19 and COVID-19 vaccination without sharing individual-level data. We accomplish this by developing a novel distributed learning method for constructing Kaplan-Meier (KM) curves and inverse propensity weighted KM curves with statistical inference. We sequentially update curves site-by-site using the KM influence function, which is a measure of the direction in which an observation should shift our estimate and so can be used to incorporate new observations without access to previous data. We show in simulations that our distributed estimator is unbiased and achieves equal efficiency to the combined data estimator. Applying our method to Beaumont Health, Spectrum Health, and Michigan Medicine data, we find a much higher covariate-adjusted incidence of blood clots after SARS-CoV-2 infection (3.13%, 95% CI: [2.93, 3.35]) compared to first COVID-19 vaccine (0.08%, 95% CI: [0.08, 0.09]). This suggests that the protection vaccines provide against COVID-19-related clots outweighs the risk of vaccine-related adverse events, and shows the potential of distributed survival analysis to provide actionable evidence for time-sensitive decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14351v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malcolm Risk, Xu Shi, Lili Zhao</dc:creator>
    </item>
    <item>
      <title>A Hybrid Mixture Approach for Clustering and Characterizing Cancer Data</title>
      <link>https://arxiv.org/abs/2507.14380</link>
      <description>arXiv:2507.14380v1 Announce Type: new 
Abstract: Model-based clustering is widely used for identifying and distinguishing types of diseases. However, modern biomedical data coming with high dimensions make it challenging to perform the model estimation in traditional cluster analysis. The incorporation of factor analyzer into the mixture model provides a way to characterize the large set of data features, but the current estimation method is computationally impractical for massive data due to the intrinsic slow convergence of the embedded algorithms, and the incapability to vary the size of the factor analyzers, preventing the implementation of a generalized mixture of factor analyzers and further characterization of the data clusters. We propose a hybrid matrix-free computational scheme to efficiently estimate the clusters and model parameters based on a Gaussian mixture along with generalized factor analyzers to summarize the large number of variables using a small set of underlying factors. Our approach outperforms the existing method with faster convergence while maintaining high clustering accuracy. Our algorithms are applied to accurately identify and distinguish types of breast cancer based on large tumor samples, and to provide a generalized characterization for subtypes of lymphoma using massive gene records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14380v1</guid>
      <category>stat.ME</category>
      <category>q-bio.TO</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazeem Kareem, Fan Dai</dc:creator>
    </item>
    <item>
      <title>Policy relevance of causal quantities in networks</title>
      <link>https://arxiv.org/abs/2507.14391</link>
      <description>arXiv:2507.14391v1 Announce Type: new 
Abstract: In settings where units' outcomes are affected by others' treatments, there has been a proliferation of ways to quantify effects of treatments on outcomes. Here we describe how many proposed estimands can be represented as involving one of two ways of averaging over units and treatment assignments. The more common representation often results in quantities that are irrelevant, or at least insufficient, for optimal choice of policies governing treatment assignment. The other representation often yields quantities that lack an interpretation as summaries of unit-level effects, but that we argue may still be relevant to policy choice. Among various estimands, the expected average outcome -- or its contrast between two different policies -- can be represented both ways and, we argue, merits further attention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14391v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahil Loomba, Dean Eckles</dc:creator>
    </item>
    <item>
      <title>Bivariate generalized autoregressive models for forecasting bivariate non-Gaussian times series</title>
      <link>https://arxiv.org/abs/2507.14442</link>
      <description>arXiv:2507.14442v1 Announce Type: new 
Abstract: This paper introduces a novel approach, the bivariate generalized autoregressive (BGAR) model, for modeling and forecasting bivariate time series data. The BGAR model generalizes the bivariate vector autoregressive (VAR) models by allowing data that does not necessarily follow a normal distribution. We consider a random vector of two time series and assume each belongs to the canonical exponential family, similarly to the univariate generalized autoregressive moving average (GARMA) model. We include autoregressive terms of one series into the dynamical structure of the other and vice versa. The model parameters are estimated using the conditional maximum likelihood (CML) method. We provide general closed-form expressions for the conditional score vector and conditional Fisher information matrix, encompassing all canonical exponential family distributions. We develop asymptotic confidence intervals and hypothesis tests. We discuss techniques for model selection, residual diagnostic analysis, and forecasting. We carry out Monte Carlo simulation studies to evaluate the performance of the finite sample CML inferences, including point and interval estimation. An application to real data analyzes the number of leptospirosis cases on hospitalizations due to leptospirosis in S\~ao Paulo state, Brazil. Competing models such as GARMA, autoregressive integrated moving average (ARIMA), and VAR models are considered for comparison purposes. The new model outperforms the competing models by providing more accurate out-of-sample forecasting and allowing quantification of the lagged effect of the case count series on hospitalizations due to leptospirosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14442v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiane Fontana Ribeiro, Airlane P. Alencar, F\'abio M. Bayer</dc:creator>
    </item>
    <item>
      <title>Parameter-transfer in spatial autoregressive models via model averaging</title>
      <link>https://arxiv.org/abs/2507.14453</link>
      <description>arXiv:2507.14453v1 Announce Type: new 
Abstract: Econometric modeling in spatial autoregressive models often suffers from insufficient samples in practice, such as spatial analysis of infectious diseases at the country level with limited data. Transfer learning offers a promising solution by leveraging information from regions or domains with similar spatial spillover effects to improve the analysis of the target data. In this paper, we propose a parameter-transfer approach based on Mallows model averaging for spatial autoregressive models to improve the prediction accuracy. Our approach does not require sharing multi-source spatial data and can be combined with various parameter estimation methods, such as the maximum likelihood and the two-stage least squares. Theoretical analyses demonstrate that our method achieves asymptotic optimality and ensures weight convergence with an explicit convergence rate. Simulation studies and the application of infection count prediction in Africa further demonstrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14453v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fen Jiang, Wenhui Li, Xinyu Zhang</dc:creator>
    </item>
    <item>
      <title>Mean Shift for Functional Data: A Scalable Algorithm and Convergence Analysis</title>
      <link>https://arxiv.org/abs/2507.14457</link>
      <description>arXiv:2507.14457v1 Announce Type: new 
Abstract: This paper extends the mean shift algorithm from vector-valued data to functional data, enabling effective clustering in infinite-dimensional settings. To address the computational challenges posed by large-scale datasets, we introduce a fast stochastic variant that significantly reduces computational complexity. We provide a rigorous analysis of convergence and stability for the full functional mean shift procedure, establishing theoretical guarantees for its behavior. For the stochastic variant, although a full convergence theory remains open, we offer partial justification for its use by showing that it approximates the full algorithm well when the subset size is large. The proposed method is further validated through a real-data application to Argo oceanographic profiles. Our key contributions include: (1) a novel extension of mean shift to functional data; (2) convergence and stability analysis of the full functional mean shift algorithm in Hilbert space; (3) a scalable stochastic variant based on random partitioning, with partial theoretical justification; and (4) a real-data application demonstrating the method's scalability and practical usefulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14457v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ting-Li Chen, Toshinari Morimoto, Su-Yun Huang, Ruey S. Tsay</dc:creator>
    </item>
    <item>
      <title>An exact test for the mixed membership stochastic block model</title>
      <link>https://arxiv.org/abs/2507.14464</link>
      <description>arXiv:2507.14464v1 Announce Type: new 
Abstract: We present the first finite-sample goodness-of-fit test for the mixed membership stochastic block model (MMSBM). Using algebraic statistics theory, we derive a Markov basis that lets a Metropolis-Hastings sampler explore exactly the set of networks compatible with a fitted MMSBM. The resulting exact $p$-value, based on a partial conjunction statistic, requires no asymptotic approximations. Simulations show nominal size and strong power against misspecified block numbers and connection patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14464v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Majumdar</dc:creator>
    </item>
    <item>
      <title>Maximum likelihood abundance estimation from capture-recapture data when covariates are missing at random</title>
      <link>https://arxiv.org/abs/2507.14486</link>
      <description>arXiv:2507.14486v1 Announce Type: new 
Abstract: In capture-recapture experiments, individual covariates may be subject to missing, especially when the number of times of being captured is small. When the covariate information is missing at random, the inverse probability weighting method and multiple imputation method are widely used to obtain the point estimators of the abundance. These point estimators are then used to construct the Wald-type confidence intervals for the abundance. However, such intervals may have severely inaccurate coverage probabilities and their lower limits can be even less than the number of individuals ever captured. In this paper, we proposed a maximum empirical likelihood estimation approach for the abundance in presence of missing covariates. We show that the maximum empirical likelihood estimator is asymptotically normal, and that the empirical likelihood ratio statistic for abundance has a chisquare limiting distribution with one degree of freedom. Simulations indicate that the proposed estimator has smaller mean square error than the existing estimators, and the proposed empirical likelihood ratio confidence interval usually has more accurate coverage probabilities than the existing Wald-type confidence intervals. We illustrate the proposed method by analyzing the bird species yellow-bellied prinia collected in Hong Kong.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14486v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/biom.13334</arxiv:DOI>
      <arxiv:journal_reference>Biometrics (2021), 77, 1050-1060</arxiv:journal_reference>
      <dc:creator>Yang Liu, Yukun Liu, Pengfei Li, Jing Qin, Lin Zhu</dc:creator>
    </item>
    <item>
      <title>Powerful Foldover Designs</title>
      <link>https://arxiv.org/abs/2507.14648</link>
      <description>arXiv:2507.14648v1 Announce Type: new 
Abstract: The foldover technique for screening designs is well known to guarantee zero aliasing of the main effect estimators with respect to two factor interactions and quadratic effects. It is a key feature of many popular response surface designs, including central composite designs, definitive screening designs, and most orthogonal, minimally-aliased response surface designs. In this paper, we show the foldover technique is even more powerful, because it produces degrees of freedom for a variance estimator that is independent of model selection. These degrees of freedom are characterized as either pure error or fake factor degrees of freedom. A fast design construction algorithm is presented that minimizes the expected confidence interval criterion to maximize the power of screening main effects. An augmented design and analysis method is also presented to avoid having too many degrees of freedom for estimating variance and to improve model selection performance for second order models. Simulation studies show our new designs are at least as good as traditional designs when effect sparsity and hierarchy hold, but do significantly better when these effect principles do not hold. A real data example is given for a 20-run experiment where optimization of ethylene concentration is performed by manipulating eight process parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14648v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan W. Stallrich, Rakhi Singh, Kyle Vogt-Lowell, Fanxing Li</dc:creator>
    </item>
    <item>
      <title>Variable Selection for Stratified Sampling Designs in Semiparametric Accelerated Failure Time Models with Clustered Failure Times</title>
      <link>https://arxiv.org/abs/2507.14689</link>
      <description>arXiv:2507.14689v1 Announce Type: new 
Abstract: In large-scale epidemiological studies, statistical inference is often complicated by high-dimensional covariates under stratified sampling designs for failure times. Variable selection methods developed for full cohort data do not extend naturally to stratified sampling designs, and appropriate adjustments for the sampling scheme are necessary. Further challenges arise when the failure times are clustered and exhibit within-cluster dependence. As an alternative of Cox proportional hazards (PH) model when the PH assumption is not valid, the penalized Buckley-James (BJ) estimating method for accelerated failure time (AFT) models can potentially handle within-cluster correlation in such setting by incorporating generalized estimating equation (GEE) techniques, though its practical implementation remains hindered by computational instability. We propose a regularized estimating method within the GEE framework for stratified sampling designs, in the spirit of the penalized BJ method but with a reliable inference procedure. We establish the consistency and asymptotic normality of the proposed estimators and show that they achieve the oracle property. Extensive simulation studies demonstrate that our method outperforms existing methods that ignore sampling bias or within-cluster dependence. Moreover, the regularization scheme effectively selects relevant variables even with moderate sample sizes. The proposed methodology is illustrated through applications to a dental study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14689v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Chen, Chuan-Fa Tang, Sy Han Chiou, Min Chen</dc:creator>
    </item>
    <item>
      <title>A Stability-Driven Framework for Long-Term Hourly Electricity Demand Forecasting</title>
      <link>https://arxiv.org/abs/2507.15001</link>
      <description>arXiv:2507.15001v1 Announce Type: new 
Abstract: Long-term electricity demand forecasting is essential for grid and operations planning, as well as for the analysis and planning of energy transition strategies. However, accurate long-term load forecasting with high temporal resolution remains challenging, as most existing approaches focus on aggregated forecasts, which require accurate prediction of numerous variables for bottom-up sectoral forecasts. In this study, we propose a parsimonious methodology that employs t-tests to verify load stability and the correlation of load with gross domestic product (GDP) to produce a long-term hourly load forecast. Applying this method to Singapore's electricity demand, analysis of multi-year historical data (2004-2022) reveals that its relative hourly load has remained statistically stable, with an overall percentage deviation of 4.24% across seasonality indices. Utilizing these stability findings, five-year-ahead total yearly forecasts were generated using GDP as a predictor, and hourly loads were forecasted using hourly seasonality index fractions. The maximum Mean Absolute Percentage Error (MAPE) across multiple experiments for six-year-ahead forecasts was 6.87%. The methodology was further applied to Belgium (an OECD country) and Bulgaria (a non-OECD country), yielding MAPE values of 6.81% and 5.64%, respectively. Additionally, stability results were incorporated into a short-term forecasting model based on exponential smoothing, demonstrating comparable or improved accuracy relative to existing machine learning-based methods. These findings indicate that parsimonious approaches can effectively produce long-term, high-resolution forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15001v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyadeep Dhar, Ayushkumar Parmar, Haifeng Qiu, Juan Ramon L. Senga, S. Viswanathan</dc:creator>
    </item>
    <item>
      <title>Time-Dependent Pseudo $\boldsymbol{R^2}$ for Assessing Predictive Performance in Competing Risks Data</title>
      <link>https://arxiv.org/abs/2507.15040</link>
      <description>arXiv:2507.15040v1 Announce Type: new 
Abstract: Evaluating and validating the performance of prediction models is a fundamental task in statistics, machine learning, and their diverse applications. However, developing robust performance metrics for competing risks time-to-event data poses unique challenges. We first highlight how certain conventional predictive performance metrics, such as the C-index, Brier score, and time-dependent AUC, can yield undesirable results when comparing predictive performance between different prediction models. To address this research gap, we introduce a novel time-dependent pseudo $R^2$ measure to evaluate the predictive performance of a predictive cumulative incidence function over a restricted time domain under right-censored competing risks time-to-event data. Specifically, we first propose a population-level time-dependent pseudo $R^2$ measures for the competing risk event of interest and then define their corresponding sample versions based on right-censored competing risks time-to-event data. We investigate the asymptotic properties of the proposed measure and demonstrate its advantages over conventional metrics through comprehensive simulation studies and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15040v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zian Zhuang, Wen Su, Eric Kawaguchi, Gang Li</dc:creator>
    </item>
    <item>
      <title>On Weighted Entropy Generating Function</title>
      <link>https://arxiv.org/abs/2507.15057</link>
      <description>arXiv:2507.15057v1 Announce Type: new 
Abstract: In this paper, we study the properties of the weighted entropy generating function (WEGF). We also introduce the weighted residual entropy generating function (WREGF) and establish some characterization results based on its connections with the hazard rate and the mean residual life function. Furthermore, we propose two new classes of life distributions derived from WREGF. We also study the non-parametric estimation of WREGF. A non-parametric test for the Pareto type I distribution is developed based on entropy characterization. To evaluate the performance of the test statistics, we conduct an extensive Monte Carlo simulation study. Finally, we apply the proposed method to two real-life datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15057v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Smitha S., Mary Andrewsa, Sudheesh K. Kattumannil</dc:creator>
    </item>
    <item>
      <title>Total Loss Functions for Measuring the Accuracy of Nonnegative Cross-Sectional Predictions</title>
      <link>https://arxiv.org/abs/2507.15136</link>
      <description>arXiv:2507.15136v1 Announce Type: new 
Abstract: The total loss function associated with a set of cross-sectional predictions, that is, estimates or forecasts, summarizes the set's overall accuracy. Its arguments are the individual cross-sectional units' loss functions. Under general assumptions, including impartiality, about the forms of the individual loss functions, and the specific assumptions that the total loss function is anonymous and monotonic, only the additive, multiplicative and L-type (with restrictions) total loss functions are found to be admissible. The first two total loss functions correspond to different interpretations of economic utility. An isomorphism exists between these two total loss functions. Thus, the additive total loss function can always be used. This isomorphism can also be used to explore the properties of various combinations of total and individual loss functions. Moreover, the additive loss function obeys the von Neumann-Morgenstern expected utility axioms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15136v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman</dc:creator>
    </item>
    <item>
      <title>Causal Mediation Analysis for Zero-inflated Mixture Mediators</title>
      <link>https://arxiv.org/abs/2507.15164</link>
      <description>arXiv:2507.15164v1 Announce Type: new 
Abstract: Causal mediation analysis is an important statistical tool to quantify effects transmitted by intermediate variables from a cause to an outcome. There is a gap in mediation analysis methods to handle mixture mediator data that are zero-inflated with multi-modality and atypical behaviors. We propose an innovative way to model zero-inflated mixture mediators from the perspective of finite mixture distributions to flexibly capture such mediator data. Multiple data types are considered for modeling such mediators including the zero-inflated log-normal mixture, zero-inflated Poisson mixture and zero-inflated negative binomial mixture. A two-part mediation effect is derived to better understand effects on outcomes attributable to the numerical change as well as binary change from 0 to 1 in mediators. The maximum likelihood estimates are obtained by an expectation maximization algorithm to account for unobserved mixture membership and whether an observed zero is a true or false zero. The optimal number of mixture components are chosen by a model selection criterion. The performance of the proposed method is demonstrated in a simulation study and an application to a neuroscience study in comparison with standard mediation analysis methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15164v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meilin Jiang, Seonjoo Lee, A. James O'Malley, Pengfei Li, Zhigang Li</dc:creator>
    </item>
    <item>
      <title>Misspecifying non-compensatory as compensatory IRT: analysis of estimated skills and variance</title>
      <link>https://arxiv.org/abs/2507.15222</link>
      <description>arXiv:2507.15222v1 Announce Type: new 
Abstract: Multidimensional item response theory is a statistical test theory used to estimate the latent skills of learners and the difficulty levels of problems based on test results. Both compensatory and non-compensatory models have been proposed in the literature. Previous studies have revealed the substantial underestimation of higher skills when the non-compensatory model is misspecified as the compensatory model. However, the underlying mechanism behind this phenomenon has not been fully elucidated. It remains unclear whether overestimation also occurs and whether issues arise regarding the variance of the estimated parameters. In this paper, we aim to provide a comprehensive understanding of both underestimation and overestimation through a theoretical approach. In addition to the previously identified underestimation of the skills, we newly discover that the overestimation of skills occurs around the origin. Furthermore, we investigate the extent to which the asymptotic variance of the estimated parameters differs when considering model misspecification compared to when it is not taken into account.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15222v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroshi Tamano, Hideitsu Hino, Daichi Mochihashi</dc:creator>
    </item>
    <item>
      <title>Robust and Differentially Private PCA for non-Gaussian data</title>
      <link>https://arxiv.org/abs/2507.15232</link>
      <description>arXiv:2507.15232v1 Announce Type: new 
Abstract: Recent advances have sparked significant interest in the development of privacy-preserving Principal Component Analysis (PCA). However, many existing approaches rely on restrictive assumptions, such as assuming sub-Gaussian data or being vulnerable to data contamination. Additionally, some methods are computationally expensive or depend on unknown model parameters that must be estimated, limiting their accessibility for data analysts seeking privacy-preserving PCA. In this paper, we propose a differentially private PCA method applicable to heavy-tailed and potentially contaminated data. Our approach leverages the property that the covariance matrix of properly rescaled data preserves eigenvectors and their order under elliptical distributions, which include Gaussian and heavy-tailed distributions. By applying a bounded transformation, we enable straightforward computation of principal components in a differentially private manner. Additionally, boundedness guarantees robustness against data contamination. We conduct both theoretical analysis and empirical evaluations of the proposed method, focusing on its ability to recover the subspace spanned by the leading principal components. Extensive numerical experiments demonstrate that our method consistently outperforms existing approaches in terms of statistical utility, particularly in non-Gaussian or contaminated data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15232v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minwoo Kim, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>Sufficiency-principled Transfer Learning via Model Averaging</title>
      <link>https://arxiv.org/abs/2507.15416</link>
      <description>arXiv:2507.15416v1 Announce Type: new 
Abstract: When the transferable set is unknowable, transfering informative knowledge as much as possible\textemdash a principle we refer to as \emph{sufficiency}, becomes crucial for enhancing transfer learning effectiveness. However, existing transfer learning methods not only overlook the sufficiency principle, but also rely on restrictive single-similarity assumptions (\eg individual or combinatorial similarity), leading to suboptimal performance. To address these limitations, we propose a sufficiency-principled transfer learning framework via unified model averaging algorithms, accommodating both individual and combinatorial similarities. Theoretically, we establish the asymptotic/high-probability optimality, enhanced convergence rate and asymptotic normality for multi-source linear regression models with a diverging number of parameters, achieving sufficiency, robustness to negative transfer, privacy protection and feasible statistical inference. Extensive simulations and an empirical data analysis of Beijing housing rental data demonstrate the promising superiority of our framework over conventional alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15416v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiyuan Zhang, Huihang Liu, Xinyu Zhang</dc:creator>
    </item>
    <item>
      <title>Prediction of linear fractional stable motions using codifference</title>
      <link>https://arxiv.org/abs/2507.15437</link>
      <description>arXiv:2507.15437v1 Announce Type: new 
Abstract: The linear fractional stable motion (LFSM) extends the fractional Brownian motion (fBm) by considering $\alpha$-stable increments. We propose a method to forecast future increments of the LFSM from past discrete-time observations, using the conditional expectation when $\alpha&gt;1$ or a semimetric projection otherwise. It relies on the codifference, which describes the serial dependence of the process, instead of the covariance. Indeed, covariance is commonly used for predicting an fBm but it is infinite when $\alpha&lt;2$. Some theoretical properties of the method and of its accuracy are studied and both a simulation study and an application to real data confirm the relevance of the approach. The LFSM-based method outperforms the fBm, when forecasting high-frequency FX rates. It also shows a promising performance in the forecast of time series of volatilities, decomposing properly, in the fractal dynamic of rough volatilities, the contribution of the kurtosis of the increments and the contribution of their serial dependence. Moreover, the analysis of hit ratios suggests that, beside independence, persistence, and antipersistence, a fourth regime of serial dependence exists for fractional processes, characterized by a selective memory controlled by a few large increments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15437v1</guid>
      <category>stat.ME</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthieu Garcin, Karl Sawaya, Thomas Valade</dc:creator>
    </item>
    <item>
      <title>Multiple Hypothesis Testing To Estimate The Number Of Communities in Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2507.15471</link>
      <description>arXiv:2507.15471v1 Announce Type: new 
Abstract: Clustering of single-cell RNA sequencing (scRNA-seq) datasets can give key insights into the biological functions of cells. Therefore, it is not surprising that network-based community detection methods (one of the better clustering methods) are increasingly being used for the clustering of scRNA-seq datasets. The main challenge in implementing network-based community detection methods for scRNA-seq datasets is that these methods \emph{apriori} require the true number of communities or blocks for estimating the community memberships. Although there are existing methods for estimating the number of communities, they are not suitable for noisy scRNA-seq datasets. Moreover, we require an appropriate method for extracting suitable networks from scRNA-seq datasets. For addressing these issues, we present a two-fold solution: i) a simple likelihood-based approach for extracting stochastic block models (SBMs) out of scRNA-seq datasets, ii) a new sequential multiple testing (SMT) method for estimating the number of communities in SBMs. We study the theoretical properties of SMT and establish its consistency under moderate sparsity conditions. In addition, we compare the numerical performance of the SMT with several existing methods. We also show that our approach performs competitively well against existing methods for estimating the number of communities on benchmark scRNA-seq datasets. Finally, we use our approach for estimating subgroups of a human retina bipolar single cell dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15471v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chetkar Jha, Mingyao Li, Ian Barnett</dc:creator>
    </item>
    <item>
      <title>Scalable Estimation of Crossed Random Effects Models via Multi-way Grouping</title>
      <link>https://arxiv.org/abs/2507.15593</link>
      <description>arXiv:2507.15593v1 Announce Type: new 
Abstract: Cross-classified data frequently arise in scientific fields such as education, healthcare, and social sciences. A common modeling strategy is to introduce crossed random effects within a regression framework. However, this approach often encounters serious computational bottlenecks, particularly for non-Gaussian outcomes. In this paper, we propose a scalable and flexible method that approximates the distribution of each random effect by a discrete distribution, effectively partitioning the random effects into a finite number of representative groups. This approximation allows us to express the model as a multi-way grouped structure, which can be efficiently estimated using a simple and fast iterative algorithm. The proposed method accommodates a wide range of outcome models and remains applicable even in settings with more than two-way cross-classification. We theoretically establish the consistency and asymptotic normality of the estimator under general settings of classification levels. Through simulation studies and real data applications, we demonstrate the practical performance of the proposed method in logistic, Poisson, and ordered probit regression models involving cross-classified structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15593v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shota Takeishi, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Inference on Nonlinear Counterfactual Functionals under a Multiplicative IV Model</title>
      <link>https://arxiv.org/abs/2507.15612</link>
      <description>arXiv:2507.15612v1 Announce Type: new 
Abstract: Instrumental variable (IV) methods play a central role in causal inference, particularly in settings where treatment assignment is confounded by unobserved variables. IV methods have been extensively developed in recent years and applied across diverse domains, from economics to epidemiology. In this work, we study the recently introduced multiplicative IV (MIV) model and demonstrate its utility for causal inference beyond the average treatment effect. In particular, we show that it enables identification and inference for a broad class of counterfactual functionals characterized by moment equations. This includes, for example, inference on quantile treatment effects. We develop methods for efficient and multiply robust estimation of such functionals, and provide inference procedures with asymptotic validity. Experimental results demonstrate that the proposed procedure performs well even with moderate sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15612v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Mengxin Yu, Jiewen Liu, Chan Park, Yunshu Zhang, James M. Robins, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Maximum multinomial likelihood estimation in compound mixture model with application to malaria study</title>
      <link>https://arxiv.org/abs/2507.15623</link>
      <description>arXiv:2507.15623v1 Announce Type: new 
Abstract: Malaria can be diagnosed by the presence of parasites and symptoms (usually fever) due to the parasites. In endemic areas, however, an individual may have fever attributable either to malaria or to other causes. Thus, the parasite level of an individual with fever follows a two-component mixture, with the two components corresponding to malaria and nonmalaria individuals. Furthermore, the parasite levels of nonmalaria individuals can be characterized as a mixture of a zero component and a positive distribution. In this article, we propose a nonparametric maximum multinomial likelihood approach for estimating the proportion of malaria using parasite-level data from two groups of individuals collected in two different seasons. We develop an EM-algorithm to numerically calculate the proposed estimates and further establish their convergence rates. Simulation results show that the proposed estimators are more efficient than existing nonparametric estimators. The proposed method is used to analyze a malaria survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15623v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10485252.2021.1898609</arxiv:DOI>
      <arxiv:journal_reference>Journal of Nonparametric Statistics, 33, 21-38 (2021)</arxiv:journal_reference>
      <dc:creator>Zhaoyang Tian, Kun Liang, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>Testing Homogeneity in a heteroscedastic contaminated normal mixture</title>
      <link>https://arxiv.org/abs/2507.15630</link>
      <description>arXiv:2507.15630v1 Announce Type: new 
Abstract: Large-scale simultaneous hypothesis testing appears in many areas such as microarray studies, genome-wide association studies, brain imaging, disease mapping and astronomical surveys. A well-known inference method is to control the false discovery rate. One popular approach is to model the $z$-scores derived from the individual $t$-tests and then use this model to control the false discovery rate. We propose a new class of contaminated normal mixtures for modelling $z$-scores. We further design an EM-test for testing homogeneity in this class of mixture models. We show that the EM-test statistic has a shifted mixture of chi-squared limiting distribution. Simulation results show that the proposed testing procedure has accurate type I error and significantly larger power than its competitors under a variety of model specifications. A real-data example is analyzed to exemplify the application of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15630v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/02664763.2018.1552668</arxiv:DOI>
      <arxiv:journal_reference>Journal of Applied Statistics, 46, 1478-1491 (2019)</arxiv:journal_reference>
      <dc:creator>Xiaoqing Niu, Pengfei Li, Yuejiao Fu</dc:creator>
    </item>
    <item>
      <title>A powerful procedure that controls the false discovery rate with directional information</title>
      <link>https://arxiv.org/abs/2507.15631</link>
      <description>arXiv:2507.15631v1 Announce Type: new 
Abstract: In many multiple testing applications in genetics, the signs of test statistics provide useful directional information, such as whether genes are potentially up- or down-regulated between two experimental conditions. However, most existing procedures that control the false discovery rate (FDR) are $p$-value based and ignore such directional information. We introduce a novel procedure, the signed-knockoff procedure, to utilize the directional information and control the FDR in finite samples. We demonstrate the power advantage of our procedure through simulation studies and two real applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15631v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/biom.13277</arxiv:DOI>
      <arxiv:journal_reference>Biometrics, 77, 212-222 (2021)</arxiv:journal_reference>
      <dc:creator>Zhaoyang Tian, Kun Liang, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>The Win Ratio at the Design Stage of Clinical Trials</title>
      <link>https://arxiv.org/abs/2507.15685</link>
      <description>arXiv:2507.15685v1 Announce Type: new 
Abstract: The win ratio offers a flexible approach to incorporate the hierarchy of clinical outcomes into the analysis of a composite endpoint, enabling simultaneous consideration of multiple outcome types, unlike traditional time-to-first-event (TTFE) analysis or focus on a single outcome. We examined the statistical power of the win ratio compared to single-endpoint analyses and TTFE analysis through a case study and simulation studies. Furthermore, we provide a novel formula to estimate the required sample size for win ratio analysis based on the desired width of its confidence interval, facilitating precision-based trial design.
  Our results indicate that win ratio analysis generally outperforms single-endpoint analyses when treatment effects on lower-ranked outcomes are moderate compared to those on higher-ranked outcomes. The win ratio can provide greater power than TTFE analysis, especially when the effect on the highest-ranked outcome is substantial, reaching increases in power up to 50\%. Further, even for moderate treatment effects on the highest-ranked outcome, win ratio analysis achieved higher power.
  Future work should expand our simulations to additional data-generating mechanisms and outcome types, particularly ordinal outcomes, where the win ratio provides an alternative to existing non-parametric and parametric methods.
  Our findings highlight the potential of the win ratio to improve statistical efficiency in pharmaceutical and other clinical trial designs using composite endpoints, particularly when no single component dominates the treatment effect. However, when continuous outcomes occupy the top of the hierarchy, these tend to drive overall analysis, sidelining contributions of lower-ranked outcomes and limiting benefits of hierarchical win ratio analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15685v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kronthaler, Matthias Schwenkglenks, Felix Beuschlein, Ulrike Held</dc:creator>
    </item>
    <item>
      <title>ACS: An interactive framework for conformal selection</title>
      <link>https://arxiv.org/abs/2507.15825</link>
      <description>arXiv:2507.15825v1 Announce Type: new 
Abstract: This paper presents adaptive conformal selection (ACS), an interactive framework for model-free selection with guaranteed error control. Building on conformal selection (Jin and Cand\`es, 2023b), ACS generalizes the approach to support human-in-the-loop adaptive data analysis. Under the ACS framework, we can partially reuse the data to boost the selection power, make decisions on the fly while exploring the data, and incorporate new information or preferences as they arise. The key to ACS is a carefully designed principle that controls the information available for decision making, allowing the data analyst to explore the data adaptively while maintaining rigorous control of the false discovery rate (FDR). Based on the ACS framework, we provide concrete selection algorithms for various goals, including model update/selection, diversified selection, and incorporating newly available labeled data. The effectiveness of ACS is demonstrated through extensive numerical simulations and real-data applications in large language model (LLM) deployment and drug discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15825v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Gui, Ying Jin, Yash Nair, Zhimei Ren</dc:creator>
    </item>
    <item>
      <title>Regional compositional trajectories and structural change: A spatiotemporal multivariate autoregressive framework</title>
      <link>https://arxiv.org/abs/2507.14389</link>
      <description>arXiv:2507.14389v1 Announce Type: cross 
Abstract: Compositional data, such as regional shares of economic sectors or property transactions, are central to understanding structural change in economic systems across space and time. This paper introduces a spatiotemporal multivariate autoregressive model tailored for panel data with composition-valued responses at each areal unit and time point. The proposed framework enables the joint modelling of temporal dynamics and spatial dependence under compositional constraints and is estimated via a quasi maximum likelihood approach. We build on recent theoretical advances to establish identifiability and asymptotic properties of the estimator when both the number of regions and time points grow. The utility and flexibility of the model are demonstrated through two applications: analysing property transaction compositions in an intra-city housing market (Berlin), and regional sectoral compositions in Spain's economy. These case studies highlight how the proposed framework captures key features of spatiotemporal economic processes that are often missed by conventional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14389v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Eckardt, Philipp Otto</dc:creator>
    </item>
    <item>
      <title>Inference for Diffusion Processes via Controlled Sequential Monte Carlo and Splitting Schemes</title>
      <link>https://arxiv.org/abs/2507.14535</link>
      <description>arXiv:2507.14535v1 Announce Type: cross 
Abstract: We introduce an inferential framework for a wide class of semi-linear stochastic differential equations (SDEs). Recent work has shown that numerical splitting schemes can preserve critical properties of such types of SDEs, give rise to explicit pseudolikelihoods, and hence allow for parameter inference for fully observed processes. Here, under several discrete time observation regimes (particularly, partially and fully observed with and without noise), we represent the implied pseudolikelihood as the normalising constant of a Feynman--Kac flow, allowing its efficient estimation via controlled sequential Monte Carlo and adapt likelihood-based methods to exploit this pseudolikelihood for inference. The strategy developed herein allows us to obtain good inferential results across a range of problems. Using diffusion bridges, we are able to computationally reduce bias coming from time-discretisation without recourse to more complex numerical schemes which typically require considerable application-specific efforts. Simulations illustrate that our method provides an excellent trade-off between computational efficiency and accuracy, under hypoellipticity, for both point and posterior estimation. Application to a neuroscience example shows the good performance of the method in challenging settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14535v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shu Huang, Richard G. Everitt, Massimiliano Tamborrino, Adam M. Johansen</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach to Estimating Effect Sizes in Educational Research</title>
      <link>https://arxiv.org/abs/2507.14848</link>
      <description>arXiv:2507.14848v1 Announce Type: cross 
Abstract: In educational research, a first step in the descriptive analysis of data from any psychometric measurement of the performance of subjects in tests at different time points and between groups is to compute relative measures of learning gains and learning achievements. For these effect sizes, there are classical approaches coming from frequentistic statistics like Student's or Welch's $t$-test with their own strengths and weaknesses. In this paper, we propose a purely Bayesian approach for analysing within-group and between-group differences in learning outcomes, taking naturally into account the multilevel structure of the data, as well as heterogeneous variances among time points and groups. We provide a detailed implementation using the brms package in R serving as a wrapper for the probabilistic programming language Stan, facilitating the implementation of these methods in future research by including online supplementary material. We recommend that for a pooled design, one computes an effect size $d_s$, and for a paired design, one should compute two possibly different quantities $d_s$ and $d_z$ to correct for correlations in within-group designs and allowing for comparability across different studies. All these effect sizes are based on ideas coming from Hedge's total effect size $\delta_t$ introduced in 2007.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14848v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yannis B\"ahni</dc:creator>
    </item>
    <item>
      <title>Learning under Latent Group Sparsity via Diffusion on Networks</title>
      <link>https://arxiv.org/abs/2507.15097</link>
      <description>arXiv:2507.15097v1 Announce Type: cross 
Abstract: Group or cluster structure on explanatory variables in machine learning problems is a very general phenomenon, which has attracted broad interest from practitioners and theoreticians alike. In this work we contribute an approach to sparse learning under such group structure, that does not require prior information on the group identities. Our paradigm is motivated by the Laplacian geometry of an underlying network with a related community structure, and proceeds by directly incorporating this into a penalty that is effectively computed via a heat-flow-based local network dynamics. The proposed penalty interpolates between the lasso and the group lasso penalties, the runtime of the heat-flow dynamics being the interpolating parameter. As such it can automatically default to lasso when the group structure reflected in the Laplacian is weak. In fact, we demonstrate a data-driven procedure to construct such a network based on the available data. Notably, we dispense with computationally intensive pre-processing involving clustering of variables, spectral or otherwise. Our technique is underpinned by rigorous theorems that guarantee its effective performance and provide bounds on its sample complexity. In particular, in a wide range of settings, it provably suffices to run the diffusion for time that is only logarithmic in the problem dimensions. We explore in detail the interfaces of our approach with key statistical physics models in network science, such as the Gaussian Free Field and the Stochastic Block Model. Our work raises the possibility of applying similar diffusion-based techniques to classical learning tasks, exploiting the interplay between geometric, dynamical and stochastic structures underlying the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15097v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhroshekhar Ghosh, Soumendu Sundar Mukherjee</dc:creator>
    </item>
    <item>
      <title>Functional Laplace Transform of a Multivariate Hawkes Process, Subsequent Characteristics, and Numerical Approximations</title>
      <link>https://arxiv.org/abs/2507.15370</link>
      <description>arXiv:2507.15370v1 Announce Type: cross 
Abstract: Numerous studies grounded on Hawkes processes have been carried out in many fields including finance, biology and social network. Hawkes processes form a class of selfexciting simple point processes. In this article, we consider a general class of multivariate Hawkes processes envisioned to model dynamics of spatio-temporal epidemics. For this class, the igniting baseline intensity is time dependent and the exciting matrix function is a general one, making the model non-Markovian in most of the cases. In this article, we first provide the closed-form expression of the multivariate multi-temporal characteristic function of these Hawkes processes, extending in a natural way the classical single-time formula found in the Hawkes literature. Then, we use the infinitely divisible property of the Hawkes process to derive the equation system related to the probability distribution of counts at each single time, adapted to the general formulation of the Hawkes model considered in this article. Next, we provide closed-form formulas for the temporal structure of the two first moments of the process, which allows us to deduce an original expression of the multivariate covariance function at two distinct times, thereby extending existing results established for more restricted classes of Hawkes processes. Based on this expression, we analytically decompose the covariance at two distinct times into singular and continuous parts. We finish with brief numerical elements: We present a simple scheme for numerical approximations of the Laplace transform and the first two moments, and give examples of solutions of the different related integral equations. We also provides illustrative simulations of the multivariate Hawkes process for different model specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15370v1</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bartholom\'e Vieille (MIA Paris-Saclay), Rachid Senoussi (BioSP), Samuel Soubeyrand</dc:creator>
    </item>
    <item>
      <title>Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces</title>
      <link>https://arxiv.org/abs/2507.15741</link>
      <description>arXiv:2507.15741v1 Announce Type: cross 
Abstract: This paper introduces a framework for uncertainty quantification in regression models defined in metric spaces. Leveraging a newly defined notion of homoscedasticity, we develop a conformal prediction algorithm that offers finite-sample coverage guarantees and fast convergence rates of the oracle estimator. In heteroscedastic settings, we forgo these non-asymptotic guarantees to gain statistical efficiency, proposing a local $k$--nearest--neighbor method without conformal calibration that is adaptive to the geometry of each particular nonlinear space. Both procedures work with any regression algorithm and are scalable to large data sets, allowing practitioners to plug in their preferred models and incorporate domain expertise. We prove consistency for the proposed estimators under minimal conditions. Finally, we demonstrate the practical utility of our approach in personalized--medicine applications involving random response objects such as probability distributions and graph Laplacians.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15741v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G\'abor Lugosi, Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>Identifying Conditional Causal Effects in MPDAGs</title>
      <link>https://arxiv.org/abs/2507.15842</link>
      <description>arXiv:2507.15842v1 Announce Type: cross 
Abstract: We consider identifying a conditional causal effect when a graph is known up to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG represents an equivalence class of graphs that is restricted by background knowledge and where all variables in the causal model are observed. We provide three results that address identification in this setting: an identification formula when the conditioning set is unaffected by treatment, a generalization of the well-known do calculus to the MPDAG setting, and an algorithm that is complete for identifying these conditional effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15842v1</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara LaPlante, Emilija Perkovi\'c</dc:creator>
    </item>
    <item>
      <title>Improving Precision of RCT-Based CATE Estimation using Data Borrowing with Double Calibration</title>
      <link>https://arxiv.org/abs/2306.17478</link>
      <description>arXiv:2306.17478v2 Announce Type: replace 
Abstract: Understanding how treatment effects vary across patient characteristics is essential for personalized medicine, yet randomized controlled trials (RCTs) are often underpowered to detect heterogeneous treatment effects (HTEs). We propose a framework that improves the efficiency of conditional average treatment effect (CATE) estimation in RCTs by leveraging large observational studies (OS) while preserving the unbiasedness of RCT estimates. By framing CATE estimation as a supervised learning problem, we show that estimation variance is minimized using the counterfactual mean outcome (CMO) as an augmentation function. We derive finite-sample error bounds and establish conditions under which OS data improves CMO estimation, and thus CATE efficiency, even in the presence of confounding in the OS or outcome distribution shifts between populations. We introduce R-OSCAR (Robust Observational Studies for CMO-Augmented RCT), a two-stage estimator that calibrates OS outcome predictions to the RCT population and corrects residual biases through regularized regression. Simulations show that R-OSCAR can reduce the RCT sample size needed for HTE detection by up to 75%, maintaining robustness to model misspecification. Application to the Tennessee STAR study confirms these efficiency gains. Our framework offers a principled approach to integrating observational and experimental data using tools from statistical learning and transfer learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.17478v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amir Asiaee, Chiara Di Gravio, Cole Beck, Yuting Mei, Samhita Pal, Jared D. Huling</dc:creator>
    </item>
    <item>
      <title>Multiscale Quantile Regression with Local Error Control</title>
      <link>https://arxiv.org/abs/2403.11356</link>
      <description>arXiv:2403.11356v2 Announce Type: replace 
Abstract: For robust and efficient detection of change points, we introduce a novel methodology MUSCLE (multiscale quantile segmentation controlling local error) that partitions serial data into multiple segments, each sharing a common quantile. It leverages multiple tests for quantile changes over different scales and locations, and variational estimation. Unlike the often adopted global error control, MUSCLE focuses on local errors defined on individual segments, significantly improving detection power in finding change points. Meanwhile, due to the built-in model complexity penalty, it enjoys the finite sample guarantee that its false discovery rate (or the expected proportion of falsely detected change points) is upper bounded by its unique tuning parameter. Further, we obtain the consistency and the localisation error rates in estimating change points, under mild signal-to-noise-ratio conditions. Both match (up to log factors) the minimax optimality results in the Gaussian setup. All theories hold under the only distributional assumption of serial independence. Incorporating the wavelet tree data structure, we develop an efficient dynamic programming algorithm for computing MUSCLE. Extensive simulations as well as real data applications in electrophysiology and geophysics demonstrate its competitiveness and effectiveness. An implementation via R package muscle is available from GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11356v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Liu, Housen Li</dc:creator>
    </item>
    <item>
      <title>Detecting and Understanding the Difference between Natural Mediation Effects and Their Randomized Interventional Analogues</title>
      <link>https://arxiv.org/abs/2407.02671</link>
      <description>arXiv:2407.02671v2 Announce Type: replace 
Abstract: In causal mediation analysis, the natural direct and indirect effects (natural effects) are nonparametrically unidentifiable in the presence of treatment-induced confounding, which motivated the development of randomized interventional analogues (RIAs) of the natural effects. Being easier to identify, the RIAs are becoming widely used in practice. However, applied researchers often interpret RIA estimates as if they were the natural effects, even though the RIAs can be poor proxies for the natural effects. This calls for practical and theoretical guidance on when the RIAs differ from or coincide with the natural effects. We develop the first empirical test to detect the divergence between the natural effects and their RIAs under the weak assumptions sufficient for identifying the RIAs and illustrate the test using the Moving to Opportunity Study. We also provide new theoretical insights on the relationship between the natural effects and the RIAs both using a covariance formulation and from a structural equation perspective. This analysis also reveals previously undocumented connections between the natural effects, the RIAs, and estimands in instrumental variable analysis and Wilcoxon-Mann-Whitney tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02671v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ang Yu, Li Ge, Felix Elwert</dc:creator>
    </item>
    <item>
      <title>Spectral Differential Network Analysis for High-Dimensional Time Series</title>
      <link>https://arxiv.org/abs/2412.07905</link>
      <description>arXiv:2412.07905v2 Announce Type: replace 
Abstract: Spectral networks derived from multivariate time series data arise in many domains, from brain science to Earth science. Often, it is of interest to study how these networks change under different conditions. For instance, to better understand epilepsy, it would be interesting to capture the changes in the brain connectivity network as a patient experiences a seizure, using electroencephalography data. A common approach relies on estimating the networks in each condition and calculating their difference. Such estimates may behave poorly in high dimensions as the networks themselves may not be sparse in structure while their difference may be. We build upon this observation to develop an estimator of the difference in inverse spectral densities across two conditions. Using an L1 penalty on the difference, consistency is established by only requiring the difference to be sparse. We illustrate the method on synthetic data experiments and on experiments with electroencephalography data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07905v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Hellstern, Byol Kim, Zaid Harchaoui, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Robust Local Polynomial Regression with Similarity Kernels</title>
      <link>https://arxiv.org/abs/2501.10729</link>
      <description>arXiv:2501.10729v2 Announce Type: replace 
Abstract: Local Polynomial Regression (LPR) is a widely used nonparametric method for modeling complex relationships due to its flexibility and simplicity. It estimates a regression function by fitting low-degree polynomials to localized subsets of the data, weighted by proximity. However, traditional LPR is sensitive to outliers and high-leverage points, which can significantly affect estimation accuracy. This paper revisits the kernel function used to compute regression weights and proposes a novel framework that incorporates both predictor and response variables in the weighting mechanism. The focus of this work is a conditional density kernel that robustly estimates weights by mitigating the influence of outliers through localized density estimation. A related joint density kernel is also discussed in an appendix. The proposed method is implemented in Python and is publicly available at https://github.com/yaniv-shulman/rsklpr, demonstrating competitive performance in synthetic benchmark experiments. Compared to standard LPR, the proposed approach consistently improves robustness and accuracy, especially in heteroscedastic and noisy environments, without requiring multiple iterations. This advancement provides a promising extension to traditional LPR, opening new possibilities for robust regression applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10729v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaniv Shulman</dc:creator>
    </item>
    <item>
      <title>Overcoming data challenges through enriched validation and targeted sampling to measure whole-person health in electronic health records</title>
      <link>https://arxiv.org/abs/2502.05380</link>
      <description>arXiv:2502.05380v4 Announce Type: replace 
Abstract: The allostatic load index (ALI) is a 10-component measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in learning health systems; however, these data are prone to missingness and errors. Validation (e.g., through chart reviews) provides better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of ALI and healthcare utilization. Employing semiparametric maximum likelihood estimation, we robustly incorporate all available patient information into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote EHR data quality and completeness. Chart reviews uncovered few errors (99% matched source documents) and recovered some missing data through auxiliary information in patients' charts. On average, validation increased the number of non-missing ALI components per patient from 6 to 7. Through simulations based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Incorporating validation data, statistical models indicated that worse whole-person health (higher ALI) was associated with higher odds of engaging in the healthcare system, adjusting for age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05380v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Sheetal Kedar, Rabeya Tahir, Aidan D. Keleghan, Amelia Miranda, Stephany N. Duda, Michael P. Bancks, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon</dc:creator>
    </item>
    <item>
      <title>Dynamic spectral co-clustering of directed networks to unveil latent community paths in VAR-type models</title>
      <link>https://arxiv.org/abs/2502.10849</link>
      <description>arXiv:2502.10849v3 Announce Type: replace 
Abstract: Identifying network Granger causality in large vector autoregressive (VAR) models enhances explanatory power by capturing complex dependencies among variables. This study proposes a methodology that explores latent community structures to uncover underlying network dynamics, rather than relying on sparse coefficient estimation for network construction. A dynamic network framework embeds directed connectivity in the transition matrices of VAR-type models, allowing the tracking of evolving community structures over time, called seasons. To account for network directionality, degree-corrected stochastic co-block models are fitted for each season, then a combination of spectral co-clustering and singular vector smoothing is utilized to refine transitions between latent communities. Periodic VAR (PVAR) and vector heterogeneous autoregressive (VHAR) models are adopted as alternatives to conventional VAR models for dynamic network construction. Theoretical results establish the validity of the proposed methodology, while empirical analyses demonstrate its effectiveness in capturing both the cyclic evolution and transient trajectories of latent communities. The proposed approach is applied to US nonfarm payroll employment data and realized stock market volatility data. Spectral co-clustering of multi-layered directed networks, constructed from high-dimensional PVAR and VHAR representations, reveals rich and dynamic latent community structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10849v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Changryong Baek</dc:creator>
    </item>
    <item>
      <title>Boosting Prediction with Data Missing Not at Random</title>
      <link>https://arxiv.org/abs/2502.21276</link>
      <description>arXiv:2502.21276v2 Announce Type: replace 
Abstract: Boosting has emerged as a useful machine learning technique over the past three decades, attracting increased attention. Most advancements in this area, however, have primarily focused on numerical implementation procedures, often lacking rigorous theoretical justifications. Moreover, these approaches are generally designed for datasets with fully observed data, and their validity can be compromised by the presence of missing observations. In this paper, we employ semiparametric estimation approaches to develop boosting prediction methods for data with missing responses. We explore two strategies for adjusting the loss functions to account for missingness effects. The proposed methods are implemented using a functional gradient descent algorithm, and their theoretical properties, including algorithm convergence and estimator consistency, are rigorously established. Numerical studies demonstrate that the proposed methods perform well in finite sample settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21276v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Bian, Grace Y. Yi, Wenqing He</dc:creator>
    </item>
    <item>
      <title>Empowering Multi-class Classification for Multivariate Functional Data with Simultaneous Feature Selection</title>
      <link>https://arxiv.org/abs/2503.03679</link>
      <description>arXiv:2503.03679v2 Announce Type: replace 
Abstract: The opportunity to utilize complex functional data types for conducting classification tasks is emerging with the growing availability of imaging data. However, the tools capable of effectively managing imaging data are limited, let alone those that can further leverage other one-dimensional functional data. Inspired by the extensive data provided by the Alzheimer's Disease Neuroimaging Initiative (ADNI), we introduce a novel classifier tailored for complex functional data. Each observation in this framework may be associated with numerous functional processes, varying in dimensions, such as curves and images. Each predictor is a random element in an infinite dimensional function space, and the number of functional predictors p can potentially be much greater than the sample size n. In this paper, we introduce a novel and scalable classifier termed functional BIC deep neural network. By adopting a sparse deep Rectified Linear Unit network architecture and incorporating the LassoNet algorithm, the proposed unified model performs feature selection and classification simultaneously, which is contrast to the existing functional data classifiers. The challenge arises from the complex inter-correlation structures among multiple functional processes, and at meanwhile without any assumptions on the distribution of these processes. Simulation study and real data application are carried out to demonstrate its favorable performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03679v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuoyang Wang, Guanqun Cao, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>Assessing the Impact of Covariate Distribution and Positivity Violation on Weighting-Based Indirect Comparisons: a Simulation Study</title>
      <link>https://arxiv.org/abs/2507.12241</link>
      <description>arXiv:2507.12241v2 Announce Type: replace 
Abstract: Population-Adjusted Indirect Comparisons (PAICs) are used to estimate treatment effects when direct comparisons are infeasible and individual patient data (IPD) are only available for one trial. Among PAIC methods, Matching-Adjusted Indirect Comparison (MAIC) is the most widely used. However, little is known about how MAIC performs under challenging conditions such as limited covariate overlap or markedly non-normal covariate distributions.
  We conducted a Monte Carlo simulation study comparing three estimators: (i) MAIC matching first moment (MAIC-1), (ii) MAIC matching first and second moments (MAIC-2), and (iii) a benchmark method leveraging full IPD -- Propensity Score Weighting (PSW). We examined eight scenarios ranging from ideal conditions to situations with positivity violations and non-normal (including bimodal) covariate distributions. We assessed both anchored and unanchored estimators and examined the impact of adjustment model misspecification. We also applied these estimators to real-world data from the AKIKI and AKIKI-2 trials, comparing renal replacement therapy strategies in critically ill patients.
  MAIC-1 demonstrated robust performance, remaining unbiased in the presence of moderate positivity violations and non-normal covariates, while MAIC-2 and PSW appeared more sensitive to positivity violations. All methods showed substantial bias when key confounders were omitted, emphasizing the importance of correct model specification. In real-world data, a consistent trend was found with MAIC-1 showing narrower confidence intervals with positivity violation.
  Our findings support the cautious use of unanchored MAICs and highlight MAIC-1's resilience across moderate violations of assumptions. However, the method's limited flexibility underscores the need for careful use in real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12241v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnaud Serret-Larmande, J\'er\^ome Lambert, St\'ephane Gaudry, David Hajage</dc:creator>
    </item>
    <item>
      <title>Statistical learning for constrained functional parameters in infinite-dimensional models</title>
      <link>https://arxiv.org/abs/2404.09847</link>
      <description>arXiv:2404.09847v2 Announce Type: replace-cross 
Abstract: We develop a general framework for estimating function-valued parameters under equality or inequality constraints in infinite-dimensional statistical models. Such constrained learning problems are common across many areas of statistics and machine learning, where estimated parameters must satisfy structural requirements such as moment restrictions, policy benchmarks, calibration criteria, or fairness considerations. To address these problems, we characterize the solution as the minimizer of a penalized population risk using a Lagrange-type formulation, and analyze it through a statistical functional lens. Central to our approach is a constraint-specific path through the unconstrained parameter space that defines the constrained solutions. For a broad class of constraint-risk pairs, this path admits closed-form expressions and reveals how constraints shape optimal adjustments. When closed forms are unavailable, we derive recursive representations that support tractable estimation. Our results also suggest natural estimators of the constrained parameter, constructed by combining estimates of unconstrained components of the data-generating distribution. Thus, our procedure can be integrated with any statistical learning approach and implemented using standard software. We provide general conditions under which the resulting estimators achieve optimal risk and constraint satisfaction, and we demonstrate the flexibility and effectiveness of the proposed method through various examples, simulations, and real-data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09847v2</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Razieh Nabi, Nima S. Hejazi, Mark J. van der Laan, David Benkeser</dc:creator>
    </item>
    <item>
      <title>Decompounding Under General Mixing Distributions</title>
      <link>https://arxiv.org/abs/2405.05419</link>
      <description>arXiv:2405.05419v2 Announce Type: replace-cross 
Abstract: This study focuses on statistical inference for compound models of the form $X=\xi_1+\ldots+\xi_N$, where $N$ is a random variable denoting the count of summands, which are independent and identically distributed (i.i.d.) random variables $\xi_1, \xi_2, \ldots$. The paper addresses the problem of reconstructing the distribution of $\xi$ from observed samples of $X$'s distribution, a process referred to as decompounding, with the assumption that $N$'s distribution is known. This work diverges from the conventional scope by not limiting $N$'s distribution to the Poisson type, thus embracing a broader context. We propose a nonparametric estimate for the density of $\xi$, derive its rates of convergence and prove that these rates are minimax optimal for suitable classes of distributions for $\xi$ and $N$. Finally, we illustrate the numerical performance of the algorithm on simulated examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05419v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Denis Belomestny, Ekaterina Morozova, Vladimir Panov</dc:creator>
    </item>
    <item>
      <title>Survey Data Integration for Distribution Function Estimation</title>
      <link>https://arxiv.org/abs/2409.14284</link>
      <description>arXiv:2409.14284v2 Announce Type: replace-cross 
Abstract: Integration of probabilistic and non-probabilistic samples for the estimation of finite population totals (or means) has recently received considerable attention in the field of survey sampling; yet, to the best of our knowledge, this framework has not been extended to cumulative distribution function (CDF) estimation. To address this gap, we propose a novel CDF estimator that integrates data from probability samples with data from, potentially big, nonprobability samples. Assuming that a set of shared covariates are observed in both, while the response variable is observed only in the latter, the proposed estimator uses a survey-weighted empirical CDF of regression residuals trained on the convenience sample to estimate the CDF of the response variable. Under some assumptions, we derive the asymptotic bias and variance of our CDF estimator and show that it is asymptotically unbiased for the finite population CDF if ignorability holds. Our empirical results imply that the proposed CDF estimator is robust to model misspecification under ignorability, and robust to ignorability under model misspecification; when both assumptions are violated, our residual-based CDF estimator still outperforms its `plug-in' mass imputation and naive siblings, albeit with noted decreases in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14284v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Flood, Sayed Mostafa</dc:creator>
    </item>
    <item>
      <title>Spatial Dependencies in Item Response Theory: Gaussian Process Priors for Geographic and Cognitive Measurement</title>
      <link>https://arxiv.org/abs/2507.09824</link>
      <description>arXiv:2507.09824v2 Announce Type: replace-cross 
Abstract: Measurement validity in Item Response Theory depends on appropriately modeling dependencies between items when these reflect meaningful theoretical structures rather than random measurement error. In ecological assessment, citizen scientists identifying species across geographic regions exhibit systematic spatial patterns in task difficulty due to environmental factors. Similarly, in Author Recognition Tests, literary knowledge organizes by genre, where familiarity with science fiction authors systematically predicts recognition of other science fiction authors. Current spatial Item Response Theory methods, represented by the 1PLUS, 2PLUS, and 3PLUS model family, address these dependencies but remain limited by (1) binary response restrictions, and (2) conditional autoregressive priors that impose rigid local correlation assumptions, preventing effective modeling of complex spatial relationships. Our proposed method, Spatial Gaussian Process Item Response Theory (SGP-IRT), addresses these limitations by replacing conditional autoregressive priors with flexible Gaussian process priors that adapt to complex dependency structures while maintaining principled uncertainty quantification. SGP-IRT accommodates polytomous responses and models spatial dependencies in both geographic and abstract cognitive spaces, where items cluster by theoretical constructs rather than physical proximity. Simulation studies demonstrate improved parameter recovery, particularly for item difficulty estimation. Empirical applications show enhanced recovery of meaningful difficulty surfaces and improved measurement precision across psychological, educational, and ecological research applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09824v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingya Huang, Soham Ghosh</dc:creator>
    </item>
  </channel>
</rss>
