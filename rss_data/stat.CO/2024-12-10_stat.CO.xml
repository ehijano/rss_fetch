<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Dec 2024 02:51:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quasi-likelihood-based EM algorithm for regime-switching SDE</title>
      <link>https://arxiv.org/abs/2412.06305</link>
      <description>arXiv:2412.06305v1 Announce Type: new 
Abstract: This paper considers estimating the parameters in a regime-switching stochastic differential equation(SDE) driven by Normal Inverse Gaussian(NIG) noise. The model under consideration incorporates a continuous-time finite state Markov chain to capture regime changes, enabling a more realistic representation of evolving market conditions or environmental factors. Although the continuous dynamics are typically observable, the hidden nature of the Markov chain introduces significant complexity, rendering standard likelihood-based methods less effective. To address these challenges, we propose an estimation algorithm designed for discrete, high-frequency observations, even when the Markov chain is not directly observed. Our approach integrates the Expectation-Maximization (EM) algorithm, which iteratively refines parameter estimates in the presence of latent variables, with a quasi-likelihood method adapted to NIG noise. Notably, this method can simultaneously estimate parameters within both the SDE coefficients and the driving noise. Simulation results are provided to evaluate the performance of the algorithm. These experiments demonstrate that the proposed method provides reasonable estimation under challenging conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06305v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuzhong Cheng, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>PolytopeWalk: Sparse MCMC Sampling over Polytopes</title>
      <link>https://arxiv.org/abs/2412.06629</link>
      <description>arXiv:2412.06629v1 Announce Type: new 
Abstract: High dimensional sampling is an important computational tool in statistics and other computational disciplines, with applications ranging from Bayesian statistical uncertainty quantification, metabolic modeling in systems biology to volume computation. We present $\textsf{PolytopeWalk}$, a new scalable Python library designed for uniform sampling over polytopes. The library provides an end-to-end solution, which includes preprocessing algorithms such as facial reduction and initialization methods. Six state-of-the-art MCMC algorithms on polytopes are implemented, including the Dikin, Vaidya, and John Walk. Additionally, we introduce novel sparse constrained formulations of these algorithms, enabling efficient sampling from sparse polytopes of the form $K_2 = \{x \in \mathbb{R}^d \ | \ Ax = b, x \succeq_k 0\}$. This implementation maintains sparsity in $A$, ensuring scalability to high dimensional settings $(d &gt; 10^5)$. We demonstrate the improved sampling efficiency and per-iteration cost on both Netlib datasets and structured polytopes. $\textsf{PolytopeWalk}$ is available at github.com/ethz-randomwalk/polytopewalk with documentation at polytopewalk.readthedocs.io .</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06629v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benny Sun, Yuansi Chen</dc:creator>
    </item>
    <item>
      <title>Improved estimation of the positive powers ordered restricted standard deviation of two normal populations</title>
      <link>https://arxiv.org/abs/2412.05620</link>
      <description>arXiv:2412.05620v1 Announce Type: cross 
Abstract: The present manuscript is concerned with component-wise estimation of the positive power of ordered restricted standard deviation of two normal populations with certain restrictions on the means. We propose several improved estimators under a general scale invariant bowl-shaped loss function. Also, we proposed a class of improved estimators. It has been shown that the boundary estimator of this class is a generalized Bayes. As an application, the improved estimators are obtained with respect to quadratic loss, entropy loss, and a symmetric loss function. We have conducted extensive Monte Carlo simulations to study and compare the risk performance of the proposed estimators. Finally, a real life data analysis is given to illustrate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05620v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Somnath Mondal, Lakshmi Kanta Patra</dc:creator>
    </item>
    <item>
      <title>A generalized Bayesian approach for high-dimensional robust regression with serially correlated errors and predictors</title>
      <link>https://arxiv.org/abs/2412.05673</link>
      <description>arXiv:2412.05673v1 Announce Type: cross 
Abstract: This paper presents a loss-based generalized Bayesian methodology for high-dimensional robust regression with serially correlated errors and predictors. The proposed framework employs a novel scaled pseudo-Huber (SPH) loss function, which smooths the well-known Huber loss, achieving a balance between quadratic and absolute linear loss behaviors. This flexibility enables the framework to accommodate both thin-tailed and heavy-tailed data effectively. The generalized Bayesian approach constructs a working likelihood utilizing the SPH loss that facilitates efficient and stable estimation while providing rigorous estimation uncertainty quantification for all model parameters. Notably, this allows formal statistical inference without requiring ad hoc tuning parameter selection while adaptively addressing a wide range of tail behavior in the errors. By specifying appropriate prior distributions for the regression coefficients -- e.g., ridge priors for small or moderate-dimensional settings and spike-and-slab priors for high-dimensional settings -- the framework ensures principled inference. We establish rigorous theoretical guarantees for the accurate estimation of underlying model parameters and the correct selection of predictor variables under sparsity assumptions for a wide range of data generating setups. Extensive simulation studies demonstrate the superiority of our approach compared to traditional quadratic and absolute linear loss-based Bayesian regression methods, highlighting its flexibility and robustness in high-dimensional and challenging data contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05673v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Chakraborty, Kshitij Khare, George Michailidis</dc:creator>
    </item>
    <item>
      <title>Leveraging Black-box Models to Assess Feature Importance in Unconditional Distribution</title>
      <link>https://arxiv.org/abs/2412.05759</link>
      <description>arXiv:2412.05759v1 Announce Type: cross 
Abstract: Understanding how changes in explanatory features affect the unconditional distribution of the outcome is important in many applications. However, existing black-box predictive models are not readily suited for analyzing such questions. In this work, we develop an approximation method to compute the feature importance curves relevant to the unconditional distribution of outcomes, while leveraging the power of pre-trained black-box predictive models. The feature importance curves measure the changes across quantiles of outcome distribution given an external impact of change in the explanatory features. Through extensive numerical experiments and real data examples, we demonstrate that our approximation method produces sparse and faithful results, and is computationally efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05759v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Zhou, Chunlin Li</dc:creator>
    </item>
    <item>
      <title>Large-sample analysis of cost functionals for inference under the coalescent</title>
      <link>https://arxiv.org/abs/2412.06004</link>
      <description>arXiv:2412.06004v1 Announce Type: cross 
Abstract: The coalescent is a foundational model of latent genealogical trees under neutral evolution, but suffers from intractable sampling probabilities. Methods for approximating these sampling probabilities either introduce bias or fail to scale to large sample sizes. We show that a class of cost functionals of the coalescent with recurrent mutation and a finite number of alleles converge to tractable processes in the infinite-sample limit. A particular choice of costs yields insight about importance sampling methods, which are a classical tool for coalescent sampling probability approximation. These insights reveal that the behaviour of coalescent importance sampling algorithms differs markedly from standard sequential importance samplers, with or without resampling. We conduct a simulation study to verify that our asymptotics are accurate for algorithms with finite (and moderate) sample sizes. Our results also facilitate the a priori optimisation of computational resource allocation for coalescent sequential importance sampling. We do not observe the same behaviour for importance sampling methods under the infinite sites model of mutation, which is regarded as a good and more tractable approximation of finite alleles mutation in most respects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06004v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martina Favero, Jere Koskela</dc:creator>
    </item>
    <item>
      <title>Stable and tempered stable distributions and processes: an overview toward trajectory simulation</title>
      <link>https://arxiv.org/abs/2412.06374</link>
      <description>arXiv:2412.06374v1 Announce Type: cross 
Abstract: Stable distributions are a celebrated class of probability laws used in various fields. The $\alpha$-stable process, and its exponentially tempered counterpart, the Classical Tempered Stable (CTS) process, are also prominent examples of L\'evy processes. Simulating these processes is critical for many applications, yet it remains computationally challenging, due to their infinite jump activity. This survey provides an overview of the key properties of these objects offering a roadmap for practitioners. The first part is a review of the stability property, sampling algorithms are provided along with numerical illustrations. Then CTS processes are presented, with the Baeumer-Meerschaert algorithm for increment simulation, and a computational analysis is provided with numerical illustrations across different time scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06374v1</guid>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taher Jalal</dc:creator>
    </item>
    <item>
      <title>Super-Efficient Exact Hamiltonian Monte Carlo for the von Mises Distribution</title>
      <link>https://arxiv.org/abs/2312.16546</link>
      <description>arXiv:2312.16546v2 Announce Type: replace 
Abstract: Markov Chain Monte Carlo algorithms, the method of choice to sample from generic high-dimensional distributions, are rarely used for continuous one-dimensional distributions, for which more effective approaches are usually available (e.g. rejection sampling). In this work we present a counter-example to this conventional wisdom for the von Mises distribution, a maximum-entropy distribution over the circle. We show that Hamiltonian Monte Carlo with Laplacian momentum has exactly solvable equations of motion and, with an appropriate travel time, the Markov chain has negative autocorrelation at odd lags for odd observables and yields a relative effective sample size bigger than one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16546v2</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.aml.2024.109284</arxiv:DOI>
      <arxiv:journal_reference>Applied Mathematics Letters, Volume 159, January 2025, 109284</arxiv:journal_reference>
      <dc:creator>Ari Pakman</dc:creator>
    </item>
    <item>
      <title>kendallknight: An R Package for Efficient Implementation of Kendall's Correlation Coefficient Computation</title>
      <link>https://arxiv.org/abs/2408.09618</link>
      <description>arXiv:2408.09618v5 Announce Type: replace 
Abstract: The kendallknight package introduces an efficient implementation of Kendall's correlation coefficient computation, significantly improving the processing time for large datasets without sacrificing accuracy. The kendallknight package, following Knight (1966) and posterior literature, reduces the computational complexity resulting in drastic reductions in computation time, transforming operations that would take minutes or hours into milliseconds or minutes, while maintaining precision and correctly handling edge cases and errors. The package is particularly advantageous in econometric and statistical contexts where rapid and accurate calculation of Kendall's correlation coefficient is desirable. Benchmarks demonstrate substantial performance gains over the base R implementation, especially for large datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09618v5</guid>
      <category>stat.CO</category>
      <category>cs.DS</category>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mauricio Vargas Sep\'ulveda</dc:creator>
    </item>
    <item>
      <title>Theoretical and Practical Limits of Kolmogorov-Zurbenko Periodograms with Dynamic Smoothing in Estimating Signal Frequencies</title>
      <link>https://arxiv.org/abs/2007.03031</link>
      <description>arXiv:2007.03031v3 Announce Type: replace-cross 
Abstract: This investigation establishes the theoretical and practical limits of Kolmogorov-Zurbenko periodograms with dynamic smoothing in their estimation of signal frequencies in terms of their sensitivity, accuracy, resolution, and robustness. While the DiRienzo-Zurbenko algorithm performs dynamic smoothing based on local variation in a periodogram, the Neagu-Zurbenko algorithm performs dynamic smoothing based on local departure from linearity in a periodogram. This article begins with a summary of the statistical foundations for both the DiRienzo-Zurbenko algorithm and the Neagu-Zurbenko algorithm, followed by instructions for accessing and utilizing these approaches within the R statistical program platform. Brief definitions, importance, statistical bases, theoretical and practical limits, and demonstrations are provided for their sensitivity, accuracy, resolution, and robustness in estimating signal frequencies. Next using a simulated time series in which two signals close in frequency are embedded in a significant level of random noise, the predictive power of these approaches are compared to the autoregressive integral moving average (ARIMA) approach, with support again garnered for their being robust when data is missing. Throughout, the article contrasts the limits of Kolmogorov-Zurbenko periodograms with dynamic smoothing to those of log-periodograms with static smoothing, while also comparing the performance of the DiRienzo-Zurbenko algorithm to that of the Neagu-Zurbenko algorithm. It concludes by delineating next steps to establish the precision with which Kolmogorov-Zurbenko periodograms with dynamic smoothing estimate signal strength.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.03031v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barry Loneck, Igor Zurbenko, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Quantile Fourier Transform, Quantile Series, and Nonparametric Estimation of Quantile Spectra</title>
      <link>https://arxiv.org/abs/2211.05844</link>
      <description>arXiv:2211.05844v2 Announce Type: replace-cross 
Abstract: A nonparametric method is proposed for estimating the quantile spectra and cross-spectra introduced in Li (2012; 2014) as bivariate functions of frequency and quantile level. The method is based on the quantile discrete Fourier transform (QDFT) defined by trigonometric quantile regression and the quantile series (QSER) defined by the inverse Fourier transform of the QDFT. A nonparametric spectral estimator is constructed from the autocovariance function of the QSER using the lag-window (LW) approach. Smoothing techniques are also employed to reduce the statistical variability of the LW estimator across quantiles when the underlying spectrum varies smoothly with respect to the quantile level. The performance of the proposed estimation method is evaluated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.05844v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ta-Hsin Li</dc:creator>
    </item>
    <item>
      <title>A flexible Bayesian g-formula for causal survival analyses with time-dependent confounding</title>
      <link>https://arxiv.org/abs/2402.02306</link>
      <description>arXiv:2402.02306v3 Announce Type: replace-cross 
Abstract: In longitudinal observational studies with time-to-event outcomes, a common objective in causal analysis is to estimate the causal survival curve under hypothetical intervention scenarios. The g-formula is a useful tool for this analysis. To enhance the traditional parametric g-formula, we developed an alternative g-formula estimator, which incorporates the Bayesian Additive Regression Trees (BART) into the modeling of the time-evolving generative components, aiming to mitigate the bias due to model misspecification. We focus on binary time-varying treatments and introduce a general class of g-formulas for discrete survival data that can incorporate the longitudinal balancing scores. The minimum sufficient formulation of these longitudinal balancing scores is linked to the nature of treatment strategies, i.e., static or dynamic. For each type of treatment strategy, we provide posterior sampling algorithms. We conducted simulations to illustrate the empirical performance of the proposed method and demonstrate its practical utility using data from the Yale New Haven Health System's electronic health records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02306v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Liangyuan Hu, Fan Li</dc:creator>
    </item>
    <item>
      <title>TopoX: A Suite of Python Packages for Machine Learning on Topological Domains</title>
      <link>https://arxiv.org/abs/2402.02441</link>
      <description>arXiv:2402.02441v5 Announce Type: replace-cross 
Abstract: We introduce TopoX, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. TopoX consists of three packages: TopoNetX facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; TopoEmbedX provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; TopoModelX is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of TopoX is available under MIT license at https://pyt-team.github.io/}{https://pyt-team.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02441v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MS</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mustafa Hajij, Mathilde Papillon, Florian Frantzen, Jens Agerberg, Ibrahem AlJabea, Rub\'en Ballester, Claudio Battiloro, Guillermo Bern\'ardez, Tolga Birdal, Aiden Brent, Peter Chin, Sergio Escalera, Simone Fiorellino, Odin Hoff Gardaa, Gurusankar Gopalakrishnan, Devendra Govil, Josef Hoppe, Maneel Reddy Karri, Jude Khouja, Manuel Lecha, Neal Livesay, Jan Mei{\ss}ner, Soham Mukherjee, Alexander Nikitin, Theodore Papamarkou, Jaro Pr\'ilepok, Karthikeyan Natesan Ramamurthy, Paul Rosen, Aldo Guzm\'an-S\'aenz, Alessandro Salatiello, Shreyas N. Samaga, Simone Scardapane, Michael T. Schaub, Luca Scofano, Indro Spinelli, Lev Telyatnikov, Quang Truong, Robin Walters, Maosheng Yang, Olga Zaghen, Ghada Zamzmi, Ali Zia, Nina Miolane</dc:creator>
    </item>
    <item>
      <title>Data Unfolding with Mean Integrated Square Error Optimization</title>
      <link>https://arxiv.org/abs/2402.12990</link>
      <description>arXiv:2402.12990v3 Announce Type: replace-cross 
Abstract: Experimental data in Particle and Nuclear physics, Particle Astrophysics and Radiation Protection Dosimetry are obtained from experimental facilities comprising a complex array of sensors, electronics and software. Computer simulation is used to study the measurement process. Probability Density Functions (PDFs) of measured physical parameters deviate from true PDFs due to resolution, bias, and efficiency effects. Good estimates of the true PDF are necessary for testing theoretical models, comparing results from different experiments, and combining results from various research endeavors. In the article, the histogram method is employed to estimate both the measured and true PDFs. The binning of histograms is determined using the K-means clustering algorithm. The true PDF is estimated through the maximization of the likelihood function with entropy regularization, utilizing a non-linear optimization algorithm specially designed for this purpose. The accuracy of the results is assessed using the Mean Integrated Square Error. To determine the optimal value for the regularization parameter, a bootstrap method is applied. Additionally, a mathematical model of the measurement system is formulated using system identification methods. This approach enhances the robustness and precision of the estimation process, providing a more reliable analysis of the system's characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12990v3</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolay D. Gagunashvili</dc:creator>
    </item>
  </channel>
</rss>
