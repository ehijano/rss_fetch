<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Jul 2025 01:39:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quantum-Resilient Privacy Ledger (QRPL): A Sovereign Digital Currency for the Post-Quantum Era</title>
      <link>https://arxiv.org/abs/2507.09067</link>
      <description>arXiv:2507.09067v1 Announce Type: new 
Abstract: The emergence of quantum computing presents profound challenges to existing cryptographic infrastructures, whilst the development of central bank digital currencies (CBDCs) has raised concerns regarding privacy preservation and excessive centralisation in digital payment systems. This paper proposes the Quantum-Resilient Privacy Ledger (QRPL) as an innovative token-based digital currency architecture that incorporates National Institute of Standards and Technology (NIST)-standardised post-quantum cryptography (PQC) with hash-based zero-knowledge proofs to ensure user sovereignty, scalability, and transaction confidentiality. Key contributions include adaptations of ephemeral proof chains for unlinkable transactions, a privacy-weighted Proof-of-Stake (PoS) consensus to promote equitable participation, and a novel zero-knowledge proof-based mechanism for privacy-preserving selective disclosure. QRPL aims to address critical shortcomings in prevailing CBDC designs, including risks of pervasive surveillance, with a 10-20 second block time to balance security and throughput in future monetary systems. While conceptual, empirical prototypes are planned. Future work includes prototype development to validate these models empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09067v1</guid>
      <category>cs.ET</category>
      <category>cs.CR</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Serhan W. Bahar</dc:creator>
    </item>
    <item>
      <title>Solving the compute crisis with physics-based ASICs</title>
      <link>https://arxiv.org/abs/2507.10463</link>
      <description>arXiv:2507.10463v1 Announce Type: new 
Abstract: Escalating artificial intelligence (AI) demands expose a critical "compute crisis" characterized by unsustainable energy consumption, prohibitive training costs, and the approaching limits of conventional CMOS scaling. Physics-based Application-Specific Integrated Circuits (ASICs) present a transformative paradigm by directly harnessing intrinsic physical dynamics for computation rather than expending resources to enforce idealized digital abstractions. By relaxing the constraints needed for traditional ASICs, like enforced statelessness, unidirectionality, determinism, and synchronization, these devices aim to operate as exact realizations of physical processes, offering substantial gains in energy efficiency and computational throughput. This approach enables novel co-design strategies, aligning algorithmic requirements with the inherent computational primitives of physical systems. Physics-based ASICs could accelerate critical AI applications like diffusion models, sampling, optimization, and neural network inference as well as traditional computational workloads like scientific simulation of materials and molecules. Ultimately, this vision points towards a future of heterogeneous, highly-specialized computing platforms capable of overcoming current scaling bottlenecks and unlocking new frontiers in computational power and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10463v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxwell Aifer, Zach Belateche, Suraj Bramhavar, Kerem Y. Camsari, Patrick J. Coles, Gavin Crooks, Douglas J. Durian, Andrea J. Liu, Anastasia Marchenkova, Antonio J. Martinez, Peter L. McMahon, Faris Sbahi, Benjamin Weiner, Logan G. Wright</dc:creator>
    </item>
    <item>
      <title>Central Bank Digital Currencies: A Survey</title>
      <link>https://arxiv.org/abs/2507.08880</link>
      <description>arXiv:2507.08880v1 Announce Type: cross 
Abstract: With the advancement of digital payment technologies, central banks worldwide have increasingly begun to explore the implementation of Central Bank Digital Currencies (CBDCs). This paper presents a comprehensive review of the latest developments in CBDC system design and implementation. By analyzing 135 research papers published between 2018 and 2025, the study provides an in-depth examination of CBDC design taxonomy and ecosystem frameworks. Grounded in the CBDC Design Pyramid, the paper refines and expands key architectural elements by thoroughly investigating innovations in ledger technologies, the selection of consensus mechanisms, and challenges associated with offline payments and digital wallet integration. Furthermore, it conceptualizes a CBDC ecosystem. A detailed comparative analysis of 26 existing CBDC systems is conducted across four dimensions: system architecture, ledger technology, access model, and application domain. The findings reveal that the most common configuration consists of a two-tier architecture, distributed ledger technology (DLT), and a token-based access model. However, no dominant trend has emerged regarding application domains. Notably, recent research shows a growing focus on leveraging CBDCs for cross-border payments to resolve inefficiencies and structural delays in current systems. Finally, the paper offers several forward-looking recommendations for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08880v1</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qifeng Tang, Yain-Whar Si</dc:creator>
    </item>
    <item>
      <title>The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations</title>
      <link>https://arxiv.org/abs/2507.08908</link>
      <description>arXiv:2507.08908v1 Announce Type: cross 
Abstract: Despite the widespread interest in machine learning (ML), the engineering industry has not yet fully adopted ML-based methods, which has left engineers and stakeholders uncertain about the legal and regulatory frameworks that govern their decisions. This gap remains unaddressed as an engineer's decision-making process, typically governed by professional ethics and practical guidelines, now intersects with complex algorithmic outputs. To bridge this gap, this paper explores how engineers can navigate legal principles and legislative justifications that support and/or contest the deployment of ML technologies. Drawing on recent precedents and experiences gained from other fields, this paper argues that analogical reasoning can provide a basis for embedding ML within existing engineering codes while maintaining professional accountability and meeting safety requirements. In exploring these issues, the discussion focuses on established liability doctrines, such as negligence and product liability, and highlights how courts have evaluated the use of predictive models. We further analyze how legislative bodies and standard-setting organizations can furnish explicit guidance equivalent to prior endorsements of emergent technologies. This exploration stresses the vitality of understanding the interplay between technical justifications and legal precedents for shaping an informed stance on ML's legitimacy in engineering practice. Finally, our analysis catalyzes a legal framework for integrating ML through which stakeholders can critically assess the responsibilities, liabilities, and benefits inherent in ML-driven engineering solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08908v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Z. Naser</dc:creator>
    </item>
    <item>
      <title>GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective</title>
      <link>https://arxiv.org/abs/2507.09495</link>
      <description>arXiv:2507.09495v1 Announce Type: cross 
Abstract: Multi-agent reinforcement learning faces fundamental challenges that conventional approaches have failed to overcome: exponentially growing joint action spaces, non-stationary environments where simultaneous learning creates moving targets, and partial observability that constrains coordination. Current methods remain reactive, employing stimulus-response mechanisms that fail when facing novel scenarios. We argue for a transformative paradigm shift from reactive to proactive multi-agent intelligence through generative AI-based reinforcement learning. This position advocates reconceptualizing agents not as isolated policy optimizers, but as sophisticated generative models capable of synthesizing complex multi-agent dynamics and making anticipatory decisions based on predictive understanding of future interactions. Rather than responding to immediate observations, generative-RL agents can model environment evolution, predict other agents' behaviors, generate coordinated action sequences, and engage in strategic reasoning accounting for long-term dynamics. This approach leverages pattern recognition and generation capabilities of generative AI to enable proactive decision-making, seamless coordination through enhanced communication, and dynamic adaptation to evolving scenarios. We envision this paradigm shift will unlock unprecedented possibilities for distributed intelligence, moving beyond individual optimization toward emergent collective behaviors representing genuine collaborative intelligence. The implications extend across autonomous systems, robotics, and human-AI collaboration, promising solutions to coordination challenges intractable under traditional reactive frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09495v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hang Wang, Junshan Zhang</dc:creator>
    </item>
    <item>
      <title>OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization</title>
      <link>https://arxiv.org/abs/2507.09682</link>
      <description>arXiv:2507.09682v1 Announce Type: cross 
Abstract: We propose a novel approach, OrQstrator, which is a modular framework for conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum (NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our orchestration engine intelligently selects among three complementary circuit optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count via learned rewrite sequences; a domain-specific optimizer that performs efficient local gate resynthesis and numeric optimization; a parameterized circuit instantiator that improves compilation by optimizing template circuits during gate set translation. These modules are coordinated by a central orchestration engine that learns coordination policies based on circuit structure, hardware constraints, and backend-aware performance features such as gate count, depth, and expected fidelity. The system outputs an optimized circuit for hardware-aware transpilation and execution, leveraging techniques from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt to backend constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09682v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laura Baird, Armin Moin</dc:creator>
    </item>
    <item>
      <title>SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning</title>
      <link>https://arxiv.org/abs/2507.10421</link>
      <description>arXiv:2507.10421v1 Announce Type: cross 
Abstract: School dropout is a serious problem in distance learning, where early detection is crucial for effective intervention and student perseverance. Predicting student dropout using available educational data is a widely researched topic in learning analytics. Our partner's distance learning platform highlights the importance of integrating diverse data sources, including socio-demographic data, behavioral data, and sentiment analysis, to accurately predict dropout risks. In this paper, we introduce a novel model that combines sentiment analysis of student comments using the Bidirectional Encoder Representations from Transformers (BERT) model with socio-demographic and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We fine-tuned BERT on student comments to capture nuanced sentiments, which were then merged with key features selected using feature importance techniques in XGBoost. Our model was tested on unseen data from the next academic year, achieving an accuracy of 84\%, compared to 82\% for the baseline model. Additionally, the model demonstrated superior performance in other metrics, such as precision and F1-score. The proposed method could be a vital tool in developing personalized strategies to reduce dropout rates and encourage student perseverance</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10421v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meriem Zerkouk, Miloud Mihoubi, Belkacem Chikhaoui</dc:creator>
    </item>
    <item>
      <title>An RRAM compute-in-memory architecture for high energy-efficient processing of binary matrix-vector multiplication in cryptography</title>
      <link>https://arxiv.org/abs/2501.10702</link>
      <description>arXiv:2501.10702v5 Announce Type: replace 
Abstract: Binary matrix-vector multiplication (BMVM) is a key operation in post-quantum cryptography schemes like the Classic McEliece cryptosystem. Conventional computing architectures incur significant energy efficiency loss due to data movement of large matrices when handling such tasks. Resistive memory (RRAM) non-volatile compute-in-memory (nvCIM) is an ideal technology for high energy-efficient BMVM processing but faces challenges, including signal margin degradation in high input-parallelism arrays due to device non-idealities and high hardware overhead from current readout and XOR operations. This work presents a RRAM nvCIM architecture featuring: 1) 1T1R cells with high-resistive-state compensation modules; and 2) pulsed current-sensing parity checkers. Based on the 180nm process and test results from RRAM devices, the computing accuracy and efficiency of the architecture are verified by simulation. The proposed architecture performs high-precision current accumulation with a maximum MAC value of 10 and achieves an energy efficiency of 1.51TOPS/W, offering approximately 1.62 times improvement compared to an advanced 28nm FPGA platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10702v5</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Yue, Yihao Chen, Tianhang Liang, Xiangrui Li, Xin Kong, Zhelong Jiang, Zhigang Li, Gang Chen, Huaxiang Lu</dc:creator>
    </item>
    <item>
      <title>Modulation of switching dynamics in magnetic tunnel junctions for low-error-rate computational random-access memory</title>
      <link>https://arxiv.org/abs/2505.14829</link>
      <description>arXiv:2505.14829v2 Announce Type: replace 
Abstract: The conventional computer architecture has been facing challenges answering the ever-increasing demands from emerging applications, such as AI, for energy-efficient computation and memory hardware systems. Computational Random Access Memory (CRAM) represents a true in-memory computing paradigm that integrates logic and memory functions within the same array. At its core, CRAM relies on Magnetic Tunnel Junctions (MTJs), which serve as the foundational building blocks for implementing both memory storage and logic operations. However, a key challenge in CRAM lies in the non-ideal error rates associated with switching dynamics of MTJs, necessitating innovative approaches to reduce errors and optimize logic margins. This work proposes a novel approach of utilizing the voltage-controlled magnetic anisotropy (VCMA) to steepen the switching probability transfer curve (SPTC), thereby significantly reducing the logic operation error rate in CRAM. Using several numerical modeling tools, we validate the effectiveness of VCMA in modulating the energy barrier and switching dynamics in MTJs. It is revealed that the VCMA effect significantly reduces the error rate of CRAM by 61.43% at a VCMA coefficient of 200 fJ/V/m compared to CRAM without VCMA. The reduction of error rate is further rapidly amplified with an increasing TMR ratio. Furthermore, the introduction of the VCMA effect decreases the logic voltage (Vlogic) required for logic operations in CRAM and results in reduction of energy consumption. Our work serves as a first exploration in reducing the error rate in CRAM by modifying SPTC in MTJs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14829v2</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Lv, Brahmdutta Dixit, Jian-Ping Wang</dc:creator>
    </item>
    <item>
      <title>Survival of the Optimized: An Evolutionary Approach to T-depth Reduction</title>
      <link>https://arxiv.org/abs/2504.09391</link>
      <description>arXiv:2504.09391v2 Announce Type: replace-cross 
Abstract: Quantum Error Correction (QEC) is the cornerstone of practical Fault-Tolerant Quantum Computing (FTQC), but incurs enormous resource overheads. Circuits must decompose into Clifford+T gates, and the non-transversal T gates demand costly magic-state distillation. As circuit complexity grows, sequential T-gate layers ("T-depth") increase, amplifying the spatiotemporal overhead of QEC. Optimizing T-depth is NP-hard, and existing greedy or brute-force strategies are either inefficient or computationally prohibitive. We frame T-depth reduction as a search optimization problem and present a Genetic Algorithm (GA) framework that approximates optimal layer-merge patterns across the non-convex search space. We introduce a mathematical formulation of the circuit expansion for systematic layer reordering and a greedy initial merge-pair selection, accelerating the convergence and enhancing the solution quality. In our benchmark with ~90-100 qubits, our method reduces T-depth by 79.23% and overall T-count by 41.86%. Compared to the reversible circuit benchmarks, we achieve a 2.58x improvement in T-depth over the state-of-the-art methods, demonstrating its viability for near-term FTQC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09391v2</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Archisman Ghosh, Avimita Chatterjee, Swaroop Ghosh</dc:creator>
    </item>
  </channel>
</rss>
