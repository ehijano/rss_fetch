<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 02:36:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multiple testing in multi-stream sequential change detection</title>
      <link>https://arxiv.org/abs/2501.04130</link>
      <description>arXiv:2501.04130v1 Announce Type: new 
Abstract: Multi-stream sequential change detection involves simultaneously monitoring many streams of data and trying to detect when their distributions change, if at all. Here, we theoretically study multiple testing issues that arise from detecting changes in many streams. We point out that any algorithm with finite average run length (ARL) must have a trivial worst-case false detection rate (FDR), family-wise error rate (FWER), and per-family error rate (PFER); thus, any attempt to control these Type I error metrics is fundamentally in conflict with the desire for a finite ARL. One of our contributions is to define a new class of metrics which can be controlled, called error over patience (EOP). We propose algorithms that combine the recent e-detector framework (which generalizes the Shiryaev-Roberts and CUSUM methods) with the recent e-Benjamini-Hochberg procedure and e-Bonferroni procedures. We prove that these algorithms control the EOP at any desired level under very general dependence structures on the data within and across the streams. In fact, we prove a more general error control that holds uniformly over all stopping times and provides a smooth trade-off between the conflicting metrics. Additionally, if finiteness of the ARL is forfeited, we show that our algorithms control the Type I error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04130v1</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Statistical estimation of a mean-field FitzHugh-Nagumo model</title>
      <link>https://arxiv.org/abs/2501.04257</link>
      <description>arXiv:2501.04257v1 Announce Type: new 
Abstract: We consider an interacting system of particles with value in $\mathbb{R}^d \times \mathbb{R}^d$, governed by transport and diffusion on the first component, on that may serve as a representative model for kinetic models with a degenerate component. In a first part, we control the fluctuations of the empirical measure of the system around the solution of the corresponding Vlasov-Fokker-Planck equation by proving a Bernstein concentration inequality, extending a previous result of arXiv:2011.03762 in several directions. In a second part, we study the nonparametric statistical estimation of the classical solution of Vlasov-Fokker-Planck equation from the observation of the empirical measure and prove an oracle inequality using the Goldenshluger-Lepski methodology and we obtain minimax optimality. We then specialise on the FitzHugh-Nagumo model for populations of neurons. We consider a version of the model proposed in Mischler et al. arXiv:1503.00492 an optimally estimate the $6$ parameters of the model by moment estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04257v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudia Fonte Sanchez, Marc Hoffmann</dc:creator>
    </item>
    <item>
      <title>Totally Concave Regression</title>
      <link>https://arxiv.org/abs/2501.04360</link>
      <description>arXiv:2501.04360v1 Announce Type: new 
Abstract: Shape constraints offer compelling advantages in nonparametric regression by enabling the estimation of regression functions under realistic assumptions, devoid of tuning parameters. However, most existing shape-constrained nonparametric regression methods, except additive models, impose too few restrictions on the regression functions. This often leads to suboptimal performance, such as overfitting, in multivariate contexts due to the curse of dimensionality. On the other hand, additive shape-constrained models are sometimes too restrictive because they fail to capture interactions among the covariates. In this paper, we introduce a novel approach for multivariate shape-constrained nonparametric regression, which allows interactions without suffering from the curse of dimensionality. Our approach is based on the notion of total concavity originally due to T. Popoviciu and recently described in Gal [24]. We discuss the characterization and computation of the least squares estimator over the class of totally concave functions and derive rates of convergence under standard assumptions. The rates of convergence depend on the number of covariates only logarithmically, and the estimator, therefore, is guaranteed to avoid the curse of dimensionality to some extent. We demonstrate that total concavity can be justified for many real-world examples and validate the efficacy of our approach through empirical studies on various real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04360v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dohyeong Ki, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>Correlation tests and sample spectral coherence matrix in the high-dimensional regime</title>
      <link>https://arxiv.org/abs/2501.04371</link>
      <description>arXiv:2501.04371v1 Announce Type: new 
Abstract: It is established that the linear spectral statistics (LSS) of the smoothed periodogram estimate of the spectral coherence matrix of a complex Gaussian high-dimensional times series (yn) n$\in$Z with independent components satisfy at each frequency a central limit theorem in the asymptotic regime where the sample size N , the dimension M of the observation, and the smoothing span B both converge towards +$\infty$ in such a way that M = O(N $\alpha$ ) for $\alpha$ \&lt; 1 and M B $\rightarrow$ c, c $\in$ (0, 1). It is deduced that two recentered and renormalized versions of the LSS, one based on an average in the frequency domain and the other one based on a sum of squares also in the frequency domain, and both evaluated over a well-chosen frequency grid, also verify a central limit theorem. These two statistics are proposed to test with controlled asymptotic level the hypothesis that the components of y are independent. Numerical simulations assess the performance of the two tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04371v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe Loubaton (LIGM), Alexis Rosuel (LIGM), Pascal Vallet (IMS)</dc:creator>
    </item>
    <item>
      <title>Choosing the Right Norm for Change Point Detection in Functional Data</title>
      <link>https://arxiv.org/abs/2501.04476</link>
      <description>arXiv:2501.04476v1 Announce Type: new 
Abstract: We consider the problem of detecting a change point in a sequence of mean functions from a functional time series. We propose an $L^1$ norm based methodology and establish its theoretical validity both for classical and for relevant hypotheses. We compare the proposed method with currently available methodology that is based on the $L^2$ and supremum norms. Additionally we investigate the asymptotic behaviour under the alternative for all three methods and showcase both theoretically and empirically that the $L^1$ norm achieves the best performance in a broad range of scenarios. We also propose a power enhancement component that improves the performance of the $L^1$ test against sparse alternatives. Finally we apply the proposed methodology to both synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04476v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Bastian</dc:creator>
    </item>
    <item>
      <title>Exact recovery in the double sparse model: sufficient and necessary signal conditions</title>
      <link>https://arxiv.org/abs/2501.04551</link>
      <description>arXiv:2501.04551v1 Announce Type: new 
Abstract: The double sparse linear model, which has both group-wise and element-wise sparsity in regression coefficients, has attracted lots of attention recently. This paper establishes the sufficient and necessary relationship between the exact support recovery and the optimal minimum signal conditions in the double sparse model. Specifically, sharply under the proposed signal conditions, a two-stage double sparse iterative hard thresholding procedure achieves exact support recovery with a suitably chosen threshold parameter. Also, this procedure maintains asymptotic normality aligning with an OLS estimator given true support, hence holding the oracle properties. Conversely, we prove that no method can achieve exact support recovery if these signal conditions are violated. This fills a critical gap in the minimax optimality theory on support recovery of the double sparse model. Finally, numerical experiments are provided to support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04551v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shixiang Liu, Zhifan Li, Yanhang Zhang, Jianxin Yin</dc:creator>
    </item>
    <item>
      <title>Sobol' Matrices For Multi-Output Models With Quantified Uncertainty</title>
      <link>https://arxiv.org/abs/2501.04602</link>
      <description>arXiv:2501.04602v1 Announce Type: new 
Abstract: Variance based global sensitivity analysis measures the relevance of inputs to a single output using Sobol' indices. This paper extends the definition in a natural way to multiple outputs, directly measuring the relevance of inputs to the linkages between outputs in a correlation-like matrix of indices. The usual Sobol' indices constitute the diagonal of this matrix. Existence, uniqueness and uncertainty quantification are established by developing the indices from a putative multi-output model with quantified uncertainty. Sobol' matrices and their standard errors are related to the moments of the multi-output model, to enable calculation. These are benchmarked numerically against test functions (with added noise) whose Sobol' matrices are calculated analytically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04602v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert A. Milton, Solomon F. Brown</dc:creator>
    </item>
    <item>
      <title>Mixing Times and Privacy Analysis for the Projected Langevin Algorithm under a Modulus of Continuity</title>
      <link>https://arxiv.org/abs/2501.04134</link>
      <description>arXiv:2501.04134v1 Announce Type: cross 
Abstract: We study the mixing time of the projected Langevin algorithm (LA) and the privacy curve of noisy Stochastic Gradient Descent (SGD), beyond nonexpansive iterations. Specifically, we derive new mixing time bounds for the projected LA which are, in some important cases, dimension-free and poly-logarithmic on the accuracy, closely matching the existing results in the smooth convex case. Additionally, we establish new upper bounds for the privacy curve of the subsampled noisy SGD algorithm. These bounds show a crucial dependency on the regularity of gradients, and are useful for a wide range of convex losses beyond the smooth case. Our analysis relies on a suitable extension of the Privacy Amplification by Iteration (PABI) framework (Feldman et al., 2018; Altschuler and Talwar, 2022, 2023) to noisy iterations whose gradient map is not necessarily nonexpansive. This extension is achieved by designing an optimization problem which accounts for the best possible R\'enyi divergence bound obtained by an application of PABI, where the tractability of the problem is crucially related to the modulus of continuity of the associated gradient mapping. We show that, in several interesting cases -- including the nonsmooth convex, weakly smooth and (strongly) dissipative -- such optimization problem can be solved exactly and explicitly. This yields the tightest possible PABI-based bounds, where our results are either new or substantially sharper than those in previous works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04134v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Bravo, Juan P. Flores-Mella, Crist\'obal Guzm\'an</dc:creator>
    </item>
    <item>
      <title>ART: Distribution-Free and Model-Agnostic Changepoint Detection with Finite-Sample Guarantees</title>
      <link>https://arxiv.org/abs/2501.04475</link>
      <description>arXiv:2501.04475v1 Announce Type: cross 
Abstract: We introduce ART, a distribution-free and model-agnostic framework for changepoint detection that provides finite-sample guarantees. ART transforms independent observations into real-valued scores via a symmetric function, ensuring exchangeability in the absence of changepoints. These scores are then ranked and aggregated to detect distributional changes. The resulting test offers exact Type-I error control, agnostic to specific distributional or model assumptions. Moreover, ART seamlessly extends to multi-scale settings, enabling robust multiple changepoint estimation and post-detection inference with finite-sample error rate control. By locally ranking the scores and performing aggregations across multiple prespecified intervals, ART identifies changepoint intervals and refines subsequent inference while maintaining its distribution-free and model-agnostic nature. This adaptability makes ART as a reliable and versatile tool for modern changepoint analysis, particularly in high-dimensional data contexts and applications leveraging machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04475v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolong Cui, Haoyu Geng, Guanghui Wang, Zhaojun Wang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Sparse free deconvolution under unknown noise level via eigenmatrix</title>
      <link>https://arxiv.org/abs/2501.04599</link>
      <description>arXiv:2501.04599v1 Announce Type: cross 
Abstract: This note considers the spectral estimation problems of sparse spectral measures under unknown noise levels. The main technical tool is the eigenmatrix method for solving unstructured sparse recovery problems. When the noise level is determined, the free deconvolution reduces the problem to an unstructured sparse recovery problem to which the eigenmatrix method can be applied. To determine the unknown noise level, we propose an optimization problem based on the singular values of an intermediate matrix of the eigenmatrix method. Numerical results are provided for both the additive and multiplicative free deconvolutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04599v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lexing Ying</dc:creator>
    </item>
    <item>
      <title>Doubly Robust and Efficient Calibration of Prediction Sets for Censored Time-to-Event Outcomes</title>
      <link>https://arxiv.org/abs/2501.04615</link>
      <description>arXiv:2501.04615v1 Announce Type: cross 
Abstract: Our objective is to construct well-calibrated prediction sets for a time-to-event outcome subject to right-censoring with guaranteed coverage. Our approach is inspired by modern conformal inference literature, in that, unlike classical frameworks, we obviate the need for a well-specified parametric or semi-parametric survival model to accomplish our goal. In contrast to existing conformal prediction methods for survival data, which restrict censoring to be of Type I, whereby potential censoring times are assumed to be fully observed on all units in both training and validation samples, we consider the more common right-censoring setting in which either only the censoring time or only the event time of primary interest is directly observed, whichever comes first. Under a standard conditional independence assumption between the potential survival and censoring times given covariates, we propose and analyze two methods to construct valid and efficient lower predictive bounds for the survival time of a future observation. The proposed methods build upon modern semiparametric efficiency theory for censored data, in that the first approach incorporates inverse-probability-of-censoring weighting (IPCW), while the second approach is based on augmented-inverse-probability-of-censoring weighting (AIPCW). For both methods, we formally establish asymptotic coverage guarantees, and demonstrate both via theory and empirical experiments that AIPCW substantially improves efficiency over IPCW in the sense that its coverage error bound is of second-order mixed bias type, that is \emph{doubly robust}, and therefore guaranteed to be asymptotically negligible relative to the coverage error of IPCW.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04615v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rebecca Farina, Arun Kumar Kuchibhotla, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>A Statistical Theory of Contrastive Pre-training and Multimodal Generative AI</title>
      <link>https://arxiv.org/abs/2501.04641</link>
      <description>arXiv:2501.04641v1 Announce Type: cross 
Abstract: Multi-modal generative AI systems, such as those combining vision and language, rely on contrastive pre-training to learn representations across different modalities. While their practical benefits are widely acknowledged, a rigorous theoretical understanding of the contrastive pre-training framework remains limited. This paper develops a theoretical framework to explain the success of contrastive pre-training in downstream tasks, such as zero-shot classification, conditional diffusion models, and vision-language models. We introduce the concept of approximate sufficient statistics, a generalization of the classical sufficient statistics, and show that near-minimizers of the contrastive pre-training loss are approximately sufficient, making them adaptable to diverse downstream tasks. We further propose the Joint Generative Hierarchical Model for the joint distribution of images and text, showing that transformers can efficiently approximate relevant functions within this model via belief propagation. Building on this framework, we derive sample complexity guarantees for multi-modal learning based on contrastive pre-trained representations. Numerical simulations validate these theoretical findings, demonstrating the strong generalization performance of contrastively pre-trained transformers in various multi-modal tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04641v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kazusato Oko, Licong Lin, Yuhang Cai, Song Mei</dc:creator>
    </item>
    <item>
      <title>Parallelized Midpoint Randomization for Langevin Monte Carlo</title>
      <link>https://arxiv.org/abs/2402.14434</link>
      <description>arXiv:2402.14434v4 Announce Type: replace 
Abstract: We study the problem of sampling from a target probability density function in frameworks where parallel evaluations of the log-density gradient are feasible. Focusing on smooth and strongly log-concave densities, we revisit the parallelized randomized midpoint method and investigate its properties using recently developed techniques for analyzing its sequential version. Through these techniques, we derive upper bounds on the Wasserstein distance between sampling and target densities. These bounds quantify the substantial runtime improvements achieved through parallel processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14434v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Yu, Arnak Dalalyan</dc:creator>
    </item>
    <item>
      <title>Estimation of Out-of-Sample Sharpe Ratio for High Dimensional Portfolio Optimization</title>
      <link>https://arxiv.org/abs/2406.03954</link>
      <description>arXiv:2406.03954v2 Announce Type: replace 
Abstract: Portfolio optimization aims at constructing a realistic portfolio with significant out-of-sample performance, which is typically measured by the out-of-sample Sharpe ratio. However, due to in-sample optimism, it is inappropriate to use the in-sample estimated covariance to evaluate the out-of-sample Sharpe, especially in the high dimensional settings. In this paper, we propose a novel method to estimate the out-of-sample Sharpe ratio using only in-sample data, based on random matrix theory. Furthermore, portfolio managers can use the estimated out-of-sample Sharpe as a criterion to decide the best tuning for constructing their portfolios. Specifically, we consider the classical framework of Markowits mean-variance portfolio optimization {under} high dimensional regime of $p/n \to c \in (0,\infty)$, where $p$ is the portfolio dimension and $n$ is the number of samples or time points. We propose to correct the sample covariance by a regularization matrix and provide a consistent estimator of its Sharpe ratio. The new estimator works well under either of the following conditions: (1) bounded covariance spectrum, (2) arbitrary number of diverging spikes when $c &lt; 1$, and (3) fixed number of diverging spikes with weak requirement on their diverging speed when $c \ge 1$. We can also extend the results to construct global minimum variance portfolio and correct out-of-sample efficient frontier. We demonstrate the effectiveness of our approach through comprehensive simulations and real data experiments. Our results highlight the potential of this methodology as a useful tool for portfolio optimization in high dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03954v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xuran Meng, Yuan Cao, Weichen Wang</dc:creator>
    </item>
    <item>
      <title>Nonparametric Regression in Dirichlet Spaces: A Random Obstacle Approach</title>
      <link>https://arxiv.org/abs/2412.14357</link>
      <description>arXiv:2412.14357v3 Announce Type: replace 
Abstract: In this paper, we consider nonparametric estimation over general Dirichlet metric measure spaces. Unlike the more commonly studied reproducing kernel Hilbert space, whose elements may be defined pointwise, a Dirichlet space typically only contain equivalence classes, i.e. its elements are only unique almost everywhere. This lack of pointwise definition presents significant challenges in the context of nonparametric estimation, for example the classical ridge regression problem is ill-posed. In this paper, we develop a new technique for renormalizing the ridge loss by replacing pointwise evaluations with certain \textit{local means} around the boundaries of obstacles centered at each data point. The resulting renormalized empirical risk functional is well-posed and even admits a representer theorem in terms of certain equilibrium potentials, which are truncated versions of the associated Green function, cut-off at a data-driven threshold. We study the global, out-of-sample consistency of the sample minimizer, and derive an adaptive upper bound on its convergence rate that highlights the interplay of the analytic, geometric, and probabilistic properties of the Dirichlet form. Our framework notably does not require the smoothness of the underlying space, and is applicable to both manifold and fractal settings. To the best of our knowledge, this is the first paper to obtain out-of-sample convergence guarantees in the framework of general metric measure Dirichlet spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14357v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prem Talwai, David Simchi-Levi</dc:creator>
    </item>
    <item>
      <title>Central limit theorems for vector-valued composite functionals with smoothing and applications</title>
      <link>https://arxiv.org/abs/2412.19367</link>
      <description>arXiv:2412.19367v3 Announce Type: replace 
Abstract: This paper focuses on vector-valued composite functionals, which may be nonlinear in probability. Our primary goal is to establish central limit theorems for these functionals when mixed estimators are employed. Our study is relevant to the evaluation and comparison of risk in decision-making contexts and extends to functionals that arise in machine learning methods. A generalized family of composite risk functionals is presented, which encompasses most of the known coherent risk measures including systemic measures of risk. The paper makes two main contributions. First, we analyze vector-valued functionals, providing a framework for evaluating high-dimensional risks. This framework facilitates the comparison of multiple risk measures, as well as the estimation and asymptotic analysis of systemic risk and its optimal value in decision-making problems. Second, we derive novel central limit theorems for optimized composite functionals when mixed types of estimators: empirical and smoothed estimators are used. We provide verifiable sufficient conditions for the central limit formulae and show their applicability to several popular measures of risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19367v3</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huihui Chen, Darinka Dentcheva, Yang Lin, Gregory J. Stock</dc:creator>
    </item>
    <item>
      <title>Non-parametric estimation of the reaction term in semi-linear SPDEs with spatial ergodicity</title>
      <link>https://arxiv.org/abs/2307.05457</link>
      <description>arXiv:2307.05457v3 Announce Type: replace-cross 
Abstract: This paper discusses the non-parametric estimation of a non-linear reaction term in a semi-linear parabolic stochastic partial differential equation (SPDE). The estimator's consistency is due to the spatial ergodicity of the SPDE while the time horizon remains fixed. The analysis of the estimation error requires the concentration of spatial averages of non-linear transformations of the SPDE. The method developed in this paper combines the Clark-Ocone formula from Malliavin calculus with the Markovianity of the SPDE and density estimates. The resulting variance bound utilises the averaging effect of the conditional expectation in the Clark-Ocone formula. The method is applied to two realistic asymptotic regimes. The focus is on a coupling between the diffusivity and the noise level, where both tend to zero. Secondly, the observation of a fixed SPDE on a growing spatial observation window is considered. Furthermore, the concentration of the occupation time around the occupation measure is proved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05457v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sascha Gaudlitz</dc:creator>
    </item>
  </channel>
</rss>
