<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Jun 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Generalizing causal effects with noncompliance: Application to deep canvassing experiments</title>
      <link>https://arxiv.org/abs/2506.00149</link>
      <description>arXiv:2506.00149v1 Announce Type: new 
Abstract: Standard approaches in generalizability often focus on generalizing the intent-to-treat (ITT). However, in practice, a more policy-relevant quantity is the generalized impact of an intervention across compliers. While instrumental variable (IV) methods are commonly used to estimate the complier average causal effect (CACE) within samples, standard approaches cannot be applied to a target population with a different distribution from the study sample. This paper makes several key contributions. First, we introduce a new set of identifying assumptions in the form of a population-level exclusion restriction that allows for identification of the target complier average causal effect (T-CACE) in both randomized experiments and observational studies. This allows researchers to identify the T-CACE without relying on standard principal ignorability assumptions. Second, we propose a class of inverse-weighted estimators for the T-CACE and derive their asymptotic properties. We provide extensions for settings in which researchers have access to auxiliary compliance information across the target population. Finally, we introduce a sensitivity analysis for researchers to evaluate the robustness of the estimators in the presence of unmeasured confounding. We illustrate our proposed method through extensive simulations and a study evaluating the impact of deep canvassing on reducing exclusionary attitudes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00149v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhongren Chen, Melody Huang</dc:creator>
    </item>
    <item>
      <title>Transporting results from a trial to an external target population when trial participation impacts adherence</title>
      <link>https://arxiv.org/abs/2506.00157</link>
      <description>arXiv:2506.00157v1 Announce Type: new 
Abstract: Randomized clinical trials are considered the gold standard for informing treatment guidelines, but results may not generalize to real-world populations. Generalizability is hindered by distributional differences in baseline covariates and treatment-outcome mediators. Approaches to address differences in covariates are well established, but approaches to address differences in mediators are more limited. Here we consider the setting where trial activities that differ from usual care settings (e.g., monetary compensation, follow-up visits frequency) affect treatment adherence. When treatment and adherence data are unavailable for the real-world target population, we cannot identify the mean outcome under a specific treatment assignment (i.e., mean potential outcome) in the target. Therefore, we propose a sensitivity analysis in which a parameter for the relative difference in adherence to a specific treatment between the trial and the target, possibly conditional on covariates, must be specified. We discuss options for specification of the sensitivity analysis parameter based on external knowledge including setting a range to estimate bounds or specifying a probability distribution from which to repeatedly draw parameter values (i.e., use Monte Carlo sampling). We introduce two estimators for the mean counterfactual outcome in the target that incorporates this sensitivity parameter, a plug-in estimator and a one-step estimator that is double robust and supports the use of machine learning for estimating nuisance models. Finally, we apply the proposed approach to the motivating application where we transport the risk of relapse under two different medications for the treatment of opioid use disorder from a trial to a real-world population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00157v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachael K. Ross, Ivan Diaz, Amy J. Pitts, Elizabeth A. Stuart, Kara E. Rudolph</dc:creator>
    </item>
    <item>
      <title>Integrating Expert Knowledge and Recursive Bayesian Inference: A Framework for Spatial and Spatio-Temporal Data Challenges</title>
      <link>https://arxiv.org/abs/2506.00221</link>
      <description>arXiv:2506.00221v1 Announce Type: new 
Abstract: Expert elicitation is a critical approach for addressing data scarcity across various disciplines. But moreover, it can also complement big data analytics by mitigating the limitations of observational data, such as incompleteness and reliability issues, thereby enhancing model estimates through the integration of disparate or conflicting data sources. The paper also outlines various strategies for integrating prior information within the Integrated Nested Laplace Approximation method and proposes a recursive approach that allows for the analysis of new data as it arrives. This paper presents a comprehensive approach to expert elicitation, with a particular emphasis on spatial and spatio-temporal contexts. Specifically, it introduces a typology of expert-based model implementations that addresses different change of support scenarios between observational and expert data. Detailed examples illustrating clear and replicable procedures for implementing expert elicitation and recursive inference are also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00221v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario Figueira, David Conesa, Antonio L\'opez-Qu\'ilez, H{\aa}vard Rue</dc:creator>
    </item>
    <item>
      <title>Estimation of Optimal Causal Bounds via Covariate-Assisted Optimal Transport</title>
      <link>https://arxiv.org/abs/2506.00257</link>
      <description>arXiv:2506.00257v1 Announce Type: new 
Abstract: We study the estimation of causal estimand involving the joint distribution of treatment and control outcomes for a single unit. In typical causal inference settings, it is impossible to observe both outcomes simultaneously, which places our estimation within the domain of partial identification (PI). Pre-treatment covariates can substantially reduce estimation uncertainty by shrinking the partially identified set. Recently, it was shown that covariate-assisted PI sets can be characterized through conditional optimal transport (COT) problems. However, finite-sample estimation of COT poses significant challenges, primarily because, as we explain, the COT functional is discontinuous under the weak topology, rendering the direct plug-in estimator inconsistent. To address this issue, existing literature relies on relaxations or indirect methods involving the estimation of non-parametric nuisance statistics. In this work, we demonstrate the continuity of the COT functional under a stronger topology induced by the adapted Wasserstein distance. Leveraging this result, we propose a direct, consistent, non-parametric estimator for COT value that avoids nuisance parameter estimation. We derive the convergence rate for our estimator and validate its effectiveness through comprehensive simulations, demonstrating its improved performance compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00257v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sirui Lin, Zijun Gao, Jose Blanchet, Peter Glynn</dc:creator>
    </item>
    <item>
      <title>Recover Experimental Data with Selection Bias using Counterfactual Logic</title>
      <link>https://arxiv.org/abs/2506.00335</link>
      <description>arXiv:2506.00335v1 Announce Type: new 
Abstract: Selection bias, arising from the systematic inclusion or exclusion of certain samples, poses a significant challenge to the validity of causal inference. While Bareinboim et al. introduced methods for recovering unbiased observational and interventional distributions from biased data using partial external information, the complexity of the backdoor adjustment and the method's strong reliance on observational data limit its applicability in many practical settings. In this paper, we formally discover the recoverability of $P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly constructing counterfactual worlds via Structural Causal Models (SCMs), we analyze how selection mechanisms in the observational world propagate to the counterfactual domain. We derive a complete set of graphical and theoretical criteria to determine that the experimental distribution remain unaffected by selection bias. Furthermore, we propose principled methods for leveraging partially unbiased observational data to recover $P(Y^*_{x^*})$ from biased experimental datasets. Simulation studies replicating realistic research scenarios demonstrate the practical utility of our approach, offering concrete guidance for mitigating selection bias in applied causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00335v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang He, Shuai Wang, Ang Li</dc:creator>
    </item>
    <item>
      <title>Estimands for Randomized Discontinuation Designs in Oncology</title>
      <link>https://arxiv.org/abs/2506.00556</link>
      <description>arXiv:2506.00556v1 Announce Type: new 
Abstract: Randomized discontinuation design (RDD) is an enrichment strategy commonly used to address limitations of traditional placebo-controlled trials, particularly the ethical concern of prolonged placebo exposure. RDD consists of two phases: an initial open-label phase in which all eligible patients receive the investigational medicinal product (IMP), followed by a double-blind phase in which responders are randomized to continue with the IMP or switch to placebo. This design tests whether the IMP provides benefit beyond the placebo effect. The estimand framework introduced in ICH E9(R1) strengthens the dialogue among clinical research stakeholders by clarifying trial objectives and aligning them with appropriate statistical analyses. However, its application in oncology trials using RDD remains unclear. This manuscript uses the phase III JAVELIN Gastric 100 trial and the phase II trial of sorafenib (BAY 43-9006) as case studies to propose an estimand framework tailored for oncology trials employing RDD in phase III and phase II settings, respectively. We highlight some similarities and differences between RDDs and traditional randomized controlled trials in the context of ICH E9(R1). This approach aims to support more efficient regulatory decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00556v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayon Mukherjee, Oleksandr Sverdlov, Ngoc-Thuy Ha, Yu Deng</dc:creator>
    </item>
    <item>
      <title>Unbiased estimation in new Gini index extensions under gamma distributions</title>
      <link>https://arxiv.org/abs/2506.00666</link>
      <description>arXiv:2506.00666v1 Announce Type: new 
Abstract: In this paper, we propose two new flexible Gini indices (extended lower and upper) defined via differences between the $i$-th observation, the smallest order statistic, and the largest order statistic, for any $1 \leqslant i \leqslant m$. For gamma-distributed data, we obtain exact expectations of the estimators and establish their unbiasedness, generalizing prior works by [Deltas, G. 2003. The small-sample bias of the gini coefficient: Results and implications for empirical research. Review of Economics and Statistics 85:226-234] and [Baydil, B., de la Pe\~na, V. H., Zou, H., and Yao, H. 2025. Unbiased estimation of the gini coefficient. Statistics &amp; Probability Letters 222:110376]. Finite-sample performance is assessed via simulation, and real income data set is analyzed to illustrate the proposed measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00666v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification of synchrosqueezing transform under complicated nonstationary noise</title>
      <link>https://arxiv.org/abs/2506.00779</link>
      <description>arXiv:2506.00779v1 Announce Type: new 
Abstract: We propose a bootstrapping algorithm to quantify the uncertainty of the time-frequency representation (TFR) generated by the short-time Fourier transform (STFT)-based synchrosqueezing transform (SST) when the input signal is oscillatory with time-varying amplitude and frequency and contaminated by complex nonstationary noise. To this end, we leverage a recently developed high-dimensional Gaussian approximation technique to establish a sequential Gaussian approximation for nonstationary random processes under mild assumptions. This result is of independent interest and enables us to quantify the approximate Gaussianity of the random field over the time-frequency domain induced by the STFT. Building on this foundation, we establish the robustness of SST-based signal decomposition in the presence of nonstationary noise. Furthermore, under the assumption that the noise is locally stationary, we develop a Gaussian auto-regressive bootstrap framework for uncertainty quantification of the TFR obtained via SST and provide a theoretical justification. We validate the proposed method through simulated examples and demonstrate its utility by analyzing spindle activity in electroencephalogram recordings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00779v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hau-Tieng Wu, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Factorized Tail Volatility Model: Augmenting Excess-over-Threshold Method for High-Dimensional Hevay-Tailed Data</title>
      <link>https://arxiv.org/abs/2506.00840</link>
      <description>arXiv:2506.00840v1 Announce Type: new 
Abstract: Ecess-over-Threshold method is a crucial technique in extreme value analysis, which approximately models larger observations over a threshold using a Generalized Pareto Distribution. This paper presents a comprehensive framework for analyzing tail risk in high-dimensional data by introducing the Factorized Tail Volatility Model (FTVM) and integrating it with central quantile models through the EoT method. This integrated framework is termed the FTVM-EoT method. In this framework, a quantile-related high-dimensional data model is employed to select an appropriate threshold at the central quantile for the EoT method, while the FTVM captures heteroscedastic tail volatility by decomposing tail quantiles into a low-rank linear factor structure and a heavy-tailed idiosyncratic component. The FTVM-EoT method is highly flexible, allowing for the joint modeling of central, intermediate, and extreme quantiles of high-dimensional data, thereby providing a holistic approach to tail risk analysis. In addition, we develop an iterative estimation algorithm for the FTVM-EoT method and establish the asymptotic properties of the estimators for latent factors, loadings, intermediate quantiles, and extreme quantiles. A validation procedure is introduced, and an information criterion is proposed for optimal factor selection. Simulation studies demonstrate that the FTVM-EoT method consistently outperforms existing methods at intermediate and extreme quantiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00840v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yifan Hu, Yanxi Hou</dc:creator>
    </item>
    <item>
      <title>Improved Risk Ratio Approximation by Complementary Log-Log Models: A Comparison with Logistic Models</title>
      <link>https://arxiv.org/abs/2506.00889</link>
      <description>arXiv:2506.00889v1 Announce Type: new 
Abstract: Odds ratios obtained from logistic models fail to approximate risk ratios with common outcomes, leading to potential misinterpretations about exposure effects by practitioners. This article investigates the complementary log-log models as a practical alternative to produce risk ratio approximation. We demonstrate that the corresponding effect measure of complementary log-log models, called the complementary log ratio in this article, consistently provides a closer approximation to risk ratios than odds ratios. To compare the approximation accuracy, we adopt the one-parameter Aranda-Ordaz family of link functions, which includes both the logit and complementary log-log link functions as special cases. Within this unified framework, we implement a theoretical comparison of approximation accuracy between the complementary log ratio and the odds ratio, showing that the former always produces smaller approximation bias. Simulation studies further reinforce our theoretical findings. Given that the complementary log-log model is easily implemented in standard statistical software such as R and SAS, we encourage more frequent use of this model as a simple and effective alternative to logistic models when the goal is to approximate risk ratios more accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00889v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuji Tsubota, Kenji Beppu</dc:creator>
    </item>
    <item>
      <title>Building nonstationary extreme value model using L-moments</title>
      <link>https://arxiv.org/abs/2506.00977</link>
      <description>arXiv:2506.00977v1 Announce Type: new 
Abstract: The maximum likelihood estimation for a time-dependent nonstationary (NS) extreme value model is often too sensitive to influential observations, such as large values toward the end of a sample. Thus, alternative methods using L-moments have been developed in NS models to address this problem while retaining the advantages of the stationary L-moment method. However, one method using L-moments displays inferior performance compared to stationary estimation when the data exhibit a positive trend in variance. To address this problem, we propose a new algorithm for efficiently estimating the NS parameters. The proposed method combines L-moments and robust regression, using standardized residuals. A simulation study demonstrates that the proposed method overcomes the mentioned problem. The comparison is conducted using conventional and redefined return level estimates. An application to peak streamflow data in Trehafod in the UK illustrates the usefulness of the proposed method. Additionally, we extend the proposed method to a NS extreme value model in which physical covariates are employed as predictors. Furthermore, we consider a model selection criterion based on the cross-validated generalized L-moment distance as an alternative to the likelihood-based criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00977v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s42952-025-00325-3</arxiv:DOI>
      <arxiv:journal_reference>Journal of the Korean Statistical Society, 2025</arxiv:journal_reference>
      <dc:creator>Yire Shin, Yonggwan Shin, Jeong-Soo Park</dc:creator>
    </item>
    <item>
      <title>A novel stratified sampler with unbalanced refinement for network reliability assessment</title>
      <link>https://arxiv.org/abs/2506.01044</link>
      <description>arXiv:2506.01044v1 Announce Type: new 
Abstract: We investigate stratified sampling in the context of network reliability assessment. We propose an unbalanced stratum refinement procedure, which operates on a partition of network components into clusters and the number of failed components within each cluster. The size of each refined stratum and the associated conditional failure probability, collectively termed failure signatures, can be calculated and estimated using the conditional Bernoulli model. The estimator is further improved by determining the minimum number of component failure $i^*$ to reach system failure and then by considering only strata with at least $i^*$ failed components. We propose a heuristic but practicable approximation of the optimal sample size for all strata, assuming a coherent network performance function. The efficiency of the proposed stratified sampler with unbalanced refinement (SSuR) is demonstrated through two network reliability problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01044v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianpeng Chan, Iason Papaioannou, Daniel Straub</dc:creator>
    </item>
    <item>
      <title>ProjMC$^2$: Scalable and Stable Posterior Inference for Bayesian Spatial Factor Models with Application to Spatial Transcriptomics</title>
      <link>https://arxiv.org/abs/2506.01098</link>
      <description>arXiv:2506.01098v1 Announce Type: new 
Abstract: Factor models exhibit a fundamental tradeoff among flexibility, identifiability, and computational efficiency. Bayesian spatial factor models, in particular, face pronounced identifiability concerns and scaling difficulties. To mitigate these issues and enhance posterior inference reliability, this work proposes Projected Markov Chain Monte Carlo (ProjMC$^2$), a novel Markov Chain Monte Carlo (MCMC) sampling algorithm employing projection techniques and conditional conjugacy. ProjMC$^2$ is showcased within the context of spatial factor analysis, significantly improving posterior stability and MCMC mixing efficiency by projecting posterior sampling of latent factors onto a subspace of a scaled Stiefel manifold. Theoretical results establish convergence to the stationary distribution irrespective of initial values. Integrating this approach with scalable univariate spatial modeling strategies yields a stable, efficient, and flexible modeling and sampling methodology for large-scale spatial factor models. Simulation studies demonstrate the effectiveness and practical advantages of the proposed methods. The practical utility of the methodology is further illustrated through an analysis of spatial transcriptomic data obtained from human kidney tissues, showcasing its potential for enhancing the interpretability and robustness of spatial transcriptomics analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01098v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Zhang</dc:creator>
    </item>
    <item>
      <title>Flexible Selective Inference with Flow-based Transport Maps</title>
      <link>https://arxiv.org/abs/2506.01150</link>
      <description>arXiv:2506.01150v1 Announce Type: new 
Abstract: Data-carving methods perform selective inference by conditioning the distribution of data on the observed selection event. However, existing data-carving approaches typically require an analytically tractable characterization of the selection event. This paper introduces a new method that leverages tools from flow-based generative modeling to approximate a potentially complex conditional distribution, even when the underlying selection event lacks an analytical description -- take, for example, the data-adaptive tuning of model parameters. The key idea is to learn a transport map that pushes forward a simple reference distribution to the conditional distribution given selection. This map is efficiently learned via a normalizing flow, without imposing any further restrictions on the nature of the selection event. Through extensive numerical experiments on both simulated and real data, we demonstrate that this method enables flexible selective inference by providing: (i) valid p-values and confidence sets for adaptively selected hypotheses and parameters, (ii) a closed-form expression for the conditional density function, enabling likelihood-based and quantile-based inference, and (iii) adjustments for intractable selection steps that can be easily integrated with existing methods designed to account for the tractable steps in a selection procedure involving multiple steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01150v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sifan Liu, Snigdha Panigrahi</dc:creator>
    </item>
    <item>
      <title>Uncovering Bias Mechanisms in Observational Studies</title>
      <link>https://arxiv.org/abs/2506.01191</link>
      <description>arXiv:2506.01191v1 Announce Type: new 
Abstract: Observational studies are a key resource for causal inference but are often affected by systematic biases. Prior work has focused mainly on detecting these biases, via sensitivity analyses and comparisons with randomized controlled trials, or mitigating them through debiasing techniques. However, there remains a lack of methodology for uncovering the underlying mechanisms driving these biases, e.g., whether due to hidden confounding or selection of participants. In this work, we show that the relationship between bias magnitude and the predictive performance of nuisance function estimators (in the observational study) can help distinguish among common sources of causal bias. We validate our methodology through extensive synthetic experiments and a real-world case study, demonstrating its effectiveness in revealing the mechanisms behind observed biases. Our framework offers a new lens for understanding and characterizing bias in observational studies, with practical implications for improving causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01191v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilker Demirel, Zeshan Hussain, Piersilvio De Bartolomeis, David Sontag</dc:creator>
    </item>
    <item>
      <title>Reluctant Interaction Inference after Additive Modeling</title>
      <link>https://arxiv.org/abs/2506.01219</link>
      <description>arXiv:2506.01219v1 Announce Type: new 
Abstract: Additive models enjoy the flexibility of nonlinear models while still being readily understandable to humans. By contrast, other nonlinear models, which involve interactions between features, are not only harder to fit but also substantially more complicated to explain. Guided by the principle of parsimony, a data analyst therefore may naturally be reluctant to move beyond an additive model unless it is truly warranted.
  To put this principle of interaction reluctance into practice, we formulate the problem as a hypothesis test with a fitted sparse additive model (SPAM) serving as the null. Because our hypotheses on interaction effects are formed after fitting a SPAM to the data, we adopt a selective inference approach to construct p-values that properly account for this data adaptivity. Our approach makes use of external randomization to obtain the distribution of test statistics conditional on the SPAM fit, allowing us to derive valid p-values, corrected for the over-optimism introduced by the data-adaptive process prior to the test. Through experiments on simulated and real data, we illustrate that--even with small amounts of external randomization--this rigorous modeling approach enjoys considerable advantages over naive methods and data splitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01219v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiling Huang, Snigdha Panigrahi, Guo Yu, Jacob Bien</dc:creator>
    </item>
    <item>
      <title>Neural Networks for Parameter Estimation of the Discretely Observed Hawkes Process</title>
      <link>https://arxiv.org/abs/2506.01258</link>
      <description>arXiv:2506.01258v1 Announce Type: new 
Abstract: When the sample path of a Hawkes process is observed discretely, such that only the total event counts in disjoint time intervals are known, the likelihood function becomes intractable. To overcome the challenge of likelihood-based inference in this setting, we propose to use a likelihood-free approach to parameter estimation, where a fully connected neural network (NN) is trained using simulated data to estimate the parameters of the Hawkes process from a summary statistic of the count data. A naive imputation estimate of the parameters forms the basis of our summary statistic, which is fast to generate and requires minimal expert knowledge to design. The resulting NN estimator is comparable to the best extant approximate likelihood estimators in terms of mean-squared error but requires significantly less computational time. We also propose a bootstrap bias correction procedure to further enhance the quality of the NN estimator. The proposed estimation procedure is applied to weekly count data for two infectious diseases, with a time-varying background rate used to capture seasonal fluctuations in infection risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01258v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason J. Lambe, Feng Chen, Tom Stindl, Tsz-Kit Jeffrey Kwan</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Regularized Additive Matrix Autoregressive Model</title>
      <link>https://arxiv.org/abs/2506.01403</link>
      <description>arXiv:2506.01403v1 Announce Type: new 
Abstract: High-dimensional time series has diverse applications in econometrics and finance. Recent models for capturing temporal dependence have employed a bilinear representation for matrix time series, or the Tucker-decomposition based representation in case of tensor time series. A bilinear or Tucker-decomposition based temporal effect is difficult to interpret on many occasions, along with its computational complexity due to the non-convex nature of the underlying optimization problem. Moreover, the existing matrix case models have not sufficiently explored the possibilities of imposing any lower-dimensional pattern on the transition matrices. In this work, we propose a regularized additive matrix autoregressive model with additive interaction of row-wise and column-wise temporal dependence, that offers more interpretability, less computational burden due to its convex nature and estimation of the underlying low rank plus sparse pattern of its transition matrices. We address the issue of identifiability of the various components in our model and subsequently develop a scalable Alternating Block Minimization algorithm for estimating the parameters. We provide a finite sample error bound under high-dimensional scaling for the model parameters. Finally, the efficacy of the proposed model is demonstrated on synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01403v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debika Ghosh, Samrat Roy, Nilanjana Chakraborty</dc:creator>
    </item>
    <item>
      <title>e-GAI: e-value-based Generalized $\alpha$-Investing for Online False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2506.01452</link>
      <description>arXiv:2506.01452v1 Announce Type: new 
Abstract: Online multiple hypothesis testing has attracted a lot of attention in many applications, e.g., anomaly status detection and stock market price monitoring. The state-of-the-art generalized $\alpha$-investing (GAI) algorithms can control online false discovery rate (FDR) on p-values only under specific dependence structures, a situation that rarely occurs in practice. The e-LOND algorithm (Xu &amp; Ramdas, 2024) utilizes e-values to achieve online FDR control under arbitrary dependence but suffers from a significant loss in power as testing levels are derived from pre-specified descent sequences. To address these limitations, we propose a novel framework on valid e-values named e-GAI. The proposed e-GAI can ensure provable online FDR control under more general dependency conditions while improving the power by dynamically allocating the testing levels. These testing levels are updated not only by relying on both the number of previous rejections and the prior costs, but also, differing from the GAI framework, by assigning less $\alpha$-wealth for each rejection from a risk aversion perspective. Within the e-GAI framework, we introduce two new online FDR procedures, e-LORD and e-SAFFRON, and provide strategies for the long-term performance to address the issue of $\alpha$-death, a common phenomenon within the GAI framework. Furthermore, we demonstrate that e-GAI can be generalized to conditionally super-uniform p-values. Both simulated and real data experiments demonstrate the advantages of both e-LORD and e-SAFFRON in FDR control and power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01452v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Zhang, Zijian Wei, Haojie Ren, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Characterization based Goodness-of-Fit for Generalized Pareto Distribution: A Blend of Stein's Identity and Dynamic Survival Extropy</title>
      <link>https://arxiv.org/abs/2506.01473</link>
      <description>arXiv:2506.01473v1 Announce Type: new 
Abstract: This paper proposes a goodness of fit test for the generalized Pareto distribution (GPD). Firstly, we provide two characterizations of GPD based on Stein's identity and dynamic survival extropy. These characterizations are used to test GPD separately for the positive and negative shape parameter cases. A Monte Carlo simulation is conducted to provide the critical values and power of the proposed test against a good number of alternatives. Our test is simple to use and it has asymptotic normality and relatively high power, which strengthened the purpose of proposing it. Considering the case of right censored data, we provide the procedure to handle censored case too. A few real-life applications are also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01473v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaurav Kandpal, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>Simulating Complex Crossectional and Longitudinal Data using the simDAG R Package</title>
      <link>https://arxiv.org/abs/2506.01498</link>
      <description>arXiv:2506.01498v1 Announce Type: new 
Abstract: Generating artificial data is a crucial step when performing Monte-Carlo simulation studies. Depending on the planned study, complex data generation processes (DGP) containing multiple, possibly time-varying, variables with various forms of dependencies and data types may be required. Simulating data from such DGP may therefore become a difficult and time-consuming endeavor. The simDAG R package offers a standardized approach to generate data from simple and complex DGP based on the definition of structural equations in directed acyclic graphs using arbitrary functions or regression models. The package offers a clear syntax with an enhanced formula interface and directly supports generating binary, categorical, count and time-to-event data with arbitrary dependencies, possibly non-linear relationships and interactions. It additionally includes a framework to conduct discrete-time based simulations which allows the generation of longitudinal data on a semi-continuous time-scale. This approach may be used to generate time-to-event data with both recurrent or competing events and possibly multiple time-varying covariates, which may themselves have arbitrary data types. In this article we demonstrate the vast amount of features included in simDAG by replicating the DGP of multiple real Monte-Carlo simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01498v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robin Denz, Nina Timmesfeld</dc:creator>
    </item>
    <item>
      <title>A nonparametric statistical method for deconvolving densities in the analysis of proteomic data</title>
      <link>https://arxiv.org/abs/2506.01540</link>
      <description>arXiv:2506.01540v1 Announce Type: new 
Abstract: In medical research, often, genomic or proteomic data are collected, with measurements frequently subject to uncertainties or errors, making it crucial to accurately separate the signals of the genes or proteins, respectively, from the noise. Such a signal separation is also of interest in skin aging research in which intrinsic aging driven by genetic factors and extrinsic, i.e.\ environmentally induced, aging are investigated by considering, e.g., the proteome of skin fibroblasts. Since extrinsic influences on skin aging can only be measured alongside intrinsic ones, it is essential to isolate the pure extrinsic signal from the combined intrinisic and extrinsic signal. In such situations, deconvolution methods can be employed to estimate the signal's density function from the data. However, existing nonparametric deconvolution approaches often fail when the variance of the mixed distribution is substantially greater than the variance of the target distribution, which is a common issue in genomic and proteomic data.
  We, therefore, propose a new nonparametric deconvolution method called N-Power Fourier Deconvolution (NPFD) that addresses this issue by employing the $N$-th power of the Fourier transform of transformed densities. This procedure utilizes the Fourier transform inversion theorem and exploits properties of Fourier transforms of density functions to mitigate numerical inaccuracies through exponentiation, leading to accurate and smooth density estimation. An extensive simulation study demonstrates that NPFD effectively handles the variance issues and performs comparably or better than existing deconvolution methods in most scenarios. Moreover, applications to real medical data, particularly to proteomic data from fibroblasts affected by intrinsic and extrinsic aging, show how NPFD can be employed to estimate the pure extrinsic density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01540v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akin Anarat, Jean Krutmann, Holger Schwender</dc:creator>
    </item>
    <item>
      <title>An Efficient and Interpretable Autoregressive Model for High-Dimensional Tensor-Valued Time Series</title>
      <link>https://arxiv.org/abs/2506.01658</link>
      <description>arXiv:2506.01658v1 Announce Type: new 
Abstract: In autoregressive modeling for tensor-valued time series, Tucker decomposition, when applied to the coefficient tensor, provides a clear interpretation of supervised factor modeling but loses its efficiency rapidly with increasing tensor order. Conversely, canonical polyadic (CP) decomposition maintains efficiency but lacks a precise statistical interpretation. To attain both interpretability and powerful dimension reduction, this paper proposes a novel approach under the supervised factor modeling paradigm, which first uses CP decomposition to extract response and covariate features separately and then regresses response features on covariate ones. This leads to a new CP-based low-rank structure for the coefficient tensor. Furthermore, to address heterogeneous signals or potential model misspecifications arising from stringent low-rank assumptions, a low-rank plus sparse model is introduced by incorporating an additional sparse coefficient tensor. Nonasymptotic properties are established for the ordinary least squares estimators, and an alternating least squares algorithm is introduced for optimization. Theoretical properties of the proposed methodology are validated by simulation studies, and its enhanced prediction performance and interpretability are demonstrated by the El Ni$\tilde{\text{n}}$o-Southern Oscillation example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01658v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxi Cai, Lan Li, Yize Wang, Guodong Li</dc:creator>
    </item>
    <item>
      <title>Tree-based methods for estimating heterogeneous model performance and model combining</title>
      <link>https://arxiv.org/abs/2506.01905</link>
      <description>arXiv:2506.01905v1 Announce Type: new 
Abstract: Model performance is frequently reported only for the overall population under consideration. However, due to heterogeneity, overall performance measures often do not accurately represent model performance within specific subgroups. We develop tree-based methods for the data-driven identification of subgroups with differential model performance, where splitting decisions are made to maximize heterogeneity in performance between subgroups. We extend these methods to tree ensembles, including both random forests and gradient boosting. Lastly, we illustrate how these ensembles can be used for model combination. We evaluate the methods through simulations and apply them to lung cancer screening data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01905v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruotao Zhang, Constantine Gatsonis, Jon Steingrimsson</dc:creator>
    </item>
    <item>
      <title>Rapid updating of multivariate resource models based on new information using EnKF-MDA and multi-Gaussian transformation</title>
      <link>https://arxiv.org/abs/2503.04694</link>
      <description>arXiv:2503.04694v2 Announce Type: cross 
Abstract: Rapid resource model updating with real-time data is important for making timely decisions in resource management and mining operations. This requires optimal merging of models and observations, which can be achieved through data assimilation, and the ensemble Kalman filter (EnKF) has become a popular method for this task. However, the modelled resources in mining usually consist of multiple variables of interest with multivariate relationships of varying complexity. EnKF is not a multivariate approach, and even for univariate cases, there may be slight deviations between its outcomes and observations. This study presents a methodology for rapidly updating multivariate resource models using the EnKF with multiple data assimilations (EnKF-MDA) combined with rotation based iterative Gaussianisation (RBIG). EnKF-MDA improves the updating by assimilating the same data multiple times with an inflated measurement error, while RBIG quickly transforms the data into multi-Gaussian factors. The application of the proposed algorithm is validated by a real case study with nine cross-correlated variables. The combination of EnKF-MDA and RBIG successfully improves the accuracy of resource model updates, minimises uncertainty, and preserves the multivariate relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04694v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/min15040424</arxiv:DOI>
      <arxiv:journal_reference>Minerals 15(4) (2025) 424</arxiv:journal_reference>
      <dc:creator>Sultan Abulkhair, Peter Dowd, Chaoshui Xu, Penny Stewart</dc:creator>
    </item>
    <item>
      <title>Bayesian Data Sketching for Varying Coefficient Regression Models</title>
      <link>https://arxiv.org/abs/2506.00270</link>
      <description>arXiv:2506.00270v1 Announce Type: cross 
Abstract: Varying coefficient models are popular for estimating nonlinear regression functions in functional data models. Their Bayesian variants have received limited attention in large data applications, primarily due to prohibitively slow posterior computations using Markov chain Monte Carlo (MCMC) algorithms. We introduce Bayesian data sketching for varying coefficient models to obviate computational challenges presented by large sample sizes. To address the challenges of analyzing large data, we compress the functional response vector and predictor matrix by a random linear transformation to achieve dimension reduction and conduct inference on the compressed data. Our approach distinguishes itself from several existing methods for analyzing large functional data in that it requires neither the development of new models or algorithms, nor any specialized computational hardware while delivering fully model-based Bayesian inference. Well-established methods and algorithms for varying coefficient regression models can be applied to the compressed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00270v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajarshi Guhaniyogi, Laura Baracaldo, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Label-shift robust federated feature screening for high-dimensional classification</title>
      <link>https://arxiv.org/abs/2506.00379</link>
      <description>arXiv:2506.00379v1 Announce Type: cross 
Abstract: Distributed and federated learning are important tools for high-dimensional classification of large datasets. To reduce computational costs and overcome the curse of dimensionality, feature screening plays a pivotal role in eliminating irrelevant features during data preprocessing. However, data heterogeneity, particularly label shifting across different clients, presents significant challenges for feature screening. This paper introduces a general framework that unifies existing screening methods and proposes a novel utility, label-shift robust federated feature screening (LR-FFS), along with its federated estimation procedure. The framework facilitates a uniform analysis of methods and systematically characterizes their behaviors under label shift conditions. Building upon this framework, LR-FFS leverages conditional distribution functions and expectations to address label shift without adding computational burdens and remains robust against model misspecification and outliers. Additionally, the federated procedure ensures computational efficiency and privacy protection while maintaining screening effectiveness comparable to centralized processing. We also provide a false discovery rate (FDR) control method for federated feature screening. Experimental results and theoretical analyses demonstrate LR-FFS's superior performance across diverse client environments, including those with varying class distributions, sample sizes, and missing categorical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00379v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Qin, Erbo Li, Xingxiang Li, Yifan Sun, Wu Wang, Chen Xu</dc:creator>
    </item>
    <item>
      <title>Learning from Double Positive and Unlabeled Data for Potential-Customer Identification</title>
      <link>https://arxiv.org/abs/2506.00436</link>
      <description>arXiv:2506.00436v1 Announce Type: cross 
Abstract: In this study, we propose a method for identifying potential customers in targeted marketing by applying learning from positive and unlabeled data (PU learning). We consider a scenario in which a company sells a product and can observe only the customers who purchased it. Decision-makers seek to market products effectively based on whether people have loyalty to the company. Individuals with loyalty are those who are likely to remain interested in the company even without additional advertising. Consequently, those loyal customers would likely purchase from the company if they are interested in the product. In contrast, people with lower loyalty may overlook the product or buy similar products from other companies unless they receive marketing attention. Therefore, by focusing marketing efforts on individuals who are interested in the product but do not have strong loyalty, we can achieve more efficient marketing. To achieve this goal, we consider how to learn, from limited data, a classifier that identifies potential customers who (i) have interest in the product and (ii) do not have loyalty to the company. Although our algorithm comprises a single-stage optimization, its objective function implicitly contains two losses derived from standard PU learning settings. For this reason, we refer to our approach as double PU learning. We verify the validity of the proposed algorithm through numerical experiments, confirming that it functions appropriately for the problem at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00436v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato, Yuki Ikeda abd Kentaro Baba, Takashi Imai, Ryo Inokuchi</dc:creator>
    </item>
    <item>
      <title>Score Matching With Missing Data</title>
      <link>https://arxiv.org/abs/2506.00557</link>
      <description>arXiv:2506.00557v1 Announce Type: cross 
Abstract: Score matching is a vital tool for learning the distribution of data with applications across many areas including diffusion processes, energy based modelling, and graphical model estimation. Despite all these applications, little work explores its use when data is incomplete. We address this by adapting score matching (and its major extensions) to work with missing data in a flexible setting where data can be partially missing over any subset of the coordinates. We provide two separate score matching variations for general use, an importance weighting (IW) approach, and a variational approach. We provide finite sample bounds for our IW approach in finite domain settings and show it to have especially strong performance in small sample lower dimensional cases. Complementing this, we show our variational approach to be strongest in more complex high-dimensional settings which we demonstrate on graphical model estimation tasks on both real and simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00557v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josh Givens, Song Liu, Henry W J Reeve</dc:creator>
    </item>
    <item>
      <title>Assessing Climate-Driven Mortality Risk: A Stochastic Approach with Distributed Lag Non-Linear Models</title>
      <link>https://arxiv.org/abs/2506.00561</link>
      <description>arXiv:2506.00561v1 Announce Type: cross 
Abstract: Assessing climate-driven mortality risk has become an emerging area of research in recent decades. In this paper, we propose a novel approach to explicitly incorporate climate-driven effects into both single- and multi-population stochastic mortality models. The new model consists of two components: a stochastic mortality model, and a distributed lag non-linear model (DLNM). The first component captures the non-climate long-term trend and volatility in mortality rates. The second component captures non-linear and lagged effects of climate variables on mortality, as well as the impact of heat waves and cold waves across different age groups. For model calibration, we propose a backfitting algorithm that allows us to disentangle the climate-driven mortality risk from the non-climate-driven stochastic mortality risk. We illustrate the effectiveness and superior performance of our model using data from three European regions: Athens, Lisbon, and Rome. Furthermore, we utilize future UTCI data generated from climate models to provide mortality projections into 2045 across these regions under two Representative Concentration Pathway (RCP) scenarios. The projections show a noticeable decrease in winter mortality alongside a rise in summer mortality, driven by a general increase in UTCI over time. Although we expect slightly lower overall mortality in the short term under RCP8.5 compared to RCP2.6, a long-term increase in total mortality is anticipated under the RCP8.5 scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00561v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiacheng Min, Han Li, Thomas Nagler, Shuanming Li</dc:creator>
    </item>
    <item>
      <title>Projection Pursuit Density Ratio Estimation</title>
      <link>https://arxiv.org/abs/2506.00866</link>
      <description>arXiv:2506.00866v1 Announce Type: cross 
Abstract: Density ratio estimation (DRE) is a paramount task in machine learning, for its broad applications across multiple domains, such as covariate shift adaptation, causal inference, independence tests and beyond. Parametric methods for estimating the density ratio possibly lead to biased results if models are misspecified, while conventional non-parametric methods suffer from the curse of dimensionality when the dimension of data is large. To address these challenges, in this paper, we propose a novel approach for DRE based on the projection pursuit (PP) approximation. The proposed method leverages PP to mitigate the impact of high dimensionality while retaining the model flexibility needed for the accuracy of DRE. We establish the consistency and the convergence rate for the proposed estimator. Experimental results demonstrate that our proposed method outperforms existing alternatives in various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00866v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meilin Wang, Wei Huang, Mingming Gong, Zheng Zhang</dc:creator>
    </item>
    <item>
      <title>Discriminating Tail Behavior Using Halfspace Depths: Population and Empirical Perspectives</title>
      <link>https://arxiv.org/abs/2506.01126</link>
      <description>arXiv:2506.01126v1 Announce Type: cross 
Abstract: We study the empirical version of halfspace depths with the objective of establishing a connection between the rates of convergence and the tail behaviour of the corresponding underlying distributions. The intricate interplay between the sample size and the parameter driving the tail behaviour forms one of the main results of this analysis. The chosen approach is mainly based on weighted empirical processes indexed by sets by Alexander (1987), which leads to relatively direct and elegant proofs, regardless of the nature of the tail. This method is further enriched by our findings on the population version, which also enable us to distinguish between light and heavy tails. These results lay the foundation for our subsequent analysis of the empirical versions. Building on these theoretical insights, we propose a methodology to assess the tail behaviour of the underlying multivariate distribution of a sample, which we illustrate on simulated data. The study concludes with an application to a real-world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01126v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sibsankar Singha, Marie Kratz, Sreekar Vadlamani</dc:creator>
    </item>
    <item>
      <title>Adversarial learning for nonparametric regression: Minimax rate and adaptive estimation</title>
      <link>https://arxiv.org/abs/2506.01267</link>
      <description>arXiv:2506.01267v1 Announce Type: cross 
Abstract: Despite tremendous advancements of machine learning models and algorithms in various application domains, they are known to be vulnerable to subtle, natural or intentionally crafted perturbations in future input data, known as adversarial attacks. While numerous adversarial learning methods have been proposed, fundamental questions about their statistical optimality in robust loss remain largely unanswered. In particular, the minimax rate of convergence and the construction of rate-optimal estimators under future $X$-attacks are yet to be worked out.
  In this paper, we address this issue in the context of nonparametric regression, under suitable assumptions on the smoothness of the regression function and the geometric structure of the input perturbation set. We first establish the minimax rate of convergence under adversarial $L_q$-risks with $1 \leq q \leq \infty$ and propose a piecewise local polynomial estimator that achieves the minimax optimality. The established minimax rate elucidates how the smoothness level and perturbation magnitude affect the fundamental limit of adversarial learning under future $X$-attacks. Furthermore, we construct a data-driven adaptive estimator that is shown to achieve, within a logarithmic factor, the optimal rate across a broad scale of nonparametric and adversarial classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01267v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingfu Peng, Yuhong Yang</dc:creator>
    </item>
    <item>
      <title>Pluri-Gaussian rapid updating of geological domains</title>
      <link>https://arxiv.org/abs/2506.01575</link>
      <description>arXiv:2506.01575v1 Announce Type: cross 
Abstract: Over the past decade, the rapid updating of resource knowledge and the integration of real-time sensor information have garnered attention in both industry and academia. However, most studies on rapid resource model updating have focused on continuous variables, such as grade variables and coal quality parameters. Geological domain modelling is an essential component of resource estimation, which is why it is crucial to extend data assimilation techniques to enable the rapid updating of categorical variables. In this paper, a methodology inspired by pluri-Gaussian simulation is proposed for near-real-time updating of geological domains, followed by the updating of grade variables within these domain boundaries. The proposed algorithm consists of a Gibbs sampler for converting geological domains into Gaussian random fields, an ensemble Kalman filter with multiple data assimilations (EnKF-MDA) for rapid updating, and rotation based iterative Gaussianisation (RBIG) for multi-Gaussian transformation. We demonstrate the algorithm using a synthetic case study with observations sampled from the ground truth, as well as a real case study that uses production drilling samples for joint updating of geological domains and grade variables. Both case studies are based on real data from an iron oxide-copper-gold deposit in South Australia. This approach enhances resource knowledge by incorporating both categorical and continuous variables, leading to improved reproduction of domain geometries, closer matches between predictions and observations, and more geologically realistic resource models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01575v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sultan Abulkhair, Peter Dowd, Chaoshui Xu</dc:creator>
    </item>
    <item>
      <title>Life Sequence Transformer: Generative Modelling for Counterfactual Simulation</title>
      <link>https://arxiv.org/abs/2506.01874</link>
      <description>arXiv:2506.01874v1 Announce Type: cross 
Abstract: Social sciences rely on counterfactual analysis using surveys and administrative data, generally depending on strong assumptions or the existence of suitable control groups, to evaluate policy interventions and estimate causal effects. We propose a novel approach that leverages the Transformer architecture to simulate counterfactual life trajectories from large-scale administrative records. Our contributions are: the design of a novel encoding method that transforms longitudinal administrative data to sequences and the proposal of a generative model tailored to life sequences with overlapping events across life domains. We test our method using data from the Istituto Nazionale di Previdenza Sociale (INPS), showing that it enables the realistic and coherent generation of life trajectories. This framework offers a scalable alternative to classical counterfactual identification strategy, such as difference-in-differences and synthetic controls, particularly in contexts where these methods are infeasible or their assumptions unverifiable. We validate the model's utility by comparing generated life trajectories against established findings from causal studies, demonstrating its potential to enrich labour market research and policy evaluation through individual-level simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01874v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Cabezas, Carlotta Montorsi</dc:creator>
    </item>
    <item>
      <title>A stableness of resistance model for nonresponse adjustment with callback data</title>
      <link>https://arxiv.org/abs/2112.02822</link>
      <description>arXiv:2112.02822v5 Announce Type: replace 
Abstract: Nonresponse arises frequently in surveys and follow-ups are routinely made to increase the response rate. In order to monitor the follow-up process, callback data have been used in social sciences and survey studies for decades. In modern surveys, the availability of callback data is increasing because the response rate is decreasing and follow-ups are essential to collect maximum information. Although callback data are helpful to reduce the bias in surveys, such data have not been widely used in statistical analysis until recently. We propose a stableness of resistance assumption for nonresponse adjustment with callback data. We establish the identification and the semiparametric efficiency theory under this assumption, and propose a suite of semiparametric estimation methods including doubly robust estimators, which generalize existing parametric approaches for callback data analysis. We apply the approach to a Consumer Expenditure Survey dataset. The results suggest an association between nonresponse and high housing expenditures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.02822v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wang Miao, Xinyu Li, Ping Zhang, Baoluo Sun</dc:creator>
    </item>
    <item>
      <title>Evaluation of binary classifiers for asymptotically dependent and independent extremes</title>
      <link>https://arxiv.org/abs/2112.13738</link>
      <description>arXiv:2112.13738v5 Announce Type: replace 
Abstract: Machine learning classification methods usually assume that all possible classes are sufficiently present within the training set. Due to their inherent rarities, extreme events are always under-represented and classifiers tailored for predicting extremes need to be carefully designed to handle this under-representation. In this paper, we address the question of how to assess and compare classifiers with respect to their capacity to capture extreme occurrences. This is also related to the topic of scoring rules used in forecasting literature. In this context, we propose and study a risk function adapted to extremal classifiers. The inferential properties of our empirical risk estimator are derived under the framework of multivariate regular variation and hidden regular variation. A simulation study compares different classifiers and indicates their performance with respect to our risk function. To conclude, we apply our framework to the analysis of extreme river discharges in the Danube river basin. The application compares different predictive algorithms and test their capacity at forecasting river discharges from other river stations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.13738v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juliette Legrand, Philippe Naveau, Marco Oesting</dc:creator>
    </item>
    <item>
      <title>Bayesian Geostatistics Using Predictive Stacking</title>
      <link>https://arxiv.org/abs/2304.12414</link>
      <description>arXiv:2304.12414v3 Announce Type: replace 
Abstract: We develop Bayesian predictive stacking for geostatistical models, where the primary inferential objective is to provide inference on the latent spatial random field and conduct spatial predictions at arbitrary locations. We exploit analytically tractable posterior distributions for regression coefficients of predictors and the realizations of the spatial process conditional upon process parameters. We subsequently combine such inference by stacking these models across the range of values of the hyper-parameters. We devise stacking of means and posterior densities in a manner that is computationally efficient without resorting to iterative algorithms such as Markov chain Monte Carlo (MCMC) and can exploit the benefits of parallel computations. We offer novel theoretical insights into the resulting inference within an infill asymptotic paradigm and through empirical results showing that stacked inference is comparable to full sampling-based Bayesian inference at a significantly lower computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12414v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Zhang, Wenpin Tang, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>A Meta-Learning Method for Estimation of Causal Excursion Effects to Assess Time-Varying Moderation</title>
      <link>https://arxiv.org/abs/2306.16297</link>
      <description>arXiv:2306.16297v3 Announce Type: replace 
Abstract: Advances in wearable technologies and health interventions delivered by smartphones have greatly increased the accessibility of mobile health (mHealth) interventions. Micro-randomized trials (MRTs) are designed to assess the effectiveness of the mHealth intervention and introduce a novel class of causal estimands called "causal excursion effects." These estimands enable the evaluation of how intervention effects change over time and are influenced by individual characteristics or context. Existing methods for analyzing causal excursion effects assume known randomization probabilities, complete observations, and a linear nuisance function with prespecified features of the high dimensional observed history. However, in complex mobile systems, these assumptions often fall short: randomization probabilities can be uncertain, observations may be incomplete, and the granularity of mHealth data makes linear modeling difficult. To address this issue, we propose a flexible and doubly robust inferential procedure, called "DR-WCLS," for estimating causal excursion effects from a meta-learner perspective. We present the bidirectional asymptotic properties of the proposed estimators and compare them with existing methods both theoretically and through extensive simulations. The results show a consistent and more efficient estimate, even with missing observations or uncertain treatment randomization probabilities. Finally, the practical utility of the proposed methods is demonstrated by analyzing data from a multiinstitution cohort of first-year medical residents in the United States (NeCamp et al., 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16297v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jieru Shi, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>Perturbative partial moment matching and gradient-flow adaptive importance sampling transformations for Bayesian leave one out cross-validation</title>
      <link>https://arxiv.org/abs/2402.08151</link>
      <description>arXiv:2402.08151v3 Announce Type: replace 
Abstract: Importance sampling (IS) allows one to approximate leave one out (LOO) cross-validation for a Bayesian model, without refitting, by inverting the Bayesian update equation to subtract a given data point from a model posterior. For each data point, one computes expectations under the corresponding LOO posterior by weighted averaging over the full data posterior. This task sometimes requires weight stabilization in the form of adapting the posterior distribution via transformation. So long as one is successful in finding a suitable transformation, one avoids refitting. To this end, we motivate the use of bijective perturbative transformations of the form $T(\boldsymbol{\theta})=\boldsymbol{\theta} + h Q(\boldsymbol{\theta}),$ for $0&lt;h\ll 1,$ and introduce two classes of such transformations: 1) partial moment matching and 2) gradient flow evolution. The former extends prior literature on moment-matching under the recognition that adaptation for LOO is a small perturbation on the full data posterior. The latter class of methods define transformations based on relaxing various statistical objectives: in our case the variance of the IS estimator and the KL divergence between the transformed distribution and the statistics of the LOO fold. Being model-specific, the gradient flow transformations require evaluating Jacobian determinants. While these quantities are generally readily available through auto-differentiation, we derive closed-form expressions in the case of logistic regression and shallow ReLU activated neural networks. We tested the methodology on an $n\ll p$ dataset that is known to produce unstable LOO IS weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08151v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua C Chang, Xiangting Li, Shixin Xu, Hao-Ren Yao, Julia Porcino, Carson Chow</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Spatial-Temporal Non-Gaussian Data Using Predictive Stacking</title>
      <link>https://arxiv.org/abs/2406.04655</link>
      <description>arXiv:2406.04655v3 Announce Type: replace 
Abstract: Analysing non-Gaussian spatial-temporal data requires introducing spatial dependence in generalised linear models through the link function of an exponential family distribution. Unlike in Gaussian likelihoods, inference is considerably encumbered by the inability to analytically integrate out the random effects and reduce the dimension of the parameter space. Iterative estimation algorithms struggle to converge due to the presence of weakly identified parameters. We devise Bayesian inference using predictive stacking that assimilates inference from analytically tractable conditional posterior distributions. We achieve this by expanding upon the Diaconis-Ylvisaker family of conjugate priors and exploiting generalised conjugate multivariate (GCM) distribution theory for exponential families, which enables exact sampling from analytically available posterior distributions conditional upon some process parameters. Subsequently, we assimilate inference over a range of values of these parameters using Bayesian predictive stacking. We evaluate inferential performance on simulated data, compare with full Bayesian inference using Markov chain Monte Carlo (MCMC) and apply our method to analyse spatially-temporally referenced avian count data from the North American Breeding Bird Survey database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04655v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyakanti Pan, Lu Zhang, Jonathan R. Bradley, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Causal Effect Identification and Inference with Endogenous Exposures and a Light-tailed Error</title>
      <link>https://arxiv.org/abs/2408.06211</link>
      <description>arXiv:2408.06211v2 Announce Type: replace 
Abstract: Endogeneity poses significant challenges in causal inference across various research domains. This paper proposes a novel approach to identify and estimate causal effects in the presence of endogeneity. We consider a structural equation with endogenous exposures and an additive error term. Assuming the light-tailedness of the error term, we show that the causal effect can be identified by contrasting extreme conditional quantiles of the outcome given the exposures. Unlike many existing results, our identification approach does not rely on additional parametric assumptions or auxiliary variables. Building on the identification result, we develop a new method that estimates the causal effect using extreme quantile regression. We establish the consistency of the proposed extreme-based estimator under a general additive structural equation and demonstrate its asymptotic normality in the linear model setting. These results reveal that extreme quantile regression is invulnerable to endogeneity when the error term is light-tailed, which is not appreciated in the literature to our knowledge. The proposed extreme-based method can be applied to causal inference problems with invalid auxiliary variables, e.g., invalid instruments or invalid negative controls, for the selection of auxiliary variables and construction of valid confidence sets for the causal effect. Simulations and data analysis of an automobile sale dataset show the effectiveness of our method in addressing endogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06211v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyu Wang, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Approximations to worst-case data dropping: unmasking failure modes</title>
      <link>https://arxiv.org/abs/2408.09008</link>
      <description>arXiv:2408.09008v5 Announce Type: replace 
Abstract: A data analyst might worry about generalization if dropping a very small fraction of data points from a study could change its substantive conclusions. Checking this non-robustness directly poses a combinatorial optimization problem and is intractable even for simple models and moderate data sizes. Recently various authors have proposed a diverse set of approximations to detect this non-robustness. In the present work, we show that, even in a setting as simple as ordinary least squares (OLS) linear regression, many of these approximations can fail to detect (true) non-robustness in realistic data arrangements. We focus on OLS in the present work due its widespread use and since some approximations work only for OLS. Across our synthetic and real-world data sets, we find that a simple recursive greedy algorithm is the sole algorithm that does not fail any of our tests and also that it can be orders of magnitude faster to run than some competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09008v5</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenny Y. Huang, David R. Burt, Yunyi Shen, Tin D. Nguyen, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Expected value of sample information calculations for risk prediction model development</title>
      <link>https://arxiv.org/abs/2410.03096</link>
      <description>arXiv:2410.03096v2 Announce Type: replace 
Abstract: Risk prediction models are often advertised as deterministic functions that map covariates to predicted risks. However, they are typically trained using finite samples, and as such, their predictions are inherently uncertain. This uncertainty has been addressed in terms of uncertainty around metrics of model performance (e.g., confidence intervals around c-statistic), as well as uncertainty or instability of predictions. Correspondingly, sample size calculations for model development studies target the precision of estimates of summary statistics and the stability of predictions. However, when evaluating the clinical utility of a model (as in Net Benefit (NB) calculations in decision curve analysis), statistical inference is less relevant. From a decision-theoretic perspective, the finite size of the sample results in utility loss due to the discrepancy between the fitted model and the correct model. From this perspective, procuring more development data is associated with an expected gain in the utility of using the model. In this work, we define the Expected Value of Sample Information (EVSI) as the expected gain in clinical utility, defined in NB terms, by procuring an additional development sample of a given size. We propose a bootstrap-based algorithm for EVSI computations and demonstrate its feasibility and face validity in a case study. We conclude that decision-theoretic metrics can complement classical inferential methods when designing studies aimed at developing risk prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03096v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdollah Safari, Paul Gustafson, Mohsen Sadatsafavi</dc:creator>
    </item>
    <item>
      <title>Data-light Uncertainty Set Merging with Admissibility: Synthetics, Aggregation, and Test Inversion</title>
      <link>https://arxiv.org/abs/2410.12201</link>
      <description>arXiv:2410.12201v2 Announce Type: replace 
Abstract: This article introduces a Synthetics, Aggregation, and Test inversion (SAT) approach for merging diverse and potentially dependent uncertainty sets into a single unified set. The procedure is data-light, relying only on initial sets and control levels, and it adapts to any user-specified initial uncertainty sets, accommodating potentially varying coverage levels. SAT is motivated by the challenge of integrating uncertainty sets when only the initial sets and their control levels are available - for example, when merging confidence sets from distributed sites under communication constraints or combining conformal prediction sets generated by different algorithms or data splits. To address this, SAT constructs and aggregates novel synthetic test statistics, and then derive merged sets through test inversion. Our method leverages the duality between set estimation and hypothesis testing, ensuring reliable coverage in dependent scenarios. A key theoretical contribution is a rigorous analysis of SAT's properties, including a proof of its admissibility in the context of deterministic set merging. Both theoretical analyses and empirical results confirm the method's finite-sample coverage validity and desirable set sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12201v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shenghao Qin, Jianliang He, Qi Kuang, Bowen Gang, Yin Xia</dc:creator>
    </item>
    <item>
      <title>Generalized entropy calibration for analyzing voluntary survey data</title>
      <link>https://arxiv.org/abs/2412.12405</link>
      <description>arXiv:2412.12405v2 Announce Type: replace 
Abstract: Statistical analysis of voluntary survey data is an important area of research in survey sampling. We consider a unified approach to voluntary survey data analysis under the assumption that the sampling mechanism is ignorable. Generalized entropy calibration is introduced as a unified tool for calibration weighting to control the selection bias. We first establish the relationship between the generalized calibration weighting and its dual expression for regression estimation. The dual relationship is critical in identifying the implied regression model and developing model selection for calibration weighting. Also, if a linear regression model for an important study variable is available, then two-step calibration method can be used to smooth the final weights and achieve the statistical efficiency. Asymptotic properties of the proposed estimator are investigated. Results from a limited simulation study are also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12405v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yonghyun Kwon, Jae Kwang Kim, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>Ergodic Network Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2412.17779</link>
      <description>arXiv:2412.17779v3 Announce Type: replace 
Abstract: We propose a novel framework for Network Stochastic Differential Equations (N-SDE), where each node in a network is governed by an SDE influenced by interactions with its neighbors. The evolution of each node is driven by the interplay of three key components: the node's intrinsic dynamics (\emph{momentum effect}), feedback from neighboring nodes (\emph{network effect}), and a \emph{stochastic volatility} term modeled by Brownian motion. Our primary objective is to estimate the parameters of the N-SDE system from high-frequency discrete-time observations. The motivation behind this model lies in its ability to analyze very high-dimensional time series by leveraging the inherent sparsity of the underlying network graph. We consider two distinct scenarios: \textit{i) known network structure}: the graph is fully specified, and we establish conditions under which the parameters can be identified, considering the linear growth of the parameter space with the number of edges. \textit{ii) unknown network structure}: the graph must be inferred from the data. For this, we develop an iterative procedure using adaptive Lasso, tailored to a specific subclass of N-SDE models. In this work, we assume the network graph is oriented, paving the way for novel applications of SDEs in causal inference, enabling the study of cause-effect relationships in dynamic systems. Through extensive simulation studies, we demonstrate the performance of our estimators across various graph topologies in high-dimensional settings. We also showcase the framework's applicability to real-world datasets, highlighting its potential for advancing the analysis of complex networked systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17779v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Francesco Iafrate, Stefano Iacus</dc:creator>
    </item>
    <item>
      <title>Family-wise Error Rate Control with E-values</title>
      <link>https://arxiv.org/abs/2501.09015</link>
      <description>arXiv:2501.09015v3 Announce Type: replace 
Abstract: The closure principle is a standard tool for achieving strong family-wise error rate (FWER) control in multiple testing problems. We develop an e-value-based closed testing framework that inherits nice properties of e-values, which are common in settings of sequential hypothesis testing or universal inference for irregular parametric models. We prove that e-value-based closed testing strongly controls the post-hoc FWER in the static setting, and has stronger anytime-valid and always-valid FWER-controlling properties in the sequential setting. Furthermore, we extend the celebrated graphical approach for FWER control of Bretz et al. (2009), using the weighted average of e-values for the local test, a strictly more powerful approach than weighted Bonferroni local tests with inverse e-values as p-values. In general, the computational cost for closed testing can be exponential in the number of hypotheses. Although the computational shortcuts for the p-value-based graphical approach are not applicable, we develop an efficient polynomial-time algorithm using dynamic programming for e-value-based graphical approaches with any directed acyclic graph, and tailored algorithms for the e-Holm procedure previously studied by Vovk and Wang (2021) and the e-Fallback procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09015v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Will Hartog, Lihua Lei</dc:creator>
    </item>
    <item>
      <title>Prior selection for the precision parameter of Dirichlet Process Mixtures</title>
      <link>https://arxiv.org/abs/2502.00864</link>
      <description>arXiv:2502.00864v2 Announce Type: replace 
Abstract: Consider a Dirichlet process mixture model (DPM) with random precision parameter $\alpha$, inducing $K_n$ clusters over $n$ observations through its latent random partition. Our goal is to specify the prior distribution $p\left(\alpha\mid\boldsymbol{\eta}\right)$, including its fixed parameter vector $\boldsymbol{\eta}$, in a way that is meaningful.
  Existing approaches can be broadly categorised into three groups. Those in the first group depend on the sample size $n$, and often rely on the linkage between $p\left(\alpha\mid\boldsymbol{\eta}\right)$ and $p\left(K_n\right)$ to draw conclusions on how to best choose $\boldsymbol{\eta}$ to reflect one's prior knowledge of $K_{n}$; we call them sample-size-dependent. Those in the second and third group consist instead of using quasi-degenerate or improper priors, respectively.
  In this article, we show how all three methods have limitations, especially for large $n$. Then we propose an alternative methodology which does not depend on $K_n$ or on the size of the available sample, but rather on the relationship between the largest stick lengths in the stick-breaking construction of the DPM; and which reflects those prior beliefs in $p\left(\alpha\mid\boldsymbol{\eta}\right)$. We conclude with an example where existing sample-size-dependent approaches fail, while our sample-size-independent approach continues to be feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00864v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlo Vicentini, Ian Hyla Jermyn</dc:creator>
    </item>
    <item>
      <title>Falsification of Unconfoundedness by Testing Independence of Causal Mechanisms</title>
      <link>https://arxiv.org/abs/2502.06231</link>
      <description>arXiv:2502.06231v2 Announce Type: replace 
Abstract: A major challenge in estimating treatment effects in observational studies is the reliance on untestable conditions such as the assumption of no unmeasured confounding. In this work, we propose an algorithm that can falsify the assumption of no unmeasured confounding in a setting with observational data from multiple heterogeneous sources, which we refer to as environments. Our proposed falsification strategy leverages a key observation that unmeasured confounding can cause observed causal mechanisms to appear dependent. Building on this observation, we develop a novel two-stage procedure that detects these dependencies with high statistical power while controlling false positives. The algorithm does not require access to randomized data and, in contrast to other falsification approaches, functions even under transportability violations when the environment has a direct effect on the outcome of interest. To showcase the practical relevance of our approach, we show that our method is able to efficiently detect confounding on both simulated and semi-synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06231v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rickard K. A. Karlsson, Jesse H. Krijthe</dc:creator>
    </item>
    <item>
      <title>Revisiting Optimal Allocations for Binary Responses: Insights from Considering Type-I Error Rate Control</title>
      <link>https://arxiv.org/abs/2502.06381</link>
      <description>arXiv:2502.06381v4 Announce Type: replace 
Abstract: This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation - an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive two optimal allocation proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power and the other minimizes the total number failures in a trial while maintaining a fixed variance level. Through simulations based on an early-phase and a confirmatory trial we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials and alternative measures of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06381v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Sof\'ia S. Villar, William F. Rosenberger</dc:creator>
    </item>
    <item>
      <title>Functional BART with Shape Priors: A Bayesian Tree Approach to Constrained Functional Regression</title>
      <link>https://arxiv.org/abs/2502.16888</link>
      <description>arXiv:2502.16888v2 Announce Type: replace 
Abstract: Motivated by the remarkable success of Bayesian additive regression trees (BART) in regression modelling, we propose a novel nonparametric Bayesian method, termed Functional BART (FBART), tailored specifically for function-on-scalar regression. FBART leverages spline-based representations for functional responses coupled with a flexible tree-based partitioning structure, effectively capturing complex and heterogeneous relationships between response curves and scalar predictors. To facilitate efficient posterior inference, we develop a customized Bayesian backfitting algorithm. Additionally, we extend FBART by introducing shape constraints (e.g., monotonicity or convexity) on the response curves, enabling enhanced estimation and prediction when prior shape information is available. The use of shape priors ensures that posterior samples respect the specified functional constraints. Under mild regularity conditions, we establish posterior convergence rates for both FBART and its shape-constrained variant, demonstrating rate adaptivity to unknown smoothness. Extensive simulation studies and analyses of two real datasets illustrate the superior estimation accuracy and predictive performance of our proposed methods compared to existing state-of-the-art alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16888v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Cao, Shiyuan He, Bohai Zhang</dc:creator>
    </item>
    <item>
      <title>Statistical parametric simulation studies based on real data</title>
      <link>https://arxiv.org/abs/2504.04864</link>
      <description>arXiv:2504.04864v2 Announce Type: replace 
Abstract: Simulation studies are indispensable for evaluating and comparing statistical methods. The most common simulation approach is parametric simulation, where the data-generating mechanism (DGM) corresponds to a predefined parametric model from which observations are drawn. Many statistical simulation studies aim to provide practical recommendations on a method's suitability for a given application; however, parametric simulations in particular are frequently criticized for being too simplistic and not reflecting reality. To overcome this drawback, it is generally considered a sensible approach to employ real data for constructing the parametric DGMs. However, while the concept of real-data-based parametric DGMs is widely recognized, the specific ways in which DGM components are inferred from real data vary, and their implications may not always be well understood. Additionally, researchers often rely on a limited selection of real datasets, with the rationale for their selection often unclear. This paper addresses these issues by formally discussing how components of parametric DGMs can be inferred from real data and how dataset selection can be performed more systematically. By doing so, we aim to support researchers in conducting simulation studies with a lower risk of overgeneralization and misinterpretation. We illustrate the construction of parametric DGMs based on a systematically selected set of real datasets using two examples: one on ordinal outcomes in randomized controlled trials and one on differential gene expression analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04864v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christina Sauer, F. Julian D. Lange, Maria Thurow, Ina Dormuth, Anne-Laure Boulesteix</dc:creator>
    </item>
    <item>
      <title>A robust mixed-effects quantile regression model using generalized Laplace mixtures to handle outliers and skewness</title>
      <link>https://arxiv.org/abs/2504.14515</link>
      <description>arXiv:2504.14515v2 Announce Type: replace 
Abstract: Mixed-effects quantile regression models are widely used to capture heterogeneous responses in hierarchically structured data. The asymmetric Laplace (AL) distribution has traditionally served as the basis for quantile regression; however, its fixed skewness limits flexibility and renders it sensitive to outliers. In contrast, the generalized asymmetric Laplace (GAL) distribution enables more flexible modeling of skewness and heavy-tailed behavior, yet it remains vulnerable to extreme observations. In this paper, we extend the GAL distribution by introducing a contaminated GAL (cGAL) mixture model that incorporates a scale-inflated component to mitigate the impact of outliers without requiring explicit outlier identification or deletion. We apply this model within a Bayesian mixed-effects quantile regression framework to model HIV viral load decay over time. Our results demonstrate that the cGAL-based model more reliably captures the dynamics of HIV viral load decay, yielding more accurate parameter estimates compared to both AL and GAL approaches. Model diagnostics and comparison statistics confirm the cGAL model as the preferred choice. A simulation study further shows that the cGAL model is more robust to outliers than the GAL and exhibits favorable frequentist properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14515v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divan A. Burger, Sean van der Merwe, Emmanuel Lesaffre</dc:creator>
    </item>
    <item>
      <title>Semiparametric Weighted Spline Regression (SWSR) in Confirmatory Clinical Trials with Time-Varying Placebo Effects</title>
      <link>https://arxiv.org/abs/2505.06939</link>
      <description>arXiv:2505.06939v2 Announce Type: replace 
Abstract: In confirmatory Phase 3 clinical trials with recruitment over the years, the underlying placebo effect may follow an unknown temporal trend. Taking a clinical trial on Hidradenitis Suppurativa (HS) as an example, fluctuations or variabilities are common in HS-related endpoints, mainly due to the natural disease characteristics, variations of evaluation from different physicians, and standard of care evolvement. The adjustment of time-varying placebo effects receives some attention in adaptive clinical trials and platform trials, but is usually ignored in traditional non-adaptive designs. However, under the impact of such a time drift, some existing methods may not simultaneously control the type I error rate and achieve satisfactory power. In this article, we propose SWSR (Semiparametric Weighted Spline Regression) to estimate the treatment effect with B-splines to accommodate the time-varying placebo effects nonparametrically. Our method aims to achieve the following three objectives: a proper type I error rate control under varying settings, an overall high power to detect a potential treatment effect, and robustness to unknown time-varying placebo effects. Simulation studies and a case study provide supporting evidence. Those three key features make SWSR an appealing option to be pre-specified for practical confirmatory clinical trials. Supplemental materials, including the R code, additional simulation results and theoretical discussion, are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06939v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhan, Yihua Gu</dc:creator>
    </item>
    <item>
      <title>Inference on Extreme Quantiles of Unobserved Individual Heterogeneity</title>
      <link>https://arxiv.org/abs/2210.08524</link>
      <description>arXiv:2210.08524v4 Announce Type: replace-cross 
Abstract: We develop a methodology for conducting inference on extreme quantiles of unobserved individual heterogeneity (e.g., heterogeneous coefficients, treatment effects) in panel data and meta-analysis settings. Inference is challenging in such settings: only noisy estimates of heterogeneity are available, and central limit approximations perform poorly in the tails. We derive a necessary and sufficient condition under which noisy estimates are informative about extreme quantiles, along with sufficient rate and moment conditions. Under these conditions, we establish an extreme value theorem and an intermediate order theorem for noisy estimates. These results yield simple optimization-free confidence intervals for extreme quantiles. Simulations show that our confidence intervals have favorable coverage and that the rate conditions matter for the validity of inference. We illustrate the method with an application to firm productivity differences between denser and less dense areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08524v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladislav Morozov</dc:creator>
    </item>
    <item>
      <title>Neural Conditional Probability for Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2407.01171</link>
      <description>arXiv:2407.01171v2 Announce Type: replace-cross 
Abstract: We introduce Neural Conditional Probability (NCP), an operator-theoretic approach to learning conditional distributions with a focus on statistical inference tasks. NCP can be used to build conditional confidence regions and extract key statistics such as conditional quantiles, mean, and covariance. It offers streamlined learning via a single unconditional training phase, allowing efficient inference without the need for retraining even when conditioning changes. By leveraging the approximation capabilities of neural networks, NCP efficiently handles a wide variety of com- plex probability distributions. We provide theoretical guarantees that ensure both optimization consistency and statistical accuracy. In experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods. This demonstrates that a a minimalistic architecture with a theoretically grounded loss can achieve competitive results, even in the face of more complex architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01171v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Advances in Neural Information Processing Systems (NeurIPS) 2024</arxiv:journal_reference>
      <dc:creator>Vladimir R. Kostic, Karim Lounici, Gregoire Pacreau, Pietro Novelli, Giacomo Turri, Massimiliano Pontil</dc:creator>
    </item>
    <item>
      <title>LETS-C: Leveraging Text Embedding for Time Series Classification</title>
      <link>https://arxiv.org/abs/2407.06533</link>
      <description>arXiv:2407.06533v2 Announce Type: replace-cross 
Abstract: Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06533v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rachneet Kaur, Zhen Zeng, Tucker Balch, Manuela Veloso</dc:creator>
    </item>
    <item>
      <title>A Randomized Exchange Algorithm for Optimal Design of Multi-Response Experiments</title>
      <link>https://arxiv.org/abs/2407.16283</link>
      <description>arXiv:2407.16283v2 Announce Type: replace-cross 
Abstract: Despite the increasing prevalence of vector observations, computation of optimal experimental design for multi-response models has received limited attention. To address this problem within the framework of approximate designs, we introduce mREX, an algorithm that generalizes the randomized exchange algorithm REX (J Am Stat Assoc 115:529, 2020), originally specialized for single-response models. The mREX algorithm incorporates several improvements: a novel method for computing efficient sparse initial designs, an extension to all differentiable Kiefer's optimality criteria, and an efficient method for performing optimal exchanges of weights. For the most commonly used D-optimality criterion, we propose a technique for optimal weight exchanges based on the characteristic matrix polynomial. The mREX algorithm is applicable to linear, nonlinear, and generalized linear models, and scales well to large problems. It typically converges to optimal designs faster than available alternative methods, although it does not require advanced mathematical programming solvers. We demonstrate the usefulness of mREX to bivariate dose-response Emax models for clinical trials, both without and with the inclusion of covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16283v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P\'al Somogyi, Samuel Rosa, Radoslav Harman</dc:creator>
    </item>
    <item>
      <title>Generalized Bayesian deep reinforcement learning</title>
      <link>https://arxiv.org/abs/2412.11743</link>
      <description>arXiv:2412.11743v2 Announce Type: replace-cross 
Abstract: Bayesian reinforcement learning (BRL) is a method that merges principles from Bayesian statistics and reinforcement learning to make optimal decisions in uncertain environments. As a model-based RL method, it has two key components: (1) inferring the posterior distribution of the model for the data-generating process (DGP) and (2) policy learning using the learned posterior. We propose to model the dynamics of the unknown environment through deep generative models, assuming Markov dependence. In the absence of likelihood functions for these models, we train them by learning a generalized predictive-sequential (or prequential) scoring rule (SR) posterior. We used sequential Monte Carlo (SMC) samplers to draw samples from this generalized Bayesian posterior distribution. In conjunction, to achieve scalability in the high-dimensional parameter space of the neural networks, we use the gradient-based Markov kernels within SMC. To justify the use of the prequential scoring rule posterior, we prove a Bernstein-von Mises-type theorem. For policy learning, we propose expected Thompson sampling (ETS) to learn the optimal policy by maximising the expected value function with respect to the posterior distribution. This improves upon traditional Thompson sampling (TS) and its extensions, which utilize only one sample drawn from the posterior distribution. This improvement is studied both theoretically and using simulation studies, assuming a discrete action space. Finally, we successfully extended our setup for a challenging problem with a continuous action space without theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11743v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shreya Sinha Roy, Richard G. Everitt, Christian P. Robert, Ritabrata Dutta</dc:creator>
    </item>
    <item>
      <title>Exploring Multi-Modal Data with Tool-Augmented LLM Agents for Precise Causal Discovery</title>
      <link>https://arxiv.org/abs/2412.13667</link>
      <description>arXiv:2412.13667v2 Announce Type: replace-cross 
Abstract: Causal discovery is an imperative foundation for decision-making across domains, such as smart health, AI for drug discovery and AIOps. Traditional statistical causal discovery methods, while well-established, predominantly rely on observational data and often overlook the semantic cues inherent in cause-and-effect relationships. The advent of Large Language Models (LLMs) has ushered in an affordable way of leveraging the semantic cues for knowledge-driven causal discovery, but the development of LLMs for causal discovery lags behind other areas, particularly in the exploration of multi-modal data. To bridge the gap, we introduce MATMCD, a multi-agent system powered by tool-augmented LLMs. MATMCD has two key agents: a Data Augmentation agent that retrieves and processes modality-augmented data, and a Causal Constraint agent that integrates multi-modal data for knowledge-driven reasoning. The proposed design of the inner-workings ensures successful cooperation of the agents. Our empirical study across seven datasets suggests the significant potential of multi-modality enhanced causal discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13667v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>ChengAo Shen, Zhengzhang Chen, Dongsheng Luo, Dongkuan Xu, Haifeng Chen, Jingchao Ni</dc:creator>
    </item>
    <item>
      <title>On the asymptotic validity of confidence sets for linear functionals of solutions to integral equations</title>
      <link>https://arxiv.org/abs/2502.16673</link>
      <description>arXiv:2502.16673v2 Announce Type: replace-cross 
Abstract: This paper examines the construction of confidence sets for parameters defined as linear functionals of a function of W and X whose conditional mean given Z and X equals the conditional mean of another variable Y given Z and X. Many estimands of interest in causal inference can be expressed in this form, including the average treatment effect in proximal causal inference and treatment effect contrasts in instrumental variable models. We derive a necessary condition for a confidence set to be uniformly valid over a model that allows for the dependence between W and Z given X to be arbitrarily weak. Specifically, we show that for any such confidence set, there must exist some laws in the model under which, with high probability, the confidence set has a diameter greater than or equal to the diameter of the parameter's range. In particular, consistent with the weak instruments literature, Wald confidence intervals are not uniformly valid over the aforementioned model. Furthermore, we argue that inverting the score test, a successful approach in that literature, generally fails for the broader class of parameters considered here. We present a method for constructing uniformly valid confidence sets in the special case where all variables, but possibly Y, are binary and discuss its limitations. Finally, we emphasize that developing uniformly valid confidence sets for the class of parameters considered in this paper remains an open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16673v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezequiel Smucler, James M. Robins, Andrea Rotnitzky</dc:creator>
    </item>
    <item>
      <title>Particle Gibbs without the Gibbs bit</title>
      <link>https://arxiv.org/abs/2505.04611</link>
      <description>arXiv:2505.04611v4 Announce Type: replace-cross 
Abstract: Exact parameter and trajectory inference in state-space models is typically achieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or particle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly proposes a new trajectory and parameter, and accepts or rejects both at once. PGibbs instead alternates between sampling from the trajectory, using an algorithm known as conditional sequential Monte Carlo (CSMC) and the parameter in a Hastings-within-Gibbs fashion. While particle independent Metropolis Hastings (PIMH), the parameter-free version of PMMH, is known to be statistically worse than CSMC, PGibbs can induce a slow mixing if the parameter and the state trajectory are very correlated. This has made PMMH the method of choice for many practitioners, despite theory and experiments favouring CSMC over PIMH for the parameter-free problem. In this article, we describe a formulation of PGibbs which bypasses the Gibbs step, essentially marginalizing over the trajectory distribution in a fashion similar to PMMH. This is achieved by considering the implementation of a CSMC algortihm for the state-space model integrated over the joint distribution of the current parameter and the parameter proposal. We illustrate the benefits of method on a simple example known to be challenging for PMMH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04611v4</guid>
      <category>stat.CO</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Experiments with Latent Outcomes: Key Results and Their Implications for Design and Analysis</title>
      <link>https://arxiv.org/abs/2505.21909</link>
      <description>arXiv:2505.21909v2 Announce Type: replace-cross 
Abstract: How should researchers analyze randomized experiments in which the main outcome is measured in multiple ways but each measure contains some degree of error? We describe modeling approaches that enable researchers to identify causal parameters of interest, suggest ways that experimental designs can be augmented so as to make linear latent variable models more credible, and discuss empirical tests of key modeling assumptions. We show that when experimental researchers invest appropriately in multiple outcome measures, an optimally weighted index of the outcome measures enables researchers to obtain efficient and interpretable estimates of causal parameters by applying standard regression methods, and that weights may be obtained using instrumental variables regression. Maximum likelihood and generalized method of moments estimators can be used to obtain estimates and standard errors in a single step. An empirical application illustrates the gains in precision and robustness that multiple outcome measures can provide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21909v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu, Donald P. Green</dc:creator>
    </item>
  </channel>
</rss>
