<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Dec 2024 02:30:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Profile least squares estimation in networks with covariates</title>
      <link>https://arxiv.org/abs/2412.16298</link>
      <description>arXiv:2412.16298v1 Announce Type: new 
Abstract: Many real world networks exhibit edge heterogeneity with different pairs of nodes interacting with different intensities. Further, nodes with similar attributes tend to interact more with each other. Thus, in the presence of observed node attributes (covariates), it is of interest to understand the extent to which these covariates explain interactions between pairs of nodes and to suitably estimate the remaining structure due to unobserved factors. For example, in the study of international relations, the extent to which country-pair specific attributes such as the number of material/verbal conflicts and volume of trade explain military alliances between different countries can lead to valuable insights. We study the model where pairwise edge probabilities are given by the sum of a linear edge covariate term and a residual term to model the remaining heterogeneity from unobserved factors. We approach estimation of the model via profile least squares and show how it leads to a simple algorithm to estimate the linear covariate term and the residual structure that is truly latent in the presence of observed covariates. Our framework lends itself naturally to a bootstrap procedure which is used to draw inference on model parameters, such as to determine significance of the homophily parameter or covariates in explaining the underlying network structure. Application to four real network datasets and comparisons using simulated data illustrate the usefulness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16298v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swati Chandna, Benjamin Bagozzi, Snigdhansu Chatterjee</dc:creator>
    </item>
    <item>
      <title>Generalizing causal effect estimates to larger populations while accounting for (uncertainty in) effect modifiers using a scaled Bayesian bootstrap with application to estimating the effect of family planning on employment in Nigeria</title>
      <link>https://arxiv.org/abs/2412.16320</link>
      <description>arXiv:2412.16320v1 Announce Type: new 
Abstract: Strategies are needed to generalize causal effects from a sample that may differ systematically from the population of interest. In a motivating case study, interest lies in the causal effect of family planning on empowerment-related outcomes among urban Nigerian women, while estimates of this effect and its variation by covariates are available only from a sample of women in six Nigerian cities. Data on covariates in target populations are available from a complex sampling design survey. Our approach, analogous to the plug-in g-formula, takes the expectation of conditional average treatment effects from the source study over the covariate distribution in the target population. This method leverages generalizability literature from randomized trials, applied to a source study using principal stratification for identification. The approach uses a scaled Bayesian bootstrap to account for the complex sampling design. We also introduce checks for sensitivity to plausible departures of assumptions. In our case study, the average effect in the target population is higher than in the source sample based on point estimates and sensitivity analysis shows that a strong omitted effect modifier must be present in at least 40% of the target population for the 95% credible interval to include the null effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16320v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Ilene Speizer, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Sharp Results for Hypothesis Testing with Risk-Sensitive Agents</title>
      <link>https://arxiv.org/abs/2412.16452</link>
      <description>arXiv:2412.16452v1 Announce Type: new 
Abstract: Statistical protocols are often used for decision-making involving multiple parties, each with their own incentives, private information, and ability to influence the distributional properties of the data. We study a game-theoretic version of hypothesis testing in which a statistician, also known as a principal, interacts with strategic agents that can generate data. The statistician seeks to design a testing protocol with controlled error, while the data-generating agents, guided by their utility and prior information, choose whether or not to opt in based on expected utility maximization. This strategic behavior affects the data observed by the statistician and, consequently, the associated testing error. We analyze this problem for general concave and monotonic utility functions and prove an upper bound on the Bayes false discovery rate (FDR). Underlying this bound is a form of prior elicitation: we show how an agent's choice to opt in implies a certain upper bound on their prior null probability. Our FDR bound is unimprovable in a strong sense, achieving equality at a single point for an individual agent and at any countable number of points for a population of agents. We also demonstrate that our testing protocols exhibit a desirable maximin property when the principal's utility is considered. To illustrate the qualitative predictions of our theory, we examine the effects of risk aversion, reward stochasticity, and signal-to-noise ratio, as well as the implications for the Food and Drug Administration's testing protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16452v1</guid>
      <category>stat.ME</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flora C. Shi, Stephen Bates, Martin J. Wainwright</dc:creator>
    </item>
    <item>
      <title>MATES: Multi-view Aggregated Two-Sample Test</title>
      <link>https://arxiv.org/abs/2412.16684</link>
      <description>arXiv:2412.16684v1 Announce Type: new 
Abstract: The two-sample test is a fundamental problem in statistics with a wide range of applications. In the realm of high-dimensional data, nonparametric methods have gained prominence due to their flexibility and minimal distributional assumptions. However, many existing methods tend to be more effective when the two distributions differ primarily in their first and/or second moments. In many real-world scenarios, distributional differences may arise in higher-order moments, rendering traditional methods less powerful. To address this limitation, we propose a novel framework to aggregate information from multiple moments to build a test statistic. Each moment is regarded as one view of the data and contributes to the detection of some specific type of discrepancy, thus allowing the test statistic to capture more complex distributional differences. The novel multi-view aggregated two-sample test (MATES) leverages a graph-based approach, where the test statistic is constructed from the weighted similarity graphs of the pooled sample. Under mild conditions on the multi-view weighted similarity graphs, we establish theoretical properties of MATES, including a distribution-free limiting distribution under the null hypothesis, which enables straightforward type-I error control. Extensive simulation studies demonstrate that MATES effectively distinguishes subtle differences between distributions. We further validate the method on the S&amp;P100 data, showcasing its power in detecting complex distributional variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16684v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zexi Cai, Wenbo Fei, Doudou Zhou</dc:creator>
    </item>
    <item>
      <title>Visualizing Linear Prediction</title>
      <link>https://arxiv.org/abs/2412.16980</link>
      <description>arXiv:2412.16980v1 Announce Type: new 
Abstract: Many statistics courses cover multiple linear regression, and present students with the formula of a prediction using the regressors, slopes, and an intercept. But is it really easy to see which terms have the largest effect, or to explain why the prediction of a specific case is unusually high or low? To assist with this the so-called grill plot is proposed. Its simplicity makes it easy to interpret, and it combines much information. Its main benefit is that it helps explainability of the linear formula as it is, without depending on how the formula was derived. The regressors can be numerical, categorical, or interaction terms, and the model can be linear or generalized linear. Another display is proposed to visualize correlations between predictors, in a way that is tailored for this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16980v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Spline Autoregression Method for Estimation of Quantile Spectrum</title>
      <link>https://arxiv.org/abs/2412.17163</link>
      <description>arXiv:2412.17163v1 Announce Type: new 
Abstract: The quantile spectrum was introduced in Li (2012; 2014) as an alternative tool for spectral analysis of time series. It has the capability of providing a richer view of time series data than that offered by the ordinary spectrum especially for nonlinear dynamics such as stochastic volatility. A novel method, called spline autoregression (SAR), is proposed in this paper for estimating the quantile spectrum as a bivaraite function of frequency and quantile level, under the assumption that the quantile spectrum varies smoothly with the quantile level. The SAR method is facilitated by the quantile discrete Fourier transform (QDFT) based on trigonometric quantile regression. It is enabled by the resulting time-domain quantile series (QSER) which represents properly scaled oscillatory characteristics of the original time series around a quantile. A functional autoregressive (AR) model is fitted to the QSER on a grid of quantile levels by penalized least-squares with the AR coefficients represented as smoothing splines of the quantile level. While the ordinary AR model is widely used for conventional spectral estimation, the proposed SAR method provides an effective way of estimating the quantile spectrum as a bivariate function in comparison with the alternatives. This is confirmed by a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17163v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ta-Hsin Li</dc:creator>
    </item>
    <item>
      <title>Spatial function-on-function regression</title>
      <link>https://arxiv.org/abs/2412.17327</link>
      <description>arXiv:2412.17327v1 Announce Type: new 
Abstract: We introduce a spatial function-on-function regression model to capture spatial dependencies in functional data by integrating spatial autoregressive techniques with functional principal component analysis. The proposed model addresses a critical gap in functional regression by enabling the analysis of functional responses influenced by spatially correlated functional predictors, a common scenario in fields such as environmental sciences, epidemiology, and socio-economic studies. The model employs a spatial functional principal component decomposition on the response and a classical functional principal component decomposition on the predictor, transforming the functional data into a finite-dimensional multivariate spatial autoregressive framework. This transformation allows efficient estimation and robust handling of spatial dependencies through least squares methods. In a series of extensive simulations, the proposed model consistently demonstrated superior performance in estimating both spatial autocorrelation and regression coefficient functions compared to some favorably existing traditional approaches, particularly under moderate to strong spatial effects. Application of the proposed model to Brazilian COVID-19 data further underscored its practical utility, revealing critical spatial patterns in confirmed cases and death rates that align with known geographic and social interactions. An R package provides a comprehensive implementation of the proposed estimation method, offering a user-friendly and efficient tool for researchers and practitioners to apply the methodology in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17327v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ufuk Beyaztas, Han Lin Shang, Gizel Bakicierler Sezer, Abhijit Mandal, Roger S. Zoh, Carmen D. Tekwe</dc:creator>
    </item>
    <item>
      <title>Hierarchical Dirichlet Process Mixture of Products of Multinomial Distributions: Applications to Survey Data with Potentially Missing Values</title>
      <link>https://arxiv.org/abs/2412.17335</link>
      <description>arXiv:2412.17335v1 Announce Type: new 
Abstract: In social science research, understanding latent structures in populations through survey data with categorical responses is a common and important task. Traditional methods like Factor Analysis and Latent Class Analysis have limitations, particularly in handling categorical data and accommodating mixed memberships in latent structures, respectively. Moreover, analyzing survey responses with missing values using these methods is quite challenging. This study introduces a Hierarchical Dirichlet Process Mixture of Products of Multinomial Distributions (HDPMPM) model, which leverages the flexibility of nonparametric Bayesian methods to address these limitations. The HDPMPM model allows for multiple latent classes within individuals and supports a potentially infinite number of mixture components. Additionally, it incorporates missing data imputation directly into the model's Gibbs sampling process. By applying a truncated stick-breaking representation of the Dirichlet process, we can derive a Gibbs sampling scheme for posterior inference. An application of the HDPMPM model to the 2016 American National Election Study (ANES) data demonstrates its effectiveness in identifying political profiles and handling missing data scenarios, including those that are missing at random (MAR) and missing completely at random (MCAR). The results show that the HDPMPM model successfully recovers dominant profiles and manages complex latent structures in survey data, providing an alternative tool for social science researchers in dealing with categorical data with missing values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17335v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chayut Wongkamthong</dc:creator>
    </item>
    <item>
      <title>Bayesian penalized empirical likelihood and MCMC sampling</title>
      <link>https://arxiv.org/abs/2412.17354</link>
      <description>arXiv:2412.17354v1 Announce Type: new 
Abstract: In this study, we introduce a novel methodological framework called Bayesian Penalized Empirical Likelihood (BPEL), designed to address the computational challenges inherent in empirical likelihood (EL) approaches. Our approach has two primary objectives: (i) to enhance the inherent flexibility of EL in accommodating diverse model conditions, and (ii) to facilitate the use of well-established Markov Chain Monte Carlo (MCMC) sampling schemes as a convenient alternative to the complex optimization typically required for statistical inference using EL. To achieve the first objective, we propose a penalized approach that regularizes the Lagrange multipliers, significantly reducing the dimensionality of the problem while accommodating a comprehensive set of model conditions. For the second objective, our study designs and thoroughly investigates two popular sampling schemes within the BPEL context. We demonstrate that the BPEL framework is highly flexible and efficient, enhancing the adaptability and practicality of EL methods. Our study highlights the practical advantages of using sampling techniques over traditional optimization methods for EL problems, showing rapid convergence to the global optima of posterior distributions and ensuring the effective resolution of complex statistical inference challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17354v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Cheng Yong Tang, Yuanzheng Zhu</dc:creator>
    </item>
    <item>
      <title>Biomarker combination based on the Youden index with and without gold standard</title>
      <link>https://arxiv.org/abs/2412.17471</link>
      <description>arXiv:2412.17471v1 Announce Type: new 
Abstract: In clinical practice, multiple biomarkers are often measured on the same subject for disease diagnosis, and combining them can improve diagnostic accuracy. Existing studies typically combine multiple biomarkers by maximizing the Area Under the ROC Curve (AUC), assuming a gold standard exists or that biomarkers follow a multivariate normal distribution. However, practical diagnostic settings require both optimal combination coefficients and an effective cutoff value, and the reference test may be imperfect. In this paper, we propose a two-stage method for identifying the optimal linear combination and cutoff value based on the Youden index. First, it maximizes an approximation of the empirical AUC to estimate the optimal linear coefficients for combining multiple biomarkers. Then, it maximizes the empirical Youden index to determine the optimal cutoff point for disease classification. Under the semiparametric single index model and regularity conditions, the estimators for the linear coefficients, cutoff point, and Youden index are consistent. This method is also applicable when the reference standard is imperfect. We demonstrate the performance of our method through simulations and apply it to construct a diagnostic scale for Chinese medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17471v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ao Sun, Yanting Li, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Integrated differential analysis of multi-omics data using a joint mixture model: idiffomix</title>
      <link>https://arxiv.org/abs/2412.17511</link>
      <description>arXiv:2412.17511v1 Announce Type: new 
Abstract: Gene expression and DNA methylation are two interconnected biological processes and understanding their relationship is important in advancing understanding in diverse areas, including disease pathogenesis, environmental adaptation, developmental biology, and therapeutic responses. Differential analysis, including the identification of differentially methylated cytosine-guanine dinucleotide (CpG) sites (DMCs) and differentially expressed genes (DEGs) between two conditions, such as healthy and affected samples, can aid understanding of biological processes and disease progression. Typically, gene expression and DNA methylation data are analysed independently to identify DMCs and DEGs which are further analysed to explore relationships between them. Such approaches ignore the inherent dependencies and biological structure within these related data.
  A joint mixture model is proposed that integrates information from the two data types at the modelling stage to capture their inherent dependency structure, enabling simultaneous identification of DMCs and DEGs. The model leverages a joint likelihood function that accounts for the nested structure in the data, with parameter estimation performed using an expectation-maximisation algorithm.
  Performance of the proposed method, idiffomix, is assessed through a thorough simulation study and application to a publicly available breast cancer dataset. Several genes, identified as non-differentially expressed when the data types were modelled independently, had high likelihood of being differentially expressed when associated methylation data were integrated into the analysis. The idiffomix approach highlights the advantage of an integrated analysis via a joint mixture model over independent analyses of the two data types; genome-wide and cross-omics information is simultaneously utilised providing a more comprehensive view.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17511v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Koyel Majumdar, Florence Jaffr\'ezic, Andrea Rau, Isobel Claire Gormley, Thomas Brendan Murphy</dc:creator>
    </item>
    <item>
      <title>Resampling NANCOVA: Nonparametric Analysis of Covariance in Small Samples</title>
      <link>https://arxiv.org/abs/2412.17513</link>
      <description>arXiv:2412.17513v1 Announce Type: new 
Abstract: Analysis of covariance is a crucial method for improving precision of statistical tests for factor effects in randomized experiments. However, existing solutions suffer from one or more of the following limitations: (i) they are not suitable for ordinal data (as endpoints or explanatory variables); (ii) they require semiparametric model assumptions; (iii) they are inapplicable to small data scenarios due to often poor type-I error control; or (iv) they provide only approximate testing procedures and (asymptotically) exact test are missing. In this paper, we investigate a resampling approach to the NANCOVA framework, which is a fully nonparametric model based on relative effects that allows for an arbitrary number of covariates and groups, where both outcome variable (endpoint) and covariates can be metric or ordinal. Thereby, we evaluate novel NANCOVA tests and a nonparametric competitor test without covariate adjustment in extensive simulations. Unlike approximate tests in the NANCOVA framework, our resampling version showed good performance in small sample scenarios and maintained the nominal type-I error well. Resampling NANCOVA also provided consistently high power: up to 26% higher than the test without covariate adjustment in a small sample scenario with 4 groups and two covariates. Moreover, we prove that resampling NANCOVA provides an asymptotically exact testing procedure, which makes it the first one in the NANCOVA framework. In summary, resampling NANCOVA can be considered a viable tool for analysis of covariance that overcomes issues (i) - (iv).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17513v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantin Emil Thiel, Paavo Sattler, Arne C Bathke, Georg Zimmermann</dc:creator>
    </item>
    <item>
      <title>Bayesian Multilevel Bivariate Spatial Modelling of Italian School Data</title>
      <link>https://arxiv.org/abs/2412.17710</link>
      <description>arXiv:2412.17710v1 Announce Type: new 
Abstract: This paper studies the relationship between the student's abilities in the second year of high school and the infrastructural endowment in all Italian municipalities, using spatial Bayesian modelling. Municipal student scores are obtained by averaging standardized and spatially homogeneous indicators of student outcomes provided by the Invalsi Institute for two subjects, Italian and Mathematics. Given the nature of the data, we employ a multilevel regression model assuming a bivariate Intrinsic Conditionally Autoregressive (ICAR) latent effect to explain the spatial variability and account for the correlation between the two subjects. Bayesian model estimation is obtained by the Integrated Nested Laplace Approximation (INLA), implemented in the \texttt{R-INLA} package. We find that alongside a significant association with the current state of school infrastructure and facilities, spatially structured latent effects are still necessary to explain the different student outcomes across municipalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17710v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo Cefalo, Alessio Pollice, Virgilio G\'omez-Rubio</dc:creator>
    </item>
    <item>
      <title>Ergodic Network Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2412.17779</link>
      <description>arXiv:2412.17779v1 Announce Type: new 
Abstract: We propose a novel framework for Network Stochastic Differential Equations (N-SDE), where each node in a network is governed by an SDE influenced by interactions with its neighbors. The evolution of each node is driven by the interplay of three key components: the node's intrinsic dynamics (\emph{momentum effect}), feedback from neighboring nodes (\emph{network effect}), and a \emph{stochastic volatility} term modeled by Brownian motion.
  Our primary objective is to estimate the parameters of the N-SDE system from high-frequency discrete-time observations. The motivation behind this model lies in its ability to analyze very high-dimensional time series by leveraging the inherent sparsity of the underlying network graph.
  We consider two distinct scenarios: \textit{i) known network structure}: the graph is fully specified, and we establish conditions under which the parameters can be identified, considering the quadratic growth of the parameter space with the number of edges. \textit{ii) unknown network structure}: the graph must be inferred from the data. For this, we develop an iterative procedure using adaptive Lasso, tailored to a specific subclass of N-SDE models. In this work, we assume the network graph is oriented, paving the way for novel applications of SDEs in causal inference, enabling the study of cause-effect relationships in dynamic systems.
  Through extensive simulation studies, we demonstrate the performance of our estimators across various graph topologies in high-dimensional settings. We also showcase the framework's applicability to real-world datasets, highlighting its potential for advancing the analysis of complex networked systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17779v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Francesco Iafrate, Stefano Iacus</dc:creator>
    </item>
    <item>
      <title>A Proximal Newton Adaptive Importance Sampler</title>
      <link>https://arxiv.org/abs/2412.16558</link>
      <description>arXiv:2412.16558v1 Announce Type: cross 
Abstract: Adaptive importance sampling (AIS) algorithms are a rising methodology in signal processing, statistics, and machine learning. An effective adaptation of the proposals is key for the success of AIS. Recent works have shown that gradient information about the involved target density can greatly boost performance, but its applicability is restricted to differentiable targets. In this paper, we propose a proximal Newton adaptive importance sampler, an algorithm for estimating expectations with respect to non-smooth target distributions. We utilize a scaled Newton proximal gradient to adapt proposal distributions, obtaining efficient and optimized moves even when the target is not a differentiable density. We demonstrate the utility of the algorithm on two scenarios, either involving convex constraints or non-smooth sparse priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16558v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V\'ictor Elvira, \'Emilie Chouzenoux, O. Deniz Akyildiz</dc:creator>
    </item>
    <item>
      <title>A Meta-Learning Approach to Bayesian Causal Discovery</title>
      <link>https://arxiv.org/abs/2412.16577</link>
      <description>arXiv:2412.16577v1 Announce Type: cross 
Abstract: Discovering a unique causal structure is difficult due to both inherent identifiability issues, and the consequences of finite data. As such, uncertainty over causal structures, such as those obtained from a Bayesian posterior, are often necessary for downstream tasks. Finding an accurate approximation to this posterior is challenging, due to the large number of possible causal graphs, as well as the difficulty in the subproblem of finding posteriors over the functional relationships of the causal edges. Recent works have used meta-learning to view the problem of estimating the maximum a-posteriori causal graph as supervised learning. Yet, these methods are limited when estimating the full posterior as they fail to encode key properties of the posterior, such as correlation between edges and permutation equivariance with respect to nodes. Further, these methods also cannot reliably sample from the posterior over causal structures. To address these limitations, we propose a Bayesian meta learning model that allows for sampling causal structures from the posterior and encodes these key properties. We compare our meta-Bayesian causal discovery against existing Bayesian causal discovery methods, demonstrating the advantages of directly learning a posterior over causal structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16577v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anish Dhir, Matthew Ashman, James Requeima, Mark van der Wilk</dc:creator>
    </item>
    <item>
      <title>Direct Inversion for the Squared Bessel Process and Applications</title>
      <link>https://arxiv.org/abs/2412.16655</link>
      <description>arXiv:2412.16655v1 Announce Type: cross 
Abstract: In this paper we derive a new direct inversion method to simulate squared Bessel processes. Since the transition probability of these processes can be represented by a non-central chi-square distribution, we construct an efficient and accurate algorithm to simulate non-central chi-square variables. In this method, the dimension of the squared Bessel process, equivalently the degrees of freedom of the chi-square distribution, is treated as a variable. We therefore use a two-dimensional Chebyshev expansion to approximate the inverse function of the central chi-square distribution with one variable being the degrees of freedom. The method is accurate and efficient for any value of degrees of freedom including the computationally challenging case of small values. One advantage of the method is that noncentral chi-square samples can be generated for a whole range of values of degrees of freedom using the same Chebyshev coefficients. The squared Bessel process is a building block for the well-known Cox-Ingersoll-Ross (CIR) processes, which can be generated from squared Bessel processes through time change and linear transformation. Our direct inversion method thus allows the efficient and accurate simulation of these processes, which are used as models in a wide variety of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16655v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>q-fin.CP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon J. A. Malham, Anke Wiese, Yifan Xu</dc:creator>
    </item>
    <item>
      <title>From Correlation to Causation: Understanding Climate Change through Causal Analysis and LLM Interpretations</title>
      <link>https://arxiv.org/abs/2412.16691</link>
      <description>arXiv:2412.16691v1 Announce Type: cross 
Abstract: This research presents a three-step causal inference framework that integrates correlation analysis, machine learning-based causality discovery, and LLM-driven interpretations to identify socioeconomic factors influencing carbon emissions and contributing to climate change. The approach begins with identifying correlations, progresses to causal analysis, and enhances decision making through LLM-generated inquiries about the context of climate change. The proposed framework offers adaptable solutions that support data-driven policy-making and strategic decision-making in climate-related contexts, uncovering causal relationships within the climate change domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16691v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shan Shan</dc:creator>
    </item>
    <item>
      <title>MOODE: An R Package for Multi-Objective Optimal Design of Experiments</title>
      <link>https://arxiv.org/abs/2412.17158</link>
      <description>arXiv:2412.17158v1 Announce Type: cross 
Abstract: We describe the R package MOODE and demonstrate its use to find multi-objective optimal experimental designs. Multi-Objective Optimal Design of Experiments (MOODE) targets the experimental objectives directly, ensuring that the full set of research questions is answered as economically as possible. In particular, individual criteria aimed at optimizing inference are combined with lack-of-fit and MSE-based components in compound optimality criteria to target multiple and competing objectives reflecting the priorities and aims of the experimentation. The package implements either a point exchange or coordinate exchange algorithm as appropriate to find nearly optimal designs. We demonstrate the functionality of MOODE through the application of the methodology to two case studies of varying complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17158v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasiliki Koutra, Olga Egorova, Steven G. Gilmour, Luzia A. Trinca</dc:creator>
    </item>
    <item>
      <title>HDTSA: An R package for high-dimensional time series analysis</title>
      <link>https://arxiv.org/abs/2412.17341</link>
      <description>arXiv:2412.17341v1 Announce Type: cross 
Abstract: High-dimensional time series analysis has become increasingly important in fields such as finance, economics, and biology. The two primary tasks for high-dimensional time series analysis are modeling and statistical inference, which aim to capture the underlying dynamic structure and investigate valuable information in the data. This paper presents the HDTSA package for R, which provides a general framework for analyzing high-dimensional time series data. This package includes four dimension reduction methods for modeling: factor models, principal component analysis, CP-decomposition, and cointegration analysis. It also implements two recently proposed white noise test and martingale difference test in high-dimensional scenario for statistical inference. The methods provided in this package can help users to analyze high-dimensional time series data and make reliable predictions. To improve computational efficiency, the HDTSA package integrates C++ through the Rcpp package. We illustrate the functions of the HDTSA package using simulated examples and real-world applications from finance and economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17341v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Jing He, Chen Lin, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>A Necessary and Sufficient Condition for Size Controllability of Heteroskedasticity Robust Test Statistics</title>
      <link>https://arxiv.org/abs/2412.17470</link>
      <description>arXiv:2412.17470v1 Announce Type: cross 
Abstract: We revisit size controllability results in P\"otscher and Preinerstorfer (2021) concerning heteroskedasticity robust test statistics in regression models. For the special, but important, case of testing a single restriction (e.g., a zero restriction on a single coefficient), we povide a necessary and sufficient condition for size controllability, whereas the condition in P\"otscher and Preinerstorfer (2021) is, in general, only sufficient (even in the case of testing a single restriction).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17470v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benedikt M. P\"otscher, David Preinerstorfer</dc:creator>
    </item>
    <item>
      <title>To Study Properties of a Known Procedure in Adaptive Sequential Sampling Design</title>
      <link>https://arxiv.org/abs/2412.17791</link>
      <description>arXiv:2412.17791v1 Announce Type: cross 
Abstract: We revisit the procedure proposed by Bhandari et al. (2009) in the context of two-treatment clinical trials, with the objective of minimizing the applications of a less effective drug to the least number of patients. Our focus is on an adaptive sequential procedure that is both simple and intuitive. Our findings show that the expected number of applications of the less effective drug remains finite. In contrast, Bhandari et al. (2009) observed that this number increases logarithmically with the total sample size. We attribute this discrepancy to differences in their choice of starting sample size and the method of analysis employed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17791v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sampurna Kundu, Jayant Jha, Subir Kumar Bhandari</dc:creator>
    </item>
    <item>
      <title>New iterative algorithms for estimation of item functioning</title>
      <link>https://arxiv.org/abs/2302.12648</link>
      <description>arXiv:2302.12648v3 Announce Type: replace 
Abstract: This paper explores innovations to parameter estimation in generalized linear and nonlinear models, which may be used in item response modeling to account for guessing/pretending or slipping/dissimulation and for the effect of covariates. We introduce a new implementation of the EM algorithm and propose a new algorithm based on the parametrized link function. The two novel iterative algorithms are compared to existing methods in a simulation study. Additionally, the study examines software implementation, including the specification of initial values for numerical algorithms and asymptotic properties with an estimation of standard errors. Overall, the newly proposed algorithm based on the parametrized link function outperforms other procedures, especially for small sample sizes. Moreover, the newly implemented EM algorithm provides additional information regarding respondents' inclination to guess or pretend and slip or dissimulate when answering the item. The study also discusses applications of the methods in the context of the detection of differential item functioning and addresses the measurement error. Methods are offered in the difNLR package and in the interactive application of the ShinyItemAnalysis package; demonstration is provided using real data from psychological and educational assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12648v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ad\'ela Hladk\'a, Patr\'icia Martinkov\'a, Marek Brabec</dc:creator>
    </item>
    <item>
      <title>Non-parametric Hypothesis Tests for Distributional Group Symmetry</title>
      <link>https://arxiv.org/abs/2307.15834</link>
      <description>arXiv:2307.15834v2 Announce Type: replace 
Abstract: Symmetry plays a central role in the sciences, machine learning, and statistics. For situations in which data are known to obey a symmetry, a multitude of methods that exploit symmetry have been developed. Statistical tests for the presence or absence of general group symmetry, however, are largely non-existent. This work formulates non-parametric hypothesis tests, based on a single independent and identically distributed sample, for distributional symmetry under a specified group. We provide a general formulation of tests for symmetry that apply to two broad settings. The first setting tests for the invariance of a marginal or joint distribution under the action of a compact group. Here, an asymptotically unbiased test only requires a computable metric on the space of probability distributions and the ability to sample uniformly random group elements. Building on this, we propose an easy-to-implement conditional Monte Carlo test and prove that it achieves exact $p$-values with finitely many observations and Monte Carlo samples. The second setting tests for the invariance or equivariance of a conditional distribution under the action of a locally compact group. We show that the test for conditional invariance or equivariance can be formulated as particular tests of conditional independence. We implement these tests from both settings using kernel methods and study them empirically on synthetic data. Finally, we apply them to testing for symmetry in geomagnetic satellite data and in two problems from high-energy particle physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15834v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenny Chiu, Benjamin Bloem-Reddy</dc:creator>
    </item>
    <item>
      <title>Generalized linear models with spatial dependence and a functional covariate</title>
      <link>https://arxiv.org/abs/2402.13472</link>
      <description>arXiv:2402.13472v2 Announce Type: replace 
Abstract: We extend generalized functional linear models under independence to a situation in which a functional covariate is related to a scalar response variable that exhibits spatial dependence-a complex yet prevalent phenomenon. For estimation, we apply basis expansion and truncation for dimension reduction of the covariate process followed by a composite likelihood estimating equation to handle the spatial dependency. We establish asymptotic results for the proposed model under a repeating lattice asymptotic context, allowing us to construct a confidence interval for the spatial dependence parameter and a confidence band for the regression parameter function. A binary conditionals model with functional covariates is presented as a concrete illustration and is used in simulation studies to verify the applicability of the asymptotic inferential results. We apply the proposed model to a problem in which the objective is to relate annual corn yield in counties of states in the Midwestern United States to daily maximum temperatures from April to September in those same geographic regions. The extension to an expanding lattice context is further discussed in the supplement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13472v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sooran Kim, Mark S. Kaiser, Xiongtao Dai</dc:creator>
    </item>
    <item>
      <title>Exploring Spatial Generalized Functional Linear Models: A Comparative Simulation Study and Analysis of COVID-19</title>
      <link>https://arxiv.org/abs/2403.03389</link>
      <description>arXiv:2403.03389v3 Announce Type: replace 
Abstract: Implementation of spatial generalized linear models with a functional covariate can be accomplished through the use of a truncated basis expansion of the covariate process. In practice, one must select a truncation level for use. We compare five criteria for the selection of an appropriate truncation level, including AIC and BIC based on a log composite likelihood, a fraction of variance explained criterion, a fitted mean squared error, and a prediction error with one standard error rule. Based on the use of extensive simulation studies, we propose that BIC constitutes a reasonable default criterion for the selection of the truncation level for use in a spatial functional generalized linear model. In addition, we demonstrate that the spatial model with a functional covariate outperforms other models when the data contain spatial structure and response variables are in fact influenced by a functional covariate process. We apply the spatial functional generalized linear model to a problem in which the objective is to relate COVID-19 vaccination rates in counties of states in the Midwestern United States to the number of new cases from previous weeks in those same geographic regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03389v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sooran Kim, Mark S. Kaiser, Xiongtao Dai</dc:creator>
    </item>
    <item>
      <title>Quantile balancing inverse probability weighting for non-probability samples</title>
      <link>https://arxiv.org/abs/2403.09726</link>
      <description>arXiv:2403.09726v3 Announce Type: replace 
Abstract: The use of non-probability data sources for statistical purposes has become increasingly popular in recent years, also in official statistics. However, statistical inference based on non-probability samples is made more difficult by nature of them being biased and not representative of the target population. In this paper we propose quantile balancing inverse probability weighting estimator (QBIPW) for non-probability samples. We use the idea of Harms and Duchesne (2006) which allows to include quantile information in the estimation process so known totals and distribution for auxiliary variables are being reproduced. We discuss the estimation of the QBIPW probabilities and its variance. Our simulation study has demonstrated that the proposed estimators are robust against model mis-specification and, as a result, help to reduce bias and mean squared error. Finally, we applied the proposed methods to estimate the share of vacancies aimed at Ukrainian workers in Poland using an integrated set of administrative and survey data about job vacancies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09726v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Ber\k{e}sewicz, Marcin Szymkowiak, Piotr Chlebicki</dc:creator>
    </item>
    <item>
      <title>Constrained least squares simplicial-simplicial regression</title>
      <link>https://arxiv.org/abs/2403.19835</link>
      <description>arXiv:2403.19835v3 Announce Type: replace 
Abstract: Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. For this setting, constrained least squares, where the regression coefficients themselves lie within the simplex, is proposed. The model is transformation-free but the adoption of a power transformation is straightforward, it can treat more than one compositional datasets as predictors and offers the possibility of weights among the simplicial predictors. Among the model's advantages are its ability to treat zeros in a natural way and a highly computationally efficient algorithm to estimate its coefficients. Resampling based hypothesis testing procedures are employed regarding inference, such as linear independence, and equality of the regression coefficients to some pre-specified values. The strategy behind the formulation of the new model is implemented is related to an existing methodology, that is of the same spirit, showcasing how other similar models can be employed as well. Finally, the performance of the proposed technique and its comparison to the existing methodology takes place using simulation studies and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19835v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Multiple testing with anytime-valid Monte-Carlo p-values</title>
      <link>https://arxiv.org/abs/2404.15586</link>
      <description>arXiv:2404.15586v3 Announce Type: replace 
Abstract: In contemporary problems involving genetic or neuroimaging data, thousands of hypotheses need to be tested. Due to their high power, and finite sample guarantees on type-I error under weak assumptions, Monte-Carlo permutation tests are often considered as gold standard for these settings. However, the enormous computational effort required for (thousands of) permutation tests is a major burden. Recently, Fischer and Ramdas (2024) constructed a permutation test for a single hypothesis in which the permutations are drawn sequentially one-by-one and the testing process can be stopped at any point without inflating the type-I error. They showed that the number of permutations can be substantially reduced (under null and alternative) while the power remains similar. We show how their approach can be modified to make it suitable for a broad class of multiple testing procedures and particularly discuss its use with the Benjamini-Hochberg procedure. The resulting method provides valid error rate control and outperforms all existing approaches significantly in terms of power and/or required computational time. We provide fast implementations and illustrate its application on large datasets, both synthetic and real.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15586v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Timothy Barry, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>How to build your latent Markov model -- the role of time and space</title>
      <link>https://arxiv.org/abs/2406.19157</link>
      <description>arXiv:2406.19157v3 Announce Type: replace 
Abstract: Statistical models that involve latent Markovian state processes have become immensely popular tools for analysing time series and other sequential data. However, the plethora of model formulations, the inconsistent use of terminology, and the various inferential approaches and software packages can be overwhelming to practitioners, especially when they are new to this area. With this review-like paper, we thus aim to provide guidance for both statisticians and practitioners working with latent Markov models by offering a unifying view on what otherwise are often considered separate model classes, from hidden Markov models over state-space models to Markov-modulated Poisson processes. In particular, we provide a roadmap for identifying a suitable latent Markov model formulation given the data to be analysed. Furthermore, we emphasise that it is key to applied work with any of these model classes to understand how recursive techniques exploiting the models' dependence structure can be used for inference. The R package LaMa adapts this unified view and provides an easy-to-use framework for very fast (C++ based) numerical maximum likelihood estimation of any of the models discussed in this paper, allowing users to tailor a latent Markov model to their data using a Lego-type approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19157v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sina Mews, Jan-Ole Koslik, Roland Langrock</dc:creator>
    </item>
    <item>
      <title>Joint Estimation of Conditional Mean and Covariance for Unbalanced Panels</title>
      <link>https://arxiv.org/abs/2410.21858</link>
      <description>arXiv:2410.21858v4 Announce Type: replace 
Abstract: We develop a nonparametric, kernel-based joint estimator for conditional mean and covariance matrices in large and unbalanced panels. The estimator is supported by rigorous consistency results and finite-sample guarantees, ensuring its reliability for empirical applications in Finance. We apply it to an extensive panel of monthly US stock excess returns from 1962 to 2021, using macroeconomic and firm-specific covariates as conditioning variables. The estimator effectively captures time-varying cross-sectional dependencies, demonstrating robust statistical and economic performance. We find that idiosyncratic risk explains, on average, more than 75% of the cross-sectional variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21858v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damir Filipovic, Paul Schneider</dc:creator>
    </item>
    <item>
      <title>Improved order selection method for hidden Markov models: a case study with movement data</title>
      <link>https://arxiv.org/abs/2411.18826</link>
      <description>arXiv:2411.18826v2 Announce Type: replace 
Abstract: Hidden Markov models (HMMs) are a versatile statistical framework commonly used in ecology to characterize behavioural patterns from animal movement data. In HMMs, the observed data depend on a finite number of underlying hidden states, generally interpreted as the animal's unobserved behaviour. The number of states is a crucial parameter, controlling the trade-off between ecological interpretability of behaviours (fewer states) and the goodness of fit of the model (more states). Selecting the number of states, commonly referred to as order selection, is notoriously challenging. Common model selection metrics, such as AIC and BIC, often perform poorly in determining the number of states, particularly when models are misspecified. Building on existing methods for HMMs and mixture models, we propose a double penalized likelihood maximum estimate (DPMLE) for the simultaneous estimation of the number of states and parameters of non-stationary HMMs. The DPMLE differs from traditional information criteria by using two penalty functions on the stationary probabilities and state-dependent parameters. For non-stationary HMMs, forward and backward probabilities are used to approximate stationary probabilities. Using a simulation study that includes scenarios with additional complexity in the data, we compare the performance of our method with that of AIC and BIC. We also illustrate how the DPMLE differs from AIC and BIC using narwhal (Monodon monoceros) movement data. The proposed method outperformed AIC and BIC in identifying the correct number of states under model misspecification. Furthermore, its capacity to handle non-stationary dynamics allowed for more realistic modeling of complex movement data, offering deeper insights into narwhal behaviour. Our method is a powerful tool for order selection in non-stationary HMMs, with potential applications extending beyond the field of ecology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18826v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fanny Dupont, Marianne Marcoux, Nigel Hussey, Marie Auger-M\'eth\'e</dc:creator>
    </item>
    <item>
      <title>A robust, scalable K-statistic for quantifying immune cell clustering in spatial proteomics data</title>
      <link>https://arxiv.org/abs/2412.08498</link>
      <description>arXiv:2412.08498v2 Announce Type: replace 
Abstract: Spatial summary statistics based on point process theory are widely used to quantify the spatial organization of cell populations in single-cell spatial proteomics data. Among these, Ripley's $K$ is a popular metric for assessing whether cells are spatially clustered or are randomly dispersed. However, the key assumption of spatial homogeneity is frequently violated in spatial proteomics data, leading to overestimates of cell clustering and colocalization. To address this, we propose a novel $K$-based method, termed \textit{KAMP} (\textbf{K} adjustment by \textbf{A}nalytical \textbf{M}oments of the \textbf{P}ermutation distribution), for quantifying the spatial organization of cells in spatial proteomics samples. \textit{KAMP} leverages background cells in each sample along with a new closed-form representation of the first and second moments of the permutation distribution of Ripley's $K$ to estimate an empirical null model. Our method is robust to inhomogeneity, computationally efficient even in large datasets, and provides approximate $p$-values for testing spatial clustering and colocalization. Methodological developments are motivated by a spatial proteomics study of 103 women with ovarian cancer, where our analysis using \textit{KAMP} shows a positive association between immune cell clustering and overall patient survival. Notably, we also find evidence that using $K$ without correcting for sample inhomogeneity may bias hazard ratio estimates in downstream analyses. \textit{KAMP} completes this analysis in just 5 minutes, compared to 538 minutes for the only competing method that adequately addresses inhomogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08498v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Wrobel, Hoseung Song</dc:creator>
    </item>
    <item>
      <title>Analyzing zero-inflated clustered longitudinal ordinal outcomes using GEE-type models with an application to dental fluorosis studies</title>
      <link>https://arxiv.org/abs/2412.11348</link>
      <description>arXiv:2412.11348v2 Announce Type: replace 
Abstract: Motivated by the Iowa Fluoride Study (IFS) dataset, which comprises zero-inflated multi-level ordinal responses on tooth fluorosis, we develop an estimation scheme leveraging generalized estimating equations (GEEs) and James-Stein shrinkage. Previous analyses of this cohort study primarily focused on caries (count response) or employed a Bayesian approach to the ordinal fluorosis outcome. This study is based on the expanded dataset that now includes observations for age 23, whereas earlier works were restricted to ages 9, 13, and/or 17 according to the participants' ages at the time of measurement. The adoption of a frequentist perspective enhances the interpretability to a broader audience. Over a choice of several covariance structures, separate models are formulated for the presence (zero versus non-zero score) and severity (non-zero ordinal scores) of fluorosis, which are then integrated through shared regression parameters. This comprehensive framework effectively identifies risk or protective effects of dietary and non-dietary factors on dental fluorosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11348v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shoumi Sarkar, Anish Mukherjee, Jeremy Gaskins, Steven Levy, Peihua Qiu, Somnath Datta</dc:creator>
    </item>
    <item>
      <title>A partial likelihood approach to tree-based density modeling and its application in Bayesian inference</title>
      <link>https://arxiv.org/abs/2412.11692</link>
      <description>arXiv:2412.11692v2 Announce Type: replace 
Abstract: Tree-based models for probability distributions are usually specified using a predetermined, data-independent collection of candidate recursive partitions of the sample space. To characterize an unknown target density in detail over the entire sample space, candidate partitions must have the capacity to expand deeply into all areas of the sample space with potential non-zero sampling probability. Such an expansive system of partitions often incurs prohibitive computational costs and makes inference prone to overfitting, especially in regions with little probability mass. Existing models typically make a compromise and rely on relatively shallow trees. This hampers one of the most desirable features of trees, their ability to characterize local features, and results in reduced statistical efficiency. Traditional wisdom suggests that this compromise is inevitable to ensure coherent likelihood-based reasoning, as a data-dependent partition system that allows deeper expansion only in regions with more observations would induce double dipping of the data and thus lead to inconsistent inference. We propose a simple strategy to restore coherency while allowing the candidate partitions to be data-dependent, using Cox's partial likelihood. This strategy parametrizes the tree-based sampling model according to the allocation of probability mass based on the observed data, and yet under appropriate specification, the resulting inference remains valid. Our partial likelihood approach is broadly applicable to existing likelihood-based methods and in particular to Bayesian inference on tree-based models. We give examples in density estimation in which the partial likelihood is endowed with existing priors on tree-based models and compare with the standard, full-likelihood approach. The results show substantial gains in estimation accuracy and computational efficiency from using the partial likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11692v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Ma, Benedetta Bruni</dc:creator>
    </item>
    <item>
      <title>Towards Instance-Wise Calibration: Local Amortized Diagnostics and Reshaping of Conditional Densities (LADaR)</title>
      <link>https://arxiv.org/abs/2205.14568</link>
      <description>arXiv:2205.14568v5 Announce Type: replace-cross 
Abstract: There is a growing interest in conditional density estimation and generative modelling of a target $y$ given complex inputs $\mathbf{x}$. However, off-the-shelf methods often lack instance-wise calibration -- that is, for individual inputs $\mathbf{x}$, the individual estimated probabilities can be very different from the true probabilities, even when the estimates are reasonable when averaged over the entire population. This paper introduces the LADaR (Local Amortized Diagnostics and Reshaping of Conditional Densities) framework and proposes an algorithm called $\texttt{Cal-PIT}$ that produces interpretable local calibration diagnostics and includes a mechanism to recalibrate the initial model. Our $\texttt{Cal-PIT}$ algorithm learns a single local probability-probability map from calibration data to assess and quantify where corrections are needed across the feature space. When necessary, it reshapes the initial distribution into an estimate with approximate instance-wise calibration. We illustrate the LADaR framework by applying $\texttt{Cal-PIT}$ to synthetic examples, including probabilistic forecasting with sequences of images as inputs, akin to predicting the wind speed of tropical cyclones from satellite imagery. Our main science application is conditional density estimation of galaxy distances given imaging data (so-called photometric redshift estimation). On a benchmark photometric redshift data challenge, $\texttt{Cal-PIT}$ achieves better conditional density estimation (as measured by the conditional density estimation loss) than all 11 other literature methods tested. This demonstrates its potential for meeting the stringent photometric redshift requirements for next generation weak gravitational lensing analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.14568v5</guid>
      <category>stat.ML</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Biprateep Dey, David Zhao, Brett H. Andrews, Jeffrey A. Newman, Rafael Izbicki, Ann B. Lee</dc:creator>
    </item>
    <item>
      <title>On the Temporal-spatial Analysis of Estimating Urban Traffic Patterns Via GPS Trace Data of Car-hailing Vehicles</title>
      <link>https://arxiv.org/abs/2306.07456</link>
      <description>arXiv:2306.07456v2 Announce Type: replace-cross 
Abstract: Car-hailing services have become a prominent data source for urban traffic studies. Extracting useful information from car-hailing trace data is essential for effective traffic management, while discrepancies between car-hailing vehicles and urban traffic should be considered. This paper proposes a generic framework for estimating and analyzing urban traffic patterns using car-hailing trace data. The framework consists of three layers: the data layer, the interactive software layer, and the processing method layer. By pre-processing car-hailing GPS trace data with operations such as data cutting, map matching, and trace correction, the framework generates tensor matrices that estimate traffic patterns for car-hailing vehicle flow and average road speed. An analysis block based on these matrices examines the relationships and differences between car-hailing vehicles and urban traffic patterns, which have been overlooked in previous research. Experimental results demonstrate the effectiveness of the proposed framework in examining temporal-spatial patterns of car-hailing vehicles and urban traffic. For temporal analysis, urban road traffic displays a bimodal characteristic while car-hailing flow exhibits a 'multi-peak' pattern, fluctuating significantly during holidays and thus generating a hierarchical structure. For spatial analysis, the heat maps generated from the matrices exhibit certain discrepancies, but the spatial distribution of hotspots and vehicle aggregation areas remains similar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07456v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiannan Mao, Lan Liu, Hao Huang, Weike Lu, Kaiyu Yang, Tianli Tang, Haotian Shi</dc:creator>
    </item>
    <item>
      <title>Variational Sequential Optimal Experimental Design using Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2306.10430</link>
      <description>arXiv:2306.10430v2 Announce Type: replace-cross 
Abstract: We present variational sequential optimal experimental design (vsOED), a novel method for optimally designing a finite sequence of experiments within a Bayesian framework with information-theoretic criteria. vsOED employs a one-point reward formulation with variational posterior approximations, providing a provable lower bound to the expected information gain. Numerical methods are developed following an actor-critic reinforcement learning approach, including derivation and estimation of variational and policy gradients to optimize the design policy, and posterior approximation using Gaussian mixture models and normalizing flows. vsOED accommodates nuisance parameters, implicit likelihoods, and multiple candidate models, while supporting flexible design criteria that can target designs for model discrimination, parameter inference, goal-oriented prediction, and their weighted combinations. We demonstrate vsOED across various engineering and science applications, illustrating its superior sample efficiency compared to existing sequential experimental design algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10430v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanggang Shen, Jiayuan Dong, Xun Huan</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Machine Learning with Multi-source Data</title>
      <link>https://arxiv.org/abs/2309.02211</link>
      <description>arXiv:2309.02211v3 Announce Type: replace-cross 
Abstract: Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the convergence rate. Our proposal can be seen as a distributionally robust federated learning approach that is computationally efficient and easy to implement using arbitrary machine learning base algorithms, satisfies some privacy constraints, and has a nice interpretation of different sources' importance for predicting a given target covariate distribution. We demonstrate the performance of our proposed group distributionally robust method on simulated and real data with random forests and neural networks as base-learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02211v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Peter B\"uhlmann, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Evaluating Interventional Reasoning Capabilities of Large Language Models</title>
      <link>https://arxiv.org/abs/2404.05545</link>
      <description>arXiv:2404.05545v2 Announce Type: replace-cross 
Abstract: Numerous decision-making tasks require estimating causal effects under interventions on different parts of a system. As practitioners consider using large language models (LLMs) to automate decisions, studying their causal reasoning capabilities becomes crucial. A recent line of work evaluates LLMs ability to retrieve commonsense causal facts, but these evaluations do not sufficiently assess how LLMs reason about interventions. Motivated by the role that interventions play in causal inference, in this paper, we conduct empirical analyses to evaluate whether LLMs can accurately update their knowledge of a data-generating process in response to an intervention. We create benchmarks that span diverse causal graphs (e.g., confounding, mediation) and variable types, and enable a study of intervention-based reasoning. These benchmarks allow us to isolate the ability of LLMs to accurately predict changes resulting from their ability to memorize facts or find other shortcuts. We evaluate six LLMs on the benchmarks, finding that GPT models show promising accuracy at predicting the intervention effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05545v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tejas Kasetty, Divyat Mahajan, Gintare Karolina Dziugaite, Alexandre Drouin, Dhanya Sridhar</dc:creator>
    </item>
    <item>
      <title>Generalized Neyman Allocation for Locally Minimax Optimal Best-Arm Identification</title>
      <link>https://arxiv.org/abs/2405.19317</link>
      <description>arXiv:2405.19317v2 Announce Type: replace-cross 
Abstract: This study investigates an asymptotically locally minimax optimal algorithm for fixed-budget best-arm identification (BAI). We propose the Generalized Neyman Allocation (GNA) algorithm and demonstrate that its worst-case upper bound on the probability of misidentifying the best arm aligns with the worst-case lower bound under the small-gap regime, where the gap between the expected outcomes of the best and suboptimal arms is small. Our lower and upper bounds are tight, matching exactly including constant terms within the small-gap regime. The GNA algorithm generalizes the Neyman allocation for two-armed bandits (Neyman, 1934; Kaufmann et al., 2016) and refines existing BAI algorithms, such as those proposed by Glynn &amp; Juneja (2004). By proposing an asymptotically minimax optimal algorithm, we address the longstanding open issue in BAI (Kaufmann, 2020) and treatment choice (Kasy &amp; Sautmann, 202) by restricting a class of distributions to the small-gap regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19317v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
