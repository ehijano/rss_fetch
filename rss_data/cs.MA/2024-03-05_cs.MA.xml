<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Mar 2024 14:39:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Offline Fictitious Self-Play for Competitive Games</title>
      <link>https://arxiv.org/abs/2403.00841</link>
      <description>arXiv:2403.00841v1 Announce Type: new 
Abstract: Offline Reinforcement Learning (RL) has received significant interest due to its ability to improve policies in previously collected datasets without online interactions. Despite its success in the single-agent setting, offline multi-agent RL remains a challenge, especially in competitive games. Firstly, unaware of the game structure, it is impossible to interact with the opponents and conduct a major learning paradigm, self-play, for competitive games. Secondly, real-world datasets cannot cover all the state and action space in the game, resulting in barriers to identifying Nash equilibrium (NE). To address these issues, this paper introduces Off-FSP, the first practical model-free offline RL algorithm for competitive games. We start by simulating interactions with various opponents by adjusting the weights of the fixed dataset with importance sampling. This technique allows us to learn best responses to different opponents and employ the Offline Self-Play learning framework. In this framework, we further implement Fictitious Self-Play (FSP) to approximate NE. In partially covered real-world datasets, our methods show the potential to approach NE by incorporating any single-agent offline RL method. Experimental results in Leduc Hold'em Poker show that our method significantly improves performances compared with state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00841v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingxiao Chen, Weiji Xie, Weinan Zhang, Yong yu, Ying Wen</dc:creator>
    </item>
    <item>
      <title>Composite Distributed Learning and Synchronization of Nonlinear Multi-Agent Systems with Complete Uncertain Dynamics</title>
      <link>https://arxiv.org/abs/2403.00987</link>
      <description>arXiv:2403.00987v1 Announce Type: new 
Abstract: This paper addresses the challenging problem of composite synchronization and learning control in a network of multi-agent robotic manipulator systems operating under heterogeneous nonlinear uncertainties within a leader-follower framework. A novel two-layer distributed adaptive learning control strategy is introduced, comprising a first-layer distributed cooperative estimator and a second-layer decentralized deterministic learning controller. The primary objective of the first layer is to facilitate each robotic agent's estimation of the leader's information. The second layer is responsible for both enabling individual robot agents to track desired reference trajectories and accurately identifying and learning their nonlinear uncertain dynamics. The proposed distributed learning control scheme represents an advancement in the existing literature due to its ability to manage robotic agents with completely uncertain dynamics including uncertain mass matrices. This framework allows the robotic control to be environment-independent which can be used in various settings, from underwater to space where identifying system dynamics parameters is challenging. The stability and parameter convergence of the closed-loop system are rigorously analyzed using the Lyapunov method. Numerical simulations conducted on multi-agent robot manipulators validate the effectiveness of the proposed scheme. The identified nonlinear dynamics can be saved and reused whenever the system restarts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00987v1</guid>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emadodin Jandaghi, Dalton L. Stein, Adam Hoburg, Mingxi Zhou, Chengzhi Yuan</dc:creator>
    </item>
    <item>
      <title>Can Poverty Be Reduced by Acting on Discrimination? An Agent-based Model for Policy Making</title>
      <link>https://arxiv.org/abs/2403.01600</link>
      <description>arXiv:2403.01600v1 Announce Type: new 
Abstract: In the last decades, there has been a deceleration in the rates of poverty reduction, suggesting that traditional redistributive approaches to poverty mitigation could be losing effectiveness, and alternative insights to advance the number one UN Sustainable Development Goal are required. The criminalization of poor people has been denounced by several NGOs, and an increasing number of voices suggest that discrimination against the poor (a phenomenon known as \emph{aporophobia}) could be an impediment to mitigating poverty. In this paper, we present the novel Aporophobia Agent-Based Model (AABM) to provide evidence of the correlation between aporophobia and poverty computationally. We present our use case built with real-world demographic data and poverty-mitigation public policies (either enforced or under parliamentary discussion) for the city of Barcelona. We classify policies as discriminatory or non-discriminatory against the poor, with the support of specialized NGOs, and we observe the results in the AABM in terms of the impact on wealth inequality. The simulation provides evidence of the relationship between aporophobia and the increase of wealth inequality levels, paving the way for a new generation of poverty reduction policies that act on discrimination and tackle poverty as a societal problem (not only a problem of the poor).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01600v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alba Aguilera, Nieves Montes, Georgina Curto, Carles Sierra, Nardine Osman</dc:creator>
    </item>
    <item>
      <title>VITAMIN: A Compositional Framework for Model Checking of Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2403.02170</link>
      <description>arXiv:2403.02170v1 Announce Type: new 
Abstract: The verification of Multi-Agent Systems (MAS) poses a significant challenge. Various approaches and methodologies exist to address this challenge; however, tools that support them are not always readily available. Even when such tools are accessible, they tend to be hard-coded, lacking in compositionality, and challenging to use due to a steep learning curve. In this paper, we introduce a methodology designed for the formal verification of MAS in a modular and versatile manner, along with an initial prototype, that we named VITAMIN. Unlike existing verification methodologies and frameworks for MAS, VITAMIN is constructed for easy extension to accommodate various logics (for specifying the properties to verify) and models (for determining on what to verify such properties).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02170v1</guid>
      <category>cs.MA</category>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angelo Ferrando, Vadim Malvone</dc:creator>
    </item>
    <item>
      <title>A New Dynamic Distributed Planning Approach: Application to DPDP Problems</title>
      <link>https://arxiv.org/abs/2403.00805</link>
      <description>arXiv:2403.00805v1 Announce Type: cross 
Abstract: In this work, we proposed a new dynamic distributed planning approach that is able to take into account the changes that the agent introduces on his set of actions to be planned in order to take into account the changes that occur in his environment. Our approach fits into the context of distributed planning for distributed plans where each agent can produce its own plans. According to our approach the generation of the plans is based on the satisfaction of the constraints by the use of the genetic algorithms. Our approach is to generate, a new plan by each agent, whenever there is a change in its set of actions to plan. This in order to take into account the new actions introduced in its new plan. In this new plan, the agent takes, each time, as a new action set to plan all the old un-executed actions of the old plan and the new actions engendered by the changes and as a new initial state; the state in which the set of actions of the agent undergoes a change. In our work, we used a concrete case to illustrate and demonstrate the utility of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00805v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zakaria Tolba</dc:creator>
    </item>
    <item>
      <title>Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2403.01112</link>
      <description>arXiv:2403.01112v1 Announce Type: cross 
Abstract: In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01112v1</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyungho Na, Yunkyeong Seo, Il-chul Moon</dc:creator>
    </item>
    <item>
      <title>Optimal Integrated Task and Path Planning and Its Application to Multi-Robot Pickup and Delivery</title>
      <link>https://arxiv.org/abs/2403.01277</link>
      <description>arXiv:2403.01277v1 Announce Type: cross 
Abstract: We propose a generic multi-robot planning mechanism that combines an optimal task planner and an optimal path planner to provide a scalable solution for complex multi-robot planning problems. The Integrated planner, through the interaction of the task planner and the path planner, produces optimal collision-free trajectories for the robots. We illustrate our general algorithm on an object pick-and-drop planning problem in a warehouse scenario where a group of robots is entrusted with moving objects from one location to another in the workspace. We solve the task planning problem by reducing it into an SMT-solving problem and employing the highly advanced SMT solver Z3 to solve it. To generate collision-free movement of the robots, we extend the state-of-the-art algorithm Conflict Based Search with Precedence Constraints with several domain-specific constraints. We evaluate our integrated task and path planner extensively on various instances of the object pick-and-drop planning problem and compare its performance with a state-of-the-art multi-robot classical planner. Experimental results demonstrate that our planning mechanism can deal with complex planning problems and outperforms a state-of-the-art classical planner both in terms of computation time and the quality of the generated plan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01277v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aman Aryan, Manan Modi, Indranil Saha, Rupak Majumdar, Swarup Mohalik</dc:creator>
    </item>
    <item>
      <title>Summary Paper: Use Case on Building Collaborative Safe Autonomous Systems-A Robotdog for Guiding Visually Impaired People</title>
      <link>https://arxiv.org/abs/2403.01286</link>
      <description>arXiv:2403.01286v1 Announce Type: cross 
Abstract: This is a summary paper of a use case of a Robotdog dedicated to guide visually impaired people in complex environment like a smart intersection. In such scenarios, the Robotdog has to autonomously decide whether it is safe to cross the intersection or not in order to further guide the human. We leverage data sharing and collaboration between the Robotdog and other autonomous systems operating in the same environment. We propose a system architecture for autonomous systems through a separation of a collaborative decision layer, to enable collective decision making processes, where data about the environment, relevant to the Robotdog decision, together with evidences for trustworthiness about other systems and the environment are shared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01286v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aman Malhotra, Selma Saidi</dc:creator>
    </item>
    <item>
      <title>Cooperative Automated Driving for Bottleneck Scenarios in Mixed Traffic</title>
      <link>https://arxiv.org/abs/2403.01512</link>
      <description>arXiv:2403.01512v1 Announce Type: cross 
Abstract: Connected automated vehicles (CAV), which incorporate vehicle-to-vehicle (V2V) communication into their motion planning, are expected to provide a wide range of benefits for individual and overall traffic flow. A frequent constraint or required precondition is that compatible CAVs must already be available in traffic at high penetration rates. Achieving such penetration rates incrementally before providing ample benefits for users presents a chicken-and-egg problem that is common in connected driving development. Based on the example of a cooperative driving function for bottleneck traffic flows (e.g. at a roadblock), we illustrate how such an evolutionary, incremental introduction can be achieved under transparent assumptions and objectives. To this end, we analyze the challenge from the perspectives of automation technology, traffic flow, human factors and market, and present a principle that 1) accounts for individual requirements from each domain; 2) provides benefits for any penetration rate of compatible CAVs between 0 % and 100 % as well as upward-compatibility for expected future developments in traffic; 3) can strictly limit the negative effects of cooperation for any participant and 4) can be implemented with close-to-market technology. We discuss the technical implementation as well as the effect on traffic flow over a wide parameter spectrum for human and technical aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01512v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/IV55152.2023.10186638</arxiv:DOI>
      <arxiv:journal_reference>35th IEEE Intelligent Vehicles Symposium (IV 2023)</arxiv:journal_reference>
      <dc:creator>M. V. Baumann, J. Beyerer, H. S. Buck, B. Deml, S. Ehrhardt, Ch. Frese, D. Kleiser, M. Lauer, M. Roschani, M. Ruf, Ch. Stiller, P. Vortisch, J. R. Ziehn</dc:creator>
    </item>
    <item>
      <title>SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for Adaptive Real-Time Subtask Recognition</title>
      <link>https://arxiv.org/abs/2403.01816</link>
      <description>arXiv:2403.01816v1 Announce Type: cross 
Abstract: Instead of making behavioral decisions directly from the exponentially expanding joint observational-action space, subtask-based multi-agent reinforcement learning (MARL) methods enable agents to learn how to tackle different subtasks. Most existing subtask-based MARL methods are based on hierarchical reinforcement learning (HRL). However, these approaches often limit the number of subtasks, perform subtask recognition periodically, and can only identify and execute a specific subtask within the predefined fixed time period, which makes them inflexible and not suitable for diverse and dynamic scenarios with constantly changing subtasks. To break through above restrictions, a \textbf{S}liding \textbf{M}ultidimensional t\textbf{A}sk window based m\textbf{U}ti-agent reinforcement learnin\textbf{G} framework (SMAUG) is proposed for adaptive real-time subtask recognition. It leverages a sliding multidimensional task window to extract essential information of subtasks from trajectory segments concatenated based on observed and predicted trajectories in varying lengths. An inference network is designed to iteratively predict future trajectories with the subtask-oriented policy network. Furthermore, intrinsic motivation rewards are defined to promote subtask exploration and behavior diversity. SMAUG can be integrated with any Q-learning-based approach. Experiments on StarCraft II show that SMAUG not only demonstrates performance superiority in comparison with all baselines but also presents a more prominent and swift rise in rewards during the initial training stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01816v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenjing Zhang, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Cognition is All You Need - The Next Layer of AI Above Large Language Models</title>
      <link>https://arxiv.org/abs/2403.02164</link>
      <description>arXiv:2403.02164v1 Announce Type: cross 
Abstract: Recent studies of the applications of conversational AI tools, such as chatbots powered by large language models, to complex real-world knowledge work have shown limitations related to reasoning and multi-step problem solving. Specifically, while existing chatbots simulate shallow reasoning and understanding they are prone to errors as problem complexity increases. The failure of these systems to address complex knowledge work is due to the fact that they do not perform any actual cognition. In this position paper, we present Cognitive AI, a higher-level framework for implementing programmatically defined neuro-symbolic cognition above and outside of large language models. Specifically, we propose a dual-layer functional architecture for Cognitive AI that serves as a roadmap for AI systems that can perform complex multi-step knowledge work. We propose that Cognitive AI is a necessary precursor for the evolution of higher forms of AI, such as AGI, and specifically claim that AGI cannot be achieved by probabilistic approaches on their own. We conclude with a discussion of the implications for large language models, adoption cycles in AI, and commercial Cognitive AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02164v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nova Spivack, Sam Douglas, Michelle Crames, Tim Connors</dc:creator>
    </item>
    <item>
      <title>Policy Space Response Oracles: A Survey</title>
      <link>https://arxiv.org/abs/2403.02227</link>
      <description>arXiv:2403.02227v1 Announce Type: cross 
Abstract: In game theory, a game refers to a model of interaction among rational decision-makers or players, making choices with the goal of achieving their individual objectives. Understanding their behavior in games is often referred to as game reasoning. This survey provides a comprehensive overview of a fast-developing game-reasoning framework for large games, known as Policy Space Response Oracles (PSRO). We first motivate PSRO, provide historical context, and position PSRO within game-reasoning approaches. We then focus on the strategy exploration issue for PSRO, the challenge of assembling an effective strategy portfolio for modeling the underlying game with minimum computational cost. We also survey current research directions for enhancing the efficiency of PSRO, and explore the applications of PSRO across various domains. We conclude by discussing open questions and future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02227v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariyan Bighashdel, Yongzhao Wang, Stephen McAleer, Rahul Savani, Frans A. Oliehoek</dc:creator>
    </item>
    <item>
      <title>Attention-Guided Contrastive Role Representations for Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2312.04819</link>
      <description>arXiv:2312.04819v2 Announce Type: replace 
Abstract: Real-world multi-agent tasks usually involve dynamic team composition with the emergence of roles, which should also be a key to efficient cooperation in multi-agent reinforcement learning (MARL). Drawing inspiration from the correlation between roles and agent's behavior patterns, we propose a novel framework of **A**ttention-guided **CO**ntrastive **R**ole representation learning for **M**ARL (**ACORM**) to promote behavior heterogeneity, knowledge transfer, and skillful coordination across agents. First, we introduce mutual information maximization to formalize role representation learning, derive a contrastive learning objective, and concisely approximate the distribution of negative pairs. Second, we leverage an attention mechanism to prompt the global state to attend to learned role representations in value decomposition, implicitly guiding agent coordination in a skillful role space to yield more expressive credit assignment. Experiments on challenging StarCraft II micromanagement and Google research football tasks demonstrate the state-of-the-art performance of our method and its advantages over existing approaches. Our code is available at [https://github.com/NJU-RL/ACORM](https://github.com/NJU-RL/ACORM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04819v2</guid>
      <category>cs.MA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zican Hu, Zongzhang Zhang, Huaxiong Li, Chunlin Chen, Hongyu Ding, Zhi Wang</dc:creator>
    </item>
    <item>
      <title>Byzantine-Resilient Distributed Optimization of Multi-Dimensional Functions</title>
      <link>https://arxiv.org/abs/2003.09038</link>
      <description>arXiv:2003.09038v5 Announce Type: replace-cross 
Abstract: The problem of distributed optimization requires a group of agents to reach agreement on a parameter that minimizes the average of their local cost functions using information received from their neighbors. While there are a variety of distributed optimization algorithms that can solve this problem, they are typically vulnerable to malicious (or ``Byzantine'') agents that do not follow the algorithm. Recent attempts to address this issue focus on single dimensional functions, or provide analysis under certain assumptions on the statistical properties of the functions at the agents. In this paper, we propose a resilient distributed optimization algorithm for multi-dimensional convex functions. Our scheme involves two filtering steps at each iteration of the algorithm: (1) distance-based and (2) component-wise removal of extreme states. We show that this algorithm can mitigate the impact of up to $F$ Byzantine agents in the neighborhood of each regular node, without knowing the identities of the Byzantine agents in advance. In particular, we show that if the network topology satisfies certain conditions, all of the regular states are guaranteed to asymptotically converge to a bounded region that contains the global minimizer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.09038v5</guid>
      <category>math.OC</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.23919/ACC45564.2020.9147396</arxiv:DOI>
      <dc:creator>Kananart Kuwaranancharoen, Lei Xin, Shreyas Sundaram</dc:creator>
    </item>
    <item>
      <title>Identity Concealment Games: How I Learned to Stop Revealing and Love the Coincidences</title>
      <link>https://arxiv.org/abs/2105.05377</link>
      <description>arXiv:2105.05377v2 Announce Type: replace-cross 
Abstract: In an adversarial environment, a hostile player performing a task may behave like a non-hostile one in order not to reveal its identity to an opponent. To model such a scenario, we define identity concealment games: zero-sum stochastic reachability games with a zero-sum objective of identity concealment. To measure the identity concealment of the player, we introduce the notion of an average player. The average player's policy represents the expected behavior of a non-hostile player. We show that there exists an equilibrium policy pair for every identity concealment game and give the optimality equations to synthesize an equilibrium policy pair. If the player's opponent follows a non-equilibrium policy, the player can hide its identity better. For this reason, we study how the hostile player may learn the opponent's policy. Since learning via exploration policies would quickly reveal the hostile player's identity to the opponent, we consider the problem of learning a near-optimal policy for the hostile player using the game runs collected under the average player's policy. Consequently, we propose an algorithm that provably learns a near-optimal policy and give an upper bound on the number of sample runs to be collected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.05377v2</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Karabag, Mustafa O., Melkior Ornik, and Ufuk Topcu. "Identity concealment games: How I learned to stop revealing and love the coincidences." Automatica 161 (2024): 111482</arxiv:journal_reference>
      <dc:creator>Mustafa O. Karabag, Melkior Ornik, Ufuk Topcu</dc:creator>
    </item>
    <item>
      <title>Distributed fixed-point algorithms for dynamic convex optimization over decentralized and unbalanced wireless networks</title>
      <link>https://arxiv.org/abs/2401.18030</link>
      <description>arXiv:2401.18030v3 Announce Type: replace-cross 
Abstract: We consider problems where agents in a network seek a common quantity, measured independently and periodically by each agent through a local time-varying process. Numerous solvers addressing such problems have been developed in the past, featuring various adaptations of the local processing and the consensus step. However, existing solvers still lack support for advanced techniques, such as superiorization and over-the-air function computation (OTA-C). To address this limitation, we introduce a comprehensive framework for the analysis of distributed algorithms by characterizing them using the quasi-Fej\'er type algorithms and an extensive communication model. Under weak assumptions, we prove almost sure convergence of the algorithm to a common estimate for all agents. Moreover, we develop a specific class of algorithms within this framework to tackle distributed optimization problems with time-varying objectives, and, assuming that a time-invariant solution exists, prove its convergence to a solution. We also present a novel OTA-C protocol for consensus step in large decentralized networks, reducing communication overhead and enhancing network autonomy as compared to the existing protocols. The effectiveness of the algorithm, featuring superiorization and OTA-C, is demonstrated in a real-world application of distributed supervised learning over time-varying wireless networks, highlighting its low-latency and energy-efficiency compared to standard approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.18030v3</guid>
      <category>math.OC</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Navneet Agrawal, Renato L. G. Cavalcante, S{\l}awomir Sta\'nczak</dc:creator>
    </item>
  </channel>
</rss>
