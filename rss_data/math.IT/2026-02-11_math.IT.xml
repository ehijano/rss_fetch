<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.IT</link>
    <description>math.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Feb 2026 02:49:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Entropy-Based Evidence for Bitcoin's Discrete Time Mechanism</title>
      <link>https://arxiv.org/abs/2602.09027</link>
      <description>arXiv:2602.09027v1 Announce Type: new 
Abstract: Bitcoin derives a verifiable temporal order from probabilistic block discovery and cumulative proof-of-work rather than from a trusted global clock. We show that block arrivals exhibit stable exponential behavior across difficulty epochs, and that the proof-of-work process maintains a high-entropy search state that collapses discretely upon the discovery of a valid block. This entropy-based interpretation provides a mechanistic account of Bitcoin's non-continuous temporal structure. In a distributed network, however, entropy collapse is not completed instantaneously across all participants. Using empirical observations of temporary forks, we show that collapse completion unfolds over a finite propagation-bounded interval, while remaining rapid in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09027v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Chen, Pan Feng</dc:creator>
    </item>
    <item>
      <title>Non-existence of Information-Geometric Fermat Structures: Violation of Dual Lattice Consistency in Statistical Manifolds with $L^n$ Structure</title>
      <link>https://arxiv.org/abs/2602.09028</link>
      <description>arXiv:2602.09028v1 Announce Type: new 
Abstract: This paper reformulates Fermat's Last Theorem as an embedding problem of information-geometric structures. We reinterpret the Fermat equation as an $n$-th moment constraint, constructing a statistical manifold $\mathcal{M}_n$ of generalized normal distributions via the Maximum Entropy Principle. By Chentsov's Theorem, the natural metric is the Fisher information metric ($L^2$); however, the global structure is governed by the $L^n$ moment constraint. This reveals a discrepancy between the local quadratic metric and the global $L^n$ structure. We axiomatically define an "Information-Geometric Fermat Solution," postulating that the lattice structure must maintain "dual lattice consistency" under the Legendre transform. We prove the non-existence of such structures for $n \ge 3$. Through the Poisson Summation Formula and Hausdorff-Young Inequality, we demonstrate that the Fourier transform induces an alteration of the function family ($L^n \to L^q$, where $1/n + 1/q = 1$), rendering dual lattice consistency analytically impossible. This identifies a geometric obstruction where integer and energy structures are incompatible within a dually flat space. We conclude by discussing the correspondence between this model and elliptic curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09028v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kanta Tochigi (Noguchi)</dc:creator>
    </item>
    <item>
      <title>Universal Asymptotics for Jensen--Shannon Divergence under Shuffling</title>
      <link>https://arxiv.org/abs/2602.09029</link>
      <description>arXiv:2602.09029v1 Announce Type: new 
Abstract: We study the Jensen--Shannon divergence (JSD) between transcript distributions induced by neighboring datasets in the shuffle model when each user applies a fixed local randomizer and a trusted shuffler releases the output histogram. Under a mild positivity assumption, we prove an explicit two-term asymptotic expansion where the leading term is chi-squared divergence divided by 8n. Binary randomized response and k-ary randomized response follow as corollaries. For multi-message protocols based on independent repetition, the leading coefficient becomes (1 + chi-squared)^m - 1. A fully explicit remainder control is provided in the appendix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09029v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Shvets</dc:creator>
    </item>
    <item>
      <title>Dispersion of Gaussian Sources with Memory and an Extension to Abstract Sources</title>
      <link>https://arxiv.org/abs/2602.09176</link>
      <description>arXiv:2602.09176v1 Announce Type: new 
Abstract: We consider finite blocklength lossy compression of information sources whose components are independent but non-identically distributed. Crucially, Gaussian sources with memory and quadratic distortion can be cast in this form. We show that under the operational constraint of exceeding distortion $d$ with probability at most $\epsilon$, the minimum achievable rate at blocklength $n$ satisfies $R(n, d, \epsilon)=\mathbb{R}_n(d)+\sqrt{\frac{\mathbb{V}_n(d)}{n}}Q^{-1}(\epsilon)+O \left(\frac{\log n}{n}\right)$, where $Q^{-1}(\cdot)$ is the inverse $Q$-function, while $\mathbb{R}_n(d)$ and $\mathbb{V}_n(d)$ are fundamental characteristics of the source computed using its $n$-letter joint distribution and the distortion measure, called the $n$th-order informational rate-distortion function and the source dispersion, respectively. Our result generalizes the existing dispersion result for abstract sources with i.i.d. components. It also sharpens and extends the only known dispersion result for a source with memory, namely, the scalar Gauss-Markov source. The key novel technical tool in our analysis is the point-mass product proxy measure, which enables the construction of typical sets. This proxy generalizes the empirical distribution beyond the i.i.d. setting by preserving additivity across coordinates and facilitating a typicality analysis for sums of independent, non-identical terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09176v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eyyup Tasci, Victoria Kostina</dc:creator>
    </item>
    <item>
      <title>On the Subpacketization Level of the Banawan-Ulukus Multi-Message PIR Scheme</title>
      <link>https://arxiv.org/abs/2602.09417</link>
      <description>arXiv:2602.09417v2 Announce Type: new 
Abstract: This note analyzes a linear recursion that arises in the computation of the subpacketization level for the multi-message PIR scheme of Banawan and Ulukus. We derive an explicit representation for the normalized subpacketization level $L$, whose smallest integer multiple yields the subpacketization level of the scheme, in terms of the number of servers $N$, the total number of messages $K$, and the number of demand messages $D$. The resulting formula shows that $L$ is a polynomial in $N$ with nonnegative coefficients, and its leading term is $N^{K-D+1}/D$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09417v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anoosheh Heidarzadeh</dc:creator>
    </item>
    <item>
      <title>Directed Information: Estimation, Optimization and Applications in Communications and Causality</title>
      <link>https://arxiv.org/abs/2602.09711</link>
      <description>arXiv:2602.09711v1 Announce Type: new 
Abstract: Directed information (DI) is an information measure that attempts to capture directionality in the flow of information from one random process to another. It is closely related to other causal influence measures, such as transfer entropy, Granger causality, and Pearl's causal framework. This monograph provides an overview of DI and its main application in information theory, namely, characterizing the capacity of channels with feedback and memory. We begin by reviewing the definitions of DI, its basic properties, and its relation to Shannon's mutual information. Next, we provide a survey of DI estimation techniques, ranging from classic plug-in estimators to modern neural-network-based estimators. Considering the application of channel capacity estimation, we describe how such estimators numerically optimize DI rate over a class of joint distributions on input and output processes. A significant part of the monograph is devoted to techniques to compute the feedback capacity of finite-state channels (FSCs). The feedback capacity of a strongly connected FSC involves the maximization of the DI rate from the channel input process to the output process. This maximization is performed over the class of causal conditioned probability input distributions. When the FSC is also unifilar, i.e., the next state is given by a time-invariant function of the current state and the new input-output symbol pair, the feedback capacity is the optimal average reward of an appropriately formulated Markov decision process (MDP). This MDP formulation has been exploited to develop several methods to compute exactly, or at least estimate closely, the feedback capacity of a unifilar FSC. This monograph describes these methods, starting from the value iteration algorithm, to Q-graph methods, and reinforcement learning algorithms that can handle large input and output alphabets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09711v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dor Tsur, Oron Sabag, Navin Kashyap, Haim Permuter, Gerhard Kramer</dc:creator>
    </item>
    <item>
      <title>METTLE: Efficient Streaming Erasure Code with Peeling Decodability</title>
      <link>https://arxiv.org/abs/2602.10020</link>
      <description>arXiv:2602.10020v1 Announce Type: new 
Abstract: In this work, we solve a long-standing open problem in coding theory with broad applications in networking and systems: designing an erasure code that simultaneously satisfies three requirements: (1) high coding efficiency, (2) low coding complexity, and (3) being a streaming code (defined as one with low decoding latency). We propose METTLE (Multi-Edge Type with Touch-less Leading Edge), the first erasure code to meet all three requirements. Compared to "streaming RaptorQ" (RaptorQ configured with a small source block size to ensure a low decoding latency), METTLE is only slightly worse in coding efficiency, but 47.7 to 84.6 times faster to decode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10020v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianru Yu (Jim), Tianji Yang (Jim), Jingfan Meng (Jim), Jun Xu (Jim)</dc:creator>
    </item>
    <item>
      <title>On the generalization of $g$-circulant MDS matrices</title>
      <link>https://arxiv.org/abs/2602.10028</link>
      <description>arXiv:2602.10028v1 Announce Type: new 
Abstract: A matrix $M$ over the finite field $ \mathbb{F}_q $ is called \emph{maximum distance separable} (MDS) if all of its square submatrices are non-singular. These MDS matrices are very important in cryptography and coding theory because they provide strong data protection and help spread information efficiently. In this paper, we introduce a new type of matrix called a \emph{consta-$g$-circulant matrix}, which extends the idea of $g$-circulant matrices. These matrices come from a linear transformation defined by the polynomial
  $
  h(x) = x^m - \lambda + \sum_{i=0}^{m-1} h_i x^i
  $
  over $ \mathbb{F}_q $. We find the upper bound of such matrices exist and give conditions to check when they are invertible. This helps us know when they are MDS matrices. If the polynomial $ x^m - \lambda $ factors as
  $
  x^m - \lambda = \prod_{i=1}^{t} f_i(x)^{e_i},
  $
  where each \( f_i(x) \) is irreducible, then the number of invertible consta-$g$-circulant matrices is
  $
  N \cdot \prod_{i=1}^{t} \left( q^{\deg f_i} - 1 \right),
  $
  where $r$ is the multiplicative order of $\lambda$, and \( N \) is the number of integers \( k \) such that
  $
  0 \leq k &lt; \left\lfloor \frac{m - 1}{r} \right\rfloor + 1 \quad \text{and} \quad \gcd(1 + rk, m) = 1.
  $
  This formula help us to reduce the number of cases to check whether such matrices is MDS. Moreover, we give complete characterization of $g$-circulant MDS matrices of order 3 and 4. Additionally, inspired by skew polynomial rings, we construct a new variant of $g$-circulant matrix. In the last, we provide some examples related to our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10028v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atif Ahmad Khan, Shakir Ali, Bhupendra Singh</dc:creator>
    </item>
    <item>
      <title>Operator-Based Information Theory for Imaging: Entropy, Capacity, and Irreversibility in Physical Measurement Systems</title>
      <link>https://arxiv.org/abs/2602.09026</link>
      <description>arXiv:2602.09026v1 Announce Type: cross 
Abstract: Imaging systems are commonly described using resolution, contrast, and signal-to-noise ratio, but these quantities do not provide a general account of how physical transformations affect the flow of information. This paper introduces an operator-based formulation of information theory for imaging. The approach models the imaging chain as a composition of bounded operators acting on functions, and characterises information redistribution using the spectral properties of these operators. Three measures are developed. Operator entropy quantifies how an operator distributes energy across its singular spectrum. Operator information capacity describes the number of modes that remain recoverable above a noise-dependent threshold. An irreversibility index measures the information lost through suppression or elimination of modes and captures the accumulation of information loss under operator composition. The framework applies to linear, nonlinear, and stochastic operators and does not depend on the specific imaging modality. Analytical examples show how attenuation, blur, and sampling affect entropy, capacity, and irreversibility in different ways. The results provide a general structure for analysing the physical limits of imaging and form the basis for subsequent work on information geometry, spatiotemporal budgets, nonlinear channels, and reconstruction algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09026v1</guid>
      <category>eess.IV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Wood</dc:creator>
    </item>
    <item>
      <title>Persistent Entropy as a Detector of Phase Transitions</title>
      <link>https://arxiv.org/abs/2602.09058</link>
      <description>arXiv:2602.09058v1 Announce Type: cross 
Abstract: Persistent entropy (PE) is an information-theoretic summary statistic of persistence barcodes that has been widely used to detect regime changes in complex systems. Despite its empirical success, a general theoretical understanding of when and why persistent entropy reliably detects phase transitions has remained limited, particularly in stochastic and data-driven settings. In this work, we establish a general, model-independent theorem providing sufficient conditions under which persistent entropy provably separates two phases. We show that persistent entropy exhibits an asymptotically non-vanishing gap across phases. The result relies only on continuity of persistent entropy along the convergent diagram sequence, or under mild regularization, and is therefore broadly applicable across data modalities, filtrations, and homological degrees. To connect asymptotic theory with finite-time computations, we introduce an operational framework based on topological stabilization, defining a topological transition time by stabilizing a chosen topological statistic over sliding windows, and a probability-based estimator of critical parameters within a finite observation horizon. We validate the framework on the Kuramoto synchronization transition, the Vicsek order-to-disorder transition in collective motion, and neural network training dynamics across multiple datasets and architectures. Across all experiments, stabilization of persistent entropy and collapse of variability across realizations provide robust numerical signatures consistent with the theoretical mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09058v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Rucco</dc:creator>
    </item>
    <item>
      <title>Optimal information deletion and Bayes' theorem</title>
      <link>https://arxiv.org/abs/2602.09061</link>
      <description>arXiv:2602.09061v1 Announce Type: cross 
Abstract: In this same journal, Arnold Zellner published a seminal paper on Bayes' theorem as an optimal information processing rule. This result led to the variational formulation of Bayes' theorem, which is the central idea in generalized variational inference. Almost 40 years later, we revisit these ideas, but from the perspective of information deletion. We investigate rules which update a posterior distribution into an antedata distribution when a portion of data is removed. In such context, a rule which does not destroy or create information is called the optimal information deletion rule and we prove that it coincides with the traditional use of Bayes' theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09061v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans Montcho, H{\aa}vard Rue</dc:creator>
    </item>
    <item>
      <title>Epistemic Throughput: Fundamental Limits of Attention-Constrained Inference</title>
      <link>https://arxiv.org/abs/2602.09127</link>
      <description>arXiv:2602.09127v1 Announce Type: cross 
Abstract: Recent generative and tool-using AI systems can surface a large volume of candidates at low marginal cost, yet only a small fraction can be checked carefully. This creates a decoder-side bottleneck: downstream decision-makers must form reliable posteriors from many public records under scarce attention. We formalize this regime via Attention-Constrained Inference (ACI), in which a cheap screening stage processes $K$ records and an expensive verification stage can follow up on at most $B$ of them. Under Bayes log-loss, we study the maximum achievable reduction in posterior uncertainty per window, which we call \emph{epistemic throughput}. Our main result is a ``JaKoB'' scaling law showing that epistemic throughput has a baseline term that grows linearly with verification and prevalence, and an additional \emph{information-leverage} term that scales as $\sqrt{JKB}$, where $J$ summarizes screening quality. Thus, expanding cheap screening can nonlinearly amplify scarce verification, even when informative records are rare. We further show that this scaling is tight in a weak-screening limit, and that in the sparse-verification regime ($B \ll K$), substantial leverage requires heavy-tailed score distributions; for light-tailed scores the amplification is only logarithmic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09127v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei You</dc:creator>
    </item>
    <item>
      <title>Optimal Estimation in Orthogonally Invariant Generalized Linear Models: Spectral Initialization and Approximate Message Passing</title>
      <link>https://arxiv.org/abs/2602.09240</link>
      <description>arXiv:2602.09240v1 Announce Type: cross 
Abstract: We consider the problem of parameter estimation from a generalized linear model with a random design matrix that is orthogonally invariant in law. Such a model allows the design have an arbitrary distribution of singular values and only assumes that its singular vectors are generic. It is a vast generalization of the i.i.d. Gaussian design typically considered in the theoretical literature, and is motivated by the fact that real data often have a complex correlation structure so that methods relying on i.i.d. assumptions can be highly suboptimal. Building on the paradigm of spectrally-initialized iterative optimization, this paper proposes optimal spectral estimators and combines them with an approximate message passing (AMP) algorithm, establishing rigorous performance guarantees for these two algorithmic steps. Both the spectral initialization and the subsequent AMP meet existing conjectures on the fundamental limits to estimation -- the former on the optimal sample complexity for efficient weak recovery, and the latter on the optimal errors. Numerical experiments suggest the effectiveness of our methods and accuracy of our theory beyond orthogonally invariant data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09240v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihan Zhang, Hong Chang Ji, Ramji Venkataramanan, Marco Mondelli</dc:creator>
    </item>
    <item>
      <title>The Critical Horizon: Inspection Design Principles for Multi-Stage Operations and Deep Reasoning</title>
      <link>https://arxiv.org/abs/2602.09394</link>
      <description>arXiv:2602.09394v1 Announce Type: cross 
Abstract: Manufacturing lines, service journeys, supply chains, and AI reasoning chains share a common challenge: attributing a terminal outcome to the intermediate stage that caused it. We establish an information-theoretic barrier to this credit assignment problem: the signal connecting early steps to final outcomes decays exponentially with depth, creating a critical horizon beyond which no algorithm can learn from endpoint data alone. We prove four results. First, a Signal Decay Bound: sample complexity for attributing outcomes to early stages grows exponentially in the number of intervening steps. Second, Width Limits: parallel rollouts provide only logarithmic relief, with correlation capping the effective number of independent samples. Third, an Objective Mismatch: additive reward aggregation optimizes the wrong quantity when sequential validity requires all steps to be correct. Fourth, Optimal Inspection Design: uniform checkpoint spacing is minimax-optimal under homogeneous signal attenuation, while a greedy algorithm yields optimal non-uniform schedules under heterogeneous attenuation. Together, these results provide a common analytical foundation for inspection design in operations and supervision design in AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09394v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seyed Morteza Emadi</dc:creator>
    </item>
    <item>
      <title>Is Memorization Helpful or Harmful? Prior Information Sets the Threshold</title>
      <link>https://arxiv.org/abs/2602.09405</link>
      <description>arXiv:2602.09405v1 Announce Type: cross 
Abstract: We examine the connection between training error and generalization error for arbitrary estimating procedures, working in an overparameterized linear model under general priors in a Bayesian setup. We find determining factors inherent to the prior distribution $\pi$, giving explicit conditions under which optimal generalization necessitates that the training error be (i) near interpolating relative to the noise size (i.e., memorization is necessary), or (ii) close to the noise level (i.e., overfitting is harmful). Remarkably, these phenomena occur when the noise reaches thresholds determined by the Fisher information and the variance parameters of the prior $\pi$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09405v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Cheng, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Geometric Analysis of Blind User Identification for Massive MIMO Networks</title>
      <link>https://arxiv.org/abs/2602.09910</link>
      <description>arXiv:2602.09910v1 Announce Type: cross 
Abstract: Applying Nearest Convex Hull Classification (NCHC) to blind user identification in a massive Multiple Input Multiple Output (MIMO) communications system is proposed. The method is blind in the way that the Base Station (BS) only requires a training sequence containing unknown data symbols obtained from the user without further knowledge on the channel, modulation, coding or even noise power. We evaluate the algorithm under the assumption of gaussian transmit signals using the non-rigorous replica method. To facilitate the computations the existence of an Operator Valued Free Fourier Transform is postulated, which is verified by Monte Carlo simulation. The replica computations are conducted in the large but finite system by applying saddle-point integration with inverse temperature $\beta$ as the large parameter. The classifier accuracy is estimated by gaussian approximation through moment-matching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09910v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Levi Bohnacker, Ralf R. M\"uller</dc:creator>
    </item>
    <item>
      <title>Decoding Golay Codes and their Related Lattices: A PAC Code Perspective</title>
      <link>https://arxiv.org/abs/2602.01657</link>
      <description>arXiv:2602.01657v2 Announce Type: replace 
Abstract: In this work, we propose a decoding method of Golay codes from the perspective of Polarization Adjusted Convolutional (PAC) codes. By invoking Forney's cubing construction of Golay codes and their generators $G^*(8,7)/(8,4)$, we found different construction methods of Golay codes from PAC codes, which result in an efficient parallel list decoding algorithm with near-maximum likelihood performance. Compared with existing methods, our method can get rid of index permutation and codeword puncturing. Using the new decoding method, some related lattices, such as Leech lattice $\Lambda_{24}$ and its principal sublattice $H_{24}$, can be also decoded efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01657v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujun Ji, Ling Liu, Shanxiang Lyu, Chao Chen, Tao Dai, Baoming Bai</dc:creator>
    </item>
    <item>
      <title>Deep learning based Channel Estimation and Beamforming in Movable Antenna Systems</title>
      <link>https://arxiv.org/abs/2602.07870</link>
      <description>arXiv:2602.07870v2 Announce Type: replace 
Abstract: Movable antenna (MA) has emerged as a promising technology for future wireless systems. Compared with traditional fixed-position antennas, MA improves system performance by antenna movement to optimize channel conditions. For multiuser wideband MA systems, this paper proposes deep learning-based framework integrating channel estimation (CE), antenna position optimization, and beamforming, with a clear workflow and enhanced efficiency. Specifically, to obtain accurate channel state information (CSI), we design a two-stage CE mechanism: first reconstructing the channel matrix from limited measurements via compressive sensing, then introducing a Swin-Transformer-based denoising network to refine CE accuracy for subsequent optimization. Building on this, we address the joint optimization challenge by proposing a Transformer-based network that intelligently maps CSI sequences of candidate positions to optimal MA positions while combining a model-driven weighted minimum mean square error (WMMSE) beamforming approach to achieve better performance. Simulation results demonstrate that the proposed methods achieve superior performance compared with existing counterparts under various conditions. The codes about this work are available at https://github.com/ZiweiWan/Code-4-DL-MA-CE-BF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07870v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kaijun Feng, Ziwei Wan, Anwen Liao, Wenyan Ma, Lipeng Zhu, Zhenyu Xiao, Zhen Gao, Rui Zhang</dc:creator>
    </item>
    <item>
      <title>Tighter Information-Theoretic Generalization Bounds via a Novel Class of Change of Measure Inequalities</title>
      <link>https://arxiv.org/abs/2602.07999</link>
      <description>arXiv:2602.07999v2 Announce Type: replace 
Abstract: In this paper, we propose a novel class of change of measure inequalities via a unified framework based on the data processing inequality for $f$-divergences, which is surprisingly elementary yet powerful enough to yield tighter inequalities. We provide change of measure inequalities in terms of a broad family of information measures, including $f$-divergences (with Kullback-Leibler divergence and $\chi^2$-divergence as special cases), R\'enyi divergence, and $\alpha$-mutual information (with maximal leakage as a special case). We then embed these inequalities into the analysis of generalization error for stochastic learning algorithms, yielding novel and tighter high-probability information-theoretic generalization bounds, while also recovering several best-known results via simplified analyses. A key advantage of our framework is its flexibility: it readily adapts to a range of settings, including the conditional mutual information framework, PAC-Bayesian theory, and differential privacy mechanisms, for which we derive new generalization bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07999v2</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanxiao Liu, Yijun Fan, Deniz G\"und\"uz</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Limits of Quantum Learning via Data Compression</title>
      <link>https://arxiv.org/abs/2112.06841</link>
      <description>arXiv:2112.06841v2 Announce Type: replace-cross 
Abstract: Understanding the power of quantum data in machine learning is central to many proposed applications of quantum technologies. While access to quantum data can offer exponential advantages for carefully designed learning tasks and often under strong assumptions on the data distribution, it remains an open question whether such advantages persist in less structured settings and under more realistic, naturally occurring distributions. Motivated by these practical concerns, we introduce a systematic framework based on quantum lossy data compression to bound the power of quantum data in the context of probably approximately correct (PAC) learning. Specifically, we provide lower bounds on the sample complexity of quantum learners for arbitrary functions when data is drawn from Zipf's distribution, a widely used model for the empirical distributions of real-world data. We also establish lower bounds on the size of quantum input data required to learn linear functions, thereby proving the optimality of previous positive results. Beyond learning theory, we show that our framework has applications in secure delegated quantum computation within the measurement-based quantum computation (MBQC) model. In particular, we constrain the amount of private information the server can infer, strengthening the security guarantees of the delegation protocol proposed in (Mantri et al., PRX, 2017).</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.06841v2</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Armando Angrisani, Brian Coyle, Elham Kashefi</dc:creator>
    </item>
    <item>
      <title>Efficient Online Random Sampling via Randomness Recycling</title>
      <link>https://arxiv.org/abs/2505.18879</link>
      <description>arXiv:2505.18879v4 Announce Type: replace-cross 
Abstract: This article studies the fundamental problem of using i.i.d. coin tosses from an entropy source to efficiently generate random variables $X_i \sim P_i$ $(i \ge 1)$, where $(P_1, P_2, \dots)$ is a random sequence of rational discrete probability distributions subject to an \textit{arbitrary} stochastic process. Our method achieves an amortized expected entropy cost within $\varepsilon &gt; 0$ bits of the information-theoretically optimal Shannon lower bound using $O(\log(1/\varepsilon))$ space. This result holds both pointwise in terms of the Shannon information content conditioned on $X_i$ and $P_i$, and in expectation to obtain a rate of $\mathbb{E}[H(P_1) + \dots + H(P_n)]/n + \varepsilon$ bits per sample as $n \to \infty$ (where $H$ is the Shannon entropy). The combination of space, time, and entropy properties of our method improves upon the Knuth and Yao (1976) entropy-optimal algorithm and Han and Hoshi (1997) interval algorithm for online sampling, which require unbounded space. It also uses exponentially less space than the more specialized methods of Kozen and Soloviev (2022) and Shao and Wang (2025) that generate i.i.d. samples from a fixed distribution. Our online sampling algorithm rests on a powerful algorithmic technique called \textit{randomness recycling}, which reuses a fraction of the random information consumed by a probabilistic algorithm to reduce its amortized entropy cost.
  On the practical side, we develop randomness recycling techniques to accelerate a variety of prominent sampling algorithms. We show that randomness recycling enables state-of-the-art runtime performance on the Fisher-Yates shuffle when using a cryptographically secure pseudorandom number generator, and that it reduces the entropy cost of discrete Gaussian sampling. Accompanying the manuscript is a performant software library in the C programming language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18879v4</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611978971.89</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2026 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 2473-2511. Society for Industrial and Applied Mathematics, 2026</arxiv:journal_reference>
      <dc:creator>Thomas L. Draper, Feras A. Saad</dc:creator>
    </item>
    <item>
      <title>Fully Parallelized BP Decoding for Quantum LDPC Codes Can Outperform BP-OSD</title>
      <link>https://arxiv.org/abs/2507.00254</link>
      <description>arXiv:2507.00254v3 Announce Type: replace-cross 
Abstract: This work presents a hardware-efficient and fully parallelizable decoder for quantum LDPC codes that leverages belief propagation (BP) with a speculative post-processing strategy inspired by classical Chase decoding algorithm. By monitoring bit-level oscillation patterns during BP, our method identifies unreliable bits and generates multiple candidate vectors to selectively flip syndromes. Each modified syndrome is then decoded independently using short-depth BP, a process we refer to as BP-SF (syndrome flip). This design eliminates the need for costly Gaussian elimination used in the current BP-OSD approaches. Our implementation achieves logical error rates comparable to or better than BP-OSD while offering significantly lower latency due to its high degree of parallelism for a variety of bivariate bicycle codes. Evaluation on the [[144,12,12]] bivariate bicycle code shows that the proposed decoder reduces average latency to approximately $70\%$ of BP-OSD. When post-processing is parallelized the average latency is reduced by $55\%$ compared to the single process implementation, with the maximum latency reaching as low as $18\%$. These advantages make it particularly well-suited for real-time and resource-constrained quantum error correction systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00254v3</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Wang, Ang Li, Frank Mueller</dc:creator>
    </item>
    <item>
      <title>On the undecidability of quantum channel capacities</title>
      <link>https://arxiv.org/abs/2601.22471</link>
      <description>arXiv:2601.22471v2 Announce Type: replace-cross 
Abstract: An important distinction in our understanding of capacities of classical versus quantum channels is marked by the following question: is there an algorithm which can compute (or even efficiently compute) the capacity? While there is overwhelming evidence suggesting that quantum channel capacities may be uncomputable, a formal proof of any such statement is elusive. We initiate the study of the hardness of computing quantum channel capacities. We show that, for a general quantum channel, it is QMA-hard to compute its quantum capacity, and that the maximal-entanglement-assisted zero-error one-shot classical capacity is uncomputable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22471v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Archishna Bhattacharyya, Arthur Mehta, Yuming Zhao</dc:creator>
    </item>
    <item>
      <title>CSRv2: Unlocking Ultra-Sparse Embeddings</title>
      <link>https://arxiv.org/abs/2602.05735</link>
      <description>arXiv:2602.05735v3 Announce Type: replace-cross 
Abstract: In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05735v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lixuan Guo, Yifei Wang, Tiansheng Wen, Yifan Wang, Aosong Feng, Bo Chen, Stefanie Jegelka, Chenyu You</dc:creator>
    </item>
  </channel>
</rss>
