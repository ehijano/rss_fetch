<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Nov 2024 03:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Skin Sympathetic Nerve Activity Driver Extraction through Non-Negative Sparse Decomposition</title>
      <link>https://arxiv.org/abs/2411.15163</link>
      <description>arXiv:2411.15163v1 Announce Type: new 
Abstract: In recent years, skin sympathetic nerve activity (SKNA) extracted from electrocardiogram has gained attention as a novel noninvasive measure of the sympathetic nervous system (SNS), while electrodermal activity (EDA) has long served this purpose. SparsEDA is a sparse deconvolution technique originally developed for EDA to extract phasic drivers indicating the start of sympathetic burst responses. Our focus is on applying this method to preprocessed SKNA signals, justified by both SKNA and EDA signals' connection to sympathetic nerve activity and prior observed similarities. In a thermal-grill pain experiment, 16 subjects underwent six stimulations each to elicit SNS responses, with simultaneous recording of EDA and SKNA. We confirmed the method's accuracy in identifying stimuli initiation. Results were assessed for burst detection and accuracy of driver placement compared to annotated labels. The SKNA drivers achieved an RMSE of 0.42 from annotated stimulations, a 97% hit rate in detecting applied stimuli, and minimal false alarms during the 2-minute control period and interstimulus intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15163v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Farnoush Baghestani, Youngsun Kong, Ki H. Chon</dc:creator>
    </item>
    <item>
      <title>Adaptive Intelligence: leveraging insights from adaptive behavior in animals to build flexible AI systems</title>
      <link>https://arxiv.org/abs/2411.15234</link>
      <description>arXiv:2411.15234v1 Announce Type: new 
Abstract: Biological intelligence is inherently adaptive -- animals continually adjust their actions based on environmental feedback. However, creating adaptive artificial intelligence (AI) remains a major challenge. The next frontier is to go beyond traditional AI to develop "adaptive intelligence," defined here as harnessing insights from biological intelligence to build agents that can learn online, generalize, and rapidly adapt to changes in their environment. Recent advances in neuroscience offer inspiration through studies that increasingly focus on how animals naturally learn and adapt their world models. In this Perspective, I will review the behavioral and neural foundations of adaptive biological intelligence, the parallel progress in AI, and explore brain-inspired approaches for building more adaptive algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15234v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mackenzie Weygandt Mathis</dc:creator>
    </item>
    <item>
      <title>Bio-inspired AI: Integrating Biological Complexity into Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2411.15243</link>
      <description>arXiv:2411.15243v1 Announce Type: new 
Abstract: The pursuit of creating artificial intelligence (AI) mirrors our longstanding fascination with understanding our own intelligence. From the myths of Talos to Aristotelian logic and Heron's inventions, we have sought to replicate the marvels of the mind. While recent advances in AI hold promise, singular approaches often fall short in capturing the essence of intelligence. This paper explores how fundamental principles from biological computation--particularly context-dependent, hierarchical information processing, trial-and-error heuristics, and multi-scale organization--can guide the design of truly intelligent systems. By examining the nuanced mechanisms of biological intelligence, such as top-down causality and adaptive interaction with the environment, we aim to illuminate potential limitations in artificial constructs. Our goal is to provide a framework inspired by biological systems for designing more adaptable and robust artificial intelligent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15243v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>cs.SC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nima Dehghani, Michael Levin</dc:creator>
    </item>
    <item>
      <title>The brain versus AI: World-model-based versatile circuit computation underlying diverse functions in the neocortex and cerebellum</title>
      <link>https://arxiv.org/abs/2411.16075</link>
      <description>arXiv:2411.16075v1 Announce Type: new 
Abstract: AI's significant recent advances using general-purpose circuit computations offer a potential window into how the neocortex and cerebellum of the brain are able to achieve a diverse range of functions across sensory, cognitive, and motor domains, despite their uniform circuit structures. However, comparing the brain and AI is challenging unless clear similarities exist, and past reviews have been limited to comparison of brain-inspired vision AI and the visual neocortex. Here, to enable comparisons across diverse functional domains, we subdivide circuit computation into three elements -- circuit structure, input/outputs, and the learning algorithm -- and evaluate the similarities for each element. With this novel approach, we identify wide-ranging similarities and convergent evolution in the brain and AI, providing new insights into key concepts in neuroscience. Furthermore, inspired by processing mechanisms of AI, we propose a new theory that integrates established neuroscience theories, particularly the theories of internal models and the mirror neuron system. Both the neocortex and cerebellum predict future world events from past information and learn from prediction errors, thereby acquiring models of the world. These models enable three core processes: (1) Prediction -- generating future information, (2) Understanding -- interpreting the external world via compressed and abstracted sensory information, and (3) Generation -- repurposing the future-information generation mechanism to produce other types of outputs. The universal application of these processes underlies the ability of the neocortex and cerebellum to accomplish diverse functions with uniform circuits. Our systematic approach, insights, and theory promise groundbreaking advances in understanding the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16075v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shogo Ohmae, Keiko Ohmae</dc:creator>
    </item>
    <item>
      <title>Global and local synchrony of coupled neurons in small-world networks</title>
      <link>https://arxiv.org/abs/2411.16374</link>
      <description>arXiv:2411.16374v1 Announce Type: new 
Abstract: Synchronous firing of neurons is thought to play important functional roles such as feature binding and switching of cognitive states. Although synchronization has mainly been investigated using model neurons with simple connection topology so far, real neural networks have more complex structures. Here we examine behavior of pulse-coupled leaky integrate-and-fire neurons with various network structures. We first show that the dispersion of the number of connections for neurons influences dynamical behavior even if other major topological statistics are kept fixed. The rewiring probability parameter representing the randomness of networks bridges two spatially opposite frameworks: precise local synchrony and rough global synchrony. Finally, cooperation of the global connections and the local clustering property, which is prominent in small-world networks, reinforces synchrony of distant neuronal groups receiving coherent inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16374v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00422-004-0471-9</arxiv:DOI>
      <arxiv:journal_reference>Biological Cybernetics, 90, 302-309 (2004)</arxiv:journal_reference>
      <dc:creator>Naoki Masuda, Kazuyuki Aihara</dc:creator>
    </item>
    <item>
      <title>Stability of Brain Functional Network During Working Memory Using Structural Balance Theory</title>
      <link>https://arxiv.org/abs/2411.16558</link>
      <description>arXiv:2411.16558v1 Announce Type: new 
Abstract: Working memory plays a crucial role in various aspects of human life. Therefore, it has been an area of interest in different research studies, especially neuroscience. The neuroscientists investigating working memory have primarily emphasized the brain's functional modularity. At the same time, a holistic perspective is still required to investigate the brain as an integrated and unified system. We hypothesized that the brain should shift towards a more stable state during working memory than the resting state. Therefore, based on the Structural Balance Theory (SBT), we aimed to address this process. To achieve this, we examined triadic associations in signed fMRI networks in healthy individuals using the N-back as the working memory task. We demonstrated that the number of balanced triads increased during the working memory task compared to the resting state, while the opposite is true for imbalanced triads. The increase of balanced triads forced the network to a more stable state with a lower balance energy level. The increase of balanced triads was crucially related to changes in anti-synchrony to synchronous activities between the Temporal Cortex, the Prefrontal Cortex, and the Parietal Cortex, which are known to be involved in various aspects of working memory, during the working memory process. We hope these findings pave the way to a better understanding the working memory process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16558v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepehr Gourabi, Masoud Lotfalipour, Reza Khosrowabadi, Reza Jafari</dc:creator>
    </item>
    <item>
      <title>Dynamic Causal Models of Time-Varying Connectivity</title>
      <link>https://arxiv.org/abs/2411.16582</link>
      <description>arXiv:2411.16582v1 Announce Type: new 
Abstract: This paper introduces a novel approach for modelling time-varying connectivity in neuroimaging data, focusing on the slow fluctuations in synaptic efficacy that mediate neuronal dynamics. Building on the framework of Dynamic Causal Modelling (DCM), we propose a method that incorporates temporal basis functions into neural models, allowing for the explicit representation of slow parameter changes. This approach balances expressivity and computational efficiency by modelling these fluctuations as a Gaussian process, offering a middle ground between existing methods that either strongly constrain or excessively relax parameter fluctuations. We validate the ensuing model through simulations and real data from an auditory roving oddball paradigm, demonstrating its potential to explain key aspects of brain dynamics. This work aims to equip researchers with a robust tool for investigating time-varying connectivity, particularly in the context of synaptic modulation and its role in both healthy and pathological brain function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16582v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Johan Medrano, Karl J. Friston, Peter Zeidman</dc:creator>
    </item>
    <item>
      <title>Probing for Consciousness in Machines</title>
      <link>https://arxiv.org/abs/2411.16262</link>
      <description>arXiv:2411.16262v1 Announce Type: cross 
Abstract: This study explores the potential for artificial agents to develop core consciousness, as proposed by Antonio Damasio's theory of consciousness. According to Damasio, the emergence of core consciousness relies on the integration of a self model, informed by representations of emotions and feelings, and a world model. We hypothesize that an artificial agent, trained via reinforcement learning (RL) in a virtual environment, can develop preliminary forms of these models as a byproduct of its primary task. The agent's main objective is to learn to play a video game and explore the environment. To evaluate the emergence of world and self models, we employ probes-feedforward classifiers that use the activations of the trained agent's neural networks to predict the spatial positions of the agent itself. Our results demonstrate that the agent can form rudimentary world and self models, suggesting a pathway toward developing machine consciousness. This research provides foundational insights into the capabilities of artificial agents in mirroring aspects of human consciousness, with implications for future advancements in artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16262v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mathis Immertreu, Achim Schilling, Andreas Maier, Patrick Krauss</dc:creator>
    </item>
    <item>
      <title>Plastic Arbor: a modern simulation framework for synaptic plasticity $\unicode{x2013}$ from single synapses to networks of morphological neurons</title>
      <link>https://arxiv.org/abs/2411.16445</link>
      <description>arXiv:2411.16445v1 Announce Type: cross 
Abstract: Arbor is a software library designed for efficient simulation of large-scale networks of biological neurons with detailed morphological structures. It combines customizable neuronal and synaptic mechanisms with high-performance computing, supporting multi-core CPU and GPU systems.
  In humans and other animals, synaptic plasticity processes play a vital role in cognitive functions, including learning and memory. Recent studies have shown that intracellular molecular processes in dendrites significantly influence single-neuron dynamics. However, for understanding how the complex interplay between dendrites and synaptic processes influences network dynamics, computational modeling is required.
  To enable the modeling of large-scale networks of morphologically detailed neurons with diverse plasticity processes, we have extended the Arbor library to the Plastic Arbor framework, supporting simulations of a large variety of spike-driven plasticity paradigms. To showcase the features of the new framework, we present examples of computational models, beginning with single-synapse dynamics, progressing to multi-synapse rules, and finally scaling up to large recurrent networks. While cross-validating our implementations by comparison with other simulators, we show that Arbor allows simulating plastic networks of multi-compartment neurons at nearly no additional cost in runtime compared to point-neuron simulations. Using the new framework, we have already been able to investigate the impact of dendritic structures on network dynamics across a timescale of several hours, showing a relation between the length of dendritic trees and the ability of the network to efficiently store information.
  By our extension of Arbor, we aim to provide a valuable tool that will support future studies on the impact of synaptic plasticity, especially, in conjunction with neuronal morphology, in large networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16445v1</guid>
      <category>cs.CE</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jannik Luboeinski, Sebastian Schmitt, Shirin Shafiee Kamalabad, Thorsten Hater, Fabian B\"osch, Christian Tetzlaff</dc:creator>
    </item>
    <item>
      <title>Comparative prospects of imaging methods for whole-brain mammalian connectomics</title>
      <link>https://arxiv.org/abs/2405.10488</link>
      <description>arXiv:2405.10488v3 Announce Type: replace 
Abstract: Mammalian whole-brain connectomes at nanoscale synaptic resolution are a crucial ingredient for holistic understanding of brain function. Imaging these connectomes at sufficient resolution to densely reconstruct cellular morphology and synapses represents a longstanding goal in neuroscience. Although the technologies needed to reconstruct whole-brain connectomes have not yet reached full maturity, they are advancing rapidly enough that the mouse brain might be within reach in the near future. Detailed exploration of these technologies is warranted to help plan projects with varying goals and requirements. Whole-brain human connectomes remain a more distant goal yet are worthy of consideration to orient large-scale neuroscience program plans. Here, we quantitatively compare existing and emerging imaging technologies that have potential to enable whole-brain mammalian connectomics. We perform calculations on electron microscopy (EM) techniques and expansion microscopy coupled with light-sheet fluorescence microscopy (ExLSFM) methods. We consider techniques from the literature that have sufficiently high resolution to identify all synapses and sufficiently high speed to be relevant for whole mammalian brains. Each imaging modality comes with benefits and drawbacks, so we suggest that attacking the problem through multiple approaches could yield the best outcomes. We offer this analysis as a resource for those considering how to organize efforts towards imaging whole-brain mammalian connectomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10488v3</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Logan Thrasher Collins, Todd Huffman, Randal Koene</dc:creator>
    </item>
    <item>
      <title>Modeling Latent Neural Dynamics with Gaussian Process Switching Linear Dynamical Systems</title>
      <link>https://arxiv.org/abs/2408.03330</link>
      <description>arXiv:2408.03330v2 Announce Type: replace 
Abstract: Understanding how the collective activity of neural populations relates to computation and ultimately behavior is a key goal in neuroscience. To this end, statistical methods which describe high-dimensional neural time series in terms of low-dimensional latent dynamics have played a fundamental role in characterizing neural systems. Yet, what constitutes a successful method involves two opposing criteria: (1) methods should be expressive enough to capture complex nonlinear dynamics, and (2) they should maintain a notion of interpretability often only warranted by simpler linear models. In this paper, we develop an approach that balances these two objectives: the Gaussian Process Switching Linear Dynamical System (gpSLDS). Our method builds on previous work modeling the latent state evolution via a stochastic differential equation whose nonlinear dynamics are described by a Gaussian process (GP-SDEs). We propose a novel kernel function which enforces smoothly interpolated locally linear dynamics, and therefore expresses flexible -- yet interpretable -- dynamics akin to those of recurrent switching linear dynamical systems (rSLDS). Our approach resolves key limitations of the rSLDS such as artifactual oscillations in dynamics near discrete state boundaries, while also providing posterior uncertainty estimates of the dynamics. To fit our models, we leverage a modified learning objective which improves the estimation accuracy of kernel hyperparameters compared to previous GP-SDE fitting approaches. We apply our method to synthetic data and data recorded in two neuroscience experiments and demonstrate favorable performance in comparison to the rSLDS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03330v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amber Hu, David Zoltowski, Aditya Nair, David Anderson, Lea Duncker, Scott Linderman</dc:creator>
    </item>
    <item>
      <title>More variable circadian rhythms in epilepsy: a retrospective cross-sectional study using long-term heart rate recordings from wearable sensors</title>
      <link>https://arxiv.org/abs/2411.04634</link>
      <description>arXiv:2411.04634v2 Announce Type: replace 
Abstract: Background: The circadian rhythm aligns physiology and behaviour with the 24-hour light-dark cycle, and its disruption is linked to neurological disorders such as epilepsy. However, how to best quantify circadian disruption remains unclear, as it can manifest across various properties and timescales. A promising but under-explored approach is to assess the intra-individual variability in circadian rhythms over timescales of weeks to years. This is yet to be studied in epilepsy.
  Methods: We retrospectively used wearable smartwatch data (Fitbit) from 143 people with epilepsy (PWE) and 31 controls. For each participant, we extracted the circadian oscillation underlying their heart rate time series and analysed the intra-individual variability of three circadian properties: period, acrophase, and amplitude.
  Findings: We found increased intra-individual variability in period (77 min vs. 62 min, z=3.32, p&lt;0.001) and acrophase (68 min vs. 54 min, z=2.97, p=0.003) for PWE compared to controls, but not in amplitude (1.98 bpm vs. 2.05 bpm, z=-0.66, p=0.51). For PWE, we did not find any correlations between seizure frequency and intra-individual variability in circadian properties, or any difference between weeks with and without seizures.
  Interpretation: This finding indicates that the circadian rhythm of heart rate is more variable for people with epilepsy and that this can be detected using a wearable device. However, we were unable to find any associations with seizure frequency or occurrence, suggesting intra-individual variability could be another manifestation of epilepsy aetiology. Future work should investigate the combined role of anti-seizure medications, demographics, co-morbidities, and health behaviours in driving the increased intra-individual variability of circadian properties in epilepsy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04634v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Billy C. Smith, Christopher Thornton, Rachel E. Stirling, Guillermo M. Besne, Peter N. Taylor, Philippa J. Karoly, Yujiang Wang</dc:creator>
    </item>
    <item>
      <title>Functional dissociations versus post-hoc selection: Moving beyond the Stockart et al. (2024) compromise</title>
      <link>https://arxiv.org/abs/2411.15078</link>
      <description>arXiv:2411.15078v2 Announce Type: replace 
Abstract: Stockart et al. (2024) recommend guidelines for best practices in the field of unconscious cognition. However, they condone the repeatedly criticized technique of excluding trials with high visibility ratings or of participants with high sensitivity for the critical stimulus. Based on standard signal detection theory for discrimination judgments, we show that post-hoc trial selection only isolates points of neutral response bias but remains consistent with uncomfortably high levels of sensitivity. We argue that post-hoc selection constitutes a sampling fallacy that capitalizes on chance, generates regression artifacts, and wrongly ascribes unconscious processing to stimulus conditions that are far from indiscriminable. As an alternative, we advocate the study of functional dissociations, where direct (D) and indirect (I) measures are conceptualized as spanning up a two-dimensional D-I-space and where single, sensitivity, and double dissociations appear as distinct curve patterns. While Stockart et al.'s recommendations cover only a single line of that space where D is close to zero, functional dissociations can utilize the entire space, circumventing requirements like null visibility and exhaustive reliability, and allowing for the planful measurement of theoretically meaningful functional relationships between experimentally controlled variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15078v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas Schmidt, Xin Ying Lee, Maximilian P. Wolkersdorfer</dc:creator>
    </item>
    <item>
      <title>Large-scale study of human memory for meaningful narratives</title>
      <link>https://arxiv.org/abs/2311.04742</link>
      <description>arXiv:2311.04742v3 Announce Type: replace-cross 
Abstract: The statistical study of human memory requires large-scale experiments, involving many stimuli conditions and test subjects. While this approach has proven to be quite fruitful for meaningless material such as random lists of words, naturalistic stimuli, like narratives, have until now resisted such a large-scale study, due to the quantity of manual labor required to design and analyze such experiments. In this work, we develop a pipeline that uses large language models (LLMs) both to design naturalistic narrative stimuli for large-scale recall and recognition memory experiments, as well as to analyze the results. We performed online memory experiments with a large number of participants and collected recognition and recall data for narratives of different sizes. We found that both recall and recognition performance scale linearly with narrative length; however, for longer narratives people tend to summarize the content rather than recalling precise details. To investigate the role of narrative comprehension in memory, we repeated these experiments using scrambled versions of the narratives. Although recall performance declined significantly, recognition remained largely unaffected. Recalls in this condition seem to follow the original narrative order rather than the actual scrambled presentation, pointing to a contextual reconstruction of the story in memory. Finally, using LLM text embeddings, we construct a simple measure for each clause based on semantic similarity to the whole narrative, that shows a strong correlation with recall probability. Overall, our work demonstrates the power of LLMs in accessing new regimes in the study of human memory, as well as suggesting novel psychologically informed benchmarks for LLM performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04742v3</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonios Georgiou, Tankut Can, Mikhail Katkov, Misha Tsodyks</dc:creator>
    </item>
  </channel>
</rss>
