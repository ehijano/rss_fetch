<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 May 2025 01:22:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient, simulation-free estimators of firing rates with Markovian surrogates</title>
      <link>https://arxiv.org/abs/2505.08254</link>
      <description>arXiv:2505.08254v2 Announce Type: new 
Abstract: Spiking neural networks (SNNs) are powerful mathematical models that integrate the biological details of neural systems, but their complexity often makes them computationally expensive and analytically untractable. The firing rate of an SNN is a crucial first-order statistic to characterize network activity. However, estimating firing rates analytically from even simplified SNN models is challenging due to 1) the intricate dependence between the nonlinear network dynamics and parameters, and 2) the singularity and irreversibility of spikes. In this Letter, we propose a class of computationally efficient, simulation-free estimators of firing rates. This is based on a hierarchy of Markovian approximations that reduces the complexity of SNN dynamics. We show that while considering firing rates alone is insufficient for accurate estimations of themselves, the information of spiking synchrony dramatically improves the estimator's accuracy. This approach provides a practical tool for brain modelers, directly mapping biological parameters to firing rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08254v2</guid>
      <category>q-bio.NC</category>
      <category>math.PR</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongyi Wang, Louis Tao, Zhuo-Cheng Xiao</dc:creator>
    </item>
    <item>
      <title>Visual Image Reconstruction from Brain Activity via Latent Representation</title>
      <link>https://arxiv.org/abs/2505.08429</link>
      <description>arXiv:2505.08429v1 Announce Type: cross 
Abstract: Visual image reconstruction, the decoding of perceptual content from brain activity into images, has advanced significantly with the integration of deep neural networks (DNNs) and generative models. This review traces the field's evolution from early classification approaches to sophisticated reconstructions that capture detailed, subjective visual experiences, emphasizing the roles of hierarchical latent representations, compositional strategies, and modular architectures. Despite notable progress, challenges remain, such as achieving true zero-shot generalization for unseen images and accurately modeling the complex, subjective aspects of perception. We discuss the need for diverse datasets, refined evaluation metrics aligned with human perceptual judgments, and compositional representations that strengthen model robustness and generalizability. Ethical issues, including privacy, consent, and potential misuse, are underscored as critical considerations for responsible development. Visual image reconstruction offers promising insights into neural coding and enables new psychological measurements of visual experiences, with applications spanning clinical diagnostics and brain-machine interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08429v1</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukiyasu Kamitani, Misato Tanaka, Ken Shirakawa</dc:creator>
    </item>
    <item>
      <title>Detection of Moving Objects Using Self-motion Constraints on Optic Flow</title>
      <link>https://arxiv.org/abs/2505.06686</link>
      <description>arXiv:2505.06686v2 Announce Type: replace 
Abstract: As we move through the world, the pattern of light projected on our eyes is complex and dynamic, yet we are still able to distinguish between moving and stationary objects. We propose that humans accomplish this by exploiting constraints that self-motion imposes on retinal velocities. When an eye translates and rotates in a stationary 3D scene, the velocity at each retinal location is constrained to a line segment in the 2D space of retinal velocities. The slope and intercept of this segment is determined by the eye's translation and rotation, and the position along the segment is determined by the local scene depth. Since all possible velocities arising from a stationary scene must lie on this segment, velocities that are not must correspond to objects moving within the scene. We hypothesize that humans make use of these constraints by using deviations of local velocity from these constraint lines to detect moving objects. To test this, we used a virtual reality headset to present rich wide-field stimuli, simulating the visual experience of translating forward in several virtual environments with varied precision of depth information. Participants had to determine if a cued object moved relative to the scene. Consistent with the hypothesis, we found that performance depended on the deviation of the object velocity from the constraint segment, rather than a difference between retinal velocities of the object and its local surround. We also found that the endpoints of the constraint segment reflected the precision of depth information available in the different virtual environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06686v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hope Lutwak, Bas Rokers, Eero P. Simoncelli</dc:creator>
    </item>
  </channel>
</rss>
