<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 02:42:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Equitable Discrimination in Survival Prediction: The Maximum Expected C-Index</title>
      <link>https://arxiv.org/abs/2506.05592</link>
      <description>arXiv:2506.05592v1 Announce Type: new 
Abstract: The C-Index measures the discrimination performance of survival prediction models. C-Index scores are often well below the upperbound of 1 that represents perfect prediction and closer to 0.5 as achieved by random prediction. Our first objective is to provide a tighter C-Index upperbound for proportional hazards models. Our second research objective is to measure discrimination performance for subpopulations, also relative to subpopulation specific upperbounds. We present the expected C-Index (ECI) as a tight upperbound for proportional hazards models. Moreover, we define the subpopulation C-Index (SUBCI) and a sub-population specific expected C-Index (SUBECI). The metrics are applied to predict death censored graft survival (DCGF) after deceased donor kidney transplant in the US with a Cox model using standard donor (KDPI), patient (EPTS), and (Class 1) mismatch predictors. With an ECI of 0.75 for 10-year survival, the new upperbound is well below 1. A C-Index performance around 0.61 or slightly above as commonly reported in literature and replicated in this study therefore closes almost half of the gap between the ECI and the 0.5 threshold. SUBECIs don't vary significantly from the overall ECI but there are substantial and significant differences among the SUBCIs. Extending this upperbound and C-Index to subpopulations enables to identify differences in discrimination upperbounds across subpopulations and in prediction model biases. A standard Cox model for DCGF in the US can be ethnically biased.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05592v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Felipe Simon, Francisco Perez-Galarce, Joris van de Klundert</dc:creator>
    </item>
    <item>
      <title>Non-Heuristic Selection via Hybrid Regularized and Machine Learning Models for Insurance</title>
      <link>https://arxiv.org/abs/2506.05609</link>
      <description>arXiv:2506.05609v1 Announce Type: new 
Abstract: In this study, machine learning models were tested to predict whether or not a customer of an insurance company would purchase a travel insurance product. For this purpose, secondary data provided by an open-source website that compiles databases from statistical modeling competitions were used. The dataset used presents approximately 2,700 records from an unidentified company in the tourism insurance sector. Initially, the feature engineering stage was carried out, which were selected through regularized models: Ridge, Lasso and Elastic-Net. In this phase, gains were observed not only in relation to dimensionality, but also in the maintenance of interpretative capacity, through the coefficients obtained. After this process, five classification models were evaluated (Random Forests, XGBoost, H2O GBM, LightGBM and CatBoost) separately and in a hybrid way with the previous regularized models, all these stages using the k-fold stratified cross-validation technique. The evaluations were conducted by traditional metrics, including AUC, precision, recall and F1 score. A very competitive hybrid model was obtained using CatBoost combined with Lasso feature selection, achieving an AUC of 0.861 and an F1 score of 0.808. These findings motivate us to present the effectiveness of using hybrid models as a way to obtain high predictive power and maintain the interpretability of the estimation process</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05609v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luciano Ribeiro Galv\~ao, Rafael de Andrade Moral</dc:creator>
    </item>
    <item>
      <title>On the stability of global forecasting models</title>
      <link>https://arxiv.org/abs/2506.05776</link>
      <description>arXiv:2506.05776v1 Announce Type: new 
Abstract: Forecast stability, that is the consistency of predictions over time, is essential in business settings where sudden shifts in forecasts can disrupt planning and erode trust in predictive systems. Despite its importance, stability is often overlooked in favor of accuracy, particularly in global forecasting models. In this study, we evaluate the stability of point and probabilistic forecasts across different retraining frequencies and ensemble strategies using two large retail datasets (M5 and VN1). To do this, we introduce a new metric for probabilistic stability (MQC) and analyze ten different global models and four ensemble configurations. The results show that less frequent retraining not only preserves but often improves forecast stability, while ensembles, especially those combining diverse pool of models, further enhance consistency without sacrificing accuracy. These findings challenge the need for continuous retraining and highlight ensemble diversity as a key factor in reducing forecast stability. The study promotes a shift toward stability-aware forecasting practices, offering practical guidelines for building more robust and sustainable prediction systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05776v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Zanotti</dc:creator>
    </item>
    <item>
      <title>Analysis of points outcome in ATP Grand Slam Tennis using big data and machine learning</title>
      <link>https://arxiv.org/abs/2506.05866</link>
      <description>arXiv:2506.05866v1 Announce Type: new 
Abstract: Tennis is one of the world's biggest and most popular sports. Multiple researchers have, with limited success, modeled the outcome of matches using probability modelling or machine learning approaches. The approach presented here predicts the outcomes of points in tennis matches. This is based on given a probability of winning a point, based on the prior history of matches, the current match, the player rankings and if the points are started with a first or second. The use of historical public data from the matches and the players' ranking has made this study possible. In addition, we interpret the models in order to reveal important strategic factors for winning points. The historical data are from the years 2016 to 2020 in the two Grand Slam tournaments, Wimbledon and US Open, resulting in a total of 709 matches. Different machine learning methods are applied for this work such as, e.g. logistic regression, Random forest, ADABoost, and XGBoost. These models are compared to a baseline model, namely a traditional statistics measure, in this case the average. An evaluation of the results showed that the models for points proved to be a fraction better than the average. However, with the applied public data and the information level of the data, the approach presented here is not optimal for predicting who wins when the opponents are on the same position on the ranking. This methodology is interesting with respect to examining which factors are important for the outcomes of who wins points in tennis matches. Other higher quality data sets exists from e.g. Hawk Eye, although these data sets are not available for the public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05866v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Martin Illum (Department of Applied Mathematics and Computer Science, Technical University of Denmark, Richard Petersens Plads, Denmark), Hans Christian Bechs{\o}fft Mikkelsen (Department of Applied Mathematics and Computer Science, Technical University of Denmark, Richard Petersens Plads, Denmark), Emil Hovad (Department of Applied Mathematics and Computer Science, Technical University of Denmark, Richard Petersens Plads, Denmark)</dc:creator>
    </item>
    <item>
      <title>Spatial Process Mining</title>
      <link>https://arxiv.org/abs/2506.06081</link>
      <description>arXiv:2506.06081v1 Announce Type: new 
Abstract: We propose a new framework that focuses on on-site entities in the digital twin, a pairing of the real world and digital space. Characteristics include active sensing to generate event logs, spatial and temporal partitioning of complex processes, and visualization and analysis of processes that can be scaled in space and time. As a specific example, a cell production system is composed of connected manufacturing spaces called cells in a manufacturing process. A cell is sensed by ceiling cameras to generate a Gantt chart that provides a bird's-eye view of the process according to the cycle of events that occur in the cell. This Gantt chart is easy to understand for experienced operators, but we also propose a method for finding the focus of causes of deviations from the usual process without special experience or knowledge. This method captures the characteristics of the processes occurring in a cell by using our own event node ranking algorithm, a modification of HITS (Hypertext Induced Topic Selection), which scores web pages against a complex network generated from a process model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06081v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shintaro Yoshizawa, Takayuki Kanai, Masahiro Kagi</dc:creator>
    </item>
    <item>
      <title>Enhancing Neural Autoregressive Distribution Estimators for Image Reconstruction</title>
      <link>https://arxiv.org/abs/2506.05391</link>
      <description>arXiv:2506.05391v1 Announce Type: cross 
Abstract: Autoregressive models are often employed to learn distributions of image data by decomposing the $D$-dimensional density function into a product of one-dimensional conditional distributions. Each conditional depends on preceding variables (pixels, in the case of image data), making the order in which variables are processed fundamental to the model performance. In this paper, we study the problem of observing a small subset of image pixels (referred to as a pixel patch) to predict the unobserved parts of the image. As our prediction mechanism, we propose a generalized and computationally efficient version of the convolutional neural autoregressive distribution estimator (ConvNADE) model adapted for real-valued and color images. Moreover, we investigate the quality of image reconstruction when observing both random pixel patches and low-discrepancy pixel patches inspired by quasi-Monte Carlo theory. Experiments on benchmark datasets demonstrate that choosing the pixels akin to a low-discrepancy sequence reduces test loss and produces more realistic reconstructed images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05391v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ambrose Emmett-Iwaniw, Nathan Kirk</dc:creator>
    </item>
    <item>
      <title>On Level Crossings and Fade Durations in von Mises-Fisher Scattering Channels</title>
      <link>https://arxiv.org/abs/2506.05898</link>
      <description>arXiv:2506.05898v1 Announce Type: cross 
Abstract: This paper investigates the second-order statistics of multipath fading channels with von Mises-Fisher (vMF) distributed scatters. Simple closed-form expressions for the mean Doppler shift and Doppler spread are derived as the key spectral moments that capture the impact of mobility and scattering characteristics on level crossings and fade durations. These expressions are then used to analyze the influence of vMF parameters on the Level-Crossing Rate (LCR) The results show that isotropic scattering yields the highest LCR, while fading dynamics reduce with the decreasing angular spread of scatterers. Moreover, obile antenna motion parallel to the mean scattering direction results in a lower LCR than the perpendicular motion, with the difference between the two cases increasing with the higher concentration of scatterers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05898v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenan Turbic, Slawomir Stanczak</dc:creator>
    </item>
    <item>
      <title>Optimal designs for identifying effective doses in drug combination studies</title>
      <link>https://arxiv.org/abs/2506.05913</link>
      <description>arXiv:2506.05913v1 Announce Type: cross 
Abstract: We consider the optimal design problem for identifying effective dose combinations within drug combination studies where the effect of the combination of two drugs is investigated. Drug combination studies are becoming increasingly important as they investigate potential interaction effects rather than the individual impacts of the drugs. In this situation, identifying effective dose combinations that yield a prespecified effect is of special interest. If nonlinear surface models are used to describe the dose combination-response relationship, these effective dose combinations result in specific contour lines of the fitted response model.
  We propose a novel design criterion that targets the precise estimation of these effective dose combinations. In particular, an optimal design minimizes the width of the confidence band of the contour lines of interest. Optimal design theory is developed for this problem, including equivalence theorems and efficiency bounds. The performance of the optimal design is illustrated in several examples modeling dose combination data by various nonlinear surface models. It is demonstrated that the proposed optimal design for identifying effective dose combinations yields a more precise estimation of the effective dose combinations than commonly used ray or factorial designs. This particularly holds true for a case study motivated by data from an oncological dose combination study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05913v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonie Sch\"urmeyer, Ludger Sandig, Leonie Theresa Hezler, Bernd-Wolfgang Igl, Kirsten Schorning</dc:creator>
    </item>
    <item>
      <title>Bayesian variable selection in a Cox proportional hazards model with the "Sum of Single Effects" prior</title>
      <link>https://arxiv.org/abs/2506.06233</link>
      <description>arXiv:2506.06233v1 Announce Type: cross 
Abstract: Motivated by genetic fine-mapping applications, we introduce a new approach to Bayesian variable selection regression (BVSR) for time-to-event (TTE) outcomes. This new approach is designed to deal with the specific challenges that arise in genetic fine-mapping, including: the presence of very strong correlations among the covariates, often exceeding 0.99; very large data sets containing potentially thousands of covariates and hundreds of thousands of samples. We accomplish this by extending the "Sum of Single Effects" (SuSiE) method to the Cox proportional hazards (CoxPH) model. We demonstrate the benefits of the new method, "CoxPH-SuSiE", over existing BVSR methods for TTE outcomes in simulated fine-mapping data sets. We also illustrate CoxPH-SuSiE on real data by fine-mapping asthma loci using data from UK Biobank. This fine-mapping identified 14 asthma risk SNPs in 8 asthma risk loci, among which 6 had strong evidence for being causal (posterior inclusion probability greater than 50%). Two of the 6 putatively causal variants are known to be pathogenic, and others lie within a genomic sequence that is known to regulate the expression of GATA3.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06233v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunqi Yang, Karl Tayeb, Peter Carbonetto, Xiaoyuan Zhong, Carole Ober, Matthew Stephens</dc:creator>
    </item>
    <item>
      <title>A Novel Criterion for Interpreting Acoustic Emission Damage Signals Based on Cluster Onset Distribution</title>
      <link>https://arxiv.org/abs/2312.13416</link>
      <description>arXiv:2312.13416v3 Announce Type: replace 
Abstract: Structural health monitoring (SHM) relies on non-destructive techniques such as acoustic emission (AE) that generate large amounts of data over the lifespan of systems. Clustering methods are used to interpret these data and gain insights into damage progression and mechanisms. Conventional methods for evaluating clustering results utilise clustering validity indices (CVI) that prioritise compact and separable clusters. This paper introduces a novel approach based on the temporal sequence of cluster onsets, indicating the initial appearance of potential damage and allowing for early detection of defect initiation. The proposed CVI is based on the Kullback-Leibler divergence and can incorporate prior information about damage onsets when available. Three experiments on real-world datasets validate the effectiveness of the proposed method. The first benchmark focuses on detecting the loosening of bolted plates under vibration, where the onset-based CVI outperforms the conventional approach in both cluster quality and the accuracy of bolt loosening detection. The results demonstrate not only superior cluster quality but also unmatched precision in identifying cluster onsets, whether during uniform or accelerated damage growth. The two additional applications stem from industrial contexts. The first focuses on micro-drilling of hard materials using electrical discharge machining, demonstrating, for the first time, that the proposed criterion can effectively retrieve electrode progression to the reference depth, thus validating the setting of the machine to ensure structural integrity. The final application involves damage understanding in a composite/metal hybrid joint structure, where the cluster timeline is used to establish a scenario leading to critical failure due to slippage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13416v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emmanuel Ramasso, Martin Mbarga Nkogo, Neha Chandarana, Gilles Bourbon, Patrice Le Moal, Quentin Lefebvre, Martial Personeni, Constantinos Soutis, Matthieu Gresil, S\'ebastien Thibaud</dc:creator>
    </item>
    <item>
      <title>Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer</title>
      <link>https://arxiv.org/abs/2404.04399</link>
      <description>arXiv:2404.04399v2 Announce Type: replace-cross 
Abstract: We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), a novel approach to estimate the counterfactual mean of outcome under dynamic treatment policies in longitudinal problem settings. Our approach utilizes a transformer architecture with heterogeneous type embedding trained using temporal-difference learning. After obtaining an initial estimate using the transformer, following the targeted minimum loss-based likelihood estimation (TMLE) framework, we statistically corrected for the bias commonly associated with machine learning algorithms. Furthermore, our method also facilitates statistical inference by enabling the provision of 95% confidence intervals grounded in asymptotic statistical theory. Simulation results demonstrate our method's superior performance over existing approaches, particularly in complex, long time-horizon scenarios. It remains effective in small-sample, short-duration contexts, matching the performance of asymptotically efficient estimators. To demonstrate our method in practice, we applied our method to estimate counterfactual mean outcomes for standard versus intensive blood pressure management strategies in a real-world cardiovascular epidemiology cohort study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04399v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 41st International Conference on Machine Learning, PMLR 235:45097-45113, 2024</arxiv:journal_reference>
      <dc:creator>Toru Shirakawa, Yi Li, Yulun Wu, Sky Qiu, Yuxuan Li, Mingduo Zhao, Hiroyasu Iso, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Graph Neural Network Generalization with Gaussian Mixture Model Based Augmentation</title>
      <link>https://arxiv.org/abs/2411.08638</link>
      <description>arXiv:2411.08638v3 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have shown great promise in tasks like node and graph classification, but they often struggle to generalize, particularly to unseen or out-of-distribution (OOD) data. These challenges are exacerbated when training data is limited in size or diversity. To address these issues, we introduce a theoretical framework using Rademacher complexity to compute a regret bound on the generalization error and then characterize the effect of data augmentation. This framework informs the design of GRATIN, an efficient graph data augmentation algorithm leveraging the capability of Gaussian Mixture Models (GMMs) to approximate any distribution. Our approach not only outperforms existing augmentation techniques in terms of generalization but also offers improved time complexity, making it highly suitable for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08638v3</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Machine Learning (ICML) 2025</arxiv:journal_reference>
      <dc:creator>Yassine Abbahaddou, Fragkiskos D. Malliaros, Johannes F. Lutzeyer, Amine Mohamed Aboussalah, Michalis Vazirgiannis</dc:creator>
    </item>
    <item>
      <title>A Bayesian Record Linkage Approach to Applications in Tree Demography Using Overlapping LiDAR Scans</title>
      <link>https://arxiv.org/abs/2501.13285</link>
      <description>arXiv:2501.13285v2 Announce Type: replace-cross 
Abstract: In the information age, it has become increasingly common for data containing records about overlapping individuals to be distributed across multiple sources, making it necessary to identify which records refer to the same individual. The goal of record linkage is to estimate this unknown structure in the absence of a unique identifiable attribute. We introduce a Bayesian hierarchical record linkage model for spatial location data motivated by the estimation of individual specific growth-size curves for conifer species using data derived from overlapping LiDAR scans. Annual tree growth may be estimated dependent upon correctly identifying unique individuals across scans in the presence of noise. We formalize a two-stage modeling framework, connecting the record linkage model and a flexible downstream individual tree growth model, that provides robust uncertainty quantification and propagation through both stages of the modeling pipeline via an extension of the linkage-averaging approach of Sadinle (2018). In this paper, we discuss the two-stage model formulation, outline the computational strategies required to achieve scalability, assess the model performance on simulated data, and fit the model to a bi-temporal dataset derived from LiDAR scans of the Upper Gunnison Watershed provided by the Rocky Mountain Biological Laboratory to assess the impact of key topographic covariates on the growth behavior of conifer species in the Southern Rocky Mountains (USA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13285v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L. Drew, A. Kaplan, I. Breckheimer</dc:creator>
    </item>
    <item>
      <title>Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations</title>
      <link>https://arxiv.org/abs/2505.21824</link>
      <description>arXiv:2505.21824v2 Announce Type: replace-cross 
Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus (T2DM), is rapidly increasing, posing significant health and economic challenges. T2DM not only disrupts blood glucose regulation but also damages vital organs such as the heart, kidneys, eyes, nerves, and blood vessels, leading to substantial morbidity and mortality. In the US alone, the economic burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of individuals at risk is critical to mitigating these impacts. While machine learning approaches for T2DM prediction are increasingly adopted, many rely on supervised learning, which is often limited by the lack of confirmed negative cases. To address this limitation, we propose a novel unsupervised framework that integrates Non-negative Matrix Factorization (NMF) with statistical techniques to identify individuals at risk of developing T2DM. Our method identifies latent patterns of multimorbidity and polypharmacy among diagnosed T2DM patients and applies these patterns to estimate the T2DM risk in undiagnosed individuals. By leveraging data-driven insights from comorbidity and medication usage, our approach provides an interpretable and scalable solution that can assist healthcare providers in implementing timely interventions, ultimately improving patient outcomes and potentially reducing the future health and economic burden of T2DM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21824v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Praveen Kumar, Vincent T. Metzger, Scott A. Malec</dc:creator>
    </item>
  </channel>
</rss>
