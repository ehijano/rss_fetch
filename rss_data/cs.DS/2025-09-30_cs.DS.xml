<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Oct 2025 02:07:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Computing k-mers in Graphs</title>
      <link>https://arxiv.org/abs/2509.22885</link>
      <description>arXiv:2509.22885v1 Announce Type: new 
Abstract: We initiate the study of computational problems on $k$-mers (strings of length $k$) in labeled graphs. As a starting point, we consider the problem of counting the number of distinct $k$-mers found on the walks of a graph. We establish that this is #P-hard, even on connected deterministic DAGs. However, in the class of deterministic Wheeler graphs (Gagie, Manzini, and Siren, TCS 2017), we show that distinct $k$-mers of such a graph $W$ can be counted using $O(|W|k)$ or $O(n^4 \log k)$ arithmetic operations, where $n$ is the number of vertices of the graph, and $|W|$ is $n$ plus the number of edges. The latter result uses a new generalization of the technique of prefix doubling to Wheeler graphs. To generalize our results beyond Wheeler graphs, we discuss ways to transform a graph into a Wheeler graph in a manner that preserves the $k$-mers.
  As an application of our $k$-mer counting algorithms, we construct a representation of the de Bruijn graph (dBg) of the $k$-mers in time $O(|dBg| + |W|k)$. Given that the Wheeler graph can be exponentially smaller than the de Bruijn graph, for large $k$ this provides a theoretical improvement over previous de Bruijn graph construction methods from graphs, which must spend $\Omega(k)$ time per $k$-mer in the graph. Our representation occupies $O(|dBg| + |W|k \log(\max_{1 \leq \ell \leq k}(n_\ell)))$ bits of space, where $n_\ell$ is the number of distinct $l$-mers in the Wheeler graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22885v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jarno N. Alanko, Maximo Perez-Lopez</dc:creator>
    </item>
    <item>
      <title>Sparse Graph Reconstruction and Seriation for Large-Scale Image Stacks</title>
      <link>https://arxiv.org/abs/2509.23084</link>
      <description>arXiv:2509.23084v1 Announce Type: new 
Abstract: We study recovering a 1D order from a noisy, locally sampled pairwise comparison matrix under a tight query budget. We recast the task as reconstructing a sparse, noisy line graph and present, to our knowledge, the first method that provably builds a sparse graph containing all edges needed for exact seriation using only O(N(log N + K)) oracle queries, which is near-linear in N for fixed window K. The approach is parallelizable and supports both binary and bounded-noise distance oracles. Our five-stage pipeline consists of: (i) a random-hook Boruvka step to connect components via short-range edges in O(N log N) queries; (ii) iterative condensation to bound graph diameter; (iii) a double-sweep BFS to obtain a provisional global order; (iv) fixed-window densification around that order; and (v) a greedy SuperChain that assembles the final permutation. Under a simple top-1 margin and bounded relative noise we prove exact recovery; empirically, SuperChain still succeeds when only about 2N/3 of true adjacencies are present. On wafer-scale serial-section EM, our method outperforms spectral, MST, and TSP baselines with far fewer comparisons, and is applicable to other locally structured sequencing tasks such as temporal snapshot ordering, archaeological seriation, and playlist/tour construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23084v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fuming Yang, Yaron Meirovitch, Jeff W. Lichtman</dc:creator>
    </item>
    <item>
      <title>Finding the diameter of a tree with distance queries</title>
      <link>https://arxiv.org/abs/2509.23326</link>
      <description>arXiv:2509.23326v1 Announce Type: new 
Abstract: We study the number of distance queries needed to identify certain properties of a hidden tree $T$ on $n$ vertices. A distance query consists of two vertices $x,y$, and the answer is the distance of $x$ and $y$ in $T$. We determine the number of queries an optimal adaptive algorithm needs to find two vertices of maximal distance up to an additive constant, and the number of queries needed to identify the hidden tree asymptotically. We also study the non-adaptive versions of these problems, determining the number of queries needed exactly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23326v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D\'aniel Gerbner, Andr\'as Imolay, Kartal Nagy, Bal\'azs Patk\'os, Krist\'of Z\'olomy</dc:creator>
    </item>
    <item>
      <title>Maximal Covering Location Problem: A Set Coverage Approach Using Dynamic Programming</title>
      <link>https://arxiv.org/abs/2509.23334</link>
      <description>arXiv:2509.23334v1 Announce Type: new 
Abstract: The Maximal Covering Location Problem (MCLP) represents a fundamental optimization challenge in facility location theory, where the objective is to maximize demand coverage while operating under resource constraints. This paper presents a comprehensive analysis of MCLP using a set coverage methodology implemented through 0/1 knapsack dynamic programming. Our approach addresses the strategic placement of facilities to achieve optimal coverage of demand points within specified service distances. This research contributes to the understanding of facility location optimization by providing both theoretical foundations and practical algorithmic solutions for real-world applications in urban planning, emergency services, and supply chain management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23334v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukanya Samanta, Abhi Rohit Kalathoti, Siva Jayanth Gonchi, Venkata Krishna Kashyap Adiraju, Sai Kiran Nettem</dc:creator>
    </item>
    <item>
      <title>Stochastic Embedding of Digraphs into DAGs</title>
      <link>https://arxiv.org/abs/2509.23458</link>
      <description>arXiv:2509.23458v1 Announce Type: new 
Abstract: Given a weighted digraph $G=(V,E,w)$, a stochastic embedding into DAGs is a distribution $\mathcal{D}$ over pairs of DAGs $(D_1,D_2)$ such that for every $u,v$: (1) the reachability is preserved: $u\rightsquigarrow_G v$ (i.e., $v$ is reachable from $u$ in $G$) implies that $u\rightsquigarrow_{D_1} v$ or $u\rightsquigarrow_{D_2} v$ (but not both), and (2) distances are dominated: $d_G(u,v)\le\min\{d_{D_1}(u,v),d_{D_2}(u,v)\}$. The stochastic embedding $\mathcal{D}$ has expected distortion $t$ if for every $u,v\in V$, \[ \mathbb{E}_{(D_{1},D_{2})\sim\mathcal{D}}\left[d_{D_{1}}(u,v)\cdot\boldsymbol{1}[u\rightsquigarrow_{D_{1}}v]+d_{D_{2}}(u,v)\cdot\boldsymbol{1}[u\rightsquigarrow_{D_{2}}v]\right]\le t\cdot d_{G}(u,v)~. \] Finally, the sparsity of $\mathcal{D}$ is the maximum number of edges in any of the DAGs in its support. Given an $n$ vertex digraph with $m$ edges, we construct a stochastic embedding into DAGs with expected distortion $\tilde{O}(\log n)$ and $\tilde{O}(m)$ sparsity, improving a previous result by Assadi, Hoppenworth, and Wein [STOC 25], which achieved expected distortion $\tilde{O}(\log^3 n)$. Further, we can sample DAGs from this distribution in $\tilde{O}(m)$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23458v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnold Filtser</dc:creator>
    </item>
    <item>
      <title>A Near-Real-Time Reduction-Based Algorithm for Coloring Massive Graphs</title>
      <link>https://arxiv.org/abs/2509.23606</link>
      <description>arXiv:2509.23606v1 Announce Type: new 
Abstract: The graph coloring problem is a classical combinatorial optimization problem with important applications such as register allocation and task scheduling, and it has been extensively studied for decades. However, near-real-time algorithms that can deliver high-quality solutions for very large real-world graphs within a strict time frame remain relatively underexplored. In this paper, we try to bridge this gap by systematically investigating reduction rules that shrink the problem size while preserving optimality. For the first time, domination reduction, complement crown reduction, and independent set reduction are applied to large-scale instances. Building on these techniques, we propose RECOL, a reduction-based algorithm that alternates between fast estimation of lower and upper bounds, graph reductions, and heuristic coloring. We evaluate RECOL on a wide range of benchmark datasets, including SNAP, the Network Repository, DIMACS10, and DIMACS2. Experimental results show that RECOL consistently outperforms state-of-the-art algorithms on very large sparse graphs within one minute. Additional experiments further highlight the pivotal role of reduction techniques in achieving this performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23606v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenghao Zhu, Yi Zhou</dc:creator>
    </item>
    <item>
      <title>Systematic Alias Sampling: an efficient and low-variance way to sample from a discrete distribution</title>
      <link>https://arxiv.org/abs/2509.24089</link>
      <description>arXiv:2509.24089v1 Announce Type: new 
Abstract: In this paper we combine the Alias method with the concept of systematic sampling, a method commonly used in particle filters for efficient low-variance resampling. The proposed method allows very fast sampling from a discrete distribution: drawing k samples is up to an order of magnitude faster than binary search from the cumulative distribution function (cdf) or inversion methods used in many libraries. The produced empirical distribution function is evaluated using a modified Cram\'er-Von Mises goodness-of-fit statistic, showing that the method compares very favourably to multinomial sampling. As continuous distributions can often be approximated with discrete ones, the proposed method can be used as a very general way to efficiently produce random samples for particle filter proposal distributions, e.g. for motion models in robotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24089v1</guid>
      <category>cs.DS</category>
      <category>cs.MS</category>
      <category>cs.RO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/2935745</arxiv:DOI>
      <arxiv:journal_reference>ACM Transactions on Mathematical Software, Vol. 43, No. 3, Article 18, pp. 1-17, August 2016</arxiv:journal_reference>
      <dc:creator>Ilari Vallivaara, Katja Poikselk\"a, Pauli Rikula, Juha R\"oning</dc:creator>
    </item>
    <item>
      <title>The Role of Commitment in Optimal Stopping</title>
      <link>https://arxiv.org/abs/2509.24132</link>
      <description>arXiv:2509.24132v1 Announce Type: new 
Abstract: We investigate the role of commitment in optimal stopping by studying all the variants between Prophet Inequality (PI) and Pandora's Box (PB). Both problems deal with a set of variables drawn from known distributions.
  In PI the gambler observes an adversarial order of these variables with the goal of selecting one that maximizes the expected value against a prophet who knows the exact values realized. The gambler has to irrevocably decide at each step whether to select the value or discard it (commitment).
  On the other hand, in PB the gambler selects the order of inspecting the variables and for each pays an observation cost to see the actual value realized, aiming to choose one to maximize the net cost of the value chosen minus the observation cost paid. The gambler in PB can return and select any variable already seen (no commitment).
  For all the variants between these problems that arise by changing parameters such as (1) commitment (2) observation cost (3) order selection, we concisely summarize the known results and fill the gaps of variants not yet studied. We also uncover connections to Ski-Rental, a classic online algorithm problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24132v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e Correa, Evangelia Gergatsouli, Bruno Ziliotto</dc:creator>
    </item>
    <item>
      <title>Optimally revealing bits for rejection sampling</title>
      <link>https://arxiv.org/abs/2509.24290</link>
      <description>arXiv:2509.24290v1 Announce Type: new 
Abstract: Rejection sampling is a popular method used to generate numbers that follow some given distribution. We study the use of this method to generate random numbers in the unit interval from increasing probability density functions. We focus on the problem of sampling from $n$ correlated random variables from a joint distribution whose marginal distributions are all increasing. We show that, in the worst case, the expected number of random bits required to accept or reject a sample grows at least linearly and at most quadratically with $n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24290v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis-Roy Langevin, Alex Waese-Perlman</dc:creator>
    </item>
    <item>
      <title>Forcing a unique minimum spanning tree and a unique shortest path</title>
      <link>https://arxiv.org/abs/2509.24309</link>
      <description>arXiv:2509.24309v1 Announce Type: new 
Abstract: A forcing set $S$ in a combinatorial problem is a set of elements such that there is a unique solution that contains all the elements in $S$. An anti-forcing set is the symmetric concept: a set $S$ of elements is called an anti-forcing set if there is a unique solution disjoint from $S$. There are extensive studies on the computational complexity of finding a minimum forcing set in various combinatorial problems, and the known results indicate that many problems would be harder than their classical counterparts: finding a minimum forcing set for perfect matchings is NP-hard [Adams et al., Discret. Math. 2004] and finding a minimum forcing set for satisfying assignments for 3CNF formulas is $\mathrm{\Sigma}_2^P$-hard [Hatami-Maserrat, DAM 2005]. In this paper, we investigate the complexity of the problems of finding minimum forcing and anti-forcing sets for the shortest $s$-$t$ path problem and the minimum weight spanning tree problem. We show that, unlike the aforementioned results, these problems are tractable, with the exception of finding a minimum anti-forcing set for shortest $s$-$t$ paths, which is NP-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24309v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatsuya Gima, Yasuaki Kobayashi, Yota Otachi, Takumi Sato</dc:creator>
    </item>
    <item>
      <title>Simple in-place yet comparison-optimal Mergesort</title>
      <link>https://arxiv.org/abs/2509.24540</link>
      <description>arXiv:2509.24540v1 Announce Type: new 
Abstract: Mergesort is one of the few efficient sorting algorithms and, despite being the oldest one, often still the method of choice today. In contrast to some alternative algorithms, it always runs efficiently using O(n log n) element comparisons and usually works in a stable manner. Its only practical disadvantage is the need for a second array, and thus twice the amount of memory. This can be an impeding factor, especially when handling large amounts of data, where it is often either impractical or even impossible to fall back to slower or unstable sorting alternatives. Therefore, many attempts have been made to fix this problem by adapting Mergesort to work in place. While it is known that such algorithms exist, the previously published solutions are mostly not efficient, become unstable, and/or are very complex. This renders them practically useless for real-world applications. In this paper, we propose a novel in-place Mergesort algorithm that is stable by design. Albeit its running time of O(n log^2 n) is not quite optimal, it still works efficiently, both in theory using the optimal number of O(n log n) comparisons and in practice with low constants, while being easily comprehensible. The baseline for this new algorithm includes just two prerequisites: 1) an optimal array rotation; and 2) the co-ranking idea, published in 2014 and originally intended to parallelize Mergesort. Although it would certainly be possible to parallelize the presented algorithm, this paper focuses on the sequential aspect of this efficient, stable and in-place Mergesort algorithm. Additionally, we implemented our algorithm and present performance results measured on one of the largest shared memory systems currently available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24540v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Siebert</dc:creator>
    </item>
    <item>
      <title>Stronger Directed Low-Diameter Decompositions with Sub-Logarithmic Diameter and Separation</title>
      <link>https://arxiv.org/abs/2509.24565</link>
      <description>arXiv:2509.24565v1 Announce Type: new 
Abstract: This paper significantly strengthens directed low-diameter decompositions in several ways.
  We define and give the first results for separated low-diameter decompositions in directed graphs, tighten and generalize probabilistic guarantees, and prove new independence results between (far away) edges. Our results are the first to give meaningful guarantees for decompositions with small diameters $D = \Omega(\log\log n)$ in contrast to the state of the art that only applies to super-logarithmic diameters $D = \omega(\log n)$.
  These results transfer several important and widely used aspects of undirected low-diameter decompositions to the directed setting. All our results are algorithmic -- small modifications to two existing directed low-diameter decompositions [Bri+25; Li25] can be used to sample decompositions with our new guarantees in near-linear time $\tilde{O}(m)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24565v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, Richard Hlad\'ik, Shengzhe Wang, Zhijun Zhang</dc:creator>
    </item>
    <item>
      <title>Algorithms and data structures for automatic precision estimation of neural networks</title>
      <link>https://arxiv.org/abs/2509.24607</link>
      <description>arXiv:2509.24607v1 Announce Type: new 
Abstract: We describe algorithms and data structures to extend a neural network library with automatic precision estimation for floating point computations. We also discuss conditions to make estimations exact and preserve high computation performance of neural networks training and inference. Numerical experiments show the consequences of significant precision loss for particular values such as inference, gradients and deviations from mathematically predicted behavior.
  It turns out that almost any neural network accumulates computational inaccuracies. As a result, its behavior does not coincide with predicted by the mathematical model of neural network. This shows that tracking of computational inaccuracies is important for reliability of inference, training and interpretability of results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24607v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor V. Netay</dc:creator>
    </item>
    <item>
      <title>Efficient Sketching and Nearest Neighbor Search Algorithms for Sparse Vector Sets</title>
      <link>https://arxiv.org/abs/2509.24815</link>
      <description>arXiv:2509.24815v1 Announce Type: new 
Abstract: Sparse embeddings of data form an attractive class due to their inherent interpretability: Every dimension is tied to a term in some vocabulary, making it easy to visually decipher the latent space. Sparsity, however, poses unique challenges for Approximate Nearest Neighbor Search (ANNS) which finds, from a collection of vectors, the k vectors closest to a query. To encourage research on this underexplored topic, sparse ANNS featured prominently in a BigANN Challenge at NeurIPS 2023, where approximate algorithms were evaluated on large benchmark datasets by throughput and accuracy. In this work, we introduce a set of novel data structures and algorithmic methods, a combination of which leads to an elegant, effective, and highly efficient solution to sparse ANNS. Our contributions range from a theoretically-grounded sketching algorithm for sparse vectors to reduce their effective dimensionality while preserving inner product-induced ranks; a geometric organization of the inverted index; and the blending of local and global information to improve the efficiency and efficacy of ANNS. Empirically, our final algorithm, dubbed Seismic, reaches sub-millisecond per-query latency with high accuracy on a large-scale benchmark dataset using a single CPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24815v1</guid>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Bruch, Franco Maria Nardini, Cosimo Rulli, Rossano Venturini</dc:creator>
    </item>
    <item>
      <title>Sample-efficient Multiclass Calibration under $\ell_{p}$ Error</title>
      <link>https://arxiv.org/abs/2509.23000</link>
      <description>arXiv:2509.23000v1 Announce Type: cross 
Abstract: Calibrating a multiclass predictor, that outputs a distribution over labels, is particularly challenging due to the exponential number of possible prediction values. In this work, we propose a new definition of calibration error that interpolates between two established calibration error notions, one with known exponential sample complexity and one with polynomial sample complexity for calibrating a given predictor. Our algorithm can calibrate any given predictor for the entire range of interpolation, except for one endpoint, using only a polynomial number of samples. At the other endpoint, we achieve nearly optimal dependence on the error parameter, improving upon previous work. A key technical contribution is a novel application of adaptive data analysis with high adaptivity but only logarithmic overhead in the sample complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23000v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantina Bairaktari, Huy L. Nguyen</dc:creator>
    </item>
    <item>
      <title>Bounds for the Permutation Flowshop Scheduling Problem: New Framework and Theoretical Insights</title>
      <link>https://arxiv.org/abs/2509.23512</link>
      <description>arXiv:2509.23512v1 Announce Type: cross 
Abstract: In this work, we use the matrix formulation of the Permutation Flowshop Scheduling Problem with makespan minimization to derive an upper bound and a general framework for obtaining lower bounds. The proposed framework involves solving a min-max or max-min expression over a set of paths. We introduce a family of such path sets for which the min-max expression can be solved in polynomial time under certain bounded parameters. To validate the proposed approach, we test it on the Taillard and VRF benchmark instances, the two most widely used datasets in PFSP research. Our method improves the bounds in $112$ out of the $120$ Taillard instances and $430$ out of the $480$ VRF instances. These improvements include both small and large instances, highlighting the scalability of the proposed methodology. Additionally, the upper bound is used to give a more accurate estimate of the number of possible makespan values for a given instance and to present asymptotic results which provide advances in a conjecture given by Taillard related to the quality of one of the most popular lower bounds, as well as the asymptotic approximation ratio of any algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23512v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. A. Alejandro-Soto, Carlos Segura, Joel Antonio Trejo-Sanchez</dc:creator>
    </item>
    <item>
      <title>Hardness and Algorithmic Results for Roman \{3\}-Domination</title>
      <link>https://arxiv.org/abs/2509.23615</link>
      <description>arXiv:2509.23615v1 Announce Type: cross 
Abstract: A Roman $\{3\}$-dominating function on a graph $G = (V, E)$ is a function $f: V \rightarrow \{0, 1, 2, 3\}$ such that for each vertex $u \in V$, if $f(u) = 0$ then $\sum_{v \in N(u)} f(v) \geq 3$ and if $f(u) = 1$ then $\sum_{v \in N(u)} f(v) \geq 2$. The weight of a Roman $\{3\}$-dominating function $f$ is $\sum_{u \in V} f(u)$. The objective of \rtd{} is to compute a Roman $\{3\}$-dominating function of minimum weight. The problem is known to be NP-complete on chordal graphs, star-convex bipartite graphs and comb-convex bipartite graphs. In this paper, we study the complexity of \rtd{} and show that the problem is NP-complete on split graphs. In addition, we prove that the problem is W[2]-hard parameterized by weight. On the positive front, we present a polynomial-time algorithm for block graphs, thereby resolving an open question posed by Chaudhary and Pradhan [Discrete Applied Mathematics, 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23615v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangam Balchandar Reddy</dc:creator>
    </item>
    <item>
      <title>Parallel Algorithms for the One Sided Crossing Minimization Problem</title>
      <link>https://arxiv.org/abs/2509.23706</link>
      <description>arXiv:2509.23706v1 Announce Type: cross 
Abstract: The One Sided Crossing Minimization (OSCM) problem is an optimization problem in graph drawing that aims to minimize the number of edge crossings in bipartite graph layouts. It has practical applications in areas such as network visualization and VLSI (Very Large Scale Integration) design, where reducing edge crossings improves the arrangement of circuit components and their interconnections. Despite the rise of multi-core systems, the parallelization of exact and fixed-parameter tractable (FPT) algorithms for OSCM remains largely unexplored. Parallel variants offer significant potential for scaling to larger graphs but require careful handling of synchronization and memory management. In this paper, we explore various previously studied exact and FPT algorithms for OSCM, implementing and analyzing them in both sequential and parallel forms. Our main contribution lies in empirically proving that these algorithms can achieve close to linear speedup under parallelization. In particular, our best result achieves a speedup of nearly 19 on a 16-core, 32-thread machine. We further investigate and discuss the reasons why linear speedup is not always attained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23706v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bogdan-Ioan Popa, Adrian-Marius Dumitran, Livia Magureanu</dc:creator>
    </item>
    <item>
      <title>Comparison of Hyperplane Rounding for Max-Cut and Quantum Approximate Optimization Algorithm over Certain Regular Graph Families</title>
      <link>https://arxiv.org/abs/2509.24108</link>
      <description>arXiv:2509.24108v1 Announce Type: cross 
Abstract: There is a strong interest in finding challenging instances of NP-hard problems, from the perspective of showing quantum advantage. Due to the limits of near-term NISQ devices, it is moreover useful if these instances are small. In this work, we identify two graph families ($|V|&lt;1000$) on which the Goemans-Williamson algorithm for approximating the Max-Cut achieves at most a 0.912-approximation. We further show that, in comparison, a recent quantum algorithm, Quantum Approximate Optimization Algorithm (depth $p=1$), is a 0.592-approximation on Karloff instances in the limit ($n \to \infty$), and is at best a $0.894$-approximation on a family of strongly-regular graphs. We further explore construction of challenging instances computationally by perturbing edge weights, which may be of independent interest, and include these in the CI-QuBe github repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24108v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reuben Tate, Swati Gupta</dc:creator>
    </item>
    <item>
      <title>Accelerating Regression Tasks with Quantum Algorithms</title>
      <link>https://arxiv.org/abs/2509.24757</link>
      <description>arXiv:2509.24757v1 Announce Type: cross 
Abstract: Regression is a cornerstone of statistics and machine learning, with applications spanning science, engineering, and economics. While quantum algorithms for regression have attracted considerable attention, most existing work has focused on linear regression, leaving many more complex yet practically important variants unexplored. In this work, we present a unified quantum framework for accelerating a broad class of regression tasks -- including linear and multiple regression, Lasso, Ridge, Huber, $\ell_p$-, and $\delta_p$-type regressions -- achieving up to a quadratic improvement in the number of samples $m$ over the best classical algorithms. This speedup is achieved by extending the recent classical breakthrough of Jambulapati et al. (STOC'24) using several quantum techniques, including quantum leverage score approximation (Apers &amp;Gribling, 2024) and the preparation of many copies of a quantum state (Hamoudi, 2022). For problems of dimension $n$, sparsity $r &lt; n$, and error parameter $\epsilon$, our algorithm solves the problem in $\widetilde{O}(r\sqrt{mn}/\epsilon + \mathrm{poly}(n,1/\epsilon))$ quantum time, demonstrating both the applicability and the efficiency of quantum computing in accelerating regression tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24757v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenghua Liu, Zhengfeng Ji</dc:creator>
    </item>
    <item>
      <title>The Popular Dimension of Matchings</title>
      <link>https://arxiv.org/abs/2509.25150</link>
      <description>arXiv:2509.25150v1 Announce Type: cross 
Abstract: We study popular matchings in three classical settings: the house allocation problem, the marriage problem, and the roommates problem. In the popular matching problem, (a subset of) the vertices in a graph have preference orderings over their potential matches. A matching is popular if it gets a plurality of votes in a pairwise election against any other matching. Unfortunately, popular matchings typically do not exist. So we study a natural relaxation, namely popular winning sets which are a set of matchings that collectively get a plurality of votes in a pairwise election against any other matching. The $\textit{popular dimension}$ is the minimum cardinality of a popular winning set, in the worst case over the problem class.
  We prove that the popular dimension is exactly $2$ in the house allocation problem, even if the voters are weighted and ties are allowed in their preference lists. For the marriage problem and the roommates problem, we prove that the popular dimension is between $2$ and $3$, when the agents are weighted and/or their preferences orderings allow ties. In the special case where the agents are unweighted and have strict preference orderings, the popular dimension of the marriage problem is known to be exactly $1$ and we prove the popular dimension of the roommates problem is exactly $2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25150v1</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frank Connor, Louis-Roy Langevin, Ndiam\'e Ndiaye, Agn\`es Totschnig, Rohit Vasishta, Adrian Vetta</dc:creator>
    </item>
    <item>
      <title>Maximum Flow by Augmenting Paths in $n^{2+o(1)}$ Time</title>
      <link>https://arxiv.org/abs/2406.03648</link>
      <description>arXiv:2406.03648v2 Announce Type: replace 
Abstract: We present a combinatorial algorithm for computing exact maximum flows in directed graphs with $n$ vertices and edge capacities from $\{1,\dots,U\}$ in $n^{2+o(1)}\log U$ time, which is almost optimal in dense graphs. Our algorithm is a novel implementation of the classical augmenting-path framework; we list augmenting paths more efficiently using a new variant of the push-relabel algorithm that uses additional edge weights to guide the algorithm, and we derive the edge weights by constructing a directed expander hierarchy.
  Even in unit-capacity graphs, this breaks the long-standing $O(m\cdot\min\{\sqrt{m},n^{2/3}\})$ time bound of the previous combinatorial algorithms by Karzanov (1973) and Even and Tarjan (1975) when the graph has $m=\omega(n^{4/3})$ edges. Notably, our approach does not rely on continuous optimization nor heavy dynamic graph data structures, both of which are crucial in the recent developments that led to the almost-linear time algorithm by Chen et al. (FOCS 2022). Our running time also matches the $n^{2+o(1)}$ time bound of the independent combinatorial algorithm by Chuzhoy and Khanna (STOC 2024) for computing the maximum bipartite matching, a special case of maximum flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03648v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron Bernstein, Joakim Blikstad, Thatchaphol Saranurak, Ta-Wei Tu</dc:creator>
    </item>
    <item>
      <title>Orientability of Undirected Phylogenetic Networks to a Desired Class: Practical Algorithms and Application to Tree-Child Orientation</title>
      <link>https://arxiv.org/abs/2407.09776</link>
      <description>arXiv:2407.09776v3 Announce Type: replace 
Abstract: The C-Orientation problem asks whether it is possible to orient an undirected graph to a directed phylogenetic network of a desired network class C. This problem arises, for example, when visualising evolutionary data, as popular methods such as Neighbor-Net are distance-based and inevitably produce undirected graphs. The complexity of C-Orientation remains open for many classes C, including binary tree-child networks, and practical methods are still lacking. In this paper, we propose an exact FPT algorithm for C-Orientation that is applicable to any class C and parameterised by the reticulation number and the maximum size of minimal basic cycles, and a very fast heuristic for Tree-Child Orientation. While the state-of-the-art for C-Orientation is a simple exponential time algorithm whose computational bottleneck lies in searching for appropriate reticulation vertex placements, our methods significantly reduce this search space. Experiments show that, although our FPT algorithm is still exponential, it significantly outperforms the existing method. The heuristic runs even faster but with increasing false negatives as the reticulation number grows. Given this trade-off, we also discuss theoretical directions for improvement and biological applicability of the heuristic approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09776v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tsuyoshi Urata, Manato Yokoyama, Haruki Miyaji, Momoko Hayamizu</dc:creator>
    </item>
    <item>
      <title>A Polynomial Kernel for Deletion to the Scattered Class of Cliques and Trees</title>
      <link>https://arxiv.org/abs/2409.14209</link>
      <description>arXiv:2409.14209v2 Announce Type: replace 
Abstract: The class of graph deletion problems has been extensively studied in theoretical computer science, particularly in the field of parameterized complexity. Recently, a new notion of graph deletion problems was introduced, called deletion to scattered graph classes, where after deletion, each connected component of the graph should belong to at least one of the given graph classes. While fixed-parameter algorithms were given for a wide variety of problems, little progress has been made on the kernelization complexity of any of them. In this paper, we present the first non-trivial polynomial kernel for one such deletion problem, where, after deletion, each connected component should be a clique or a tree - that is, as densest as possible or as sparsest as possible (while being connected). We develop a kernel consisting of O(k^5) vertices for this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14209v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.ISAAC.2024.41</arxiv:DOI>
      <dc:creator>Ashwin Jacob, Diptapriyo Majumdar, Meirav Zehavi</dc:creator>
    </item>
    <item>
      <title>A Partition Cover Approach to Tokenization</title>
      <link>https://arxiv.org/abs/2501.06246</link>
      <description>arXiv:2501.06246v3 Announce Type: replace-cross 
Abstract: Tokenization is the process of encoding strings into tokens of a fixed vocabulary size, and is widely utilized in Natural Language Processing applications. The leading tokenization algorithm today is Byte-Pair Encoding (BPE), which formulates the tokenization problem as a compression problem and tackles it by performing sequences of merges. In this work, we formulate tokenization as an optimization objective, show that it is NP-hard via a simple reduction from vertex cover, and propose a polynomial-time greedy algorithm GreedTok. Our formulation naturally relaxes to the well-studied weighted maximum coverage problem which has a simple $(1 - 1/e)$-approximation algorithm GreedWMC. Through empirical evaluations on real-world corpora, we show that GreedTok outperforms BPE and Unigram on compression and achieves a covering score comparable to GreedWMC. Finally, our extensive pre-training for two transformer-based language models with 1 billion parameters, comparing the choices of BPE and GreedTok as the tokenizer, shows that GreedTok achieves a lower bit per byte even when we control for either the total dataset proportion or total training tokens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06246v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia Peng Lim, Shawn Tan, Davin Choo, Hady W. Lauw</dc:creator>
    </item>
    <item>
      <title>Efficient $\varepsilon$-approximate minimum-entropy couplings</title>
      <link>https://arxiv.org/abs/2509.19598</link>
      <description>arXiv:2509.19598v2 Announce Type: replace-cross 
Abstract: Given $m \ge 2$ discrete probability distributions over $n$ states each, the minimum-entropy coupling is the minimum-entropy joint distribution whose marginals are the same as the input distributions. Computing the minimum-entropy coupling is NP-hard, but there has been significant progress in designing approximation algorithms; prior to this work, the best known polynomial-time algorithms attain guarantees of the form $H(\operatorname{ALG}) \le H(\operatorname{OPT}) + c$, where $c \approx 0.53$ for $m=2$, and $c \approx 1.22$ for general $m$ [CKQGK '23].
  A main open question is whether this task is APX-hard, or whether there exists a polynomial-time approximation scheme (PTAS). In this work, we design an algorithm that produces a coupling with entropy $H(\operatorname{ALG}) \le H(\operatorname{OPT}) + \varepsilon$ in running time $n^{O(\operatorname{poly}(1/\varepsilon) \cdot \operatorname{exp}(m) )}$: showing a PTAS exists for constant $m$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19598v2</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Spencer Compton</dc:creator>
    </item>
  </channel>
</rss>
