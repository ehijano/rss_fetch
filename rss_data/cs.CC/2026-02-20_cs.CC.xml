<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Feb 2026 05:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Separations above TFNP from Sherali-Adams Lower Bounds</title>
      <link>https://arxiv.org/abs/2602.16810</link>
      <description>arXiv:2602.16810v1 Announce Type: new 
Abstract: Unlike in TFNP, for which there is an abundance of problems capturing natural existence principles which are incomparable (in the black-box setting), Kleinberg et al. [KKMP21] observed that many of the natural problems considered so far in the second level of the total function polynomial hierarchy (TF$\Sigma_2$) reduce to the Strong Avoid problem. In this work, we prove that the Linear Ordering Principle does not reduce to Strong Avoid in the black-box setting, exhibiting the first TF$\Sigma_2$ problem that lies outside of the class of problems reducible to Strong Avoid.
  The proof of our separation exploits a connection between total search problems in the polynomial hierarchy and proof complexity, recently developed by Fleming, Imrek, and Marciot [FIM25]. In particular, this implies that to show our separation, it suffices to show that there is no small proof of the Linear Ordering Principle in a $\Sigma_2$-variant of the Sherali-Adams proof system. To do so, we extend the classical pseudo-expectation method to the $\Sigma_2$ setting, showing that the existence of a $\Sigma_2$ pseudo-expectation precludes a $\Sigma_2$ Sherali-Adams proof. The main technical challenge is in proving the existence of such a pseudo-expectation, we manage to do so by solving a combinatorial covering problem about permutations. We also show that the extended pseudo-expectation bound implies that the Linear Ordering Principle cannot be reduced to any problem admitting a low-degree Sherali-Adams refutation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16810v1</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Gal, Noah Fleming, Deniz Imrek, Christophe Marciot</dc:creator>
    </item>
    <item>
      <title>On the complexity of covering points by disjoint segments and by guillotine cuts</title>
      <link>https://arxiv.org/abs/2602.17294</link>
      <description>arXiv:2602.17294v1 Announce Type: cross 
Abstract: We show that two geometric cover problems in the plane, related to covering points with disjoint line segments, are NP-complete. Given $n$ points in the plane and a value $k$, the first problem asks if all points can be covered by $k$ disjoint line segments; the second problem treats the analogous question for $k$ guillotine cuts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17294v1</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Delia Garijo, Alberto M\'arquez, Rodrigo I. Silveira</dc:creator>
    </item>
    <item>
      <title>Some Remarks on Marginal Code Languages</title>
      <link>https://arxiv.org/abs/2602.17309</link>
      <description>arXiv:2602.17309v1 Announce Type: cross 
Abstract: A prefix code L satisfies the condition that no word of L is a proper prefix of another word of L. Recently, Ko, Han and Salomaa relaxed this condition by allowing a word of L to be a proper prefix of at most k words of L, for some `margin' k, introducing thus the class of k-prefix-free languages, as well as the similar classes of k-suffix-free and k-infix-free languages. Here we unify the definitions of these three classes of languages into one uniform definition in two ways: via the method of partial orders and via the method of transducers. Thus, for any known class of code-related languages definable via the transducer method, one gets a marginal version of that class. Building on the techniques of Ko, Han and Salomaa, we discuss the \emph{uniform} satisfaction and maximality problems for marginal classes of languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17309v1</guid>
      <category>cs.FL</category>
      <category>cs.CC</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stavros Konstantinidis</dc:creator>
    </item>
    <item>
      <title>Provably Explaining Neural Additive Models</title>
      <link>https://arxiv.org/abs/2602.17530</link>
      <description>arXiv:2602.17530v1 Announce Type: cross 
Abstract: Despite significant progress in post-hoc explanation methods for neural networks, many remain heuristic and lack provable guarantees. A key approach for obtaining explanations with provable guarantees is by identifying a cardinally-minimal subset of input features which by itself is provably sufficient to determine the prediction. However, for standard neural networks, this task is often computationally infeasible, as it demands a worst-case exponential number of verification queries in the number of input features, each of which is NP-hard.
  In this work, we show that for Neural Additive Models (NAMs), a recent and more interpretable neural network family, we can efficiently generate explanations with such guarantees. We present a new model-specific algorithm for NAMs that generates provably cardinally-minimal explanations using only a logarithmic number of verification queries
  in the number of input features, after a parallelized preprocessing step with logarithmic runtime in the required precision is applied to each small univariate NAM component.
  Our algorithm not only makes the task of obtaining cardinally-minimal explanations feasible, but even outperforms existing algorithms designed to find the relaxed variant of subset-minimal explanations - which may be larger and less informative but easier to compute - despite our algorithm solving a much more difficult task.
  Our experiments demonstrate that, compared to previous algorithms, our approach provides provably smaller explanations than existing works and substantially reduces the computation time. Moreover, we show that our generated provable explanations offer benefits that are unattainable by standard sampling-based techniques typically used to interpret NAMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17530v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahaf Bassan, Yizhak Yisrael Elboher, Tobias Ladner, Volkan \c{S}ahin, Jan Kretinsky, Matthias Althoff, Guy Katz</dc:creator>
    </item>
    <item>
      <title>Pseudo-deterministic Quantum Algorithms</title>
      <link>https://arxiv.org/abs/2602.17647</link>
      <description>arXiv:2602.17647v1 Announce Type: cross 
Abstract: We initiate a systematic study of pseudo-deterministic quantum algorithms. These are quantum algorithms that, for any input, output a canonical solution with high probability. Focusing on the query complexity model, our main contributions include the following complexity separations, which require new lower bound techniques specifically tailored to pseudo-determinism:
  - We exhibit a problem, Avoid One Encrypted String (AOES), whose classical randomized query complexity is $O(1)$ but is maximally hard for pseudo-deterministic quantum algorithms ($\Omega(N)$ query complexity).
  - We exhibit a problem, Quantum-Locked Estimation (QL-Estimation), for which pseudo-deterministic quantum algorithms admit an exponential speed-up over classical pseudo-deterministic algorithms ($O(\log(N))$ vs. $\Theta(\sqrt{N})$), while the randomized query complexity is $O(1)$.
  Complementing these separations, we show that for any total problem $R$, pseudo-deterministic quantum algorithms admit at most a quintic advantage over deterministic algorithms, i.e., $D(R) = \tilde O(psQ(R)^5)$.
  On the algorithmic side, we identify a class of quantum search problems that can be made pseudo-deterministic with small overhead, including Grover search, element distinctness, triangle finding, $k$-sum, and graph collision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17647v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Aaronson, Tom Gur, Jiawei Li</dc:creator>
    </item>
    <item>
      <title>Finding Minimum Matching Cuts in $H$-free Graphs</title>
      <link>https://arxiv.org/abs/2502.18942</link>
      <description>arXiv:2502.18942v2 Announce Type: replace-cross 
Abstract: A matching cut is a matching that is also an edge cut. In the problem Minimum Matching Cut, we ask for a matching cut with the minimum number of edges in the matching. We investigate the differences in complexity between Minimum Matching Cut, its counterpart Maximum Matching Cut, and the decision problem Matching Cut. Our polynomial-time algorithms for $P_8$-free, $S_{1,1,3}$-free and $(P_6 + P_4)$-free graphs extend the cases where Minimum Matching Cut and Maximum Matching Cut are known to differ in complexity. In addition, they solve open cases for the well-studied problem Matching Cut. The NP-hardness proof for $3P_3$-free graphs implies that Minimum Matching Cut and Matching Cut, which is polynomial-time solvable even for $sP_3$-free graphs, for any $s \geq 1$, differ in complexity on certain graph classes. Further, we give complexity dichotomies for both general and bipartite graphs of bounded radius and diameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18942v2</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felicia Lucke, Joseph Marchand, Jannik Olbrich</dc:creator>
    </item>
    <item>
      <title>Finding a Fair Scoring Function for Top-$k$ Selection: From Hardness to Practice</title>
      <link>https://arxiv.org/abs/2503.11575</link>
      <description>arXiv:2503.11575v4 Announce Type: replace-cross 
Abstract: Selecting a subset of the $k$ "best" items from a dataset of $n$ items, based on a scoring function, is a key task in decision-making. Given the rise of automated decision-making software, it is important that the outcome of this process, called top-$k$ selection, is fair. Here we consider the problem of identifying a fair linear scoring function for top-$k$ selection. The function computes a score for each item as a weighted sum of its (numerical) attribute values, and must ensure that the selected subset includes adequate representation of a minority or historically disadvantaged group. Existing algorithms do not scale efficiently, particularly in higher dimensions. Our hardness analysis shows that in more than two dimensions, no algorithm is likely to achieve good scalability with respect to dataset size, and the computational complexity is likely to increase rapidly with dimensionality. However, the hardness results also provide key insights guiding algorithm design, leading to our two-pronged solution: (1) For small values of $k$, our hardness analysis reveals a gap in the hardness barrier. By addressing various engineering challenges, including achieving efficient parallelism, we turn this potential of efficiency into an optimized algorithm delivering substantial practical performance gains. (2) For large values of $k$, where the hardness is robust, we employ a practically efficient algorithm which, despite being theoretically worse, achieves superior real-world performance. Experimental evaluations on real-world datasets then explore scenarios where worst-case behavior does not manifest, identifying areas critical to practical performance. Our solution achieves speed-ups of up to several orders of magnitude compared to SOTA, an efficiency made possible through a tight integration of hardness analysis, algorithm design, practical engineering, and empirical evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11575v4</guid>
      <category>cs.DB</category>
      <category>cs.CC</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangya Cai</dc:creator>
    </item>
    <item>
      <title>Minimum Selective Subset on Unit Disk Graphs and Circle Graphs</title>
      <link>https://arxiv.org/abs/2510.01931</link>
      <description>arXiv:2510.01931v4 Announce Type: replace-cross 
Abstract: In a connected simple graph G = (V(G),E(G)), each vertex is assigned one of c colors, where V(G) can be written as a union of a total of c subsets V_{1},...,V_{c} and V_{i} denotes the set of vertices of color i. A subset S of V(G) is called a selective subset if, for every i, every vertex v in V_{i} has at least one nearest neighbor in $S \cup (V(G) \setminus V_{i})$ that also lies in V_{i}. The Minimum Selective Subset (MSS) problem asks for a selective subset of minimum size.
  We show that the MSS problem is log-APX-hard on general graphs, even when c=2. As a consequence, the problem does not admit a polynomial-time approximation scheme (PTAS) unless P = NP. On the positive side, we present a PTAS for unit disk graphs, which works without requiring a geometric representation and applies for arbitrary c. We further prove that MSS remains NP-complete in unit disk graphs for arbitrary c. In addition, we show that the MSS problem is log-APX-hard on circle graphs, even when c=2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01931v4</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bubai Manna</dc:creator>
    </item>
    <item>
      <title>Active Learning for Decision Trees with Provable Guarantees</title>
      <link>https://arxiv.org/abs/2601.20775</link>
      <description>arXiv:2601.20775v2 Announce Type: replace-cross 
Abstract: This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity, (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+\epsilon)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm's dependence on the error tolerance $\epsilon$ is close to optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20775v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Arshia Soltani Moakhar, Tanapoom Laoaron, Faraz Ghahremani, Kiarash Banihashem, MohammadTaghi Hajiaghayi</dc:creator>
    </item>
  </channel>
</rss>
