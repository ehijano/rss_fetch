<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Dec 2025 02:35:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tolerance Intervals Using Dirichlet Processes</title>
      <link>https://arxiv.org/abs/2512.02178</link>
      <description>arXiv:2512.02178v1 Announce Type: new 
Abstract: In nonclinical pharmaceutical development, tolerance intervals are critical in ensuring product and process quality. They are statistical intervals designed to contain a specified proportion of the population with a given confidence level. Parametric and non-parametric methods have been developed to obtain tolerance intervals. The former work with small samples but can be affected by distribution misspecification. The latter offer larger flexibility but require large sample sizes. As an alternative, we propose Dirichlet process-based Bayesian nonparametric tolerance intervals to overcome the limitations. We develop a computationally efficient tolerance interval construction algorithm based on the analytically tractable quantile process of the Dirichlet process. Simulation studies show that our new approach is very robust to distributional assumptions and performs as efficiently as existing tolerance interval methods. To illustrate how the model works in practice, we apply our method to the tolerance interval estimation for potency data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02178v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seokjun Choi, Tony Pourmohamad, Bruno Sans\'o</dc:creator>
    </item>
    <item>
      <title>Efficient and Intuitive Two-Phase Validation Across Multiple Models via Principal Components</title>
      <link>https://arxiv.org/abs/2512.02182</link>
      <description>arXiv:2512.02182v1 Announce Type: new 
Abstract: Two-phase sampling offers a cost-effective way to validate error-prone measurements in observational databases or randomized trials. Inexpensive or easy-to-obtain information is collected for the entire study in Phase I. Then, a subset of patients undergoes cost-intensive validation to collect more accurate data in Phase II. Critically, any Phase I variables can be used to strategically select the Phase II subset, often enriched for a particular model of interest. However, when balancing primary and secondary analyses in the same study, competing models and priorities can result in poorly defined objectives for the most informative Phase II sampling criterion. We propose an intuitive, easy-to-use solution that balances and prioritizes explaining the largest amount of variability across all models of interest. Using principal components to succinctly summarize the inherent variability of the error-prone covariates for all models. Then, we sample patients with the most "extreme" principal components (i.e., the smallest or largest values) for validation. Through simulations and an application to data from the National Health and Nutrition Examination Survey (NHANES), we show that extreme tail sampling on the first principal component offers simultaneous efficiency gains across multiple models of interest relative to sampling for one specific model. Our proposed sampling strategy is implemented in the open-source R package, auditDesignR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02182v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Cole Manschot</dc:creator>
    </item>
    <item>
      <title>Discrete Sequential Barycenter Arrays: Representation, Approximation, and Modeling of Probability Measures</title>
      <link>https://arxiv.org/abs/2512.02249</link>
      <description>arXiv:2512.02249v1 Announce Type: new 
Abstract: Constructing flexible probability models that respect constraints on key functionals -- such as the mean -- is a fundamental problem in nonparametric statistics. Existing approaches lack systematic tools for enforcing such constraints while retaining full modeling flexibility. This paper introduces a new representation for univariate probability measures based on discrete sequential barycenter arrays (SBA). We study structural properties of SBA representations and establish new approximation results. In particular, we show that for any target distribution, its SBA-based discrete approximations converge in both the weak topology and in Wasserstein distances, and that the representation is exact for all distributions with finite discrete support. We further characterize a broad class of measures whose SBA partitions exhibit regularity and induce increasingly fine meshes, and we prove that this class is dense in standard probabilistic topologies. These theoretical results enable the construction of probability models that preserve prescribed values -- or full distributions -- of the mean while maintaining large support. As an application, we derive a mixture model for density estimation whose induced mixing distribution has a fixed or user-specified mean. The resulting framework provides a principled mechanism for incorporating mean constraints in nonparametric modeling while preserving strong approximation properties. The approach is illustrated using both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02249v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alejandro Jara, Carlos Sing-Long</dc:creator>
    </item>
    <item>
      <title>Implicit score-driven filters for time-varying parameter models</title>
      <link>https://arxiv.org/abs/2512.02744</link>
      <description>arXiv:2512.02744v1 Announce Type: new 
Abstract: We propose an observation-driven modeling framework that permits time variation in the model parameters using an implicit score-driven (ISD) update. The ISD update maximizes the logarithmic observation density with respect to the parameter vector, while penalizing the weighted L2 norm relative to a one-step-ahead predicted parameter. This yields an implicit stochastic-gradient update. We show that the popular class of explicit score-driven (ESD) models arises if the observation log density is linearly approximated around the prediction. By preserving the full density, the ISD update globalizes favorable local properties of the ESD update. Namely, for log-concave observation densities, whether correctly specified or not, the ISD filter is stable for all learning rates, while its updates are contractive in mean squared error toward the (pseudo-)true parameter at every time step. We demonstrate the usefulness of ISD filters in simulations and empirical illustrations in finance and macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02744v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rutger-Jan Lange, Bram van Os, Dick van Dijk</dc:creator>
    </item>
    <item>
      <title>Analysis of hypothesis tests for multiple uncertain finite populations with applications to normal uncertainty distributions</title>
      <link>https://arxiv.org/abs/2512.02832</link>
      <description>arXiv:2512.02832v1 Announce Type: new 
Abstract: Hypothesis test plays a key role in uncertain statistics based on uncertain measure. This paper extends the parametric hypothesis of a single uncertain population to multiple cases, thereby addressing a broader range of scenarios. First, an uncertain family-wise error rate is defined to control the overall error in simultaneous testing. Subsequently, a hypothesis test of two uncertain populations is proposed, and the rejection region for the null hypothesis at a significance level is derived, laying the foundation for further analysis. Building on this, a homogeneity test for multiple populations is developed to assess whether the unknown population parameters differ significantly. When there is no significant difference in these parameters among finite populations or within a subset, a common test is used to determine whether they equal a fixed constant. Finally, homogeneity and common tests for normal uncertain populations with means and standard deviations are conducted under three cases: only means, only standard deviations, or both are unknown. Numerical simulations demonstrate the feasibility and accuracy of the proposed methods, and a real example is provided to illustrate their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02832v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Zhang, Zhiming Li</dc:creator>
    </item>
    <item>
      <title>Correcting for sampling variability in maximum likelihood-based one-sample log-rank tests</title>
      <link>https://arxiv.org/abs/2512.02878</link>
      <description>arXiv:2512.02878v1 Announce Type: new 
Abstract: Single-arm studies in the early development phases of new treatments are not uncommon in the context of rare diseases or in paediatrics. If an assessment of efficacy is to be made at the end of such a study, the observed endpoints can be compared with reference values that can be derived from historical data. For a time-to-event endpoint, a statistical comparison with a reference curve can be made using the one-sample log-rank test. In order to ensure the interpretability of the results of this test, the role of the reference curve is crucial. This quantity is often estimated from a historical control group using a parametric procedure. Hence, it should be noted that it is subject to estimation uncertainty. However, this aspect is not taken into account in the one-sample log-rank test statistic. We analyse this estimation uncertainty for the common situation that the reference curve is estimated parametrically using the maximum likelihood method, and indicate how the variance estimation of the one-sample log-rank test can be adapted in order to take this variability into account. The resulting test procedures are illustrated using a data example and analysed in more detail using simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02878v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Fabian Danzer, Rene Schmidt</dc:creator>
    </item>
    <item>
      <title>Adaptive Decentralized Federated Learning for Robust Optimization</title>
      <link>https://arxiv.org/abs/2512.02852</link>
      <description>arXiv:2512.02852v2 Announce Type: cross 
Abstract: In decentralized federated learning (DFL), the presence of abnormal clients, often caused by noisy or poisoned data, can significantly disrupt the learning process and degrade the overall robustness of the model. Previous methods on this issue often require a sufficiently large number of normal neighboring clients or prior knowledge of reliable clients, which reduces the practical applicability of DFL. To address these limitations, we develop here a novel adaptive DFL (aDFL) approach for robust estimation. The key idea is to adaptively adjust the learning rates of clients. By assigning smaller rates to suspicious clients and larger rates to normal clients, aDFL mitigates the negative impact of abnormal clients on the global model in a fully adaptive way. Our theory does not put any stringent conditions on neighboring nodes and requires no prior knowledge. A rigorous convergence analysis is provided to guarantee the oracle property of aDFL. Extensive numerical experiments demonstrate the superior performance of the aDFL method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02852v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyuan Wu, Feifei Wang, Yuan Gao, Rui Wang, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>HeteroJIVE: Joint Subspace Estimation for Heterogeneous Multi-View Data</title>
      <link>https://arxiv.org/abs/2512.02866</link>
      <description>arXiv:2512.02866v1 Announce Type: cross 
Abstract: Many modern datasets consist of multiple related matrices measured on a common set of units, where the goal is to recover the shared low-dimensional subspace. While the Angle-based Joint and Individual Variation Explained (AJIVE) framework provides a solution, it relies on equal-weight aggregation, which can be strictly suboptimal when views exhibit significant statistical heterogeneity (arising from varying SNR and dimensions) and structural heterogeneity (arising from individual components). In this paper, we propose HeteroJIVE, a weighted two-stage spectral algorithm tailored to such heterogeneity. Theoretically, we first revisit the ``non-diminishing" error barrier with respect to the number of views $K$ identified in recent literature for the equal-weight case. We demonstrate that this barrier is not universal: under generic geometric conditions, the bias term vanishes and our estimator achieves the $O(K^{-1/2})$ rate without the need for iterative refinement. Extending this to the general-weight case, we establish error bounds that explicitly disentangle the two layers of heterogeneity. Based on this, we derive an oracle-optimal weighting scheme implemented via a data-driven procedure. Extensive simulations corroborate our theoretical findings, and an application to TCGA-BRCA multi-omics data validates the superiority of HeteroJIVE in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02866v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyang Li, Zhongyuan Lyu</dc:creator>
    </item>
    <item>
      <title>Balancing Weights for Causal Inference in Observational Factorial Studies</title>
      <link>https://arxiv.org/abs/2310.04660</link>
      <description>arXiv:2310.04660v3 Announce Type: replace 
Abstract: Many scientific questions in biomedical, environmental, and psychological research involve understanding the effects of multiple factors on outcomes. While factorial experiments are ideal for this purpose, randomized controlled treatment assignment is generally infeasible in many empirical studies. Therefore, investigators must rely on observational data, where drawing reliable causal inferences for multiple factors remains challenging. As the number of treatment combinations grows exponentially with the number of factors, some treatment combinations can be rare or missing by chance in observed data, further complicating factorial effects estimation. To address these challenges, we propose a novel weighting method tailored to observational studies with multiple factors. Our approach uses weighted observational data to emulate a randomized factorial experiment, enabling simultaneous estimation of the effects of multiple factors and their interactions. Our investigations reveal a crucial nuance: achieving balance among covariates, as in single-factor scenarios, is necessary but insufficient for unbiasedly estimating factorial effects; balancing the factors is also essential in multi-factor settings. Moreover, we extend our weighting method to handle missing treatment combinations in observed data. Finally, we study the asymptotic behavior of the new weighting estimators and propose a consistent variance estimator, providing reliable inferences on factorial effects in observational studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04660v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoqi Yu, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Causal inference for N-of-1 trials</title>
      <link>https://arxiv.org/abs/2406.10360</link>
      <description>arXiv:2406.10360v3 Announce Type: replace 
Abstract: The aim of personalized medicine is to tailor treatment decisions to individuals' characteristics. N-of-1 trials are within-person crossover trials that hold the promise of targeting individual-specific effects. While the idea behind N-of-1 trials might seem simple, analyzing and interpreting N-of-1 trials is not straightforward. Here we ground N-of-1 trials in a formal causal inference framework and formalize intuitive claims from the N-of-1 trials literature. We focus on causal inference from a single N-of-1 trial and define a conditional average treatment effect (CATE) that represents a target in this setting, which we call the U-CATE. We discuss assumptions sufficient for identification and estimation of the U-CATE under different causal models where the treatment schedule is assigned at baseline. A simple mean difference is an unbiased, asymptotically normal estimator of the U-CATE in simple settings. We also consider settings where carryover effects, trends over time, time-varying common causes of the outcome, and outcome-outcome effects are present. In these more complex settings, we show that a time-varying g-formula identifies the U-CATE under explicit assumptions. Finally, we analyze data from N-of-1 trials about acne symptoms and show how different assumptions about the data generating process can lead to different analytical strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10360v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Piccininni, Mats J. Stensrud, Zachary Shahn, Stefan Konigorski</dc:creator>
    </item>
    <item>
      <title>Jackknife Empirical Likelihood Method for U Statistics Based on Multivariate Samples and its Applications</title>
      <link>https://arxiv.org/abs/2408.14038</link>
      <description>arXiv:2408.14038v2 Announce Type: replace 
Abstract: We develop a jackknife empirical likelihood (JEL) framework for inference on parameters defined through multivariate three-sample U-statistic. From three independent multivariate samples, we construct JEL ratio statistic based on suitable jackknife pseudo-values and, under mild regularity conditions, establish a Wilks-type result showing that the log JEL ratio converges in distribution to a chi-square limit. This provides asymptotically valid confidence intervals for the parameter of interest without explicit variance estimation or heavy resampling. To illustrate the usefulness of the proposed method, we construct confidence intervals for differences in volume under the surface (VUS) measures, which are widely used in classification problems. Through Monte Carlo simulations, we compare the performance of JEL-based confidence intervals with those obtained from normal approximation of U-statistic and kernel-based methods. The findings indicate that the proposed JEL approach outperforms existing methods in terms of coverage probability and computational efficiency. Finally, we apply our methods to a recent real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14038v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Naresh Garg, Litty Mathew, Isha Dewan, Sudheesh Kumar Kattumannil</dc:creator>
    </item>
    <item>
      <title>A Two-Stage Bayesian Approach for Variable Selection in Joint Modeling of Multiple Longitudinal Markers with Competing Risks</title>
      <link>https://arxiv.org/abs/2412.03797</link>
      <description>arXiv:2412.03797v2 Announce Type: replace 
Abstract: In many clinical and epidemiological studies, collecting longitudinal measurements together with time-to-event outcomes is essential. Accurately estimating the association between longitudinal markers and event risks, as well as identifying key markers for prediction, is especially important in the presence of competing risks. However, as the number of markers increases, fitting full joint models becomes computationally difficult and may lead to convergence issues. We propose a two-stage Bayesian approach for variable selection in joint models with multiple longitudinal markers and competing risks. The method efficiently identifies important longitudinal markers and covariates. In the first stage, a one-marker joint model is fitted for each marker with the competing risks outcome, and individual marker trajectories are predicted, reducing bias from informative dropout. In the second stage, a cause-specific hazards model is fitted, incorporating the predicted current values of all markers as time-dependent covariates. We consider both continuous and Dirac spike-and-slab priors for Bayesian variable selection, implemented through MCMC algorithms. Our approach enables risk prediction using a large number of longitudinal markers, which is often infeasible for standard joint models. We evaluate performance through simulation studies, examining both variable selection and predictive accuracy. Finally, we apply the method to predict dementia risk in the Three-City (3C) study, a French cohort with competing risks of death. To facilitate use, we provide an R package, VSJM, available at: https:/github.com/tbaghfalaki/VSJM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03797v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taban Baghfalaki, Reza Hashemi, Christophe Tzourio, Catherine Helmer, Helene Jacqmin-Gadda</dc:creator>
    </item>
    <item>
      <title>Parameter-Specific Bias Diagnostics in Random-Effects Panel Data Models</title>
      <link>https://arxiv.org/abs/2412.20555</link>
      <description>arXiv:2412.20555v2 Announce Type: replace 
Abstract: The Hausman specification test detects inconsistency in random-effects estimators by comparing them with alternative fixed-effects estimators. This note shows how a recently proposed bias diagnostic for linear mixed models can complement this test in random-effects panel-data applications. The diagnostic delivers parameter-specific internal estimates of finite-sample bias, together with permutation-based $p$-values, from a single fitted random-effects model. We illustrate its use in a gasoline-demand panel and in a value-added model for teacher evaluation, using publicly available R packages, and we discuss how the resulting bias summaries can be incorporated into routine practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20555v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew T. Karl</dc:creator>
    </item>
    <item>
      <title>Robust Semiparametric Graphical Models with Skew-Elliptical Distributions</title>
      <link>https://arxiv.org/abs/2501.08033</link>
      <description>arXiv:2501.08033v3 Announce Type: replace 
Abstract: We propose semiparametric estimators, called elliptical skew-(S)KEPTIC, for efficiently and robustly estimating non-Gaussian graphical models. Our approach extends the semiparametric elliptical framework to the meta skew-elliptical family, which accommodates skewness. Theoretically, we show that the elliptical skew-(S)KEPTIC estimators achieve robust convergence rates for both graph recovery and parameter estimation. Through numerical simulations, we illustrate the reliable graph recovery performance of the elliptical skew-(S)KEPTIC estimators. Finally, we apply the new method to the daily log-returns of the stocks in the S\&amp;P 500 index and obtain a sparser graph than with Gaussian copula graphical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08033v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Di Luzio, Giacomo Morelli</dc:creator>
    </item>
    <item>
      <title>Model-based calibration of gear-specific fish abundance survey data as a change-of-support problem</title>
      <link>https://arxiv.org/abs/2505.05767</link>
      <description>arXiv:2505.05767v2 Announce Type: replace 
Abstract: For commercial and recreational fisheries of a wide-ranging species to be sustainable, abundance studies from neighboring regions should be unified. For the first time in the USA, a single research project to estimate the abundance of the Greater Amberjack {Seriola dumerili) is being undertaken at the continental scale. A major methodological challenge lies in 1) the difference in fish detection gears deployed by regional survey teams that produce gear-specific relative abundance indices, and 2) the unknown relationship between actual abundance and these indices. In this paper, we develop a conversion tool that is operationalized from a Bayesian hierarchical model in an inferential context akin to the change-of-support problem often encountered in large-scale spatial studies; though, the context here is to reconcile abundance data observed at various gear-specific scales. To this end, we consider a small calibration experiment in which 2 to 4 different underwater video camera types were simultaneously deployed on each of 21 boat trips. Alongside the suite of deployed cameras was also an acoustic echosounder that recorded fish signals along surrounding transects. Our modeling framework is used to derive calibration formulae for translating camera-specific relative indices to the actual abundance scale in surveys that deploy a single camera. Cross-validation is conducted using mark-recapture abundance estimates (only available for 10 trips, all observed at a single habitat type) and through a separate simulation study. We also briefly discuss the case when surveys pair one camera with the echosounder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05767v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grace S. Chiu, Anton H. Westveld, Mark A. Albins, Kevin M. Boswell, John M. Hoenig, Sean P. Powers, S. Lynne Stokes, Allison L. White</dc:creator>
    </item>
    <item>
      <title>Ordinal regression for meta-analysis of test accuracy: a flexible approach for utilising all threshold data</title>
      <link>https://arxiv.org/abs/2505.23393</link>
      <description>arXiv:2505.23393v3 Announce Type: replace 
Abstract: Standard (network) meta-analysis methods for medical test accuracy evaluation analyse the data separately for each test threshold - wasting data - unless every study reports all thresholds. Previously proposed "multiple threshold" models either fail to provide threshold-specific summary estimates, or they assume that ordinal tests (e.g., questionnaires) are continuous.
  We propose two ordinal regression models - ordinal-bivariate and ordinal-HSROC - using an induced-Dirichlet framework for cutpoint parameters, enabling intuitive priors and both fixed-effects and random-effects cutpoints.
  We conducted a simulation study to evaluate the performance of our proposed models, with the simulated data being based on real anxiety screening data spanning 7, 22, and 64 ordinal categories, with 15%, 40% and 55% missing threshold data.
  Our proposed ordinal-bivariate model with fixed-effect cutpoints tended to obtain the best RMSE and bias, including when data was generated from a recently proposed continuous-assumption model. For instance - even with 64 categories - continuous models performed 10%-30% worse than our models, contradicting the common assumption that many categories justify treating ordinal tests as continuous. Furthermore, the standard stratified-bivariate approach showed worse performance, especially for tests with higher missingness.
  We implemented the models in the MetaOrdDTA R package (https://github.com/CerulloE1996/MetaOrdDTA), which provides features such as: Stan estimation, K-fold cross-validation for model selection, meta-regression, network meta-analysis extensions, and visualisation tools including sROC plots with credible/prediction regions.
  Overall, our simulation study suggests that our proposed models may obtain better accuracy estimates than previous approaches for ordinal tests, even when the number of ordinal categories is very high.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23393v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enzo Cerullo, Klaus Linde, Haley E. Jones, Efthymia Derezea, Tim Lucas, Nicola J. Cooper, Alex J. Sutton</dc:creator>
    </item>
    <item>
      <title>Estimation and inference in generalised linear models with constrained iteratively-reweighted least squares</title>
      <link>https://arxiv.org/abs/2509.18406</link>
      <description>arXiv:2509.18406v2 Announce Type: replace 
Abstract: We propose a simple and flexible framework for generalised linear models (GLM) with linear constraints on the coefficients. Linear constraints are useful in a wide range of applications, allowing the fitting of model with high-dimensional or highly collinear predictors, as well as encoding assumptions on the association between some or all predictors and the response. We propose the constrained iteratively-reweighted least squares (CIRLS) to fit the model, iterating quadratic programs to ensure the coefficient vector remains feasible according to the constraints. Inference for constrained coefficients can be obtained by simulating from a truncated multivariate normal distribution and computing empirical confidence intervals or variance-covariance matrix from the simulated coefficient vectors. We additionally discuss the complexity of a constrained GLM, proposing a measure of expected degrees of freedom which accounts for the stringency of constraints in the reduction of the model degrees of freedom. An extensive simulations study shows that constraining the coefficients introduces some bias to the estimation, but also decreases the estimator variance. This trade-off results in an improved estimator when constraints are chosen appropriately. The simulations also show that our proposed inference results in error in variance estimation and coverage. The proposed framework is illustrated on two case studies, showing its usefulness as well as some of its weaknesses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18406v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Masselot, Devon Nenon, Jacopo Vanoli, Zaid Chalabi, Antonio Gasparrini</dc:creator>
    </item>
    <item>
      <title>On Statistical Inference for High-Dimensional Binary Time Series</title>
      <link>https://arxiv.org/abs/2512.00338</link>
      <description>arXiv:2512.00338v2 Announce Type: replace 
Abstract: The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00338v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dehao Dai, Yunyi Zhang</dc:creator>
    </item>
    <item>
      <title>Autoregressive networks with dependent edges</title>
      <link>https://arxiv.org/abs/2404.15654</link>
      <description>arXiv:2404.15654v3 Announce Type: replace-cross 
Abstract: We propose an autoregressive framework for modelling dynamic networks with dependent edges. It encompasses models that accommodate, for example, transitivity, degree heterogenenity, and other stylized features often observed in real network data. By assuming the edges of networks at each time are independent conditionally on their lagged values, the models, which exhibit a close connection with temporal ERGMs, facilitate both simulation and the maximum likelihood estimation in a straightforward manner. Due to the possibly large number of parameters in the models, the natural MLEs may suffer from slow convergence rates. An improved estimator for each component parameter is proposed based on an iteration employing projection, which mitigates the impact of the other parameters (Chang et al., 2021; Chang et al., 2023). Leveraging a martingale difference structure, the asymptotic distribution of the improved estimator is derived without the assumption of stationarity. The limiting distribution is not normal in general, although it reduces to normal when the underlying process satisfies some mixing conditions. Illustration with a transitivity model was carried out in both simulation and a real network data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15654v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Qin Fang, Eric D. Kolaczyk, Peter W. MacDonald, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees</title>
      <link>https://arxiv.org/abs/2505.18659</link>
      <description>arXiv:2505.18659v2 Announce Type: replace-cross 
Abstract: Selecting artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference (PPI) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional methods using only real-world data. In this paper, we propose R-AutoEval+, a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency compared to conventional methods. The key innovation of R-AutoEval+ is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM, for prompt design in LLMs, and for test-time reasoning budget allocation in LLMs confirm the reliability and efficiency of R-AutoEval+.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18659v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sangwoo Park, Matteo Zecchin, Osvaldo Simeone</dc:creator>
    </item>
    <item>
      <title>kNNSampler: Stochastic Imputations for Recovering Missing Value Distributions</title>
      <link>https://arxiv.org/abs/2509.08366</link>
      <description>arXiv:2509.08366v2 Announce Type: replace-cross 
Abstract: We study a missing-value imputation method, termed kNNSampler, that imputes a given unit's missing response by randomly sampling from the observed responses of the $k$ most similar units to the given unit in terms of the observed covariates. This method can sample unknown missing values from their distributions, quantify the uncertainties of missing values, and be readily used for multiple imputation. Unlike popular kNNImputer, which estimates the conditional mean of a missing response given an observed covariate, kNNSampler is theoretically shown to estimate the conditional distribution of a missing response given an observed covariate. Experiments illustrate the performance of kNNSampler. The code for kNNSampler is made publicly available (https://github.com/SAP/knn-sampler).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08366v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parastoo Pashmchi, J\'er\^ome Benoit, Motonobu Kanagawa</dc:creator>
    </item>
  </channel>
</rss>
