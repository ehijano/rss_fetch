<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Jan 2025 02:39:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>MCMC Importance Sampling via Moreau-Yosida Envelopes</title>
      <link>https://arxiv.org/abs/2501.02228</link>
      <description>arXiv:2501.02228v1 Announce Type: new 
Abstract: The use of non-differentiable priors is standard in modern parsimonious Bayesian models. Lack of differentiability, however, precludes gradient-based Markov chain Monte Carlo (MCMC) methods for posterior sampling. Recently proposed proximal MCMC approaches can partially remedy this limitation. These approaches use gradients of a smooth approximation, constructed via Moreau-Yosida (MY) envelopes, to make proposals. In this work, we build an importance sampling paradigm by using the MY envelope as an importance distribution. Leveraging properties of the envelope, we establish asymptotic normality of the importance sampling estimator with an explicit expression for the asymptotic covariance matrix. Since the MY envelope density is smooth, it is amenable to gradient-based samplers. We provide sufficient conditions for geometric ergodicity of Metropolis-adjusted Langevin and Hamiltonian Monte Carlo algorithms, sampling from this importance distribution. A variety of numerical studies show that the proposed scheme can yield lower variance estimators compared to existing proximal MCMC alternatives, and is effective in both low and high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02228v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apratim Shukla, Dootika Vats, Eric C. Chi</dc:creator>
    </item>
    <item>
      <title>Various approaches to solving nonlinear equations</title>
      <link>https://arxiv.org/abs/2501.02390</link>
      <description>arXiv:2501.02390v1 Announce Type: new 
Abstract: Modelling real world systems frequently requires the solution of systems of nonlinear equations. A number of approaches have been suggested and developed for this computational problem. However, it is also possible to attempt solutions using more general nonlinear least squares or function minimization techniques. There are concerns, nonetheless, that we may fail to find solutions, or that the process will be inefficient. Examples are presented with R with the goal of providing guidance on the solution of nonlinear equations problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02390v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John C. Nash, Ravi Varadhan</dc:creator>
    </item>
    <item>
      <title>Tactics for Improving Least Squares Estimation</title>
      <link>https://arxiv.org/abs/2501.02475</link>
      <description>arXiv:2501.02475v1 Announce Type: new 
Abstract: This paper deals with tactics for fast computation in least squares regression in high dimensions. These tactics include: (a) the majorization-minimization (MM) principle, (b) smoothing by Moreau envelopes, and (c) the proximal distance principal for constrained estimation. In iteratively reweighted least squares, the MM principle can create a surrogate function that trades case weights for adjusted responses. Reduction to ordinary least squares then permits the reuse of the Gram matrix and its Cholesky decomposition across iterations. This tactic is pertinent to estimation in L2E regression and generalized linear models. For problems such as quantile regression, non-smooth terms of an objective function can be replaced by their Moreau envelope approximations and majorized by spherical quadratics. Finally, penalized regression with distance-to-set penalties also benefits from this perspective. Our numerical experiments validate the speed and utility of deweighting and Moreau envelope approximations. Julia software implementing these experiments is available on our web page.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02475v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Heng, Hua Zhou, Kenneth Lange</dc:creator>
    </item>
    <item>
      <title>Fast and light-weight energy statistics using the \textit{R} package \textsf{Rfast}</title>
      <link>https://arxiv.org/abs/2501.02849</link>
      <description>arXiv:2501.02849v2 Announce Type: new 
Abstract: Energy statistics ($\mathcal{\varepsilon}$-statistics) are functions of distances between statistical observations. This class of functions has enabled the development of non-linear statistical concepts, termed distance variance, distance covariance, distance correlation, etc. The computational burden associated with the $\mathcal{\varepsilon}$-statistical quantities is really heavy and when the data reside in the multivariate space, the task becomes even harder. We alleviate this cost by tremendously reducing the memory requirements and essentially making the computations faster. We show the process for the cases of (univariate and multivariate) distance variance, distance covariance, (partial) distance correlation, energy distance and hypothesis testing for the equality of univariate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02849v2</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris, Manos Papadakis</dc:creator>
    </item>
    <item>
      <title>MCBench: A Benchmark Suite for Monte Carlo Sampling Algorithms</title>
      <link>https://arxiv.org/abs/2501.03138</link>
      <description>arXiv:2501.03138v1 Announce Type: new 
Abstract: In this paper, we present MCBench, a benchmark suite designed to assess the quality of Monte Carlo (MC) samples. The benchmark suite enables quantitative comparisons of samples by applying different metrics, including basic statistical metrics as well as more complex measures, in particular the sliced Wasserstein distance and the maximum mean discrepancy. We apply these metrics to point clouds of both independent and identically distributed (IID) samples and correlated samples generated by MC techniques, such as Markov Chain Monte Carlo or Nested Sampling. Through repeated comparisons, we evaluate test statistics of the metrics, allowing to evaluate the quality of the MC sampling algorithms.
  Our benchmark suite offers a variety of target functions with different complexities and dimensionalities, providing a versatile platform for testing the capabilities of sampling algorithms. Implemented as a Julia package, MCBench enables users to easily select test cases and metrics from the provided collections, which can be extended as needed. Users can run external sampling algorithms of their choice on these test functions and input the resulting samples to obtain detailed metrics that quantify the quality of their samples compared to the IID samples generated by our package. This approach yields clear, quantitative measures of sampling quality and allows for informed decisions about the effectiveness of different sampling methods.
  By offering such a standardized method for evaluating MC sampling quality, our benchmark suite provides researchers and practitioners from many scientific fields, such as the natural sciences, engineering, or the social sciences with a valuable tool for developing, validating and refining sampling algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03138v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyu Ding, Cornelius Grunwald, Katja Ickstadt, Kevin Kr\"oninger, Salvatore La Cagnina</dc:creator>
    </item>
    <item>
      <title>Majorization-Minimization Dual Stagewise Algorithm for Generalized Lasso</title>
      <link>https://arxiv.org/abs/2501.02197</link>
      <description>arXiv:2501.02197v1 Announce Type: cross 
Abstract: The generalized lasso is a natural generalization of the celebrated lasso approach to handle structural regularization problems. Many important methods and applications fall into this framework, including fused lasso, clustered lasso, and constrained lasso. To elevate its effectiveness in large-scale problems, extensive research has been conducted on the computational strategies of generalized lasso. However, to our knowledge, most studies are under the linear setup, with limited advances in non-Gaussian and non-linear models. We propose a majorization-minimization dual stagewise (MM-DUST) algorithm to efficiently trace out the full solution paths of the generalized lasso problem. The majorization technique is incorporated to handle different convex loss functions through their quadratic majorizers. Utilizing the connection between primal and dual problems and the idea of ``slow-brewing'' from stagewise learning, the minimization step is carried out in the dual space through a sequence of simple coordinate-wise updates on the dual coefficients with a small step size. Consequently, selecting an appropriate step size enables a trade-off between statistical accuracy and computational efficiency. We analyze the computational complexity of MM-DUST and establish the uniform convergence of the approximated solution paths. Extensive simulation studies and applications with regularized logistic regression and Cox model demonstrate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02197v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianmin Chen, Kun Chen</dc:creator>
    </item>
    <item>
      <title>Optimization problems constrained by parameter sums</title>
      <link>https://arxiv.org/abs/2501.02388</link>
      <description>arXiv:2501.02388v1 Announce Type: cross 
Abstract: This article presents a discussion of optimization problems where the objective function f(x) has parameters that are constrained by some scaling, so that q(x) = constant, where this function q() involves a sum of the parameters, their squares, or similar simple function. Our focus is on ways to use standardized optimization programs to solve such problems rather than specialized codes. Examples are presented with R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02388v1</guid>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John C. Nash, Ravi Varadhan</dc:creator>
    </item>
    <item>
      <title>Unsupervised Training of Convex Regularizers using Maximum Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2404.05445</link>
      <description>arXiv:2404.05445v3 Announce Type: replace-cross 
Abstract: Imaging is a standard example of an inverse problem, where the task of reconstructing a ground truth from a noisy measurement is ill-posed. Recent state-of-the-art approaches for imaging use deep learning, spearheaded by unrolled and end-to-end models and trained on various image datasets. However, many such methods require the availability of ground truth data, which may be unavailable or expensive, leading to a fundamental barrier that can not be bypassed by choice of architecture. Unsupervised learning presents an alternative paradigm that bypasses this requirement, as they can be learned directly on noisy data and do not require any ground truths. A principled Bayesian approach to unsupervised learning is to maximize the marginal likelihood with respect to the given noisy measurements, which is intrinsically linked to classical variational regularization. We propose an unsupervised approach using maximum marginal likelihood estimation to train a convex neural network-based image regularization term directly on noisy measurements, improving upon previous work in both model expressiveness and dataset size. Experiments demonstrate that the proposed method produces priors that are near competitive when compared to the analogous supervised training method for various image corruption operators, maintaining significantly better generalization properties when compared to end-to-end methods. Moreover, we provide a detailed theoretical analysis of the convergence properties of our proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05445v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Ziruo Cai, Marcelo Pereyra, Subhadip Mukherjee, Junqi Tang, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
    <item>
      <title>From Poisson Observations to Fitted Negative Binomial Distribution</title>
      <link>https://arxiv.org/abs/2404.07457</link>
      <description>arXiv:2404.07457v2 Announce Type: replace-cross 
Abstract: Negative binomial distribution has been widely used as a more flexible model than Poisson distribution for count data. When the observations come from a Poisson distribution, it is often challenging to rule out the possibility that the data come from a negative binomial distribution with extreme parameter values. To address this phenomenon, we develop a more efficient and accurate algorithm for finding the maximum likelihood estimate of negative binomial parameters, which outperforms the state-of-the-art programs for the same purpose. We also theoretically justify that the negative binomial distribution with parameters estimated from Poisson data converges to the true Poisson distribution with probability one. As a solution to this phenomenon, we extend the negative binomial distributions with a new parameterization, which include Poisson distributions as a special class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07457v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Yang, Niloufar Dousti Mousavi, Zhou Yu, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Error estimation for quasi-Monte Carlo</title>
      <link>https://arxiv.org/abs/2501.00150</link>
      <description>arXiv:2501.00150v2 Announce Type: replace-cross 
Abstract: Quasi-Monte Carlo sampling can attain far better accuracy than plain Monte Carlo sampling. However, with plain Monte Carlo sampling it is much easier to estimate the attained accuracy. This article describes methods old and new to quantify the error in quasi-Monte Carlo estimates. An important challenge in this setting is that the goal of getting accuracy conflicts with that of estimating the attained accuracy. A related challenge is that rigorous uncertainty quantifications can be extremely conservative. A recent surprise is that some RQMC estimates have nearly symmetric distributions and that has the potential to allow confidence intervals that do not require either a central limit theorem or a consistent variance estimate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00150v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Art B. Owen</dc:creator>
    </item>
  </channel>
</rss>
