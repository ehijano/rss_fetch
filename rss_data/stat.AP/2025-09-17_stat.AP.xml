<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 04:02:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Correcting for partial verification bias in diagnostic accuracy studies: A tutorial using R</title>
      <link>https://arxiv.org/abs/2509.12217</link>
      <description>arXiv:2509.12217v1 Announce Type: new 
Abstract: Diagnostic tests play a crucial role in medical care. Thus any new diagnostic tests must undergo a thorough evaluation. New diagnostic tests are evaluated in comparison with the respective gold standard tests. The performance of binary diagnostic tests is quantified by accuracy measures, with sensitivity and specificity being the most important measures. In any diagnostic accuracy study, the estimates of these measures are often biased owing to selective verification of the patients, which is referred to as partial verification bias. Several methods for correcting partial verification bias are available depending on the scale of the index test, target outcome and missing data mechanism. However, these are not easily accessible to the researchers due to the complexity of the methods. This article aims to provide a brief overview of the methods available to correct for partial verification bias involving a binary diagnostic test and provide a practical tutorial on how to implement the methods using the statistical programming language R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12217v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.9311</arxiv:DOI>
      <arxiv:journal_reference>Statistics in Medicine 41 (2022) 1709-1727</arxiv:journal_reference>
      <dc:creator>Wan Nor Arifin, Umi Kalsom Yusof</dc:creator>
    </item>
    <item>
      <title>Transporting Predictions via Double Machine Learning: Predicting Partially Unobserved Students' Outcomes</title>
      <link>https://arxiv.org/abs/2509.12533</link>
      <description>arXiv:2509.12533v1 Announce Type: new 
Abstract: Educational policymakers often lack data on student outcomes in regions where standardized tests were not administered. Machine learning techniques can be used to predict unobserved outcomes in target populations by training models on data from a source population. However, differences between the source and target populations, particularly in covariate distributions, can reduce the transportability of these models, potentially reducing predictive accuracy and introducing bias. We propose using double machine learning for a covariate-shift weighted model. First, we estimate the overlap score-namely, the probability that an observation belongs to the source dataset given its covariates. Second, balancing weights, defined as the density ratio of target-to-source membership probabilities, are used to reweight the individual observations' contribution to the loss or likelihood function in the target outcome prediction model. This approach downweights source observations that are less similar to the target population, allowing predictions to rely more heavily on observations with greater overlap. As a result, predictions become more generalizable under covariate shift. We illustrate this framework in the context of uncertain data on students' standardized financial literacy scores (FLS). Using Bayesian Additive Regression Trees (BART), we predict missing FLS. We find minimal differences in predictive performance between the weighted and unweighted models, suggesting limited covariate shift in our empirical setting. Nonetheless, the proposed approach provides a principled framework for addressing covariate shift and is broadly applicable to predictive modeling in the social and health sciences, where differences between source and target populations are common.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12533v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Falco J. Bargagli-Stoffi, Emma Landry, Kevin P. Josey, Kenneth De Beckker, Joana E. Maldonado, Kristof De Witte</dc:creator>
    </item>
    <item>
      <title>A Doubly-Flexible Model Based on Generalized Gamma Frailty for Two-component Load-sharing Systems</title>
      <link>https://arxiv.org/abs/2509.12686</link>
      <description>arXiv:2509.12686v1 Announce Type: new 
Abstract: For two-component load-sharing systems, a doubly-flexible model is developed where the generalized Fruend bivariate (GFB) distribution is used for the baseline of the component lifetimes, and the generalized gamma (GG) family of distributions is used to incorporate a shared frailty that captures dependence between the component lifetimes. The proposed model structure results in a very general two-way class of models that enables a researcher to choose an appropriate model for a given two-component load-sharing data within the respective families of distributions. The GFB-GG model structure provides better fit to two-component load-sharing systems compared to existing models. Fitting methods for the proposed model, based on direct optimization and an expectation maximization (EM) type algorithm, are discussed. Through simulations, effectiveness of the fitting methods is demonstrated. Also, through simulations, it is shown that the proposed model serves the intended purpose of model choice for a given two-component load-sharing data. A simulation case, and analysis of a real dataset are presented to illustrate the strength of the proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12686v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shilpi Biswas, Ayon Ganguly, Debanjan Mitra</dc:creator>
    </item>
    <item>
      <title>Shape-to-Scale InSAR Adaptive Filtering and Phase Linking under Complex Elliptical Models</title>
      <link>https://arxiv.org/abs/2509.12700</link>
      <description>arXiv:2509.12700v1 Announce Type: new 
Abstract: Distributed scatterers in InSAR (DS-InSAR) processing are essential for retrieving surface deformation in areas lacking strong point targets. Conventional workflows typically involve selecting statistically homogeneous pixels based on amplitude similarity, followed by phase estimation under the complex circular Gaussian model. However, amplitude statistics primarily reflect the backscattering strength of surface targets and may not sufficiently capture differences in decorrelation behavior. For example, when distinct scatterers exhibit similar backscatter strength but differ in coherence, amplitude-based selection methods may fail to differentiate them. Moreover, CCG-based phase estimators may lack robustness and suffer performance degradation under non-Rayleigh amplitude fluctuations.
  Centered around scale-invariant second-order statistics, we propose ``Shape-to-Scale,'' a novel DS-InSAR framework. We first identify pixels that share a common angular scattering structure (``shape statistically homogeneous pixels'') with an angular consistency adaptive filter: a parametric selection method based on the complex angular central Gaussian distribution. Then, we introduce a complex generalized Gaussian-based phase estimation approach that is robust to potential non-Rayleigh scattering.
  Experiments on both simulated and SAR datasets show that the proposed framework improves coherence structure clustering and enhances phase estimation robustness. This work provides a unified and physically interpretable strategy for DS-InSAR processing and offers new insights for high-resolution SAR time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12700v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyi Yao, Alejandro C. Frery, Timo Balz</dc:creator>
    </item>
    <item>
      <title>PDE-Based Bayesian Hierarchical Modeling for Event Spread, with Application to COVID-19 Infection</title>
      <link>https://arxiv.org/abs/2509.13174</link>
      <description>arXiv:2509.13174v1 Announce Type: new 
Abstract: We extended the Wikle's Bayesian hierarchical model based on a diffusion-reaction equation [Wikle, 2003] to investigate the COVID-19 spatio-temporal spread events across the USA from Mar 2020 to Feb 2022. Our model incorporated an advection term to account for the intra-state spread trend. We applied a Markov chain Monte Carlo (MCMC) method to obtain samples from the posterior distribution of the parameters. We implemented the approach via the collection of the COVID-19 infections across the states overtime from the New York Times. Our analysis shows that our approach can be robust to model misspecification to a certain extent and outperforms a few other approaches in the simulation settings. Our analysis results confirm that the diffusion rate is heterogeneous across the USA, and both the growth rate and the advection velocity are time-varying.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13174v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengqi Cen, Xuejing Meng, X. Joan Hu, Juxin Liu, Jianhong Wu</dc:creator>
    </item>
    <item>
      <title>Inferring Soil Drydown Behaviour with Adaptive Bayesian Online Changepoint Analysis</title>
      <link>https://arxiv.org/abs/2509.13293</link>
      <description>arXiv:2509.13293v1 Announce Type: new 
Abstract: Continuous soil-moisture measurements provide a direct lens on subsurface hydrological processes, notably the post-rainfall "drydown" phase. Because these records consist of distinct, segment-specific behaviours whose forms and scales vary over time, realistic inference demands a model that captures piecewise dynamics while accommodating parameters that are unknown a priori. Building on Bayesian Online Changepoint Detection (BOCPD), we introduce two complementary extensions: a particle-filter variant that substitutes exact marginalisation with sequential Monte Carlo to enable real-time inference when critical parameters cannot be integrated out analytically, and an online-gradient variant that embeds stochastic gradient updates within BOCPD to learn application-relevant parameters on the fly without prohibitive computational cost. After validating both algorithms on synthetic data that replicate the temporal structure of field observations-detailing hyperparameter choices, priors, and cost-saving strategies-we apply them to soil-moisture series from experimental sites in Austria and the United States, quantifying site-specific drydown rates and demonstrating the advantages of our adaptive framework over static models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13293v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengyi Gong, Christopher Nemeth, Rebecca Killick, Peter Strauss, John Quinton</dc:creator>
    </item>
    <item>
      <title>Significant inference and confidence sets for graphical models</title>
      <link>https://arxiv.org/abs/2509.12292</link>
      <description>arXiv:2509.12292v1 Announce Type: cross 
Abstract: The problem of identifying statistically significant inferences about the structure of the graphical model is considered, along with the related task of constructing a confidence set for a graphical model. It has been proven that the procedure for constructing such set is equivalent to the procedure for simultaneous testing of hypotheses and alternatives regarding the composition of the graphical model. Some variants of the simultaneous testing of hypotheses and alternatives are discussed. It is shown that under the condition of free combination of hypotheses and alternatives, a simple generalization of the closure method leads to singlestep procedures for simultaneous testing of hypotheses and alternatives. The structure of the confidence set for the graphical model is analyzed, demonstrating how the confidence set leads to a separation of inferences about the graphical model into statistically significant and insignificant categories, or into an area of uncertainty. General results are detailed by analyzing confidence sets for undirected Gaussian graphical model selection. Examples are provided that illustrate the separation of inferences about the composition of undirected Gaussian graphical models into significant results and areas of uncertainty, and a comparison is made with known results obtained using the SINful approach to undirected Gaussian graphical model selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12292v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. A. Koldanov, A. P. Koldanov</dc:creator>
    </item>
    <item>
      <title>Surrogate Representation Inference for Noisy Text and Image Annotations</title>
      <link>https://arxiv.org/abs/2509.12416</link>
      <description>arXiv:2509.12416v1 Announce Type: cross 
Abstract: As researchers increasingly rely on machine learning models and LLMs to annotate unstructured data, such as texts or images, various approaches have been proposed to correct bias in downstream statistical analysis. However, existing methods tend to yield large standard errors and require some error-free human annotation. In this paper, I introduce Surrogate Representation Inference (SRI), which assumes that unstructured data fully mediate the relationship between human annotations and structured variables. The assumption is guaranteed by design provided that human coders rely only on unstructured data for annotation. Under this setting, I propose a neural network architecture that learns a low-dimensional representation of unstructured data such that the surrogate assumption remains to be satisfied. When multiple human annotations are available, SRI can further correct non-differential measurement errors that may exist in human annotations. Focusing on text-as-outcome settings, I formally establish the identification conditions and semiparametric efficient estimation strategies that enable learning and leveraging such a low-dimensional representation. Simulation studies and a real-world application demonstrate that SRI reduces standard errors by over 50% when machine learning prediction accuracy is moderate and provides valid inference even when human annotations contain non-differential measurement errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12416v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kentaro Nakamura</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal DeepKriging in PyTorch: A Supplementary Application to Precipitation Data for Interpolation and Probabilistic Forecasting</title>
      <link>https://arxiv.org/abs/2509.12708</link>
      <description>arXiv:2509.12708v1 Announce Type: cross 
Abstract: A detailed analysis of precipitation data over Europe is presented, with a focus on interpolation and forecasting applications. A Spatio-temporal DeepKriging (STDK) framework has been implemented using the PyTorch platform to achieve these objectives. The proposed model is capable of handling spatio-temporal irregularities while generating high-resolution interpolations and multi-step forecasts. Reproducible code modules have been developed as standalone PyTorch implementations for the interpolation\footnote[2]{Interpolation - https://github.com/pratiknag/Spatio-temporalDeepKriging-Pytorch.git} and forecasting\footnote[3]{Forecasting - https://github.com/pratiknag/pytorch-convlstm.git}, facilitating broader application to similar climate datasets. The effectiveness of this approach is demonstrated through extensive evaluation on daily precipitation measurements, highlighting predictive performance and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12708v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pratik Nag</dc:creator>
    </item>
    <item>
      <title>Multivariate Low-Rank State-Space Model with SPDE Approach for High-Dimensional Data</title>
      <link>https://arxiv.org/abs/2509.12825</link>
      <description>arXiv:2509.12825v1 Announce Type: cross 
Abstract: This paper proposes a novel low-rank approximation to the multivariate State-Space Model. The Stochastic Partial Differential Equation (SPDE) approach is applied component-wise to the independent-in-time Mat\'ern Gaussian innovation term in the latent equation, assuming component independence. This results in a sparse representation of the latent process on a finite element mesh, allowing for scalable inference through sparse matrix operations. Dependencies among observed components are introduced through a matrix of weights applied to the latent process. Model parameters are estimated using the Expectation-Maximisation algorithm, which features closed-form updates for most parameters and efficient numerical routines for the remaining parameters. We prove theoretical results regarding the accuracy and convergence of the SPDE-based approximation under fixed-domain asymptotics. Simulation studies show our theoretical results. We include an empirical application on air quality to demonstrate the practical usefulness of the proposed model, which maintains computational efficiency in high-dimensional settings. In this application, we reduce computation time by about 93%, with only a 15% increase in the validation error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12825v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacopo Rodeschini, Lorenzo Tedesco, Francesco Finazzi, Philipp Otto, Alessandro Fass\`o</dc:creator>
    </item>
    <item>
      <title>Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder</title>
      <link>https://arxiv.org/abs/2509.12991</link>
      <description>arXiv:2509.12991v1 Announce Type: cross 
Abstract: ECG foundation models are increasingly popular due to their adaptability across various tasks. However, their clinical applicability is often limited by performance gaps compared to task-specific models, even after pre-training on large ECG datasets and fine-tuning on target data. This limitation is likely due to the lack of an effective post-training strategy. In this paper, we propose a simple yet effective post-training approach to enhance ECGFounder, a state-of-the-art ECG foundation model pre-trained on over 7 million ECG recordings. Experiments on the PTB-XL benchmark show that our approach improves the baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% in macro AUPRC. Additionally, our method outperforms several recent state-of-the-art approaches, including task-specific and advanced architectures. Further evaluation reveals that our method is more stable and sample-efficient compared to the baseline, achieving a 9.1% improvement in macro AUROC and a 34.9% improvement in macro AUPRC using just 10% of the training data. Ablation studies identify key components, such as stochastic depth and preview linear probing, that contribute to the enhanced performance. These findings underscore the potential of post-training strategies to improve ECG foundation models, and we hope this work will contribute to the continued development of foundation models in the ECG domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12991v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ya Zhou, Yujie Yang, Xiaohan Fan, Wei Zhao</dc:creator>
    </item>
    <item>
      <title>A hidden benefit of incomplete round-robin tournaments: Encouraging offensive play</title>
      <link>https://arxiv.org/abs/2509.13141</link>
      <description>arXiv:2509.13141v1 Announce Type: cross 
Abstract: This paper aims to explore the impact of tournament design on the incentives of the contestants. We develop a simulation framework to quantify the potential gain and loss from attacking based on changes in the probability of reaching the critical ranking thresholds. The model is applied to investigate the 2024/25 UEFA Champions League reform. The novel incomplete round-robin league phase is found to create more powerful incentives for offensive play than the previous group stage, with an average increase of 119\% (58\%) regarding the first (second) prize. Our study provides the first demonstration that the tournament format itself can strongly influence team behaviour in sports.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13141v1</guid>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Robust a posteriori estimation of probit-lognormal seismic fragility curves via sequential design of experiments and constrained reference prior</title>
      <link>https://arxiv.org/abs/2503.07343</link>
      <description>arXiv:2503.07343v2 Announce Type: replace 
Abstract: A seismic fragility curve expresses the probability of failure of a structure conditional to an intensity measure (IM) derived from seismic signals. When only limited data is available, the practitioner often refers to the probit-lognormal model coupled with maximum likelihood estimation (MLE) to obtain estimates of these curves. This means that only a binary indicator of the state (BIS) of the structure is known, namely a failure or non-failure state indicator, when it is subjected to a seismic signal with an intensity measure IM. In this context, the objective of this work is to propose a method for optimally estimating such curves by obtaining the most precise estimate possible with the minimum of data. The novelty of our work is twofold. First, we present and show how to mitigate the likelihood degeneracy problem which is ubiquitous with small data sets and hampers frequentist approaches such as MLE. Second, we propose a novel strategy for sequential design of experiments (DoE) that selects seismic signals from a large database of synthetic or real signals via their IM values, to be applied to structures to evaluate the corresponding BISs. This strategy relies on a criterion based on information theory in a Bayesian framework. It therefore aims to sequentially designate the IM value such that the pair (IM, BIS) has on average, with respect to the BIS of the structure, the greatest impact on the posterior distribution of the fragility curve. The methodology is applied to a case study from the nuclear industry. The results demonstrate its ability to efficiently and robustly estimate the fragility curve, and to avoid degeneracy even with a limited amount of data. Furthermore, we demonstrate that the estimates quickly reach the model bias induced by the probit-lognormal modeling. Eventually, two criteria are suggested to help the user stop the DoE algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07343v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Van Biesbroeck, Cl\'ement Gauchy, Cyril Feau, Josselin Garnier</dc:creator>
    </item>
    <item>
      <title>Causal Feedback Discovery using Convergence Cross Mapping on Sea Ice Data</title>
      <link>https://arxiv.org/abs/2505.09001</link>
      <description>arXiv:2505.09001v3 Announce Type: replace 
Abstract: Identifying causal relationships in climate systems remains challenging due to nonlinear, coupled dynamics that limit the effectiveness of linear and stochastic causal discovery approaches. This study benchmarks Convergence Cross Mapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both synthetic datasets with ground truth causal links and 41 years of Arctic climate data (1979--2021). Unlike stochastic models that rely on autoregressive residual dependence, CCM leverages Takens' state-space reconstruction and delay-embedding to reconstruct attractor manifolds from time series. Cross mapping between reconstructed manifolds exploits deterministic signatures of causation, enabling the detection of weak and bidirectional causal links that linear models fail to resolve. Results demonstrate that CCM achieves higher specificity and fewer false positives on synthetic benchmarks, while maintaining robustness under observational noise and limited sample lengths. On Arctic data, CCM reveals significant causal interactions between sea ice extent and atmospheric variables like specific humidity, longwave radiation, and surface temperature with a $p$-value of $0.009$, supporting ice-albedo feedbacks and moisture-radiation couplings central to Arctic amplification. In contrast, stochastic approaches miss these nonlinear dependencies or infer spurious causal relations. This work establishes CCM as a robust causal inference tool for nonlinear climate dynamics and provides the first systematic benchmarking framework for method selection in climate research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09001v3</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francis Nji, Seraj Al Mahmud Mostafa, Jianwu Wang</dc:creator>
    </item>
    <item>
      <title>Direction Preferring Confidence Intervals</title>
      <link>https://arxiv.org/abs/2404.00319</link>
      <description>arXiv:2404.00319v2 Announce Type: replace-cross 
Abstract: Confidence intervals (CIs) are instrumental in statistical analysis, providing a range estimate of the parameters. In modern statistics, selective inference is common, where only certain parameters are highlighted. However, this selective approach can bias the inference, leading some to advocate for the use of CIs over p-values. To increase the flexibility of confidence intervals, we introduce direction-preferring CIs, enabling analysts to focus on parameters trending in a particular direction. We present these types of CIs in two settings: First, when there is no selection of parameters; and second, for situations involving parameter selection, where we offer a conditional version of the direction-preferring CIs. Both of these methods build upon the foundations of Modified Pratt CIs, which rely on non-equivariant acceptance regions to achieve longer intervals in exchange for improved sign exclusions. We show that for selected parameters out of m &gt; 1 initial parameters of interest, CIs aimed at controlling the false coverage rate, have higher power to determine the sign compared to conditional CIs. We also show that conditional confidence intervals control the marginal false coverage rate (mFCR) under any dependency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00319v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tzviel Frostig, Yoav Benjamini, Ruth Heller</dc:creator>
    </item>
    <item>
      <title>Clustering methods for Categorical Time Series and Sequences : A scoping review</title>
      <link>https://arxiv.org/abs/2509.07885</link>
      <description>arXiv:2509.07885v2 Announce Type: replace-cross 
Abstract: Objective: To provide an overview of clustering methods for categorical time series (CTS), a data structure commonly found in epidemiology, sociology, biology, and marketing, and to support method selection in regards to data characteristics.
  Methods: We searched PubMed, Web of Science, and Google Scholar, from inception up to November 2024 to identify articles that propose and evaluate clustering techniques for CTS. Methods were classified according to three major families -- distance-based, feature-based, and model-based -- and assessed on their ability to handle data challenges such as variable sequence length, multivariate data, continuous time, missing data, time-invariant covariates, and large data volumes.
  Results: Out of 14607 studies, we included 124 articles describing 129 methods, spanning domains such as artificial intelligence, social sciences, and epidemiology. Distance-based methods, particularly those using Optimal Matching, were most prevalent, with 56 methods. We identified 28 model-based methods, which demonstrated superior flexibility for handling complex data structures such as multivariate data, continuous time and time-invariant covariates. We also recorded 45 feature-based approaches, which were on average more scalable but less flexible. A searchable Web application was developed to facilitate method selection based on dataset characteristics ( https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/ )
  Discussion: While distance-based methods dominate, model-based approaches offer the richest modeling potential but are less scalable. Feature-based methods favor performance over flexibility, with limited support for complex data structures.
  Conclusion: This review highlights methodological diversity and gaps in CTS clustering. The proposed typology aims to guide researchers in selecting methods for their specific use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07885v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ottavio Khalifa, Viet-Thi Tran, Alan Balendran, Fran\c{c}ois Petit</dc:creator>
    </item>
  </channel>
</rss>
