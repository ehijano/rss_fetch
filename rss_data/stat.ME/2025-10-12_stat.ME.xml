<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Oct 2025 04:01:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Design-based Solution for Causal Inference with Text: Can a Language Model Be Too Large?</title>
      <link>https://arxiv.org/abs/2510.08758</link>
      <description>arXiv:2510.08758v1 Announce Type: new 
Abstract: Many social science questions ask how linguistic properties causally affect an audience's attitudes and behaviors. Because text properties are often interlinked (e.g., angry reviews use profane language), we must control for possible latent confounding to isolate causal effects. Recent literature proposes adapting large language models (LLMs) to learn latent representations of text that successfully predict both treatment and the outcome. However, because the treatment is a component of the text, these deep learning methods risk learning representations that actually encode the treatment itself, inducing overlap bias. Rather than depending on post-hoc adjustments, we introduce a new experimental design that handles latent confounding, avoids the overlap issue, and unbiasedly estimates treatment effects. We apply this design in an experiment evaluating the persuasiveness of expressing humility in political communication. Methodologically, we demonstrate that LLM-based methods perform worse than even simple bag-of-words models using our real text and outcomes from our experiment. Substantively, we isolate the causal effect of expressing humility on the perceived persuasiveness of political statements, offering new insights on communication effects for social media platforms, policy makers, and social scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08758v1</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Graham Tierney, Srikar Katta, Christopher Bail, Sunshine Hillygus, Alexander Volfovsky</dc:creator>
    </item>
    <item>
      <title>Repulsive Mixture Model with Projection Determinantal Point Process</title>
      <link>https://arxiv.org/abs/2510.08838</link>
      <description>arXiv:2510.08838v1 Announce Type: new 
Abstract: In many scientific domains, clustering aims to reveal interpretable latent structure that reflects relevant subpopulations or processes. Widely used Bayesian mixture models for model-based clustering often produce overlapping or redundant components because priors on cluster locations are specified independently, hindering interpretability. To mitigate this, repulsive priors have been proposed to encourage well-separated components, yet existing approaches face both computational and theoretical challenges. We introduce a fully tractable Bayesian repulsive mixture model by assigning a projection Determinantal Point Process (DPP) prior to the component locations. Projection DPPs induce strong repulsion and allow exact sampling, enabling parsimonious and interpretable posterior clustering. Leveraging their analytical tractability, we derive closed-form posterior and predictive distributions. These results, in turn, enable two efficient inference algorithms: a conditional Gibbs sampler and the first fully implementable marginal sampler for DPP-based mixtures. We also provide strong frequentist guarantees, including posterior consistency for density estimation, elimination of redundant components, and contraction of the mixing measure. Simulation studies confirm superior mixing and clustering performance compared to alternatives in misspecified settings. Finally, we demonstrate the utility of our method on event-related potential functional data, where it uncovers interpretable neuro-cognitive subgroups. Our results support the projection DPP mixtures as a theoretically sound and practically effective solution for Bayesian clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08838v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyi Song, Federico Camerlenghi, Weining Shen, Michele Guindani, Mario Beraha</dc:creator>
    </item>
    <item>
      <title>Uncovering All Highly Credible Binary Treatment Hierarchy Questions in Network Meta-Analysis</title>
      <link>https://arxiv.org/abs/2510.08853</link>
      <description>arXiv:2510.08853v1 Announce Type: new 
Abstract: In recent years, there has been growing research interest in addressing treatment hierarchy questions within network meta-analysis (NMA). In NMAs involving many treatments, the number of possible hierarchy questions becomes prohibitively large. To manage this complexity, previous work has recommended pre-selecting specific hierarchy questions of interest (e.g., ``among options A, B, C, D, E, do treatments A and B have the two best effects in terms of improving outcome X?") and calculating the empirical probabilities of the answers being true given the data. In contrast, we propose an efficient and scalable algorithmic approach that eliminates the need for pre-specification by systematically generating a comprehensive catalog of highly credible treatment hierarchy questions, specifically, those with empirical probabilities exceeding a chosen threshold (e.g., 95%). This enables decision-makers to extract all meaningful insights supported by the data. An additional algorithm trims redundant insights from the output to facilitate interpretation. We define and address six broad types of binary hierarchy questions (i.e., those with true/false answers), covering standard hierarchy questions answered using existing ranking metrics - pairwise comparisons and (cumulative) ranking probabilities - as well as many other complex hierarchy questions. We have implemented our methods in an R package and illustrate their application using real NMA datasets on diabetes and depression interventions. Beyond NMA, our approach is relevant to any decision problem concerning three or more treatment options.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08853v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caitlin H. Daly, Chloe Tan, Audrey B\'eliveau</dc:creator>
    </item>
    <item>
      <title>Multidimensional Poverty Mapping for Small Areas</title>
      <link>https://arxiv.org/abs/2510.08898</link>
      <description>arXiv:2510.08898v1 Announce Type: new 
Abstract: Many countries measure poverty based only on income or consumption. However, there is a growing awareness of measuring poverty through multiple dimensions that captures a more reasonable status of poverty. Estimating poverty measure(s) for small geographical areas, commonly referred to as poverty mapping, is challenging due to small or no sample for the small areas. While there is a huge literature available on unidimensional poverty mapping, only a limited effort has been made to address special challenges that arise only in the multidimensional poverty mapping. For example, in multidimensional poverty mapping, a new problem arises involving estimation of relative contributions of different dimensions to overall poverty for small areas. This problem has been grossly ignored in the small area estimation (SAE) literature. We address this issue using a multivariate hierarchical model implemented via a Bayesian method. Moreover, we demonstrate how a multidimensional poverty composite measure can be estimated for small areas. In this paper, we demonstrate our proposed methodology using a survey data specially designed by one of us for multidimensional poverty mapping. This paper adds a new direction to poverty mapping literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08898v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soumojit Das, Dilshanie Deepawansa, Partha Lahiri</dc:creator>
    </item>
    <item>
      <title>Robust and Efficient Semiparametric Inference for the Stepped Wedge Design</title>
      <link>https://arxiv.org/abs/2510.08972</link>
      <description>arXiv:2510.08972v1 Announce Type: new 
Abstract: Stepped wedge designs (SWDs) are increasingly used to evaluate longitudinal cluster-level interventions but pose substantial challenges for valid inference. Because crossover times are randomized, intervention effects are intrinsically confounded with secular time trends, while heterogeneity across clusters, complex correlation structures, baseline covariate imbalances, and small numbers of clusters further complicate inference. We propose a unified semiparametric framework for estimating possibly time-varying intervention effects in SWDs. Under a semiparametric model on treatment contrast, we develop a nonstandard semiparametric efficiency theory that accommodates correlated observations within clusters, varying cluster-period sizes, and weakly dependent treatment assignments. The resulting estimator is consistent and asymptotically normal even under misspecified covariance structure and control cluster-period means, and is efficient when both are correctly specified. To enable inference with few clusters, we exploit the permutation structure of treatment assignment to propose a standard error estimator that reflects finite-sample variability, with a leave-one-out correction to reduce plug-in bias. The framework also allows incorporation of effect modification and adjustment for imbalanced precision variables through design-based adjustment or double adjustment that additionally incorporates an outcome-based component. Simulations and application to a public health trial demonstrate the robustness and efficiency of the proposed method relative to standard approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08972v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fan Xia, K. C. Gary Chan, Emily Voldal, Avi Kenny, Patrick J. Heagerty, James P. Hughes</dc:creator>
    </item>
    <item>
      <title>Revisiting Madigan and Mosurski: Collapsibility via Minimal Separators</title>
      <link>https://arxiv.org/abs/2510.09024</link>
      <description>arXiv:2510.09024v1 Announce Type: new 
Abstract: Collapsibility provides a principled approach for dimension reduction in contingency tables and graphical models. Madigan and Mosurski (1990) pioneered the study of minimal collapsible sets in decomposable models, but existing algorithms for general graphs remain computationally demanding. We show that a model is collapsible onto a target set precisely when that set contains all minimal separators between its non-adjacent vertices. This insight motivates the Close Minimal Separator Absorption (CMSA) algorithm, which constructs minimal collapsible sets using only local separator searches at very low costs. Simulations confirm substantial efficiency gains, making collapsibility analysis practical in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09024v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pei Heng, Yi Sun, Shiyuan He, Jianhua Guo</dc:creator>
    </item>
    <item>
      <title>The bixplot: A variation on the boxplot suited for bimodal data</title>
      <link>https://arxiv.org/abs/2510.09276</link>
      <description>arXiv:2510.09276v1 Announce Type: new 
Abstract: Boxplots and related visualization methods are widely used exploratory tools for taking a first look at collections of univariate variables. In this note an extension is provided that is specifically designed to detect and display bimodality and multimodality when the data warrant it. For this purpose a univariate clustering method is constructed that ensures contiguous clusters, meaning that no cluster has members inside another cluster, and such that each cluster contains at least a given number of unique members. The resulting bixplot display facilitates the identification and interpretation of potentially meaningful subgroups underlying the data. The bixplot also displays the individual data values, which can draw attention to isolated points. Implementations of the bixplot are available in both Python and R, and their many options are illustrated on several real datasets. For instance, an external variable can be visualized by color gradations inside the display.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09276v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camille M. Montalcini, Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Reliability Sensitivity with Response Gradient</title>
      <link>https://arxiv.org/abs/2510.09315</link>
      <description>arXiv:2510.09315v1 Announce Type: new 
Abstract: Engineering risk is concerned with the likelihood of failure and the scenarios when it occurs. The sensitivity of failure probability to change in system parameters is relevant to risk-informed decision making. Computing sensitivity is at least one level more difficult than the probability itself, which is already challenged by a large number of input random variables, rare events and implicit nonlinear `black-box' response. Finite difference with Monte Carlo probability estimates is spurious, requiring the number of samples to grow with the reciprocal of step size to suppress estimation variance. Many existing works gain efficiency by exploiting a specific class of input variables, sensitivity parameters, or response in its exact or surrogate form. For general systems, this work presents a theory and associated Monte Carlo strategy for computing sensitivity using response values and gradients with respect to sensitivity parameters. It is shown that the sensitivity at a given response threshold can be expressed via the expectation of response gradient conditional on the threshold. Determining the expectation requires conditioning on the threshold that is a zero-probability event, but it can be resolved by the concept of kernel smoothing. The proposed method offers sensitivity estimates for all response thresholds generated in a single Monte Carlo run. It is investigated in a number of examples featuring sensitivity parameters of different nature. As response gradient becomes increasingly available, it is hoped that this work can provide the basis for embedding sensitivity calculations with reliability in the same Monte Carlo run.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09315v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siu-Kui Au, Zi-Jun Cao</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for Multi-level Models Using the Survey-Weighted Pseudo-Posterior</title>
      <link>https://arxiv.org/abs/2510.09401</link>
      <description>arXiv:2510.09401v1 Announce Type: new 
Abstract: Parameter estimation and inference from complex survey samples typically focuses on global model parameters whose estimators have asymptotic properties, such as from fixed effects regression models. We present a motivating example of Bayesian inference for a multi-level or mixed effects model in which both the local parameters (e.g. group level random effects) and the global parameters may need to be adjusted for the complex sampling design. We evaluate the limitations of the survey-weighted pseudo-posterior and an existing automated post-processing method to incorporate the complex survey sample design for a wide variety of Bayesian models. We propose modifications to the automated process and demonstrate their improvements for multi-level models via a simulation study and a motivating example from the National Survey on Drug Use and Health. Reproduction examples are available from the authors and the updated R package is available via github:https://github.com/RyanHornby/csSampling</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09401v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew R. Williams, F. Hunter McGuire, Terrance D. Savitsky</dc:creator>
    </item>
    <item>
      <title>Defensive Model Expansion for Robust Bayesian Inference</title>
      <link>https://arxiv.org/abs/2510.09598</link>
      <description>arXiv:2510.09598v1 Announce Type: new 
Abstract: Some applied researchers hesitate to use nonparametric methods, worrying that they will lose power in small samples or overfit the data when simpler models are sufficient. We argue that at least some of these concerns are unfounded when nonparametric models are strongly shrunk towards parametric submodels. We consider expanding a parametric model with a nonparametric component that is heavily shrunk toward zero. This construction allows the model to adapt automatically: if the parametric model is correct, the nonparametric component disappears, recovering parametric efficiency, while if it is misspecified, the flexible component activates to capture the missing signal. We show that this adaptive behavior follows from simple and general conditions. Specifically, we prove that Bayesian nonparametric models anchored to linear regression, including variants of Gaussian processes regression and Bayesian additive regression trees, consistently identify the correct parametric submodel when it holds and give asymptotically efficient inference for regression coefficients. In simulations, we find that the "general BART" model performs identically to correctly specified linear regression when the parametric model holds, and substantially outperform it when nonlinear effects are present. This suggests a practical paradigm: "defensive model expansion" as a safeguard against model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09598v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio R. Linero</dc:creator>
    </item>
    <item>
      <title>SHAP-Based Supervised Clustering for Sample Classification and the Generalized Waterfall Plot</title>
      <link>https://arxiv.org/abs/2510.08737</link>
      <description>arXiv:2510.08737v1 Announce Type: cross 
Abstract: In this growing age of data and technology, large black-box models are becoming the norm due to their ability to handle vast amounts of data and learn incredibly complex input-output relationships. The deficiency of these methods, however, is their inability to explain the prediction process, making them untrustworthy and their use precarious in high-stakes situations. SHapley Additive exPlanations (SHAP) analysis is an explainable AI method growing in popularity for its ability to explain model predictions in terms of the original features. For each sample and feature in the data set, we associate a SHAP value that quantifies the contribution of that feature to the prediction of that sample. Clustering these SHAP values can provide insight into the data by grouping samples that not only received the same prediction, but received the same prediction for similar reasons. In doing so, we map the various pathways through which distinct samples arrive at the same prediction. To showcase this methodology, we present a simulated experiment in addition to a case study in Alzheimer's disease using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. We also present a novel generalization of the waterfall plot for multi-classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08737v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Lin, Julia Fukuyama</dc:creator>
    </item>
    <item>
      <title>Theoretical guarantees for change localization using conformal p-values</title>
      <link>https://arxiv.org/abs/2510.08749</link>
      <description>arXiv:2510.08749v1 Announce Type: cross 
Abstract: Changepoint localization aims to provide confidence sets for a changepoint (if one exists). Existing methods either relying on strong parametric assumptions or providing only asymptotic guarantees or focusing on a particular kind of change(e.g., change in the mean) rather than the entire distributional change. A method (possibly the first) to achieve distribution-free changepoint localization with finite-sample validity was recently introduced by \cite{dandapanthula2025conformal}. However, while they proved finite sample coverage, there was no analysis of set size. In this work, we provide rigorous theoretical guarantees for their algorithm. We also show the consistency of a point estimator for change, and derive its convergence rate without distributional assumptions. Along that line, we also construct a distribution-free consistent test to assess whether a particular time point is a changepoint or not. Thus, our work provides unified distribution-free guarantees for changepoint detection, localization, and testing. In addition, we present various finite sample and asymptotic properties of the conformal $p$-value in the distribution change setup, which provides a theoretical foundation for many applications of the conformal $p$-value. As an application of these properties, we construct distribution-free consistent tests for exchangeability against distribution-change alternatives and a new, computationally tractable method of optimizing the powers of conformal tests. We run detailed simulation studies to corroborate the performance of our methods and theoretical results. Together, our contributions offer a comprehensive and theoretically principled approach to distribution-free changepoint inference, broadening both the scope and credibility of conformal methods in modern changepoint analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08749v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Swapnaneel Bhattacharyya, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Invariant measures of disagreement with stochastic dominance</title>
      <link>https://arxiv.org/abs/1804.02905</link>
      <description>arXiv:1804.02905v4 Announce Type: replace 
Abstract: Stochastic dominance has not been too employed in practice due to its important limitations. To increase its versatility, the concept has recently been adapted by introducing various indices that measure the degree to which one probability distribution stochastically dominates another. In this paper, starting from the fundamentals and using very simple examples, we present and discuss some of these indices when one intends to maintain invariance through increasing functions. This naturally leads to consideration of the appealing common representation, $\theta(F,G)=P(X&gt;Y)$, where $(X, Y)$ is a random vector with marginal distributions $F$ and $G$. The indices considered here arise from different dependencies between X and Y. This includes the case of independent marginals, as well as other indices related to a contamination model or to a joint quantile representation. We emphasize the complementary role of some of these indices, which, in addition to measuring disagreement with respect to stochastic dominance, enable us to describe the maximum possible difference in the status of a value $x\in \Rea$ under $F$ or $G$. We apply these indices to simulated and real-world datasets, exploring their practical advantages and limitations.
  The tour includes lesser-known facets of well-known statistics such as Mann-Whitney, one-tailed Kolmogorov-Smirnov and Galton's rank statistics, even providing additional theory for the latter.</description>
      <guid isPermaLink="false">oai:arXiv.org:1804.02905v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/00031305.2025.2554737</arxiv:DOI>
      <arxiv:journal_reference>The American Statistician (2025)</arxiv:journal_reference>
      <dc:creator>E. del Barrio, J. A. Cuesta-Albertos, C. Matran</dc:creator>
    </item>
    <item>
      <title>Direct-Assisted Bayesian Unit-level Modeling for Small Area Estimation of Rare Event Prevalence</title>
      <link>https://arxiv.org/abs/2408.16129</link>
      <description>arXiv:2408.16129v3 Announce Type: replace 
Abstract: Small area estimation using survey data can be achieved by using either a design-based or a model-based inferential approach. Design-based direct estimators are generally preferable because of their consistency, asymptotic normality, and reliance on fewer assumptions. However, when data are sparse at the desired area level, as is often the case when measuring rare events, these direct estimators can have extremely large uncertainty, making a model-based approach preferable. A model-based approach with a random spatial effect borrows information from surrounding areas at the cost of inducing shrinkage. As a result, estimates may be over-smoothed and inconsistent with design-based estimates at higher area levels when aggregated. We propose two unit-level Bayesian models for small area estimation of rare event prevalence which use design-based direct estimates at a higher area level to increase consistency in aggregation. This model framework is designed to accommodate sparse data obtained from two-stage stratified cluster sampling, which is particularly relevant to applications in low- and middle-income countries. After introducing the model framework and its implementation, we conduct a simulation study to evaluate its properties and apply it to the estimation of the neonatal mortality rate in Zambia, using 2014 Demographic Health Surveys data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16129v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alana McGovern, Katherine Wilson, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Fast Bayesian Functional Principal Components Analysis</title>
      <link>https://arxiv.org/abs/2412.11340</link>
      <description>arXiv:2412.11340v4 Announce Type: replace 
Abstract: Functional Principal Components Analysis (FPCA) is a widely used analytic tool for dimension reduction of functional data. Traditional implementations of FPCA estimate the principal components from the data, then treat these estimates as fixed in subsequent analyses. To account for the uncertainty of PC estimates, we propose FAST, a fully-Bayesian FPCA with three core components: (1) projection of eigenfunctions onto an orthonormal spline basis; (2) efficient sampling of the orthonormal spline coefficient matrix using a parameter expansion scheme based on polar decomposition; and (3) ordering eigenvalues during sampling. Extensive simulation studies show that FAST is very stable and performs better compared to existing methods. FAST is motivated by and applied to a study of the variability in mealtime glucose from the Dietary Approaches to Stop Hypertension for Diabetes Continuous Glucose Monitoring (DASH4D CGM) study. All relevant STAN code and simulation routines are available as supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11340v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joseph Sartini, Xinkai Zhou, Liz Selvin, Scott Zeger, Ciprian Crainiceanu</dc:creator>
    </item>
    <item>
      <title>A Hybrid Framework Combining Autoregression and Common Factors for Matrix Time Series Modeling</title>
      <link>https://arxiv.org/abs/2503.05340</link>
      <description>arXiv:2503.05340v2 Announce Type: replace 
Abstract: Matrix-valued time series are increasingly common in economics and finance, but existing approaches such as matrix autoregressive and dynamic matrix factor models often impose restrictive assumptions and fail to capture complex dependencies. We propose a hybrid framework that integrates autoregressive dynamics with a shared low-rank common factor structure, enabling flexible modeling of temporal dependence and cross-sectional correlation while achieving dimension reduction. The model captures dynamic relationships through lagged matrix terms and leverages low-rank structures across predictor and response matrices, with connections between their row and column subspaces established via common latent bases to improve interpretability and efficiency. We develop a computationally efficient gradient-based estimation method and establish theoretical guarantees for statistical consistency and algorithmic convergence. Extensive simulations show robust performance under various data-generating processes, and in an application to multinational macroeconomic data, the model outperforms existing methods in forecasting and reveals meaningful interactions among economic factors and countries. The proposed framework provides a practical, interpretable, and theoretically grounded tool for analyzing high-dimensional matrix time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05340v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyun Fan, Xiaoyu Zhang, Mingyang Chen, Di Wang</dc:creator>
    </item>
    <item>
      <title>Statistical methods: Basic concepts, interpretations, and cautions</title>
      <link>https://arxiv.org/abs/2508.10168</link>
      <description>arXiv:2508.10168v3 Announce Type: replace 
Abstract: The study of associations and their causal explanations is a central research activity whose methodology varies tremendously across fields. Even within specialized subfields, comparisons across textbooks and journals reveals that the basics are subject to considerable variation and controversy. This variation is often obscured by the singular viewpoints presented within textbooks and journal guidelines, which may be deceptively written as if the norms they adopt are unchallenged. Furthermore, human limitations and the vastness within fields imply that no one can have expertise across all subfields and that interpretations will be severely constrained by the limitations of studies of human populations.
  The present chapter outlines an approach to statistical methods that attempts to recognize these problems from the start, rather than assume they are absent as in the claims of 'statistical significance' and 'confidence' ordinarily attached to statistical tests and interval estimates. It does so by grounding models and statistics in data description, and treating inferences from them as speculations based on assumptions that cannot be fully validated or checked using the analysis data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10168v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sander Greenland</dc:creator>
    </item>
    <item>
      <title>Bridging Control Variates and Regression Adjustment in A/B Testing: From Design-Based to Model-Based Frameworks</title>
      <link>https://arxiv.org/abs/2509.13944</link>
      <description>arXiv:2509.13944v2 Announce Type: replace 
Abstract: A B testing serves as the gold standard for large scale, data driven decision making in online businesses. To mitigate metric variability and enhance testing sensitivity, control variates and regression adjustment have emerged as prominent variance reduction techniques, leveraging pre experiment data to improve estimator performance. Over the past decade, these methods have spawned numerous derivatives, yet their theoretical connections and comparative properties remain underexplored. In this paper, we conduct a comprehensive analysis of their statistical properties, establish a formal bridge between the two frameworks in practical implementations, and extend the investigation from design based to model-based frameworks. Through simulation studies and real world experiments at ByteDance, we validate our theoretical insights across both frameworks. Our work aims to provide rigorous guidance for practitioners in online controlled experiments, addressing critical considerations of internal and external validity. The recommended method control variates with group specific coefficient estimates has been fully implemented and deployed on ByteDance's experimental platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13944v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhang, Bokui Wan, Yongli Qin</dc:creator>
    </item>
    <item>
      <title>A General CoVaR Based on Entropy Pooling</title>
      <link>https://arxiv.org/abs/2509.21904</link>
      <description>arXiv:2509.21904v2 Announce Type: replace 
Abstract: We propose a general CoVaR framework that extends the traditional CoVaR by incorporating diverse expert views and information, such as asset moment characteristics, quantile insights, and perspectives on the relative loss distribution between two assets. To integrate these expert views effectively while minimizing deviations from the prior distribution, we employ the entropy pooling method to derive the posterior distribution, which in turn enables us to compute the general CoVaR. Assuming bivariate normal distributions, we derive its analytical expressions under various perspectives. Sensitivity analysis reveals that CoVaR exhibits a linear relationship with both the expectations of the variables in the views and the differences in expectations between them. In contrast, CoVaR shows nonlinear dependencies with respect to the variance, quantiles, and correlation within these views.
  Empirical analysis of the US banking system during the Federal Reserve's interest rate hikes demonstrates the effectiveness of the general CoVaR when expert views are appropriately specified. Furthermore, we extend this framework to a general $\Delta$CoVaR, which allows for the assessment of risk spillover effects from various perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21904v2</guid>
      <category>stat.ME</category>
      <category>q-fin.MF</category>
      <category>q-fin.RM</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuhong Xu, Xinyao Zhao</dc:creator>
    </item>
    <item>
      <title>Understanding How Network Geometry Influences Diffusion Processes in Complex Networks: A Focus on Cryptocurrency Blockchains and Critical Infrastructure Networks</title>
      <link>https://arxiv.org/abs/2509.23450</link>
      <description>arXiv:2509.23450v2 Announce Type: replace 
Abstract: This study provides essential insights into how diffusion processes unfold in complex networks, with a focus on cryptocurrency blockchains and infrastructure networks. The structural properties of these networks, such as hub-dominated, heavy-tailed topology, network motifs, and node centrality, significantly influence diffusion speed and reach. Using epidemic diffusion models, specifically the Kertesz threshold model and the Susceptible-Infected (SI) model, we analyze key factors affecting diffusion dynamics. To assess the uncertainty in the fraction of infected nodes over time, we employ bootstrap confidence intervals, while Bayesian credible intervals are constructed to quantify parameter uncertainties in the SI models. Our findings reveal substantial variations across different network types, including Erd\H{o}s--R\'enyi networks, Geometric Random Graphs, and Delaunay Triangulation networks, emphasizing the role of network architecture in failure propagation. We identify that network motifs are crucial in diffusion. We highlight that hub-dominated networks, which dominate blockchain ecosystems, provide resilience against random failures but remain vulnerable to targeted attacks, posing significant risks to network stability. Furthermore, centrality measures such as degree, betweenness, and clustering coefficient strongly influence the transmissibility of diffusion in both blockchain and critical infrastructure networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23450v2</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssa/qnaf146</arxiv:DOI>
      <dc:creator>S M Mustaquim, Asim K. Dey, Abhijit Mandal</dc:creator>
    </item>
    <item>
      <title>Total Robustness in Bayesian Nonlinear Regression for Measurement Error Problems under Model Misspecification</title>
      <link>https://arxiv.org/abs/2510.03131</link>
      <description>arXiv:2510.03131v2 Announce Type: replace 
Abstract: Modern regression analyses are often undermined by covariate measurement error, misspecification of the regression model, and misspecification of the measurement error distribution. We present, to the best of our knowledge, the first Bayesian nonparametric framework targeting total robustness that tackles all three challenges in general nonlinear regression. The framework assigns a Dirichlet process prior to the latent covariate-response distribution and updates it with posterior pseudo-samples of the latent covariates, thereby providing the Dirichlet process posterior with observation-informed latent inputs and yielding estimators that minimise the discrepancy between Dirichlet process realisations and the model-induced joint law. This design allows practitioners to (i) encode prior beliefs, (ii) choose between pseudo-sampling latent covariates or working directly with error-prone observations, and (iii) tune the influence of prior and data. We establish generalisation bounds that tighten whenever the prior or pseudo-sample generator aligns with the underlying data generating process, ensuring robustness without sacrificing consistency. A gradient-based algorithm enables efficient computations; simulations and two real-world studies show lower estimation error and reduced estimation sensitivity to misspecification compared to Bayesian and frequentist competitors. The framework, therefore, offers a practical and interpretable paradigm for trustworthy regression when data and models are jointly imperfect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03131v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengqi Chen, Charita Dellaporta, Thomas B. Berrett, Theodoros Damoulas</dc:creator>
    </item>
    <item>
      <title>From Contextual Data to Newsvendor Decisions: On the Actual Performance of Data-Driven Algorithms</title>
      <link>https://arxiv.org/abs/2302.08424</link>
      <description>arXiv:2302.08424v5 Announce Type: replace-cross 
Abstract: In this work, we study how the relevance/quality and quantity of past data influence performance by analyzing a contextual Newsvendor problem, in which a decision-maker trades off between underage and overage costs under uncertain demand. We consider a setting in which past demands observed under ``close by'' contexts come from close by distributions and analyze the performance of data-driven algorithms through a notion of context-dependent worst-case expected regret. We analyze the broad class of Weighted Empirical Risk Minimization (WERM) policies which weigh past data according to their similarity in the contextual space. This class includes classical policies such as ERM, k-Nearest Neighbors and kernel-based policies. Our main methodological contribution is to characterize exactly the worst-case regret of any WERM policy on any given configuration of contexts. To the best of our knowledge, this provides the first understanding of tight performance guarantees in any contextual decision-making problem, with past literature focusing on upper bounds via concentration inequalities. We instead take an optimization approach, and isolate a structure in the Newsvendor loss function that allows to reduce the infinite-dimensional optimization problem over worst-case distributions to a simple line search. This in turn allows us to unveil fundamental insights that were obfuscated by previous general-purpose bounds. We characterize actual guaranteed performance as a function of the contexts, as well as granular insights on the learning curve of algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.08424v5</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Besbes, Will Ma, Omar Mouchtaki</dc:creator>
    </item>
    <item>
      <title>Positive Semidefinite Matrix Supermartingales</title>
      <link>https://arxiv.org/abs/2401.15567</link>
      <description>arXiv:2401.15567v5 Announce Type: replace-cross 
Abstract: We explore the asymptotic convergence and nonasymptotic maximal inequalities of supermartingales and backward submartingales in the space of positive semidefinite matrices. These are natural matrix analogs of scalar nonnegative supermartingales and backward nonnegative submartingales, whose convergence and maximal inequalities are the theoretical foundations for a wide and ever-growing body of results in statistics, econometrics, and theoretical computer science.
  Our results lead to new concentration inequalities for either martingale dependent or exchangeable random symmetric matrices under a variety of tail conditions, encompassing now-standard Chernoff bounds to self-normalized heavy-tailed settings. Further, these inequalities are usually expressed in the Loewner order, are sometimes valid simultaneously for all sample sizes or at an arbitrary data-dependent stopping time, and can often be tightened via an external randomization factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15567v5</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Learning the Effect of Persuasion via Difference-In-Differences</title>
      <link>https://arxiv.org/abs/2410.14871</link>
      <description>arXiv:2410.14871v3 Announce Type: replace-cross 
Abstract: We develop a difference-in-differences framework to measure the persuasive impact of informational treatments on behavior. We introduce two causal parameters, the forward and backward average persuasion rates on the treated, which refine the average treatment effect on the treated. The forward rate excludes cases of "preaching to the converted," while the backward rate omits "talking to a brick wall" cases. We propose both regression-based and semiparametrically efficient estimators. The framework applies to both two-period and staggered treatment settings, including event studies, and we demonstrate its usefulness with applications to a British election and a Chinese curriculum reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14871v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sung Jae Jun, Sokbae Lee</dc:creator>
    </item>
    <item>
      <title>Causal Additive Models with Unobserved Causal Paths and Backdoor Paths</title>
      <link>https://arxiv.org/abs/2502.07646</link>
      <description>arXiv:2502.07646v2 Announce Type: replace-cross 
Abstract: Causal additive models provide a tractable yet expressive framework for causal discovery in the presence of hidden variables. However, when unobserved backdoor or causal paths exist between two variables, their causal relationship is often unidentifiable under existing theories. We establish sufficient conditions under which causal directions can be identified in many such cases. In particular, we derive conditions that enable identification of the parent-child relationship in a bow, an adjacent pair of observed variables sharing a hidden common parent. This represents a notoriously difficult case in causal discovery, and, to our knowledge, no prior work has established such identifiability in any causal model without imposing assumptions on the hidden variables. Our conditions rely on new characterizations of regression sets and a hybrid approach that combines independence among regression residuals with conditional independencies among observed variables. We further provide a sound and complete algorithm that incorporates these insights, and empirical evaluations demonstrate competitive performance with state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07646v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thong Pham, Takashi Nicholas Maeda, Shohei Shimizu</dc:creator>
    </item>
    <item>
      <title>PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2505.21641</link>
      <description>arXiv:2505.21641v2 Announce Type: replace-cross 
Abstract: The average treatment effect (ATE) is widely used to evaluate the effectiveness of drugs and other medical interventions. In safety-critical applications like medicine, reliable inferences about the ATE typically require valid uncertainty quantification, such as through confidence intervals (CIs). However, estimating treatment effects in these settings often involves sensitive data that must be kept private. In this work, we present PrivATE, a novel machine learning framework for computing CIs for the ATE under differential privacy. Specifically, we focus on deriving valid privacy-preserving CIs for the ATE from observational data. Our PrivATE framework consists of three steps: (i) estimating the differentially private ATE through output perturbation; (ii) estimating the differentially private variance in a doubly robust manner; and (iii) constructing the CIs while accounting for the uncertainty from both the estimation and privatization steps. Our PrivATE framework is model agnostic, doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our framework using synthetic and real-world medical datasets. To the best of our knowledge, we are the first to derive a general, doubly robust framework for valid CIs of the ATE under ($\varepsilon,\delta$)-differential privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21641v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maresa Schr\"oder, Justin Hartenstein, Stefan Feuerriegel</dc:creator>
    </item>
    <item>
      <title>Integrated Analysis for Electronic Health Records with Structured and Sporadic Missingness</title>
      <link>https://arxiv.org/abs/2506.09208</link>
      <description>arXiv:2506.09208v2 Announce Type: replace-cross 
Abstract: Objectives: We propose a novel imputation method tailored for Electronic Health Records (EHRs) with structured and sporadic missingness. Such missingness frequently arises in the integration of heterogeneous EHR datasets for downstream clinical applications. By addressing these gaps, our method provides a practical solution for integrated analysis, enhancing data utility and advancing the understanding of population health.
  Materials and Methods: We begin by demonstrating structured and sporadic missing mechanisms in the integrated analysis of EHR data. Following this, we introduce a novel imputation framework, Macomss, specifically designed to handle structurally and heterogeneously occurring missing data. We establish theoretical guarantees for Macomss, ensuring its robustness in preserving the integrity and reliability of integrated analyses. To assess its empirical performance, we conduct extensive simulation studies that replicate the complex missingness patterns observed in real-world EHR systems, complemented by validation using EHR datasets from the Duke University Health System (DUHS).
  Results: Simulation studies show that our approach consistently outperforms existing imputation methods. Using datasets from three hospitals within DUHS, Macomss achieves the lowest imputation errors for missing data in most cases and provides superior or comparable downstream prediction performance compared to benchmark methods.
  Conclusions: We provide a theoretically guaranteed and practically meaningful method for imputing structured and sporadic missing data, enabling accurate and reliable integrated analysis across multiple EHR datasets. The proposed approach holds significant potential for advancing research in population health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09208v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Yan Zhang, Chuan Hong, T. Tony Cai, Tianxi Cai, Anru R. Zhang</dc:creator>
    </item>
  </channel>
</rss>
