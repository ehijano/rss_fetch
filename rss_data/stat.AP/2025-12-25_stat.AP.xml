<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Dec 2025 05:01:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Profit-Based Measure of Lending Discrimination</title>
      <link>https://arxiv.org/abs/2512.20753</link>
      <description>arXiv:2512.20753v1 Announce Type: new 
Abstract: Algorithmic lending has transformed the consumer credit landscape, with complex machine learning models now commonly used to make or assist underwriting decisions. To comply with fair lending laws, these algorithms typically exclude legally protected characteristics, such as race and gender. Yet algorithmic underwriting can still inadvertently favor certain groups, prompting new questions about how to audit lending algorithms for potentially discriminatory behavior. Building on prior theoretical work, we introduce a profit-based measure of lending discrimination in loan pricing. Applying our approach to approximately 80,000 personal loans from a major U.S. fintech platform, we find that loans made to men and Black borrowers yielded lower profits than loans to other groups, indicating that men and Black applicants benefited from relatively favorable lending decisions. We trace these disparities to miscalibration in the platform's underwriting model, which underestimates credit risk for Black borrowers and overestimates risk for women. We show that one could correct this miscalibration -- and the corresponding lending disparities -- by explicitly including race and gender in underwriting models, illustrating a tension between competing notions of fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20753v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Madison Coots, Robert Bartlett, Julian Nyarko, Sharad Goel</dc:creator>
    </item>
    <item>
      <title>The Whittle likelihood for mixed models with application to groundwater level time series</title>
      <link>https://arxiv.org/abs/2512.20810</link>
      <description>arXiv:2512.20810v1 Announce Type: cross 
Abstract: Understanding the processes that influence groundwater levels is crucial for forecasting and responding to hazards such as groundwater droughts. Mixed models, which combine a fixed mean, expressed using independent predictors, with autocorrelated random errors, are used for inference, forecasting and filling in missing values in groundwater level time series. Estimating parameters of mixed models using maximum likelihood has high computational complexity. For large datasets, this leads to restrictive simplifying assumptions such as fixing certain free parameters in practical implementations. In this paper, we propose a method to jointly estimate all parameters of mixed models using the Whittle likelihood, a frequency-domain quasi-likelihood. Our method is robust to missing and non-Gaussian data and can handle much larger data sizes. We demonstrate the utility of our method both in a simulation study and with real-world data, comparing against maximum likelihood and an alternative two-stage approach that estimates fixed and random effect parameters separately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20810v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakub J. Pypkowski, Adam M. Sykulski, James S. Martin, Ben P. Marchant</dc:creator>
    </item>
    <item>
      <title>The Area Signal-to-Noise Ratio: A Robust Alternative to Peak-Based SNR in Spectroscopic Analysis</title>
      <link>https://arxiv.org/abs/2512.20830</link>
      <description>arXiv:2512.20830v1 Announce Type: cross 
Abstract: In spectroscopic analysis, the peak-based signal-to-noise ratio (pSNR) is commonly used but suffers from limitations such as sensitivity to noise spikes and reduced effectiveness for broader peaks. We introduce the area-based signal-to-noise ratio (aSNR) as a robust alternative that integrates the signal over a defined region of interest, reducing noise variance and improving detection for various lineshapes. We used Monte Carlo simulations (n=2,000 trials per condition) to test aSNR on Gaussian, Lorentzian, and Voigt lineshapes. We found that aSNR requires significantly lower amplitudes than pSNR to achieve a 50% detection probability. Receiver operating characteristic (ROC) curves show that aSNR performs better than pSNR at low amplitudes. Our results show that aSNR works especially advantageously for broad peaks and could be extended to volume-based SNR for multidimensional spectra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20830v1</guid>
      <category>eess.SP</category>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Yu, Huaqing Zhao, Lin Z. Li</dc:creator>
    </item>
    <item>
      <title>Invariant Feature Extraction Through Conditional Independence and the Optimal Transport Barycenter Problem: the Gaussian case</title>
      <link>https://arxiv.org/abs/2512.20914</link>
      <description>arXiv:2512.20914v1 Announce Type: cross 
Abstract: A methodology is developed to extract $d$ invariant features $W=f(X)$ that predict a response variable $Y$ without being confounded by variables $Z$ that may influence both $X$ and $Y$.
  The methodology's main ingredient is the penalization of any statistical dependence between $W$ and $Z$ conditioned on $Y$, replaced by the more readily implementable plain independence between $W$ and the random variable $Z_Y = T(Z,Y)$ that solves the [Monge] Optimal Transport Barycenter Problem for $Z\mid Y$. In the Gaussian case considered in this article, the two statements are equivalent.
  When the true confounders $Z$ are unknown, other measurable contextual variables $S$ can be used as surrogates, a replacement that involves no relaxation in the Gaussian case if the covariance matrix $\Sigma_{ZS}$ has full range. The resulting linear feature extractor adopts a closed form in terms of the first $d$ eigenvectors of a known matrix. The procedure extends with little change to more general, non-Gaussian / non-linear cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20914v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Bounos, Pablo Groisman, Mariela Sued, Esteban Tabak</dc:creator>
    </item>
    <item>
      <title>Modeling gap acceptance behavior allowing for perceptual distortions and exogenous influences</title>
      <link>https://arxiv.org/abs/2512.21136</link>
      <description>arXiv:2512.21136v1 Announce Type: cross 
Abstract: This work on gap acceptance is based on the premise that the decision to accept/reject a gap happens in a person's mind and therefore must be based on the perceived gap and not the measured gap. The critical gap must also exist in a person's mind and hence, together with the perceived gap, is a latent variable. Finally, it is also proposed that the critical gap is influenced by various exogenous variables such as subject and opposing vehicle types, and perceived waiting time. Mathematical models that (i) incorporate systematic and random distortions during the perception process and (ii) account for the effect of the various influencing variables are developed. The parameters of these models are estimated for two different gap acceptance data sets using the maximum likelihood technique. The data is collected as part of this study. The estimated parameters throw valuable insights into how these influencing variables affect the critical gap. The results corroborate the initial predictions on the nature of influence these variables must exert and give strength to the gap acceptance decision-making construct proposed here. This work also proposes a methodology to estimate a measurable/observable world emulator of the latent variable critical gap. The use of the emulator critical gap provides improved estimates of derived quantities like the average waiting time of subject vehicles. Finally, studies are also conducted to show that the number of rejected gaps can work as a reasonable surrogate for the influencing variable, waiting time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21136v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ankita Sharma, Partha Chakroborty, Pranamesh Chakraborty</dc:creator>
    </item>
    <item>
      <title>Bias correction of satellite and reanalysis products for daily rainfall occurrence and intensity</title>
      <link>https://arxiv.org/abs/2510.27456</link>
      <description>arXiv:2510.27456v3 Announce Type: replace 
Abstract: In data-sparse regions, satellite and reanalysis rainfall estimates (SREs) are vital but limited by inherent biases. This study evaluates bias correction (BC) methods, including traditional statistical (LOCI, QM) and machine learning (SVR, GPR), applied to seven SREs across 38 stations in Ghana and Zambia. We introduce a constrained LOCI method to prevent the unrealistically high rainfall values produced by the original approach. Results indicate that statistical methods generally outperformed machine learning, though QM tended to inflate rainfall. Corrected SREs showed high capability in detecting dry days (POD $\ge$ 0.80). The ENACTS product, which integrates numerous station records, was the most amenable to correction in Zambia; most BC methods reduced mean error at &gt;70% of stations. However, ENACTS performed less reliably at an independent station (Moorings), highlighting the need for broader validation at locations not incorporated into the product. Crucially, even after correction, most SREs (except ENACTS) failed to improve the detection of heavy and violent rainfall (POD $\le$ 0.2). This limits their utility for flood risk assessment and highlights a vital research gap regarding extreme event estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27456v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>John Bagiliko, David Stern, Francis Feehi Torgbor, Danny Parsons, Samuel Owusu Ansah, Denis Ndanguza</dc:creator>
    </item>
    <item>
      <title>Smoothing Variances Across Time: Adaptive Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2408.11315</link>
      <description>arXiv:2408.11315v5 Announce Type: replace-cross 
Abstract: We introduce a novel Bayesian framework for estimating time-varying volatility by extending the Random Walk Stochastic Volatility (RWSV) model with Dynamic Shrinkage Processes (DSP) in log-variances. Unlike the classical Stochastic Volatility (SV) or GARCH-type models with restrictive parametric stationarity assumptions, our proposed Adaptive Stochastic Volatility (ASV) model provides smooth yet dynamically adaptive estimates of evolving volatility and its uncertainty. We further enhance the model by incorporating a nugget effect, allowing it to flexibly capture small-scale variability while preserving smoothness elsewhere. We derive the theoretical properties of the global-local shrinkage prior DSP. Through simulation studies, we show that ASV exhibits remarkable misspecification resilience and low prediction error across various data-generating processes. Furthermore, ASV's capacity to yield locally smooth and interpretable estimates facilitates a clearer understanding of the underlying patterns and trends in volatility. As an extension, we develop the Bayesian Trend Filter with ASV (BTF-ASV) which allows joint modeling of the mean and volatility with abrupt changes. Finally, our proposed models are applied to time series data from finance, econometrics, and environmental science, highlighting their flexibility and broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11315v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason B. Cho, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>Aging health dynamics cross a tipping point near age 75</title>
      <link>https://arxiv.org/abs/2412.07795</link>
      <description>arXiv:2412.07795v2 Announce Type: replace-cross 
Abstract: Aging includes both continuous gradual decline from microscopic mechanisms together with major deficit onset events such as morbidity, disability and ultimately death. These deficit events are stochastic, obscuring the connection between aging mechanisms and overall health. We propose a framework for modelling both the gradual effects of aging together with health deficit onset events, as reflected in the frailty index (FI) - a quantitative measure of overall age-related health. We model damage and repair dynamics of the FI from individual health transitions within two large longitudinal studies of aging health, the Health and Retirement Study (HRS) and the English Longitudinal Study of Ageing (ELSA), which together included N=47592 individuals. We find that both damage resistance (robustness) and damage recovery (resilience) rates decline smoothly with both increasing age and with increasing FI, for both sexes. This leads to two distinct dynamical states: a robust and resilient young state of stable good health (low FI) and an older state that drifts towards poor health (high FI). These two health states are separated by a sharp transition near age 75. Since FI accumulation risk accelerates dramatically across this tipping point, ages 70-80 are crucial for understanding and managing late-life decline in health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07795v2</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glen Pridham, Kenneth Rockwood, Andrew Rutenberg</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for Causal Explainability</title>
      <link>https://arxiv.org/abs/2512.20219</link>
      <description>arXiv:2512.20219v2 Announce Type: replace-cross 
Abstract: Understanding how much each variable contributes to an outcome is a central question across disciplines. A causal view of explainability is favorable for its ability in uncovering underlying mechanisms and generalizing to new contexts. Based on a family of causal explainability quantities, we develop methods for their estimation and inference. In particular, we construct a one-step correction estimator using semi-parametric efficiency theory, which explicitly leverages the independence structure of variables to reduce the asymptotic variance. For a null hypothesis on the boundary, i.e., zero explainability, we show its equivalence to Fisher's sharp null, which motivates a randomization-based inference procedure. Finally, we illustrate the empirical efficacy of our approach through simulations as well as an immigration experiment dataset, where we investigate how features and their interactions shape public opinion toward admitting immigrants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20219v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weihan Zhang, Zijun Gao</dc:creator>
    </item>
  </channel>
</rss>
