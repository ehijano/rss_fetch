<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Aug 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Misrepresented Technological Solutions in Imagined Futures: The Origins and Dangers of AI Hype in the Research Community</title>
      <link>https://arxiv.org/abs/2408.15244</link>
      <description>arXiv:2408.15244v1 Announce Type: new 
Abstract: Technology does not exist in a vacuum; technological development, media representation, public perception, and governmental regulation cyclically influence each other to produce the collective understanding of a technology's capabilities, utilities, and risks. When these capabilities are overestimated, there is an enhanced risk of subjecting the public to dangerous or harmful technology, artificially restricting research and development directions, and enabling misguided or detrimental policy. The dangers of technological hype are particularly relevant in the rapidly evolving space of AI. Centering the research community as a key player in the development and proliferation of hype, we examine the origins and risks of AI hype to the research community and society more broadly and propose a set of measures that researchers, regulators, and the public can take to mitigate these risks and reduce the prevalence of unfounded claims about the technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15244v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Savannah Thais</dc:creator>
    </item>
    <item>
      <title>Multi-Class Plant Leaf Disease Detection: A CNN-based Approach with Mobile App Integration</title>
      <link>https://arxiv.org/abs/2408.15289</link>
      <description>arXiv:2408.15289v1 Announce Type: new 
Abstract: Plant diseases significantly impact agricultural productivity, resulting in economic losses and food insecurity. Prompt and accurate detection is crucial for the efficient management and mitigation of plant diseases. This study investigates advanced techniques in plant disease detection, emphasizing the integration of image processing, machine learning, deep learning methods, and mobile technologies. High-resolution images of plant leaves were captured and analyzed using convolutional neural networks (CNNs) to detect symptoms of various diseases, such as blight, mildew, and rust. This study explores 14 classes of plants and diagnoses 26 unique plant diseases. We focus on common diseases affecting various crops. The model was trained on a diverse dataset encompassing multiple crops and disease types, achieving 98.14% accuracy in disease diagnosis. Finally integrated this model into mobile apps for real-time disease diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15289v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Aziz Hosen Foysal, Foyez Ahmed, Md Zahurul Haque</dc:creator>
    </item>
    <item>
      <title>Antivax and off-label medication communities on brazilian Telegram: between esotericism as a gateway and the monetization of false miraculous cures</title>
      <link>https://arxiv.org/abs/2408.15308</link>
      <description>arXiv:2408.15308v1 Announce Type: new 
Abstract: Conspiracy theories, particularly those focused on anti-vaccine narratives and the promotion of off-label medications such as MMS and CDS, have proliferated on Telegram, including in Brazil, finding fertile ground among communities that share esoteric beliefs and distrust towards scientific institutions. In this context, this study seeks to answer how Brazilian conspiracy theory communities on Telegram are characterized and articulated concerning anti-vaccine themes and off-label medications? It is important to highlight that this study is part of a series of seven studies aimed at understanding and characterizing Brazilian conspiracy theory communities on Telegram. This series of seven studies is openly and originally available on the arXiv of Cornell University, applying a mirrored method across all studies, changing only the thematic object of analysis and providing replicable research, including proprietary and original codes developed, contributing to the culture of free and open-source software. Regarding the main findings of this study, it was observed: Themes such as the New World Order and Apocalypse and Survivalism act as significant gateways to anti-vaccine narratives, connecting them to theories of global control; Globalism and New World Order stand out as the main communities receiving invitations from anti-vaccine communities; Occultism and Esotericism emerge as the largest sources of invitations to off-label medication communities, creating a strong connection between esoteric beliefs and the promotion of non-scientific treatments; Anti-vaccine narratives experienced a 290% increase during the COVID-19 pandemic, evidencing a growing interconnectedness with other conspiracy theories; The overlap of themes between anti-vaccine and other conspiracy theories creates an interdependent disinformation network, where different narratives mutually reinforce each other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15308v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ergon Cugler de Moraes Silva</dc:creator>
    </item>
    <item>
      <title>Climate change denial and anti-science communities on brazilian Telegram: climate disinformation as a gateway to broader conspiracy networks</title>
      <link>https://arxiv.org/abs/2408.15311</link>
      <description>arXiv:2408.15311v1 Announce Type: new 
Abstract: Conspiracy theories related to climate change denial and anti-science have found fertile ground on Telegram, particularly among Brazilian communities that distrust scientific institutions and oppose global environmental policies. This study seeks to answer the research question: how are Brazilian conspiracy theory communities on climate change and anti-science themes characterized and articulated on Telegram? It is worth noting that this study is part of a series of seven studies aimed at understanding and characterizing Brazilian conspiracy theory communities on Telegram. This series of studies is openly and originally available on arXiv from Cornell University, applying a mirrored method across all seven studies, changing only the thematic focus of analysis, and providing replicable investigation methods, including custom-developed and proprietary codes, contributing to the culture of open-source software. Regarding the main findings of this study, the following observations were made: Climate change denial and anti-science communities interact synergistically, creating a complex network that mutually reinforces disinformation narratives; Apocalyptic themes, such as Apocalypse and Survivalism, act as gateways to climate denial, with 5,057 links directed to these communities; Anti-science communities function as gatekeepers, distributing links evenly to theories such as the New World Order and Globalism, among others; During the COVID-19 pandemic, anti-science discussions experienced a significant peak, driven by vaccine disinformation; The intersection between anti-science narratives and esoteric beliefs reinforces the idea of a supposed alternative truth that challenges science; Since 2022, discussions on climate change have evolved to align with global domination theories; Additionally, the UN's 2030 Agenda is portrayed as part of a global conspiracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15311v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ergon Cugler de Moraes Silva</dc:creator>
    </item>
    <item>
      <title>An evidence-based and critical analysis of the Fediverse decentralization promises</title>
      <link>https://arxiv.org/abs/2408.15383</link>
      <description>arXiv:2408.15383v1 Announce Type: new 
Abstract: This paper examines the potential of the Fediverse, a federated network of social media and content platforms, to counter the centralization and dominance of commercial platforms on the social Web. We gather evidence from the technology powering the Fediverse (especially the ActivityPub protocol), current statistical data regarding Fediverse user distribution over instances, and the status of two older, similar, decentralized technologies: e-mail and the Web. Our findings suggest that Fediverse will face significant challenges in fulfilling its decentralization promises, potentially hindering its ability to positively impact the social Web on a large scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15383v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Henrique S. Xavier</dc:creator>
    </item>
    <item>
      <title>Navigating the Future of Education: Educators' Insights on AI Integration and Challenges in Greece, Hungary, Latvia, Ireland and Armenia</title>
      <link>https://arxiv.org/abs/2408.15686</link>
      <description>arXiv:2408.15686v1 Announce Type: new 
Abstract: Understanding teachers' perspectives on AI in Education (AIEd) is crucial for its effective integration into the educational framework. This paper aims to explore how teachers currently use AI and how it can enhance the educational process. We conducted a cross-national study spanning Greece, Hungary, Latvia, Ireland, and Armenia, surveying 1754 educators through an online questionnaire, addressing three research questions. Our first research question examines educators' understanding of AIEd, their skepticism, and its integration within schools. Most educators report a solid understanding of AI and acknowledge its potential risks. AIEd is primarily used for educator support and engaging students. However, concerns exist about AI's impact on fostering critical thinking and exposing students to biased data. The second research question investigates student engagement with AI tools from educators' perspectives. Teachers indicate that students use AI mainly to manage their academic workload, while outside school, AI tools are primarily used for entertainment. The third research question addresses future implications of AI in education. Educators are optimistic about AI's potential to enhance educational processes, particularly through personalized learning experiences. Nonetheless, they express significant concerns about AI's impact on cultivating critical thinking and ethical issues related to potential misuse. There is a strong emphasis on the need for professional development through training seminars, workshops, and online courses to integrate AI effectively into teaching practices. Overall, the findings highlight a cautious optimism among educators regarding AI in education, alongside a clear demand for targeted professional development to address concerns and enhance skills in using AI tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15686v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evangelia Daskalaki, Katerina Psaroudaki, Paraskevi Fragopoulou</dc:creator>
    </item>
    <item>
      <title>Comparing diversity, negativity, and stereotypes in Chinese-language AI technologies: a case study on Baidu, Ernie and Qwen</title>
      <link>https://arxiv.org/abs/2408.15696</link>
      <description>arXiv:2408.15696v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and search engines have the potential to perpetuate biases and stereotypes by amplifying existing prejudices in their training data and algorithmic processes, thereby influencing public perception and decision-making. While most work has focused on Western-centric AI technologies, we study Chinese-based tools by investigating social biases embedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie and Qwen. Leveraging a dataset of 240 social groups across 13 categories describing Chinese society, we collect over 30k views encoded in the aforementioned tools by prompting them for candidate words describing such groups. We find that language models exhibit a larger variety of embedded views compared to the search engine, although Baidu and Qwen generate negative content more often than Ernie. We also find a moderate prevalence of stereotypes embedded in the language models, many of which potentially promote offensive and derogatory views. Our work highlights the importance of promoting fairness and inclusivity in AI technologies with a global perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15696v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geng Liu, Carlo Alberto Bono, Francesco Pierri</dc:creator>
    </item>
    <item>
      <title>Artificial Data, Real Insights: Evaluating Opportunities and Risks of Expanding the Data Ecosystem with Synthetic Data</title>
      <link>https://arxiv.org/abs/2408.15260</link>
      <description>arXiv:2408.15260v1 Announce Type: cross 
Abstract: Synthetic Data is not new, but recent advances in Generative AI have raised interest in expanding the research toolbox, creating new opportunities and risks. This article provides a taxonomy of the full breadth of the Synthetic Data domain. We discuss its place in the research ecosystem by linking the advances in computational social science with the idea of the Fourth Paradigm of scientific discovery that integrates the elements of the evolution from empirical to theoretic to computational models. Further, leveraging the framework of Truth, Beauty, and Justice, we discuss how evaluation criteria vary across use cases as the information is used to add value and draw insights. Building a framework to organize different types of synthetic data, we end by describing the opportunities and challenges with detailed examples of using Generative AI to create synthetic quantitative and qualitative datasets and discuss the broader spectrum including synthetic populations, expert systems, survey data replacement, and personabots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15260v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard Timpone, Yongwei Yang</dc:creator>
    </item>
    <item>
      <title>People over trust AI-generated medical responses and view them to be as valid as doctors, despite low accuracy</title>
      <link>https://arxiv.org/abs/2408.15266</link>
      <description>arXiv:2408.15266v1 Announce Type: cross 
Abstract: This paper presents a comprehensive analysis of how AI-generated medical responses are perceived and evaluated by non-experts. A total of 300 participants gave evaluations for medical responses that were either written by a medical doctor on an online healthcare platform, or generated by a large language model and labeled by physicians as having high or low accuracy. Results showed that participants could not effectively distinguish between AI-generated and Doctors' responses and demonstrated a preference for AI-generated responses, rating High Accuracy AI-generated responses as significantly more valid, trustworthy, and complete/satisfactory. Low Accuracy AI-generated responses on average performed very similar to Doctors' responses, if not more. Participants not only found these low-accuracy AI-generated responses to be valid, trustworthy, and complete/satisfactory but also indicated a high tendency to follow the potentially harmful medical advice and incorrectly seek unnecessary medical attention as a result of the response provided. This problematic reaction was comparable if not more to the reaction they displayed towards doctors' responses. This increased trust placed on inaccurate or inappropriate AI-generated medical advice can lead to misdiagnosis and harmful consequences for individuals seeking help. Further, participants were more trusting of High Accuracy AI-generated responses when told they were given by a doctor and experts rated AI-generated responses significantly higher when the source of the response was unknown. Both experts and non-experts exhibited bias, finding AI-generated responses to be more thorough and accurate than Doctors' responses but still valuing the involvement of a Doctor in the delivery of their medical advice. Ensuring AI systems are implemented with medical professionals should be the future of using AI for the delivery of medical advice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15266v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shruthi Shekar, Pat Pataranutaporn, Chethan Sarabu, Guillermo A. Cecchi, Pattie Maes</dc:creator>
    </item>
    <item>
      <title>Possibilities and challenges of STEAM pedagogies</title>
      <link>https://arxiv.org/abs/2408.15282</link>
      <description>arXiv:2408.15282v1 Announce Type: cross 
Abstract: This paper examines the integration of STEAM (Science, Technology, Engineering, Arts, and Mathematics) into education, emphasizing the inclusion of the Arts to foster creativity alongside traditional STEM skills. STEAM encourages multidisciplinary, student-centered approaches like project-based and inquiry-based learning, promoting real-world problem-solving. However, significant challenges arise in implementing STEAM, particularly for teachers who often lack interdisciplinary training and face rigid school structures. Assessing STEAM outcomes also remains complex. The paper highlights the need for reforms in teacher education to support interdisciplinary teaching, along with addressing "disciplinary egocentrism" in higher education. Despite these challenges, STEAM has shown promise in enhancing student engagement, creativity, and critical thinking. To unlock its full potential, systemic changes in curriculum design, educational practices, and teacher training are essential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15282v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.CY</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Iv\'an S\'anchez Milara, Marta Cort\'es Ordu\~na</dc:creator>
    </item>
    <item>
      <title>What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users</title>
      <link>https://arxiv.org/abs/2408.15354</link>
      <description>arXiv:2408.15354v1 Announce Type: cross 
Abstract: Interest is growing in artificial empathy, but so is confusion about what artificial empathy is or needs to be. This confusion makes it challenging to navigate the technical and ethical issues that accompany empathic AI development. Here, we outline a framework for thinking about empathic AI based on the premise that different constellations of capabilities associated with empathy are important for different empathic AI applications. We describe distinctions of capabilities that we argue belong under the empathy umbrella, and show how three medical empathic AI use cases require different sets of these capabilities. We conclude by discussing why appreciation of the diverse capabilities under the empathy umbrella is important for both AI creators and users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15354v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jana Schaich Borg, Hannah Read</dc:creator>
    </item>
    <item>
      <title>An Investigation of Warning Erroneous Chat Translations in Cross-lingual Communication</title>
      <link>https://arxiv.org/abs/2408.15543</link>
      <description>arXiv:2408.15543v1 Announce Type: cross 
Abstract: The complexities of chats pose significant challenges for machine translation models. Recognizing the need for a precise evaluation metric to address the issues of chat translation, this study introduces Multidimensional Quality Metrics for Chat Translation (MQM-Chat). Through the experiments of five models using MQM-Chat, we observed that all models generated certain fundamental errors, while each of them has different shortcomings, such as omission, overly correcting ambiguous source content, and buzzword issues, resulting in the loss of stylized information. Our findings underscore the effectiveness of MQM-Chat in evaluating chat translation, emphasizing the importance of stylized content and dialogue consistency for future studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15543v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.18653/v1/2023.ijcnlp-srw.2</arxiv:DOI>
      <arxiv:journal_reference>IJCNLP-AACL 2023 Student Research Workshop</arxiv:journal_reference>
      <dc:creator>Yunmeng Li, Jun Suzuki, Makoto Morishita, Kaori Abe, Kentaro Inui</dc:creator>
    </item>
    <item>
      <title>Different Facets for Different Experts: A Framework for Streamlining The Integration of Qualitative Insights into ABM Development</title>
      <link>https://arxiv.org/abs/2408.15725</link>
      <description>arXiv:2408.15725v1 Announce Type: cross 
Abstract: A key problem in agent-based simulation is that integrating qualitative insights from multiple discipline experts is extremely hard. In most simulations, agent capabilities and corresponding behaviour needs to be programmed into the agent. We report on the architecture of a tool that disconnects the programmed functions of the agent, from the acquisition of capability and displayed behaviour. This allows multiple different domain experts to represent qualitative insights, without the need for code to be changed. It also allows a continuous integration (or even change) of qualitative behaviour processes, as more insights are gained. The consequent behaviour observed in the model is both, more faithful to the expert's insight as well as able to be contrasted against other models representing other insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15725v1</guid>
      <category>cs.MA</category>
      <category>cs.CY</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivek Nallur, Pedram Aghaei, Graham Finlay</dc:creator>
    </item>
    <item>
      <title>Red-Teaming for Generative AI: Silver Bullet or Security Theater?</title>
      <link>https://arxiv.org/abs/2401.15897</link>
      <description>arXiv:2401.15897v3 Announce Type: replace 
Abstract: In response to rising concerns surrounding the safety, security, and trustworthiness of Generative AI (GenAI) models, practitioners and regulators alike have pointed to AI red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite AI red-teaming's central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing GenAI harm mitigations, and that industry may effectively apply red-teaming and other strategies behind closed doors to safeguard AI, gestures towards red-teaming (based on public definitions) as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative AI, we synthesize our recommendations into a question bank meant to guide and scaffold future AI red-teaming practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15897v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Feffer, Anusha Sinha, Wesley Hanwen Deng, Zachary C. Lipton, Hoda Heidari</dc:creator>
    </item>
    <item>
      <title>Articulation Work and Tinkering for Fairness in Machine Learning</title>
      <link>https://arxiv.org/abs/2407.16496</link>
      <description>arXiv:2407.16496v2 Announce Type: replace 
Abstract: The field of fair AI aims to counter biased algorithms through computational modelling. However, it faces increasing criticism for perpetuating the use of overly technical and reductionist methods. As a result, novel approaches appear in the field to address more socially-oriented and interdisciplinary (SOI) perspectives on fair AI. In this paper, we take this dynamic as the starting point to study the tension between computer science (CS) and SOI research. By drawing on STS and CSCW theory, we position fair AI research as a matter of 'organizational alignment': what makes research 'doable' is the successful alignment of three levels of work organization (the social world, the laboratory, and the experiment). Based on qualitative interviews with CS researchers, we analyze the tasks, resources, and actors required for doable research in the case of fair AI. We find that CS researchers engage with SOI research to some extent, but organizational conditions, articulation work, and ambiguities of the social world constrain the doability of SOI research for them. Based on our findings, we identify and discuss problems for aligning CS and SOI as fair AI continues to evolve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16496v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miriam Fahimi, Mayra Russo, Kristen M. Scott, Maria-Esther Vidal, Bettina Berendt, Katharina Kinder-Kurlanda</dc:creator>
    </item>
    <item>
      <title>Towards Human-Level Text Coding with LLMs: The Case of Fatherhood Roles in Public Policy Documents</title>
      <link>https://arxiv.org/abs/2311.11844</link>
      <description>arXiv:2311.11844v3 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) like GPT-3.5 and GPT-4 promise automation with better results and less programming, opening up new opportunities for text analysis in political science. In this study, we evaluate LLMs on three original coding tasks involving typical complexities encountered in political science settings: a non-English language, legal and political jargon, and complex labels based on abstract constructs. Along the paper, we propose a practical workflow to optimize the choice of the model and the prompt. We find that the best prompting strategy consists of providing the LLMs with a detailed codebook, as the one provided to human coders. In this setting, an LLM can be as good as or possibly better than a human annotator while being much faster, considerably cheaper, and much easier to scale to large amounts of text. We also provide a comparison of GPT and popular open-source LLMs, discussing the trade-offs in the model's choice. Our software allows LLMs to be easily used as annotators and is publicly available: https://github.com/lorelupo/pappa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11844v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Lupo, Oscar Magnusson, Dirk Hovy, Elin Naurin, Lena W\"angnerud</dc:creator>
    </item>
    <item>
      <title>Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model</title>
      <link>https://arxiv.org/abs/2402.09786</link>
      <description>arXiv:2402.09786v4 Announce Type: replace-cross 
Abstract: Generative adversarial networks (GANs) generate photorealistic faces that are often indistinguishable by humans from real faces. While biases in machine learning models are often assumed to be due to biases in training data, we find pathological internal color and luminance biases in the discriminator of a pre-trained StyleGAN3-r model that are not explicable by the training data. We also find that the discriminator systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine axes common in research on stereotyping in social psychology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09786v4</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alvin Grissom II, Ryan F. Lei, Matt Gusdorff, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter</dc:creator>
    </item>
    <item>
      <title>From Complexity to Clarity: How AI Enhances Perceptions of Scientists and the Public's Understanding of Science</title>
      <link>https://arxiv.org/abs/2405.00706</link>
      <description>arXiv:2405.00706v3 Announce Type: replace-cross 
Abstract: This paper evaluated the effectiveness of using generative AI to simplify science communication and enhance the public's understanding of science. By comparing lay summaries of journal articles from PNAS, yoked to those generated by AI, this work first assessed linguistic simplicity differences across such summaries and public perceptions in follow-up experiments. Specifically, Study 1a analyzed simplicity features of PNAS abstracts (scientific summaries) and significance statements (lay summaries), observing that lay summaries were indeed linguistically simpler, but effect size differences were small. Study 1b used a large language model, GPT-4, to create significance statements based on paper abstracts and this more than doubled the average effect size without fine-tuning. Study 2 experimentally demonstrated that simply-written GPT summaries facilitated more favorable perceptions of scientists (they were perceived as more credible and trustworthy, but less intelligent) than more complexly-written human PNAS summaries. Crucially, Study 3 experimentally demonstrated that participants comprehended scientific writing better after reading simple GPT summaries compared to complex PNAS summaries. In their own words, participants also summarized scientific papers in a more detailed and concrete manner after reading GPT summaries compared to PNAS summaries of the same article. AI has the potential to engage scientific communities and the public via a simple language heuristic, advocating for its integration into scientific dissemination for a more informed society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00706v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David M. Markowitz</dc:creator>
    </item>
    <item>
      <title>Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI</title>
      <link>https://arxiv.org/abs/2408.01959</link>
      <description>arXiv:2408.01959v2 Announce Type: replace-cross 
Abstract: Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we use a hierarchical clustering approach to show that dataset size predicts the extent to which the underlying structure of facial impression bias resembles that of facial impression bias in humans. Finally, we show that Stable Diffusion models employing CLIP as a text encoder learn facial impression biases, and that these biases intersect with racial biases in Stable Diffusion XL-Turbo. While pretrained CLIP models may prove useful for scientific studies of bias, they will also require significant dataset curation when intended for use as general-purpose models in a zero-shot setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01959v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Robert Wolfe, Aayushi Dangol, Alexis Hiniker, Bill Howe</dc:creator>
    </item>
    <item>
      <title>ML-EAT: A Multilevel Embedding Association Test for Interpretable and Transparent Social Science</title>
      <link>https://arxiv.org/abs/2408.01966</link>
      <description>arXiv:2408.01966v2 Announce Type: replace-cross 
Abstract: This research introduces the Multilevel Embedding Association Test (ML-EAT), a method designed for interpretable and transparent measurement of intrinsic bias in language technologies. The ML-EAT addresses issues of ambiguity and difficulty in interpreting the traditional EAT measurement by quantifying bias at three levels of increasing granularity: the differential association between two target concepts with two attribute concepts; the individual effect size of each target concept with two attribute concepts; and the association between each individual target concept and each individual attribute concept. Using the ML-EAT, this research defines a taxonomy of EAT patterns describing the nine possible outcomes of an embedding association test, each of which is associated with a unique EAT-Map, a novel four-quadrant visualization for interpreting the ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2 language models, and a CLIP language-and-image model shows that EAT patterns add otherwise unobservable information about the component biases that make up an EAT; reveal the effects of prompting in zero-shot models; and can also identify situations when cosine similarity is an ineffective metric, rendering an EAT unreliable. Our work contributes a method for rendering bias more observable and interpretable, improving the transparency of computational investigations into human minds and societies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01966v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Robert Wolfe, Alexis Hiniker, Bill Howe</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study</title>
      <link>https://arxiv.org/abs/2408.14438</link>
      <description>arXiv:2408.14438v2 Announce Type: replace-cross 
Abstract: The advent of large language models such as ChatGPT, Gemini, and others has underscored the importance of evaluating their diverse capabilities, ranging from natural language understanding to code generation. However, their performance on spatial tasks has not been comprehensively assessed. This study addresses this gap by introducing a novel multi-task spatial evaluation dataset, designed to systematically explore and compare the performance of several advanced models on spatial tasks. The dataset encompasses twelve distinct task types, including spatial understanding and path planning, each with verified, accurate answers. We evaluated multiple models, including OpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase testing approach. Initially, we conducted zero-shot testing, followed by categorizing the dataset by difficulty and performing prompt tuning tests. Results indicate that gpt-4o achieved the highest overall accuracy in the first phase, with an average of 71.3%. Although moonshot-v1-8k slightly underperformed overall, it surpassed gpt-4o in place name recognition tasks. The study also highlights the impact of prompt strategies on model performance in specific tasks. For example, the Chain-of-Thought (COT) strategy increased gpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot strategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to 76.3%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14438v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liuchang Xu, Shuo Zhao, Qingming Lin, Luyao Chen, Qianqian Luo, Sensen Wu, Xinyue Ye, Hailin Feng, Zhenhong Du</dc:creator>
    </item>
  </channel>
</rss>
