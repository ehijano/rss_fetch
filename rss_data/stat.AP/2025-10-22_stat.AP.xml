<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Oct 2025 01:41:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Finding the Sweet Spot: Optimal Data Augmentation Ratio for Imbalanced Credit Scoring Using ADASYN</title>
      <link>https://arxiv.org/abs/2510.18252</link>
      <description>arXiv:2510.18252v1 Announce Type: new 
Abstract: Credit scoring models face a critical challenge: severe class imbalance, with default rates typically below 10%, which hampers model learning and predictive performance. While synthetic data augmentation techniques such as SMOTE and ADASYN have been proposed to address this issue, the optimal augmentation ratio remains unclear, with practitioners often defaulting to full balancing (1:1 ratio) without empirical justification.
  This study systematically evaluates 10 data augmentation scenarios using the Give Me Some Credit dataset (97,243 observations, 7% default rate), comparing SMOTE, BorderlineSMOTE, and ADASYN at different multiplication factors (1x, 2x, 3x). All models were trained using XGBoost and evaluated on a held-out test set of 29,173 real observations. Statistical significance was assessed using bootstrap testing with 1,000 iterations.
  Key findings reveal that ADASYN with 1x multiplication (doubling the minority class) achieved optimal performance with AUC of 0.6778 and Gini coefficient of 0.3557, representing statistically significant improvements of +0.77% and +3.00% respectively (p = 0.017, bootstrap test). Higher multiplication factors (2x and 3x) resulted in performance degradation, with 3x showing a -0.48% decrease in AUC, suggesting a "law of diminishing returns" for synthetic oversampling. The optimal class imbalance ratio was found to be 6.6:1 (majority:minority), contradicting the common practice of balancing to 1:1.
  This work provides the first empirical evidence of an optimal "sweet spot" for data augmentation in credit scoring, with practical guidelines for industry practitioners and researchers working with imbalanced datasets. While demonstrated on a single representative dataset, the methodology provides a reproducible framework for determining optimal augmentation ratios in other imbalanced domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18252v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis H. Chia</dc:creator>
    </item>
    <item>
      <title>Distributional regression for seasonal data: an application to river flows</title>
      <link>https://arxiv.org/abs/2510.18639</link>
      <description>arXiv:2510.18639v1 Announce Type: new 
Abstract: Risk assessment in casualty insurance, such as flood risk, traditionally relies on extreme-value methods that emphasizes rare events. These approaches are well-suited for characterizing tail risk, but do not capture the broader dynamics of environmental variables such as moderate or frequent loss events. To complement these methods, we propose a modelling framework for estimating the full (daily) distribution of environmental variables as a function of time, that is a distributional version of typical climatological summary statistics, thereby incorporating both seasonal variation and gradual long-term changes. Aside from the time trend, to capture seasonal variation our approach simultaneously estimates the distribution for each instant of the seasonal cycle, without explicitly modelling the temporal dependence present in the data. To do so, we adopt a framework inspired by GAMLSS (Generalized Additive Models for Location, Scale, and Shape), where the parameters of the distribution vary over the seasonal cycle as a function of explanatory variables depending only on the time of year, and not on the past values of the process under study. Ignoring the temporal dependence in the seasonal variation greatly simplifies the modelling but poses inference challenges that we clarify and overcome.
  We apply our framework to daily river flow data from three hydrometric stations along the Fraser River in British Columbia, Canada, and analyse the flood of the Fraser River in early winter of 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18639v1</guid>
      <category>stat.AP</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samuel Perreault, Silvana M. Pesenti, Daniyal Shahzad</dc:creator>
    </item>
    <item>
      <title>Comparison of Simulation-Guided Design to Closed-Form Power Calculations in Planning a Cluster Randomized Trial with Covariate-Constrained Randomization: A Case Study in Rural Chad</title>
      <link>https://arxiv.org/abs/2510.18818</link>
      <description>arXiv:2510.18818v1 Announce Type: new 
Abstract: Current practices for designing cluster-randomized trials (cRCTs) typically rely on closed-form formulas for power calculations. For cRCTs using covariate-constrained randomization, the utility of conventional calculations might be limited, particularly when data is nested. We compared simulation-based planning of a nested cRCT using covariate-constrained randomization to conventional power calculations using OptiMAx-Chad as a case study. OptiMAx-Chad will examine the impact of embedding mass distribution of small-quantity lipid-based nutrient supplements within an expanded programme on immunization on first-dose measles-containing vaccine (MCV1) coverage among children aged 12-24 months in rural villages in Ngouri. Within the 12 health areas to be randomized, a random subset of villages will be selected for outcome collection. 1,000,000 assignments of health areas with different possible village selections were generated using covariate-constrained randomization to balance baseline village characteristics. The empirically estimated intracluster correlation coefficient (ICC) and the World Health Organization (WHO) recommended values of 1/3 and 1/6 were considered. The desired operating characteristics were 80% power at 0.05 one-sided type I error rate. Using conventional calculations target power for a realistic treatment effect could not be achieved with the WHO recommended values. Conventional calculations also showed a plateau in power after a certain cluster size. Our simulations matched the design of OptiMAx-Chad with covariate adjustment and random selection, and showed that power did not plateau. Instead, power increased with increasing cluster size. Planning complex cRCTs with covariate constrained randomization and a multi-nested data structure with conventional closed-form formulas can be misleading. Simulations can improve the planning of cRCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18818v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jay JH Park, Rebecca K. Metcalfe, Nathaniel Dyrkton, Yichen Yan, Shomoita Alam, Kevin Phelan, Ibrahim Sana, Susan Shepherd</dc:creator>
    </item>
    <item>
      <title>A new test for assessing the covariate effect in ROC curves</title>
      <link>https://arxiv.org/abs/2411.17464</link>
      <description>arXiv:2411.17464v1 Announce Type: cross 
Abstract: The ROC curve is a statistical tool that analyses the accuracy of a diagnostic test in which a variable is used to decide whether an individual is healthy or not. Along with that diagnostic variable it is usual to have information of some other covariates. In some situations it is advisable to incorporate that information into the study, as the performance of the ROC curves can be affected by them. Using the covariate-adjusted, the covariate-specific or the pooled ROC curves we discuss how to decide if we can exclude the covariates from our study or not, and the implications this may have in further analyses of the ROC curve. A new test for comparing the covariate-adjusted and the pooled ROC curve is proposed, and the problem is illustrated by analysing a real database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17464v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ar\'is Fanjul-Hevia, Juan Carlos Pardo-Fern\'andez, Wenceslao Gonz\'alez-Manteiga</dc:creator>
    </item>
    <item>
      <title>Accelerating Bayesian Inference via Multi-Fidelity Transport Map Coupling</title>
      <link>https://arxiv.org/abs/2510.17946</link>
      <description>arXiv:2510.17946v1 Announce Type: cross 
Abstract: Mathematical models in computational physics contain uncertain parameters that impact prediction accuracy. In turbulence modeling, this challenge is especially significant: Reynolds averaged Navier-Stokes (RANS) models, such as the Spalart-Allmaras (SA) model, are widely used for their speed and robustness but often suffer from inaccuracies and associated uncertainties due to imperfect model parameters. Reliable quantification of these uncertainties is becoming increasingly important in aircraft certification by analysis, where predictive credibility is critical. Bayesian inference provides a framework to estimate these parameters and quantify output uncertainty, but traditional methods are prohibitively expensive, especially when relying on high-fidelity simulations. We address the challenge of expensive Bayesian parameter estimation by developing a multi-fidelity framework that combines Markov chain Monte Carlo (MCMC) methods with multilevel Monte Carlo (MLMC) estimators to efficiently solve inverse problems. The MLMC approach requires correlated samples across different fidelity levels, achieved through a novel transport map-based coupling algorithm. We demonstrate a 50% reduction in inference cost compared to traditional single-fidelity methods on the challenging NACA0012 airfoil at high angles of attack near stall, while delivering realistic uncertainty bounds for model predictions in complex separated flow regimes. These results demonstrate that multi-fidelity approaches significantly improve turbulence parameter calibration, paving the way for more accurate and efficient aircraft certification by analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17946v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanjan C. Muchandimath, Joaquim R. R. A. Martins, Alex A. Gorodetsky</dc:creator>
    </item>
    <item>
      <title>On the Kolmogorov Distance of Max-Stable Distributions</title>
      <link>https://arxiv.org/abs/2510.18094</link>
      <description>arXiv:2510.18094v1 Announce Type: cross 
Abstract: In this contribution, we derive explicit bounds on the Kolmogorov distance for multivariate max-stable distributions with Fr\'echet margins. We formulate those bounds in terms of (i) Wasserstein distances between de Haan representers, (ii) total variation distances between spectral/angular measures - removing the dimension factor from earlier results in the canonical sphere case - and (iii) discrepancies of the Psi-functions in the inf-argmax decomposition. Extensions to different margins and Archimax/clustered Archimax copulas are further discussed. Examples include logistic, comonotonic, independent and Brown-Resnick models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18094v1</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enkelejd Hashorva</dc:creator>
    </item>
    <item>
      <title>Interval Prediction of Annual Average Daily Traffic on Local Roads via Quantile Random Forest with High-Dimensional Spatial Data</title>
      <link>https://arxiv.org/abs/2510.18548</link>
      <description>arXiv:2510.18548v1 Announce Type: cross 
Abstract: Accurate annual average daily traffic (AADT) data are vital for transport planning and infrastructure management. However, automatic traffic detectors across national road networks often provide incomplete coverage, leading to underrepresentation of minor roads. While recent machine learning advances have improved AADT estimation at unmeasured locations, most models produce only point predictions and overlook estimation uncertainty. This study addresses that gap by introducing an interval prediction approach that explicitly quantifies predictive uncertainty. We integrate a Quantile Random Forest model with Principal Component Analysis to generate AADT prediction intervals, providing plausible traffic ranges bounded by estimated minima and maxima. Using data from over 2,000 minor roads in England and Wales, and evaluated with specialized interval metrics, the proposed method achieves an interval coverage probability of 88.22%, a normalized average width of 0.23, and a Winkler Score of 7,468.47. By combining machine learning with spatial and high-dimensional analysis, this framework enhances both the accuracy and interpretability of AADT estimation, supporting more robust and informed transport planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18548v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Yao, Daniel J. Graham</dc:creator>
    </item>
    <item>
      <title>Designing a Circular Economy Network for PPE Masks Supply Chain: A Case Study of British Columbia, Canada</title>
      <link>https://arxiv.org/abs/2510.18735</link>
      <description>arXiv:2510.18735v1 Announce Type: cross 
Abstract: In recent years, there has been growing interest in building closed-loop supply chains (SCs). However, many of the current methods struggle when it comes to fully embracing circular economy principles. Enhancing the design and management of these networks holds significant potential to promote stronger collaboration among supply chain partners, ultimately fostering more sustainable and efficient operational practices. For this purpose, this study addresses a new circular economy model based on a real case study of a mask SC in the healthcare sector in British Columbia, Canada. The objective is to show that implementing circular practices can lead to considerable financial and environmental benefits, as well as the creation of new job opportunities. To achieve this, a multi-objective mixed-integer linear programming model is developed to identify the most efficient trade-off among sustainable objectives, while adhering to imposed constraints. The proposed closed-loop SC model outperforms the existing linear model in all three aspects of sustainability, namely economic, environmental and social. The improvement leads to significant economic and environmental benefits by preventing the disposal of used masks, giving them a second chance for disinfection and reprocessing, and reintroducing them into the SC as new ones. While the results of the circular economy model demonstrate profit gains and environmental recovery, the linear SC model showed negative profit with higher carbon emissions. Regarding the social aspects, compared to the current system, our approach not only nearly doubles the number of jobs created but also significantly reduces shortages, highlighting sustainable development aspects related to equity and social welfare. The findings offer valuable insights for researchers and practitioners seeking to implement sustainability within a circular economy framework in SCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18735v1</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jainil Dharmil Shah, Behrooz Khorshidvand, Niloofar Gilani Larimi, Adel Guitouni</dc:creator>
    </item>
    <item>
      <title>Modelling the Spatially Varying Non-Linear Effects of Heat Exposure</title>
      <link>https://arxiv.org/abs/2502.20745</link>
      <description>arXiv:2502.20745v2 Announce Type: replace 
Abstract: Exposure to high ambient temperatures is a significant driver of preventable mortality, with non-linear health effects and elevated risks in specific regions. To capture this complexity and account for spatial dependencies across small areas, we propose a Bayesian framework that integrates non-linear functions with the Besag, York, and Mollie (BYM2) model. Applying this framework to all-cause mortality data in Switzerland, we quantified spatial inequalities in heat-related mortality. We retrieved daily all-cause mortality at small areas (2,145 municipalities) for people older than 65 years from the Swiss Federal Office of Public Health and daily mean temperature at 1km$\times$1km grid from the Swiss Federal Office of Meteorology. By fully propagating uncertainties, we derived key epidemiological metrics, including heat-related excess mortality and minimum mortality temperature (MMT). Heat-related excess mortality rates were higher in northern Switzerland, while lower MMTs were observed in mountainous regions. Further, we explored the role of the proportion of individuals older than 85 years, green space, average temperature, deprivation, urbanicity, air pollution, and language regions in explaining these discrepancies. We found that spatial disparities in heat-related excess mortality were primarily driven by population age distribution, green space, and vulnerabilities associated with elevated temperature exposure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20745v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Chen, Marta Blangiardo, Connor Gascoigne, Garyfallos Konstantinoudis</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification of a multi-component Hall thruster model at varying facility pressures</title>
      <link>https://arxiv.org/abs/2507.08113</link>
      <description>arXiv:2507.08113v2 Announce Type: replace 
Abstract: Bayesian inference is applied to calibrate and quantify prediction uncertainty in a coupled multi-component Hall thruster model. The model consists of cathode, discharge, and plume sub-models and outputs thruster performance metrics, one-dimensional plasma properties, and the angular distribution of the current density in the plume. The simulated thrusters include a magnetically shielded thruster operating on krypton, the H9, and an unshielded thruster operating on xenon, the SPT-100, at pressures between 4.3--43 $\mu$Torr-Kr and 1.7--80 $\mu$Torr-Xe, respectively. After calibration, the model captures key pressure-related trends, including changes in thrust and upstream shifts in the ion acceleration region. Furthermore, the model exhibits predictive accuracy to within 10\% when evaluated on flow rates and pressures not included in the training data, and can predict some performance characteristics across test facilities to within the same range of conditions. Compared to a previous model calibrated on some of the same data [Eckels et al. 2024], the model reduced predictive errors in thrust and discharge current by greater than 50%. An extrapolation to on-orbit performance is performed with an error of 9%, capturing trends in discharge current but not thrust. These findings are discussed in the context of using data for predictive Hall thruster modeling in the presence of facility effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08113v2</guid>
      <category>stat.AP</category>
      <category>physics.comp-ph</category>
      <category>physics.plasm-ph</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0283796</arxiv:DOI>
      <arxiv:journal_reference>J. Appl. Phys. 138, 153305 (2025)</arxiv:journal_reference>
      <dc:creator>Thomas A. Marks, Joshua D. Eckels, Gabriel E. Mora, Alex A. Gorodetsky</dc:creator>
    </item>
    <item>
      <title>A Spatio-temporal CP decomposition analysis of New England region in the US</title>
      <link>https://arxiv.org/abs/2510.10322</link>
      <description>arXiv:2510.10322v2 Announce Type: replace 
Abstract: Spatio temporal data consist of measurement for one or more raster fields such as weather, traffic volume, crime rate, or disease incidents. Advances in modern technology have increased the number of available information for this type of data hence the rise of multidimensional data. In this paper we take advantage of the multidimensional structure of the data but also its temporal and spatial structure. In fact, we will be using the NCAR Climate Data Gateway website which provides data discovery and access services for global and regional climate model data. The daily values of total precipitation (prec), maximum (tmax), and minimum (tmin) temperature are combined to create a multidimensional data called tensor (a multidimensional array). In this paper, we propose a spatio temporal principal component analysis to initialize CP decomposition component. We take full advantage of the spatial and temporal structure of the data in the initialization step for cp component analysis. The performance of our method is tested via comparison with most popular initialization method. We also run a clustering analysis to further show the performance of our analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10322v2</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatoumata Sanogo</dc:creator>
    </item>
    <item>
      <title>On minimal predictable intensity of point processes</title>
      <link>https://arxiv.org/abs/2407.21651</link>
      <description>arXiv:2407.21651v3 Announce Type: replace-cross 
Abstract: An adapted, right-continuous, non-decreasing, integer-valued process with unit jumps and starting at zero has a minimal predictable intensity if and only if it is a standard Poisson process under an absolutely continuous transformation of measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21651v3</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoming Wang</dc:creator>
    </item>
  </channel>
</rss>
