<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Aug 2025 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Worst-case Nonparametric Bounds for the Student T-statistic</title>
      <link>https://arxiv.org/abs/2508.13226</link>
      <description>arXiv:2508.13226v1 Announce Type: new 
Abstract: We address the problem of finding worst-case nonparametric bounds for T-statistic bythe considering the extremal problem of maximising $\mathbb{P}(S(w)\ge t)$ over nonnegative weight vectors $w\in\mathbb{R}^n$ with $\|w\|_2=1$, where $S(w)=\sum_{i=1}^n w_i \varepsilon_i$ and $\varepsilon_i$ are independent Rademacher variables. While classical results of Hoeffding [1] and Chernoff [2] provide sub-Gaussian upper bounds, and optimal-order inequalities were later obtained by the author [3,4], the associated extremal problem has remained unsolved. We resolve this problem exactly: for each $t&gt;0$ and each $n$, we determine the maximal value and characterise all maximising weights. The maximisers are $k$-sparse equal-weight vectors with weights $1/\sqrt{k}$, and the optimal support size $k$ is found by a finite search over at most $n$ candidates. This yields an explicit envelope $M_n(t)$ and its universal limit as $n$ grows. Our results provide exact solutions to problems that have been studied through bounds and approximations for over sixty years, with applications to nonparametric inference, self-standardised statistics, and robust hypothesis testing under symmetry assumptions</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13226v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Edelman</dc:creator>
    </item>
    <item>
      <title>Conditionally specified graphical modeling of stationary multivariate time series</title>
      <link>https://arxiv.org/abs/2508.13572</link>
      <description>arXiv:2508.13572v1 Announce Type: new 
Abstract: Graphical models are ubiquitous for summarizing conditional relations in multivariate data. In many applications involving multivariate time series, it is of interest to learn an interaction graph that treats each individual time series as nodes of the graph, with the presence of an edge between two nodes signifying conditional dependence given the others. Typically, the partial covariance is used as a measure of conditional dependence. However, in many applications, the outcomes may not be Gaussian and/or could be a mixture of different outcomes. For such time series using the partial covariance as a measure of conditional dependence may be restrictive. In this article, we propose a broad class of time series models which are specifically designed to succinctly encode process-wide conditional independence in its parameters. For each univariate component in the time series, we model its conditional distribution with a distribution from the exponential family. We develop a notion of process-wide compatibility under which such conditional specifications can be stitched together to form a well-defined strictly stationary multivariate time series. We call this construction a conditionally exponential stationary graphical model ({\it CEStGM}). A central quantity underlying CEStGM is a positive kernel which we call the interaction kernel. Spectral properties of such positive kernel operators constitute a core technical foundation of this work. We establish process-wide local and global Markov properties of CEStGM exploiting a Hammersley-Clifford type decomposition of the interaction kernel. Further, we study various probabilistic properties of CEStGM and show that it is geometrically mixing. An approximate Gibbs sampler is also developed to simulate sample paths of CEStGM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13572v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Bhattacharya, Jan Johannes, Suhasini Subba Rao</dc:creator>
    </item>
    <item>
      <title>Random positive linear operators and their applications to nonparametric statistics</title>
      <link>https://arxiv.org/abs/2508.13931</link>
      <description>arXiv:2508.13931v1 Announce Type: new 
Abstract: We outline a general procedure on how to apply random positive linear operators in nonparametric estimation. As a consequence, we give explicit confidence bands and intervals for a distribution function $F$ concentrated on $[0,1]$ by means of random Bernstein polynomials, and for the derivatives of $F$ by using random Bernstein-Kantorovich type operators. In each case, the lengths of such bands and intervals depend upon the degree of smoothness of $F$ or its corresponding derivatives, measured in terms of appropriate moduli of smoothness. In particular, we estimate the uniform distribution function by means of a random polynomial of second order. This estimator is much simpler and performs better than the classical uniform empirical process used in the celebrated Dvoretzky-Kiefer-Wolfowitz inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13931v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e A. Adell, J. T. Alcal\'a, C. Sang\"uesa</dc:creator>
    </item>
    <item>
      <title>Convergence Rates for Realizations of Gaussian Random Variables</title>
      <link>https://arxiv.org/abs/2508.13940</link>
      <description>arXiv:2508.13940v1 Announce Type: new 
Abstract: This paper investigates the approximation of Gaussian random variables in Banach spaces, focusing on the high-probability bounds for the approximation of Gaussian random variables using finitely many observations. We derive non-asymptotic error bounds for the approximation of a Gaussian process $ X $ by its conditional expectation, given finitely many linear functionals. Specifically, we quantify the difference between the covariance of $ X $ and its finite-dimensional approximation, establishing a direct relationship between the quality of the covariance approximation and the convergence of the process in the Banach space norm. Our approach avoids the reliance on spectral methods or eigenfunction expansions commonly used in Hilbert space settings, and instead uses finite, linear observations. This makes our result particularly suitable for practical applications in nonparametric statistics, machine learning, and Bayesian inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13940v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Winkle, Ingo Steinwart, Bernard Haasdonk</dc:creator>
    </item>
    <item>
      <title>Towards multi-purpose locally differentially-private synthetic data release via spline wavelet plug-in estimation</title>
      <link>https://arxiv.org/abs/2508.13969</link>
      <description>arXiv:2508.13969v1 Announce Type: new 
Abstract: We develop plug-in estimators for locally differentially private semi-parametric estimation via spline wavelets. The approach leads to optimal rates of convergence for a large class of estimation problems that are characterized by (differentiable) functionals $\Lambda(f)$ of the true data generating density $f$. The crucial feature of the locally private data $Z_1,\dots, Z_n$ we generate is that it does not depend on the particular functional $\Lambda$ (or the unknown density $f$) the analyst wants to estimate. Hence, the synthetic data can be generated and stored a priori and can subsequently be used by any number of analysts to estimate many vastly different functionals of interest at the provably optimal rate. In principle, this removes a long standing practical limitation in statistics of differential privacy, namely, that optimal privacy mechanisms need to be tailored towards the specific estimation problem at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13969v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibault Randrianarisoa, Lukas Steinberger, Botond Szab\'o</dc:creator>
    </item>
    <item>
      <title>Structural Foundations for Leading Digit Laws: Beyond Probabilistic Mixtures</title>
      <link>https://arxiv.org/abs/2508.13237</link>
      <description>arXiv:2508.13237v1 Announce Type: cross 
Abstract: This article presents a modern deterministic framework for the study of leading significant digit distributions in numerical data. Rather than relying on traditional probabilistic or mixture-based explanations, we demonstrate that the observed frequencies of leading digits are determined by the underlying arithmetic, algorithmic, and structural properties of the data-generating process. Our approach centers on a shift-invariant functional equation, whose general solution is given by explicit affine-plus-periodic formulas. This structural formulation explains the diversity of digit distributions encountered in both empirical and mathematical datasets, including cases with pronounced deviations from logarithmic or scale-invariant profiles.
  We systematically analyze digit distributions in finite and infinite datasets, address deterministic sequences such as prime numbers and recurrence relations, and highlight the emergence of block-structured and fractal features. The article provides critical examination of probabilistic models, explicit examples and counterexamples, and discusses limitations and open problems for further research. Overall, this work establishes a unified mathematical foundation for digital phenomena and offers a versatile toolset for modeling and analyzing digit patterns in applied and theoretical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13237v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Berman</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Subgraph Frequencies of Exchangeable Hyperedge Models</title>
      <link>https://arxiv.org/abs/2508.13258</link>
      <description>arXiv:2508.13258v1 Announce Type: cross 
Abstract: In statistical network analysis, models for binary adjacency matrices satisfying vertex exchangeability are commonly used. However, such models may fail to capture key features of the data-generating process when interactions, rather than nodes, are fundamental units. We study statistical inference for subgraph counts under an exchangeable hyperedge model. We introduce several classes of subgraph statistics for hypergraphs and develop inferential tools for subgraph frequencies that account for edge multiplicity. We show that a subclass of these subgraph statistics is robust to the deletion of low-degree nodes, enabling inference in settings where low-degree nodes are more likely to be missing. We also examine a more traditional notion of subgraph frequency that ignores multiplicity, showing that while inference based on limiting distributions is feasible in some cases, a non-degenerate limiting distribution may not exist in others. Empirically, we assess our methods through simulations and newly collected real-world hypergraph data on academic and movie collaborations, where our inferential tools outperform traditional approaches based on binary adjacency matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13258v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayoushman Bhattacharya, Nilanjan Chakraborty, Robert Lunde</dc:creator>
    </item>
    <item>
      <title>Dimension lower bounds for linear approaches to function approximation</title>
      <link>https://arxiv.org/abs/2508.13346</link>
      <description>arXiv:2508.13346v1 Announce Type: cross 
Abstract: This short note presents a linear algebraic approach to proving dimension lower bounds for linear methods that solve $L^2$ function approximation problems. The basic argument has appeared in the literature before (e.g., Barron, 1993) for establishing lower bounds on Kolmogorov $n$-widths. The argument is applied to give sample size lower bounds for kernel methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13346v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Hsu</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Multi-order Tensor Factor Models</title>
      <link>https://arxiv.org/abs/2508.13418</link>
      <description>arXiv:2508.13418v1 Announce Type: cross 
Abstract: We propose a novel framework in high-dimensional factor models to simultaneously analyze multiple tensor time series, each with potentially different tensor orders and dimensionality. The connection between different tensor time series is through their global factors that are correlated to each other. A salient feature of our model is that when all tensor time series have the same order, it can be regarded as an extension of multilevel factor models from vectors to general tensors. Under very mild conditions, we separate the global and local components in the proposed model. Parameter estimation is thoroughly discussed. With strong correlation between global factors and noise allowed, we derive the rates of convergence of our estimators, which can be more superior than those of existing methods for multilevel factor models. We also develop estimators that are more computationally efficient, with rates of convergence spelt out. Extensive experiments are performed under various settings, corroborating with the pronounced theoretical results. As a real application example, we analyze a set of taxi data to study the traffic flow between Times Squares and its neighboring areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13418v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen</dc:creator>
    </item>
    <item>
      <title>Partial Identification of Causal Effects for Endogenous Continuous Treatments</title>
      <link>https://arxiv.org/abs/2508.13946</link>
      <description>arXiv:2508.13946v1 Announce Type: cross 
Abstract: No unmeasured confounding is a common assumption when reasoning about counterfactual outcomes, but such an assumption may not be plausible in observational studies. Sensitivity analysis is often employed to assess the robustness of causal conclusions to unmeasured confounding, but existing methods are predominantly designed for binary treatments. In this paper, we provide natural extensions of two extensively used sensitivity frameworks -- the Rosenbaum and Marginal sensitivity models -- to the setting of continuous exposures. Our generalization replaces scalar sensitivity parameters with sensitivity functions that vary with exposure level, enabling richer modeling and sharper identification bounds. We develop a unified pseudo-outcome regression formulation for bounding the counterfactual dose-response curve under both models, and propose corresponding nonparametric estimators which have second order bias. These estimators accommodate modern machine learning methods for obtaining nuisance parameter estimators, which are shown to achieve $L^2$- consistency, minimax rates of convergence under suitable conditions. Our resulting estimators of bounds for the counterfactual dose-response curve are shown to be consistent and asymptotic normal allowing for a user-specified bound on the degree of uncontrolled exposure endogeneity. We also offer a geometric interpretation that relates the Rosenbaum and Marginal sensitivity model and guides their practical usage in global versus targeted sensitivity analysis. The methods are validated through simulations and a real-data application on the effect of second-hand smoke exposure on blood lead levels in children.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13946v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinandan Dalal, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>A PC Algorithm for Max-Linear Bayesian Networks</title>
      <link>https://arxiv.org/abs/2508.13967</link>
      <description>arXiv:2508.13967v1 Announce Type: cross 
Abstract: Max-linear Bayesian networks (MLBNs) are a relatively recent class of structural equation models which arise when the random variables involved have heavy-tailed distributions. Unlike most directed graphical models, MLBNs are typically not faithful to d-separation and thus classical causal discovery algorithms such as the PC algorithm or greedy equivalence search can not be used to accurately recover the true graph structure. In this paper, we begin the study of constraint-based discovery algorithms for MLBNs given an oracle for testing conditional independence in the true, unknown graph. We show that if the oracle is given by the $\ast$-separation criteria in the true graph, then the PC algorithm remains consistent despite the presence of additional CI statements implied by $\ast$-separation. We also introduce a new causal discovery algorithm named "PCstar" which assumes faithfulness to $C^\ast$-separation and is able to orient additional edges which cannot be oriented with only d- or $\ast$-separation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13967v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Am\'endola, Benjamin Hollering, Francesco Nowell</dc:creator>
    </item>
    <item>
      <title>Simultaneous inference for monotone and smoothly time-varying functions under complex temporal dynamics</title>
      <link>https://arxiv.org/abs/2310.02177</link>
      <description>arXiv:2310.02177v4 Announce Type: replace 
Abstract: We propose a new framework for the simultaneous inference of monotone and smoothly time-varying functions under complex temporal dynamics. This will be done utilizing the monotone rearrangement and the nonparametric estimation. We capitalize the Gaussian approximation for the nonparametric monotone estimator and construct the asymptotically correct simultaneous confidence bands (SCBs) using designed bootstrap methods. We investigate two general and practical scenarios. The first is the simultaneous inference of monotone smooth trends from moderately high-dimensional time series. The proposed algorithm has been employed for the joint inference of temperature curves from multiple areas. Specifically, most existing methods are designed for a single monotone smooth trend. In such cases, our proposed SCB empirically exhibits the narrowest width among existing approaches while maintaining confidence levels. It has also been used for testing several hypotheses tailored to global warming. The second scenario involves simultaneous inference of monotone and smoothly time-varying regression coefficients in time-varying coefficient linear models. The proposed algorithm has been utilized for testing the impact of sunshine duration on temperature which is believed to be increasing due to severe greenhouse effect. The validity of the proposed methods has been justified in theory as well as by extensive simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02177v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianpai Luo, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Rate Optimality and Phase Transition for User-Level Local Differential Privacy</title>
      <link>https://arxiv.org/abs/2405.11923</link>
      <description>arXiv:2405.11923v2 Announce Type: replace 
Abstract: Most of the literature on differential privacy considers the item-level case where each user has a single observation, but a growing field of interest is that of user-level privacy where each of the $n$ users holds $T$ observations and wishes to maintain the privacy of their entire collection.
  In this paper, we derive a general minimax lower bound, which shows that, for locally private user-level estimation problems, the risk cannot, in general, be made to vanish for a fixed number of users even when each user holds an arbitrarily large number of observations. We then derive matching, up to logarithmic factors, lower and upper bounds for univariate and multidimensional mean estimation, sparse mean estimation and non-parametric density estimation. In particular, with other model parameters held fixed, we observe phase transition phenomena in the minimax rates as $T$ the number of observations each user holds varies.
  In the case of (non-sparse) mean estimation and density estimation, we see that, for $T$ below a phase transition boundary, the rate is the same as having $nT$ users in the item-level setting. Different behaviour is however observed in the case of $s$-sparse $d$-dimensional mean estimation, wherein consistent estimation is impossible when $d$ exceeds the number of observations in the item-level setting, but is possible in the user-level setting when $T \gtrsim s \log (d)$, up to logarithmic factors. This may be of independent interest for applications as an example of a high-dimensional problem that is feasible under local privacy constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11923v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Kent, Thomas B. Berrett, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Huber means on Riemannian manifolds</title>
      <link>https://arxiv.org/abs/2407.15764</link>
      <description>arXiv:2407.15764v2 Announce Type: replace 
Abstract: This article introduces Huber means on Riemannian manifolds, providing a robust alternative to the Frechet mean by integrating elements of both square and absolute loss functions. The Huber means are designed to be highly resistant to outliers while maintaining efficiency, making it a valuable generalization of Huber's M-estimator for manifold-valued data. We comprehensively investigate the statistical and computational aspects of Huber means, demonstrating their utility in manifold-valued data analysis. Specifically, we establish nearly minimal conditions for ensuring the existence and uniqueness of the Huber mean and discuss regularity conditions for unbiasedness. The Huber means are consistent and enjoy the central limit theorem. Additionally, we propose a novel moment-based estimator for the limiting covariance matrix, which is used to construct a robust one-sample location test procedure and an approximate confidence region for location parameters. The Huber mean is shown to be highly robust and efficient in the presence of outliers or under heavy-tailed distributions. Specifically, it achieves a breakdown point of at least 0.5, the highest among all isometric equivariant estimators, and is more efficient than the Frechet mean under heavy-tailed distributions. Numerical examples on spheres and the space of symmetric positive-definite matrices further illustrate the efficiency and reliability of the proposed Huber means on Riemannian manifolds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15764v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jongmin Lee, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>Parallel Network Reconstruction with Multi-directional Regularization</title>
      <link>https://arxiv.org/abs/2411.11464</link>
      <description>arXiv:2411.11464v2 Announce Type: replace 
Abstract: Reconstructing large-scale latent networks from observed dynamics is crucial for understanding complex systems. However, the existing methods based on compressive sensing are often rendered infeasible in practice by prohibitive computational and memory costs. To address this challenge, we introduce a new distributed computing framework for efficient large-scale network reconstruction with parallel computing, namely PALMS (Parallel Adaptive Lasso with Multi-directional Signals). The core idea of PALMS is to decompose the complex global problem by partitioning network nodes, enabling the parallel estimation of sub-networks across multiple computing units. This strategy substantially reduces the computational complexity and storage requirements of classic methods. By using the adaptive multi-directional regularization on each computing unit, we also establish the consistency of PALMS estimator theoretically. Extensive simulation studies and empirical analyses on several large-scale real-world networks validate the computational efficiency and robust reconstruction accuracy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11464v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoyu Xing, Wei Zhong</dc:creator>
    </item>
    <item>
      <title>Bayesian Analysis of Spiked Covariance Models: Correcting Eigenvalue Bias and Determining the Number of Spikes</title>
      <link>https://arxiv.org/abs/2412.10753</link>
      <description>arXiv:2412.10753v2 Announce Type: replace 
Abstract: We study Bayesian inference in the spiked covariance model, where a small number of spiked eigenvalues dominate the spectrum. Our goal is to infer the spiked eigenvalues, their corresponding eigenvectors, and the number of spikes, providing a Bayesian solution to principal component analysis with uncertainty quantification. We place an inverse-Wishart prior on the covariance matrix to derive posterior distributions for the spiked eigenvalues and eigenvectors. Although posterior sampling is computationally efficient due to conjugacy, a bias may exist in the posterior eigenvalue estimates under high-dimensional settings. To address this, we propose two bias correction strategies: (i) a hyperparameter adjustment method, and (ii) a post-hoc multiplicative correction. For inferring the number of spikes, we develop a BIC-type approximation to the marginal likelihood and prove posterior consistency in the high-dimensional regime $p&gt;n$. Furthermore, we establish concentration inequalities and posterior contraction rates for the leading eigenstructure, demonstrating minimax optimality for the spiked eigenvector in the single-spike case. Simulation studies and a real data application show that our method performs better than existing approaches in providing accurate quantification of uncertainty for both eigenstructure estimation and estimation of the number of spikes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10753v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangmin Lee, Sewon Park, Seongmin Kim, Jaeyong Lee</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient Descent</title>
      <link>https://arxiv.org/abs/2502.06719</link>
      <description>arXiv:2502.06719v2 Announce Type: replace-cross 
Abstract: In this paper, we establish the non-asymptotic validity of the multiplier bootstrap procedure for constructing the confidence sets using the Stochastic Gradient Descent (SGD) algorithm. Under appropriate regularity conditions, our approach avoids the need to approximate the limiting covariance of Polyak-Ruppert SGD iterates, which allows us to derive approximation rates in convex distance of order up to $1/\sqrt{n}$. Notably, this rate can be faster than the one that can be proven in the Polyak-Juditsky central limit theorem. To our knowledge, this provides the first fully non-asymptotic bound on the accuracy of bootstrap approximations in SGD algorithms. Our analysis builds on the Gaussian approximation results for nonlinear statistics of independent random variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06719v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marina Sheshukova, Sergey Samsonov, Denis Belomestny, Eric Moulines, Qi-Man Shao, Zhuo-Song Zhang, Alexey Naumov</dc:creator>
    </item>
    <item>
      <title>Rectifying Conformity Scores for Better Conditional Coverage</title>
      <link>https://arxiv.org/abs/2502.16336</link>
      <description>arXiv:2502.16336v2 Announce Type: replace-cross 
Abstract: We present a new method for generating confidence sets within the split conformal prediction framework. Our method performs a trainable transformation of any given conformity score to improve conditional coverage while ensuring exact marginal coverage. The transformation is based on an estimate of the conditional quantile of conformity scores. The resulting method is particularly beneficial for constructing adaptive confidence sets in multi-output problems where standard conformal quantile regression approaches have limited applicability. We develop a theoretical bound that captures the influence of the accuracy of the quantile estimate on the approximate conditional validity, unlike classical bounds for conformal prediction methods that only offer marginal coverage. We experimentally show that our method is highly adaptive to the local data structure and outperforms existing methods in terms of conditional coverage, improving the reliability of statistical inference in various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16336v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Plassier, Alexander Fishkov, Victor Dheur, Mohsen Guizani, Souhaib Ben Taieb, Maxim Panov, Eric Moulines</dc:creator>
    </item>
  </channel>
</rss>
