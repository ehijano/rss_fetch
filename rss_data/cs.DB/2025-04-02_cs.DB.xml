<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Apr 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>ACTIVE: Continuous Similarity Search for Vessel Trajectories</title>
      <link>https://arxiv.org/abs/2504.01142</link>
      <description>arXiv:2504.01142v1 Announce Type: new 
Abstract: Publicly available vessel trajectory data is emitted continuously from the global AIS system. Continuous trajectory similarity search on this data has applications in, e.g., maritime navigation and safety. Existing proposals typically assume an offline setting and focus on finding similarities between complete trajectories. Such proposals are less effective when applied to online scenarios, where similarity comparisons must be performed continuously as new trajectory data arrives and trajectories evolve. We therefore propose a real-time continuous trajectory similarity search method for vessels (ACTIVE). We introduce a novel similarity measure, object-trajectory real-time distance, that emphasizes the anticipated future movement trends of vessels, enabling more predictive and forward-looking comparisons. Next, we propose a segment-based vessel trajectory index structure that organizes historical trajectories into smaller and manageable segments, facilitating accelerated similarity computations. Leveraging this index, we propose an efficient continuous similar trajectory search (CSTS) algorithm together with a variety of search space pruning strategies that reduce unnecessary computations during the continuous similarity search, thereby further improving efficiency. Extensive experiments on two large real-world AIS datasets offer evidence that ACTIVE is capable of outperforming state-of-the-art methods considerably. ACTIVE significantly reduces index construction costs and index size while achieving a 70% reduction in terms of query time and a 60% increase in terms of hit rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01142v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tiantian Liu, Hengyu Liu, Tianyi Li, Kristian Torp, Christian S. Jensen</dc:creator>
    </item>
    <item>
      <title>Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB</title>
      <link>https://arxiv.org/abs/2504.01157</link>
      <description>arXiv:2504.01157v1 Announce Type: new 
Abstract: Knowledge-intensive analytical applications retrieve context from both structured tabular data and unstructured, text-free documents for effective decision-making. Large language models (LLMs) have made it significantly easier to prototype such retrieval and reasoning data pipelines. However, implementing these pipelines efficiently still demands significant effort and has several challenges. This often involves orchestrating heterogeneous data systems, managing data movement, and handling low-level implementation details, e.g., LLM context management.
  To address these challenges, we introduce FlockMTL: an extension for DBMSs that deeply integrates LLM capabilities and retrieval-augmented generation (RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling chained predictions through tuple-level mappings and reductions. Drawing inspiration from the relational model, FlockMTL incorporates: (i) cost-based optimizations, which seamlessly apply techniques such as batching and caching; and (ii) resource independence, enabled through novel SQL DDL abstractions: PROMPT and MODEL, introduced as first-class schema objects alongside TABLE. FlockMTL streamlines the development of knowledge-intensive analytical applications, and its optimizations ease the implementation burden.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01157v1</guid>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anas Dorbani, Sunny Yasser, Jimmy Lin, Amine Mhedhbi</dc:creator>
    </item>
    <item>
      <title>SOLAR: Scalable Distributed Spatial Joins through Learning-based Optimization</title>
      <link>https://arxiv.org/abs/2504.01292</link>
      <description>arXiv:2504.01292v1 Announce Type: new 
Abstract: The proliferation of location-based services has led to massive spatial data generation. Spatial join is a crucial database operation that identifies pairs of objects from two spatial datasets based on spatial relationships. Due to the intensive computational demands, spatial joins are often executed in a distributed manner across clusters. However, current systems fail to recognize similarities in the partitioning of spatial data, leading to redundant computations and increased overhead. Recently, incorporating machine learning optimizations into database operations has enhanced efficiency in traditional joins by predicting optimal strategies. However, applying these optimizations to spatial joins poses challenges due to the complex nature of spatial relationships and the variability of spatial data. This paper introduces SOLAR, scalable distributed spatial joins through learning-based optimization. SOLAR operates through offline and online phases. In the offline phase, it learns balanced spatial partitioning based on the similarities between datasets in query workloads seen so far. In the online phase, when a new join query is received, SOLAR evaluates the similarity between the datasets in the new query and the already-seen workloads using the trained learning model. Then, it decides to either reuse an existing partitioner, avoiding unnecessary computational overhead, or partition from scratch. Our extensive experimental evaluation on real-world datasets demonstrates that SOLAR achieves up to 3.6X speedup in overall join runtime and 2.71X speedup in partitioning time compared to state-of-the-art systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01292v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongyi Liu, Ahmed Mahmood, Amr Magdy, Minyao Zhu</dc:creator>
    </item>
    <item>
      <title>K2: On Optimizing Distributed Transactions in a Multi-region Data Store with TrueTime Clocks (Extended Version)</title>
      <link>https://arxiv.org/abs/2504.01460</link>
      <description>arXiv:2504.01460v1 Announce Type: new 
Abstract: TrueTime clocks (TTCs) that offer accurate and reliable time within limited uncertainty bounds have been increasingly implemented in many clouds. Multi-region data stores that seek decentralized synchronization for high performance represent an ideal application of TTC. However, the co-designs between the two were often undervalued or failed to realize their full potential.
  This paper proposes K2, a multi-region data store that intensely explores the opportunity of using TTC for distributed transactions. Compared to its pioneer, Google Spanner, K2 augments TTC's semantics in three core design pillars. First, K2 carries a new timestamp-generating scheme that is capable of providing a small time uncertainty bound at scale. Second, K2 revitalizes existing multi-version timestamp-ordered concurrency control to realize multi-version properties for read-write transactions. Third, K2 introduces a new TTC-based visibility control protocol that provides efficient reads at replicas. Our evaluation shows that, K2 achieves an order of magnitude higher transaction throughput relative to other practical geo-distributed transaction protocols while ensuring a lower visibility delay at asynchronous replicas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01460v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haoze Song, Yongqi Wang, Xusheng Chen, Hao Feng, Yazhi Feng, Xieyun Fang, Heming Cui, Linghe Kong</dc:creator>
    </item>
    <item>
      <title>Online Timestamp-based Transactional Isolation Checking of Database Systems (Extended Version)</title>
      <link>https://arxiv.org/abs/2504.01477</link>
      <description>arXiv:2504.01477v1 Announce Type: new 
Abstract: Serializability (SER) and snapshot isolation (SI) are widely used transactional isolation levels in database systems. The isolation checking problem asks whether a given execution history of a database system satisfies a specified isolation level. However, existing SER and SI checkers, whether traditional black-box checkers or recent timestamp-based white-box ones, operate offline and require the entire history to be available to construct a dependency graph, making them unsuitable for continuous and ever-growing histories.
  This paper addresses online isolation checking by extending the timestamp-based isolation checking approach to online settings. Specifically, we design CHRONOS, an efficient timestamp-based offline SI checker. CHRONOS is incremental and avoids constructing a start-ordered serialization graph for the entire history, making it well-suited for online scenarios. We further extend CHRONOS into an online SI checker, AION, addressing several key challenges unique to online settings. Additionally, we develop AION-SER for online SER checking. Experiments highlight that CHRONOS processes offline histories with up to one million transactions in seconds, greatly outperforming existing SI checkers. Furthermore, AION and AION-SER sustain a throughput of approximately 12K transactions per second, demonstrating their practicality for online isolation checking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01477v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hexu Li, Hengfeng Wei, Hongrong Ouyang, Yuxing Chen, Na Yang, Ruohao Zhang, Anqun Pan</dc:creator>
    </item>
    <item>
      <title>Bhakti: A Lightweight Vector Database Management System for Endowing Large Language Models with Semantic Search Capabilities and Memory</title>
      <link>https://arxiv.org/abs/2504.01553</link>
      <description>arXiv:2504.01553v1 Announce Type: new 
Abstract: With the rapid development of big data and artificial intelligence technologies, the demand for effective processing and retrieval of vector data is growing. Against this backdrop, I have developed the Bhakti vector database, aiming to provide a lightweight and easy-to-deploy solution to meet the storage and semantic search needs of small and medium-sized datasets. Bhakti supports a variety of similarity calculation methods and a domain-specific language (DSL) for document-based pattern matching pre-filtering, facilitating migration of data with its portable data files, flexible data management and seamless integration with Python3. Furthermore, I propose a memory-enhanced large language model dialogue solution based on the Bhakti database, which can assign different weights to the question and answer in dialogue history, achieving fine-grained control over the semantic importance of each segment in a single dialogue history. Through experimental validation, my method shows significant performance in the application of semantic search and question-answering systems. Although there are limitations in processing large datasets, such as not supporting approximate calculation methods like HNSW, the lightweight nature of Bhakti gives it a clear advantage in scenarios involving small and medium-sized datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01553v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihao Wu</dc:creator>
    </item>
    <item>
      <title>FastER: Fast On-Demand Entity Resolution in Property Graphs</title>
      <link>https://arxiv.org/abs/2504.01557</link>
      <description>arXiv:2504.01557v1 Announce Type: new 
Abstract: Entity resolution (ER) is the problem of identifying and linking database records that refer to the same real-world entity. Traditional ER methods use batch processing, which becomes impractical with growing data volumes due to high computational costs and lack of real-time capabilities. In many applications, users need to resolve entities for only a small portion of their data, making full data processing unnecessary -- a scenario known as "ER-on-demand". This paper proposes FastER, an efficient ER-on-demand framework for property graphs. Our approach uses graph differential dependencies (GDDs) as a knowledge encoding language to design effective filtering mechanisms that leverage both structural and attribute semantics of graphs. We construct a blocking graph from filtered subgraphs to reduce the number of candidate entity pairs requiring comparison. Additionally, FastER incorporates Progressive Profile Scheduling (PPS), allowing the system to incrementally produce results throughout the resolution process. Extensive evaluations on multiple benchmark datasets demonstrate that FastER significantly outperforms state-of-the-art ER methods in computational efficiency and real-time processing for on-demand tasks while ensuring reliability. We make FastER publicly available at: https://anonymous.4open.science/r/On_Demand_Entity_Resolution-9DFB</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01557v1</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shujing Wang (Huazhong Agricultural University, Wuhan, China), Selasi Kwashie (AI &amp; Cyber Futures Institute, Charles Sturt University, Australia), Michael Bewong (School of Computing, Mathematics and Engineering, Charles Sturt University, Australia), Junwei Hu (Huazhong Agricultural University, Wuhan, China), Vincent M. Nofong (Department of Computer Science and Engineering, University of Mines and Technology, Ghana), Shiqi Miao (Huazhong Agricultural University, Wuhan, China), Zaiwen Feng (Huazhong Agricultural University, Wuhan, China)</dc:creator>
    </item>
    <item>
      <title>Who is Responsible When AI Fails? Mapping Causes, Entities, and Consequences of AI Privacy and Ethical Incidents</title>
      <link>https://arxiv.org/abs/2504.01029</link>
      <description>arXiv:2504.01029v1 Announce Type: cross 
Abstract: The rapid growth of artificial intelligence (AI) technologies has changed decision-making in many fields. But, it has also raised major privacy and ethical concerns. However, many AI incidents taxonomies and guidelines for academia, industry, and government lack grounding in real-world incidents. We analyzed 202 real-world AI privacy and ethical incidents. This produced a taxonomy that classifies incident types across AI lifecycle stages. It accounts for contextual factors such as causes, responsible entities, disclosure sources, and impacts. Our findings show insufficient incident reporting from AI developers and users. Many incidents are caused by poor organizational decisions and legal non-compliance. Only a few legal actions and corrective measures exist, while risk-mitigation efforts are limited. Our taxonomy contributes a structured approach in reporting of future AI incidents. Our findings demonstrate that current AI governance frameworks are inadequate. We urgently need child-specific protections and AI policies on social media. They must moderate and reduce the spread of harmful AI-generated content. Our research provides insights for policymakers and practitioners, which lets them design ethical AI. It also support AI incident detection and risk management. Finally, it guides AI policy development. Improved policies will protect people from harmful AI applications and support innovation in AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01029v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.31076.90244</arxiv:DOI>
      <dc:creator>Hilda Hadan, Reza Hadi Mogavi, Leah Zhang-Kennedy, Lennart E. Nacke</dc:creator>
    </item>
    <item>
      <title>SplineSketch: Even More Accurate Quantiles with Error Guarantees</title>
      <link>https://arxiv.org/abs/2504.01206</link>
      <description>arXiv:2504.01206v1 Announce Type: cross 
Abstract: Space-efficient estimation of quantiles in massive datasets is a fundamental problem with numerous applications in data monitoring and analysis. While theoretical research led to optimal algorithms, such as the Greenwald-Khanna algorithm or the KLL sketch, practitioners often use other sketches that perform significantly better in practice but lack theoretical guarantees. Most notably, the widely used t-digest has unbounded worst-case error.
  In this paper, we seek to get the best of both worlds. We present a new quantile summary, SplineSketch, for numeric data, offering near-optimal theoretical guarantees and outperforming t-digest by a factor of 2-20 on a range of synthetic and real-world datasets with non-skewed frequency distributions. To achieve such performance, we develop a novel approach that maintains a dynamic subdivision of the input range into buckets while fitting the input distribution using monotone cubic spline interpolation. The core challenge is implementing this method in a space-efficient manner while ensuring strong worst-case guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01206v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>stat.CO</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksander {\L}ukasiewicz, Jakub T\v{e}tek, Pavel Vesel\'y</dc:creator>
    </item>
    <item>
      <title>CLaP -- State Detection from Time Series</title>
      <link>https://arxiv.org/abs/2504.01783</link>
      <description>arXiv:2504.01783v1 Announce Type: cross 
Abstract: The ever-growing amount of sensor data from machines, smart devices, and the environment leads to an abundance of high-resolution, unannotated time series (TS). These recordings encode the recognizable properties of latent states and transitions from physical phenomena that can be modelled as abstract processes. The unsupervised localization and identification of these states and their transitions is the task of time series state detection (TSSD). We introduce CLaP, a new, highly accurate and efficient algorithm for TSSD. It leverages the predictive power of time series classification for TSSD in an unsupervised setting by applying novel self-supervision techniques to detect whether data segments emerge from the same state or not. To this end, CLaP cross-validates a classifier with segment-labelled subsequences to quantify confusion between segments. It merges labels from segments with high confusion, representing the same latent state, if this leads to an increase in overall classification quality. We conducted an experimental evaluation using 391 TS from four benchmarks and found CLaP to be significantly more precise in detecting states than five state-of-the-art competitors. It achieves the best accuracy-runtime tradeoff and is scalable to large TS. We provide a Python implementation of CLaP, which can be deployed in TS analysis workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01783v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arik Ermshaus, Patrick Sch\"afer, Ulf Leser</dc:creator>
    </item>
    <item>
      <title>PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System</title>
      <link>https://arxiv.org/abs/2504.01948</link>
      <description>arXiv:2504.01948v1 Announce Type: cross 
Abstract: Database Management Systems (DBMSs) are crucial for efficient data management and analytics, and are used in several different application domains. Due to the increasing volume of data a DBMS deals with, current processor-centric architectures (e.g., CPUs, GPUs) suffer from data movement bottlenecks when executing key DBMS operations (e.g., selection, aggregation, ordering, and join). This happens mostly due to the limited memory bandwidth between compute and memory resources. Data-centric architectures like Processing-in-Memory (PIM) are a promising alternative for applications bottlenecked by data, placing compute resources close to where data resides. Previous works have evaluated using PIM for data analytics. However, they either do not use real-world architectures or they consider only a subset of the operators used in analytical queries. This work aims to fully evaluate a data-centric approach to data analytics, by using the real-world UPMEM PIM system. To this end we first present the PIM Data Analytics Library (PIMDAL), which implements four major DB operators: selection, aggregation, ordering and join. Second, we use hardware performance metrics to understand which properties of a PIM system are important for a high-performance implementation. Third, we compare PIMDAL to reference implementations on high-end CPU and GPU systems. Fourth, we use PIMDAL to implement five TPC-H queries to gain insights into analytical queries. We analyze and show how to overcome the three main limitations of the UPMEM system when implementing DB operators: (I) low arithmetic performance, (II) explicit memory management and (III) limited communication between compute units. Our evaluation shows PIMDAL achieves 3.9x the performance of a high-end CPU, on average across the five TPC-H queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01948v1</guid>
      <category>cs.AR</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manos Frouzakis, Juan G\'omez-Luna, Geraldo F. Oliveira, Mohammad Sadrosadati, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>DocETL: Agentic Query Rewriting and Evaluation for Complex Document Processing</title>
      <link>https://arxiv.org/abs/2410.12189</link>
      <description>arXiv:2410.12189v3 Announce Type: replace 
Abstract: Analyzing unstructured data has been a persistent challenge in data processing. Large Language Models (LLMs) have shown promise in this regard, leading to recent proposals for declarative frameworks for LLM-powered processing of unstructured data. However, these frameworks focus on reducing cost when executing user-specified operations using LLMs, rather than improving accuracy, executing most operations as-is (in a single LLM call). This is problematic for complex tasks and data, where LLM outputs for user-defined operations are often inaccurate, even with optimized prompts. For example, an LLM may struggle to identify {\em all} instances of specific clauses, like force majeure or indemnification, in lengthy legal documents, requiring decomposition of the data, the task, or both.
  We present DocETL, a system that optimizes complex document processing pipelines, while accounting for LLM shortcomings. DocETL offers a declarative interface for users to define such pipelines and uses an agent-based approach to automatically optimize them, leveraging novel agent-based rewrites (that we call rewrite directives), as well as an optimization and evaluation framework. We introduce (i) logical rewriting of pipelines, tailored for LLM-based tasks, (ii) an agent-guided plan evaluation mechanism that synthesizes and orchestrates task-specific validation prompts, and (iii) an optimization algorithm that efficiently finds promising plans, considering the latencies of agent-based plan generation and evaluation. Our evaluation on four different unstructured document analysis tasks demonstrates that DocETL finds plans with outputs that are 25 to 80% more accurate than well-engineered baselines, addressing a critical gap in unstructured data analysis. DocETL is open-source at docetl.org, and as of March 2025, has amassed over 1.7k GitHub Stars, with users spanning a variety of domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12189v3</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shreya Shankar, Tristan Chambers, Tarak Shah, Aditya G. Parameswaran, Eugene Wu</dc:creator>
    </item>
    <item>
      <title>Stochastic SketchRefine: Scaling In-Database Decision-Making under Uncertainty to Millions of Tuples</title>
      <link>https://arxiv.org/abs/2411.17915</link>
      <description>arXiv:2411.17915v2 Announce Type: replace 
Abstract: Decision making under uncertainty often requires choosing packages, or bags of tuples, that collectively optimize expected outcomes while limiting risks. Processing Stochastic Package Queries (SPQs) involves solving very large optimization problems on uncertain data. Monte Carlo methods create numerous scenarios, or sample realizations of the stochastic attributes of all the tuples, and generate packages with optimal objective values across these scenarios. The number of scenarios needed for accurate approximation - and hence the size of the optimization problem when using prior methods - increases with variance in the data, and the search space of the optimization problem increases exponentially with the number of tuples in the relation. Existing solvers take hours to process SPQs on large relations containing stochastic attributes with high variance. Besides enriching the SPaQL language to capture a broader class of risk specifications, we make two fundamental contributions towards scalable SPQ processing. First, to handle high variance, we propose risk-constraint linearization (RCL), which converts SPQs into Integer Linear Programs (ILPs) whose size is independent of the number of scenarios used. Solving these ILPs gives us feasible and near-optimal packages. Second, we propose Stochastic SketchRefine, a divide and conquer framework that breaks down a large stochastic optimization problem into subproblems involving smaller subsets of tuples. Our experiments show that, together, RCL and Stochastic SketchRefine produce high-quality packages in orders of magnitude lower runtime than the state of the art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17915v2</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riddho R. Haque, Anh L. Mai, Matteo Brucato, Azza Abouzied, Peter J. Haas, Alexandra Meliou</dc:creator>
    </item>
    <item>
      <title>Towards Interoperable Data Spaces: Comparative Analysis of Data Space Implementations between Japan and Europe</title>
      <link>https://arxiv.org/abs/2501.15738</link>
      <description>arXiv:2501.15738v2 Announce Type: replace 
Abstract: Data spaces are evolving rapidly. In Europe, the concept of data spaces, which emphasises the importance of trust, sovereignty, and interoperability, is being implemented as a platform such as Catena-X. Meanwhile, Japan has been developing its approach to data sharing, in line with global trends but also to address unique domestic challenges, resulting a platform such as DATA-EX. Achieving interoperability between European and Japanese data spaces remains a critical challenge due to the differences created by these parallel advances. Although interoperability between data spaces has several aspects, compatibility of trust in the participating entities and the data exchanged is a significant aspect due to its influence on business. This paper undertakes a comparative analysis of DATA-EX and Catena-X while focusing on aspect of trust, to explore the challenges and opportunities for achieving interoperability between Japanese and European data spaces. By examining common data exchange processes, key objects such as datasets, and specific evaluation criteria, the study identifies gaps, challenges, and proposes actionable solutions such as inter-exchangeable topology. Through this analysis, the paper aims to contribute to the ongoing discourse on global data interoperability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15738v2</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shun Ishihara, Taka Matsutsuka</dc:creator>
    </item>
    <item>
      <title>Data Driven Decision Making with Time Series and Spatio-temporal Data</title>
      <link>https://arxiv.org/abs/2503.08473</link>
      <description>arXiv:2503.08473v3 Announce Type: replace 
Abstract: Time series data captures properties that change over time. Such data occurs widely, ranging from the scientific and medical domains to the industrial and environmental domains. When the properties in time series exhibit spatial variations, we often call the data spatio-temporal. As part of the continued digitalization of processes throughout society, increasingly large volumes of time series and spatio-temporal data are available. In this tutorial, we focus on data-driven decision making with such data, e.g., enabling greener and more efficient transportation based on traffic time series forecasting. The tutorial adopts the holistic paradigm of ``data-governance-analytics-decision.'' We first introduce the data foundation of time series and spatio-temporal data, which is often heterogeneous. Next, we discuss data governance methods that aim to improve data quality. We then cover data analytics, focusing on the ``AGREE'' principles: Automation, Generalization, Robustness, Explainability, and Efficiency. We finally cover data-driven decision making strategies and briefly discuss promising research directions. We hope that the tutorial will serve as a primary resource for researchers and practitioners who are interested in value creation from time series and spatio-temporal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08473v3</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bin Yang, Yuxuan Liang, Chenjuan Guo, Christian S. Jensen</dc:creator>
    </item>
  </channel>
</rss>
