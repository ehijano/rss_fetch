<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Energy-Efficient Satellite IoT Optical Downlinks Using Weather-Adaptive Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2501.11198</link>
      <description>arXiv:2501.11198v1 Announce Type: new 
Abstract: Internet of Things (IoT) devices have become increasingly ubiquitous with applications not only in urban areas but remote areas as well. These devices support industries such as agriculture, forestry, and resource extraction. Due to the device location being in remote areas, satellites are frequently used to collect and deliver IoT device data to customers. As these devices become increasingly advanced and numerous, the amount of data produced has rapidly increased potentially straining the ability for radio frequency (RF) downlink capacity. Free space optical communications with their wide available bandwidths and high data rates are a potential solution, but these communication systems are highly vulnerable to weather-related disruptions. This results in certain communication opportunities being inefficient in terms of the amount of data received versus the power expended. In this paper, we propose a deep reinforcement learning (DRL) method using Deep Q-Networks that takes advantage of weather condition forecasts to improve energy efficiency while delivering the same number of packets as schemes that don't factor weather into routing decisions. We compare this method with simple approaches that utilize simple cloud cover thresholds to improve energy efficiency. In testing the DRL approach provides improved median energy efficiency without a significant reduction in median delivery ratio. Simple cloud cover thresholds were also found to be effective but the thresholds with the highest energy efficiency had reduced median delivery ratio values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11198v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ethan Fettes, Pablo G. Madoery, Halim Yanikomeroglu, Gunes Karabulut-Kurt, Abhishek Naik, Colin Bellinger, Stephane Martel, Khaled Ahmed, Sameera Siddiqui</dc:creator>
    </item>
    <item>
      <title>Orbit-Aware Split Learning: Optimizing LEO Satellite Networks for Distributed Online Learning</title>
      <link>https://arxiv.org/abs/2501.11410</link>
      <description>arXiv:2501.11410v1 Announce Type: new 
Abstract: This paper proposes a novel split learning architecture designed to exploit the cyclical movement of Low Earth Orbit (LEO) satellites in non-terrestrial networks (NTNs). Although existing research focuses on offloading tasks to the NTN infrastructure, these approaches overlook the dynamic movement patterns of LEO satellites that can be used to efficiently distribute the learning task. In this work, we analyze how LEO satellites, from the perspective of ground terminals, can participate in a time-window-based model training. By splitting the model between a LEO and a ground terminal, the computational burden on the satellite segment is reduced, while each LEO satellite offloads the partially trained model to the next satellite in the constellation. This cyclical training process allows larger and more energy-intensive models to be deployed and trained across multiple LEO satellites, despite their limited energy resources. We formulate an optimization problem that manages radio and processing resources, ensuring the entire data is processed during each satellite pass while minimizing the energy consumption. Our results demonstrate that this approach offers a more scalable and energy-efficient way to train complex models, enhancing the capabilities of LEO satellite constellations in the context of Artificial Intelligence-driven applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11410v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Martinez-Gost, Ana P\'erez-Neira</dc:creator>
    </item>
    <item>
      <title>Routing Optimization Based on Distributed Intelligent Network Softwarization for the Internet of Things</title>
      <link>https://arxiv.org/abs/2501.11484</link>
      <description>arXiv:2501.11484v1 Announce Type: new 
Abstract: The Internet of Things (IoT) establishes connectivity between billions of heterogeneous devices that provide a variety of essential everyday services. The IoT faces several challenges, including energy efficiency and scalability, that require consideration of enabling technologies such as network softwarization. This technology is an appropriate solution for IoT, leveraging Software Defined Networking (SDN) and Network Function Virtualization (NFV) as two main techniques, especially when combined with Machine Learning (ML). Although many efforts have been made to optimize routing in softwarized IoT, the existing solutions do not take advantage of distributed intelligence. In this paper, we propose to optimize routing in softwarized IoT networks using Federated Deep Reinforcement Learning (FDRL), where distributed network softwarization and intelligence (i.e., FDRL) join forces to improve routing in constrained IoT networks. Our proposal introduces the combination of two novelties (i.e., distributed controller design and intelligent routing) to meet the IoT requirements (mainly performance and energy efficiency). The simulation results confirm the effectiveness of our proposal compared to the conventional counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11484v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3605098.3635995</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing. 2024. p. 1757-1764</arxiv:journal_reference>
      <dc:creator>Mohamed Ali Zormati, Hicham Lakhlef, Sofiane Ouni</dc:creator>
    </item>
    <item>
      <title>A Deep Reinforcement Learning based Scheduler for IoT Devices in Co-existence with 5G-NR</title>
      <link>https://arxiv.org/abs/2501.11574</link>
      <description>arXiv:2501.11574v1 Announce Type: new 
Abstract: Co-existence of 5G New Radio (5G-NR) with IoT devices is considered as a promising technique to enhance the spectral usage and efficiency of future cellular networks. In this paper, a unified framework has been proposed for allocating in-band resource blocks (RBs), i.e., within a multi-cell network, to 5G-NR users in co-existence with NB-IoT and LTE-M devices. First, a benchmark (upper-bound) scheduler has been designed for joint sub-carrier (SC) and modulation and coding scheme (MCS) allocation that maximizes instantaneous throughput and fairness among users/devices, while considering synchronous RB allocation in the neighboring cells. A series of numerical simulations with realistic ICI in an urban scenario have been used to compute benchmark upper-bound solutions for characterizing performance in terms of throughput, fairness, and delay. Next, an edge learning based multi-agent deep reinforcement learning (DRL) framework has been developed for different DRL algorithms, specifically, a policy-based gradient network (PGN), a deep Q-learning based network (DQN), and an actor-critic based deep deterministic policy gradient network (DDPGN). The proposed DRL framework depends on interference allocation, where the actions are based on inter-cell-interference (ICI) instead of power, which can bypass the need for raw data sharing and/or inter-agent communication. The numerical results reveal that the interference allocation based DRL schedulers can significantly outperform their counterparts, where the actions are based on power allocation. Further, the performance of the proposed policy-based edge learning algorithms is close to the centralized ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11574v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shahida Jabeen</dc:creator>
    </item>
    <item>
      <title>Message Replication for Improving Reliability of LR-FHSS Direct-to-Satellite IoT</title>
      <link>https://arxiv.org/abs/2501.11984</link>
      <description>arXiv:2501.11984v1 Announce Type: new 
Abstract: Long-range frequency-hopping spread spectrum (LR-FHSS) promises to enhance network capacity by integrating frequency hopping into existing Long Range Wide Area Networks (LoRaWANs). Due to its simplicity and scalability, LR-FHSS has generated significant interest as a potential candidate for direct-to-satellite IoT (D2S-IoT) applications. This paper explores methods to improve the reliability of data transfer on the uplink (i.e., from terrestrial IoT nodes to satellite) of LR-FHSS D2S-IoT networks.
  Because D2S-IoT networks are expected to support large numbers of potentially uncoordinated IoT devices per satellite, acknowledgment-cum-retransmission-aided reliability mechanisms are not suitable due to their lack of scalability. We therefore leverage message-replication, wherein every application-layer message is transmitted multiple times to improve the probability of reception without the use of receiver acknowledgments. We propose two message-replication schemes. One scheme is based on conventional replication, where multiple replicas of a message are transmitted, each as a separate link-layer frame. In the other scheme, multiple copies of a message is included in the payload of a single link-layer frame. We show that both techniques improve LR-FHSS reliability. Which method is more suitable depends on the network's traffic characteristics. We provide guidelines to choose the optimal method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11984v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sonu Rathi, Siddhartha S. Borkotoky</dc:creator>
    </item>
    <item>
      <title>Power Amplifier-Aware Transmit Power Optimization for OFDM and SC-FDMA Systems</title>
      <link>https://arxiv.org/abs/2501.11994</link>
      <description>arXiv:2501.11994v1 Announce Type: new 
Abstract: The Single Carrier-Frequency Division Multiple Access (SC-FDMA) is a transmission technique used in the uplink of Long Term Evolution (LTE) and 5G systems, as it is characterized by reduced transmitted signal envelope fluctuations in comparison to Orthogonal Frequency Division Multiplexing (OFDM) technique used in the downlink. This allows for higher energy efficiency of User Equipments (UEs) while maintaining sufficient signal quality, measured by Error Vector Magnitude (EVM), at the transmitter. This paper proposes to model a nonlinear Power Amplifier (PA) influence while optimizing the transmit power in order to maximize the Signal to Noise and Distortion power Ratio (SNDR) at the receiver, removing the transmitter-based EVM constraint. An analytic model of SNDR for the OFDM system and a semi-analytical model for the SC-FDMA system are provided. Numerical investigations show that the proposed transmit power optimization allows for improved signal quality at the receiver for both OFDM and SC-FDMA systems. However, SC-FDMA still outperforms OFDM in this matter. Such a power amplifier-aware wireless transmitter optimization should be considered to boost the performance and sustainability of next-generation wireless systems, including Internet of Things (IoT) ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11994v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pawel Kryszkiewicz</dc:creator>
    </item>
    <item>
      <title>Harnessing Generative Pre-Trained Transformer for Datacenter Packet Trace Generation</title>
      <link>https://arxiv.org/abs/2501.12033</link>
      <description>arXiv:2501.12033v1 Announce Type: new 
Abstract: Today, the rapid growth of applications reliant on datacenters calls for new advancements to meet the increasing traffic and computational demands. Traffic traces from datacenters are essential for further development and optimization of future datacenters. However, traces are rarely released to the public. Researchers often use simplified mathematical models that lack the depth needed to recreate intricate traffic patterns and, thus, miss optimization opportunities found in realistic traffic. In this preliminary work, we introduce DTG-GPT, a packet-level Datacenter Traffic Generator (DTG), based on the generative pre-trained transformer (GPT) architecture used by many state-of-the-art large language models. We train our model on a small set of available traffic traces from different domains and offer a simple methodology to evaluate the fidelity of the generated traces to their original counterparts. We show that DTG-GPT can synthesize novel traces that mimic the spatiotemporal patterns found in real traffic traces. We further demonstrate that DTG-GPT can generate traces for networks of different scales while maintaining fidelity. Our findings indicate the potential that, in the future, similar models to DTG-GPT will allow datacenter operators to release traffic information to the research community via trained GPT models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12033v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Griner</dc:creator>
    </item>
    <item>
      <title>QoS-Aware Radio Access Technology (RAT) Selection in Hybrid Vehicular Networks</title>
      <link>https://arxiv.org/abs/2501.12304</link>
      <description>arXiv:2501.12304v1 Announce Type: new 
Abstract: The increasing number of wireless communication technologies and standards bring immense opportunities and challenges to provide seamless connectivity in Hybrid Vehicular Networks (HVNs). HVNs could not only enhance existing applications but could also spur an array of new services. However, due to sheer number of use cases and applications with diverse and stringent QoS performance requirements it is very critical to efficiently decide on which radio access technology (RAT) to select. In this paper a QoS aware RAT selection algorithm is proposed for HVN. The proposed algorithm switches between IEEE 802.11p based ad hoc network and LTE cellular network by considering network load and application's QoS requirements. The simulation-based studies show that the proposed RAT selection mechanism results in lower number of Vertical Handovers (VHOs) and significant performance improvements in terms of packet delivery ratio, latency and application-level throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12304v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-319-17765-6_11</arxiv:DOI>
      <dc:creator>Zeeshan Hameed Mir, Jamal Toutouh, Fethi Filali, Enrique Alba</dc:creator>
    </item>
    <item>
      <title>Light commodity devices for building vehicular ad hoc networks: An experimental study</title>
      <link>https://arxiv.org/abs/2501.12317</link>
      <description>arXiv:2501.12317v1 Announce Type: new 
Abstract: Vehicular communication networks represent both an opportunity and a challenge for providing smart mobility services by using a hybrid solution that relies on cellular connectivity and short range communications. The evaluation of this kind of network is overwhelmingly carried out in the present literature with simulations. However, the degree of realism of the results obtained is limited because simulations simplify real world interactions too much in many cases. In this article, we define an outdoor testbed to evaluate the performance of short range vehicular communications by using real world personal portable devices (smartphones, tablets, and laptops), two different PHY standards (IEEE 802.11g and IEEE 802.11a), and vehicles. Our test results on the 2.4 GHz band show that smartphones can be used to communicate vehicles within a range up to 75 m, while tablets can attain up to 125 m in mobility conditions. Moreover, we observe that vehicles equipped with laptops exchange multimedia information with nodes located further than 150 m. The communications on the 5 GHz band achieved an effective transmission range of up to 100 m. This, together with the optimization of the protocols used, could take our commodity lightweight devices to a new realm of use in the next generation of ad hoc mobility communications for moving through the city.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12317v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.adhoc.2015.09.013</arxiv:DOI>
      <arxiv:journal_reference>Ad Hoc Networks, 37, 499-511 (2016)</arxiv:journal_reference>
      <dc:creator>Jamal Toutouh, Enrique Alba</dc:creator>
    </item>
    <item>
      <title>AI-Powered Urban Transportation Digital Twin: Methods and Applications</title>
      <link>https://arxiv.org/abs/2501.10396</link>
      <description>arXiv:2501.10396v1 Announce Type: cross 
Abstract: We present a survey paper on methods and applications of digital twins (DT) for urban traffic management. While the majority of studies on the DT focus on its "eyes," which is the emerging sensing and perception like object detection and tracking, what really distinguishes the DT from a traditional simulator lies in its ``brain," the prediction and decision making capabilities of extracting patterns and making informed decisions from what has been seen and perceived. In order to add values to urban transportation management, DTs need to be powered by artificial intelligence and complement with low-latency high-bandwidth sensing and networking technologies. We will first review the DT pipeline leveraging cyberphysical systems and propose our DT architecture deployed on a real-world testbed in New York City. This survey paper can be a pointer to help researchers and practitioners identify challenges and opportunities for the development of DTs; a bridge to initiate conversations across disciplines; and a road map to exploiting potentials of DTs for diverse urban transportation applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10396v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Di, Yongjie Fu, Mehmet K. Turkcan, Mahshid Ghasemi, Zhaobin Mo, Chengbo Zang, Abhishek Adhikari, Zoran Kostic, Gil Zussman</dc:creator>
    </item>
    <item>
      <title>Using hypervisors to create a cyber polygon</title>
      <link>https://arxiv.org/abs/2501.10403</link>
      <description>arXiv:2501.10403v1 Announce Type: cross 
Abstract: Cyber polygon used to train cybersecurity professionals, test new security technologies and simulate attacks play an important role in ensuring cybersecurity. The creation of such training grounds is based on the use of hypervisors, which allow efficient management of virtual machines, isolating operating systems and resources of a physical computer from virtual machines, ensuring a high level of security and stability. The paper analyses various aspects of using hypervisors in cyber polygons, including types of hypervisors, their main functions, and the specifics of their use in modelling cyber threats. The article shows the ability of hypervisors to increase the efficiency of hardware resources, create complex virtual environments for detailed modelling of network structures and simulation of real situations in cyberspace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10403v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.31891/2219-9365-2024-79-7</arxiv:DOI>
      <arxiv:journal_reference>Measuring and computing devices in technological processes, 2024, Issue 3</arxiv:journal_reference>
      <dc:creator>Dmytro Tymoshchuk, Vasyl Yatskiv</dc:creator>
    </item>
    <item>
      <title>AI/ML Based Detection and Categorization of Covert Communication in IPv6 Network</title>
      <link>https://arxiv.org/abs/2501.10627</link>
      <description>arXiv:2501.10627v1 Announce Type: cross 
Abstract: The flexibility and complexity of IPv6 extension headers allow attackers to create covert channels or bypass security mechanisms, leading to potential data breaches or system compromises. The mature development of machine learning has become the primary detection technology option used to mitigate covert communication threats. However, the complexity of detecting covert communication, evolving injection techniques, and scarcity of data make building machine-learning models challenging. In previous related research, machine learning has shown good performance in detecting covert communications, but oversimplified attack scenario assumptions cannot represent the complexity of modern covert technologies and make it easier for machine learning models to detect covert communications. To bridge this gap, in this study, we analyzed the packet structure and network traffic behavior of IPv6, used encryption algorithms, and performed covert communication injection without changing network packet behavior to get closer to real attack scenarios. In addition to analyzing and injecting methods for covert communications, this study also uses comprehensive machine learning techniques to train the model proposed in this study to detect threats, including traditional decision trees such as random forests and gradient boosting, as well as complex neural network architectures such as CNNs and LSTMs, to achieve detection accuracy of over 90\%. This study details the methods used for dataset augmentation and the comparative performance of the applied models, reinforcing insights into the adaptability and resilience of the machine learning application in IPv6 covert communication. In addition, we also proposed a Generative AI-assisted interpretation concept based on prompt engineering as a preliminary study of the role of Generative AI agents in covert communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10627v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammad Wali Ur Rahman, Yu-Zheng Lin, Carter Weeks, David Ruddell, Jeff Gabriellini, Bill Hayes, Salim Hariri, Edward V. Ziegler Jr</dc:creator>
    </item>
    <item>
      <title>Poisson Hail on a Wireless Ground</title>
      <link>https://arxiv.org/abs/2501.10712</link>
      <description>arXiv:2501.10712v1 Announce Type: cross 
Abstract: This paper defines a new model which incorporates three key ingredients of a large class of wireless communication systems: (1) spatial interactions through interference, (2) dynamics of the queueing type, with users joining and leaving, and (3) carrier sensing and collision avoidance as used in, e.g., WiFi. In systems using (3), rather than directly accessing the shared resources upon arrival, a customer is considerate and waits to access them until nearby users in service have left. This new model can be seen as a missing piece of a larger puzzle that contains such dynamics as spatial birth-and-death processes, the Poisson-Hail model, and wireless dynamics as key other pieces. It is shown that, under natural assumptions, this model can be represented as a Markov process on the space of counting measures. The main results are then two-fold. The first is on the shape of the stability region and, more precisely, on the characterization of the critical value of the arrival rate that separates stability from instability. The second is of a more qualitative or perhaps even ethical nature. There is evidence that for natural values of the system parameters, the implementation of sensing and collision avoidance stabilizes a system that would be unstable if immediate access to the shared resources would be granted. In other words, for these parameters, renouncing greedy access makes sharing sustainable, whereas indulging in greedy access kills the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10712v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Baccelli, Ke Feng, Sergey Foss</dc:creator>
    </item>
    <item>
      <title>ChaosEater: Fully Automating Chaos Engineering with Large Language Models</title>
      <link>https://arxiv.org/abs/2501.11107</link>
      <description>arXiv:2501.11107v1 Announce Type: cross 
Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resiliency of distributed systems. It involves artificially injecting specific failures into a distributed system and observing its behavior in response. Based on the observation, the system can be proactively improved to handle those failures. Recent CE tools realize the automated execution of predefined CE experiments. However, defining these experiments and reconfiguring the system after the experiments still remain manual. To reduce the costs of the manual operations, we propose \textsc{ChaosEater}, a \textit{system} for automating the entire CE operations with Large Language Models (LLMs). It pre-defines the general flow according to the systematic CE cycle and assigns subdivided operations within the flow to LLMs. We assume systems based on Infrastructure as Code (IaC), wherein the system configurations and artificial failures are managed through code. Hence, the LLMs' operations in our \textit{system} correspond to software engineering tasks, including requirement definition, code generation and debugging, and testing. We validate our \textit{system} through case studies on both small and large systems. The results demonstrate that our \textit{system} significantly reduces both time and monetary costs while completing reasonable single CE cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11107v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daisuke Kikuta, Hiroki Ikeuchi, Kengo Tajiri, Yuusuke Nakano</dc:creator>
    </item>
    <item>
      <title>Multivariate Wireless Link Quality Prediction Based on Pre-trained Large Language Models</title>
      <link>https://arxiv.org/abs/2501.11247</link>
      <description>arXiv:2501.11247v1 Announce Type: cross 
Abstract: Accurate and reliable link quality prediction (LQP) is crucial for optimizing network performance, ensuring communication stability, and enhancing user experience in wireless communications. However, LQP faces significant challenges due to the dynamic and lossy nature of wireless links, which are influenced by interference, multipath effects, fading, and blockage. In this paper, we propose GAT-LLM, a novel multivariate wireless link quality prediction model that combines Large Language Models (LLMs) with Graph Attention Networks (GAT) to enable accurate and reliable multivariate LQP of wireless communications. By framing LQP as a time series prediction task and appropriately preprocessing the input data, we leverage LLMs to improve the accuracy of link quality prediction. To address the limitations of LLMs in multivariate prediction due to typically handling one-dimensional data, we integrate GAT to model interdependencies among multiple variables across different protocol layers, enhancing the model's ability to handle complex dependencies. Experimental results demonstrate that GAT-LLM significantly improves the accuracy and robustness of link quality prediction, particularly in multi-step prediction scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11247v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuangzhuang Yan, Xinyu Gu, Shilong Fan, Zhenyu Liu</dc:creator>
    </item>
    <item>
      <title>A Dynamic Improvement Framework for Vehicular Task Offloading</title>
      <link>https://arxiv.org/abs/2501.11333</link>
      <description>arXiv:2501.11333v1 Announce Type: cross 
Abstract: In this paper, the task offloading from vehicles with random velocities is optimized via a novel dynamic improvement framework. Particularly, in a vehicular network with multiple vehicles and base stations (BSs), computing tasks of vehicles are offloaded via BSs to an edge server. Due to the random velocities, the exact trajectories of vehicles cannot be predicted in advance. Hence, instead of deterministic optimization, the cell association, uplink time and throughput allocation of multiple vehicles in a period of task offloading are formulated as a finite-horizon Markov decision process. In the proposed solution framework, we first obtain a reference scheduling scheme of cell association, uplink time and throughput allocation via deterministic optimization at the very beginning. The reference scheduling scheme is then used to approximate the value functions of the Bellman's equations, and the actual scheduling action is determined in each time slot according to the current system state and approximate value functions. Thus, the intensive computation for value iteration in the conventional solution is eliminated. Moreover, a non-trivial average cost upper bound is provided for the proposed solution framework. In the simulation, the random trajectories of vehicles are generated from a high-fidelity traffic simulator. It is shown that the performance gain of the proposed scheduling framework over the baselines is significant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11333v1</guid>
      <category>eess.SY</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qianren Li, Yuncong Hong, Bojie Lv, Rui Wang</dc:creator>
    </item>
    <item>
      <title>Optimal User and Target Scheduling, User-Target Pairing, and Low-Resolution Phase-Only Beamforming for ISAC Systems</title>
      <link>https://arxiv.org/abs/2501.11593</link>
      <description>arXiv:2501.11593v1 Announce Type: cross 
Abstract: We investigate the joint user and target scheduling, user-target pairing, and low-resolution phase-only beamforming design for integrated sensing and communications (ISAC). Scheduling determines which users and targets are served, while pairing specifies which users and targets are grouped into pairs. Additionally, the beamformers are designed using few-bit constant-modulus phase shifts. This resource allocation problem is a nonconvex mixed-integer nonlinear program (MINLP) and challenging to solve. To address it, we propose an exact mixed-integer linear program (MILP) reformulation, which leads to a globally optimal solution. Our results demonstrate the superiority of an optimal joint design compared to heuristic stage-wise approaches, which are highly sensitive to scenario characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11593v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis F. Abanto-Leon, Setareh Maghsudi</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Social Networks: Lessons from Bluesky Starter Packs</title>
      <link>https://arxiv.org/abs/2501.11605</link>
      <description>arXiv:2501.11605v1 Announce Type: cross 
Abstract: Microblogging is a crucial mode of online communication. However, launching a new microblogging platform remains challenging, largely due to network effects. This has resulted in entrenched (and undesirable) dominance by established players, such as X/Twitter. To overcome these network effects, Bluesky, an emerging microblogging platform, introduced starter packs -- curated lists of accounts that users can follow with a single click. We ask if starter packs have the potential to tackle the critical problem of social bootstrapping in new online social networks? This paper is the first to address this question: we asses whether starter packs have been indeed helpful in supporting Bluesky growth. Our dataset includes $25.05 \times 10^6$ users and $335.42 \times 10^3$ starter packs with $1.73 \times 10^6$ members, covering the entire lifecycle of Bluesky. We study the usage of these starter packs, their ability to drive network and activity growth, and their potential downsides. We also quantify the benefits of starter packs for members and creators on user visibility and activity while identifying potential challenges. By evaluating starter packs' effectiveness and limitations, we contribute to the broader discourse on platform growth strategies and competitive innovation in the social media landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11605v1</guid>
      <category>cs.SI</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonhard Balduf, Saidu Sokoto, Onur Ascigil, Gareth Tyson, Ignacio Castro, Andrea Baronchelli, George Pavlou, Bj\"orn Scheuermann, Micha{\l} Kr\'ol</dc:creator>
    </item>
    <item>
      <title>A Stochastic Geometry Based Techno-Economic Analysis of RIS-Assisted Cellular Networks</title>
      <link>https://arxiv.org/abs/2501.12037</link>
      <description>arXiv:2501.12037v1 Announce Type: cross 
Abstract: Reconfigurable intelligent surfaces (RISs) are a promising technology for enhancing cellular network performance and yielding additional value to network operators. This paper proposes a techno-economic analysis of RIS-assisted cellular networks to guide operators in deciding between deploying additional RISs or base stations (BS). We assume a relative cost model that considers the total cost of ownership (TCO) of deploying additional nodes, either BSs or RISs. We assume a return on investment (RoI) that is proportional to the system's spectral efficiency. The latter is evaluated based on a stochastic geometry model that gives an integral formula for the ergodic rate in cellular networks equipped with RISs. The marginal RoI for any investment strategy is determined by the partial derivative of this integral expression with respect to node densities. We investigate two case studies: throughput enhancement and coverage hole mitigation. These examples demonstrate how operators could determine the optimal investment strategy in scenarios defined by the current densities of BSs and RISs, and their relative costs. Numerical results illustrate the evolution of ergodic rates based on the proposed investment strategy, demonstrating the investment decision-making process while considering technological and economic factors. This work quantitatively demonstrates that strategically investing in RISs can offer better system-level benefits than solely investing in BS densification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12037v1</guid>
      <category>cs.PF</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guodong Sun, Francois Baccelli, Luis Uzeda Garcia, Stefano Paris</dc:creator>
    </item>
    <item>
      <title>Graph neural network for in-network placement of real-time metaverse tasks in next-generation network</title>
      <link>https://arxiv.org/abs/2403.01780</link>
      <description>arXiv:2403.01780v2 Announce Type: replace 
Abstract: This study addresses the challenge of real-time metaverse applications by proposing an in-network placement and task-offloading solution for delay-constrained computing tasks in next-generation networks. The metaverse, envisioned as a parallel virtual world, requires seamless real-time experiences across diverse applications. The study introduces a software-defined networking (SDN)-based architecture and employs graph neural network (GNN) techniques for intelligent and adaptive task allocation in in-network computing (INC). Considering time constraints and computing capabilities, the proposed model optimally decides whether to offload rendering tasks to INC nodes or edge server. Extensive experiments demonstrate the superior performance of the proposed GNN model, achieving 97% accuracy compared to 72% for multilayer perceptron (MLP) and 70% for decision trees (DTs). The study fills the research gap in in-network placement for real-time metaverse applications, offering insights into efficient rendering task handling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01780v2</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sulaiman Muhammad Rashid, Ibrahim Aliyu, Il-Kwon Jeong, Tai-Won Um, Jinsul Kim</dc:creator>
    </item>
    <item>
      <title>TwinRAN: Twinning the 5G RAN in Azure Cloud</title>
      <link>https://arxiv.org/abs/2407.13340</link>
      <description>arXiv:2407.13340v2 Announce Type: replace 
Abstract: The proliferation of 5G technology necessitates advanced network management strategies to ensure optimal performance and reliability. Digital Twin (DT)s have emerged as a promising paradigm for modeling and simulating complex systems like the 5G Radio Access Network (RAN). In this paper, we present TwinRAN, a DT of the 5G RAN built leveraging the Azure DT platform. TwinRAN is built on top of the Open RAN (O-RAN) architecture and is agnostic to the vendor of the underlying equipment. We demonstrate three applications using TwinRAN and evaluate the required resources and their performance for a network with 800 users and eight gNBs. We first evaluate the performance and limitations of the Azure DT platform, measuring the latency under different conditions. The results from this evaluation allow us to optimize TwinRAN for the DT platform it uses. Then, we present the system's architectural design, emphasizing its components and interactions. We propose that two types of twin graphs be simultaneously maintained on the cloud: one for intercell operations, keeping a broad overview of all the cells in the network, and another where each cell is spawned in a separate Azure DT instance for more granular operation and monitoring of intracell tasks. We evaluate the performance and operating costs of TwinRAN for each of the three applications. The TwinRAN DT in the cloud can keep track of its physical twin within a few hundred milliseconds, extending its utility to many 5G network management tasks, some of which are shown in this paper. The novel framework for building and maintaining a DT of the 5G RAN presented in this paper offers network operators enhanced capabilities, empowering efficient deployments and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13340v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yash Deshpande, Eni Sulkaj, Wolfgang Kellerer</dc:creator>
    </item>
    <item>
      <title>Goal-Oriented Status Updating for Real-time Remote Inference over Networks with Two-Way~Delay</title>
      <link>https://arxiv.org/abs/2410.08706</link>
      <description>arXiv:2410.08706v2 Announce Type: replace 
Abstract: We study a setting where an intelligent model (e.g., a pre-trained neural network) predicts the real-time value of a target signal using data samples transmitted from a remote source according to a scheduling policy. The scheduler decides on i) the age of the samples to be sent, ii) when to send them, and iii) the length of each packet (i.e., the number of samples contained in each packet). The dependence of inference quality on the Age of Information (AoI) for a given packet length is modeled by a general relationship. Previous work assumed i.i.d. transmission delays with immediate feedback or were restricted to the case where inference performance degrades as the input data ages. Our formulation, in addition to capturing non-monotone age dependence, also covers Markovian delay on both forward and feedback links. We model this as an infinite-horizon average-cost Semi-Markov Decision Process. We obtain a closed-form solution that decides on (i) and (ii) for any constant packet length. The solution for when to send is an index-based threshold policy, where the index function is expressed in terms of the delay state and AoI at the receiver. The age of the packet selected is a function of the delay state. We separately optimize the value of the constant length. We also develop an index-based threshold policy for the variable length case, which allows a complexity reduction. In simulation results, we observe that our goal-oriented scheduler drops inference error down to one sixth with respect to age-based scheduling of unit-length packets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08706v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cagri Ari, Md Kamran Chowdhury Shisher, Yin Sun, Elif Uysal</dc:creator>
    </item>
    <item>
      <title>Fair AI-STA for Legacy Wi-Fi: Enhancing Sensing and Power Management with Deep Q-Learning</title>
      <link>https://arxiv.org/abs/2412.10874</link>
      <description>arXiv:2412.10874v2 Announce Type: replace 
Abstract: With the increasing complexity of Wi-Fi networks and the iterative evolution of 802.11 protocols, the Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) protocol faces significant challenges in achieving fair channel access and efficient resource allocation between legacy and modern Wi-Fi devices. To address these challenges, we propose an AI-driven Station (AI-STA) equipped with a Deep Q-Learning (DQN) module that dynamically adjusts its receive sensitivity threshold and transmit power. The AI-STA algorithm aims to maximize fairness in resource allocation while ensuring diverse Quality of Service (QoS) requirements are met. The performance of the AI-STA is evaluated through discrete event simulations in a Wi-Fi network, demonstrating that it outperforms traditional stations in fairness and QoS metrics. Although the AI-STA does not exhibit exceptionally superior performance, it holds significant potential for meeting QoS and fairness requirements with the inclusion of additional MAC parameters. The proposed AI-driven Sensitivity and Power algorithm offers a robust framework for optimizing sensitivity and power control in AI-STA devices within legacy Wi-Fi networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10874v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peini Yi, Wenchi Cheng, Zhanyu Ju, Jingqing Wang, Jinzhe Pan, Yuehui Ouyang, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Certificates in P and Subquadratic-Time Computation of Radius, Diameter, and all Eccentricities in Graphs</title>
      <link>https://arxiv.org/abs/1803.04660</link>
      <description>arXiv:1803.04660v3 Announce Type: replace-cross 
Abstract: In the context of fine-grained complexity, we investigate the notion of certificate enabling faster polynomial-time algorithms. We specifically target radius (minimum eccentricity), diameter (maximum eccentricity), and all-eccentricity computations for which quadratic-time lower bounds are known under plausible conjectures. In each case, we introduce a notion of certificate as a specific set of nodes from which appropriate bounds on all eccentricities can be derived in subquadratic time when this set has sublinear size. The existence of small certificates is a barrier against SETH-based lower bounds for these problems. We indeed prove that for graph classes with small certificates, there exist randomized subquadratic-time algorithms for computing the radius, the diameter, and all eccentricities respectively.Moreover, these notions of certificates are tightly related to algorithms probing the graph through one-to-all distance queries and allow to explain the efficiency of practical radius and diameter algorithms from the literature. Our formalization enables a novel primal-dual analysis of a classical approach for diameter computation that leads to algorithms for radius, diameter and all eccentricities with theoretical guarantees with respect to certain graph parameters. This is complemented by experimental results on various types of real-world graphs showing that these parameters appear to be low in practice. Finally, we obtain refined results for several graph classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:1803.04660v3</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feodor F. Dragan (UniBuc, ICI), Guillaume Ducoffe (UniBuc, ICI), Michel Habib (IRIF), Laurent Viennot (DI-ENS, ARGO)</dc:creator>
    </item>
    <item>
      <title>On The Complexity of Maximizing Temporal Reachability via Trip Temporalisation</title>
      <link>https://arxiv.org/abs/2111.08328</link>
      <description>arXiv:2111.08328v2 Announce Type: replace-cross 
Abstract: We consider the problem of assigning appearing times to the edges of a digraph in order to maximize the (average) temporal reachability between pairs of nodes. Motivated by the application to public transit networks, where edges cannot be scheduled independently one of another, we consider the setting where the edges are grouped into certain walks (called trips) in the digraph and where assigning the appearing time to the first edge of a trip forces the appearing times of the subsequent edges. In this setting, we show that, quite surprisingly, it is NP-complete to decide whether there exists an assignment of times connecting a given pair of nodes. This result allows us to prove that the problem of maximising the temporal reachability cannot be approximated within a factor better than some polynomial term in the size of the graph. We thus focus on the case where, for each pair of nodes, there exists an assignment of times such that one node is reachable from the other. We call this property strong temporalisability. It is a very natural assumption for the application to public transit networks. On the negative side, the problem of maximising the temporal reachability remains hard to approximate within a factor $\sqrt$ n/12 in that setting. Moreover, we show the existence of collections of trips that are strongly temporalisable but for which any assignment of starting times to the trips connects at most an O(1/ $\sqrt$ n) fraction of all pairs of nodes. On the positive side, we show that there must exist an assignment of times that connects a constant fraction of all pairs in the strongly temporalisable and symmetric case, that is, when the set of trips to be scheduled is such that, for each trip, there is a symmetric trip visiting the same nodes in reverse order. Keywords:edge labeling edge scheduled network network optimisation temporal graph temporal path temporal reachability time assignment</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.08328v2</guid>
      <category>cs.DM</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/net.22123</arxiv:DOI>
      <arxiv:journal_reference>Networks, 2022, 81 (2), pp.27</arxiv:journal_reference>
      <dc:creator>Filippo Brunelli (UPCit\'e, IRIF), Pierluigi Crescenzi (GSSI), Laurent Viennot (ARGO)</dc:creator>
    </item>
    <item>
      <title>ESASCF: Expertise Extraction, Generalization and Reply Framework for an Optimized Automation of Network Security Compliance</title>
      <link>https://arxiv.org/abs/2307.10967</link>
      <description>arXiv:2307.10967v2 Announce Type: replace-cross 
Abstract: The Cyber threats exposure has created worldwide pressure on organizations to comply with cyber security standards and policies for protecting their digital assets. Vulnerability assessment (VA) and Penetration Testing (PT) are widely adopted Security Compliance (SC) methods to identify security gaps and anticipate security breaches. In the computer networks context and despite the use of autonomous tools and systems, security compliance remains highly repetitive and resources consuming. In this paper, we proposed a novel method to tackle the ever-growing problem of efficiency and effectiveness in network infrastructures security auditing by formally introducing, designing, and developing an Expert-System Automated Security Compliance Framework (ESASCF) that enables industrial and open-source VA and PT tools and systems to extract, process, store and re-use the expertise in a human-expert way to allow direct application in similar scenarios or during the periodic re-testing. The implemented model was then integrated within the ESASCF and tested on different size networks and proved efficient in terms of time-efficiency and testing effectiveness allowing ESASCF to take over autonomously the SC in Re-testing and offloading Expert by automating repeated segments SC and thus enabling Experts to prioritize important tasks in Ad-Hoc compliance tests. The obtained results validate the performance enhancement notably by cutting the time required for an expert to 50% in the context of typical corporate networks first SC and 20% in re-testing, representing a significant cost-cutting. In addition, the framework allows a long-term impact illustrated in the knowledge extraction, generalization, and re-utilization, which enables better SC confidence independent of the human expert skills, coverage, and wrong decisions resulting in impactful false negatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10967v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed C. Ghanem, Thomas M. Chen, Mohamed A. Ferrag, Mohyi E. Kettouche</dc:creator>
    </item>
    <item>
      <title>Asynchronous Fractional Multi-Agent Deep Reinforcement Learning for Age-Minimal Mobile Edge Computing</title>
      <link>https://arxiv.org/abs/2409.16832</link>
      <description>arXiv:2409.16832v4 Announce Type: replace-cross 
Abstract: In the realm of emerging real-time networked applications like cyber-physical systems (CPS), the Age of Information (AoI) has merged as a pivotal metric for evaluating the timeliness. To meet the high computational demands, such as those in intelligent manufacturing within CPS, mobile edge computing (MEC) presents a promising solution for optimizing computing and reducing AoI. In this work, we study the timeliness of computational-intensive updates and explores jointly optimize the task updating and offloading policies to minimize AoI. Specifically, we consider edge load dynamics and formulate a task scheduling problem to minimize the expected time-average AoI. The fractional objective introduced by AoI and the semi-Markov game nature of the problem render this challenge particularly difficult, with existing approaches not directly applicable. To this end, we present a comprehensive framework to fractional reinforcement learning (RL). We first introduce a fractional single-agent RL framework and prove its linear convergence. We then extend this to a fractional multi-agent RL framework with a convergence analysis. To tackle the challenge of asynchronous control in semi-Markov game, we further design an asynchronous model-free fractional multi-agent RL algorithm, where each device makes scheduling decisions with the hybrid action space without knowing the system dynamics and decisions of other devices. Experimental results show that our proposed algorithms reduce the average AoI by up to 52.6% compared with the best baseline algorithm in our experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16832v4</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lyudong Jin, Ming Tang, Jiayu Pan, Meng Zhang, Hao Wang</dc:creator>
    </item>
    <item>
      <title>Smells-sus: Sustainability Smells in IaC</title>
      <link>https://arxiv.org/abs/2501.07676</link>
      <description>arXiv:2501.07676v2 Announce Type: replace-cross 
Abstract: Practitioners use Infrastructure as Code (IaC) scripts to efficiently configure IT infrastructures through machine-readable definition files. However, during the development of these scripts, some code patterns or deployment choices may lead to sustainability issues like inefficient resource utilization or redundant provisioning for example. We call this type of patterns sustainability smells. These inefficiencies pose significant environmental and financial challenges, given the growing scale of cloud computing. This research focuses on Terraform, a widely adopted IaC tool. Our study involves defining seven sustainability smells and validating them through a survey with 19 IaC practitioners. We utilized a dataset of 28,327 Terraform scripts from 395 open-source repositories. We performed a detailed qualitative analysis of a randomly sampled 1,860 Terraform scripts from the original dataset to identify code patterns that correspond to the sustainability smells and used the other 26,467 Terraform scripts to study the prevalence of the defined sustainability smells. Our results indicate varying prevalence rates of these smells across the dataset. The most prevalent smell is Monolithic Infrastructure, which appears in 9.67\% of the scripts. Additionally, our findings highlight the complexity of conducting root cause analysis for sustainability issues, as these smells often arise from a confluence of script structures, configuration choices, and deployment contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07676v2</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Seif Kosbar, Mohammad Hamdaqa</dc:creator>
    </item>
  </channel>
</rss>
