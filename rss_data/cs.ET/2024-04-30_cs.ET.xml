<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 May 2024 04:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 01 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An Extensive Survey of Digital Image Steganography: State of the Art</title>
      <link>https://arxiv.org/abs/2404.19548</link>
      <description>arXiv:2404.19548v1 Announce Type: new 
Abstract: The need to protect sensitive information privacy duringinformation exchange over the internet/intranet has led towider adoption of cryptography and steganography. The cryptography approaches convert the information into an unreadable format however draws the attention of cryptanalyst owing to the uncommon random nature flow of the bytes when viewing the flowing structured bytes on a computer. While steganography, in contrast, conceals the very existence of covert communication using digital media. Although any digital media (text, image, video, audio) can covey the sensitive information, the media with higher redundant bits are more favorable for embedding the sensitive information without distorting the media. Digital images are majorly used in conveying sensitive information compared to others owing to their higher rate of tolerating distortions, highly available, smaller sizes with high redundant bits. However, the need for maximizing the redundancy bits for the optimum embedding of secret information has been a paramount issue due to the imperceptibility prerequisite which deteriorates with an increase in payload thus, resulting in a tradeoff. This has limited steganography to only applications with lower payload requirements, thus limiting the adoption for wider deployment. This paper critically analyzes the current steganographic techniques, recent trends, and challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19548v1</guid>
      <category>cs.ET</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Idakwo M. A., Muazu M. B., Adedokun A. E., Sadiq B. O</dc:creator>
    </item>
    <item>
      <title>DelGrad: Exact gradients in spiking networks for learning transmission delays and weights</title>
      <link>https://arxiv.org/abs/2404.19165</link>
      <description>arXiv:2404.19165v1 Announce Type: cross 
Abstract: Spiking neural networks (SNNs) inherently rely on the timing of signals for representing and processing information. Transmission delays play an important role in shaping these temporal characteristics. Recent work has demonstrated the substantial advantages of learning these delays along with synaptic weights, both in terms of accuracy and memory efficiency. However, these approaches suffer from drawbacks in terms of precision and efficiency, as they operate in discrete time and with approximate gradients, while also requiring membrane potential recordings for calculating parameter updates. To alleviate these issues, we propose an analytical approach for calculating exact loss gradients with respect to both synaptic weights and delays in an event-based fashion. The inclusion of delays emerges naturally within our proposed formalism, enriching the model's search space with a temporal dimension. Our algorithm is purely based on the timing of individual spikes and does not require access to other variables such as membrane potentials. We explicitly compare the impact on accuracy and parameter efficiency of different types of delays - axonal, dendritic and synaptic. Furthermore, while previous work on learnable delays in SNNs has been mostly confined to software simulations, we demonstrate the functionality and benefits of our approach on the BrainScaleS-2 neuromorphic platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19165v1</guid>
      <category>cs.NE</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian G\"oltz, Jimmy Weber, Laura Kriener, Peter Lake, Melika Payvand, Mihai A. Petrovici</dc:creator>
    </item>
    <item>
      <title>Connecting physics to systems with modular spin-circuits</title>
      <link>https://arxiv.org/abs/2404.19345</link>
      <description>arXiv:2404.19345v1 Announce Type: cross 
Abstract: An emerging paradigm in modern electronics is that of CMOS + $\sf X$ requiring the integration of standard CMOS technology with novel materials and technologies denoted by $\sf X$. In this context, a crucial challenge is to develop accurate circuit models for $\sf X$ that are compatible with standard models for CMOS-based circuits and systems. In this perspective we present physics-based, experimentally benchmarked modular circuit models that can be used to evaluate a class of CMOS + $\sf X$ systems, where $\sf X$ denotes magnetic and spintronic materials and phenomena. This class of materials is particularly challenging because they go beyond conventional charge-based phenomena and involve the spin degree of freedom which involves non-trivial quantum effects. Starting from density matrices $-$ the central quantity in quantum transport $-$ using well-defined approximations, it is possible to obtain spin-circuits that generalize ordinary circuit theory to 4-component currents and voltages (1 for charge and 3 for spin). With step-by-step examples that progressively go higher in the computing stack, we illustrate how the spin-circuit approach can be used to start from the physics of magnetism and spintronics to enable accurate system-level evaluations. We believe the core approach can be extended to include other quantum degrees of freedom like valley and pseudospins starting from corresponding density matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19345v1</guid>
      <category>cond-mat.mes-hall</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kemal Selcuk, Saleh Bunaiyan, Nihal Sanjay Singh, Shehrin Sayed, Samiran Ganguly, Giovanni Finocchio, Supriyo Datta, Kerem Y. Camsari</dc:creator>
    </item>
    <item>
      <title>EvGNN: An Event-driven Graph Neural Network Accelerator for Edge Vision</title>
      <link>https://arxiv.org/abs/2404.19489</link>
      <description>arXiv:2404.19489v1 Announce Type: cross 
Abstract: Edge vision systems combining sensing and embedded processing promise low-latency, decentralized, and energy-efficient solutions that forgo reliance on the cloud. As opposed to conventional frame-based vision sensors, event-based cameras deliver a microsecond-scale temporal resolution with sparse information encoding, thereby outlining new opportunities for edge vision systems. However, mainstream algorithms for frame-based vision, which mostly rely on convolutional neural networks (CNNs), can hardly exploit the advantages of event-based vision as they are typically optimized for dense matrix-vector multiplications. While event-driven graph neural networks (GNNs) have recently emerged as a promising solution for sparse event-based vision, their irregular structure is a challenge that currently hinders the design of efficient hardware accelerators. In this paper, we propose EvGNN, the first event-driven GNN accelerator for low-footprint, ultra-low-latency, and high-accuracy edge vision with event-based cameras. It relies on three central ideas: (i) directed dynamic graphs exploiting single-hop nodes with edge-free storage, (ii) event queues for the efficient identification of local neighbors within a spatiotemporally decoupled search range, and (iii) a novel layer-parallel processing scheme enabling the low-latency execution of multi-layer GNNs. We deployed EvGNN on a Xilinx KV260 Ultrascale+ MPSoC platform and benchmarked it on the N-CARS dataset for car recognition, demonstrating a classification accuracy of 87.8% and an average latency per event of 16$\mu$s, thereby enabling real-time, microsecond-resolution event-based vision at the edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19489v1</guid>
      <category>cs.CV</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufeng Yang, Adrian Kneip, Charlotte Frenkel</dc:creator>
    </item>
    <item>
      <title>Ferroelectrically-enhanced Schottky barrier transistors for Logic-in-Memory applications</title>
      <link>https://arxiv.org/abs/2404.19535</link>
      <description>arXiv:2404.19535v1 Announce Type: cross 
Abstract: Artificial neural networks (ANNs) have had an enormous impact on a multitude of sectors, from research to industry, generating an unprecedented demand for tailor-suited hardware platforms. Their training and execution is highly memory-intensive, clearly evidencing the limitations affecting the currently available hardware based on the von Neumann architecture, which requires frequent data shuttling due to the physical separation of logic and memory units. This does not only limit the achievable performances but also greatly increases the energy consumption, hindering the integration of ANNs into low-power platforms. New Logic in Memory (LiM) architectures, able to unify memory and logic functionalities into a single component, are highly promising for overcoming these limitations, by drastically reducing the need of data transfers. Recently, it has been shown that a very flexible platform for logic applications can be realized recurring to a multi-gated Schottky-Barrier Field Effect Transistor (SBFET). If equipped with memory capabilities, this architecture could represent an ideal building block for versatile LiM hardware. To reach this goal, here we investigate the integration of a ferroelectric Hf$_{0.5}$Zr$_{0.5}$O$_2$ (HZO) layer onto Dual Top Gated SBFETs. We demonstrate that HZO polarization charges can be successfully employed to tune the height of the two Schottky barriers, influencing the injection behavior, thus defining the transistor mode, switching it between n and p-type transport. The modulation strength is strongly dependent on the polarization pulse height, allowing for the selection of multiple current levels. All these achievable states can be well retained over time, thanks to the HZO stability. The presented result show how ferroelectric-enhanced SBFETs are promising for the realization of novel LiM hardware, enabling low-power circuits for ANNs execution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19535v1</guid>
      <category>physics.app-ph</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Nazzari, Lukas Wind, Masiar Sistani, Dominik Mayr, Kihye Kim, Walter M. Weber</dc:creator>
    </item>
    <item>
      <title>Radio Resource Management Design for RSMA: Optimization of Beamforming, User Admission, and Discrete/Continuous Rates with Imperfect SIC</title>
      <link>https://arxiv.org/abs/2404.19611</link>
      <description>arXiv:2404.19611v1 Announce Type: cross 
Abstract: This paper investigates the radio resource management (RRM) design for multiuser rate-splitting multiple access (RSMA), accounting for various characteristics of practical wireless systems, such as the use of discrete rates, the inability to serve all users, and the imperfect successive interference cancellation (SIC). Specifically, failure to consider these characteristics in RRM design may lead to inefficient use of radio resources. Therefore, we formulate the RRM of RSMA as optimization problems to maximize respectively the weighted sum rate (WSR) and weighted energy efficiency (WEE), and jointly optimize the beamforming, user admission, discrete/continuous rates, accounting for imperfect SIC, which result in nonconvex mixed-integer nonlinear programs that are challenging to solve. Despite the difficulty of the optimization problems, we develop algorithms that can find high-quality solutions. We show via simulations that carefully accounting for the aforementioned characteristics, can lead to significant gains. Precisely, by considering that transmission rates are discrete, the transmit power can be utilized more intelligently, allocating just enough power to guarantee a given discrete rate. Additionally, we reveal that user admission plays a crucial role in RSMA, enabling additional gains compared to random admission by facilitating the servicing of selected users with mutually beneficial channel characteristics. Furthermore, provisioning for possibly imperfect SIC makes RSMA more robust and reliable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19611v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L. F. Abanto-Leon, A. Krishnamoorthy, A. Garcia-Saavedra, G. H. Sim, R. Schober, M. Hollick</dc:creator>
    </item>
    <item>
      <title>Quantum Cloud Computing: Trends and Challenges</title>
      <link>https://arxiv.org/abs/2404.19612</link>
      <description>arXiv:2404.19612v1 Announce Type: cross 
Abstract: Quantum computing (QC) is a new paradigm that will revolutionize various areas of computing, especially cloud computing. QC, still in its infancy, is a costly technology capable of operating in highly isolated environments due to its rapid response to environmental factors. For this reason, it is still a challenging technology for researchers to reach. Integrating QC into an isolated remote server, like a cloud, and making it available to users can overcome these problems. Furthermore, experts predict that QC, with its ability to swiftly resolve complex and computationally intensive operations, will offer significant benefits in systems that process large amounts of data, like cloud computing. This article presents the vision and challenges for the quantum cloud computing (QCC) paradigm that will emerge with the integration of quantum and cloud computing. Next, we present the advantages of QC over classical computing applications. We analyze the effects of QC on cloud systems, such as cost, security, and scalability. Besides all of these advantages, we highlight research gaps in QCC, such as qubit stability and efficient resource allocation. This article identifies QCC's advantages and challenges for future research, highlighting research gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19612v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Muhammed Golec, Emir Sahin Hatay, Mustafa Golec, Murat Uyar, Merve Golec, Sukhpal Singh Gill</dc:creator>
    </item>
    <item>
      <title>Realization of a programmable multi-purpose photonic quantum memory with over-thousand qubit manipulations</title>
      <link>https://arxiv.org/abs/2311.10292</link>
      <description>arXiv:2311.10292v3 Announce Type: replace-cross 
Abstract: Quantum networks can enable various applications such as distributed quantum computing, long-distance quantum communication, and network-based quantum sensing with unprecedented performances. One of the most important building blocks for a quantum network is a photonic quantum memory which serves as the interface between the communication channel and the local functional unit. A programmable quantum memory which can process a large stream of flying qubits and fulfill the requirements of multiple core functions in a quantum network is still to-be-realized. Here we report a high-performance quantum memory which can simultaneously store 72 optical qubits carried by 144 spatially separated atomic ensembles and support up to a thousand consecutive write or read operations in a random access way, two orders of magnitude larger than the previous record. Due to the built-in programmability, this quantum memory can be adapted on-demand for several functions. As example applications, we realize quantum queue, stack, and buffer which closely resemble the counterpart devices for classical information processing. We further demonstrate the synchronization and reshuffle of 4 entangled pairs of photonic pulses with probabilistic arrival time and arbitrary release order via the memory, which is an essential requirement for the realization of quantum repeaters and efficient routing in quantum networks. Realization of this multi-purpose programmable quantum memory thus constitutes a key enabling building block for future large-scale fully-functional quantum networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10292v3</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>physics.optics</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevX.14.021018</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. X 14, 021018 (2024)</arxiv:journal_reference>
      <dc:creator>Sheng Zhang, Jixuan Shi, Zhaibin Cui, Ye Wang, Yukai Wu, Luming Duan, Yunfei Pu</dc:creator>
    </item>
    <item>
      <title>Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data</title>
      <link>https://arxiv.org/abs/2404.01413</link>
      <description>arXiv:2404.01413v2 Announce Type: replace-cross 
Abstract: The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops proposed that such loops would lead to a phenomenon termed model collapse, under which performance progressively degrades with each model-data feedback iteration until fitted models become useless. However, those studies largely assumed that new data replace old data over time, where an arguably more realistic assumption is that data accumulate over time. In this paper, we ask: what effect does accumulating data have on model collapse? We empirically study this question by pretraining sequences of language models on text corpora. We confirm that replacing the original real data by each generation's synthetic data does indeed tend towards model collapse, then demonstrate that accumulating the successive generations of synthetic data alongside the original real data avoids model collapse; these results hold across a range of model sizes, architectures, and hyperparameters. We obtain similar results for deep generative models on other types of real data: diffusion models for molecule conformation generation and variational autoencoders for image generation. To understand why accumulating data can avoid model collapse, we use an analytically tractable framework introduced by prior work in which a sequence of linear models are fit to the previous models' outputs. Previous work used this framework to show that if data are replaced, the test error increases with the number of model-fitting iterations; we extend this argument to prove that if data instead accumulate, the test error has a finite upper bound independent of the number of iterations, meaning model collapse no longer occurs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01413v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Henry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai, Andrey Gromov, Daniel A. Roberts, Diyi Yang, David L. Donoho, Sanmi Koyejo</dc:creator>
    </item>
    <item>
      <title>Proof-of-Learning with Incentive Security</title>
      <link>https://arxiv.org/abs/2404.09005</link>
      <description>arXiv:2404.09005v3 Announce Type: replace-cross 
Abstract: Most concurrent blockchain systems rely heavily on the Proof-of-Work (PoW) or Proof-of-Stake (PoS) mechanisms for decentralized consensus and security assurance. However, the substantial energy expenditure stemming from computationally intensive yet meaningless tasks has raised considerable concerns surrounding traditional PoW approaches, The PoS mechanism, while free of energy consumption, is subject to security and economic issues. Addressing these issues, the paradigm of Proof-of-Useful-Work (PoUW) seeks to employ challenges of practical significance as PoW, thereby imbuing energy consumption with tangible value. While previous efforts in Proof of Learning (PoL) explored the utilization of deep learning model training SGD tasks as PoUW challenges, recent research has revealed its vulnerabilities to adversarial attacks and the theoretical hardness in crafting a byzantine-secure PoL mechanism. In this paper, we introduce the concept of incentive-security that incentivizes rational provers to behave honestly for their best interest, bypassing the existing hardness to design a PoL mechanism with computational efficiency, a provable incentive-security guarantee and controllable difficulty. Particularly, our work is secure against two attacks to the recent work of Jia et al. [2021], and also improves the computational overhead from $\Theta(1)$ to $O(\frac{\log E}{E})$. Furthermore, while most recent research assumes trusted problem providers and verifiers, our design also guarantees frontend incentive-security even when problem providers are untrusted, and verifier incentive-security that bypasses the Verifier's Dilemma. By incorporating ML training into blockchain consensus mechanisms with provable guarantees, our research not only proposes an eco-friendly solution to blockchain systems, but also provides a proposal for a completely decentralized computing power market in the new AI age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09005v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zishuo Zhao, Zhixuan Fang, Xuechao Wang, Xi Chen, Yuan Zhou</dc:creator>
    </item>
  </channel>
</rss>
