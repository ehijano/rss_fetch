<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Feb 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scaling Optimization Over Uncertainty via Compilation</title>
      <link>https://arxiv.org/abs/2502.18728</link>
      <description>arXiv:2502.18728v1 Announce Type: new 
Abstract: Probabilistic inference is fundamentally hard, yet many tasks require optimization on top of inference, which is even harder. We present a new \textit{optimization-via-compilation} strategy to scalably solve a certain class of such problems. In particular, we introduce a new intermediate representation (IR), binary decision diagrams weighted by a novel notion of \textit{branch-and-bound semiring}, that enables a scalable branch-and-bound based optimization procedure. This IR automatically \textit{factorizes} problems through program structure and \textit{prunes} suboptimal values via a straightforward branch-and-bound style algorithm to find optima. Additionally, the IR is naturally amenable to \textit{staged compilation}, allowing the programmer to query for optima mid-compilation to inform further executions of the program. We showcase the effectiveness and flexibility of the IR by implementing two performant languages that both compile to it: \textsc{dappl} and \textsc{pineappl}. \textsc{dappl} is a functional language that solves maximum expected utility problems with first-class support for rewards, decision making, and conditioning. \textsc{pineappl} is an imperative language that performs exact probabilistic inference with support for nested marginal maximum a posteriori (MMAP) optimization via staging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18728v1</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minsung Cho, John Gouwar, Steven Holtzen</dc:creator>
    </item>
    <item>
      <title>Adaptive Shielding via Parametric Safety Proofs</title>
      <link>https://arxiv.org/abs/2502.18879</link>
      <description>arXiv:2502.18879v1 Announce Type: new 
Abstract: A major challenge to deploying cyber-physical systems with learning-enabled controllers is to ensure their safety, especially in the face of changing environments that necessitate runtime knowledge acquisition. Model-checking and automated reasoning have been successfully used for shielding, i.e., to monitor untrusted controllers and override potentially unsafe decisions, but only at the cost of hard tradeoffs in terms of expressivity, safety, adaptivity, precision and runtime efficiency. We propose a programming-language framework that allows experts to statically specify adaptive shields for learning-enabled agents, which enforce a safe control envelope that gets more permissive as knowledge is gathered at runtime. A shield specification provides a safety model that is parametric in the current agent's knowledge. In addition, a nondeterministic inference strategy can be specified using a dedicated domain-specific language, enforcing that such knowledge parameters are inferred at runtime in a statistically-sound way. By leveraging language design and theorem proving, our proposed framework empowers experts to design adaptive shields with an unprecedented level of modeling flexibility, while providing rigorous, end-to-end probabilistic safety guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18879v1</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Feng, Jun Zhu, Andr\'e Platzer, Jonathan Laurent</dc:creator>
    </item>
    <item>
      <title>ClassInvGen: Class Invariant Synthesis using Large Language Models</title>
      <link>https://arxiv.org/abs/2502.18917</link>
      <description>arXiv:2502.18917v1 Announce Type: new 
Abstract: Formal program specifications in the form of preconditions, postconditions, and class invariants have several benefits for the construction and maintenance of programs. They not only aid in program understanding due to their unambiguous semantics but can also be enforced dynamically (or even statically when the language supports a formal verifier). However, synthesizing high-quality specifications in an underlying programming language is limited by the expressivity of the specifications or the need to express them in a declarative manner. Prior work has demonstrated the potential of large language models (LLMs) for synthesizing high-quality method pre/postconditions for Python and Java, but does not consider class invariants.
  In this work, we describe ClassInvGen, a method for co-generating executable class invariants and test inputs to produce high-quality class invariants for a mainstream language such as C++, leveraging LLMs' ability to synthesize pure functions. We show that ClassInvGen outperforms a pure LLM-based technique to generate specifications (from code) as well as prior data-driven invariant inference techniques such as Daikon. We contribute a benchmark of standard C++ data structures along with a harness that can help measure both the correctness and completeness of generated specifications using tests and mutants. We also demonstrate its applicability to real-world code by performing a case study on several classes within a widely used and high-integrity C++ codebase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18917v1</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuyue Sun, Viraj Agashe, Saikat Chakraborty, Jubi Taneja, Clark Barrett, David Dill, Xiaokang Qiu, Shuvendu K. Lahiri</dc:creator>
    </item>
    <item>
      <title>Language-Parametric Reference Synthesis (Extended)</title>
      <link>https://arxiv.org/abs/2502.19143</link>
      <description>arXiv:2502.19143v1 Announce Type: new 
Abstract: Modern Integrated Development Environments (IDEs) offer automated refactorings to aid programmers in developing and maintaining software. However, implementing sound automated refactorings is challenging, as refactorings may inadvertently introduce name-binding errors or cause references to resolve to incorrect declarations. To address these issues, previous work by Sch\"afer et al. proposed replacing concrete references with locked references to separate binding preservation from transformation. Locked references vacuously resolve to a specific declaration, and after transformation must be replaced with concrete references that also resolve to that declaration. Synthesizing these references requires a faithful inverse of the name lookup functions of the underlying language.
  Manually implementing such inverse lookup functions is challenging due to the complex name-binding features in modern programming languages. Instead, we propose to automatically derive this function from type system specifications written in the Statix meta-DSL. To guide the synthesis of qualified references we use scope graphs, which represent the binding structure of a program, to infer their names and discover their syntactic structure.
  We evaluate our approach by synthesizing concrete references for locked references in 2528 Java, 196 ChocoPy, and 49 Featherweight Generic Java test programs. Our approach yields a principled language-parametric method for synthesizing references.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19143v1</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3720481</arxiv:DOI>
      <dc:creator>Daniel A. A. Pelsmaeker, Aron Zwaan, Casper Bach, Arjan J. Mooij</dc:creator>
    </item>
    <item>
      <title>The Simulation Semantics of Synthesisable Verilog</title>
      <link>https://arxiv.org/abs/2502.19348</link>
      <description>arXiv:2502.19348v1 Announce Type: new 
Abstract: Despite numerous previous formalisation projects targeting Verilog, the semantics of Verilog defined by the Verilog standard -- Verilog's simulation semantics -- has thus far eluded definitive mathematical formalisation. Previous projects on formalising the semantics have made good progress but no previous project provides a formalisation that can be used to execute or formally reason about real-world hardware designs. In this paper, we show that the reason for this is that the Verilog standard is inconsistent both with Verilog practice and itself. We pinpoint a series of problems in the Verilog standard that we have identified in how the standard defines the semantics of the subset of Verilog used to describe hardware designs, that is, the synthesisable subset of Verilog. We show how the most complete Verilog formalisation to date inherits these problems and how, after we repair these problems in an executable implementation of the formalisation, the repaired implementation can be used to execute real-world hardware designs. The existing formalisation together with the repairs hence constitute the first formalisation of Verilog's simulation semantics compatible with real-world hardware designs. Additionally, to make the results of this paper accessible to a wider (nonmathematical) audience, we provide a visual formalisation of Verilog's simulation semantics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19348v1</guid>
      <category>cs.PL</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3720484</arxiv:DOI>
      <arxiv:journal_reference>OOPSLA 2025</arxiv:journal_reference>
      <dc:creator>Andreas L\"o\"ow</dc:creator>
    </item>
    <item>
      <title>Qmod: Expressive High-Level Quantum Modeling</title>
      <link>https://arxiv.org/abs/2502.19368</link>
      <description>arXiv:2502.19368v1 Announce Type: cross 
Abstract: Quantum computing hardware is advancing at a rapid pace, yet the lack of high-level programming abstractions remains a serious bottleneck in the development of new applications. Widely used frameworks still rely on gate-level circuit descriptions, causing the algorithm's functional intent to become lost in low-level implementation details, and hindering flexibility and reuse. While various high-level quantum programming languages have emerged in recent years - offering a significant step toward higher abstraction - many still lack support for classical-like expression syntax, and native constructs for useful quantum algorithmic idioms. This paper presents Qmod, a high-level quantum programming language designed to capture algorithmic intent in natural terms while delegating implementation decisions to automation. Qmod introduces quantum numeric variables and expressions, including digital fixed-point arithmetic tuned for compact representations and optimal resource usage. Beyond digital encoding, Qmod also supports non-digital expression modes - phase and amplitude encoding - frequently exploited by quantum algorithms to achieve computational advantages. We describe the language's constructs, demonstrate practical usage examples, and outline future work on evaluating Qmod across a broader set of use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19368v1</guid>
      <category>quant-ph</category>
      <category>cs.PL</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matan Vax, Peleg Emanuel, Eyal Cornfeld, Israel Reichental, Ori Opher, Ori Roth, Tal Michaeli, Lior Preminger, Lior Gazit, Amir Naveh, Yehuda Naveh</dc:creator>
    </item>
    <item>
      <title>Foundations for Deductive Verification of Continuous Probabilistic Programs: From Lebesgue to Riemann and Back</title>
      <link>https://arxiv.org/abs/2502.19388</link>
      <description>arXiv:2502.19388v1 Announce Type: cross 
Abstract: We lay out novel foundations for the computer-aided verification of guaranteed bounds on expected outcomes of imperative probabilistic programs featuring (i) general loops, (ii) continuous distributions, and (iii) conditioning. To handle loops we rely on user-provided quantitative invariants, as is well established. However, in the realm of continuous distributions, invariant verification becomes extremely challenging due to the presence of integrals in expectation-based program semantics. Our key idea is to soundly under- or over-approximate these integrals via Riemann sums. We show that this approach enables the SMT-based invariant verification for programs with a fairly general control flow structure. On the theoretical side, we prove convergence of our Riemann approximations, and establish coRE-completeness of the central verification problems. On the practical side, we show that our approach enables to use existing automated verifiers targeting discrete probabilistic programs for the verification of programs involving continuous sampling. Towards this end, we implement our approach in the recent quantitative verification infrastructure Caesar by encoding Riemann sums in its intermediate verification language. We present several promising case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19388v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Batz, Joost-Pieter Katoen, Francesca Randone, Tobias Winkler</dc:creator>
    </item>
    <item>
      <title>Crash-Stop Failures in Asynchronous Multiparty Session Types</title>
      <link>https://arxiv.org/abs/2311.11851</link>
      <description>arXiv:2311.11851v5 Announce Type: replace-cross 
Abstract: Session types provide a typing discipline for message-passing systems. However, their theory often assumes an ideal world: one in which everything is reliable and without failures. Yet this is in stark contrast with distributed systems in the real world. To address this limitation, we introduce a new asynchronous multiparty session types (MPST) theory with crash-stop failures, where processes may crash arbitrarily and cease to interact after crashing. We augment asynchronous MPST and processes with crash handling branches, and integrate crash-stop failure semantics into types and processes. Our approach requires no user-level syntax extensions for global types, and features a formalisation of global semantics, which captures complex behaviours induced by crashed/crash handling processes.
  Our new theory covers the entire spectrum, ranging from the ideal world of total reliability to entirely unreliable scenarios where any process may crash, using optional reliability assumptions. Under these assumptions, we demonstrate the sound and complete correspondence between global and local type semantics, which guarantee deadlock-freedom, protocol conformance, and liveness of well-typed processes by construction, even in the presence of crashes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11851v5</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam D. Barwell, Ping Hou, Nobuko Yoshida, Fangyi Zhou</dc:creator>
    </item>
    <item>
      <title>Guarding the Privacy of Label-Only Access to Neural Network Classifiers via iDP Verification</title>
      <link>https://arxiv.org/abs/2502.16519</link>
      <description>arXiv:2502.16519v2 Announce Type: replace-cross 
Abstract: Neural networks are susceptible to privacy attacks that can extract private information of the training set. To cope, several training algorithms guarantee differential privacy (DP) by adding noise to their computation. However, DP requires to add noise considering every possible training set. This leads to a significant decrease in the network's accuracy. Individual DP (iDP) restricts DP to a given training set. We observe that some inputs deterministically satisfy iDP without any noise. By identifying them, we can provide iDP label-only access to the network with a minor decrease to its accuracy. However, identifying the inputs that satisfy iDP without any noise is highly challenging. Our key idea is to compute the iDP deterministic bound (iDP-DB), which overapproximates the set of inputs that do not satisfy iDP, and add noise only to their predicted labels. To compute the tightest iDP-DB, which enables to guard the label-only access with minimal accuracy decrease, we propose LUCID, which leverages several formal verification techniques. First, it encodes the problem as a mixed-integer linear program, defined over a network and over every network trained identically but without a unique data point. Second, it abstracts a set of networks using a hyper-network. Third, it eliminates the overapproximation error via a novel branch-and-bound technique. Fourth, it bounds the differences of matching neurons in the network and the hyper-network and employs linear relaxation if they are small. We show that LUCID can provide classifiers with a perfect individuals' privacy guarantee (0-iDP) -- which is infeasible for DP training algorithms -- with an accuracy decrease of 1.4%. For more relaxed $\varepsilon$-iDP guarantees, LUCID has an accuracy decrease of 1.2%. In contrast, existing DP training algorithms reduce the accuracy by 12.7%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16519v2</guid>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anan Kabaha, Dana Drachsler-Cohen</dc:creator>
    </item>
  </channel>
</rss>
