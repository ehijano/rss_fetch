<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 May 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>DX100: A Programmable Data Access Accelerator for Indirection</title>
      <link>https://arxiv.org/abs/2505.23073</link>
      <description>arXiv:2505.23073v1 Announce Type: new 
Abstract: Indirect memory accesses frequently appear in applications where memory bandwidth is a critical bottleneck. Prior indirect memory access proposals, such as indirect prefetchers, runahead execution, fetchers, and decoupled access/execute architectures, primarily focus on improving memory access latency by loading data ahead of computation but still rely on the DRAM controllers to reorder memory requests and enhance memory bandwidth utilization. DRAM controllers have limited visibility to future memory accesses due to the small capacity of request buffers and the restricted memory-level parallelism of conventional core and memory systems. We introduce DX100, a programmable data access accelerator for indirect memory accesses. DX100 is shared across cores to offload bulk indirect memory accesses and associated address calculation operations. DX100 reorders, interleaves, and coalesces memory requests to improve DRAM row-buffer hit rate and memory bandwidth utilization. DX100 provides a general-purpose ISA to support diverse access types, loop patterns, conditional accesses, and address calculations. To support this accelerator without significant programming efforts, we discuss a set of MLIR compiler passes that automatically transform legacy code to utilize DX100. Experimental evaluations on 12 benchmarks spanning scientific computing, database, and graph applications show that DX100 achieves performance improvements of 2.6x over a multicore baseline and 2.0x over the state-of-the-art indirect prefetcher.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23073v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3695053.3731015</arxiv:DOI>
      <dc:creator>Alireza Khadem, Kamalavasan Kamalakkannan, Zhenyan Zhu, Akash Poptani, Yufeng Gu, Jered Benjamin Dominguez-Trujillo, Nishil Talati, Daichi Fujiki, Scott Mahlke, Galen Shipman, Reetuparna Das</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores</title>
      <link>https://arxiv.org/abs/2505.23351</link>
      <description>arXiv:2505.23351v1 Announce Type: new 
Abstract: Optimizing performance and energy efficiency in many-core processors, especially within Non-Uniform Cache Access (NUCA) architectures, remains a critical challenge. The performance heterogeneity inherent in S-NUCA systems complicates task scheduling due to varying cache access latencies across cores. This paper introduces a novel QoS management policy to maintain application execution within predefined Quality of Service (QoS) targets, measured using the Application Heartbeats framework. QoS metrics like Heartbeats ensure predictable application performance in dynamic computing environments. The proposed policy dynamically controls QoS by orchestrating task migrations within the S-NUCA many-core system and adjusting the clock frequency of cores. After satisfying the QoS objectives, the policy optimizes energy efficiency, reducing overall system energy consumption without compromising performance constraints. Our work leverages the state-of-the-art multi-/many-core simulator {\em HotSniper}. We have extended it with two key components: an integrated heartbeat framework for precise, application-specific performance monitoring, and our QoS management policy that maintains application QoS requirements while minimizing the system's energy consumption. Experimental evaluations demonstrate that our approach effectively maintains desired QoS levels and achieves 18.7\% energy savings compared to state-of-the-art scheduling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23351v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sudam M. Wasala, Jurre Wolff, Yixian Shen, Anuj Pathania, Clemens Grelck, Andy D. Pimentel</dc:creator>
    </item>
    <item>
      <title>A Novel Cost-Effective MIMO Architecture with Ray Antenna Array for Enhanced Wireless Communication Performance</title>
      <link>https://arxiv.org/abs/2505.23394</link>
      <description>arXiv:2505.23394v1 Announce Type: new 
Abstract: This paper proposes a novel multi-antenna architecture, termed ray antenna array (RAA), which practically enables flexible beamforming and also enhances wireless communication performance for high frequency systems in a cost-effective manner. RAA consists of a large number of inexpensive antenna elements and a few radio frequency (RF) chains. These antenna elements are arranged in a novel ray like structure, where each ray corresponds to one simple uniform linear array (sULA) with a carefully designed orientation. The antenna elements within each sULA are directly connected, so that each sULA is able to form a beam towards a direction matching the ray orientation without relying on any analog or digital beamforming. By further designing a ray selection network (RSN), appropriate sULAs are selected to connect to the RF chains for subsequent baseband processing. Compared to conventional multi-antenna architectures such as the uniform linear array (ULA) with hybrid analog/digital beamforming (HBF), the proposed RAA enjoys three appealing advantages: (i) finer and uniform angular resolution for all signal directions; (ii) enhanced beamforming gain by using antenna elements with higher directivity, as each sULA is only responsible for a small portion of the total angle coverage range; and (iii) dramatically reduced hardware cost since no phase shifters are required, which are expensive and difficult to design in high-frequency systems such as mmWave and THz systems. To validate such advantages, we first present the input-output mathematical model for RAA-based wireless communications. Efficient algorithms for joint RAA beamforming and ray selection are then proposed for single-user and multi-user RAA-based wireless communications. Simulation results demonstrate that RAA achieves superior performance compared to the conventional ULA with HBF, while significantly reducing hardware cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23394v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenjun Dong, Zhiwen Zhou, Yong Zeng</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Mapping and Synthesis of Approximate R-Blocks CGRAs</title>
      <link>https://arxiv.org/abs/2505.23553</link>
      <description>arXiv:2505.23553v1 Announce Type: new 
Abstract: The ever-increasing complexity and operational diversity of modern Neural Networks (NNs) have caused the need for low-power and, at the same time, high-performance edge devices for AI applications. Coarse Grained Reconfigurable Architectures (CGRAs) form a promising design paradigm to address these challenges, delivering a close-to-ASIC performance while allowing for hardware programmability. In this paper, we introduce a novel end-to-end exploration and synthesis framework for approximate CGRA processors that enables transparent and optimized integration and mapping of state-of-the-art approximate multiplication components into CGRAs. Our methodology introduces a per-channel exploration strategy that maps specific output features onto approximate components based on accuracy degradation constraints. This enables the optimization of the system's energy consumption while retaining the accuracy above a certain threshold. At the circuit level, the integration of approximate components enables the creation of voltage islands that operate at reduced voltage levels, which is attributed to their inherently shorter critical paths. This key enabler allows us to effectively reduce the overall power consumption by an average of 30% across our analyzed architectures, compared to their baseline counterparts, while incurring only a minimal 2% area overhead. The proposed methodology was evaluated on a widely used NN model, MobileNetV2, on the ImageNet dataset, demonstrating that the generated architectures can deliver up to 440 GOPS/W with relatively small output error during inference, outperforming several State-of-the-Art CGRA architectures in terms of throughput and energy efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23553v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Alexandris, Panagiotis Chaidos, Alexis Maras, Barry de Bruin, Manil Dev Gomony, Henk Corporaal, Dimitrios Soudris, Sotirios Xydis</dc:creator>
    </item>
    <item>
      <title>CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM Systems</title>
      <link>https://arxiv.org/abs/2505.22868</link>
      <description>arXiv:2505.22868v1 Announce Type: cross 
Abstract: In this paper, we propose the CrossNAS framework, an automated approach for exploring a vast, multidimensional search space that spans various design abstraction layers-circuits, architecture, and systems-to optimize the deployment of machine learning workloads on analog processing-in-memory (PIM) systems. CrossNAS leverages the single-path one-shot weight-sharing strategy combined with the evolutionary search for the first time in the context of PIM system mapping and optimization. CrossNAS sets a new benchmark for PIM neural architecture search (NAS), outperforming previous methods in both accuracy and energy efficiency while maintaining comparable or shorter search times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22868v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Hasibul Amin, Mohammadreza Mohammadi, Jason D. Bakos, Ramtin Zand</dc:creator>
    </item>
    <item>
      <title>Towards LLM-based Generation of Human-Readable Proofs in Polynomial Formal Verification</title>
      <link>https://arxiv.org/abs/2505.23311</link>
      <description>arXiv:2505.23311v1 Announce Type: cross 
Abstract: Verification is one of the central tasks in circuit and system design. While simulation and emulation are widely used, complete correctness can only be ensured based on formal proof techniques. But these approaches often have very high run time and memory requirements. Recently, Polynomial Formal Verification (PFV) has been introduced showing that for many instances of practical relevance upper bounds on needed resources can be given. But proofs have to be provided that are human-readable.
  Here, we study how modern approaches from Artificial Intelligence (AI) based on Large Language Models (LLMs) can be used to generate proofs that later on can be validated based on reasoning engines. Examples are given that show how LLMs can interact with proof engines, and directions for future work are outlined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23311v1</guid>
      <category>cs.LO</category>
      <category>cs.AR</category>
      <category>cs.SC</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rolf Drechsler</dc:creator>
    </item>
    <item>
      <title>Carbon-Efficient 3D DNN Acceleration: Optimizing Performance and Sustainability</title>
      <link>https://arxiv.org/abs/2504.09851</link>
      <description>arXiv:2504.09851v2 Announce Type: replace 
Abstract: As Deep Neural Networks (DNNs) continue to drive advancements in artificial intelligence, the design of hardware accelerators faces growing concerns over embodied carbon footprint due to complex fabrication processes. 3D integration improves performance but introduces sustainability challenges, making carbon-aware optimization essential. In this work, we propose a carbon-efficient design methodology for 3D DNN accelerators, leveraging approximate computing and genetic algorithm-based design space exploration to optimize Carbon Delay Product (CDP). By integrating area-efficient approximate multipliers into Multiply-Accumulate (MAC) units, our approach effectively reduces silicon area and fabrication overhead while maintaining high computational accuracy. Experimental evaluations across three technology nodes (45nm, 14nm, and 7nm) show that our method reduces embodied carbon by up to 30% with negligible accuracy drop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09851v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aikaterini Maria Panteleaki, Konstantinos Balaskas, Georgios Zervakis, Hussam Amrouch, Iraklis Anagnostopoulos</dc:creator>
    </item>
    <item>
      <title>CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark</title>
      <link>https://arxiv.org/abs/2505.16968</link>
      <description>arXiv:2505.16968v3 Announce Type: replace 
Abstract: We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA &lt;--&gt; HIP) and assembly-level (Nvidia SASS &lt;--&gt; AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16968v3</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed Heakl, Sarim Hashmi, Gustavo Bertolo Stahl, Seung Hun Eddie Han, Salman Khan, Abdulrahman Mahmoud</dc:creator>
    </item>
    <item>
      <title>Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View</title>
      <link>https://arxiv.org/abs/2502.11256</link>
      <description>arXiv:2502.11256v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) offer powerful capabilities but come with significant environmental impact, particularly in carbon emissions. Existing studies benchmark carbon emissions but lack a standardized basis for comparison across different model configurations. To address this, we introduce the concept of functional unit (FU) as a standardized basis and develop FUEL, the first FU-based framework for evaluating LLM serving's environmental impact. Through three case studies, we uncover key insights and trade-offs in reducing carbon emissions by optimizing model size, quantization strategy, and hardware choice, paving the way for more sustainable LLM serving. The code is available at https://github.com/jojacola/FUEL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11256v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.CL</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanran Wu, Inez Hua, Yi Ding</dc:creator>
    </item>
    <item>
      <title>$\Delta$-Nets: Interaction-Based System for Optimal Parallel $\lambda$-Reduction</title>
      <link>https://arxiv.org/abs/2505.20314</link>
      <description>arXiv:2505.20314v2 Announce Type: replace-cross 
Abstract: I present a model of universal parallel computation called $\Delta$-Nets, and a method to translate $\lambda$-terms into $\Delta$-nets and back. Together, the model and the method constitute an algorithm for optimal parallel $\lambda$-reduction, solving the longstanding enigma with groundbreaking clarity. I show that the $\lambda$-calculus can be understood as a projection of $\Delta$-Nets -- one that severely restricts the structure of sharing, among other drawbacks. Unhindered by these restrictions, the $\Delta$-Nets model opens the door to new highly parallel programming language implementations and computer architectures that are more efficient and performant than previously possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20314v2</guid>
      <category>cs.LO</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Augusto Rizzi Salvadori</dc:creator>
    </item>
  </channel>
</rss>
