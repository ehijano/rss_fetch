<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Apr 2025 02:47:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian Rao test for distributed target detection in interference and noise with limited training data</title>
      <link>https://arxiv.org/abs/2504.13235</link>
      <description>arXiv:2504.13235v1 Announce Type: new 
Abstract: This paper has studied the problem of detecting a range-spread target in interference and noise when the number of training data is limited. The interference is located within a certain subspace with an unknown coordinate, while the noise follows a Gaussian distribution with an unknown covariance matrix. We concentrate on the scenarios where the training data are limited and employ a Bayesian framework to ffnd a solution. Speciffcally, the covariance matrix is assumed to follow an inverse Wishart distribution. Then, we introduce the Bayesian detector according to the Rao test, which, demonstrated by both simulation experiment and real data, has superior detection performance to the existing detectors in certain situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13235v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daipeng Xiao, Weijian Liu, Jun Liu, Yuntao Wu, Qinglei Du, Xiaoqiang Hua</dc:creator>
    </item>
    <item>
      <title>Estimating equations for survival analysis with pooled logistic regression</title>
      <link>https://arxiv.org/abs/2504.13291</link>
      <description>arXiv:2504.13291v1 Announce Type: new 
Abstract: Pooled logistic regression models are commonly applied in survival analysis. However, the standard implementation can be computationally demanding, which is further exacerbated when using the nonparametric bootstrap for inference. To ease these computational burdens, investigators often coarsen time intervals or assume a parametric models for time. These approaches impose restrictive assumptions, which may not always have a well-motivated substantive justification. Here, the pooled logistic regression model is re-framed using estimating equations to simplify computations and allow for inference via the empirical sandwich variance estimator, thus avoiding the more computationally demanding bootstrap. The proposed method is demonstrated using two examples with publicly available data. The performance of the empirical sandwich variance estimator is illustrated using a Monte Carlo simulation study. The implementation proposed here offers an improved alternative to the standard implementation of pooled logistic regression without needing to impose restrictive constraints on time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13291v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Stephen R Cole, Bonnie E Shook-Sa, Justin B DeMonte, Jessie K Edwards</dc:creator>
    </item>
    <item>
      <title>Active Learning of Computer Experiment with both Quantitative and Qualitative Inputs</title>
      <link>https://arxiv.org/abs/2504.13441</link>
      <description>arXiv:2504.13441v1 Announce Type: new 
Abstract: Computer experiments refer to the study of real systems using complex simulation models. They have been widely used as alternatives to physical experiments. Design and analysis of computer experiments have attracted great attention in past three decades. The bulk of the work, however, often focus on experiments with only quantitative inputs. In recent years, research on design and analysis for computer experiments have gain momentum. Statistical methodology for design, modeling and inference of such experiments have been developed. In this chapter, we review some of those key developments, and propose active learning approaches for modeling, optimization, contour estimation of computer experiments with both types of inputs. Numerical studies are conducted to evaluate the performance of the proposed methods in comparison with other existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13441v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anita Shahrokhian, Xinwei Deng, C. Devon Lin</dc:creator>
    </item>
    <item>
      <title>A Comparative Evaluation of a Conditional Median-Based Bayesian Growth Curve Modeling Approach with Missing Data</title>
      <link>https://arxiv.org/abs/2504.13451</link>
      <description>arXiv:2504.13451v1 Announce Type: new 
Abstract: Longitudinal data are essential for studying within subject change and between subject differences in change. However, missing data, especially when the observed variables are nonnormal, remain a significant challenge in longitudinal analysis. Full information maximum likelihood estimation (FIML) and a two stage robust estimation (TSRE) are widely used to handle missing data, but their effectiveness may diminish with data skewness, high missingness rates, and nonignorable missingness. Recently, a robust median \textendash based Bayesian (RMB) approach for growth curve modeling (GCM) was proposed to handle nonnormal longitudinal data, yet its performance with missing data has not been fully investigated. This study fills that gap by using Monte Carlo simulations to evaluate RMB relative to FIML and TSRE. Overall, the RMB \textendash based GCM is shown to be a reliable option for managing both ignorable and nonignorable missing data across a variety of distributional scenarios. An empirical example illustrates the application of these methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13451v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dandan Tang, Xin Tong, Jianhui Zhou</dc:creator>
    </item>
    <item>
      <title>Efficient Estimation under Multiple Missing Patterns via Balancing Weights</title>
      <link>https://arxiv.org/abs/2504.13467</link>
      <description>arXiv:2504.13467v1 Announce Type: new 
Abstract: As one of the most commonly seen data challenges, missing data, in particular, multiple, non-monotone missing patterns, complicates estimation and inference due to the fact that missingness mechanisms are often not missing at random, and conventional methods cannot be applied. Pattern graphs have recently been proposed as a tool to systematically relate various observed patterns in the sample. We extend its scope to the estimation of parameters defined by moment equations, including common regression models, via solving weighted estimating equations with weights constructed using a sequential balancing approach. These novel weights are carefully crafted to address the instability issue of the straightforward approach based on local balancing. We derive the efficiency bound for the model parameters and show that our proposed method, albeit relatively simple, is asymptotically efficient. Simulation results demonstrate the superior performance of the proposed method, and real-data applications illustrate how the results are robust to the choice of identification assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13467v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jianing Dong, Raymond K. W. Wong, Kwun Chuen Gary Chan</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Averaging in Causal Instrumental Variable Models</title>
      <link>https://arxiv.org/abs/2504.13520</link>
      <description>arXiv:2504.13520v1 Announce Type: new 
Abstract: Instrumental variables are a popular tool to infer causal effects under unobserved confounding, but choosing suitable instruments is challenging in practice. We propose gIVBMA, a Bayesian model averaging procedure that addresses this challenge by averaging across different sets of instrumental variables and covariates in a structural equation model. Our approach extends previous work through a scale-invariant prior structure and accommodates non-Gaussian outcomes and treatments, offering greater flexibility than existing methods. The computational strategy uses conditional Bayes factors to update models separately for the outcome and treatments. We prove that this model selection procedure is consistent. By explicitly accounting for model uncertainty, gIVBMA allows instruments and covariates to switch roles and provides robustness against invalid instruments. In simulation experiments, gIVBMA outperforms current state-of-the-art methods. We demonstrate its usefulness in two empirical applications: the effects of malaria and institutions on income per capita and the returns to schooling. A software implementation of gIVBMA is available in Julia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13520v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gregor Steiner, Mark Steel</dc:creator>
    </item>
    <item>
      <title>MR-MAGIC: Robust Causal Inference Using Many Weak Genetic Interactions</title>
      <link>https://arxiv.org/abs/2504.13565</link>
      <description>arXiv:2504.13565v1 Announce Type: new 
Abstract: Mendelian randomization (MR) studies commonly use genetic variants as instrumental variables to estimate causal effects of exposures on outcomes. However, the presence of invalid instruments-even when numerous-can lead to biased causal estimates. We propose a novel identification strategy that remains valid even when all candidate instruments are invalid by leveraging genetic interactions that collectively explain substantial exposure variation. Recognizing that individual interaction effects may be weak, we develop MR-MAGIC (Mendelian Randomization with MAny weak Genetic Interactions for Causality), a robust method that simultaneously addresses instrument invalidity and improves estimation efficiency. MR-MAGIC provides consistent and asymptotically normal estimates under a many-weak-interactions asymptotic framework. Comprehensive simulations and applications to UK Biobank data demonstrate that MR-MAGIC outperforms conventional MR methods in practice, offering reliable causal inference when standard approaches fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13565v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Zhang, Minhao Yao, Zhonghua Liu, Baoluo Sun</dc:creator>
    </item>
    <item>
      <title>Modeling Bounded Count Environmental Data Using a Contaminated Beta-Binomial Regression Model</title>
      <link>https://arxiv.org/abs/2504.13665</link>
      <description>arXiv:2504.13665v1 Announce Type: new 
Abstract: This paper investigates two environmental applications related to climate change, where observations consist of bounded counts. The binomial and beta-binomial (BB) models are commonly used for bounded count data, with the BB model offering the advantage of accounting for potential overdispersion. However, extreme observations in real-world applications may hinder the performance of the BB model and lead to misleading inferences. To address this issue, we propose the contaminated beta-binomial (cBB) distribution (cBB-D), which provides the necessary flexibility to accommodate extreme observations. The cBB model accounts for overdispersion and extreme values while maintaining the mean and variance properties of the BB distribution. The availability of covariates that improve inference about the mean of the bounded count variable motivates the further proposal of the cBB regression model (cBB-RM). Different versions of the cBB-RM model - where none, some, or all of the cBB parameters are regressed on available covariates - are fitted to the datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13665v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Arnoldus F. Otto (Department of Statistics, University of Pretoria, Pretoria, South Africa), Antonio Punzo (Department of Economics and Business, University of Catania, Catania, Italy), Johannes T. Ferreira (Department of Statistics, University of Pretoria, Pretoria, South Africa, School of Statistics and Actuarial Science, University of the Witwatersrand, Johannesburg, South Africa), Andri\"ette Bekker (Department of Statistics, University of Pretoria, Pretoria, South Africa, Centre for Environmental Studies, Department of Geography, Geoinformatics and Meteorology, University of Pretoria, Pretoria, South Africa), Salvatorie D. Tomarchio (Department of Economics and Business, University of Catania, Catania, Italy), Cristina Tortora (Department of Mathematics and Statistics, San Jos\'e State University, California, United States of America)</dc:creator>
    </item>
    <item>
      <title>Addressing outliers in mixed-effects logistic regression: a more robust modeling approach</title>
      <link>https://arxiv.org/abs/2504.13781</link>
      <description>arXiv:2504.13781v1 Announce Type: new 
Abstract: This study introduces an outlier-robust model for analyzing hierarchically structured bounded count data within a Bayesian framework, utilizing a logistic regression approach implemented in JAGS. Our model incorporates a t-distributed latent variable to address overdispersion and outliers, improving robustness compared to conventional models such as the beta-binomial, binomial-logit-normal, and standard binomial models. Notably, our approach models the median of the response variable, presenting a more convenient and interpretable measure of central tendency, which is available in closed form. For comparability between all models, we also make predictions based on the mean proportion; however, this involves an integration step for the t-distributed nuisance parameter. While limited literature specifically addresses outliers in mixed models for bounded count data, this research fills that gap. The practical utility of the model is demonstrated using a longitudinal medication adherence dataset, where patient behavior often results in abrupt changes and outliers within individual trajectories. A simulation study demonstrates the binomial-logit-t model's strong performance, with comparison statistics favoring it among the four evaluated models. An additional data contamination simulation confirms its robustness against outliers. Our robust approach maintains the integrity of the dataset, effectively handling outliers to provide more accurate and reliable parameter estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13781v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divan A. Burger, Sean van der Merwe, Emmanuel Lesaffre</dc:creator>
    </item>
    <item>
      <title>Closed-form formulas for the biases of the Theil and Atkinson index estimators in Gamma distributed populations</title>
      <link>https://arxiv.org/abs/2504.13806</link>
      <description>arXiv:2504.13806v1 Announce Type: new 
Abstract: This paper presents an analysis of the Theil and Atkinson index estimators for gamma populations, highlighting the presence of bias in both cases. Theoretical expressions for the biases are obtained, and bias-corrected estimators are constructed and evaluated through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13806v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>How Much Weak Overlap Can Doubly Robust T-Statistics Handle?</title>
      <link>https://arxiv.org/abs/2504.13273</link>
      <description>arXiv:2504.13273v1 Announce Type: cross 
Abstract: In the presence of sufficiently weak overlap, it is known that no regular root-n-consistent estimators exist and standard estimators may fail to be asymptotically normal. This paper shows that a thresholded version of the standard doubly robust estimator is asymptotically normal with well-calibrated Wald confidence intervals even when constructed using nonparametric estimates of the propensity score and conditional mean outcome. The analysis implies a cost of weak overlap in terms of black-box nuisance rates, borne when the semiparametric bound is infinite, and the contribution of outcome smoothness to the outcome regression rate, which is incurred even when the semiparametric bound is finite. As a byproduct of this analysis, I show that under weak overlap, the optimal global regression rate is the same as the optimal pointwise regression rate, without the usual polylogarithmic penalty. The high-level conditions yield new rules of thumb for thresholding in practice. In simulations, thresholded AIPW can exhibit moderate overrejection in small samples, but I am unable to reject a null hypothesis of exact coverage in large samples. In an empirical application, the clipped AIPW estimator that targets the standard average treatment effect yields similar precision to a heuristic 10% fixed-trimming approach that changes the target sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13273v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Dorn</dc:creator>
    </item>
    <item>
      <title>Using Multiple Outcomes to Adjust Standard Errors for Spatial Correlation</title>
      <link>https://arxiv.org/abs/2504.13295</link>
      <description>arXiv:2504.13295v1 Announce Type: cross 
Abstract: Empirical research in economics often examines the behavior of agents located in a geographic space. In such cases, statistical inference is complicated by the interdependence of economic outcomes across locations. A common approach to account for this dependence is to cluster standard errors based on a predefined geographic partition. A second strategy is to model dependence in terms of the distance between units. Dependence, however, does not necessarily stop at borders and is typically not determined by distance alone. This paper introduces a method that leverages observations of multiple outcomes to adjust standard errors for cross-sectional dependence. Specifically, a researcher, while interested in a particular outcome variable, often observes dozens of other variables for the same units. We show that these outcomes can be used to estimate dependence under the assumption that the cross-sectional correlation structure is shared across outcomes. We develop a procedure, which we call Thresholding Multiple Outcomes (TMO), that uses this estimate to adjust standard errors in a given regression setting. We show that adjustments of this form can lead to sizable reductions in the bias of standard errors in calibrated U.S. county-level regressions. Re-analyzing nine recent papers, we find that the proposed correction can make a substantial difference in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13295v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano DellaVigna, Guido Imbens, Woojin Kim, David M. Ritzwoller</dc:creator>
    </item>
    <item>
      <title>Towards R-learner with Continuous Treatments</title>
      <link>https://arxiv.org/abs/2208.00872</link>
      <description>arXiv:2208.00872v3 Announce Type: replace 
Abstract: The R-learner is widely used in causal inference due to its flexibility and efficiency in estimating the conditional average treatment effect. However, extending the R-learner framework from binary to continuous treatments introduces a non-identifiability issue, as the functional zero constraint inherent to the conditional average treatment effect cannot be directly imposed in the R-loss under continuous treatments. To address this, we propose a two-step identification strategy: we first identify an intermediary function via Tikhonov regularization, and then recover the conditional average treatment effect using a zero-constraining operator. Building on this strategy, an $\ell_2$-regularized R-learner framework is developed to estimate the conditional average treatment effect for continuous treatments. The new framework accommodates modern, flexible machine learning algorithms to estimate both nuisance functions and target estimand. Theoretical properties are demonstrated when the target estimand is approximated by sieve approximation with B-splines, including error rates, asymptotic normality, and confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.00872v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichi Zhang, Dehan Kong, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Navigating Unmeasured Confounding in Quantitative Sociology: A Sensitivity Framework</title>
      <link>https://arxiv.org/abs/2311.13410</link>
      <description>arXiv:2311.13410v3 Announce Type: replace 
Abstract: Unmeasured confounding remains a critical challenge in causal inference for the social sciences. This paper proposes a sensitivity analysis framework to systematically evaluate how unmeasured confounders influence statistical inference in sociology. Given these sensitivity analysis methods, we introduce a five-step workflow that integrates sensitivity analysis into research design rather than treating it as a post-hoc robustness check. Using the Blau and Duncan (1967) study as an empirical example, we demonstrate how different sensitivity methods provide complementary insights. By extending existing frameworks, we show how sensitivity analysis enhances causal transparency, offering a practical tool for assessing uncertainty in observational research. Our approach contributes to a more rigorous application of causal inference in sociology, bridging gaps between theory, identification strategies, and statistical modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13410v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cheng Lin, Jose M. Pena, Adel Daoud</dc:creator>
    </item>
    <item>
      <title>Confidence on the Focal: Conformal Prediction with Selection-Conditional Coverage</title>
      <link>https://arxiv.org/abs/2403.03868</link>
      <description>arXiv:2403.03868v3 Announce Type: replace 
Abstract: Conformal prediction builds marginally valid prediction intervals that cover the unknown outcome of a randomly drawn test point with a prescribed probability. However, in practice, data-driven methods are often used to identify specific test unit(s) of interest, requiring uncertainty quantification tailored to these focal units. In such cases, marginally valid conformal prediction intervals may fail to provide valid coverage for the focal unit(s) due to selection bias. This paper presents a general framework for constructing a prediction set with finite-sample exact coverage, conditional on the unit being selected by a given procedure. The general form of our method accommodates arbitrary selection rules that are invariant to the permutation of the calibration units, and generalizes Mondrian Conformal Prediction to multiple test units and non-equivariant classifiers. We also work out computationally efficient implementation of our framework for a number of realistic selection rules, including top-K selection, optimization-based selection, selection based on conformal p-values, and selection based on properties of preliminary conformal prediction sets. The performance of our methods is demonstrated via applications in drug discovery and health risk prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03868v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Jin, Zhimei Ren</dc:creator>
    </item>
    <item>
      <title>Efficient estimation of semiparametric spatial point processes with V-fold random thinning</title>
      <link>https://arxiv.org/abs/2410.04359</link>
      <description>arXiv:2410.04359v2 Announce Type: replace 
Abstract: We study a broad class of models called semiparametric spatial point processes where the intensity function contains both a parametric component and a nonparametric component. We propose a novel estimator of the parametric component based on random thinning, a common sampling technique in point processes. The proposed estimator of the parametric component is shown to be consistent and asymptotically normal if the nonparametric component can be estimated at the desired rate. We then extend a popular kernel-based estimator in i.i.d. settings and establish convergence rates that will enable inference for the parametric component. Next, we generalize the notion of semiparametric efficiency lower bound in i.i.d. settings to spatial point processes and show that the proposed estimator achieves the efficiency lower bound if the process is Poisson. Computationally, we show how to efficiently evaluate the proposed estimator with existing software for generalized partial linear models in i.i.d. settings by tailoring the sampling weights to replicate the dependence induced by the point process. We conclude with a small simulation study and a re-analysis of the spatial distribution of rainforest trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04359v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xindi Lin, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>A Bayesian nonparametric approach to mediation and spillover effects with multiple mediators in cluster-randomized trials</title>
      <link>https://arxiv.org/abs/2411.03489</link>
      <description>arXiv:2411.03489v2 Announce Type: replace 
Abstract: Cluster randomized trials (CRTs) with multiple unstructured mediators present significant methodological challenges for causal inference due to within-cluster correlation, interference among units, and the complexity introduced by multiple mediators. Existing causal mediation methods often fall short in simultaneously addressing these complexities, particularly in disentangling mediator-specific effects under interference that are central to studying complex mechanisms. To address this gap, we propose new causal estimands for spillover mediation effects that differentiate the roles of each individual's own mediator and the spillover effects resulting from interactions among individuals within the same cluster. We establish identification results for each estimand and, to flexibly model the complex data structures inherent in CRTs, we develop a new Bayesian nonparametric prior -- the Nested Dependent Dirichlet Process Mixture -- designed for flexibly capture the outcome and mediator surfaces at different levels. We conduct extensive simulations across various scenarios to evaluate the frequentist performance of our methods, compare them with a Bayesian parametric counterpart and illustrate our new methods in an analysis of a completed CRT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03489v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuki Ohnishi, Fan Li</dc:creator>
    </item>
    <item>
      <title>The Group R2D2 Shrinkage Prior for Sparse Linear Models with Grouped Covariates</title>
      <link>https://arxiv.org/abs/2412.15293</link>
      <description>arXiv:2412.15293v2 Announce Type: replace 
Abstract: Shrinkage priors are a popular Bayesian paradigm to handle sparsity in high-dimensional regression. Still limited, however, is a flexible class of shrinkage priors to handle grouped sparsity, where covariates exhibit some natural grouping structure. This paper proposes a novel extension of the $R^2$-induced Dirichlet Decomposition (R2D2) prior to accommodate grouped variable selection in linear regression models. The proposed method, called the Group R2D2 prior, employs a Dirichlet prior distribution on the coefficient of determination for each group, allowing for a flexible and adaptive shrinkage that operates at both group and individual variable levels. This approach improves the original R2D2 prior to handle grouped predictors, providing a balance between within-group dependence and group-level sparsity. We present several theoretical properties of this proposed prior distribution while also developing a Markov Chain Monte Carlo algorithm. Through simulation studies and real-data analysis, we demonstrate that our method outperforms traditional shrinkage priors in terms of both estimation accuracy, inference and prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15293v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko, Kaoru Irie, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Linear Shrinkage Convexification of Penalized Linear Regression With Missing Data</title>
      <link>https://arxiv.org/abs/2412.19963</link>
      <description>arXiv:2412.19963v2 Announce Type: replace 
Abstract: One of the common challenges faced by researchers in recent data analysis is missing values. In the context of penalized linear regression, which has been extensively explored over several decades, missing values introduce bias and yield a non-positive definite covariance matrix of the covariates, rendering the least square loss function non-convex. In this paper, we propose a novel procedure called the linear shrinkage positive definite (LPD) modification to address this issue. The LPD modification aims to modify the covariance matrix of the covariates in order to ensure consistency and positive definiteness. Employing the new covariance estimator, we are able to transform the penalized regression problem into a convex one, thereby facilitating the identification of sparse solutions. Notably, the LPD modification is computationally efficient and can be expressed analytically. In the presence of missing values, we establish the selection consistency and prove the convergence rate of the $\ell_1$-penalized regression estimator with LPD, showing an $\ell_2$-error convergence rate of square-root of $\log p$ over $n$ by a factor of $(s_0)^{3/2}$ ($s_0$: the number of non-zero coefficients). To further evaluate the effectiveness of our approach, we analyze real data from the Genomics of Drug Sensitivity in Cancer (GDSC) dataset. This dataset provides incomplete measurements of drug sensitivities of cell lines and their protein expressions. We conduct a series of penalized linear regression models with each sensitivity value serving as a response variable and protein expressions as explanatory variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19963v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seongoh Park, Seongjin Lee, Nguyen Thi Hai Yen, Nguyen Phuoc Long, Johan Lim</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Inference on Causal Derivative Effects for Continuous Treatments</title>
      <link>https://arxiv.org/abs/2501.06969</link>
      <description>arXiv:2501.06969v2 Announce Type: replace 
Abstract: Statistical methods for causal inference with continuous treatments mainly focus on estimating the mean potential outcome function, commonly known as the dose-response curve. However, it is often not the dose-response curve but its derivative function that signals the treatment effect. In this paper, we investigate nonparametric inference on the derivative of the dose-response curve with and without the positivity condition. Under the positivity and other regularity conditions, we propose a doubly robust (DR) inference method for estimating the derivative of the dose-response curve using kernel smoothing. When the positivity condition is violated, we demonstrate the inconsistency of conventional inverse probability weighting (IPW) and DR estimators, and introduce novel bias-corrected IPW and DR estimators. In all settings, our DR estimator achieves asymptotic normality at the standard nonparametric rate of convergence with nonparametric efficiency guarantees. Additionally, our approach reveals an interesting connection to nonparametric support and level set estimation problems. Finally, we demonstrate the applicability of our proposed estimators through simulations and a case study of evaluating a job training program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06969v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Stability and performance guarantees for misspecified multivariate score-driven filters</title>
      <link>https://arxiv.org/abs/2502.05021</link>
      <description>arXiv:2502.05021v2 Announce Type: replace 
Abstract: We address the problem of tracking multivariate unobserved time-varying parameters under potential model misspecification. Specifically, we examine implicit and explicit score-driven (ISD and ESD) filters, which update parameter predictions using the gradient of the postulated logarithmic observation density (commonly referred to as the score). For both filter types, we derive novel sufficient conditions that ensure the invertibility of the filtered parameter path and the existence of a finite mean squared error (MSE) bound relative to the pseudo-true parameter path. Our (non-)asymptotic MSE bounds rely on mild moment conditions on the data-generating process, while our invertibility result is agnostic about the true process. For the ISD filter, concavity of the postulated log density combined with simple parameter restrictions is sufficient (though not necessary) to guarantee stability. In contrast, the ESD filter additionally requires the score to be Lipschitz continuous. We validate our theoretical findings and highlight the superior stability and performance of ISD over ESD filters through extensive simulation studies. Finally, we demonstrate the practical relevance of our approach through an empirical application to U.S. Treasury-bill rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05021v2</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Donker van Heel, Rutger-Jan Lange, Dick van Dijk, Bram van Os</dc:creator>
    </item>
    <item>
      <title>A Powerful Bootstrap Test of Independence in High Dimensions</title>
      <link>https://arxiv.org/abs/2503.21715</link>
      <description>arXiv:2503.21715v2 Announce Type: replace 
Abstract: This paper proposes a nonparametric test of pairwise independence of one random variable from a large pool of other random variables. The test statistic is the maximum of several Chatterjee's rank correlations and critical values are computed via a block multiplier bootstrap. The test is shown to asymptotically control size uniformly over a large class of data-generating processes, even when the number of variables is much larger than sample size. The test is consistent against any fixed alternative. It can be combined with a stepwise procedure for selecting those variables from the pool that violate independence, while controlling the family-wise error rate. All formal results leave the dependence among variables in the pool completely unrestricted. In simulations, we find that our test is very powerful, outperforming existing tests in most scenarios considered, particularly in high dimensions and/or when the variables in the pool are dependent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21715v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauricio Olivares, Tomasz Olma, Daniel Wilhelm</dc:creator>
    </item>
    <item>
      <title>Granger Causality in High-Dimensional Networks of Time Series</title>
      <link>https://arxiv.org/abs/2406.02360</link>
      <description>arXiv:2406.02360v3 Announce Type: replace-cross 
Abstract: A novel approach is developed for discovering directed connectivity between specified pairs of nodes in a high-dimensional network (HDN) of brain signals. To accurately identify causal connectivity for such specified objectives, it is necessary to properly address the influence of all other nodes within the network. The proposed procedure herein starts with the estimation of a low-dimensional representation of the other nodes in the network utilizing (frequency-domain-based) spectral dynamic principal component analysis (sDPCA). The resulting scores can then be removed from the nodes of interest, thus eliminating the confounding effect of other nodes within the HDN. Accordingly, causal interactions can be dissected between nodes that are isolated from the effects of the network. Extensive simulations have demonstrated the effectiveness of this approach as a tool for causality analysis in complex time series networks. The proposed methodology has also been shown to be applicable to multichannel EEG networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02360v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>A theory of generalised coordinates for stochastic differential equations</title>
      <link>https://arxiv.org/abs/2409.15532</link>
      <description>arXiv:2409.15532v2 Announce Type: replace-cross 
Abstract: Stochastic differential equations are ubiquitous modelling tools in physics and the sciences. In most modelling scenarios, random fluctuations driving dynamics or motion have some non-trivial temporal correlation structure, which renders the SDE non-Markovian; a phenomenon commonly known as ``colored'' noise. Thus, an important objective is to develop effective tools for mathematically and numerically studying (possibly non-Markovian) SDEs. In this report, we formalise a mathematical theory for analysing and numerically studying SDEs based on so-called `generalised coordinates of motion'. Like the theory of rough paths, we analyse SDEs pathwise for any given realisation of the noise, not solely probabilistically. Like the established theory of Markovian realisation, we realise non-Markovian SDEs as a Markov process in an extended space. Unlike the established theory of Markovian realisation however, the Markovian realisations here are accurate on short timescales and may be exact globally in time, when flows and fluctuations are analytic. This theory is exact for SDEs with analytic flows and fluctuations, and is approximate when flows and fluctuations are differentiable. It provides useful analysis tools, which we employ to solve linear SDEs with analytic fluctuations. It may also be useful for studying rougher SDEs, as these may be identified as the limit of smoother ones. This theory supplies effective, computationally straightforward methods for simulation, filtering and control of SDEs; amongst others, we re-derive generalised Bayesian filtering, a state-of-the-art method for time-series analysis. Looking forward, this report suggests that generalised coordinates have far-reaching applications throughout stochastic differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15532v2</guid>
      <category>math.PR</category>
      <category>math.DS</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lancelot Da Costa, Natha\"el Da Costa, Conor Heins, Johan Medrano, Grigorios A. Pavliotis, Thomas Parr, Ajith Anil Meera, Karl Friston</dc:creator>
    </item>
  </channel>
</rss>
