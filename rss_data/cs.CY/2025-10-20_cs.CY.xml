<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Oct 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Revisiting UTAUT for the Age of AI: Understanding Employees AI Adoption and Usage Patterns Through an Extended UTAUT Framework</title>
      <link>https://arxiv.org/abs/2510.15142</link>
      <description>arXiv:2510.15142v1 Announce Type: new 
Abstract: This study investigates whether demographic factors shape adoption and attitudes among employees toward artificial intelligence (AI) technologies at work. Building on an extended Unified Theory of Acceptance and Use of Technology (UTAUT), which reintroduces affective dimensions such as attitude, self-efficacy, and anxiety, we surveyed 2,257 professionals across global regions and organizational levels within a multinational consulting firm. Non-parametric tests examined whether three demographic factors (i.e., years of experience, hierarchical level in the organization, and geographic region) were associated with AI adoption, usage intensity, and eight UTAUT constructs. Organizational level significantly predicted AI adoption, with senior employees showing higher usage rates, while experience and region were unrelated to adoption. Among AI users (n = 1,256), frequency and duration of use showed minimal demographic variation. However, omnibus tests revealed small but consistent group differences across several UTAUT constructs, particularly anxiety, performance expectancy, and behavioral intention, suggesting that emotional and cognitive responses to AI vary modestly across contexts. These findings highlight that demographic factors explain limited variance in AI acceptance but remain relevant for understanding contextual nuances in technology-related attitudes. The results underscore the need to integrate affective and organizational factors into models of technology acceptance to support equitable, confident, and sustainable engagement with AI in modern workplaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15142v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diana Wolfe, Matt Price, Alice Choe, Fergus Kidd, Hannah Wagner</dc:creator>
    </item>
    <item>
      <title>From Murals to Memes: A Theory of Aesthetic Asymmetry in Political Mobilization</title>
      <link>https://arxiv.org/abs/2510.15256</link>
      <description>arXiv:2510.15256v1 Announce Type: new 
Abstract: Why have left-wing movements historically integrated participatory art forms (such as murals and protest songs) into their praxis, while right-wing movements have prioritized strategic communication and, more recently, the digital culture of memes? This article introduces the concept of aesthetic asymmetry to explain this divergence in political action. We argue that the asymmetry is not coincidental but the result of four interconnected structural factors: the organizational ecosystem, the moral and emotional framework, the material supports, and the historical tradition of each political spectrum. While the left tends to use art in a constitutive manner to forge community, solidarity, and hope, the contemporary right tends to use it instrumentally to mobilize polarizing affects such as humor and resentment. Drawing on comparative literature from the Theatre of the Oppressed to analyses of alt-right meme wars, we nuance this distinction and show how the aesthetic logic of each pole aligns with its strategic objectives. The article culminates in a prescriptive model for artistic action, synthesizing keys to effective mobilization into emotional, narrative, and formatting strategies. Understanding this asymmetry is crucial for analyzing political communication and for designing cultural interventions capable of generating profound social change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15256v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Alonzo Fern\'andez Salguero</dc:creator>
    </item>
    <item>
      <title>VERA-MH Concept Paper</title>
      <link>https://arxiv.org/abs/2510.15297</link>
      <description>arXiv:2510.15297v1 Announce Type: new 
Abstract: We introduce VERA-MH (Validation of Ethical and Responsible AI in Mental Health), an automated evaluation of the safety of AI chatbots used in mental health contexts, with an initial focus on suicide risk.
  Practicing clinicians and academic experts developed a rubric informed by best practices for suicide risk management for the evaluation. To fully automate the process, we used two ancillary AI agents. A user-agent model simulates users engaging in a mental health-based conversation with the chatbot under evaluation. The user-agent role-plays specific personas with pre-defined risk levels and other features. Simulated conversations are then passed to a judge-agent who scores them based on the rubric. The final evaluation of the chatbot being tested is obtained by aggregating the scoring of each conversation.
  VERA-MH is actively under development and undergoing rigorous validation by mental health clinicians to ensure user-agents realistically act as patients and that the judge-agent accurately scores the AI chatbot. To date we have conducted preliminary evaluation of GPT-5, Claude Opus and Claude Sonnet using initial versions of the VERA-MH rubric and used the findings for further design development. Next steps will include more robust clinical validation and iteration, as well as refining actionable scoring. We are seeking feedback from the community on both the technical and clinical aspects of our evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15297v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Belli, Kate Bentley, Will Alexander, Emily Ward, Matt Hawrilenko, Kelly Johnston, Mill Brown, Adam Chekroud</dc:creator>
    </item>
    <item>
      <title>Identifying curriculum disruptions in engineering education through serious gaming</title>
      <link>https://arxiv.org/abs/2510.15442</link>
      <description>arXiv:2510.15442v1 Announce Type: new 
Abstract: This workshop introduces participants to SUCRE, a serious game designed to enhance curriculum resilience in higher education by simulating crisis scenarios. While applicable to various disciplines, this session focuses on engineering curricula, identifying discipline-specific challenges and potential adaptations. Participants will engage in Step 1 of the game, analyzing trigger events and their impacts on curriculum structures. At the end of the workshop, attendees will be able to identify key triggers that may affect curricula, assess their cascading effects, and reflect on the applicability of SUCRE within their own institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15442v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>SEFI Annual Conference 2025, Sep 2025, Tampere, Finland</arxiv:journal_reference>
      <dc:creator>Roger Waldeck (LEGO, IMT Atlantique - DI2S), Ann-Kristin Winkens (RWTH Aachen), Clara Lemke (RWTH Aachen), Carmen Leicht-Scholten (RWTH Aachen), Haraldur Audunsson</dc:creator>
    </item>
    <item>
      <title>AI Adoption in NGOs: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2510.15509</link>
      <description>arXiv:2510.15509v1 Announce Type: new 
Abstract: AI has the potential to significantly improve how NGOs utilize their limited resources for societal benefits, but evidence about how NGOs adopt AI remains scattered. In this study, we systematically investigate the types of AI adoption use cases in NGOs and identify common challenges and solutions, contextualized by organizational size and geographic context. We review the existing primary literature, including studies that investigate AI adoption in NGOs related to social impact between 2020 and 2025 in English. Following the PRISMA protocol, two independent reviewers conduct study selection, with regular cross-checking to ensure methodological rigour, resulting in a final literature body of 65 studies. Leveraging a thematic and narrative approach, we identify six AI use case categories in NGOs - Engagement, Creativity, Decision-Making, Prediction, Management, and Optimization - and extract common challenges and solutions within the Technology-Organization-Environment (TOE) framework. By integrating our findings, this review provides a novel understanding of AI adoption in NGOs, linking specific use cases and challenges to organizational and environmental factors. Our results demonstrate that while AI is promising, adoption among NGOs remains uneven and biased towards larger organizations. Nevertheless, following a roadmap grounded in literature can help NGOs overcome initial barriers to AI adoption, ultimately improving effectiveness, engagement, and social impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15509v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Janne Rotter, William Bailkoski</dc:creator>
    </item>
    <item>
      <title>Quantifying the Engagement Effectiveness of Cyber Cognitive Attacks: A Behavioral Metric for Disinformation Campaigns</title>
      <link>https://arxiv.org/abs/2510.15805</link>
      <description>arXiv:2510.15805v1 Announce Type: new 
Abstract: As disinformation-driven cognitive attacks become increasingly sophisticated, the ability to quantify their impact is essential for advancing cybersecurity defense strategies. This paper presents a novel framework for measuring the engagement effectiveness of cognitive attacks by introducing a weighted interaction metric that accounts for both the type and volume of user engagement relative to the number of attacker-generated transmissions. Applying this model to real-world disinformation campaigns across social media platforms, we demonstrate how the metric captures not just reach but the behavioral depth of user engagement. Our findings provide new insights into the behavioral dynamics of cognitive warfare and offer actionable tools for researchers and practitioners seeking to assess and counter the spread of malicious influence online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15805v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.IT</category>
      <category>cs.SI</category>
      <category>math.IT</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bonnie Rushing, Shouhuai Xu</dc:creator>
    </item>
    <item>
      <title>A physically extended EEIO framework for material efficiency assessment in United States manufacturing supply chains</title>
      <link>https://arxiv.org/abs/2510.15121</link>
      <description>arXiv:2510.15121v1 Announce Type: cross 
Abstract: A physical assessment of material flows in an economy (e.g., material flow quantification) can support the development of sustainable decarbonization and circularity strategies by providing the tangible physical context of industrial production quantities and supply chain relationships. However, completing a physical assessment is challenging due to the scarcity of high-quality raw data and poor harmonization across industry classification systems used in data reporting. Here we describe a new physical extension for the U.S. Department of Energy's (DOE's) EEIO for Industrial Decarbonization (EEIO-IDA) model, yielding an expanded EEIO model that is both physically and environmentally extended. In the model framework, the U.S. economy is divided into goods-producing and service-producing subsectors, and mass flows are quantified for each goods-producing subsector using a combination of trade data (e.g., UN Comtrade) and physical production data (e.g., U.S. Geological Survey). Given that primary-source production data are not available for all subsectors, price-imputation and mass-balance assumptions are developed and used to complete the physical flows dataset with high-quality estimations. The resulting dataset, when integrated with the EEIO-IDA tool, enables the quantification of environmental impact intensity metrics on a mass basis (e.g., CO$_2$eq/kg)) for each industrial subsector. This work is designed to align with existing DOE frameworks and tools, including the EEIO-IDA tool, the DOE Industrial Decarbonization Roadmap (2022), and Pathways for U.S. Industrial Transformations study (2025).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15121v1</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heather Liddell, Beth Kelley, Liz Wachs, Alberta Carpenter, Joe Cresko</dc:creator>
    </item>
    <item>
      <title>Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis</title>
      <link>https://arxiv.org/abs/2510.15125</link>
      <description>arXiv:2510.15125v1 Announce Type: cross 
Abstract: Social media platforms play a pivotal role in shaping political discourse, but analyzing their vast and rapidly evolving content remains a major challenge. We introduce an end-to-end framework for automatically generating an interpretable topic taxonomy from an unlabeled corpus. By combining unsupervised clustering with prompt-based labeling, our method leverages large language models (LLMs) to iteratively construct a taxonomy without requiring seed sets or domain expertise. We apply this framework to a large corpus of Meta (previously known as Facebook) political ads from the month ahead of the 2024 U.S. Presidential election. Our approach uncovers latent discourse structures, synthesizes semantically rich topic labels, and annotates topics with moral framing dimensions. We show quantitative and qualitative analyses to demonstrate the effectiveness of our framework. Our findings reveal that voting and immigration ads dominate overall spending and impressions, while abortion and election-integrity achieve disproportionate reach. Funding patterns are equally polarized: economic appeals are driven mainly by conservative PACs, abortion messaging splits between pro- and anti-rights coalitions, and crime-and-justice campaigns are fragmented across local committees. The framing of these appeals also diverges--abortion ads emphasize liberty/oppression rhetoric, while economic messaging blends care/harm, fairness/cheating, and liberty/oppression narratives. Topic salience further reveals strong correlations between moral foundations and issues. Demographic targeting also emerges. This work supports scalable, interpretable analysis of political messaging on social media, enabling researchers, policymakers, and the public to better understand emerging narratives, polarization dynamics, and the moral underpinnings of digital political communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15125v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Brady, Tunazzina Islam</dc:creator>
    </item>
    <item>
      <title>HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks</title>
      <link>https://arxiv.org/abs/2510.15144</link>
      <description>arXiv:2510.15144v1 Announce Type: cross 
Abstract: Simulating human reasoning in open-ended tasks has been a long-standing aspiration in AI and cognitive science. While large language models now approximate human responses at scale, they remain tuned to population-level consensus, often erasing the individuality of reasoning styles and belief trajectories. To advance the vision of more human-like reasoning in machines, we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for average-to-individual reasoning adaptation. The task is to predict how a specific person would reason and update their beliefs in novel scenarios, given partial evidence of their past views. HugAgent adopts a dual-track design: a synthetic track for scale and systematic stress tests, and a human track for ecologically valid, "out-loud" reasoning data. This design enables scalable, reproducible evaluation of intra-agent fidelity: whether models can capture not just what people believe, but how their reasoning evolves. Experiments with state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent as the first extensible benchmark for aligning machine reasoning with the individuality of human thought. Our benchmark and chatbot are open-sourced as HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking (https://anonymous.4open.science/r/trace-your-thinking).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15144v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chance Jiajie Li, Zhenze Mo, Yuhan Tang, Ao Qu, Jiayi Wu, Kaiya Ivy Zhao, Yulu Gan, Jie Fan, Jiangbo Yu, Hang Jiang, Paul Pu Liang, Jinhua Zhao, Luis Alberto Alonso Pastor, Kent Larson</dc:creator>
    </item>
    <item>
      <title>Game mechanics for cyber-harm awareness in the metaverse</title>
      <link>https://arxiv.org/abs/2510.15180</link>
      <description>arXiv:2510.15180v1 Announce Type: cross 
Abstract: Educating children and young people to be safe online is essential, especially as the metaverse, a next-generation internet blending immersive technologies, promises to reshape their interactions and amplify their experiences. While virtual reality offers fully immersive, highly interactive, and multi-sensory engagement, it also heightens cyber harm risks for young or vulnerable users. To address this, the CyberNinjas VR experience was developed to educate children aged 8 to 16 on safe metaverse behaviours, providing clear referral steps for harmful interactions. Understanding user engagement in metaverse gaming will aid the design of future VR environments which prioritize safety and inclusivity. This project analyses CyberNinjas to understand how game mechanics can foster cyber-safe behaviours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15180v1</guid>
      <category>cs.MM</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sophie McKenzie, Jeb Webb, Robin Doss</dc:creator>
    </item>
    <item>
      <title>WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing</title>
      <link>https://arxiv.org/abs/2510.15221</link>
      <description>arXiv:2510.15221v1 Announce Type: cross 
Abstract: Automated emotion recognition in real-world workplace settings remains a challenging problem in affective computing due to the scarcity of large-scale, longitudinal datasets collected in naturalistic environments. We present a novel dataset comprising 733,651 facial expression records from 38 employees collected over 30.5 months (November 2021 to May 2024) in an authentic office environment. Each record contains seven emotion probabilities (neutral, happy, sad, surprised, fear, disgusted, angry) derived from deep learning-based facial expression recognition, along with comprehensive metadata including job roles, employment outcomes, and personality traits. The dataset uniquely spans the COVID-19 pandemic period, capturing emotional responses to major societal events including the Shanghai lockdown and policy changes. We provide 32 extended emotional metrics computed using established affective science methods, including valence, arousal, volatility, predictability, inertia, and emotional contagion strength. Technical validation demonstrates high data quality through successful replication of known psychological patterns (weekend effect: +192% valence improvement, p &lt; 0.001; diurnal rhythm validated) and perfect predictive validity for employee turnover (AUC=1.0). Baseline experiments using Random Forest and LSTM models achieve 91.2% accuracy for emotion classification and R2 = 0.84 for valence prediction. This is the largest and longest longitudinal workplace emotion dataset publicly available, enabling research in emotion recognition, affective dynamics modeling, emotional contagion, turnover prediction, and emotion-aware system design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15221v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiao Sun</dc:creator>
    </item>
    <item>
      <title>From Checklists to Clusters: A Homeostatic Account of AGI Evaluation</title>
      <link>https://arxiv.org/abs/2510.15236</link>
      <description>arXiv:2510.15236v1 Announce Type: cross 
Abstract: Contemporary AGI evaluations report multidomain capability profiles, yet they typically assign symmetric weights and rely on snapshot scores. This creates two problems: (i) equal weighting treats all domains as equally important when human intelligence research suggests otherwise, and (ii) snapshot testing can't distinguish durable capabilities from brittle performances that collapse under delay or stress. I argue that general intelligence -- in humans and potentially in machines -- is better understood as a homeostatic property cluster: a set of abilities plus the mechanisms that keep those abilities co-present under perturbation. On this view, AGI evaluation should weight domains by their causal centrality (their contribution to cluster stability) and require evidence of persistence across sessions. I propose two battery-compatible extensions: a centrality-prior score that imports CHC-derived weights with transparent sensitivity analysis, and a Cluster Stability Index family that separates profile persistence, durable learning, and error correction. These additions preserve multidomain breadth while reducing brittleness and gaming. I close with testable predictions and black-box protocols labs can adopt without architectural access.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15236v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brett Reynolds</dc:creator>
    </item>
    <item>
      <title>DSSmoothing: Toward Certified Dataset Ownership Verification for Pre-trained Language Models via Dual-Space Smoothing</title>
      <link>https://arxiv.org/abs/2510.15303</link>
      <description>arXiv:2510.15303v1 Announce Type: cross 
Abstract: Large web-scale datasets have driven the rapid advancement of pre-trained language models (PLMs), but unauthorized data usage has raised serious copyright concerns. Existing dataset ownership verification (DOV) methods typically assume that watermarks remain stable during inference; however, this assumption often fails under natural noise and adversary-crafted perturbations. We propose the first certified dataset ownership verification method for PLMs based on dual-space smoothing (i.e., DSSmoothing). To address the challenges of text discreteness and semantic sensitivity, DSSmoothing introduces continuous perturbations in the embedding space to capture semantic robustness and applies controlled token reordering in the permutation space to capture sequential robustness. DSSmoothing consists of two stages: in the first stage, triggers are collaboratively embedded in both spaces to generate norm-constrained and robust watermarked datasets; in the second stage, randomized smoothing is applied in both spaces during verification to compute the watermark robustness (WR) of suspicious models and statistically compare it with the principal probability (PP) values of a set of benign models. Theoretically, DSSmoothing provides provable robustness guarantees for dataset ownership verification by ensuring that WR consistently exceeds PP under bounded dual-space perturbations. Extensive experiments on multiple representative web datasets demonstrate that DSSmoothing achieves stable and reliable verification performance and exhibits robustness against potential adaptive attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15303v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ting Qiao, Xing Liu, Wenke Huang, Jianbin Li, Zhaoxin Fan, Yiming Li</dc:creator>
    </item>
    <item>
      <title>Automatic essay scoring: leveraging Jaccard coefficient and Cosine similaritywith n-gram variation in vector space model approach</title>
      <link>https://arxiv.org/abs/2510.15311</link>
      <description>arXiv:2510.15311v1 Announce Type: cross 
Abstract: Automated essay scoring (AES) is a vital area of research aiming to provide efficient and accurate assessment tools for evaluating written content. This study investigates the effectiveness of two popular similarity metrics, Jaccard coefficient, and Cosine similarity, within the context of vector space models(VSM)employing unigram, bigram, and trigram representations. The data used in this research was obtained from the formative essay of the citizenship education subject in a junior high school. Each essay undergoes preprocessing to extract features using n-gram models, followed by vectorization to transform text data into numerical representations. Then, similarity scores are computed between essays using both Jaccard coefficient and Cosine similarity. The performance of the system is evaluated by analyzing the root mean square error (RMSE), which measures the difference between the scores given by human graders and those generated by the system. The result shows that the Cosine similarity outperformed the Jaccard coefficient. In terms of n-gram, unigrams have lower RMSE compared to bigrams and trigrams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15311v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.11591/ijai.v14.i5.pp3599-3612</arxiv:DOI>
      <dc:creator>Andharini Dwi Cahyani, Moh. Wildan Fathoni, Fika Hastarita Rachman, Ari Basuki, Salman Amin, Bain Khusnul Khotimah</dc:creator>
    </item>
    <item>
      <title>Towards Proactive Defense Against Cyber Cognitive Attacks</title>
      <link>https://arxiv.org/abs/2510.15801</link>
      <description>arXiv:2510.15801v1 Announce Type: cross 
Abstract: Cyber cognitive attacks leverage disruptive innovations (DIs) to exploit psychological biases and manipulate decision-making processes. Emerging technologies, such as AI-driven disinformation and synthetic media, have accelerated the scale and sophistication of these threats. Prior studies primarily categorize current cognitive attack tactics, lacking predictive mechanisms to anticipate future DIs and their malicious use in cognitive attacks. This paper addresses these gaps by introducing a novel predictive methodology for forecasting the emergence of DIs and their malicious uses in cognitive attacks. We identify trends in adversarial tactics and propose proactive defense strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15801v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bonnie Rushing, Mac-Rufus Umeokolo, Shouhuai Xu</dc:creator>
    </item>
    <item>
      <title>Establishing trust in automated reasoning</title>
      <link>https://arxiv.org/abs/2309.12351</link>
      <description>arXiv:2309.12351v2 Announce Type: replace 
Abstract: Since its beginnings in the 1940s, automated reasoning by computers has become a tool of ever growing importance in scientific research. So far, the rules underlying automated reasoning have mainly been formulated by humans, in the form of program source code. Rules derived from large amounts of data, via machine learning techniques, are a complementary approach currently under intense development. The question of why we should trust these systems, and the results obtained with their help, has been discussed by philosophers of science but has so far received little attention by practitioners. The present work focuses on independent reviewing, an important source of trust in science, and identifies the characteristics of automated reasoning systems that affect their reviewability. It also discusses possible steps towards increasing reviewability and trustworthiness via a combination of technical and social measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12351v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konrad Hinsen (SSOLEIL, CBM)</dc:creator>
    </item>
    <item>
      <title>Unfair Learning: GenAI Exceptionalism and Copyright Law</title>
      <link>https://arxiv.org/abs/2504.00955</link>
      <description>arXiv:2504.00955v3 Announce Type: replace 
Abstract: This paper challenges the argument that generative artificial intelligence (GenAI) is entitled to broad immunity from copyright law for reproducing copyrighted works without authorization due to a fair use defense. It examines fair use legal arguments and eight distinct substantive arguments, contending that every legal and substantive argument favoring fair use for GenAI applies equally, if not more so, to humans. Therefore, granting GenAI exceptional privileges in this domain is legally and logically inconsistent with withholding broad fair use exemptions from individual humans. It would mean no human would need to pay for virtually any copyright work again. The solution is to take a circumspect view of any fair use claim for mass copyright reproduction by any entity and focus on the first principles of whether permitting such exceptionalism for GenAI promotes science and the arts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00955v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Atkinson</dc:creator>
    </item>
    <item>
      <title>Competition and Diversity in Generative AI</title>
      <link>https://arxiv.org/abs/2412.08610</link>
      <description>arXiv:2412.08610v2 Announce Type: replace-cross 
Abstract: Recent evidence, both in the lab and in the wild, suggests that the use of generative artificial intelligence reduces the diversity of content produced. The use of the same or similar AI models appears to lead to more homogeneous behavior. Our work begins with the observation that there is a force pushing in the opposite direction: compe- tition. When producers compete with one another (e.g., for customers or attention), they are incentivized to create novel or unique content. We explore the impact com- petition has on both content diversity and overall social welfare. Through a formal game-theoretic model, we show that competitive markets select for diverse AI models, mitigating monoculture. We further show that a generative AI model that performs well in isolation (i.e., according to a benchmark) may fail to provide value in a compet- itive market. Our results highlight the importance of evaluating generative AI models across the breadth of their output distributions, particularly when they will be deployed in competitive environments. We validate our results empirically by using language models to play Scattergories, a word game in which players are rewarded for answers that are both correct and unique. Overall, our results suggest that homogenization due to generative AI is unlikely to persist in competitive markets, and instead, competition in downstream markets may drive diversification in AI model development</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08610v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manish Raghavan</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Dataset Combination</title>
      <link>https://arxiv.org/abs/2502.05765</link>
      <description>arXiv:2502.05765v3 Announce Type: replace-cross 
Abstract: Access to diverse, high-quality datasets is crucial for machine learning model performance, yet data sharing remains limited by privacy concerns and competitive interests, particularly in regulated domains like healthcare. This dynamic especially disadvantages smaller organizations that lack resources to purchase data or negotiate favorable sharing agreements, due to the inability to \emph{privately} assess external data's utility.
  To resolve privacy and uncertainty tensions simultaneously, we introduce {\SecureKL}, the first secure protocol for dataset-to-dataset evaluations with zero privacy leakage, designed to be applied preceding data sharing. {\SecureKL} evaluates a source dataset against candidates, performing dataset divergence metrics internally with private computations, all without assuming downstream models.
  On real-world data, {\SecureKL} achieves high consistency ($&gt;90\%$ correlation with non-private counterparts) and successfully identifies beneficial data collaborations in highly-heterogeneous domains (ICU mortality prediction across hospitals and income prediction across states). Our results highlight that secure computation maximizes data utilization, outperforming privacy-agnostic utility assessments that leak information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05765v3</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keren Fuentes, Mimee Xu, Irene Chen</dc:creator>
    </item>
    <item>
      <title>MemeSense: An Adaptive In-Context Framework for Social Commonsense Driven Meme Moderation</title>
      <link>https://arxiv.org/abs/2502.11246</link>
      <description>arXiv:2502.11246v2 Announce Type: replace-cross 
Abstract: Online memes are a powerful yet challenging medium for content moderation, often masking harmful intent behind humor, irony, or cultural symbolism. Conventional moderation systems "especially those relying on explicit text" frequently fail to recognize such subtle or implicit harm. We introduce MemeSense, an adaptive framework designed to generate socially grounded interventions for harmful memes by combining visual and textual understanding with curated, semantically aligned examples enriched with commonsense cues. This enables the model to detect nuanced complexed threats like misogyny, stereotyping, or vulgarity "even in memes lacking overt language". Across multiple benchmark datasets, MemeSense outperforms state-of-the-art methods, achieving up to 35% higher semantic similarity and 9% improvement in BERTScore for non-textual memes, and notable gains for text-rich memes as well. These results highlight MemeSense as a promising step toward safer, more context-aware AI systems for real-world content moderation. Code and data available at: https://github.com/sayantan11995/MemeSense</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11246v2</guid>
      <category>cs.IR</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayantan Adak, Somnath Banerjee, Rajarshi Mandal, Avik Halder, Sayan Layek, Rima Hazra, Animesh Mukherjee</dc:creator>
    </item>
    <item>
      <title>Operationalizing Automated Essay Scoring: A Human-Aware Approach</title>
      <link>https://arxiv.org/abs/2506.21603</link>
      <description>arXiv:2506.21603v2 Announce Type: replace-cross 
Abstract: This paper explores the human-centric operationalization of Automated Essay Scoring (AES) systems, addressing aspects beyond accuracy. We compare various machine learning-based approaches with Large Language Models (LLMs) approaches, identifying their strengths, similarities and differences. The study investigates key dimensions such as bias, robustness, and explainability, considered important for human-aware operationalization of AES systems. Our study shows that ML-based AES models outperform LLMs in accuracy but struggle with explainability, whereas LLMs provide richer explanations. We also found that both approaches struggle with bias and robustness to edge scores. By analyzing these dimensions, the paper aims to identify challenges and trade-offs between different methods, contributing to more reliable and trustworthy AES methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21603v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yenisel Plasencia-Cala\~na</dc:creator>
    </item>
    <item>
      <title>Dr. Bias: Social Disparities in AI-Powered Medical Guidance</title>
      <link>https://arxiv.org/abs/2510.09162</link>
      <description>arXiv:2510.09162v2 Announce Type: replace-cross 
Abstract: With the rapid progress of Large Language Models (LLMs), the general public now has easy and affordable access to applications capable of answering most health-related questions in a personalized manner. These LLMs are increasingly proving to be competitive, and now even surpass professionals in some medical capabilities. They hold particular promise in low-resource settings, considering they provide the possibility of widely accessible, quasi-free healthcare support. However, evaluations that fuel these motivations highly lack insights into the social nature of healthcare, oblivious to health disparities between social groups and to how bias may translate into LLM-generated medical advice and impact users. We provide an exploratory analysis of LLM answers to a series of medical questions spanning key clinical domains, where we simulate these questions being asked by several patient profiles that vary in sex, age range, and ethnicity. By comparing natural language features of the generated responses, we show that, when LLMs are used for medical advice generation, they generate responses that systematically differ between social groups. In particular, Indigenous and intersex patients receive advice that is less readable and more complex. We observe these trends amplify when intersectional groups are considered. Considering the increasing trust individuals place in these models, we argue for higher AI literacy and for the urgent need for investigation and mitigation by AI developers to ensure these systemic differences are diminished and do not translate to unjust patient support. Our code is publicly available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09162v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Kondrup, Anne Imouza</dc:creator>
    </item>
    <item>
      <title>Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers</title>
      <link>https://arxiv.org/abs/2510.13939</link>
      <description>arXiv:2510.13939v2 Announce Type: replace-cross 
Abstract: The use of copyrighted books for training AI models has led to numerous lawsuits from authors concerned about AI's ability to generate derivative content. Yet it's unclear if these models can generate high quality literary text while emulating authors' styles. To answer this we conducted a preregistered study comparing MFA-trained expert writers with three frontier AI models: ChatGPT, Claude &amp; Gemini in writing up to 450 word excerpts emulating 50 award-winning authors' diverse styles. In blind pairwise evaluations by 159 representative expert &amp; lay readers, AI-generated text from in-context prompting was strongly disfavored by experts for both stylistic fidelity (OR=0.16, p&lt;10^-8) &amp; writing quality (OR=0.13, p&lt;10^-7) but showed mixed results with lay readers. However, fine-tuning ChatGPT on individual authors' complete works completely reversed these findings: experts now favored AI-generated text for stylistic fidelity (OR=8.16, p&lt;10^-13) &amp; writing quality (OR=1.87, p=0.010), with lay readers showing similar shifts. These effects generalize across authors &amp; styles. The fine-tuned outputs were rarely flagged as AI-generated (3% rate v. 97% for in-context prompting) by best AI detectors. Mediation analysis shows this reversal occurs because fine-tuning eliminates detectable AI stylistic quirks (e.g., cliche density) that penalize in-context outputs. While we do not account for additional costs of human effort required to transform raw AI output into cohesive, publishable prose, the median fine-tuning &amp; inference cost of $81 per author represents a dramatic 99.7% reduction compared to typical professional writer compensation. Author-specific fine-tuning thus enables non-verbatim AI writing that readers prefer to expert human writing, providing empirical evidence directly relevant to copyright's fourth fair-use factor, the "effect upon the potential market or value" of the source works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13939v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuhin Chakrabarty, Jane C. Ginsburg, Paramveer Dhillon</dc:creator>
    </item>
  </channel>
</rss>
