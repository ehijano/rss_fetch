<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Targeting Neurodegeneration: Three Machine Learning Methods for G9a Inhibitors Discovery Using PubChem and Scikit-learn</title>
      <link>https://arxiv.org/abs/2503.16214</link>
      <description>arXiv:2503.16214v1 Announce Type: new 
Abstract: In light of the increasing interest in G9a's role in neuroscience, three machine learning (ML) models, that are time efficient and cost effective, were developed to support researchers in this area. The models are based on data provided by PubChem and performed by algorithms interpreted by the scikit-learn Python-based ML library. The first ML model aimed to predict the efficacy magnitude of active G9a inhibitors. The ML models were trained with 3,112 and tested with 778 samples. The Gradient Boosting Regressor perform the best, achieving 17.81% means relative error (MRE), 21.48% mean absolute error (MAE), 27.39% root mean squared error (RMSE) and 0.02 coefficient of determination (R2) error. The goal of the second ML model called a CID_SID ML model, utilised PubChem identifiers to predict the G9a inhibition probability of a small biomolecule that has been primarily designed for different purposes. The ML models were trained with 58,552 samples and tested with 14,000. The most suitable classifier for this case study was the Extreme Gradient Boosting Classifier, which obtained 78.1% accuracy, 84.3% precision,69.1% recall, 75.9% F1-score and 8.1% Receiver-operating characteristic (ROC). The third ML model based on the Random Forest Classifier algorithm led to the generation of a list of descending-ordered functional groups based on their importance to the G9a inhibition. The model was trained with 19,455 samples and tested with 14,100. The probability of this rank was 70% accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16214v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mariya L. Ivanova, Nicola Russo, Konstantin Nikolic</dc:creator>
    </item>
    <item>
      <title>Karyotype AI for Precision Oncology</title>
      <link>https://arxiv.org/abs/2211.14312</link>
      <description>arXiv:2211.14312v4 Announce Type: replace 
Abstract: We present a machine learning method capable of accurately detecting chromosome abnormalities that cause blood cancers directly from microscope images of the metaphase stage of cell division. The pipeline is built on a series of fine-tuned Vision Transformers. Current state of the art (and standard clinical practice) requires expensive, manual expert analysis, whereas our pipeline takes only 15 seconds per metaphase image. Using a novel pretraining-finetuning strategy to mitigate the challenge of data scarcity, we achieve a high precision-recall score of 94% AUC for the clinically significant del(5q) and t(9;22) anomalies. Our method also unlocks zero-shot detection of rare aberrations based on model latent embeddings. The ability to quickly, accurately, and scalably diagnose genetic abnormalities directly from metaphase images could transform karyotyping practice and improve patient outcomes. We will make code publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14312v4</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zahra Shamsi, Drew Bryant, Jacob Wilson, Xiaoyu Qu, Avinava Dubey, Konik Kothari, Mostafa Dehghani, Mariya Chavarha, Valerii Likhosherstov, Brian Williams, Michael Frumkin, Fred Appelbaum, Krzysztof Choromanski, Ali Bashir, Min Fang</dc:creator>
    </item>
    <item>
      <title>FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification</title>
      <link>https://arxiv.org/abs/2411.14743</link>
      <description>arXiv:2411.14743v2 Announce Type: replace-cross 
Abstract: Few-shot learning presents a critical solution for cancer diagnosis in computational pathology (CPath), addressing fundamental limitations in data availability, particularly the scarcity of expert annotations and patient privacy constraints. A key challenge in this paradigm stems from the inherent disparity between the limited training set of whole slide images (WSIs) and the enormous number of contained patches, where a significant portion of these patches lacks diagnostically relevant information, potentially diluting the model's ability to learn and focus on critical diagnostic features. While recent works attempt to address this by incorporating additional knowledge, several crucial gaps hinder further progress: (1) despite the emergence of powerful pathology foundation models (FMs), their potential remains largely untapped, with most approaches limiting their use to basic feature extraction; (2) current language guidance mechanisms attempt to align text prompts with vast numbers of WSI patches all at once, struggling to leverage rich pathological semantic information. To this end, we introduce the knowledge-enhanced adaptive visual compression framework, dubbed FOCUS, which uniquely combines pathology FMs with language prior knowledge to enable a focused analysis of diagnostically relevant regions by prioritizing discriminative WSI patches. Our approach implements a progressive three-stage compression strategy: we first leverage FMs for global visual redundancy elimination, and integrate compressed features with language prompts for semantic relevance assessment, then perform neighbor-aware visual token filtering while preserving spatial coherence. Extensive experiments on pathological datasets spanning breast, lung, and ovarian cancers demonstrate its superior performance in few-shot pathology diagnosis. Codes are available at https://github.com/dddavid4real/FOCUS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14743v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengrui Guo, Conghao Xiong, Jiabo Ma, Qichen Sun, Lishuang Feng, Jinzhuo Wang, Hao Chen</dc:creator>
    </item>
    <item>
      <title>Functional Correspondences in the Human and Marmoset Visual Cortex During Movie Watching: Insights from Correlation, Redundancy, and Synergy</title>
      <link>https://arxiv.org/abs/2503.15218</link>
      <description>arXiv:2503.15218v2 Announce Type: replace-cross 
Abstract: The world of beauty is deeply connected to the visual cortex, as perception often begins with vision in both humans and marmosets. Quantifying functional correspondences in the visual cortex across species can help us understand how information is processed in the primate visual cortex, while also providing deeper insights into human visual cortex functions through the study of marmosets. In this study, we measured pairwise and beyond pairwise correlation, redundancy, and synergy in movie-driven fMRI data across species. Our first key finding was that humans and marmosets exhibited significant overlaps in functional synergy. Second, we observed that the strongest functional correspondences between the human peri-entorhinal and entorhinal cortex (PeEc) and the occipitotemporal higher-level visual regions in the marmoset during movie watching reflected a functional synergistic relationship. These regions are known to correspond to face-selective areas in both species. Third, redundancy measures maintained stable high-order hubs, indicating a steady core of shared information processing, while synergy measures revealed a dynamic shift from low- to high-level visual regions as interaction increased, reflecting adaptive integration. This highlights distinct patterns of information processing across the visual hierarchy. Ultimately, our results reveal the marmoset as a compelling model for investigating visual perception, distinguished by its remarkable functional parallels to the human visual cortex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15218v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Li, Ting Xu, Vince D. Calhoun</dc:creator>
    </item>
  </channel>
</rss>
