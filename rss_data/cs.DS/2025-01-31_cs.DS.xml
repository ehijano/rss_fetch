<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sequential Testing with Subadditive Costs</title>
      <link>https://arxiv.org/abs/2501.18010</link>
      <description>arXiv:2501.18010v1 Announce Type: new 
Abstract: In the classic sequential testing problem, we are given a system with several components each of which fails with some independent probability. The goal is to identify whether or not some component has failed. When the test costs are additive, it is well known that a greedy algorithm finds an optimal solution. We consider a much more general setting with subadditive cost functions and provide a $(4\rho+\gamma)$-approximation algorithm, assuming a $\gamma$-approximate value oracle (that computes the cost of any subset) and a $\rho$-approximate ratio oracle (that finds a subset with minimum ratio of cost to failure probability). While the natural greedy algorithm has a poor approximation ratio in the subadditive case, we show that a suitable truncation achieves the above guarantee. Our analysis is based on a connection to the minimum sum set cover problem. As applications, we obtain the first approximation algorithms for sequential testing under various cost-structures: $(5+\epsilon)$-approximation for tree-based costs, $9.5$-approximation for routing costs and $(4+\ln n)$ for machine activation costs. We also show that sequential testing under submodular costs does not admit any poly-logarithmic approximation (assuming the exponential time hypothesis).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18010v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Blake Harris, Viswanath Nagarajan, Rayen Tan</dc:creator>
    </item>
    <item>
      <title>Facility Location on High-dimensional Euclidean Spaces</title>
      <link>https://arxiv.org/abs/2501.18105</link>
      <description>arXiv:2501.18105v1 Announce Type: new 
Abstract: Recent years have seen great progress in the approximability of fundamental clustering and facility location problems on high-dimensional Euclidean spaces, including $k$-Means and $k$-Median. While they admit strictly better approximation ratios than their general metric versions, their approximation ratios are still higher than the hardness ratios for general metrics, leaving the possibility that the ultimate optimal approximation ratios will be the same between Euclidean and general metrics. Moreover, such an improved algorithm for Euclidean spaces is not known for Uncapaciated Facility Location (UFL), another fundamental problem in the area.
  In this paper, we prove that for any $\gamma \geq 1.6774$ there exists $\varepsilon &gt; 0$ such that Euclidean UFL admits a $(\gamma, 1 + 2e^{-\gamma} - \varepsilon)$-bifactor approximation algorithm, improving the result of Byrka and Aardal. Together with the $(\gamma, 1 + 2e^{-\gamma})$ NP-hardness in general metrics, it shows the first separation between general and Euclidean metrics for the aforementioned basic problems. We also present an $(\alpha_{Li} - \varepsilon)$-(unifactor) approximation algorithm for UFL for some $\varepsilon &gt; 0$ in Euclidean spaces, where $\alpha_{Li} \approx 1.488$ is the best-known approximation ratio for UFL by Li.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18105v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Euiwoong Lee, Kijun Shin</dc:creator>
    </item>
    <item>
      <title>Graph Exploration with Edge Weight Estimates</title>
      <link>https://arxiv.org/abs/2501.18496</link>
      <description>arXiv:2501.18496v1 Announce Type: new 
Abstract: In the Travelling Salesman Problem, every vertex of an edge-weighted graph has to be visited by an agent who traverses the edges of the graph. In this problem, it is usually assumed that the costs of each edge are given in advance, making it computationally hard but possible to calculate an optimal tour for the agent.
  Also in the Graph Exploration Problem, every vertex of a given graph must be visited, but here the graph is not known in the beginning - at every point, an algorithm only knows about the already visited vertices and their neighbors.
  Both however are not necessarily realistic settings: Usually the structure of the graph (for example underlying road network) is known in advance, but the details are not. One usually has a prediction of how long it takes to traverse through a particular road, but due to road conditions or imprecise maps the agent might realize that a road will take slightly longer than expected when arriving on it. To deal with those deviations, it is natural to assume that the agent is able to adapt to the situation: When realizing that taking a particular road is more expensive than expected, recalculating the tour and taking another road instead is possible.
  We analyze the competitive ratio of this problem based on the perturbation factor $\alpha$ of the edge weights. For general graphs we show that for realistic factors smaller than $2$ there is no strategy that achieves a competitive ratio better than $\alpha$, which can be matched by a simple algorithm.
  In addition, we prove an algorithm which has a competitive ratio of $\frac{1+\alpha}{2}$ for restricted graph classes like complete graphs with uniform announced edge weights. Here, we present a matching lower bound as well, proving that the strategy for those graph classes is best possible.
  We conclude with a remark about special graph classes like cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18496v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthias Gehnen, Ralf Klasing, \'Emile Naquin</dc:creator>
    </item>
    <item>
      <title>Digital Quantum Simulations of the Non-Resonant Open Tavis-Cummings Model</title>
      <link>https://arxiv.org/abs/2501.18522</link>
      <description>arXiv:2501.18522v1 Announce Type: cross 
Abstract: The open Tavis-Cummings model consists of $N$ quantum emitters interacting with a common cavity mode, accounts for losses and decoherence, and is frequently explored for quantum information processing and designing quantum devices. As $N$ increases, it becomes harder to simulate the open Tavis-Cummings model using traditional methods. To address this problem, we implement two quantum algorithms for simulating the dynamics of this model in the inhomogenous, non-resonant regime, with up to three excitations in the cavity. We show that the implemented algorithms have gate complexities that scale polynomially, as $O(N^2)$ and $O(N^3)$. One of these algorithms is the sampling-based wave matrix Lindbladization algorithm, for which we propose two protocols to implement its system-independent fixed interaction, resolving key open questions of [Patel and Wilde, Open Sys. &amp; Info. Dyn., 30:2350014 (2023)]. Furthermore, we benchmark our results against a classical differential equation solver and demonstrate the ability to simulate classically intractable systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18522v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>physics.comp-ph</category>
      <category>physics.optics</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aidan N. Sims, Dhrumil Patel, Aby Philip, Alex H. Rubin, Rahul Bandyopadhyay, Marina Radulaski, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>Connected Components in Linear Work and Near-Optimal Time</title>
      <link>https://arxiv.org/abs/2312.02332</link>
      <description>arXiv:2312.02332v4 Announce Type: replace 
Abstract: Computing the connected components of a graph is a fundamental problem in algorithmic graph theory. A major question in this area is whether we can compute connected components in $o(\log n)$ parallel time. Recent works showed an affirmative answer in the Massively Parallel Computation (MPC) model for a wide class of graphs. Specifically, Behnezhad et al. (FOCS'19) showed that connected components can be computed in $O(\log d + \log \log n)$ rounds in the MPC model. More recently, Liu et al. (SPAA'20) showed that the same result can be achieved in the standard PRAM model but their result incurs $\Theta((m+n) \cdot (\log d + \log \log n))$ work which is sub-optimal.
  In this paper, we show that for graphs that contain \emph{well-connected} components, we can compute connected components on a PRAM in sub-logarithmic parallel time with \emph{optimal}, i.e., $O(m+n)$ total work. Specifically, our algorithm achieves $O(\log(1/\lambda) + \log \log n)$ parallel time with high probability, where $\lambda$ is the minimum spectral gap of any connected component in the input graph. The algorithm requires no prior knowledge on $\lambda$.
  Additionally, based on the \textsc{2-Cycle} Conjecture we provide a time lower bound of $\Omega(\log(1/\lambda))$ for solving connected components on a PRAM with $O(m+n)$ total memory when $\lambda \le (1/\log n)^c$, giving conditional optimality to the running time of our algorithm as a parameter of $\lambda$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02332v4</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Farhadi, S. Cliff Liu, Elaine Shi</dc:creator>
    </item>
    <item>
      <title>Exact and Approximate High-Multiplicity Scheduling on Identical Machines</title>
      <link>https://arxiv.org/abs/2404.17274</link>
      <description>arXiv:2404.17274v2 Announce Type: replace 
Abstract: Goemans and Rothvoss (SODA'14) gave a framework for solving problems which can be described as finding a point in $int.cone(P\cap\mathbb{Z}^N)\cap Q$, where $P,Q\subset\mathbb{R}^N$ are (bounded) polyhedra. The running time for solving such a problem is $enc(P)^{2^{O(N)}}enc(Q)^{O(1)}$. This framework can be used to solve various scheduling problems, but the encoding length $enc(P)$ usually involves large parameters like the makespan. We describe three tools to improve the framework:
  - Problem-specific preprocessing can be used to greatly reduce $enc(P)$.
  - By solving a certain LP relaxation and then using the classical result by Frank and Tardos (J. Comb. '87), we get a more compact encoding of $P$ in general.
  - A result by Jansen and Klein (SODA'17) makes the running time depend on the number of vertices of the integer hull of $P$. We provide a new bound for this number that is similar to the one by Berndt et al. (SOSA'21) but better for our setting.
  For example, applied to the scheduling problem $P||C_{\max}$, these tools improve the running time from $(\log(C_{\max}))^{2^{O(d)}}enc(I)^{O(1)}$ to the possibly much better $(\log(p_{\max}))^{2^{O(d)}}enc(I)^{O(1)}$. Here, $p_{\max}$ is the largest processing time, $d$ is the number of different processing times, $C_{\max}$ is the makespan and $enc(I)$ is the encoding length of the instance. On the complexity side, we use reductions from the literature to provide new parameterized lower bounds for $P||C_{\max}$. Finally, we show that the big open question asked by Mnich and van Bevern (Comput. Oper. Res. '18) whether $P||C_{\max}$ is FPT w.r.t. the number of job types $d$ has the same answer as the question whether $Q||C_{\max}$ is FPT w.r.t. the number of job and machine types $d+\tau$ (all in high-multiplicity encoding). The same holds for objective $C_{\min}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17274v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Klaus Jansen, Kai Kahler, Esther Zwanger</dc:creator>
    </item>
    <item>
      <title>Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming</title>
      <link>https://arxiv.org/abs/2305.19706</link>
      <description>arXiv:2305.19706v4 Announce Type: replace-cross 
Abstract: Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show the necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on five application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.19706v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacobus G. M. van der Linden, Mathijs M. de Weerdt, Emir Demirovi\'c</dc:creator>
    </item>
    <item>
      <title>Optimal Decentralized Smoothed Online Convex Optimization</title>
      <link>https://arxiv.org/abs/2411.08355</link>
      <description>arXiv:2411.08355v2 Announce Type: replace-cross 
Abstract: We study the multi-agent Smoothed Online Convex Optimization (SOCO) problem, where $N$ agents interact through a communication graph. In each round, each agent $i$ receives a strongly convex hitting cost function $f^i_t$ in an online fashion and selects an action $x^i_t \in \mathbb{R}^d$. The objective is to minimize the global cumulative cost, which includes the sum of individual hitting costs $f^i_t(x^i_t)$, a temporal "switching cost" for changing decisions, and a spatial "dissimilarity cost" that penalizes deviations in decisions among neighboring agents. We propose the first truly decentralized algorithm ACORD for multi-agent SOCO that provably exhibits asymptotic optimality. Our approach allows each agent to operate using only local information from its immediate neighbors in the graph. For finite-time performance, we establish that the optimality gap in the competitive ratio decreases with time horizon $T$ and can be conveniently tuned based on the per-round computation available to each agent. Our algorithm benefits from a provably scalable computational complexity that depends only logarithmically on the number of agents and almost linearly on their degree within the graph. Moreover, our results hold even when the communication graph changes arbitrarily and adaptively over time. Finally, ACORD, by virtue of its asymptotic-optimality, is shown to be provably superior to the state-of-the-art LPC algorithm, while exhibiting much lower computational complexity. Extensive numerical experiments across various network topologies further corroborate our theoretical claims.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08355v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neelkamal Bhuyan, Debankur Mukherjee, Adam Wierman</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Algorithms for Omniprediction</title>
      <link>https://arxiv.org/abs/2501.17205</link>
      <description>arXiv:2501.17205v2 Announce Type: replace-cross 
Abstract: Omnipredictors are simple prediction functions that encode loss-minimizing predictions with respect to a hypothesis class $\mathcal{H}$, simultaneously for every loss function within a class of losses $\mathcal{L}$. In this work, we give near-optimal learning algorithms for omniprediction, in both the online and offline settings. To begin, we give an oracle-efficient online learning algorithm that acheives $(\mathcal{L},\mathcal{H})$-omniprediction with $\tilde{O}(\sqrt{T \log |\mathcal{H}|})$ regret for any class of Lipschitz loss functions $\mathcal{L} \subseteq \mathcal{L}_\mathrm{Lip}$. Quite surprisingly, this regret bound matches the optimal regret for \emph{minimization of a single loss function} (up to a $\sqrt{\log(T)}$ factor). Given this online algorithm, we develop an online-to-offline conversion that achieves near-optimal complexity across a number of measures. In particular, for all bounded loss functions within the class of Bounded Variation losses $\mathcal{L}_\mathrm{BV}$ (which include all convex, all Lipschitz, and all proper losses) and any (possibly-infinite) $\mathcal{H}$, we obtain an offline learning algorithm that, leveraging an (offline) ERM oracle and $m$ samples from $\mathcal{D}$, returns an efficient $(\mathcal{L}_{\mathrm{BV}},\mathcal{H},\varepsilon(m))$-omnipredictor for $\varepsilon(m)$ scaling near-linearly in the Rademacher complexity of $\mathrm{Th} \circ \mathcal{H}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17205v2</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Princewill Okoroafor, Robert Kleinberg, Michael P. Kim</dc:creator>
    </item>
  </channel>
</rss>
