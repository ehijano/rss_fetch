<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 02:04:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Using AI Alignment Theory to understand the potential pitfalls of regulatory frameworks</title>
      <link>https://arxiv.org/abs/2410.19749</link>
      <description>arXiv:2410.19749v1 Announce Type: new 
Abstract: This paper leverages insights from Alignment Theory (AT) research, which primarily focuses on the potential pitfalls of technical alignment in Artificial Intelligence, to critically examine the European Union's Artificial Intelligence Act (EU AI Act). In the context of AT research, several key failure modes - such as proxy gaming, goal drift, reward hacking or specification gaming - have been identified. These can arise when AI systems are not properly aligned with their intended objectives. The central logic of this report is: what can we learn if we treat regulatory efforts in the same way as we treat advanced AI systems? As we systematically apply these concepts to the EU AI Act, we uncover potential vulnerabilities and areas for improvement in the regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19749v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Tlaie</dc:creator>
    </item>
    <item>
      <title>Interval-valued q-rung orthopair fuzzy Weber operator and its group decision-making application</title>
      <link>https://arxiv.org/abs/2410.19752</link>
      <description>arXiv:2410.19752v1 Announce Type: new 
Abstract: The evaluation of learning effectiveness requires the integration of objective test results and analysis of uncertain subjective evaluations. Fuzzy theory methods are suitable for handling fuzzy information and uncertainty to obtain comprehensive and accurate evaluation results. In this paper, we develop a Swing-based multi-attribute group decision-making (MAGDM) method under interval-valued q-rung orthopair fuzzy sets (IVq-ROFSs). Firstly, an extended interval-valued q rung orthopair Weber ordered weighted average (IVq-ROFWOWA) operator is introduced. Then the attribute weights deriving method is designed by using the optimized Swing algorithm. Furthermore, we develop a MAGDM method for evaluating students' learning effectiveness using the IVq-ROFWOWA operator and the Swing algorithm. Finally, a case of evaluating students' learning effectiveness is illustrated by using the proposed MAGDM method. The implementing results demonstrate that the proposed MAGDM method is feasible and effective, and the Swing algorithm enhances better differentiation in ranking alternatives compared to other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19752v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benting Wana, Zhuocheng Wua, Mengjie Hanb, Minjun Wana</dc:creator>
    </item>
    <item>
      <title>A Comparative Analysis on Ethical Benchmarking in Large Language Models</title>
      <link>https://arxiv.org/abs/2410.19753</link>
      <description>arXiv:2410.19753v1 Announce Type: new 
Abstract: This work contributes to the field of Machine Ethics (ME) benchmarking, which develops tests to assess whether intelligent systems accurately represent human values and act accordingly. We identify three major issues with current ME benchmarks: limited ecological validity due to unrealistic ethical dilemmas, unstructured question generation without clear inclusion/exclusion criteria, and a lack of scalability due to reliance on human annotations. Moreover, benchmarks often fail to include sufficient syntactic variations, reducing the robustness of findings. To address these gaps, we introduce two new ME benchmarks: the Triage Benchmark and the Medical Law (MedLaw) Benchmark, both featuring real-world ethical dilemmas from the medical domain. The MedLaw Benchmark, fully AI-generated, offers a scalable alternative. We also introduce context perturbations in our benchmarks to assess models' worst-case performance. Our findings reveal that ethics prompting does not always improve decision-making. Furthermore, context perturbations not only significantly reduce model performance but can also reverse error patterns and shift relative performance rankings. Lastly, our comparison of worst-case performance suggests that general model capability does not always predict strong ethical decision-making. We argue that ME benchmarks must approximate real-world scenarios and worst-case performance to ensure robust evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19753v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kira Sam, Raja Vavekanand</dc:creator>
    </item>
    <item>
      <title>Establishing Nationwide Power System Vulnerability Index across US Counties Using Interpretable Machine Learning</title>
      <link>https://arxiv.org/abs/2410.19754</link>
      <description>arXiv:2410.19754v2 Announce Type: new 
Abstract: Power outages have become increasingly frequent, intense, and prolonged in the US due to climate change, aging electrical grids, and rising energy demand. However, largely due to the absence of granular spatiotemporal outage data, we lack data-driven evidence and analytics-based metrics to quantify power system vulnerability. This limitation has hindered the ability to effectively evaluate and address vulnerability to power outages in US communities. Here, we collected ~179 million power outage records at 15-minute intervals across 3022 US contiguous counties (96.15% of the area) from 2014 to 2023. We developed a power system vulnerability assessment framework based on three dimensions (intensity, frequency, and duration) and applied interpretable machine learning models (XGBoost and SHAP) to compute Power System Vulnerability Index (PSVI) at the county level. Our analysis reveals a consistent increase in power system vulnerability over the past decade. We identified 318 counties across 45 states as hotspots for high power system vulnerability, particularly in the West Coast (California and Washington), the East Coast (Florida and the Northeast area), the Great Lakes megalopolis (Chicago-Detroit metropolitan areas), and the Gulf of Mexico (Texas). Heterogeneity analysis indicates that urban counties, counties with interconnected grids, and states with high solar generation exhibit significantly higher vulnerability. Our results highlight the significance of the proposed PSVI for evaluating the vulnerability of communities to power outages. The findings underscore the widespread and pervasive impact of power outages across the country and offer crucial insights to support infrastructure operators, policymakers, and emergency managers in formulating policies and programs aimed at enhancing the resilience of the US power infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19754v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junwei Ma, Bo Li, Olufemi A. Omitaomu, Ali Mostafavi</dc:creator>
    </item>
    <item>
      <title>Gender Bias of LLM in Economics: An Existentialism Perspective</title>
      <link>https://arxiv.org/abs/2410.19775</link>
      <description>arXiv:2410.19775v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as GPT-4 and BERT, have rapidly gained traction in natural language processing (NLP) and are now integral to financial decision-making. However, their deployment introduces critical challenges, particularly in perpetuating gender biases that can distort decision-making outcomes in high-stakes economic environments. This paper investigates gender bias in LLMs through both mathematical proofs and empirical experiments using the Word Embedding Association Test (WEAT), demonstrating that LLMs inherently reinforce gender stereotypes even without explicit gender markers. By comparing the decision-making processes of humans and LLMs, we reveal fundamental differences: while humans can override biases through ethical reasoning and individualized understanding, LLMs maintain bias as a rational outcome of their mathematical optimization on biased data. Our analysis proves that bias in LLMs is not an unintended flaw but a systematic result of their rational processing, which tends to preserve and amplify existing societal biases encoded in training data. Drawing on existentialist theory, we argue that LLM-generated bias reflects entrenched societal structures and highlights the limitations of purely technical debiasing methods. This research underscores the need for new theoretical frameworks and interdisciplinary methodologies that address the ethical implications of integrating LLMs into economic and financial decision-making. We advocate for a reconceptualization of how LLMs influence economic decisions, emphasizing the importance of incorporating human-like ethical considerations into AI governance to ensure fairness and equity in AI-driven financial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19775v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hui Zhong, Songsheng Chen, Mian Liang</dc:creator>
    </item>
    <item>
      <title>A Human-Centered Approach for Improving Supervised Learning</title>
      <link>https://arxiv.org/abs/2410.19778</link>
      <description>arXiv:2410.19778v1 Announce Type: new 
Abstract: Supervised Learning is a way of developing Artificial Intelligence systems in which a computer algorithm is trained on labeled data inputs. Effectiveness of a Supervised Learning algorithm is determined by its performance on a given dataset for a particular problem. In case of Supervised Learning problems, Stacking Ensembles usually perform better than individual classifiers due to their generalization ability. Stacking Ensembles combine predictions from multiple Machine Learning algorithms to make final predictions. Inspite of Stacking Ensembles superior performance, the overhead of Stacking Ensembles such as high cost, resources, time, and lack of explainability create challenges in real-life applications. This paper shows how we can strike a balance between performance, time, and resource constraints. Another goal of this research is to make Ensembles more explainable and intelligible using the Human-Centered approach. To achieve the aforementioned goals, we proposed a Human-Centered Behavior-inspired algorithm that streamlines the Ensemble Learning process while also reducing time, cost, and resource overhead, resulting in the superior performance of Supervised Learning in real-world applications. To demonstrate the effectiveness of our method, we perform our experiments on nine real-world datasets. Experimental results reveal that the proposed method satisfies our goals and outperforms the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19778v1</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubhi Bansal, Atharva Tendulkar, Nagendra Kumar</dc:creator>
    </item>
    <item>
      <title>Exploring Older Adults' Perceptions and Experiences with Online Dating</title>
      <link>https://arxiv.org/abs/2410.19783</link>
      <description>arXiv:2410.19783v1 Announce Type: new 
Abstract: The rise of online dating apps has transformed how individuals connect and seek companionship, with an increase in usage among older adults. While these platforms offer opportunities for emotional support and social connection, they also present significant challenges, including a concerning trend of online dating scams targeting this demographic. To address these issues, we conducted a semi-structured interview focused on the online dating experiences of older adults (65+). Initially, we conducted a pre-screening survey, followed by focused semi-structured interviews with 11 of the selected older adults. Through this study, we investigate older adults' security and privacy concerns, the significance of design elements and accessibility, and identify areas needing improvement. Our findings reveal challenges such as deceptive practices, including catfishing and fraud, concerns over disclosing sensitive information, non-inclusive app design features, and the need for more informative visualization of match requests. We offer recommendations for enhanced identity verification, inclusive privacy controls by app developers, and increased digital literacy efforts to enable older adults to navigate these platforms safely and confidently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19783v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Muskan Fatima, Naheem Noah, Sanchari Das</dc:creator>
    </item>
    <item>
      <title>Substance Beats Style: Why Beginning Students Fail to Code with LLMs</title>
      <link>https://arxiv.org/abs/2410.19792</link>
      <description>arXiv:2410.19792v1 Announce Type: new 
Abstract: Although LLMs are increasing the productivity of professional programmers, existing work shows that beginners struggle to prompt LLMs to solve text-to-code tasks. Why is this the case? This paper explores two competing hypotheses about the cause of student-LLM miscommunication: (1) students simply lack the technical vocabulary needed to write good prompts, and (2) students do not understand the extent of information that LLMs need to solve code generation tasks. We study (1) with a causal intervention experiment on technical vocabulary and (2) by analyzing graphs that abstract how students edit prompts and the different failures that they encounter. We find that substance beats style: a poor grasp of technical vocabulary is merely correlated with prompt failure; that the information content of prompts predicts success; that students get stuck making trivial edits; and more. Our findings have implications for the use of LLMs in programming education, and for efforts to make computing more accessible with LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19792v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesca Lucchetti, Zixuan Wu, Arjun Guha, Molly Q Feldman, Carolyn Jane Anderson</dc:creator>
    </item>
    <item>
      <title>First-Person Fairness in Chatbots</title>
      <link>https://arxiv.org/abs/2410.19803</link>
      <description>arXiv:2410.19803v1 Announce Type: new 
Abstract: Chatbots like ChatGPT are used for diverse purposes, ranging from resume writing to entertainment. These real-world applications are different from the institutional uses, such as resume screening or credit scoring, which have been the focus of much of AI research on fairness. Ensuring equitable treatment for all users in these first-person contexts is critical. In this work, we study "first-person fairness," which means fairness toward the chatbot user. This includes providing high-quality responses to all users regardless of their identity or background and avoiding harmful stereotypes.
  We propose a scalable, privacy-preserving method for evaluating one aspect of first-person fairness across a large, heterogeneous corpus of real-world chatbot interactions. Specifically, we assess potential bias linked to users' names, which can serve as proxies for demographic attributes like gender or race, in chatbot systems such as ChatGPT, which provide mechanisms for storing and using user names. Our method leverages a second language model to privately analyze name-sensitivity in the chatbot's responses. We verify the validity of these annotations through independent human evaluation. Further, we show that post-training interventions, including RL, significantly mitigate harmful stereotypes.
  Our approach also yields succinct descriptions of response differences across tasks. For instance, in the "writing a story" task, chatbot responses show a tendency to create protagonists whose gender matches the likely gender inferred from the user's name. Moreover, a pattern emerges where users with female-associated names receive responses with friendlier and simpler language slightly more often than users with male-associated names. Finally, we provide the system messages required for external researchers to further investigate ChatGPT's behavior with hypothetical user profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19803v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tyna Eloundou, Alex Beutel, David G. Robinson, Keren Gu-Lemberg, Anna-Luisa Brakman, Pamela Mishkin, Meghan Shah, Johannes Heidecke, Lilian Weng, Adam Tauman Kalai</dc:creator>
    </item>
    <item>
      <title>Learning to Adopt Generative AI</title>
      <link>https://arxiv.org/abs/2410.19806</link>
      <description>arXiv:2410.19806v1 Announce Type: new 
Abstract: ChatGPT, a large language model providing natural language responses, has become a powerful tool integrated into many people's daily routines. Despite its capabilities, the benefits it provides may not be equally distributed among individuals-a phenomenon referred to as the digital divide. Building upon prior literature, we propose two forms of digital divide in the generative AI adoption process: (i) the learning divide, capturing individuals' heterogeneous abilities to update their perceived utility of ChatGPT; and (ii) the utility divide, representing differences in individuals' actual utility gains per usage from ChatGPT. To evaluate these two divides, we develop a Bayesian learning model that incorporates demographic heterogeneities in both the utility and signal functions. Leveraging a six-month clickstream dataset, we estimate the model and find significant learning and utility divides across various demographic attributes. Surprisingly, lowereducated and non-white individuals derive higher utility gains from ChatGPT but learn about its utility at a slower rate. Furthermore, males, younger individuals, and those with an IT background not only derive higher utility per use from ChatGPT but also learn about its utility more rapidly. Besides, we document a phenomenon termed the belief trap, wherein users underestimate ChatGPT's utility, opt not to use the tool, and consequently lack new experiences to update their perceptions, leading to continued underutilization. We further demonstrate that the learning divide can significantly affect the probability of falling into the belief trap, another form of the digital divide in adoption outcomes (i.e., outcome divide); however, offering training programs can alleviate the belief trap and mitigate the divide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19806v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lijia Ma, Xingchen Xu, Yumei He, Yong Tan</dc:creator>
    </item>
    <item>
      <title>Ethics Whitepaper: Whitepaper on Ethical Research into Large Language Models</title>
      <link>https://arxiv.org/abs/2410.19812</link>
      <description>arXiv:2410.19812v1 Announce Type: new 
Abstract: This whitepaper offers an overview of the ethical considerations surrounding research into or with large language models (LLMs). As LLMs become more integrated into widely used applications, their societal impact increases, bringing important ethical questions to the forefront. With a growing body of work examining the ethical development, deployment, and use of LLMs, this whitepaper provides a comprehensive and practical guide to best practices, designed to help those in research and in industry to uphold the highest ethical standards in their work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19812v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eddie L. Ungless, Nikolas Vitsakis, Zeerak Talat, James Garforth, Bj\"orn Ross, Arno Onken, Atoosa Kasirzadeh, Alexandra Birch</dc:creator>
    </item>
    <item>
      <title>Human-Centric eXplainable AI in Education</title>
      <link>https://arxiv.org/abs/2410.19822</link>
      <description>arXiv:2410.19822v1 Announce Type: new 
Abstract: As artificial intelligence (AI) becomes more integrated into educational environments, how can we ensure that these systems are both understandable and trustworthy? The growing demand for explainability in AI systems is a critical area of focus. This paper explores Human-Centric eXplainable AI (HCXAI) in the educational landscape, emphasizing its role in enhancing learning outcomes, fostering trust among users, and ensuring transparency in AI-driven tools, particularly through the innovative use of large language models (LLMs). What challenges arise in the implementation of explainable AI in educational contexts? This paper analyzes these challenges, addressing the complexities of AI models and the diverse needs of users. It outlines comprehensive frameworks for developing HCXAI systems that prioritize user understanding and engagement, ensuring that educators and students can effectively interact with these technologies. Furthermore, what steps can educators, developers, and policymakers take to create more effective, inclusive, and ethically responsible AI solutions in education? The paper provides targeted recommendations to address this question, highlighting the necessity of prioritizing explainability. By doing so, how can we leverage AI's transformative potential to foster equitable and engaging educational experiences that support diverse learners?</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19822v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhankar Maity, Aniket Deroy</dc:creator>
    </item>
    <item>
      <title>Evaluating Progress in Web3 Grants: Introducing the Grant Maturity Index</title>
      <link>https://arxiv.org/abs/2410.19828</link>
      <description>arXiv:2410.19828v1 Announce Type: new 
Abstract: This report introduces the Grant Maturity Index (GMI), a novel evaluative framework designed to assess the maturity and operational effectiveness of Web3 grant programs. As Web3 continues to develop, the decentralized nature of these programs brings both opportunities and challenges, particularly when it comes to governance, transparency, and community engagement. Traditional funding models are often governed by standardized processes, but Web3 grants lack such consistency, making it difficult for grant operators to measure the long-term success of their programs.The Grant Maturity Index (GMI) was created through exploratory applied research to address this gap. Inspired by the World Bank's GovTech Maturity Index (GTMI), the GMI is tailored specifically for the decentralized Web3 ecosystem. The GMI evaluates key dimensions of grant programs governance, transparency, operational efficiency, and community engagement, providing grant operators with a clear benchmark for assessing and improving their programs. The primary objectives of this research are to, first, identify the structural indicators that adequately describe Web3 grant programs. Second, to describe optimal outcomes for programs by evaluating their maturity across key operational areas. The GMI is applied to four major Ethereum Layer 2 grant programs, namely Arbitrum, Mantle, Taiko Labs, and Optimism. These case studies highlight areas where Web3 grant programs require improvement, particularly in standardizing processes, enhancing transparency, and increasing community participation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19828v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ben Biedermann, Fahima Gibrel</dc:creator>
    </item>
    <item>
      <title>Federated Anomaly Detection for Early-Stage Diagnosis of Autism Spectrum Disorders using Serious Game Data</title>
      <link>https://arxiv.org/abs/2410.20003</link>
      <description>arXiv:2410.20003v1 Announce Type: new 
Abstract: Early identification of Autism Spectrum Disorder (ASD) is considered critical for effective intervention to mitigate emotional, financial and societal burdens. Although ASD belongs to a group of neurodevelopmental disabilities that are not curable, researchers agree that targeted interventions during childhood can drastically improve the overall well-being of individuals. However, conventional ASD detection methods such as screening tests, are often costly and time-consuming. This study presents a novel semi-supervised approach for ASD detection using AutoEncoder-based Machine Learning (ML) methods due to the challenge of obtaining ground truth labels for the associated task. Our approach utilizes data collected manually through a serious game specifically designed for this purpose. Since the sensitive data collected by the gamified application are susceptible to privacy leakage, we developed a Federated Learning (FL) framework that can enhance user privacy without compromising the overall performance of the ML models. The framework is further enhanced with Fully Homomorphic Encryption (FHE) during model aggregation to minimize the possibility of inference attacks and client selection mechanisms as well as state-of-the-art aggregators to improve the model's predictive accuracy. Our results demonstrate that semi-supervised FL can effectively predict an ASD risk indicator for each case while simultaneously addressing privacy concerns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20003v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolaos Pavlidis, Vasileios Perifanis, Eleni Briola, Christos-Chrysanthos Nikolaidis, Eleftheria Katsiri, Pavlos S. Efraimidis, Despina Elisabeth Filippidou</dc:creator>
    </item>
    <item>
      <title>Effective Data Stewardship in Higher Education: Skills, Competences, and the Emerging Role of Open Data Stewards</title>
      <link>https://arxiv.org/abs/2410.20361</link>
      <description>arXiv:2410.20361v1 Announce Type: new 
Abstract: The significance of open data in higher education stems from the changing tendencies towards open science, and open research in higher education encourages new ways of making scientific inquiry more transparent, collaborative and accessible. This study focuses on the critical role of open data stewards in this transition, essential for managing and disseminating research data effectively in universities, while it also highlights the increasing demand for structured training and professional policies for data stewards in academic settings. Building upon this context, the paper investigates the essential skills and competences required for effective data stewardship in higher education institutions by elaborating on a critical literature review, coupled with practical engagement in open data stewardship at universities, provided insights into the roles and responsibilities of data stewards. In response to these identified needs, the paper proposes a structured training framework and comprehensive curriculum for data stewardship, a direct response to the gaps identified in the literature. It addresses five key competence categories for open data stewards, aligning them with current trends and essential skills and knowledge in the field. By advocating for a structured approach to data stewardship education, this work sets the foundation for improved data management in universities and serves as a critical step towards professionalizing the role of data stewards in higher education. The emphasis on the role of open data stewards is expected to advance data accessibility and sharing practices, fostering increased transparency, collaboration, and innovation in academic research. This approach contributes to the evolution of universities into open ecosystems, where there is free flow of data for global education and research advancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20361v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panos Fitsilis, Vyron Damasiotis, Charalampos Dervenis, Vasileios Kyriatzis, Paraskevi Tsoutsa</dc:creator>
    </item>
    <item>
      <title>Smart Transport Infrastructure Maintenance: A Smart-Contract Blockchain Approach</title>
      <link>https://arxiv.org/abs/2410.20431</link>
      <description>arXiv:2410.20431v1 Announce Type: new 
Abstract: Infrastructure maintenance is inherently complex, especially for widely dispersed transport systems like roads and railroads. Maintaining this infrastructure involves multiple partners working together to ensure safe, efficient upkeep that meets technical and safety standards, with timely materials and budget adherence. Traditionally, these requirements are managed on paper, with each contract step checked manually. Smart contracts, based on blockchain distributed ledger technology, offer a new approach. Distributed ledgers facilitate secure, transparent transactions, enabling decentralized agreements where contract terms automatically execute when conditions are met. Beyond financial transactions, blockchains can track complex agreements, recording each stage of contract fulfillment between multiple parties. A smart contract is a set of coded rules stored on the blockchain that automatically executes each term upon meeting specified conditions. In infrastructure maintenance, this enables end-to-end automation-from contractor assignment to maintenance completion. Using an immutable, decentralized record, contract terms and statuses are transparent to all parties, enhancing trust and efficiency. Creating smart contracts for infrastructure requires a comprehensive understanding of procedural workflows to foresee all requirements and liabilities. This workflow includes continuous infrastructure monitoring through a dynamic, data-driven maintenance model that triggers necessary actions. Modern process mining can develop a resilient Maintenance Process Model, helping Operations Management to define contract terms, including asset allocation, logistics, materials, and skill requirements. Automation and reliable data quality across the procedural chain are essential, supported by IoT sensors, big data analytics, predictive maintenance, intelligent logistics, and asset management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20431v1</guid>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatjon Seraj</dc:creator>
    </item>
    <item>
      <title>Towards a Blockchain and Opportunistic Edge Driven Metaverse of Everything</title>
      <link>https://arxiv.org/abs/2410.20594</link>
      <description>arXiv:2410.20594v1 Announce Type: new 
Abstract: Decentralized Metaverses, built on Web 3.0 and Web 4.0 technologies, have attracted significant attention across various fields. This innovation leverages blockchain, Decentralized Autonomous Organizations (DAOs), Extended Reality (XR) and advanced technologies to create immersive and interconnected digital environments that mirror the real world. This article delves into the Metaverse of Everything (MoE), a platform that fuses the Metaverse concept with the Internet of Everything (IoE), an advanced version of the Internet of Things (IoT) that connects not only physical devices but also people, data and processes within a networked environment. Thus, the MoE integrates generated data and virtual entities, creating an extensive network of interconnected components. This article seeks to advance current MoE, examining decentralization and the application of Opportunistic Edge Computing (OEC) for interactions with surrounding IoT devices and IoE entities. Moreover, it outlines the main challenges to guide researchers and businesses towards building a future cyber-resilient opportunistic MoE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20594v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paula Fraga-Lamas, S\'ergio Ivan Lopes, Tiago M. Fern\'andez-Caram\'es</dc:creator>
    </item>
    <item>
      <title>Convergences and Divergences in the 2024 Judicial Reform in Mexico: A Neural Network Analysis of Transparency, Judicial Autonomy, and Public Acceptance</title>
      <link>https://arxiv.org/abs/2410.20676</link>
      <description>arXiv:2410.20676v1 Announce Type: new 
Abstract: This study utilizes neural networks to evaluate the 2024 judicial reform in Mexico, a proposal designed to overhaul the judicial system by increasing transparency, judicial autonomy, and introducing the popular election of judges. The neural network model analyzes both converging and diverging factors that influence the reforms viability and public acceptance. Key areas of convergence include enhanced transparency and judicial autonomy, which are seen as improvements to the system. However, major points of divergence, such as the high costs of implementation and concerns about the legitimacy of electing judges, pose significant challenges. By integrating variables like transparency, decision quality, judicial independence, and implementation costs, the model predicts levels of public and professional acceptance of the reform. The neural networks multilayered structure allows for the modeling of complex relationships, offering predictive insights into how the reform may impact the Mexican judicial system. Initial findings suggest that while the reform could strengthen judicial autonomy, the risks of politicizing the judiciary and the financial burden it entails may reduce its overall acceptance. This research highlights the importance of using advanced AI tools to simulate public policy outcomes, providing valuable data to guide lawmakers in refining their proposals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20676v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.12376.71685</arxiv:DOI>
      <dc:creator>Carlos Medel-Ram\'irez</dc:creator>
    </item>
    <item>
      <title>Let a million entrepreneurs grow!</title>
      <link>https://arxiv.org/abs/2410.20709</link>
      <description>arXiv:2410.20709v1 Announce Type: new 
Abstract: India produces about nine hundred thousand (900K) engineers annually, and many seek computer science and related technology jobs. Given that the IT workforce in India is still young, new graduates get jobs only when the industry grows. A liberal estimate based on the data from MeitY (Ministry of Electronics and Information Technology) and NASSCOM puts the annual job growth to three hundred thousand (300K), less than one-third of the graduation rate. In other words, about half a million graduates don't get a job every year (even when we consider that some students don't opt for jobs or go for higher studies).
  This position paper demonstrates that given the current growth rate of the Indian economy, such a significant shortfall will continue to exist. It then proposes a way to address this shortfall.
  The paper proposes to develop micro-entrepreneurs at scale, enabling many graduates to start micro-enterprises focused on AI, Software, and Technology (MAST). These MAST enterprises offer technology products and services to meet the hyperlocal needs of the businesses and individuals in the local community (a retailer in the neighborhood, a high net-worth person, or a factory).
  Such an endeavor will require curricular, policy, and societal interventions. The paper presents an approach to enable MAST education across campuses, outlining the key curricular changes required and important policies that must be created and implemented.
  This supply-demand gap is an existential problem for engineering education in India, and this position paper aims to trigger debates and collaborations to devise solutions that will work at India scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20709v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mrityunjay Kumar</dc:creator>
    </item>
    <item>
      <title>Lecture II: Communicative Justice and the Distribution of Attention</title>
      <link>https://arxiv.org/abs/2410.20718</link>
      <description>arXiv:2410.20718v1 Announce Type: new 
Abstract: Algorithmic intermediaries govern the digital public sphere through their architectures, amplification algorithms, and moderation practices. In doing so, they shape public communication and distribute attention in ways that were previously infeasible with such subtlety, speed and scale. From misinformation and affective polarisation to hate speech and radicalisation, the many pathologies of the digital public sphere attest that they could do so better. But what ideals should they aim at? Political philosophy should be able to help, but existing theories typically assume that a healthy public sphere will spontaneously emerge if only we get the boundaries of free expression right. They offer little guidance on how to intentionally constitute the digital public sphere. In addition to these theories focused on expression, we need a further theory of communicative justice, targeted specifically at the algorithmic intermediaries that shape communication and distribute attention. This lecture argues that political philosophy urgently owes an account of how to govern communication in the digital public sphere, and introduces and defends a democratic egalitarian theory of communicative justice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20718v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seth Lazar</dc:creator>
    </item>
    <item>
      <title>Lecture I: Governing the Algorithmic City</title>
      <link>https://arxiv.org/abs/2410.20720</link>
      <description>arXiv:2410.20720v1 Announce Type: new 
Abstract: A century ago, John Dewey observed that '[s]team and electricity have done more to alter the conditions under which men associate together than all the agencies which affected human relationships before our time'. In the last few decades, computing technologies have had a similar effect. Political philosophy's central task is to help us decide how to live together, by analysing our social relations, diagnosing their failings, and articulating ideals to guide their revision. But these profound social changes have left scarcely a dent in the model of social relations that (analytical) political philosophers assume. This essay aims to reverse that trend. It first builds a model of our novel social relations as they are now, and as they are likely to evolved, and then explores how those differences affect our theories of how to live together. I introduce the 'Algorithmic City', the network of algorithmically-mediated social relations, then characterise the intermediary power by which it is governed. I show how algorithmic governance raises new challenges for political philosophy concerning the justification of authority, the foundations of procedural legitimacy, and the possibility of justificatory neutrality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20720v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seth Lazar</dc:creator>
    </item>
    <item>
      <title>Co-produced decentralised surveys as a trustworthy vector to put employees' well-being at the core of companies' performance</title>
      <link>https://arxiv.org/abs/2410.20919</link>
      <description>arXiv:2410.20919v1 Announce Type: new 
Abstract: Assessing employees' well-being has become central to fostering an environment where employees can thrive and contribute to companies' adaptability and competitiveness in the market. Traditional methods for assessing well-being often face significant challenges, with a major issue being the lack of trust and confidence employees may have in these processes. Employees may hesitate to provide honest feedback due to concerns not only about data integrity and confidentiality, but also about power imbalances among stakeholders. In this context, blockchain-based decentralised surveys, leveraging the immutability, transparency, and pseudo-anonymity of blockchain technology, offer significant improvements in aligning responsive actions with employees' feedback securely and transparently. Nevertheless, their implementation raises complex issues regarding the balance between trust and confidence. While blockchain can function as a confidence machine for data processing and management, it does not inherently address the equally important cultural element of trust. To effectively integrate blockchain technology into well-being assessments, decentralised well-being surveys must be supported by cultural practices that build and sustain trust. Drawing on blockchain technology management and relational cultural theory, we explain how trust-building can be achieved through the co-production of decentralised well-being surveys, which helps address power imbalances between the implementation team and stakeholders. Our goal is to provide a dual cultural-technological framework along with conceptual clarity on how the technological implementation of confidence can connect with the cultural development of trust, ensuring that blockchain-based decentralised well-being surveys are not only secure and reliable but also perceived as trustworthy vector to improve workplace conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20919v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ad\`ele Br\'eart De Boisanger, Wendy Sims-Schouten, Francois Sicard</dc:creator>
    </item>
    <item>
      <title>Colorimetric skin tone scale for improved accuracy and reduced perceptual bias of human skin tone annotations</title>
      <link>https://arxiv.org/abs/2410.21005</link>
      <description>arXiv:2410.21005v1 Announce Type: new 
Abstract: Human image datasets used to develop and evaluate technology should represent the diversity of human phenotypes, including skin tone. Datasets that include skin tone information frequently rely on manual skin tone ratings based on the Fitzpatrick Skin Type (FST) or the Monk Skin Tone (MST) scales in lieu of the actual measured skin tone of the image dataset subjects. However, perceived skin tone is subject to known biases and skin tone appearance in digital images can vary substantially depending on the capture camera and environment, confounding manual ratings. Surprisingly, the relationship between skin-tone ratings and measured skin tone has not been explored. To close this research gap, we measured the relationship between skin tone ratings from existing scales (FST, MST) and skin tone values measured by a calibrated colorimeter. We also propose and assess a novel Colorimetric Skin Tone (CST) scale developed based on prior colorimetric measurements. Using experiments requiring humans to rate their own skin tone and the skin tone of subjects in images, we show that the new CST scale is more sensitive, consistent, and colorimetrically accurate. While skin tone ratings appeared to correct for some color variation across images, they introduced biases related to race and other factors. These biases must be considered before using manual skin-tone ratings in technology evaluations or for engineering decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21005v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Cynthia M. Cook, John J. Howard, Laura R. Rabbitt, Isabelle M. Shuggi, Yevgeniy B. Sirotin, Jerry L. Tipton, Arun R. Vemury</dc:creator>
    </item>
    <item>
      <title>A New Perspective to Boost Performance Fairness for Medical Federated Learning</title>
      <link>https://arxiv.org/abs/2410.19765</link>
      <description>arXiv:2410.19765v1 Announce Type: cross 
Abstract: Improving the fairness of federated learning (FL) benefits healthy and sustainable collaboration, especially for medical applications. However, existing fair FL methods ignore the specific characteristics of medical FL applications, i.e., domain shift among the datasets from different hospitals. In this work, we propose Fed-LWR to improve performance fairness from the perspective of feature shift, a key issue influencing the performance of medical FL systems caused by domain shift. Specifically, we dynamically perceive the bias of the global model across all hospitals by estimating the layer-wise difference in feature representations between local and global models. To minimize global divergence, we assign higher weights to hospitals with larger differences. The estimated client weights help us to re-aggregate the local models per layer to obtain a fairer global model. We evaluate our method on two widely used federated medical image segmentation benchmarks. The results demonstrate that our method achieves better and fairer performance compared with several state-of-the-art fair FL methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19765v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>eess.IV</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-72117-5_2</arxiv:DOI>
      <arxiv:journal_reference>International Conference on Medical Image Computing and Computer-Assisted Intervention 2024</arxiv:journal_reference>
      <dc:creator>Yunlu Yan, Lei Zhu, Yuexiang Li, Xinxing Xu, Rick Siow Mong Goh, Yong Liu, Salman Khan, Chun-Mei Feng</dc:creator>
    </item>
    <item>
      <title>Enhancing Safety in Reinforcement Learning with Human Feedback via Rectified Policy Optimization</title>
      <link>https://arxiv.org/abs/2410.19933</link>
      <description>arXiv:2410.19933v1 Announce Type: cross 
Abstract: Balancing helpfulness and safety (harmlessness) is a critical challenge in aligning large language models (LLMs). Current approaches often decouple these two objectives, training separate preference models for helpfulness and safety, while framing safety as a constraint within a constrained Markov Decision Process (CMDP) framework. However, these methods can lead to ``safety interference'', where average-based safety constraints compromise the safety of some prompts in favor of others. To address this issue, we propose \textbf{Rectified Policy Optimization (RePO)}, which replaces the average safety constraint with stricter (per prompt) safety constraints. At the core of RePO is a policy update mechanism driven by rectified policy gradients, which penalizes the strict safety violation of every prompt, thereby enhancing safety across nearly all prompts. Our experiments on Alpaca-7B demonstrate that RePO improves the safety alignment and reduces the safety interference compared to baseline methods. Code is available at https://github.com/pxyWaterMoon/RePO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19933v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiyue Peng, Hengquan Guo, Jiawei Zhang, Dongqing Zou, Ziyu Shao, Honghao Wei, Xin Liu</dc:creator>
    </item>
    <item>
      <title>Roles of LLMs in the Overall Mental Architecture</title>
      <link>https://arxiv.org/abs/2410.20037</link>
      <description>arXiv:2410.20037v1 Announce Type: cross 
Abstract: To better understand existing LLMs, we may examine the human mental (cognitive/psychological) architecture, and its components and structures. Based on psychological, philosophical, and cognitive science literatures, it is argued that, within the human mental architecture, existing LLMs correspond well with implicit mental processes (intuition, instinct, and so on). However, beyond such implicit processes, explicit processes (with better symbolic capabilities) are also present within the human mental architecture, judging from psychological, philosophical, and cognitive science literatures. Various theoretical and empirical issues and questions in this regard are explored. Furthermore, it is argued that existing dual-process computational cognitive architectures (models of the human cognitive/psychological architecture) provide usable frameworks for fundamentally enhancing LLMs by introducing dual processes (both implicit and explicit) and, in the meantime, can also be enhanced by LLMs. The results are synergistic combinations (in several different senses simultaneously).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20037v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ron Sun</dc:creator>
    </item>
    <item>
      <title>hateUS -- Analysis, impact of Social media use and Hate speech over University Student platforms: Case study, Problems, and Solutions</title>
      <link>https://arxiv.org/abs/2410.20070</link>
      <description>arXiv:2410.20070v1 Announce Type: cross 
Abstract: The use of social media applications, hate speech engagement, and public debates among teenagers, primarily by university and college students, is growing day by day. The feelings of tremendous stress, anxiety, and depression via social media among our youths have a direct impact on their daily lives and personal workspace apart from delayed sleep, social media addictions, and memory loss. The use of NO phone times and NO phone zones is now popular in workplaces and family cultures. The use of hate speech, negotiations, and toxic words can lead to verbal abuse and cybercrime. Growing concern of mobile device security, cyberbullying, ransomware attacks, and mental health issues are another serious impact of social media among university students. The future challenges including health issues of social media use and hate speech has a serious impact on livelihood, freedom, and diverse communities of university students. Our case study is related to social media use and hate speech related to public debates over university students. We have presented the analysis and impact of social media and hate speech with several conclusions, cybercrimes, and components. The use of questionnaires for collecting primary data over university students help in the analysis of case study. The conclusion of case study and future scope of the research is extremely important to counter negative impacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20070v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naresh Kshetri, Will Carter, Seth Kern, Richard Mensah, Bishwo Prakash Pokharel</dc:creator>
    </item>
    <item>
      <title>"My Replika Cheated on Me and She Liked It": A Taxonomy of Algorithmic Harms in Human-AI Relationships</title>
      <link>https://arxiv.org/abs/2410.20130</link>
      <description>arXiv:2410.20130v1 Announce Type: cross 
Abstract: As conversational AI systems increasingly permeate the socio-emotional realms of human life, they bring both benefits and risks to individuals and society. Despite extensive research on detecting and categorizing harms in AI systems, less is known about the harms that arise from social interactions with AI chatbots. Through a mixed-methods analysis of 35,390 conversation excerpts shared on r/replika, an online community for users of the AI companion Replika, we identified six categories of harmful behaviors exhibited by the chatbot: relational transgression, verbal abuse and hate, self-inflicted harm, harassment and violence, mis/disinformation, and privacy violations. The AI contributes to these harms through four distinct roles: perpetrator, instigator, facilitator, and enabler. Our findings highlight the relational harms of AI chatbots and the danger of algorithmic compliance, enhancing the understanding of AI harms in socio-emotional interactions. We also provide suggestions for designing ethical and responsible AI systems that prioritize user safety and well-being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20130v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renwen Zhang, Han Li, Han Meng, Jinyuan Zhan, Hongyuan Gan, Yi-Chieh Lee</dc:creator>
    </item>
    <item>
      <title>Generative AI in Health Economics and Outcomes Research: A Taxonomy of Key Definitions and Emerging Applications, an ISPOR Working Group Report</title>
      <link>https://arxiv.org/abs/2410.20204</link>
      <description>arXiv:2410.20204v1 Announce Type: cross 
Abstract: Objective: This article offers a taxonomy of generative artificial intelligence (AI) for health economics and outcomes research (HEOR), explores its emerging applications, and outlines methods to enhance the accuracy and reliability of AI-generated outputs. Methods: The review defines foundational generative AI concepts and highlights current HEOR applications, including systematic literature reviews, health economic modeling, real-world evidence generation, and dossier development. Approaches such as prompt engineering (zero-shot, few-shot, chain-of-thought, persona pattern prompting), retrieval-augmented generation, model fine-tuning, and the use of domain-specific models are introduced to improve AI accuracy and reliability. Results: Generative AI shows significant potential in HEOR, enhancing efficiency, productivity, and offering novel solutions to complex challenges. Foundation models are promising in automating complex tasks, though challenges remain in scientific reliability, bias, interpretability, and workflow integration. The article discusses strategies to improve the accuracy of these AI tools. Conclusion: Generative AI could transform HEOR by increasing efficiency and accuracy across various applications. However, its full potential can only be realized by building HEOR expertise and addressing the limitations of current AI technologies. As AI evolves, ongoing research and innovation will shape its future role in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20204v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachael Fleurence, Xiaoyan Wang, Jiang Bian, Mitchell K. Higashi, Turgay Ayer, Hua Xu, Dalia Dawoud, Jagpreet Chhatwal</dc:creator>
    </item>
    <item>
      <title>You Never Know: Quantization Induces Inconsistent Biases in Vision-Language Foundation Models</title>
      <link>https://arxiv.org/abs/2410.20265</link>
      <description>arXiv:2410.20265v1 Announce Type: cross 
Abstract: We study the impact of a standard practice in compressing foundation vision-language models - quantization - on the models' ability to produce socially-fair outputs. In contrast to prior findings with unimodal models that compression consistently amplifies social biases, our extensive evaluation of four quantization settings across three datasets and three CLIP variants yields a surprising result: while individual models demonstrate bias, we find no consistent change in bias magnitude or direction across a population of compressed models due to quantization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20265v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Slyman, Anirudh Kanneganti, Sanghyun Hong, Stefan Lee</dc:creator>
    </item>
    <item>
      <title>AI-Driven Cyber Threat Intelligence Automation</title>
      <link>https://arxiv.org/abs/2410.20287</link>
      <description>arXiv:2410.20287v1 Announce Type: cross 
Abstract: This study introduces an innovative approach to automating Cyber Threat Intelligence (CTI) processes in industrial environments by leveraging Microsoft's AI-powered security technologies. Historically, CTI has heavily relied on manual methods for collecting, analyzing, and interpreting data from various sources such as threat feeds. This study introduces an innovative approach to automating CTI processes in industrial environments by leveraging Microsoft's AI-powered security technologies. Historically, CTI has heavily relied on manual methods for collecting, analyzing, and interpreting data from various sources such as threat feeds, security logs, and dark web forums -- a process prone to inefficiencies, especially when rapid information dissemination is critical. By employing the capabilities of GPT-4o and advanced one-shot fine-tuning techniques for large language models, our research delivers a novel CTI automation solution. The outcome of the proposed architecture is a reduction in manual effort while maintaining precision in generating final CTI reports. This research highlights the transformative potential of AI-driven technologies to enhance both the speed and accuracy of CTI and reduce expert demands, offering a vital advantage in today's dynamic threat landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20287v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shrit Shah, Fatemeh Khoda Parast</dc:creator>
    </item>
    <item>
      <title>Malinowski in the Age of AI: Can large language models create a text game based on an anthropological classic?</title>
      <link>https://arxiv.org/abs/2410.20536</link>
      <description>arXiv:2410.20536v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) like ChatGPT and GPT-4 have shown remarkable abilities in a wide range of tasks such as summarizing texts and assisting in coding. Scientific research has demonstrated that these models can also play text-adventure games. This study aims to explore whether LLMs can autonomously create text-based games based on anthropological classics, evaluating also their effectiveness in communicating knowledge. To achieve this, the study engaged anthropologists in discussions to gather their expectations and design inputs for an anthropologically themed game. Through iterative processes following the established HCI principle of 'design thinking', the prompts and the conceptual framework for crafting these games were refined. Leveraging GPT3.5, the study created three prototypes of games centered around the seminal anthropological work of the social anthropologist's Bronislaw Malinowski's "Argonauts of the Western Pacific" (1922). Subsequently, evaluations were conducted by inviting senior anthropologists to playtest these games, and based on their inputs, the game designs were refined. The tests revealed promising outcomes but also highlighted key challenges: the models encountered difficulties in providing in-depth thematic understandings, showed suspectibility to misinformation, tended towards monotonic responses after an extended period of play, and struggled to offer detailed biographical information. Despite these limitations, the study's findings open up new research avenues at the crossroads of artificial intelligence, machine learning, LLMs, ethnography, anthropology and human-computer interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20536v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Peter Hoffmann, Jan Fillies, Adrian Paschke</dc:creator>
    </item>
    <item>
      <title>Advancing Towards Green Blockchain: A Practical Energy-Efficient Blockchain Based Application for CV Verification</title>
      <link>https://arxiv.org/abs/2410.20605</link>
      <description>arXiv:2410.20605v1 Announce Type: cross 
Abstract: Blockchain has been widely criticized due to the use of inefficient consensus protocols and energy-intensive mechanisms that derived into a global enormous power consumption. Fortunately, since the first blockchain was conceived in 2008 (the one that supports Bitcoin), hardware and consensus protocols have evolved, decreasing energy consumption significantly. This article describes a green blockchain solution and quantifies energy savings when deploying the system on traditional computers and embedded Single-Board Computers (SBCs). To illustrate such savings, it is proposed a solution for tackling the problem of academic certificate forgery, which has a significant cost to society, since it harms the trustworthiness of certificates and academic institutions. The proposed solution is aimed at recording and verifying academic records (ARs) through a decentralized application (DApp) that is supported by a smart contract deployed in the Ethereum blockchain. The application stores the raw data (i.e., the data that are not managed by the blockchain) on a decentralized storage system based on Inter-Planetary File System (IPFS). To demonstrate the efficiency of the developed solution, it is evaluated in terms of performance (transaction latency and throughput) and efficiency (CPU usage and energy consumption), comparing the results obtained with a traditional Proof-of-Work (PoW) consensus protocol and the new Proof-of-Authority (PoA) protocol. The results shown in this paper indicate that the latter is clearly greener and demands less CPU load. Moreover, this article compares the performance of a traditional computer and two SBCs (a Raspberry Pi 4 and an Orange Pi One), showing that is possible to make use of the latter low-power devices to implement blockchain nodes for proposed DApp, but at the cost of higher response latency that varies greatly depending on the used SBCs [...]</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20605v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Fern\'andez-Blanco, Iv\'an Froiz-M\'iguez, Paula Fraga-Lamas, Tiago M. Fern\'andez-Caram\'es</dc:creator>
    </item>
    <item>
      <title>Evolving interdisciplinary contributions to global societal challenges: A 50-year overview</title>
      <link>https://arxiv.org/abs/2410.20619</link>
      <description>arXiv:2410.20619v1 Announce Type: cross 
Abstract: Addressing global societal challenges necessitates insights and expertise that transcend the boundaries of individual disciplines. In recent decades, interdisciplinary collaboration has been recognised as a vital driver of innovation and effective problem-solving, with the potential to profoundly influence policy and practice worldwide. However, quantitative evidence remains limited regarding how cross-disciplinary efforts contribute to societal challenges, as well as the evolving roles and relevance of specific disciplines in addressing these issues. To fill this gap, this study examines the long-term evolution of interdisciplinary contributions to the United Nations' Sustainable Development Goals (SDGs), drawing on extensive bibliometric data from OpenAlex. By analysing publication and citation trends across 19 research fields from 1970 to 2022, we reveal how the relative presence of different disciplines in addressing particular SDGs has shifted over time. Our results also provide unique evidence of the increasing interconnection between fields since the 2000s, coinciding with the United Nations' initiative to tackle global societal challenges through interdisciplinary efforts. These insights will benefit policymakers and practitioners as they reflect on past progress and plan for future action, particularly with the SDG target deadline approaching in the next five years.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20619v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keisuke Okamura</dc:creator>
    </item>
    <item>
      <title>ElectionSim: Massive Population Election Simulation Powered by Large Language Model Driven Agents</title>
      <link>https://arxiv.org/abs/2410.20746</link>
      <description>arXiv:2410.20746v1 Announce Type: cross 
Abstract: The massive population election simulation aims to model the preferences of specific groups in particular election scenarios. It has garnered significant attention for its potential to forecast real-world social trends. Traditional agent-based modeling (ABM) methods are constrained by their ability to incorporate complex individual background information and provide interactive prediction results. In this paper, we introduce ElectionSim, an innovative election simulation framework based on large language models, designed to support accurate voter simulations and customized distributions, together with an interactive platform to dialogue with simulated voters. We present a million-level voter pool sampled from social media platforms to support accurate individual simulation. We also introduce PPE, a poll-based presidential election benchmark to assess the performance of our framework under the U.S. presidential election scenario. Through extensive experiments and analyses, we demonstrate the effectiveness and robustness of our framework in U.S. presidential election simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20746v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinnong Zhang, Jiayu Lin, Libo Sun, Weihong Qi, Yihang Yang, Yue Chen, Hanjia Lyu, Xinyi Mou, Siming Chen, Jiebo Luo, Xuanjing Huang, Shiping Tang, Zhongyu Wei</dc:creator>
    </item>
    <item>
      <title>Generative Example-Based Explanations: Bridging the Gap between Generative Modeling and Explainability</title>
      <link>https://arxiv.org/abs/2410.20890</link>
      <description>arXiv:2410.20890v1 Announce Type: cross 
Abstract: Recently, several methods have leveraged deep generative modeling to produce example-based explanations of decision algorithms for high-dimensional input data. Despite promising results, a disconnect exists between these methods and the classical explainability literature, which focuses on lower-dimensional data with semantically meaningful features. This conceptual and communication gap leads to misunderstandings and misalignments in goals and expectations. In this paper, we bridge this gap by proposing a novel probabilistic framework for local example-based explanations. Our framework integrates the critical characteristics of classical local explanation desiderata while being amenable to high-dimensional data and their modeling through deep generative models. Our aim is to facilitate communication, foster rigor and transparency, and improve the quality of peer discussion and research progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20890v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Vaeth, Alexander M. Fruehwald, Benjamin Paassen, Magda Gregorova</dc:creator>
    </item>
    <item>
      <title>Fast Calibrated Explanations: Efficient and Uncertainty-Aware Explanations for Machine Learning Models</title>
      <link>https://arxiv.org/abs/2410.21129</link>
      <description>arXiv:2410.21129v1 Announce Type: cross 
Abstract: This paper introduces Fast Calibrated Explanations, a method designed for generating rapid, uncertainty-aware explanations for machine learning models. By incorporating perturbation techniques from ConformaSight - a global explanation framework - into the core elements of Calibrated Explanations (CE), we achieve significant speedups. These core elements include local feature importance with calibrated predictions, both of which retain uncertainty quantification. While the new method sacrifices a small degree of detail, it excels in computational efficiency, making it ideal for high-stakes, real-time applications. Fast Calibrated Explanations are applicable to probabilistic explanations in classification and thresholded regression tasks, where they provide the likelihood of a target being above or below a user-defined threshold. This approach maintains the versatility of CE for both classification and probabilistic regression, making it suitable for a range of predictive tasks where uncertainty quantification is crucial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21129v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tuwe L\"ofstr\"om, Fatima Rabia Yapicioglu, Alessandra Stramiglio, Helena L\"ofstr\"om, Fabio Vitali</dc:creator>
    </item>
    <item>
      <title>Belief in the Machine: Investigating Epistemological Blind Spots of Language Models</title>
      <link>https://arxiv.org/abs/2410.21195</link>
      <description>arXiv:2410.21195v1 Announce Type: cross 
Abstract: As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to significant consequences in areas such as medical diagnosis, legal judgments, and dissemination of fake news. Despite this, current literature has largely focused on more complex issues such as theory of mind, overlooking more fundamental epistemic challenges. This study systematically evaluates the epistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and Llama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13 tasks. Our results reveal key limitations. First, while LMs achieve 86% accuracy on factual scenarios, their performance drops significantly with false scenarios, particularly in belief-related tasks. Second, LMs struggle with recognizing and affirming personal beliefs, especially when those beliefs contradict factual data, which raises concerns for applications in healthcare and counseling, where engaging with a person's beliefs is critical. Third, we identify a salient bias in how LMs process first-person versus third-person beliefs, performing better on third-person tasks (80.7%) compared to first-person tasks (54.4%). Fourth, LMs lack a robust understanding of the factive nature of knowledge, namely, that knowledge inherently requires truth. Fifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the deeper reasoning. These findings highlight significant concerns about current LMs' ability to reason about truth, belief, and knowledge while emphasizing the need for advancements in these areas before broad deployment in critical sectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21195v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mirac Suzgun, Tayfun Gur, Federico Bianchi, Daniel E. Ho, Thomas Icard, Dan Jurafsky, James Zou</dc:creator>
    </item>
    <item>
      <title>GPT-4o System Card</title>
      <link>https://arxiv.org/abs/2410.21276</link>
      <description>arXiv:2410.21276v1 Announce Type: cross 
Abstract: GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21276v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> OpenAI (Tony),  : (Tony), Aaron Hurst (Tony), Adam Lerer (Tony), Adam P. Goucher (Tony), Adam Perelman (Tony), Aditya Ramesh (Tony), Aidan Clark (Tony), AJ Ostrow (Tony), Akila Welihinda (Tony), Alan Hayes (Tony), Alec Radford (Tony), Aleksander M\k{a}dry (Tony), Alex Baker-Whitcomb (Tony), Alex Beutel (Tony), Alex Borzunov (Tony), Alex Carney (Tony), Alex Chow (Tony), Alex Kirillov (Tony), Alex Nichol (Tony), Alex Paino (Tony), Alex Renzin (Tony), Alex Tachard Passos (Tony), Alexander Kirillov (Tony), Alexi Christakis (Tony), Alexis Conneau (Tony), Ali Kamali (Tony), Allan Jabri (Tony), Allison Moyer (Tony), Allison Tam (Tony), Amadou Crookes (Tony), Amin Tootoochian (Tony), Amin Tootoonchian (Tony), Ananya Kumar (Tony), Andrea Vallone (Tony), Andrej Karpathy (Tony), Andrew Braunstein (Tony), Andrew Cann (Tony), Andrew Codispoti (Tony), Andrew Galu (Tony), Andrew Kondrich (Tony), Andrew Tulloch (Tony), Andrey Mishchenko (Tony), Angela Baek (Tony), Angela Jiang (Tony), Antoine Pelisse (Tony), Antonia Woodford (Tony), Anuj Gosalia (Tony), Arka Dhar (Tony), Ashley Pantuliano (Tony), Avi Nayak (Tony), Avital Oliver (Tony), Barret Zoph (Tony), Behrooz Ghorbani (Tony), Ben Leimberger (Tony), Ben Rossen (Tony), Ben Sokolowsky (Tony), Ben Wang (Tony), Benjamin Zweig (Tony), Beth Hoover (Tony), Blake Samic (Tony), Bob McGrew (Tony), Bobby Spero (Tony), Bogo Giertler (Tony), Bowen Cheng (Tony), Brad Lightcap (Tony), Brandon Walkin (Tony), Brendan Quinn (Tony), Brian Guarraci (Tony), Brian Hsu (Tony), Bright Kellogg (Tony), Brydon Eastman (Tony), Camillo Lugaresi (Tony), Carroll Wainwright (Tony), Cary Bassin (Tony), Cary Hudson (Tony), Casey Chu (Tony), Chad Nelson (Tony), Chak Li (Tony), Chan Jun Shern (Tony), Channing Conger (Tony), Charlotte Barette (Tony), Chelsea Voss (Tony), Chen Ding (Tony), Cheng Lu (Tony), Chong Zhang (Tony), Chris Beaumont (Tony), Chris Hallacy (Tony), Chris Koch (Tony), Christian Gibson (Tony), Christina Kim (Tony), Christine Choi (Tony), Christine McLeavey (Tony), Christopher Hesse (Tony), Claudia Fischer (Tony), Clemens Winter (Tony), Coley Czarnecki (Tony), Colin Jarvis (Tony), Colin Wei (Tony), Constantin Koumouzelis (Tony), Dane Sherburn (Tony), Daniel Kappler (Tony), Daniel Levin (Tony), Daniel Levy (Tony), David Carr (Tony), David Farhi (Tony), David Mely (Tony), David Robinson (Tony), David Sasaki (Tony), Denny Jin (Tony), Dev Valladares (Tony), Dimitris Tsipras (Tony), Doug Li (Tony), Duc Phong Nguyen (Tony), Duncan Findlay (Tony), Edede Oiwoh (Tony), Edmund Wong (Tony), Ehsan Asdar (Tony), Elizabeth Proehl (Tony), Elizabeth Yang (Tony), Eric Antonow (Tony), Eric Kramer (Tony), Eric Peterson (Tony), Eric Sigler (Tony), Eric Wallace (Tony), Eugene Brevdo (Tony), Evan Mays (Tony), Farzad Khorasani (Tony), Felipe Petroski Such (Tony), Filippo Raso (Tony), Francis Zhang (Tony), Fred von Lohmann (Tony), Freddie Sulit (Tony), Gabriel Goh (Tony), Gene Oden (Tony), Geoff Salmon (Tony), Giulio Starace (Tony), Greg Brockman (Tony), Hadi Salman (Tony), Haiming Bao (Tony), Haitang Hu (Tony), Hannah Wong (Tony), Haoyu Wang (Tony), Heather Schmidt (Tony), Heather Whitney (Tony), Heewoo Jun (Tony), Hendrik Kirchner (Tony), Henrique Ponde de Oliveira Pinto (Tony), Hongyu Ren (Tony), Huiwen Chang (Tony), Hyung Won Chung (Tony), Ian Kivlichan (Tony), Ian O'Connell (Tony), Ian O'Connell (Tony), Ian Osband (Tony), Ian Silber (Tony), Ian Sohl (Tony), Ibrahim Okuyucu (Tony), Ikai Lan (Tony), Ilya Kostrikov (Tony), Ilya Sutskever (Tony), Ingmar Kanitscheider (Tony), Ishaan Gulrajani (Tony), Jacob Coxon (Tony), Jacob Menick (Tony), Jakub Pachocki (Tony), James Aung (Tony), James Betker (Tony), James Crooks (Tony), James Lennon (Tony), Jamie Kiros (Tony), Jan Leike (Tony), Jane Park (Tony), Jason Kwon (Tony), Jason Phang (Tony), Jason Teplitz (Tony), Jason Wei (Tony), Jason Wolfe (Tony), Jay Chen (Tony), Jeff Harris (Tony), Jenia Varavva (Tony), Jessica Gan Lee (Tony), Jessica Shieh (Tony), Ji Lin (Tony), Jiahui Yu (Tony), Jiayi Weng (Tony), Jie Tang (Tony), Jieqi Yu (Tony), Joanne Jang (Tony), Joaquin Quinonero Candela (Tony), Joe Beutler (Tony), Joe Landers (Tony), Joel Parish (Tony), Johannes Heidecke (Tony), John Schulman (Tony), Jonathan Lachman (Tony), Jonathan McKay (Tony), Jonathan Uesato (Tony), Jonathan Ward (Tony), Jong Wook Kim (Tony), Joost Huizinga (Tony), Jordan Sitkin (Tony), Jos Kraaijeveld (Tony), Josh Gross (Tony), Josh Kaplan (Tony), Josh Snyder (Tony), Joshua Achiam (Tony), Joy Jiao (Tony), Joyce Lee (Tony), Juntang Zhuang (Tony), Justyn Harriman (Tony), Kai Fricke (Tony), Kai Hayashi (Tony), Karan Singhal (Tony), Katy Shi (Tony), Kavin Karthik (Tony), Kayla Wood (Tony), Kendra Rimbach (Tony), Kenny Hsu (Tony), Kenny Nguyen (Tony), Keren Gu-Lemberg (Tony), Kevin Button (Tony), Kevin Liu (Tony), Kiel Howe (Tony), Krithika Muthukumar (Tony), Kyle Luther (Tony), Lama Ahmad (Tony), Larry Kai (Tony), Lauren Itow (Tony), Lauren Workman (Tony), Leher Pathak (Tony), Leo Chen (Tony), Li Jing (Tony), Lia Guy (Tony), Liam Fedus (Tony), Liang Zhou (Tony), Lien Mamitsuka (Tony), Lilian Weng (Tony), Lindsay McCallum (Tony), Lindsey Held (Tony), Long Ouyang (Tony), Louis Feuvrier (Tony), Lu Zhang (Tony), Lukas Kondraciuk (Tony), Lukasz Kaiser (Tony), Luke Hewitt (Tony), Luke Metz (Tony), Lyric Doshi (Tony), Mada Aflak (Tony), Maddie Simens (Tony), Madelaine Boyd (Tony), Madeleine Thompson (Tony), Marat Dukhan (Tony), Mark Chen (Tony), Mark Gray (Tony), Mark Hudnall (Tony), Marvin Zhang (Tony), Marwan Aljubeh (Tony), Mateusz Litwin (Tony), Matthew Zeng (Tony), Max Johnson (Tony), Maya Shetty (Tony), Mayank Gupta (Tony), Meghan Shah (Tony), Mehmet Yatbaz (Tony), Meng Jia Yang (Tony), Mengchao Zhong (Tony), Mia Glaese (Tony), Mianna Chen (Tony), Michael Janner (Tony), Michael Lampe (Tony), Michael Petrov (Tony), Michael Wu (Tony), Michele Wang (Tony), Michelle Fradin (Tony), Michelle Pokrass (Tony), Miguel Castro (Tony), Miguel Oom Temudo de Castro (Tony), Mikhail Pavlov (Tony), Miles Brundage (Tony), Miles Wang (Tony), Minal Khan (Tony), Mira Murati (Tony), Mo Bavarian (Tony), Molly Lin (Tony), Murat Yesildal (Tony), Nacho Soto (Tony), Natalia Gimelshein (Tony), Natalie Cone (Tony), Natalie Staudacher (Tony), Natalie Summers (Tony), Natan LaFontaine (Tony), Neil Chowdhury (Tony), Nick Ryder (Tony), Nick Stathas (Tony), Nick Turley (Tony), Nik Tezak (Tony), Niko Felix (Tony), Nithanth Kudige (Tony), Nitish Keskar (Tony), Noah Deutsch (Tony), Noel Bundick (Tony), Nora Puckett (Tony), Ofir Nachum (Tony), Ola Okelola (Tony), Oleg Boiko (Tony), Oleg Murk (Tony), Oliver Jaffe (Tony), Olivia Watkins (Tony), Olivier Godement (Tony), Owen Campbell-Moore (Tony), Patrick Chao (Tony), Paul McMillan (Tony), Pavel Belov (Tony), Peng Su (Tony), Peter Bak (Tony), Peter Bakkum (Tony), Peter Deng (Tony), Peter Dolan (Tony), Peter Hoeschele (Tony), Peter Welinder (Tony), Phil Tillet (Tony), Philip Pronin (Tony), Philippe Tillet (Tony), Prafulla Dhariwal (Tony), Qiming Yuan (Tony), Rachel Dias (Tony), Rachel Lim (Tony), Rahul Arora (Tony), Rajan Troll (Tony), Randall Lin (Tony), Rapha Gontijo Lopes (Tony), Raul Puri (Tony), Reah Miyara (Tony), Reimar Leike (Tony), Renaud Gaubert (Tony), Reza Zamani (Tony), Ricky Wang (Tony), Rob Donnelly (Tony), Rob Honsby (Tony), Rocky Smith (Tony), Rohan Sahai (Tony), Rohit Ramchandani (Tony), Romain Huet (Tony), Rory Carmichael (Tony), Rowan Zellers (Tony), Roy Chen (Tony), Ruby Chen (Tony), Ruslan Nigmatullin (Tony), Ryan Cheu (Tony), Saachi Jain (Tony), Sam Altman (Tony), Sam Schoenholz (Tony), Sam Toizer (Tony), Samuel Miserendino (Tony), Sandhini Agarwal (Tony), Sara Culver (Tony), Scott Ethersmith (Tony), Scott Gray (Tony), Sean Grove (Tony), Sean Metzger (Tony), Shamez Hermani (Tony), Shantanu Jain (Tony), Shengjia Zhao (Tony), Sherwin Wu (Tony), Shino Jomoto (Tony), Shirong Wu (Tony),  Shuaiqi (Tony),  Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence for the Internal Democracy of Political Parties</title>
      <link>https://arxiv.org/abs/2405.09529</link>
      <description>arXiv:2405.09529v2 Announce Type: replace 
Abstract: The article argues that AI can enhance the measurement and implementation of democratic processes within political parties, known as Intra-Party Democracy (IPD). It identifies the limitations of traditional methods for measuring IPD, which often rely on formal parameters, self-reported data, and tools like surveys. Such limitations lead to the collection of partial data, rare updates, and significant demands on resources. To address these issues, the article suggests that specific data management and Machine Learning (ML) techniques, such as natural language processing and sentiment analysis, can improve the measurement (ML about) and practice (ML for) of IPD. The article concludes by considering some of the principal risks of ML for IPD, including concerns over data privacy, the potential for manipulation, and the dangers of overreliance on technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09529v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11023-024-09693-x</arxiv:DOI>
      <arxiv:journal_reference>Minds &amp; Machines 34, 36 (2024)</arxiv:journal_reference>
      <dc:creator>Claudio Novelli, Giuliano Formisano, Prathm Juneja, Giulia Sandri, Luciano Floridi</dc:creator>
    </item>
    <item>
      <title>A Robust Governance for the AI Act: AI Office, AI Board, Scientific Panel, and National Authorities</title>
      <link>https://arxiv.org/abs/2407.10369</link>
      <description>arXiv:2407.10369v2 Announce Type: replace 
Abstract: Regulation is nothing without enforcement. This particularly holds for the dynamic field of emerging technologies. Hence, this article has two ambitions. First, it explains how the EU's new Artificial Intelligence Act (AIA) will be implemented and enforced by various institutional bodies, thus clarifying the governance framework of the AIA. Second, it proposes a normative model of governance, providing recommendations to ensure uniform and coordinated execution of the AIA and the fulfilment of the legislation. Taken together, the article explores how the AIA may be implemented by national and EU institutional bodies, encompassing longstanding bodies, such as the European Commission, and those newly established under the AIA, such as the AI Office. It investigates their roles across supranational and national levels, emphasizing how EU regulations influence institutional structures and operations. These regulations may not only directly dictate the structural design of institutions but also indirectly request administrative capacities needed to enforce the AIA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10369v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1017/err.2024.57</arxiv:DOI>
      <dc:creator>Claudio Novelli, Philipp Hacker, Jessica Morley, Jarle Trondal, Luciano Floridi</dc:creator>
    </item>
    <item>
      <title>Assistive AI for Augmenting Human Decision-making</title>
      <link>https://arxiv.org/abs/2410.14353</link>
      <description>arXiv:2410.14353v2 Announce Type: replace 
Abstract: Regulatory frameworks for the use of AI are emerging. However, they trail behind the fast-evolving malicious AI technologies that can quickly cause lasting societal damage. In response, we introduce a pioneering Assistive AI framework designed to enhance human decision-making capabilities. This framework aims to establish a trust network across various fields, especially within legal contexts, serving as a proactive complement to ongoing regulatory efforts. Central to our framework are the principles of privacy, accountability, and credibility. In our methodology, the foundation of reliability of information and information sources is built upon the ability to uphold accountability, enhance security, and protect privacy. This approach supports, filters, and potentially guides communication, thereby empowering individuals and communities to make well-informed decisions based on cutting-edge advancements in AI. Our framework uses the concept of Boards as proxies to collectively ensure that AI-assisted decisions are reliable, accountable, and in alignment with societal values and legal standards. Through a detailed exploration of our framework, including its main components, operations, and sample use cases, the paper shows how AI can assist in the complex process of decision-making while maintaining human oversight. The proposed framework not only extends regulatory landscapes but also highlights the synergy between AI technology and human judgement, underscoring the potential of AI to serve as a vital instrument in discerning reality from fiction and thus enhancing the decision-making process. Furthermore, we provide domain-specific use cases to highlight the applicability of our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14353v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Natabara M\'at\'e Gy\"ongy\"ossy, Bern\'at T\"or\"ok, Csilla Farkas, Laura Lucaj, Attila Menyh\'ard, Krisztina Menyh\'ard-Bal\'azs, Andr\'as Simonyi, Patrick van der Smagt, Zsolt Z\H{o}di, Andr\'as L\H{o}rincz</dc:creator>
    </item>
    <item>
      <title>Evaluation of Systems Programming Exercises through Tailored Static Analysis</title>
      <link>https://arxiv.org/abs/2410.17260</link>
      <description>arXiv:2410.17260v2 Announce Type: replace 
Abstract: In large programming classes, it takes a significant effort from teachers to evaluate exercises and provide detailed feedback. In systems programming, test cases are not sufficient to assess exercises, since concurrency and resource management bugs are difficult to reproduce. This paper presents an experience report on static analysis for the automatic evaluation of systems programming exercises. We design systems programming assignments with static analysis rules that are tailored for each assignment, to provide detailed and accurate feedback. Our evaluation shows that static analysis can identify a significant number of erroneous submissions missed by test cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17260v2</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3641554.3701836</arxiv:DOI>
      <dc:creator>Roberto Natella</dc:creator>
    </item>
    <item>
      <title>Rules, Cases, and Reasoning: Positivist Legal Theory as a Framework for Pluralistic AI Alignment</title>
      <link>https://arxiv.org/abs/2410.17271</link>
      <description>arXiv:2410.17271v3 Announce Type: replace 
Abstract: Legal theory can address two related key problems of alignment: pluralism and specification. Alignment researchers must determine how to specify what is concretely meant by vague principles like helpfulness and fairness and they must ensure that their techniques do not exclude alternative perspectives on life and values. The law faces these same problems. Leading legal theories suggest the law solves these problems through the interaction of rules and cases, where general rules promulgated by a democratic authority are given specific content through their application over time. Concrete applications allow for convergence on practical meaning while preserving space for disagreement on values. These approaches suggest improvements to existing democratic alignment processes that use AI to create cases that give content to rules, allowing for more pluralist alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17271v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas A. Caputo</dc:creator>
    </item>
    <item>
      <title>Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents</title>
      <link>https://arxiv.org/abs/2402.12327</link>
      <description>arXiv:2402.12327v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have increasingly been utilized in social simulations, where they are often guided by carefully crafted instructions to stably exhibit human-like behaviors during simulations. Nevertheless, we doubt the necessity of shaping agents' behaviors for accurate social simulations. Instead, this paper emphasizes the importance of spontaneous phenomena, wherein agents deeply engage in contexts and make adaptive decisions without explicit directions. We explored spontaneous cooperation across three competitive scenarios and successfully simulated the gradual emergence of cooperation, findings that align closely with human behavioral data. This approach not only aids the computational social science community in bridging the gap between simulations and real-world dynamics but also offers the AI community a novel method to assess LLMs' capability of deliberate reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12327v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zengqing Wu, Run Peng, Shuyuan Zheng, Qianying Liu, Xu Han, Brian Inhyuk Kwon, Makoto Onizuka, Shaojie Tang, Chuan Xiao</dc:creator>
    </item>
    <item>
      <title>Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation</title>
      <link>https://arxiv.org/abs/2402.14744</link>
      <description>arXiv:2402.14744v3 Announce Type: replace-cross 
Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14744v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Noboru Koshizuka, Chuan Xiao</dc:creator>
    </item>
    <item>
      <title>Leveraging Self-Supervised Learning for Scene Classification in Child Sexual Abuse Imagery</title>
      <link>https://arxiv.org/abs/2403.01183</link>
      <description>arXiv:2403.01183v2 Announce Type: replace-cross 
Abstract: Crime in the 21st century is split into a virtual and real world. However, the former has become a global menace to people's well-being and security in the latter. The challenges it presents must be faced with unified global cooperation, and we must rely more than ever on automated yet trustworthy tools to combat the ever-growing nature of online offenses. Over 10 million child sexual abuse reports are submitted to the US National Center for Missing \&amp; Exploited Children every year, and over 80% originate from online sources. Therefore, investigation centers cannot manually process and correctly investigate all imagery. In light of that, reliable automated tools that can securely and efficiently deal with this data are paramount. In this sense, the scene classification task looks for contextual cues in the environment, being able to group and classify child sexual abuse data without requiring to be trained on sensitive material. The scarcity and limitations of working with child sexual abuse images lead to self-supervised learning, a machine-learning methodology that leverages unlabeled data to produce powerful representations that can be more easily transferred to downstream tasks. This work shows that self-supervised deep learning models pre-trained on scene-centric data can reach 71.6% balanced accuracy on our indoor scene classification task and, on average, 2.2 percentage points better performance than a fully supervised version. We cooperate with Brazilian Federal Police experts to evaluate our indoor classification model on actual child abuse material. The results demonstrate a notable discrepancy between the features observed in widely used scene datasets and those depicted on sensitive materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01183v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pedro H. V. Valois, Jo\~ao Macedo, Leo S. F. Ribeiro, Jefersson A. dos Santos, Sandra Avila</dc:creator>
    </item>
    <item>
      <title>Beyond Data Points: Regionalizing Crowdsourced Latency Measurements</title>
      <link>https://arxiv.org/abs/2405.11138</link>
      <description>arXiv:2405.11138v4 Announce Type: replace-cross 
Abstract: Despite significant investments in access network infrastructure, universal access to high-quality Internet connectivity remains a challenge. Policymakers often rely on large-scale, crowdsourced measurement datasets to assess the distribution of access network performance across geographic areas. These decisions typically rest on the assumption that Internet performance is uniformly distributed within predefined social boundaries. However, this assumption may not be valid for two reasons: crowdsourced measurements often exhibit non-uniform sampling densities within geographic areas; and predefined social boundaries may not align with the actual boundaries of Internet infrastructure. In this paper, we present a spatial analysis on crowdsourced datasets for constructing stable boundaries for sampling Internet performance. We hypothesize that greater stability in sampling boundaries will reflect the true nature of Internet performance disparities than misleading patterns observed as a result of data sampling variations. We apply and evaluate a series of statistical techniques to: aggregate Internet performance over geographic regions; overlay interpolated maps with various sampling unit choices; and spatially cluster boundary units to identify contiguous areas with similar performance characteristics. We assess the effectiveness of the techniques we apply by comparing the similarity of the resulting boundaries for monthly samples drawn from the dataset. Our evaluation shows that the combination of techniques we apply achieves higher similarity compared to directly calculating central measures of network metrics over census tracts or neighborhood boundaries. These findings underscore the important role of spatial modeling in accurately assessing and optimizing the distribution of Internet performance, to inform policy, network operations, and long-term planning decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11138v4</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Taveesh Sharma, Paul Schmitt, Francesco Bronzino, Nick Feamster, Nicole Marwell</dc:creator>
    </item>
    <item>
      <title>A Comparative Analysis of Wealth Index Predictions in Africa between three Multi-Source Inference Models</title>
      <link>https://arxiv.org/abs/2408.01631</link>
      <description>arXiv:2408.01631v3 Announce Type: replace-cross 
Abstract: Poverty map inference has become a critical focus of research, utilizing both traditional and modern techniques, ranging from regression models to convolutional neural networks applied to tabular data, satellite imagery, and networks. While much attention has been given to validating models during the training phase, the final predictions have received less scrutiny. In this study, we analyze the International Wealth Index (IWI) predicted by Lee and Braithwaite (2022) and Esp\'in-Noboa et al. (2023), alongside the Relative Wealth Index (RWI) inferred by Chi et al. (2022), across six Sub-Saharan African countries. Our analysis reveals trends and discrepancies in wealth predictions between these models. In particular, significant and unexpected discrepancies between the predictions of Lee and Braithwaite and Esp\'in-Noboa et al., even after accounting for differences in training data. In contrast, the shape of the wealth distributions predicted by Esp\'in-Noboa et al. and Chi et al. are more closely aligned, suggesting similar levels of skewness. These findings raise concerns about the validity of certain models and emphasize the importance of rigorous audits for wealth prediction algorithms used in policy-making. Continuous validation and refinement are essential to ensure the reliability of these models, particularly when they inform poverty alleviation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01631v3</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>M\'arton Karsai, J\'anos Kert\'esz, Lisette Esp\'in-Noboa</dc:creator>
    </item>
    <item>
      <title>An Effective Theory of Bias Amplification</title>
      <link>https://arxiv.org/abs/2410.17263</link>
      <description>arXiv:2410.17263v3 Announce Type: replace-cross 
Abstract: Machine learning models may capture and amplify biases present in data, leading to disparate test performance across social groups. To better understand, evaluate, and mitigate these possible biases, a deeper theoretical understanding of how model design choices and data distribution properties could contribute to bias is needed. In this work, we contribute a precise analytical theory in the context of ridge regression, both with and without random projections, where the former models neural networks in a simplified regime. Our theory offers a unified and rigorous explanation of machine learning bias, providing insights into phenomena such as bias amplification and minority-group bias in various feature and parameter regimes. For example, we demonstrate that there may be an optimal regularization penalty or training time to avoid bias amplification, and there can be fundamental differences in test error between groups that do not vanish with increased parameterization. Importantly, our theoretical predictions align with several empirical observations reported in the literature. We extensively empirically validate our theory on diverse synthetic and semi-synthetic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17263v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arjun Subramonian, Samuel J. Bell, Levent Sagun, Elvis Dohmatob</dc:creator>
    </item>
  </channel>
</rss>
