<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Dec 2024 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal Static Dictionary with Worst-Case Constant Query Time</title>
      <link>https://arxiv.org/abs/2412.10655</link>
      <description>arXiv:2412.10655v1 Announce Type: new 
Abstract: In this paper, we design a new succinct static dictionary with worst-case constant query time. A dictionary data structure stores a set of key-value pairs with distinct keys in $[U]$ and values in $[\sigma]$, such that given a query $x\in [U]$, it quickly returns if $x$ is one of the input keys, and if so, also returns its associated value. The textbook solution to dictionaries is hash tables. On the other hand, the (information-theoretical) optimal space to encode such a set of key-value pairs is only $\text{OPT} := \log\binom{U}{n}+n\log \sigma$.
  We construct a dictionary that uses $\text{OPT} + n^{\epsilon}$ bits of space, and answers queries in constant time in worst case. Previously, constant-time dictionaries are only known with $\text{OPT} + n/\text{poly}\log n$ space [P\v{a}tra\c{s}cu 2008], or with $\text{OPT}+n^{\epsilon}$ space but expected constant query time [Yu 2020]. We emphasize that most of the extra $n^{\epsilon}$ bits are used to store a lookup table that does not depend on the input, and random bits for hash functions. The "main" data structure only occupies $\text{OPT}+\text{poly}\log n$ bits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10655v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Hu, Jingxun Liang, Huacheng Yu, Junkai Zhang, Renfei Zhou</dc:creator>
    </item>
    <item>
      <title>Breaking the Barrier: A Polynomial-Time Polylogarithmic Approximation for Directed Steiner Tree</title>
      <link>https://arxiv.org/abs/2412.10744</link>
      <description>arXiv:2412.10744v1 Announce Type: new 
Abstract: The Directed Steiner Tree (DST) problem is defined on a directed graph $G=(V,E)$, where we are given a designated root vertex $r$ and a set of $k$ terminals $K \subseteq V \setminus {r}$. The goal is to find a minimum-cost subgraph that provides directed $r \rightarrow t$ paths for all terminals $t \in K$.
  The approximability of DST has long been a central open problem in network design. Although there exist polylogarithmic-approximation algorithms with quasi-polynomial running times (Charikar et al. 1998; Grandoni, Laekhanukit, and Li 2019; Ghuge and Nagarajan 2020), the best-known polynomial-time approximation until now has remained at $k^\epsilon$ for any constant $\epsilon &gt; 0$. Whether a polynomial-time algorithm achieving a polylogarithmic approximation exists has been a longstanding mystery.
  In this paper, we resolve this question by presenting a polynomial-time algorithm that achieves an $O(\log^3 k)$-approximation for DST on arbitrary directed graphs. This result nearly matches the state-of-the-art $O(\log^2 k / \log\log k)$ approximations known only via quasi-polynomial-time algorithms. The resulting gap -- $O(\log^3 k)$ versus $O(\log^2 k / \log\log k)$ -- mirrors the known complexity landscape for the Group Steiner Tree problem. This parallel suggests intriguing new directions: Is there a hardness result that provably separates the power of polynomial-time and quasi-polynomial-time algorithms for DST?</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10744v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bundit Laekhanukit</dc:creator>
    </item>
    <item>
      <title>Fixed Order Scheduling with Deadlines</title>
      <link>https://arxiv.org/abs/2412.10760</link>
      <description>arXiv:2412.10760v1 Announce Type: new 
Abstract: This paper studies a scheduling problem in a parallel machine setting, where each machine must adhere to a predetermined fixed order for processing the jobs. Given $n$ jobs, each with processing times and deadlines, we aim to minimize the number of machines while ensuring deadlines are met and the fixed order is maintained. We show that the first-fit algorithm solves the problem optimally with unit processing times and is a 2-approximation in the following four cases: (1) the order aligns with non-increasing slacks, (2) the order aligns with non-decreasing slacks, (3) the order aligns with non-increasing deadlines, and (4) the optimal solution uses at most 3 machines. For the general problem we provide an $O(\log n)$-approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10760v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andre Berger, Arman Rouhani, Marc Schroder</dc:creator>
    </item>
    <item>
      <title>Catch Me If You Can: Finding the Source of Infections in Temporal Networks</title>
      <link>https://arxiv.org/abs/2412.10877</link>
      <description>arXiv:2412.10877v1 Announce Type: new 
Abstract: Source detection (SD) is the task of finding the origin of a spreading process in a network. Algorithms for SD help us combat diseases, misinformation, pollution, and more, and have been studied by physicians, physicists, sociologists, and computer scientists. The field has received considerable attention and been analyzed in many settings (e.g., under different models of spreading processes), yet all previous work shares the same assumption that the network the spreading process takes place in has the same structure at every point in time. For example, if we consider how a disease spreads through a population, it is unrealistic to assume that two people can either never or at every time infect each other, rather such an infection is possible precisely when they meet. Therefore, we propose an extended model of SD based on temporal graphs, where each link between two nodes is only present at some time step. Temporal graphs have become a standard model of time-varying graphs, and, recently, researchers have begun to study infection problems (such as influence maximization) on temporal graphs (arXiv:2303.11703, [Gayraud et al., 2015]). We give the first formalization of SD on temporal graphs. For this, we employ the standard SIR model of spreading processes ([Hethcote, 1989]). We give both lower bounds and algorithms for the SD problem in a number of different settings, such as with consistent or dynamic source behavior and on general graphs as well as on trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10877v1</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Bals, Michelle D\"oring, Nicolas Klodt, George Skretas</dc:creator>
    </item>
    <item>
      <title>Dynamic Network Discovery via Infection Tracing</title>
      <link>https://arxiv.org/abs/2412.10881</link>
      <description>arXiv:2412.10881v1 Announce Type: new 
Abstract: Researchers, policy makers, and engineers need to make sense of data from spreading processes as diverse as rumor spreading in social networks, viral infections, and water contamination. Classical questions include predicting infection behavior in a given network or deducing the network structure from infection data. Most of the research on network infections studies static graphs, that is, the connections in the network are assumed to not change. More recently, temporal graphs, in which connections change over time, have been used to more accurately represent real-world infections, which rarely occur in unchanging networks. We propose a model for temporal graph discovery that is consistent with previous work on static graphs and embraces the greater expressiveness of temporal graphs. For this model, we give algorithms and lower bounds which are often tight. We analyze different variations of the problem, which make our results widely applicable and it also clarifies which aspects of temporal infections make graph discovery easier or harder. We round off our analysis with an experimental evaluation of our algorithm on real-world interaction data from the Stanford Network Analysis Project and on temporal Erd\H{o}s-Renyi graphs. On Erd\H{o}s-Renyi graphs, we uncover a threshold behavior, which can be explained by a novel connectivity parameter that we introduce during our theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10881v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Bals, Michelle D\"oring, Nicolas Klodt, George Skretas</dc:creator>
    </item>
    <item>
      <title>Sequential Diversification with Provable Guarantees</title>
      <link>https://arxiv.org/abs/2412.10944</link>
      <description>arXiv:2412.10944v1 Announce Type: new 
Abstract: Diversification is a useful tool for exploring large collections of information items. It has been used to reduce redundancy and cover multiple perspectives in information-search settings. Diversification finds applications in many different domains, including presenting search results of information-retrieval systems and selecting suggestions for recommender systems.
  Interestingly, existing measures of diversity are defined over \emph{sets} of items, rather than evaluating \emph{sequences} of items. This design choice comes in contrast with commonly-used relevance measures, which are distinctly defined over sequences of items, taking into account the ranking of items. The importance of employing sequential measures is that information items are almost always presented in a sequential manner, and during their information-exploration activity users tend to prioritize items with higher~ranking.
  In this paper, we study the problem of \emph{maximizing sequential diversity}. This is a new measure of \emph{diversity}, which accounts for the \emph{ranking} of the items, and incorporates \emph{item relevance} and \emph{user behavior}. The overarching framework can be instantiated with different diversity measures, and here we consider the measures of \emph{sum~diversity} and \emph{coverage~diversity}. The problem was recently proposed by Coppolillo et al.~\citep{coppolillo2024relevance}, where they introduce empirical methods that work well in practice. Our paper is a theoretical treatment of the problem: we establish the problem hardness and present algorithms with constant approximation guarantees for both diversity measures we consider. Experimentally, we demonstrate that our methods are competitive against strong baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10944v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Honglian Wang, Sijing Tu, Aristides Gionis</dc:creator>
    </item>
    <item>
      <title>New Approximation Guarantees for The Economic Warehouse Lot Scheduling Problem</title>
      <link>https://arxiv.org/abs/2412.11184</link>
      <description>arXiv:2412.11184v1 Announce Type: new 
Abstract: In this paper, we present long-awaited algorithmic advances toward the efficient construction of near-optimal replenishment policies for a true inventory management classic, the economic warehouse lot scheduling problem. While this paradigm has accumulated a massive body of surrounding literature since its inception in the late '50s, we are still very much in the dark as far as basic computational questions are concerned, perhaps due to the evasive nature of dynamic policies in this context. The latter feature forced earlier attempts to either study highly-structured classes of policies or to forgo provably-good performance guarantees altogether; to this day, rigorously analyzable results have been few and far between.
  The current paper develops novel analytical foundations for directly competing against dynamic policies. Combined with further algorithmic progress and newly-gained insights, these ideas culminate to a polynomial-time approximation scheme for constantly-many commodities as well as to a proof-of-concept $(2-\frac{17}{5000} + \epsilon)$-approximation for general problem instances. In this regard, the efficient design of $\epsilon$-optimal dynamic policies appeared to have been out of reach, since beyond algorithmic challenges by themselves, even the polynomial-space representation of such policies has been a fundamental open question. On the other front, our sub-$2$-approximation constitutes the first improvement over the performance guarantees achievable via ``stationary order sizes and stationary intervals'' (SOSI) policies, which have been state-of-the-art since the mid-'90s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11184v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Danny Segev</dc:creator>
    </item>
    <item>
      <title>New results for the detection of bicliques</title>
      <link>https://arxiv.org/abs/2412.11234</link>
      <description>arXiv:2412.11234v1 Announce Type: new 
Abstract: Building on existing algorithms and results, we offer new insights and algorithms for various problems related to detecting maximal and maximum bicliques. Most of these results focus on graphs with small maximum degree, providing improved complexities when this parameter is constant; a common characteristic in real-world graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11234v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Manoussakis</dc:creator>
    </item>
    <item>
      <title>Logarithmic Positional Partition Interval Encoding</title>
      <link>https://arxiv.org/abs/2412.11236</link>
      <description>arXiv:2412.11236v1 Announce Type: new 
Abstract: One requirement of maintaining digital information is storage. With the latest advances in the digital world, new emerging media types have required even more storage space to be kept than before. In fact, in many cases it is required to have larger amounts of storage to keep up with protocols that support more types of information at the same time. In contrast, compression algorithms have been integrated to facilitate the transfer of larger data. Numerical representations are construed as embodiments of information. However, this correct association of a sequence could feasibly be inverted to signify an elongated series of numerals. In this work, a novel mathematical paradigm was introduced to engineer a methodology reliant on iterative logarithmic transformations, finely tuned to numeric sequences. Through this fledgling approach, an intricate interplay of polymorphic numeric manipulations was conducted. By applying repeated logarithmic operations, the data were condensed into a minuscule representation. Approximately thirteen times surpassed the compression method, ZIP. Such extreme compaction, achieved through iterative reduction of expansive integers until they manifested as single-digit entities, conferred a novel sense of informational embodiment. Instead of relegating data to classical discrete encodings, this method transformed them into a quasi-continuous, logarithmically. By contrast, this introduced approach revealed that morphing data into deeply compressed numerical substrata beyond conventional boundaries was feasible. A holistic perspective emerges, validating that numeric data can be recalibrated into ephemeral sequences of logarithmic impressions. It was not merely a matter of reducing digits, but of reinterpreting data through a resolute numeric vantage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11236v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vasileios Alevizos, Nikitas Gerolimos, Sabrina Edralin, Clark Xu, Akebu Simasiku, Georgios Priniotakis, George Papakostas, Zongliang Yue</dc:creator>
    </item>
    <item>
      <title>Proportionally Fair Matching via Randomized Rounding</title>
      <link>https://arxiv.org/abs/2412.11238</link>
      <description>arXiv:2412.11238v1 Announce Type: new 
Abstract: Given an edge-colored graph, the goal of the proportional fair matching problem is to find a maximum weight matching
  while ensuring proportional representation (with respect to the number of edges) of each color. The colors may correspond to demographic groups or other protected traits where we seek to ensure
  roughly equal representation from each group.
  It is known that, assuming ETH, it is impossible to approximate the problem with $\ell$ colors in time $2^{o(\ell)} n^{\mathcal{O}(1)}$ (i.e., subexponential in $\ell$) even on \emph{unweighted path graphs}. Further, even determining the existence of a non-empty matching satisfying proportionality is NP-Hard.
  To overcome this hardness, we relax the stringent
  proportional fairness constraints to a probabilistic notion. We introduce a notion we call $\delta$-\textsc{ProbablyAlmostFair}, where we ensure proportionality up to a factor of at most $(1 \pm \delta)$ for some small $\delta &gt;0$ with high probability. The violation $\delta$ can be brought arbitrarily close to $0$ for some \emph{good} instances with large values of matching size.
  We propose and analyze simple and fast algorithms for bipartite graphs that achieve
  constant-factor approximation guarantees, and return a $\delta$-\textsc{ProbablyAlmostFair} matching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11238v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sharmila Duppala, Nathaniel Grammel, Juan Luque, Calum MacRury, Aravind Srinivasan</dc:creator>
    </item>
    <item>
      <title>Regularized Dikin Walks for Sampling Truncated Logconcave Measures, Mixed Isoperimetry and Beyond Worst-Case Analysis</title>
      <link>https://arxiv.org/abs/2412.11303</link>
      <description>arXiv:2412.11303v1 Announce Type: new 
Abstract: We study the problem of drawing samples from a logconcave distribution truncated on a polytope, motivated by computational challenges in Bayesian statistical models with indicator variables, such as probit regression. Building on interior point methods and the Dikin walk for sampling from uniform distributions, we analyze the mixing time of regularized Dikin walks. Our contributions are threefold. First, for a logconcave and log-smooth distribution with condition number $\kappa$, truncated on a polytope in $\mathbb{R}^n$ defined with $m$ linear constraints, we prove that the soft-threshold Dikin walk mixes in $\widetilde{O}((m+\kappa)n)$ iterations from a warm initialization. It improves upon prior work which required the polytope to be bounded and involved a bound dependent on the radius of the bounded region. Moreover, we introduce the regularized Dikin walk using Lewis weights for approximating the John ellipsoid. We show that it mixes in $\widetilde{O}((n^{2.5}+\kappa n)$. Second, we extend the mixing time guarantees mentioned above to weakly log-concave distributions truncated on polytopes, provided that they have a finite covariance matrix. Third, going beyond worst-case mixing time analysis, we demonstrate that soft-threshold Dikin walk can mix significantly faster when only a limited number of constraints intersect the high-probability mass of the distribution, improving the $\widetilde{O}((m+\kappa)n)$ upper bound to $\widetilde{O}(m + \kappa n)$. Additionally, per-iteration complexity of regularized Dikin walk and ways to generate a warm initialization are discussed to facilitate practical implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11303v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minhui Jiang, Yuansi Chen</dc:creator>
    </item>
    <item>
      <title>Counting Butterflies over Streaming Bipartite Graphs with Duplicate Edges</title>
      <link>https://arxiv.org/abs/2412.11488</link>
      <description>arXiv:2412.11488v1 Announce Type: new 
Abstract: Bipartite graphs are commonly used to model relationships between two distinct entities in real-world applications, such as user-product interactions, user-movie ratings and collaborations between authors and publications. A butterfly (a 2x2 bi-clique) is a critical substructure in bipartite graphs, playing a significant role in tasks like community detection, fraud detection, and link prediction. As more real-world data is presented in a streaming format, efficiently counting butterflies in streaming bipartite graphs has become increasingly important. However, most existing algorithms typically assume that duplicate edges are absent, which is hard to hold in real-world graph streams, as a result, they tend to sample edges that appear multiple times, leading to inaccurate results. The only algorithm designed to handle duplicate edges is FABLE, but it suffers from significant limitations, including high variance, substantial time complexity, and memory inefficiency due to its reliance on a priority queue. To overcome these limitations, we introduce DEABC (Duplicate-Edge-Aware Butterfly Counting), an innovative method that uses bucket-based priority sampling to accurately estimate the number of butterflies, accounting for duplicate edges. Compared to existing methods, DEABC significantly reduces memory usage by storing only the essential sampled edge data while maintaining high accuracy. We provide rigorous proofs of the unbiasedness and variance bounds for DEABC, ensuring they achieve high accuracy. We compare DEABC with state-of-the-art algorithms on real-world streaming bipartite graphs. The results show that our DEABC outperforms existing methods in memory efficiency and accuracy, while also achieving significantly higher throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11488v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingkai Meng, Long Yuan, Xuemin Lin, Chengjie Li, Kai Wang, Wenjie Zhang</dc:creator>
    </item>
    <item>
      <title>Adaptive Manipulation for Coalitions in Knockout Tournaments</title>
      <link>https://arxiv.org/abs/2412.11799</link>
      <description>arXiv:2412.11799v1 Announce Type: new 
Abstract: Knockout tournaments, also known as single-elimination or cup tournaments, are a popular form of sports competitions. In the standard probabilistic setting, for each pairing of players, one of the players wins the game with a certain (a priori known) probability. Due to their competitive nature, tournaments are prone to manipulation. We investigate the computational problem of determining whether, for a given tournament, a coalition has a manipulation strategy that increases the winning probability of a designated player above a given threshold. More precisely, in every round of the tournament, coalition players can strategically decide which games to throw based on the advancement of other players to the current round. We call this setting adaptive constructive coalition manipulation. To the best of our knowledge, while coalition manipulation has been studied in the literature, this is the first work to introduce adaptiveness to this context.
  We show that the above problem is hard for every complexity class in the polynomial hierarchy. On the algorithmic side, we show that the problem is solvable in polynomial time when the coalition size is a constant. Furthermore, we show that the problem is fixed-parameter tractable when parameterized by the coalition size and the size of a minimum player set that must include at least one player from each non-deterministic game. Lastly, we investigate a generalized setting where the tournament tree can be imbalanced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11799v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juhi Chaudhary, Hendrik Molter, Meirav Zehavi</dc:creator>
    </item>
    <item>
      <title>Witty: An Efficient Solver for Computing Minimum-Size Decision Trees</title>
      <link>https://arxiv.org/abs/2412.11954</link>
      <description>arXiv:2412.11954v1 Announce Type: new 
Abstract: Decision trees are a classic model for summarizing and classifying data. To enhance interpretability and generalization properties, it has been proposed to favor small decision trees. Accordingly, in the minimum-size decision tree training problem (MSDT), the input is a set of training examples in $\mathbb{R}^d$ with class labels and we aim to find a decision tree that classifies all training examples correctly and has a minimum number of nodes. MSDT is NP-hard and therefore presumably not solvable in polynomial time. Nevertheless, Komusiewicz et al. [ICML '23] developed a promising algorithmic paradigm called witness trees which solves MSDT efficiently if the solution tree is small. In this work, we test this paradigm empirically. We provide an implementation, augment it with extensive heuristic improvements, and scrutinize it on standard benchmark instances. The augmentations achieve a mean 324-fold (median 84-fold) speedup over the naive implementation. Compared to the state of the art they achieve a mean 32-fold (median 7-fold) speedup over the dynamic programming based MurTree solver [Demirovi\'c et al., J. Mach. Learn. Res. '22] and a mean 61-fold (median 25-fold) speedup over SAT-based implementations [Janota and Morgado, SAT '20]. As a theoretical result we obtain an improved worst-case running-time bound for MSDT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11954v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Pascal Staus, Christian Komusiewicz, Frank Sommer, Manuel Sorge</dc:creator>
    </item>
    <item>
      <title>Approximating the Top Eigenvector in Random Order Streams</title>
      <link>https://arxiv.org/abs/2412.11963</link>
      <description>arXiv:2412.11963v1 Announce Type: new 
Abstract: When rows of an $n \times d$ matrix $A$ are given in a stream, we study algorithms for approximating the top eigenvector of the matrix ${A}^TA$ (equivalently, the top right singular vector of $A$). We consider worst case inputs $A$ but assume that the rows are presented to the streaming algorithm in a uniformly random order. We show that when the gap parameter $R = \sigma_1(A)^2/\sigma_2(A)^2 = \Omega(1)$, then there is a randomized algorithm that uses $O(h \cdot d \cdot \operatorname{polylog}(d))$ bits of space and outputs a unit vector $v$ that has a correlation $1 - O(1/\sqrt{R})$ with the top eigenvector $v_1$. Here $h$ denotes the number of \emph{heavy rows} in the matrix, defined as the rows with Euclidean norm at least $\|{A}\|_F/\sqrt{d \cdot \operatorname{polylog}(d)}$. We also provide a lower bound showing that any algorithm using $O(hd/R)$ bits of space can obtain at most $1 - \Omega(1/R^2)$ correlation with the top eigenvector. Thus, parameterizing the space complexity in terms of the number of heavy rows is necessary for high accuracy solutions.
  Our results improve upon the $R = \Omega(\log n \cdot \log d)$ requirement in a recent work of Price and Xun (FOCS 2024). We note that the algorithm of Price and Xun works for arbitrary order streams whereas our algorithm requires a stronger assumption that the rows are presented in a uniformly random order. We additionally show that the gap requirements in their analysis can be brought down to $R = \Omega(\log^2 d)$ for arbitrary order streams and $R = \Omega(\log d)$ for random order streams. The requirement of $R = \Omega(\log d)$ for random order streams is nearly tight for their analysis as we obtain a simple instance with $R = \Omega(\log d/\log\log d)$ for which their algorithm, with any fixed learning rate, cannot output a vector approximating the top eigenvector $v_1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11963v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Praneeth Kacham, David P. Woodruff</dc:creator>
    </item>
    <item>
      <title>Differentially Private Multi-Sampling from Distributions</title>
      <link>https://arxiv.org/abs/2412.10512</link>
      <description>arXiv:2412.10512v1 Announce Type: cross 
Abstract: Many algorithms have been developed to estimate probability distributions subject to differential privacy (DP): such an algorithm takes as input independent samples from a distribution and estimates the density function in a way that is insensitive to any one sample. A recent line of work, initiated by Raskhodnikova et al. (Neurips '21), explores a weaker objective: a differentially private algorithm that approximates a single sample from the distribution. Raskhodnikova et al. studied the sample complexity of DP \emph{single-sampling} i.e., the minimum number of samples needed to perform this task. They showed that the sample complexity of DP single-sampling is less than the sample complexity of DP learning for certain distribution classes. We define two variants of \emph{multi-sampling}, where the goal is to privately approximate $m&gt;1$ samples. This better models the realistic scenario where synthetic data is needed for exploratory data analysis.
  A baseline solution to \emph{multi-sampling} is to invoke a single-sampling algorithm $m$ times on independently drawn datasets of samples. When the data comes from a finite domain, we improve over the baseline by a factor of $m$ in the sample complexity. When the data comes from a Gaussian, Ghazi et al. (Neurips '23) show that \emph{single-sampling} can be performed under approximate differential privacy; we show it is possible to \emph{single- and multi-sample Gaussians with known covariance subject to pure DP}. Our solution uses a variant of the Laplace mechanism that is of independent interest.
  We also give sample complexity lower bounds, one for strong multi-sampling of finite distributions and another for weak multi-sampling of bounded-covariance Gaussians.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10512v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Albert Cheu, Debanuj Nayak</dc:creator>
    </item>
    <item>
      <title>Meeting Utility Constraints in Differential Privacy: A Privacy-Boosting Approach</title>
      <link>https://arxiv.org/abs/2412.10612</link>
      <description>arXiv:2412.10612v1 Announce Type: cross 
Abstract: Data engineering often requires accuracy (utility) constraints on results, posing significant challenges in designing differentially private (DP) mechanisms, particularly under stringent privacy parameter $\epsilon$. In this paper, we propose a privacy-boosting framework that is compatible with most noise-adding DP mechanisms. Our framework enhances the likelihood of outputs falling within a preferred subset of the support to meet utility requirements while enlarging the overall variance to reduce privacy leakage. We characterize the privacy loss distribution of our framework and present the privacy profile formulation for $(\epsilon,\delta)$-DP and R\'enyi DP (RDP) guarantees. We study special cases involving data-dependent and data-independent utility formulations. Through extensive experiments, we demonstrate that our framework achieves lower privacy loss than standard DP mechanisms under utility constraints. Notably, our approach is particularly effective in reducing privacy loss with large query sensitivity relative to the true answer, offering a more practical and flexible approach to designing differentially private mechanisms that meet specific utility constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10612v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Jiang, Wanrong Zhang, Donghang Lu, Jian Du, Sagar Sharma, Qiang Yan</dc:creator>
    </item>
    <item>
      <title>Bi-Criteria Metric Distortion</title>
      <link>https://arxiv.org/abs/2412.10671</link>
      <description>arXiv:2412.10671v1 Announce Type: cross 
Abstract: Selecting representatives based on voters' preferences is a fundamental problem in social choice theory. While cardinal utility functions offer a detailed representation of preferences, ordinal rankings are often the only available information due to their simplicity and practical constraints. The metric distortion framework addresses this issue by modeling voters and candidates as points in a metric space, with distortion quantifying the efficiency loss from relying solely on ordinal rankings. Existing works define the cost of a voter with respect to a candidate as their distance and set the overall cost as either the sum (utilitarian) or maximum (egalitarian) of these costs across all voters. They show that deterministic algorithms achieve a best-possible distortion of 3 for any metric when considering a single candidate.
  This paper explores whether one can obtain a better approximation compared to an optimal candidate by relying on a committee of $k$ candidates ($k \ge 1$), where the cost of a voter is defined as its distance to the closest candidate in the committee. We answer this affirmatively in the case of line metrics, demonstrating that with $O(1)$ candidates, it is possible to achieve optimal cost. Our results extend to both utilitarian and egalitarian objectives, providing new upper bounds for the problem. We complement our results with lower bounds for both the line and 2-D Euclidean metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10671v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kiarash Banihashem, Diptarka Chakraborty, Shayan Chashm Jahan, Iman Gholami, MohammadTaghi Hajiaghayi, Mohammad Mahdavi, Max Springer</dc:creator>
    </item>
    <item>
      <title>Stochastic $k$-Submodular Bandits with Full Bandit Feedback</title>
      <link>https://arxiv.org/abs/2412.10682</link>
      <description>arXiv:2412.10682v1 Announce Type: cross 
Abstract: In this paper, we present the first sublinear $\alpha$-regret bounds for online $k$-submodular optimization problems with full-bandit feedback, where $\alpha$ is a corresponding offline approximation ratio. Specifically, we propose online algorithms for multiple $k$-submodular stochastic combinatorial multi-armed bandit problems, including (i) monotone functions and individual size constraints, (ii) monotone functions with matroid constraints, (iii) non-monotone functions with matroid constraints, (iv) non-monotone functions without constraints, and (v) monotone functions without constraints. We transform approximation algorithms for offline $k$-submodular maximization problems into online algorithms through the offline-to-online framework proposed by Nie et al. (2023a). A key contribution of our work is analyzing the robustness of the offline algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10682v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanyu Nie, Vaneet Aggarwal, Christopher John Quinn</dc:creator>
    </item>
    <item>
      <title>Scaling Up Graph Propagation Computation on Large Graphs: A Local Chebyshev Approximation Approach</title>
      <link>https://arxiv.org/abs/2412.10789</link>
      <description>arXiv:2412.10789v1 Announce Type: cross 
Abstract: Graph propagation (GP) computation plays a crucial role in graph data analysis, supporting various applications such as graph node similarity queries, graph node ranking, graph clustering, and graph neural networks. Existing methods, mainly relying on power iteration or push computation frameworks, often face challenges with slow convergence rates when applied to large-scale graphs. To address this issue, we propose a novel and powerful approach that accelerates power iteration and push methods using Chebyshev polynomials. Specifically, we first present a novel Chebyshev expansion formula for general GP functions, offering a new perspective on GP computation and achieving accelerated convergence. Building on these theoretical insights, we develop a novel Chebyshev power iteration method (\ltwocheb) and a novel Chebyshev push method (\chebpush). Our \ltwocheb method demonstrates an approximate acceleration of $O(\sqrt{N})$ compared to existing power iteration techniques for both personalized PageRank and heat kernel PageRank computations, which are well-studied GP problems. For \chebpush, we propose an innovative subset Chebyshev recurrence technique, enabling the design of a push-style local algorithm with provable error guarantee and reduced time complexity compared to existing push methods. We conduct extensive experiments using 5 large real-world datasets to evaluate our proposed algorithms, demonstrating their superior efficiency compared to state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10789v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichun Yang, Rong-Hua Li, Meihao Liao, Longlong Lin, Guoren Wang</dc:creator>
    </item>
    <item>
      <title>Linear Programming based Approximation to Individually Fair k-Clustering with Outliers</title>
      <link>https://arxiv.org/abs/2412.10923</link>
      <description>arXiv:2412.10923v1 Announce Type: cross 
Abstract: Individual fairness guarantees are often desirable properties to have, but they become hard to formalize when the dataset contains outliers. Here, we investigate the problem of developing an individually fair $k$-means clustering algorithm for datasets that contain outliers. That is, given $n$ points and $k$ centers, we want that for each point which is not an outlier, there must be a center within the $\frac{n}{k}$ nearest neighbours of the given point. While a few of the recent works have looked into individually fair clustering, this is the first work that explores this problem in the presence of outliers for $k$-means clustering.
  For this purpose, we define and solve a linear program (LP) that helps us identify the outliers. We exclude these outliers from the dataset and apply a rounding algorithm that computes the $k$ centers, such that the fairness constraint of the remaining points is satisfied. We also provide theoretical guarantees that our method leads to a guaranteed approximation of the fair radius as well as the clustering cost. We also demonstrate our techniques empirically on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10923v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binita Maity, Shrutimoy Das, Anirban Dasgupta</dc:creator>
    </item>
    <item>
      <title>Deterministic Even-Cycle Detection in Broadcast CONGEST</title>
      <link>https://arxiv.org/abs/2412.11195</link>
      <description>arXiv:2412.11195v1 Announce Type: cross 
Abstract: We show that, for every $k\geq 2$, $C_{2k}$-freeness can be decided in $O(n^{1-1/k})$ rounds in the Broadcast CONGEST model, by a deterministic algorithm. This (deterministic) round-complexity is optimal for $k=2$ up to logarithmic factors thanks to the lower bound for $C_4$-freeness by Drucker et al. [PODC 2014], which holds even for randomized algorithms. Moreover it matches the round-complexity of the best known randomized algorithms by Censor-Hillel et al. [DISC 2020] for $k\in\{3,4,5\}$, and by Fraigniaud et al. [PODC 2024] for $k\geq 6$. Our algorithm uses parallel BFS-explorations with deterministic selections of the set of paths that are forwarded at each round, in a way similar to what is done for the detection of odd-length cycles, by Korhonen and Rybicki [OPODIS 2017]. However, the key element in the design and analysis of our algorithm is a new combinatorial result bounding the ''local density'' of graphs without $2k$-cycles, which we believe is interesting on its own.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11195v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Fraigniaud, Ma\"el Luce, Fr\'ed\'eric Magniez, Ioan Todinca</dc:creator>
    </item>
    <item>
      <title>Quantum search in a dictionary based on fingerprinting-hashing</title>
      <link>https://arxiv.org/abs/2412.11422</link>
      <description>arXiv:2412.11422v1 Announce Type: cross 
Abstract: In this work, we present a quantum query algorithm for searching a word of length $m$ in an unsorted dictionary of size $n$. The algorithm uses $O(\sqrt{n})$ queries (Grover operators), like previously known algorithms.
  What is new is that the algorithm is based on the quantum fingerprinting-hashing technique, which (a) provides a first level of amplitude amplification before applying the sequence of Grover amplitude amplification operators and (b) makes the algorithm more efficient in terms of memory use -- it requires $O(\log n + \log m)$ qubits.
  Note that previously developed algorithms by other researchers without hashing require $O(\log n + m)$ qubits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11422v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farid Ablayev, Nailya Salikhova, Marat Ablayev</dc:creator>
    </item>
    <item>
      <title>Shortest Paths without a Map, but with an Entropic Regularizer</title>
      <link>https://arxiv.org/abs/2202.04551</link>
      <description>arXiv:2202.04551v2 Announce Type: replace 
Abstract: In a 1989 paper titled "shortest paths without a map", Papadimitriou and Yannakakis introduced an online model of searching in a weighted layered graph for a target node, while attempting to minimize the total length of the path traversed by the searcher. This problem, later called layered graph traversal, is parametrized by the maximum cardinality $k$ of a layer of the input graph. It is an online setting for dynamic programming, and it is known to be a rather general and fundamental model of online computing, which includes as special cases other acclaimed models. The deterministic competitive ratio for this problem was soon discovered to be exponential in $k$, and it is now nearly resolved: it lies between $\Omega(2^k)$ and $O(k2^k)$. Regarding the randomized competitive ratio, in 1993 Ramesh proved, surprisingly, that this ratio has to be at least $\Omega(k^2 / \log^{1+\epsilon} k)$ (for any constant $\epsilon &gt; 0$). In the same paper, Ramesh also gave an $O(k^{13})$-competitive randomized online algorithm. Between 1993 and the results obtained in this paper, no progress has been reported on the randomized competitive ratio of layered graph traversal. In this work we show how to apply the mirror descent framework on a carefully selected evolving metric space, and obtain an $O(k^2)$-competitive randomized online algorithm. This matches asymptotically an improvement of the aforementioned lower bound (Bubeck, Coester, Rabani; STOC 2023), which we announced (among other results) after the initial publication of the results here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.04551v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'ebastien Bubeck, Christian Coester, Yuval Rabani</dc:creator>
    </item>
    <item>
      <title>Approximating optimization problems in graphs with locational uncertainty</title>
      <link>https://arxiv.org/abs/2206.08187</link>
      <description>arXiv:2206.08187v3 Announce Type: replace 
Abstract: Many combinatorial optimization problems can be formulated as the search for a subgraph that satisfies certain properties and minimizes the total weight. We assume here that the vertices correspond to points in a metric space and can take any position in given uncertainty sets. Then, the cost function to be minimized is the sum of the distances for the worst positions of the vertices in their uncertainty sets. We propose two types of polynomial-time approximation algorithms. The first one relies on solving a deterministic counterpart of the problem where the uncertain distances are replaced with maximum pairwise distances. We study in details the resulting approximation ratio, which depends on the structure of the feasible subgraphs and whether the metric space is Ptolemaic or not. The second algorithm is a fully-polynomial time approximation scheme for the special case of $s-t$ paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08187v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marin Bougeret, J\'er\'emy Omer, Michael Poss</dc:creator>
    </item>
    <item>
      <title>Maximum $k$- vs. $\ell$-colourings of graphs</title>
      <link>https://arxiv.org/abs/2311.00440</link>
      <description>arXiv:2311.00440v5 Announce Type: replace 
Abstract: We present polynomial-time SDP-based algorithms for the following problem: For fixed $k \leq \ell$, given a real number $\epsilon&gt;0$ and a graph $G$ that admits a $k$-colouring with a $\rho$-fraction of the edges coloured properly, it returns an $\ell$-colouring of $G$ with an $(\alpha \rho - \epsilon)$-fraction of the edges coloured properly in polynomial time in $G$ and $1 / \epsilon$. Our algorithms are based on the algorithms of Frieze and Jerrum [Algorithmica'97] and of Karger, Motwani and Sudan [JACM'98].
  When $k$ is fixed and $\ell$ grows large, our algorithm achieves an approximation ratio of $\alpha = 1 - o(1 / \ell)$. When $k, \ell$ are both large, our algorithm achieves an approximation ratio of $\alpha = 1 - 1 / \ell + 2 \ln \ell / k \ell - o(\ln \ell / k \ell) - O(1 / k^2)$; if we fix $d = \ell - k$ and allow $k, \ell$ to grow large, this is $\alpha = 1 - 1 / \ell + 2 \ln \ell / k \ell - o(\ln \ell / k \ell)$.
  By extending the results of Khot, Kindler, Mossel and O'Donnell [SICOMP'07] to the promise setting, we show that for large $k$ and $\ell$, assuming Khot's Unique Games Conjecture (\UGC), it is \NP-hard to achieve an approximation ratio $\alpha$ greater than $1 - 1 / \ell + 2 \ln \ell / k \ell + o(\ln \ell / k \ell)$, provided that $\ell$ is bounded by a function that is $o(\exp(\sqrt[3]{k}))$. For the case where $d = \ell - k$ is fixed, this bound matches the performance of our algorithm up to $o(\ln \ell / k \ell)$. Furthermore, by extending the results of Guruswami and Sinop [ToC'13] to the promise setting, we prove that it is \NP-hard to achieve an approximation ratio greater than $1 - 1 / \ell + 8 \ln \ell / k \ell + o(\ln \ell / k \ell)$, provided again that $\ell$ is bounded as before (but this time without assuming the \UGC).</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00440v5</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tamio-Vesa Nakajima, Stanislav \v{Z}ivn\'y</dc:creator>
    </item>
    <item>
      <title>Investigating Methods for Weighted Reservoir Sampling with Replacement</title>
      <link>https://arxiv.org/abs/2403.20256</link>
      <description>arXiv:2403.20256v4 Announce Type: replace 
Abstract: Reservoir sampling techniques can be used to extract a sample from a population of unknown size, where units are observed sequentially. Most of attention has been placed to sampling without replacement, with only a small number of studies focusing on sampling with replacement. In this paper, we clarify some statements appearing in the literature about the reduction of reservoir sampling with replacement to single reservoir sampling without replacement, exploring in detail how to deal with the weighted case. Then, we demonstrate that the results shown in Park et al. (2004) can be further generalized to develop a skip-based algorithm more efficient than previous methods, and, additionally, we provide a single-pass merging strategy which can be executed on multiple streams in parallel. Finally, we establish that the skip-based algorithm is faster than standard methods when used to extract a single sample from the population in a non-streaming scenario when the sample ratio is approximately less than 10% of the population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20256v4</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adriano Meligrana</dc:creator>
    </item>
    <item>
      <title>L'algorithme : pourquoi et comment le d{\'e}finir pour l'enseigner</title>
      <link>https://arxiv.org/abs/2406.04385</link>
      <description>arXiv:2406.04385v3 Announce Type: replace 
Abstract: The question of the definition of what is an algorithm is recurrent. It is found in teaching, at different levels and particularly in secondary education because of the recent evolutions in high school, with immediate consequences in higher education. It is found in mediation, with the different meanings that the word ``algorithm'' is charged with in the media space. It is also found in research, with issues in different branches of computer science, from foundations in computability and complexity to applications in big data. Beyond the issue of definition, it is the raison d'{\^e}tre of the notion of algorithm that should be questioned: what do we want to do with it and what is at stake? It is by trying to specify this that we can identify didactic elements that are likely to help teach the algorithm, in interaction with mathematics or not, and to different audiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04385v3</guid>
      <category>cs.DS</category>
      <category>math.HO</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emmanuel Beffara (MeTAH, LIG, IREM)</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic Graph Algorithms with Edge Differential Privacy</title>
      <link>https://arxiv.org/abs/2409.17623</link>
      <description>arXiv:2409.17623v2 Announce Type: replace 
Abstract: We study differentially private algorithms for analyzing graphs in the challenging setting of continual release with fully dynamic updates, where edges are inserted and deleted over time, and the algorithm is required to update the solution at every time step. Previous work has presented differentially private algorithms for many graph problems that can handle insertions only or deletions only (called partially dynamic algorithms) and obtained some hardness results for the fully dynamic setting. The only algorithms in the latter setting were for the edge count, given by Fichtenberger, Henzinger, and Ost (ESA 21), and for releasing the values of all graph cuts, given by Fichtenberger, Henzinger, and Upadhyay (ICML 23). We provide the first differentially private and fully dynamic graph algorithms for several other fundamental graph statistics (including the triangle count, the number of connected components, the size of the maximum matching, and the degree histogram), analyze their error and show strong lower bounds on the error for all algorithms in this setting. We study two variants of edge differential privacy for fully dynamic graph algorithms: event-level and item-level. We give upper and lower bounds on the error of both event-level and item-level fully dynamic algorithms for several fundamental graph problems. No fully dynamic algorithms that are private at the item-level (the more stringent of the two notions) were known before. In the case of item-level privacy, for several problems, our algorithms match our lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17623v2</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sofya Raskhodnikova, Teresa Anna Steiner</dc:creator>
    </item>
  </channel>
</rss>
