<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Oct 2024 04:01:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Level of Scientific Readiness with Ternary Data Types</title>
      <link>https://arxiv.org/abs/2410.09073</link>
      <description>arXiv:2410.09073v1 Announce Type: new 
Abstract: In addition to the technology readiness level (TRL the scientific readiness level (SRL) has been introduced as a more authentic and adequate tool for determining the status quo of scientific and scientific-technical projects of fundamental or applied nature. The SRL includes 10 levels of scientific readiness, namely, the FRL (Fundamental Readiness Level), the ARL (Applied Readiness Level), and the IRL (Innovation Readiness Level). For quantitative and visual assessment of the level of scientific readiness, a system of positional ternary (three-valued, trinary) codes with integer trits (ternary digit) is introduced. The ternary code interprets the degree of project elaboration according to the fundamental, applied, and innovation vectors/trits FRL/ARL/IRL. Examples of scientific project assessments are provided. The main characteristics of the new scale and assessment of the level of readiness and the application areas of the scientific readiness level are noted. Tasks for further development of the system of scientific readiness levels are formulated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09073v1</guid>
      <category>cs.DL</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eldar Knar</dc:creator>
    </item>
    <item>
      <title>AI in Archival Science -- A Systematic Review</title>
      <link>https://arxiv.org/abs/2410.09086</link>
      <description>arXiv:2410.09086v1 Announce Type: new 
Abstract: The rapid expansion of records creates significant challenges in management, including retention and disposition, appraisal, and organization. Our study underscores the benefits of integrating artificial intelligence (AI) within the broad realm of archival science. In this work, we start by performing a thorough analysis to understand the current use of AI in this area and identify the techniques employed to address challenges. Subsequently, we document the results of our review according to specific criteria. Our findings highlight key AI driven strategies that promise to streamline record-keeping processes and enhance data retrieval efficiency. We also demonstrate our review process to ensure transparency regarding our methodology. Furthermore, this review not only outlines the current state of AI in archival science and records management but also lays the groundwork for integrating new techniques to transform archival practices. Our research emphasizes the necessity for enhanced collaboration between the disciplines of artificial intelligence and archival science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09086v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaurav Shinde, Tiana Kirstein, Souvick Ghosh, Patricia C. Franks</dc:creator>
    </item>
    <item>
      <title>Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research</title>
      <link>https://arxiv.org/abs/2410.09090</link>
      <description>arXiv:2410.09090v1 Announce Type: new 
Abstract: Bibliometric analysis is essential for understanding research trends, scope, and impact in urban science, especially in high-impact journals, such Nature Portfolios. However, traditional methods, relying on keyword searches and basic NLP techniques, often fail to uncover valuable insights not explicitly stated in article titles or keywords. These approaches are unable to perform semantic searches and contextual understanding, limiting their effectiveness in classifying topics and characterizing studies. In this paper, we address these limitations by leveraging Generative AI models, specifically transformers and Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric analysis. We developed a technical workflow that integrates a vector database, Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and Large Language Models (LLMs) to enable contextual search, topic ranking, and characterization of research using customized prompt templates. A pilot study analyzing 223 urban science-related articles published in Nature Communications over the past decade highlights the effectiveness of our approach in generating insightful summary statistics on the quality, scope, and characteristics of papers in high-impact journals. This study introduces a new paradigm for enhancing bibliometric analysis and knowledge retrieval in urban research, positioning an AI agent as a powerful tool for advancing research evaluation and understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09090v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3681780.3697252</arxiv:DOI>
      <dc:creator>Haowen Xu (Jamie), Xueping Li (Jamie), Jose Tupayachi (Jamie),  Jianming (Jamie),  Lian, Femi Omitaomu</dc:creator>
    </item>
    <item>
      <title>HLM-Cite: Hybrid Language Model Workflow for Text-based Scientific Citation Prediction</title>
      <link>https://arxiv.org/abs/2410.09112</link>
      <description>arXiv:2410.09112v1 Announce Type: new 
Abstract: Citation networks are critical in modern science, and predicting which previous papers (candidates) will a new paper (query) cite is a critical problem. However, the roles of a paper's citations vary significantly, ranging from foundational knowledge basis to superficial contexts. Distinguishing these roles requires a deeper understanding of the logical relationships among papers, beyond simple edges in citation networks. The emergence of LLMs with textual reasoning capabilities offers new possibilities for discerning these relationships, but there are two major challenges. First, in practice, a new paper may select its citations from gigantic existing papers, where the texts exceed the context length of LLMs. Second, logical relationships between papers are implicit, and directly prompting an LLM to predict citations may result in surface-level textual similarities rather than the deeper logical reasoning. In this paper, we introduce the novel concept of core citation, which identifies the critical references that go beyond superficial mentions. Thereby, we elevate the citation prediction task from a simple binary classification to distinguishing core citations from both superficial citations and non-citations. To address this, we propose $\textbf{HLM-Cite}$, a $\textbf{H}$ybrid $\textbf{L}$anguage $\textbf{M}$odel workflow for citation prediction, which combines embedding and generative LMs. We design a curriculum finetune procedure to adapt a pretrained text embedding model to coarsely retrieve high-likelihood core citations from vast candidates and then design an LLM agentic workflow to rank the retrieved papers through one-shot reasoning, revealing the implicit relationships among papers. With the pipeline, we can scale the candidate sets to 100K papers. We evaluate HLM-Cite across 19 scientific fields, demonstrating a 17.6% performance improvement comparing SOTA methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09112v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qianyue Hao, Jingyang Fan, Fengli Xu, Jian Yuan, Yong Li</dc:creator>
    </item>
    <item>
      <title>Scito2M: A 2 Million, 30-Year Cross-disciplinary Dataset for Temporal Scientometric Analysis</title>
      <link>https://arxiv.org/abs/2410.09510</link>
      <description>arXiv:2410.09510v1 Announce Type: new 
Abstract: Understanding the creation, evolution, and dissemination of scientific knowledge is crucial for bridging diverse subject areas and addressing complex global challenges such as pandemics, climate change, and ethical AI. Scientometrics, the quantitative and qualitative study of scientific literature, provides valuable insights into these processes. We introduce Scito2M, a longitudinal scientometric dataset with over two million academic publications, providing comprehensive contents information and citation graphs to support cross-disciplinary analyses. Using Scito2M, we conduct a temporal study spanning over 30 years to explore key questions in scientometrics: the evolution of academic terminology, citation patterns, and interdisciplinary knowledge exchange. Our findings reveal critical insights, such as disparities in epistemic cultures, knowledge production modes, and citation practices. For example, rapidly developing, application-driven fields like LLMs exhibit significantly shorter citation age (2.48 years) compared to traditional theoretical disciplines like oral history (9.71 years).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09510v1</guid>
      <category>cs.DL</category>
      <category>cs.CL</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqiao Jin, Yijia Xiao, Yiyang Wang, Jindong Wang</dc:creator>
    </item>
    <item>
      <title>Navigating Discoverability in the Digital Era: A Theoretical Framework</title>
      <link>https://arxiv.org/abs/2410.09917</link>
      <description>arXiv:2410.09917v1 Announce Type: new 
Abstract: The proliferation of digital technologies in the distribution of digital content has prompted concerns about the effects on cultural diversity in the digital era. The concept of discoverability has been presented as a theoretical tool through which to consider the likelihood that content will be interacted with. The multifaceted nature of this broad theme has been explored through a variety of domains that explore the ripple effects of platformization, each with its own unique lexicography. However, there is yet to be a unified framework through which to consider the complex pathways of discovery. In this work we present the discovery ecosystem, consisting of six individual, interconnected components, that encompass the pathway of discovery from start to finish</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09917v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Rebecca Salganik, Valdy Wiratama, Heritiana Ranaivoson, Adelaida Afilipoaie</dc:creator>
    </item>
    <item>
      <title>SUS-Lib: An automated tool for usability evaluation based on the Software Usability Scale from user feedback</title>
      <link>https://arxiv.org/abs/2410.09534</link>
      <description>arXiv:2410.09534v1 Announce Type: cross 
Abstract: Usability evaluation has received considerable attention from both the research and practice communities. While there are many evaluation tools available, the Software Usability Scale (SUS) is the most widely used. In this paper, we introduce and describe the SUS-Lib software package, which aims to compute SUS scores and generate graphical figures based on user input. SUS-Lib responds to the need for user-friendly software that requires only basic knowledge and skills of the Python environment and command line tools. By using open source solutions and low hardware resources, SUS-Lib is a cost-effective solution. In addition, due to its generic nature, SUS-Lib can also be used in different research setups and settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09534v1</guid>
      <category>cs.SE</category>
      <category>cs.DL</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pawe{\l} Weichbroth, Ma{\l}gorzata Giedrowicz</dc:creator>
    </item>
    <item>
      <title>'Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated peer-reviews</title>
      <link>https://arxiv.org/abs/2410.09770</link>
      <description>arXiv:2410.09770v1 Announce Type: cross 
Abstract: The integrity of the peer-review process is vital for maintaining scientific rigor and trust within the academic community. With the steady increase in the usage of large language models (LLMs) like ChatGPT in academic writing, there is a growing concern that AI-generated texts could compromise scientific publishing, including peer-reviews. Previous works have focused on generic AI-generated text detection or have presented an approach for estimating the fraction of peer-reviews that can be AI-generated. Our focus here is to solve a real-world problem by assisting the editor or chair in determining whether a review is written by ChatGPT or not. To address this, we introduce the Term Frequency (TF) model, which posits that AI often repeats tokens, and the Review Regeneration (RR) model, which is based on the idea that ChatGPT generates similar outputs upon re-prompting. We stress test these detectors against token attack and paraphrasing. Finally, we propose an effective defensive strategy to reduce the effect of paraphrasing on our models. Our findings suggest both our proposed methods perform better than the other AI text detectors. Our RR model is more robust, although our TF model performs better than the RR model without any attacks. We make our code, dataset, and model public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09770v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sandeep Kumar, Mohit Sahu, Vardhan Gacche, Tirthankar Ghosal, Asif Ekbal</dc:creator>
    </item>
    <item>
      <title>A Comparative Study of PDF Parsing Tools Across Diverse Document Categories</title>
      <link>https://arxiv.org/abs/2410.09871</link>
      <description>arXiv:2410.09871v1 Announce Type: cross 
Abstract: PDF is one of the most prominent data formats, making PDF parsing crucial for information extraction and retrieval, particularly with the rise of RAG systems. While various PDF parsing tools exist, their effectiveness across different document types remains understudied, especially beyond academic papers. Our research aims to address this gap by comparing 10 popular PDF parsing tools across 6 document categories using the DocLayNet dataset. These tools include PyPDF, pdfminer.six, PyMuPDF, pdfplumber, pypdfium2, Unstructured, Tabula, Camelot, as well as the deep learning-based tools Nougat and Table Transformer(TATR). We evaluated both text extraction and table detection capabilities. For text extraction, PyMuPDF and pypdfium generally outperformed others, but all parsers struggled with Scientific and Patent documents. For these challenging categories, learning-based tools like Nougat demonstrated superior performance. In table detection, TATR excelled in the Financial, Patent, Law &amp; Regulations, and Scientific categories. Table detection tool Camelot performed best for tender documents, while PyMuPDF performed superior in the Manual category. Our findings highlight the importance of selecting appropriate parsing tools based on document type and specific tasks, providing valuable insights for researchers and practitioners working with diverse document sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09871v1</guid>
      <category>cs.IR</category>
      <category>cs.DL</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Narayan S. Adhikari, Shradha Agarwal</dc:creator>
    </item>
    <item>
      <title>Enhancing Peer Review in Astronomy: A Machine Learning and Optimization Approach to Reviewer Assignments for ALMA</title>
      <link>https://arxiv.org/abs/2410.10009</link>
      <description>arXiv:2410.10009v1 Announce Type: cross 
Abstract: The increasing volume of papers and proposals undergoing peer review emphasizes the pressing need for greater automation to effectively manage the growing scale. In this study, we present the deployment and evaluation of machine learning and optimization techniques for assigning proposals to reviewers that was developed for the Atacama Large Millimeter/submillimeter Array (ALMA) during the Cycle 10 Call for Proposals issued in 2023. By utilizing topic modeling algorithms, we identify the proposal topics and assess reviewers' expertise based on their historical ALMA proposal submissions. We then apply an adapted version of the assignment optimization algorithm from PeerReview4All (Stelmakh et al. 2021a) to maximize the alignment between proposal topics and reviewer expertise. Our evaluation shows a significant improvement in matching reviewer expertise: the median similarity score between the proposal topic and reviewer expertise increased by 51 percentage points compared to the previous cycle, and the percentage of reviewers reporting expertise in their assigned proposals rose by 20 percentage points. Furthermore, the assignment process proved highly effective in that no proposals required reassignment due to significant mismatches, resulting in a savings of 3 to 5 days of manual effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10009v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John M. Carpenter, Andrea Corvill\'on, Nihar B. Shah</dc:creator>
    </item>
    <item>
      <title>A Capability Maturity Model for Urban Dataset Meta-data</title>
      <link>https://arxiv.org/abs/2402.05211</link>
      <description>arXiv:2402.05211v5 Announce Type: replace 
Abstract: In the current environment of data generation and publication, there is an ever-growing number of datasets available for download. This growth precipitates an existing challenge: sourcing and integrating relevant datasets for analysis is becoming more complex. Despite efforts by open data platforms, obstacles remain, predominantly rooted in inadequate metadata, unsuitable data presentation, complications in pinpointing desired data, and data integration. This paper delves into the intricacies of dataset retrieval, emphasizing the pivotal role of metadata in aligning datasets with user queries. Through an exploration of existing literature, it underscores prevailing issues such as the identification of valuable metadata and the development of tools to maintain and annotate them effectively. The central contribution of this research is the proposition of a dataset metadata maturity model. Deriving inspiration from software engineering maturity models, this framework delineates a progression from rudimentary metadata documentation to advanced levels, aiding dataset creators in their documentation efforts. The model encompasses seven pivotal dimensions, spanning content to quality information, each stratified across five maturity levels to guide the optimal documentation of datasets, ensuring ease of discovery, relevance assessment, and comprehensive dataset understanding. This paper also incorporates the maturity model into a data cataloguing tool called CKAN through a custom plugin, CKANext-udc. The plugin introduces custom fields based on different maturity levels, allows for user interface customisation, and integrates with a graph database, converting catalogue data into a knowledge graph based on the Maturity Model ontology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05211v5</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark S. Fox, Bart Gajderowicz, Dishu Lyu</dc:creator>
    </item>
    <item>
      <title>Data Science from 1963 to 2012</title>
      <link>https://arxiv.org/abs/2311.03292</link>
      <description>arXiv:2311.03292v3 Announce Type: replace-cross 
Abstract: Consensus on the definition of data science remains low despite the widespread establishment of academic programs in the field and continued demand for data scientists in industry. Definitions range from rebranded statistics to data-driven science to the science of data to simply the application of machine learning to so-called big data to solve real-world problems. Current efforts to trace the history of the field in order to clarify its definition, such as Donoho's "50 Years of Data Science" (Donoho 2017), tend to focus on a short period when a small group of statisticians adopted the term in an unsuccessful attempt to rebrand their field in the face of the overshadowing effects of computational statistics and data mining. Using textual evidence from primary sources, this essay traces the history of the term to the 1960s, when it was first used by the US Air Force in a surprisingly similar way to its current usage, to 2012, the year that Harvard Business Review published the enormously influential article "Data Scientist: The Sexiest Job of the 21st Century" (Davenport and Patil 2012) and the American Statistical Association acknowledged a profound disconnect between statistics and data science (Rodriguez 2012). Among the themes that emerge from this review are (1) the long-standing opposition between data analysts and data miners that continues to animate the field, (2) an established definition of the term as the practice of managing and processing scientific data that has been occluded by recent usage, and (3) the phenomenon of data impedance -- the disproportion between surplus data, indexed by phrases like data deluge and big data, and the limitations of computational machinery and methods to process them. This persistent condition appears to have motivated the use of the term and the field itself since its beginnings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03292v3</guid>
      <category>cs.GL</category>
      <category>cs.DL</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rafael C. Alvarado</dc:creator>
    </item>
    <item>
      <title>SETI in 2022</title>
      <link>https://arxiv.org/abs/2410.08253</link>
      <description>arXiv:2410.08253v2 Announce Type: replace-cross 
Abstract: In this third installment of SETI in 20xx, we very briefly and subjectively review developments in SETI in 2022. Our primary focus is 80 papers and books published or made public in 2022, which we sort into six broad categories: results from actual searches, new search methods and instrumentation, target and frequency selection, the development of technosignatures, theory of ETIs, and social aspects of SETI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08253v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.EP</category>
      <category>astro-ph.SR</category>
      <category>cs.DL</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.actaastro.2024.09.072</arxiv:DOI>
      <dc:creator>Jason T. Wright, Macy Huston, Aidan Groenendaal, Lennon Nichol, Nick Tusay</dc:creator>
    </item>
  </channel>
</rss>
