<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Nov 2024 05:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hardness Results on Characteristics for Elastic-Degenerated Strings</title>
      <link>https://arxiv.org/abs/2411.10653</link>
      <description>arXiv:2411.10653v1 Announce Type: new 
Abstract: Generalizations of plain strings have been proposed as a compact way to represent a collection of nearly identical sequences or to express uncertainty at specific text positions by enumerating all possibilities. While a plain string stores a character at each of its positions, generalizations consider a set of characters (indeterminate strings), a set of strings of equal length (generalized degenerate strings, or shortly GD strings), or a set of strings of arbitrary lengths (elastic-degenerate strings, or shortly ED strings). These generalizations are of importance to compactly represent such type of data, and find applications in bioinformatics for representing and maintaining a set of genetic sequences of the same taxonomy or a multiple sequence alignment. To be of use, attention has been drawn to answering various query types such as pattern matching or measuring similarity of ED strings by generalizing techniques known to plain strings. However, for some types of queries, it has been shown that a generalization of a polynomial-time solvable query on classic strings becomes NP-hard on ED strings, e.g. [Russo et al.,2022]. In that light, we wonder about other types of queries, which are of particular interest to bioinformatics: the search for the longest repeating factor, unique substrings, absent words, anti-powers, and longest previous factors. While we obtain a polynomial time algorithm for the first problem on ED strings, we show that all others are NP-hard to compute, some of them even under the restriction that the input can be modelled as an indeterminate or GD string.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10653v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominik K\"oppl, Jannik Olbrich</dc:creator>
    </item>
    <item>
      <title>Differentiable Extensions with Rounding Guarantees for Combinatorial Optimization over Permutations</title>
      <link>https://arxiv.org/abs/2411.10707</link>
      <description>arXiv:2411.10707v1 Announce Type: new 
Abstract: We present Birkhoff Extension (BE), an almost-everywhere-differentiable continuous polytime-computable extension of any real-valued function on permutations to doubly stochastic matrices. Our approach is based on Birkhoff decomposition (also referred to as Birkhoff von-Neumann decomposition) which allows construction of an extension that is always a convex combination of the objective's values at permutations. We show how to construct a specific family of Birkhoff decompositions that are continuous. In addition to continuity, our extension has several nice properties making it appealing for optimization problems. First, BE provides a rounding guarantee, namely any solution to the extension can be efficiently rounded to a permutation without increasing the function value. Furthermore, an approximate solution in the relaxed case (with extension) will give rise to an approximate solution in the space of permutations. Second, using BE, any real-valued optimization objective on permutations can be extended to an almost everywhere differentiable objective function over the space of doubly stochastic matrices. This makes our BE amenable to not only gradient-descent based optimizations, but also unsupervised neural combinatorial optimization where training often requires a differentiable loss. Third, based on the above properties, we present a simple optimization procedure which can be readily combined with existing optimization approaches to offer local improvements (i.e., the quality of the final solution is no worse than the initial solution). We present preliminary experimental results to verify our theoretical results on several combinatorial optimization problems related to permutations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10707v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Robert R. Nerem, Zhishang Luo, Akbar Rafiey, Yusu Wang</dc:creator>
    </item>
    <item>
      <title>Maximization of Approximately Submodular Functions</title>
      <link>https://arxiv.org/abs/2411.10949</link>
      <description>arXiv:2411.10949v1 Announce Type: new 
Abstract: We study the problem of maximizing a function that is approximately submodular under a cardinality constraint. Approximate submodularity implicitly appears in a wide range of applications as in many cases errors in evaluation of a submodular function break submodularity. Say that $F$ is $\varepsilon$-approximately submodular if there exists a submodular function $f$ such that $(1-\varepsilon)f(S) \leq F(S)\leq (1+\varepsilon)f(S)$ for all subsets $S$. We are interested in characterizing the query-complexity of maximizing $F$ subject to a cardinality constraint $k$ as a function of the error level $\varepsilon&gt;0$. We provide both lower and upper bounds: for $\varepsilon&gt;n^{-1/2}$ we show an exponential query-complexity lower bound. In contrast, when $\varepsilon&lt; {1}/{k}$ or under a stronger bounded curvature assumption, we give constant approximation algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10949v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Advances in Neural Information Processing Systems 29, NeurIPS 2016, pp. 3045-3053</arxiv:journal_reference>
      <dc:creator>Thibaut Horel, Yaron Singer</dc:creator>
    </item>
    <item>
      <title>Fault-Equivalent Lowest Common Ancestors</title>
      <link>https://arxiv.org/abs/2411.11049</link>
      <description>arXiv:2411.11049v1 Announce Type: new 
Abstract: Let $T$ be a rooted tree in which a set $M$ of vertices are marked. The lowest common ancestor (LCA) of $M$ is the unique vertex $\ell$ with the following property: after failing (i.e., deleting) any single vertex $x$ from $T$, the root remains connected to $\ell$ if and only if it remains connected to some marked vertex. In this note, we introduce a generalized notion called $f$-fault-equivalent LCAs ($f$-FLCA), obtained by adapting the above view to $f$ failures for arbitrary $f \geq 1$. We show that there is a unique vertex set $M^* = \operatorname{FLCA}(M,f)$ of minimal size such after the failure of any $f$ vertices (or less), the root remains connected to some $v \in M$ iff it remains connected to some $u \in M^*$. Computing $M^*$ takes linear time. A bound of $|M^*| \leq 2^{f-1}$ always holds, regardless of $|M|$, and holds with equality for some choice of $T$ and $M$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11049v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asaf Petruschka</dc:creator>
    </item>
    <item>
      <title>Amortized Analysis of Leftist Heaps</title>
      <link>https://arxiv.org/abs/2411.11051</link>
      <description>arXiv:2411.11051v1 Announce Type: new 
Abstract: Leftist heaps and skew heaps are two well-known data structures for mergeable priority queues. Leftist heaps are constructed for efficiency in the worst-case sense whereas skew heaps are self-adjusting, designed for efficiency in the amortized sense. In this paper, we analyze the amortized complexity of leftist heaps to initiate a full performance comparison with skew heaps. We consider both the leftist heaps originally developed by Crane and Knuth, which are also referred to as rank-biased (or, height-biased) leftist heaps, and the weight-biased leftist heaps introduced by Cho and Sahni. We show how weight-biased leftist heaps satisfy the same exact amortized bounds as skew heaps. With these matching bounds we establish a nice trade-off in which storage of weights is used to limit the worst-case complexity of leftist heaps, without affecting the amortized complexity compared to skew heaps. For rank-biased leftist heaps, we obtain the same amortized lower bounds as for skew heaps, but whether these bounds are tight is left as an open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11051v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-75783-9_3</arxiv:DOI>
      <arxiv:journal_reference>Principles of Verification: Cycling the Probabilistic Landscape, Part I, LNCS 15260, Springer 2024, pp. 73-84</arxiv:journal_reference>
      <dc:creator>Berry Schoenmakers</dc:creator>
    </item>
    <item>
      <title>Connectivity Certificate against Bounded-Degree Faults: Simpler, Better and Supporting Vertex Faults</title>
      <link>https://arxiv.org/abs/2411.11054</link>
      <description>arXiv:2411.11054v1 Announce Type: new 
Abstract: An $f$-edge (or vertex) connectivity certificate is a sparse subgraph that maintains connectivity under the failure of at most $f$ edges (or vertices). It is well known that any $n$-vertex graph admits an $f$-edge (or vertex) connectivity certificate with $\Theta(f n)$ edges (Nagamochi and Ibaraki, Algorithmica 1992). A recent work by (Bodwin, Haeupler and Parter, SODA 2024) introduced a new and considerably stronger variant of connectivity certificates that can preserve connectivity under any failing set of edges with bounded degree. For every $n$-vertex graph $G=(V,E)$ and a degree threshold $f$, an $f$-Edge-Faulty-Degree (EFD) certificate is a subgraph $H \subseteq G$ with the following guarantee: For any subset $F \subseteq E$ with $deg(F)\leq f$ and every pair $u,v \in V$, $u$ and $v$ are connected in $H - F$ iff they are connected in $G - F$. For example, a $1$-EFD certificate preserves connectivity under the failing of any matching edge set $F$ (hence, possibly $|F|=\Theta(n)$). In their work, [BHP'24] presented an expander-based approach (e.g., using the tools of expander decomposition and expander routing) for computing $f$-EFD certificates with $O(f n \cdot poly(\log n))$ edges. They also provided a lower bound of $\Omega(f n\cdot \log_f n)$, hence $\Omega(n\log n)$ for $f=O(1)$. In this work, we settle the optimal existential size bounds for $f$-EFD certificates (up to constant factors), and also extend it to support vertex failures with bounded degrees (where each vertex is incident to at most $f$ faulty vertices). Specifically, we show that for every $n&gt;f/2$, any $n$-vertex graph admits an $f$-EFD (and $f$-VFD) certificates with $O(f n \cdot \log(n/f))$ edges and that this bound is tight. Our upper bound arguments are considerably simpler compared to prior work, do not use expanders, and only exploit the basic structure of bounded degree edge and vertex cuts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11054v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Merav Parter, Elad Tzalik</dc:creator>
    </item>
    <item>
      <title>Approximation algorithms for non-sequential star packing problems</title>
      <link>https://arxiv.org/abs/2411.11136</link>
      <description>arXiv:2411.11136v1 Announce Type: new 
Abstract: For a positive integer $k \ge 1$, a $k$-star ($k^+$-star, $k^-$-star, respectively) is a connected graph containing a degree-$\ell$ vertex and $\ell$ degree-$1$ vertices, where $\ell = k$ ($\ell \ge k$, $1 \le \ell \le k$, respectively). The $k^+$-star packing problem is to cover as many vertices of an input graph $G$ as possible using vertex-disjoint $k^+$-stars in $G$; and given $k &gt; t \ge 1$, the $k^-/t$-star packing problem is to cover as many vertices of $G$ as possible using vertex-disjoint $k^-$-stars but no $t$-stars in $G$. Both problems are NP-hard for any fixed $k \ge 2$. We present a $(1 + \frac {k^2}{2k+1})$- and a $\frac 32$-approximation algorithms for the $k^+$-star packing problem when $k \ge 3$ and $k = 2$, respectively, and a $(1 + \frac 1{t + 1 + 1/k})$-approximation algorithm for the $k^-/t$-star packing problem when $k &gt; t \ge 2$. They are all local search algorithms and they improve the best known approximation algorithms for the problems, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11136v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mengyuan Hu, An Zhang, Yong Chen, Mingyang Gong, Guohui Lin</dc:creator>
    </item>
    <item>
      <title>Massively Parallel Maximum Coverage Revisited</title>
      <link>https://arxiv.org/abs/2411.11277</link>
      <description>arXiv:2411.11277v1 Announce Type: new 
Abstract: We study the maximum set coverage problem in the massively parallel model. In this setting, $m$ sets that are subsets of a universe of $n$ elements are distributed among $m$ machines. In each round, these machines can communicate with each other, subject to the memory constraint that no machine may use more than $\tilde{O}(n)$ memory. The objective is to find the $k$ sets whose coverage is maximized. We consider the regime where $k = \Omega(m)$, $m = O(n)$, and each machine has $\tilde{O}(n)$ memory. Maximum coverage is a special case of the submodular maximization problem subject to a cardinality constraint. This problem can be approximated to within a $1-1/e$ factor using the greedy algorithm, but this approach is not directly applicable to parallel and distributed models. When $k = \Omega(m)$, to obtain a $1-1/e-\epsilon$ approximation, previous work either requires $\tilde{O}(mn)$ memory per machine which is not interesting compared to the trivial algorithm that sends the entire input to a single machine, or requires $2^{O(1/\epsilon)} n$ memory per machine which is prohibitively expensive even for a moderately small value $\epsilon$. Our result is a randomized $(1-1/e-\epsilon)$-approximation algorithm that uses $O(1/\epsilon^3 \cdot \log m \cdot (\log (1/\epsilon) + \log m))$ rounds. Our algorithm involves solving a slightly transformed linear program of the maximum coverage problem using the multiplicative weights update method, classic techniques in parallel computing such as parallel prefix, and various combinatorial arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11277v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thai Bui, Hoa T. Vu</dc:creator>
    </item>
    <item>
      <title>SpiderDAN: Matching Augmentation in Demand-Aware Networks</title>
      <link>https://arxiv.org/abs/2411.11426</link>
      <description>arXiv:2411.11426v1 Announce Type: new 
Abstract: Graph augmentation is a fundamental and well-studied problem that arises in network optimization. We consider a new variant of this model motivated by reconfigurable communication networks. In this variant, we consider a given physical network and the measured communication demands between the nodes. Our goal is to augment the given physical network with a matching, so that the shortest path lengths in the augmented network, weighted with the demands, are minimal.We prove that this problem is NP-hard, even if the physical network is a cycle. We then use results from demand-aware network design to provide a constant-factor approximation algorithm for adding a matching in case that only a few nodes in the network cause almost all the communication. For general real-world communication patterns, we design and evaluate a series of heuristics that can deal with arbitrary graphs as the underlying network structure. Our algorithms are validated experimentally using real-world traces (from e.g., Facebook) of data centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11426v1</guid>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleksander Figiel, Darya Melnyk, Andr\'e Nichterlein, Arash Pourdamghani, Stefan Schmid</dc:creator>
    </item>
    <item>
      <title>The Complexity Landscape of Dynamic Distributed Subgraph Finding</title>
      <link>https://arxiv.org/abs/2411.11544</link>
      <description>arXiv:2411.11544v1 Announce Type: new 
Abstract: Bonne and Censor-Hillel (ICALP 2019) initiated the study of distributed subgraph finding in dynamic networks of limited bandwidth. For the case where the target subgraph is a clique, they determined the tight bandwidth complexity bounds in nearly all settings. However, several open questions remain, and very little is known about finding subgraphs beyond cliques. In this work, we consider these questions and explore subgraphs beyond cliques.
  For finding cliques, we establish an $\Omega(\log \log n)$ bandwidth lower bound for one-round membership-detection under edge insertions only and an $\Omega(\log \log \log n)$ bandwidth lower bound for one-round detection under both edge insertions and node insertions. Moreover, we demonstrate new algorithms to show that our lower bounds are tight in bounded-degree networks when the target subgraph is a triangle. Prior to our work, no lower bounds were known for these problems.
  For finding subgraphs beyond cliques, we present a complete characterization of the bandwidth complexity of the membership-listing problem for every target subgraph, every number of rounds, and every type of topological change: node insertions, node deletions, edge insertions, and edge deletions. We also show partial characterizations for one-round membership-detection and listing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11544v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yi-Jun Chang, Lyuting Chen, Yanyu Chen, Gopinath Mishra, Mingyang Yang</dc:creator>
    </item>
    <item>
      <title>A Bicriterion Concentration Inequality and Prophet Inequalities for $k$-Fold Matroid Unions</title>
      <link>https://arxiv.org/abs/2411.11741</link>
      <description>arXiv:2411.11741v1 Announce Type: new 
Abstract: We investigate prophet inequalities with competitive ratios approaching $1$, seeking to generalize $k$-uniform matroids. We first show that large girth does not suffice: for all $k$, there exists a matroid of girth $\geq k$ and a prophet inequality instance on that matroid whose optimal competitive ration is $\frac{1}{2}$. Next, we show $k$-fold matroid unions do suffice: we provide a prophet inequality with competitive ratio $1-O(\sqrt{\frac{\log k}{k}})$ for any $k$-fold matroid union. Our prophet inequality follows from an online contention resolution scheme.
  The key technical ingredient in our online contention resolution scheme is a novel bicriterion concentration inequality for arbitrary monotone $1$-Lipschitz functions over independent items which may be of independent interest. Applied to our particular setting, our bicriterion concentration inequality yields "Chernoff-strength" concentration for a $1$-Lipschitz function that is not (approximately) self-bounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11741v1</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noga Alon, Nick Gravin, Tristan Pollner, Aviad Rubinstein, Hongao Wang, S. Matthew Weinberg, Qianfan Zhang</dc:creator>
    </item>
    <item>
      <title>Towards Scalable and Practical Batch-Dynamic Connectivity</title>
      <link>https://arxiv.org/abs/2411.11781</link>
      <description>arXiv:2411.11781v1 Announce Type: new 
Abstract: We study the problem of dynamically maintaining the connected components of an undirected graph subject to edge insertions and deletions. We give the first parallel algorithm for the problem which is work-efficient, supports batches of updates, runs in polylogarithmic depth, and uses only linear total space. The existing algorithms for the problem either use super-linear space, do not come with strong theoretical bounds, or are not parallel. On the empirical side, we provide the first implementation of the cluster forest algorithm, the first linear-space and poly-logarithmic update time algorithm for dynamic connectivity. Experimentally, we find that our algorithm uses up to 19.7x less space and is up to 6.2x faster than the level-set algorithm of HDT, arguably the most widely-implemented dynamic connectivity algorithm with strong theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11781v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quinten De Man, Laxman Dhulipala, Adam Karczmarz, Jakub {\L}\k{a}cki, Julian Shun, Zhongqi Wang</dc:creator>
    </item>
    <item>
      <title>iFlow: An Interactive Max-Flow/Min-Cut Algorithms Visualizer</title>
      <link>https://arxiv.org/abs/2411.10484</link>
      <description>arXiv:2411.10484v1 Announce Type: cross 
Abstract: The Max-Flow/Min-Cut problem is a fundamental tool in graph theory, with applications in many domains, including data mining, image segmentation, transportation planning, and many types of assignment problems, in addition to being an essential building block for many other algorithms. The Ford-Fulkerson Algorithm for Max-Flow/Min-Cut and its variants are therefore commonly taught in undergraduate and beginning graduate algorithms classes. However, these algorithms -- and in particular the so-called residual graphs they utilize -- often pose significant challenges for students.
  To help students achieve a deeper understanding, we developed iFlow, an interactive visualization tool for the Ford-Fulkerson Algorithm and its variants. iFlow lets users design or import flow networks, and execute the algorithm by hand. In particular, the user can select an augmentation path and amount, and then update the residual graph. The user is given detailed feedback on mistakes, and can also have iFlow auto-complete each step, to use it as a demonstration tool while still in the initial learning stages. iFlow has been made publicly available and open-sourced.
  We deployed iFlow in an undergraduate algorithms class, and collected students' self-reported learning benefits via an optional survey. All respondents considered the tool at least somewhat useful and engaging, with most rating it either as useful/engaging or very useful/engaging. Students also generally reported a significant increase in understanding of the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10484v1</guid>
      <category>cs.HC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muyang Ye, Tianrui Xia, Tianxin Zu, Qian Wang, David Kempe</dc:creator>
    </item>
    <item>
      <title>Computational Complexity of Envy-free and Exchange-stable Seat Arrangement Problems on Grid Graphs</title>
      <link>https://arxiv.org/abs/2411.10719</link>
      <description>arXiv:2411.10719v1 Announce Type: cross 
Abstract: The Seat Arrangement Problem is a problem of finding a desirable seat arrangement for given preferences of agents and a seat graph that represents a configuration of seats. In this paper, we consider decision problems of determining if an envy-free arrangement exists and an exchange-stable arrangement exists, when a seat graph is an $\ell \times m$ grid graph. When $\ell=1$, the seat graph is a path of length $m$ and both problems have been known to be NP-complete. In this paper, we extend it and show that both problems are NP-complete for any integer $\ell \geq 2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10719v1</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sota Kawase, Shuichi Miyazaki</dc:creator>
    </item>
    <item>
      <title>Classical optimization with imaginary time block encoding on quantum computers: The MaxCut problem</title>
      <link>https://arxiv.org/abs/2411.10737</link>
      <description>arXiv:2411.10737v1 Announce Type: cross 
Abstract: Finding ground state solutions of diagonal Hamiltonians is relevant for both theoretical as well as practical problems of interest in many domains such as finance, physics and computer science. These problems are typically very hard to tackle by classical computing and quantum computing could help in speeding up computations and efficiently tackling larger problems. Here we use imaginary time evolution through a new block encoding scheme to obtain the ground state of such problems and apply our method to MaxCut as an illustration. Our method, which for simplicity we call ITE-BE, requires no variational parameter optimization as all the parameters in the procedure are expressed as analytical functions of the couplings of the Hamiltonian. We demonstrate that our method can be successfully combined with other quantum algorithms such as quantum approximate optimization algorithm (QAOA). We find that the QAOA ansatz increases the post-selection success of ITE-BE, and shallow QAOA circuits, when boosted with ITE-BE, achieve better performance than deeper QAOA circuits. For the special case of the transverse initial state, we adapt our block encoding scheme to allow for a deterministic application of the first layer of the circuit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10737v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.ET</category>
      <category>math.OC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dawei Zhong, Akhil Francis, Ermal Rrapaj</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Averaging Samplers and Matrix Samplers</title>
      <link>https://arxiv.org/abs/2411.10870</link>
      <description>arXiv:2411.10870v1 Announce Type: cross 
Abstract: We present the first efficient averaging sampler that achieves asymptotically optimal randomness complexity and near-optimal sample complexity. For any $\delta &lt; \varepsilon$ and any constant $\alpha &gt; 0$, our sampler uses $m + O(\log (1 / \delta))$ random bits to output $t = O((\frac{1}{\varepsilon^2} \log \frac{1}{\delta})^{1 + \alpha})$ samples $Z_1, \dots, Z_t \in \{0, 1\}^m$ such that for any function $f: \{0, 1\}^m \to [0, 1]$, \[ \Pr\left[\left|\frac{1}{t}\sum_{i=1}^t f(Z_i) - \mathbb{E}[f]\right| \leq \varepsilon\right] \geq 1 - \delta. \] The randomness complexity is optimal up to a constant factor, and the sample complexity is optimal up to the $O((\frac{1}{\varepsilon^2} \log \frac{1}{\delta})^{\alpha})$ factor.
  Our technique generalizes to matrix samplers. A matrix sampler is defined similarly, except that $f: \{0, 1\}^m \to \mathbb{C}^{d \times d}$ and the absolute value is replaced by the spectral norm. Our matrix sampler achieves randomness complexity $m + \tilde O (\log(d / \delta))$ and sample complexity $ O((\frac{1}{\varepsilon^2} \log \frac{d}{\delta})^{1 + \alpha})$ for any constant $\alpha &gt; 0$, both near-optimal with only a logarithmic factor in randomness complexity and an additional $\alpha$ exponent on the sample complexity.
  We use known connections with randomness extractors and list-decodable codes to give applications to these objects. Specifically, we give the first extractor construction with optimal seed length up to an arbitrarily small constant factor above 1, when the min-entropy $k = \beta n$ for a large enough constant $\beta &lt; 1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10870v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyang Xun, David Zuckerman</dc:creator>
    </item>
    <item>
      <title>Efficient, Low-Regret, Online Reinforcement Learning for Linear MDPs</title>
      <link>https://arxiv.org/abs/2411.10906</link>
      <description>arXiv:2411.10906v1 Announce Type: cross 
Abstract: Reinforcement learning algorithms are usually stated without theoretical guarantees regarding their performance. Recently, Jin, Yang, Wang, and Jordan (COLT 2020) showed a polynomial-time reinforcement learning algorithm (namely, LSVI-UCB) for the setting of linear Markov decision processes, and provided theoretical guarantees regarding its running time and regret. In real-world scenarios, however, the space usage of this algorithm can be prohibitive due to a utilized linear regression step. We propose and analyze two modifications of LSVI-UCB, which alternate periods of learning and not-learning, to reduce space and time usage while maintaining sublinear regret. We show experimentally, on synthetic data and real-world benchmarks, that our algorithms achieve low space usage and running time, while not significantly sacrificing regret.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10906v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philips George John, Arnab Bhattacharyya, Silviu Maniu, Dimitrios Myrisiotis, Zhenan Wu</dc:creator>
    </item>
    <item>
      <title>The Conflict Graph Design: Estimating Causal Effects under Arbitrary Neighborhood Interference</title>
      <link>https://arxiv.org/abs/2411.10908</link>
      <description>arXiv:2411.10908v1 Announce Type: cross 
Abstract: A fundamental problem in network experiments is selecting an appropriate experimental design in order to precisely estimate a given causal effect of interest. In fact, optimal rates of estimation remain unknown for essentially all causal effects in network experiments. In this work, we propose a general approach for constructing experiment designs under network interference with the goal of precisely estimating a pre-specified causal effect. A central aspect of our approach is the notion of a conflict graph, which captures the fundamental unobservability associated with the casual effect and the underlying network. We refer to our experimental design as the Conflict Graph Design. In order to estimate effects, we propose a modified Horvitz--Thompson estimator. We show that its variance under the Conflict Graph Design is bounded as $O(\lambda(H) / n )$, where $\lambda(H)$ is the largest eigenvalue of the adjacency matrix of the conflict graph. These rates depend on both the underlying network and the particular causal effect under investigation. Not only does this yield the best known rates of estimation for several well-studied causal effects (e.g. the global and direct effects) but it also provides new methods for effects which have received less attention from the perspective of experiment design (e.g. spill-over effects). Our results corroborate two implicitly understood points in the literature: (1) that in order to increase precision, experiment designs should be tailored to specific causal effects of interest and (2) that "more local" effects are easier to estimate than "more global" effects. In addition to point estimation, we construct conservative variance estimators which facilitate the construction of asymptotically valid confidence intervals for the casual effect of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10908v1</guid>
      <category>stat.ME</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vardis Kandiros, Charilaos Pipis, Constantinos Daskalakis, Christopher Harshaw</dc:creator>
    </item>
    <item>
      <title>Learning the Sherrington-Kirkpatrick Model Even at Low Temperature</title>
      <link>https://arxiv.org/abs/2411.11174</link>
      <description>arXiv:2411.11174v1 Announce Type: cross 
Abstract: We consider the fundamental problem of learning the parameters of an undirected graphical model or Markov Random Field (MRF) in the setting where the edge weights are chosen at random. For Ising models, we show that a multiplicative-weight update algorithm due to Klivans and Meka learns the parameters in polynomial time for any inverse temperature $\beta \leq \sqrt{\log n}$.
  This immediately yields an algorithm for learning the Sherrington-Kirkpatrick (SK) model beyond the high-temperature regime of $\beta &lt; 1$. Prior work breaks down at $\beta = 1$ and requires heavy machinery from statistical physics or functional inequalities. In contrast, our analysis is relatively simple and uses only subgaussian concentration.
  Our results extend to MRFs of higher order (such as pure $p$-spin models), where even results in the high-temperature regime were not known.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11174v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gautam Chandrasekaran, Adam Klivans</dc:creator>
    </item>
    <item>
      <title>Reliable Learning of Halfspaces under Gaussian Marginals</title>
      <link>https://arxiv.org/abs/2411.11238</link>
      <description>arXiv:2411.11238v1 Announce Type: cross 
Abstract: We study the problem of PAC learning halfspaces in the reliable agnostic model of Kalai et al. (2012). The reliable PAC model captures learning scenarios where one type of error is costlier than the others. Our main positive result is a new algorithm for reliable learning of Gaussian halfspaces on $\mathbb{R}^d$ with sample and computational complexity $$d^{O(\log (\min\{1/\alpha, 1/\epsilon\}))}\min (2^{\log(1/\epsilon)^{O(\log (1/\alpha))}},2^{\mathrm{poly}(1/\epsilon)})\;,$$ where $\epsilon$ is the excess error and $\alpha$ is the bias of the optimal halfspace. We complement our upper bound with a Statistical Query lower bound suggesting that the $d^{\Omega(\log (1/\alpha))}$ dependence is best possible. Conceptually, our results imply a strong computational separation between reliable agnostic learning and standard agnostic learning of halfspaces in the Gaussian setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11238v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Lisheng Ren, Nikos Zarifis</dc:creator>
    </item>
    <item>
      <title>On the compressiveness of the Burrows-Wheeler transform</title>
      <link>https://arxiv.org/abs/2411.11298</link>
      <description>arXiv:2411.11298v1 Announce Type: cross 
Abstract: The Burrows-Wheeler transform (BWT) is a reversible transform that converts a string $w$ into another string $\mathsf{BWT}(w)$. The size of the run-length encoded BWT (RLBWT) can be interpreted as a measure of repetitiveness in the class of representations called dictionary compression which are essentially representations based on copy and paste operations. In this paper, we shed new light on the compressiveness of BWT and the bijective BWT (BBWT). We first extend previous results on the relations of their run-length compressed sizes $r$ and $r_B$. We also show that the so-called ``clustering effect'' of BWT and BBWT can be captured by measures other than empirical entropy or run-length encoding. In particular, we show that BWT and BBWT do not increase the repetitiveness of the string with respect to various measures based on dictionary compression by more than a polylogarithmic factor. Furthermore, we show that there exists an infinite family of strings that are maximally incompressible by any dictionary compression measure, but become very compressible after applying BBWT. An interesting implication of this result is that it is possible to transcend dictionary compression in some cases by simply applying BBWT before applying dictionary compression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11298v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hideo Bannai, Tomohiro I, Yuto Nakashima</dc:creator>
    </item>
    <item>
      <title>Efficient Sample-optimal Learning of Gaussian Tree Models via Sample-optimal Testing of Gaussian Mutual Information</title>
      <link>https://arxiv.org/abs/2411.11516</link>
      <description>arXiv:2411.11516v1 Announce Type: cross 
Abstract: Learning high-dimensional distributions is a significant challenge in machine learning and statistics. Classical research has mostly concentrated on asymptotic analysis of such data under suitable assumptions. While existing works [Bhattacharyya et al.: SICOMP 2023, Daskalakis et al.: STOC 2021, Choo et al.: ALT 2024] focus on discrete distributions, the current work addresses the tree structure learning problem for Gaussian distributions, providing efficient algorithms with solid theoretical guarantees. This is crucial as real-world distributions are often continuous and differ from the discrete scenarios studied in prior works.
  In this work, we design a conditional mutual information tester for Gaussian random variables that can test whether two Gaussian random variables are independent, or their conditional mutual information is at least $\varepsilon$, for some parameter $\varepsilon \in (0,1)$ using $\mathcal{O}(\varepsilon^{-1})$ samples which we show to be near-optimal. In contrast, an additive estimation would require $\Omega(\varepsilon^{-2})$ samples. Our upper bound technique uses linear regression on a pair of suitably transformed random variables. Importantly, we show that the chain rule of conditional mutual information continues to hold for the estimated (conditional) mutual information. As an application of such a mutual information tester, we give an efficient $\varepsilon$-approximate structure-learning algorithm for an $n$-variate Gaussian tree model that takes $\widetilde{\Theta}(n\varepsilon^{-1})$ samples which we again show to be near-optimal. In contrast, when the underlying Gaussian model is not known to be tree-structured, we show that $\widetilde{{{\Theta}}}(n^2\varepsilon^{-2})$ samples are necessary and sufficient to output an $\varepsilon$-approximate tree structure. We perform extensive experiments that corroborate our theoretical convergence bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11516v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sutanu Gayen, Sanket Kale, Sayantan Sen</dc:creator>
    </item>
    <item>
      <title>Explicit Two-Sided Vertex Expanders Beyond the Spectral Barrier</title>
      <link>https://arxiv.org/abs/2411.11627</link>
      <description>arXiv:2411.11627v1 Announce Type: cross 
Abstract: We construct the first explicit two-sided vertex expanders that bypass the spectral barrier.
  Previously, the strongest known explicit vertex expanders were given by $d$-regular Ramanujan graphs, whose spectral properties imply that every small subset of vertices $S$ has at least $0.5d|S|$ distinct neighbors. However, it is possible to construct Ramanujan graphs containing a small set $S$ with no more than $0.5d|S|$ neighbors. In fact, no explicit construction was known to break the $0.5 d$-barrier.
  In this work, we give an explicit construction of an infinite family of $d$-regular graphs (for large enough $d$) where every small set expands by a factor of $\approx 0.6d$. More generally, for large enough $d_1,d_2$, we give an infinite family of $(d_1,d_2)$-biregular graphs where small sets on the left expand by a factor of $\approx 0.6d_1$, and small sets on the right expand by a factor of $\approx 0.6d_2$. In fact, our construction satisfies an even stronger property: small sets on the left and right have unique-neighbor expansion $0.6d_1$ and $0.6d_2$ respectively.
  Our construction follows the tripartite line product framework of Hsieh, McKenzie, Mohanty &amp; Paredes, and instantiates it using the face-vertex incidence of the $4$-dimensional Ramanujan clique complex as its base component. As a key part of our analysis, we derive new bounds on the triangle density of small sets in the Ramanujan clique complex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11627v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun-Ting Hsieh, Ting-Chun Lin, Sidhanth Mohanty, Ryan O'Donnell, Rachel Yun Zhang</dc:creator>
    </item>
    <item>
      <title>Hash &amp; Adjust: Competitive Demand-Aware Consistent Hashing</title>
      <link>https://arxiv.org/abs/2411.11665</link>
      <description>arXiv:2411.11665v1 Announce Type: cross 
Abstract: Distributed systems often serve dynamic workloads and resource demands evolve over time. Such a temporal behavior stands in contrast to the static and demand-oblivious nature of most data structures used by these systems. In this paper, we are particularly interested in consistent hashing, a fundamental building block in many large distributed systems. Our work is motivated by the hypothesis that a more adaptive approach to consistent hashing can leverage structure in the demand, and hence improve storage utilization and reduce access time. We initiate the study of demand-aware consistent hashing. Our main contribution is H&amp;A, a constant-competitive online algorithm (i.e., it comes with provable performance guarantees over time). H&amp;A is demand-aware and optimizes its internal structure to enable faster access times, while offering a high utilization of storage. We further evaluate H&amp;A empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11665v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.OPODIS.2024.26</arxiv:DOI>
      <dc:creator>Arash Pourdamghani, Chen Avin, Robert Sama, Maryam Shiran, Stefan Schmid</dc:creator>
    </item>
    <item>
      <title>Distributed Maximum Flow in Planar Graphs</title>
      <link>https://arxiv.org/abs/2411.11718</link>
      <description>arXiv:2411.11718v1 Announce Type: cross 
Abstract: The dual of a planar graph $G$ is a planar graph $G^*$ that has a vertex for each face of $G$ and an edge for each pair of adjacent faces of $G$. The profound relationship between a planar graph and its dual has been the algorithmic basis for solving numerous (centralized) classical problems on planar graphs. In the distributed setting however, the only use of planar duality is for finding a recursive decomposition of $G$ [DISC 2017, STOC 2019].
  We extend the distributed algorithmic toolkit to work on the dual graph $G^*$. These tools can then facilitate various algorithms on $G$ by solving a suitable dual problem on $G^*$.
  Given a directed planar graph $G$ with positive and negative edge-lengths and hop-diameter $D$, our key result is an $\tilde{O}(D^2)$-round algorithm for Single Source Shortest Paths on $G^*$, which then implies an $\tilde{O}(D^2)$-round algorithm for Maximum $st$-Flow on $G$. Prior to our work, no $\tilde{O}(\text{poly}(D))$-round algorithm was known for Maximum $st$-Flow. We further obtain a $D\cdot n^{o(1)}$-rounds $(1-\epsilon)$-approximation algorithm for Maximum $st$-Flow on $G$ when $G$ is undirected and $st$-planar. Finally, we give a near optimal $\tilde O(D)$-round algorithm for computing the weighted girth of $G$.
  The main challenges in our work are that $G^*$ is not the communication graph (e.g., a vertex of $G$ is mapped to multiple vertices of $G^*$), and that the diameter of $G^*$ can be much larger than $D$ (i.e., possibly by a linear factor). We overcome these challenges by carefully defining and maintaining subgraphs of the dual graph $G^*$ while applying the recursive decomposition on the primal graph $G$. The main technical difficulty, is that along the recursive decomposition, a face of $G$ gets shattered into (disconnected) components yet we still need to treat it as a dual node.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11718v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yaseen Abd-Elhaleem (University of Haifa), Michal Dory (University of Haifa), Merav Parter (Weizmann Institute of Science), Oren Weimann (University of Haifa)</dc:creator>
    </item>
    <item>
      <title>Solving convex QPs with structured sparsity under indicator conditions</title>
      <link>https://arxiv.org/abs/2411.11722</link>
      <description>arXiv:2411.11722v1 Announce Type: cross 
Abstract: We study convex optimization problems where disjoint blocks of variables are controlled by binary indicator variables that are also subject to conditions, e.g., cardinality. Several classes of important examples can be formulated in such a way that both the objective and the constraints are separable convex quadratics. We describe a family of polynomial-time approximation algorithms and negative complexity results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11722v1</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Bienstock, Tongtong Chen</dc:creator>
    </item>
    <item>
      <title>Stability of the Lanczos Method for Matrix Function Approximation</title>
      <link>https://arxiv.org/abs/1708.07788</link>
      <description>arXiv:1708.07788v2 Announce Type: replace 
Abstract: The ubiquitous Lanczos method can approximate $f(A)x$ for any symmetric $n \times n$ matrix $A$, vector $x$, and function $f$. In exact arithmetic, the method's error after $k$ iterations is bounded by the error of the best degree-$k$ polynomial uniformly approximating $f(x)$ on the range $[\lambda_{min}(A), \lambda_{max}(A)]$. However, despite decades of work, it has been unclear if this powerful guarantee holds in finite precision.
  We resolve this problem, proving that when $\max_{x \in [\lambda_{min}, \lambda_{max}]}|f(x)| \le C$, Lanczos essentially matches the exact arithmetic guarantee if computations use roughly $\log(nC\|A\|)$ bits of precision. Our proof extends work of Druskin and Knizhnerman [DK91], leveraging the stability of the classic Chebyshev recurrence to bound the stability of any polynomial approximating $f(x)$.
  We also study the special case of $f(A) = A^{-1}$, where stronger guarantees hold. In exact arithmetic Lanczos performs as well as the best polynomial approximating $1/x$ at each of $A$'s eigenvalues, rather than on the full eigenvalue range. In seminal work, Greenbaum gives an approach to extending this bound to finite precision: she proves that finite precision Lanczos and the related CG method match any polynomial approximating $1/x$ in a tiny range around each eigenvalue [Gre89].
  For $A^{-1}$, this bound appears stronger than ours. However, we exhibit matrices with condition number $\kappa$ where exact arithmetic Lanczos converges in $polylog(\kappa)$ iterations, but Greenbaum's bound predicts $\Omega(\kappa^{1/5})$ iterations. It thus cannot offer significant improvement over the $O(\kappa^{1/2})$ bound achievable via our result. Our analysis raises the question of if convergence in less than $poly(\kappa)$ iterations can be expected in finite precision, even for matrices with clustered, skewed, or otherwise favorable eigenvalue distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:1708.07788v2</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cameron Musco, Christopher Musco, Aaron Sidford</dc:creator>
    </item>
    <item>
      <title>Graph Reconstruction via MIS Queries</title>
      <link>https://arxiv.org/abs/2401.05845</link>
      <description>arXiv:2401.05845v3 Announce Type: replace 
Abstract: In the Graph Reconstruction (GR) problem, a player initially only knows the vertex set $V$ of an input graph $G=(V, E)$ and is required to learn its set of edges $E$. To this end, the player submits queries to an oracle and must deduce $E$ from the oracle's answers.
  In this paper, we initiate the study of GR via Maximal Independent Set (MIS) queries, a more powerful variant of Independent Set (IS) queries. Given a query $U \subseteq V$, the oracle responds with any, potentially adversarially chosen, maximal independent set $I \subseteq U$ in the induced subgraph $G[U]$.
  We show that, for GR, MIS queries are strictly more powerful than IS queries when parametrized by the maximum degree $\Delta$ of the input graph. We give tight (up to poly-logarithmic factors) upper and lower bounds for this problem:
  1. We observe that the simple strategy of taking uniform independent random samples of $V$ and submitting those to the oracle yields a non-adaptive randomized algorithm that executes $O(\Delta^2 \cdot \log n)$ queries and succeeds with high probability. Furthermore, combining the strategy of taking uniform random samples of $V$ with the probabilistic method, we show the existence of a deterministic non-adaptive algorithm that executes $O(\Delta^3 \cdot \log(\frac{n}{\Delta}))$ queries.
  2. Regarding lower bounds, we prove that the additional $\Delta$ factor when going from randomized non-adaptive algorithms to deterministic non-adaptive algorithms is necessary. We show that every non-adaptive deterministic algorithm requires $\Omega(\Delta^3 / \log^2 \Delta)$ queries. For arbitrary randomized adaptive algorithms, we show that $\Omega(\Delta^2)$ queries are necessary in graphs of maximum degree $\Delta$, and that $\Omega(\log n)$ queries are necessary, even when the input graph is an $n$-vertex cycle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05845v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Konrad, Conor O'Sullivan, Victor Traistaru</dc:creator>
    </item>
    <item>
      <title>Adaptive Dynamic Bitvectors</title>
      <link>https://arxiv.org/abs/2405.15088</link>
      <description>arXiv:2405.15088v3 Announce Type: replace 
Abstract: While operations {\em rank} and {\em select} on static bitvectors can be supported in constant time, lower bounds show that supporting updates raises the cost per operation to $\Theta(\log n/ \log\log n)$ on bitvectors holding $n$ bits. This is a shame in scenarios where updates are possible but uncommon. We develop a representation of bitvectors that we call adaptive dynamic bitvector, which uses the asymptotically optimal $n+o(n)$ bits of space and, if there are $q$ queries per update, supports all the operations in $O(\log(n/q)/\log\log n)$ amortized time. Further, we prove that this time is optimal in the cell probe model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15088v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gonzalo Navarro</dc:creator>
    </item>
    <item>
      <title>Learning-Augmented Priority Queues</title>
      <link>https://arxiv.org/abs/2406.04793</link>
      <description>arXiv:2406.04793v2 Announce Type: replace 
Abstract: Priority queues are one of the most fundamental and widely used data structures in computer science. Their primary objective is to efficiently support the insertion of new elements with assigned priorities and the extraction of the highest priority element. In this study, we investigate the design of priority queues within the learning-augmented framework, where algorithms use potentially inaccurate predictions to enhance their worst-case performance. We examine three prediction models spanning different use cases, and show how the predictions can be leveraged to enhance the performance of priority queue operations. Moreover, we demonstrate the optimality of our solution and discuss some possible applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04793v2</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyad Benomar, Christian Coester</dc:creator>
    </item>
    <item>
      <title>Lookback Prophet Inequalities</title>
      <link>https://arxiv.org/abs/2406.06805</link>
      <description>arXiv:2406.06805v2 Announce Type: replace 
Abstract: Prophet inequalities are fundamental optimal stopping problems, where a decision-maker observes sequentially items with values sampled independently from known distributions, and must decide at each new observation to either stop and gain the current value or reject it irrevocably and move to the next step. This model is often too pessimistic and does not adequately represent real-world online selection processes. Potentially, rejected items can be revisited and a fraction of their value can be recovered. To analyze this problem, we consider general decay functions $D_1,D_2,\ldots$, quantifying the value to be recovered from a rejected item, depending on how far it has been observed in the past. We analyze how lookback improves, or not, the competitive ratio in prophet inequalities in different order models. We show that, under mild monotonicity assumptions on the decay functions, the problem can be reduced to the case where all the decay functions are equal to the same function $x \mapsto \gamma x$, where $\gamma = \inf_{x&gt;0} \inf_{j \geq 1} D_j(x)/x$. Consequently, we focus on this setting and refine the analyses of the competitive ratios, with upper and lower bounds expressed as increasing functions of $\gamma$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06805v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyad Benomar, Dorian Baudry, Vianney Perchet</dc:creator>
    </item>
    <item>
      <title>The Power of Proportional Fairness for Non-Clairvoyant Scheduling under Polyhedral Constraints</title>
      <link>https://arxiv.org/abs/2408.14310</link>
      <description>arXiv:2408.14310v2 Announce Type: replace 
Abstract: The Polytope Scheduling Problem (PSP) was introduced by Im, Kulkarni, and Munagala (JACM 2018) as a very general abstraction of resource allocation over time and captures many well-studied problems including classical unrelated machine scheduling, multidimensional scheduling, and broadcast scheduling. In PSP, jobs with different arrival times receive processing rates that are subject to arbitrary packing constraints. An elegant and well-known algorithm for instantaneous rate allocation with good fairness and efficiency properties is the Proportional Fairness algorithm (PF), which was analyzed for PSP by Im et al.
  We drastically improve the analysis of the PF algorithm for both the general PSP and several of its important special cases subject to the objective of minimizing the sum of weighted completion times. We reduce the upper bound on the competitive ratio from 128 to 27 for general PSP and to 4 for the prominent class of monotone PSP. For certain heterogeneous machine environments we even close the substantial gap to the lower bound of 2 for non-clairvoyant scheduling. Our analysis also gives the first polynomial-time improvements over the nearly 30-year-old bounds on the competitive ratio of the doubling framework by Hall, Shmoys, and Wein (SODA 1996) for clairvoyant online preemptive scheduling on unrelated machines. Somewhat surprisingly, we achieve this improvement by a non-clairvoyant algorithm, thereby demonstrating that non-clairvoyance is not a (significant) hurdle.
  Our improvements are based on exploiting monotonicity properties of PSP, providing tight dual fitting arguments on structured instances, and showing new additivity properties on the optimal objective value for scheduling on unrelated machines. Finally, we establish new connections of PF to matching markets, and thereby provide new insights on equilibria and their computational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14310v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sven J\"ager, Alexander Lindermayr, Nicole Megow</dc:creator>
    </item>
    <item>
      <title>Structured Decompositions: Structural and Algorithmic Compositionality</title>
      <link>https://arxiv.org/abs/2207.06091</link>
      <description>arXiv:2207.06091v5 Announce Type: replace-cross 
Abstract: We introduce structured decompositions. These are category-theoretic data structures which simlutaneously generalize notions from graph theory (including tree-width, layered tree-width, co-tree-width and graph decomposition width) geometric group theory (specifically Bass-Serre theory) and dynamical systems (e.g. hybrid dynamical systems). Furthermore, structured decompositions allow us to generalize these aforementioned combinatorial invariants, which have played a central role in the study of structural and algorithmic compositionality in both graph theory and parameterized complexity, to new settings. For example, in any category with enough colimits they describe algorithmically useful structural compositionality: as an application of our theory we prove an algorithmic meta-theorem for the Sub_P-composition problem. In concrete terms, when instantiated in the category of graphs, this meta-theorem yields compositional algorithms for NP-hard problems such as: Maximum Bipartite Subgraph, Maximum Planar Subgraph and Longest Path.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.06091v5</guid>
      <category>math.CT</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Merlin Bumpus, Zoltan A. Kocsis, Jade Edenstar Master</dc:creator>
    </item>
    <item>
      <title>Addressing Bias in Online Selection with Limited Budget of Comparisons</title>
      <link>https://arxiv.org/abs/2303.09205</link>
      <description>arXiv:2303.09205v4 Announce Type: replace-cross 
Abstract: Consider a hiring process with candidates coming from different universities. It is easy to order candidates with the same background, yet it can be challenging to compare them otherwise. The latter case requires additional costly assessments, leading to a potentially high total cost for the hiring organization. Given an assigned budget, what would be an optimal strategy to select the most qualified candidate? We model the above problem as a multicolor secretary problem, allowing comparisons between candidates from distinct groups at a fixed cost. Our study explores how the allocated budget enhances the success probability of online selection algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.09205v4</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ziyad Benomar, Evgenii Chzhen, Nicolas Schreuder, Vianney Perchet</dc:creator>
    </item>
    <item>
      <title>A hierarchy of eigencomputations for polynomial optimization on the sphere</title>
      <link>https://arxiv.org/abs/2310.17827</link>
      <description>arXiv:2310.17827v2 Announce Type: replace-cross 
Abstract: We introduce a convergent hierarchy of lower bounds on the minimum value of a real form over the unit sphere. The main practical advantage of our hierarchy over the real sum-of-squares (RSOS) hierarchy is that the lower bound at each level of our hierarchy is obtained by a minimum eigenvalue computation, as opposed to the full semidefinite program (SDP) required at each level of RSOS. In practice, this allows us to compute bounds on much larger forms than are computationally feasible for RSOS. Our hierarchy outperforms previous alternatives to RSOS, both asymptotically and in numerical experiments. We obtain our hierarchy by proving a reduction from real optimization on the sphere to Hermitian optimization on the sphere, and invoking the Hermitian sum-of-squares (HSOS) hierarchy. This opens the door to using other Hermitian optimization techniques for real optimization, and gives a path towards developing spectral hierarchies for more general constrained real optimization problems. To this end, we use our techniques to develop a hierarchy of eigencomputations for computing the real tensor spectral norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17827v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>math.AG</category>
      <category>quant-ph</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Lovitz, Nathaniel Johnston</dc:creator>
    </item>
    <item>
      <title>Amortized Analysis via Coalgebra</title>
      <link>https://arxiv.org/abs/2404.03641</link>
      <description>arXiv:2404.03641v3 Announce Type: replace-cross 
Abstract: Amortized analysis is a cost analysis technique for data structures in which cost is studied in aggregate: rather than considering the maximum cost of a single operation, one bounds the total cost encountered throughout a session. Traditionally, amortized analysis has been phrased inductively, quantifying over finite sequences of operations. Connecting to prior work on coalgebraic semantics for data structures, we develop the alternative perspective that amortized analysis is naturally viewed coalgebraically in a category of cost algebras, where a morphism of coalgebras serves as a first-class generalization of potential function suitable for integrating cost and behavior. Using this simple definition, we consider amortization of other sample effects, non-commutative printing and randomization. To support imprecise amortized upper bounds, we adapt our discussion to the bicategorical setting, where a potential function is a colax morphism of coalgebras. We support algebraic and coalgebraic operations simultaneously by using coalgebras for an endoprofunctor instead of an endofunctor, combining potential using a monoidal structure on the underlying category. Finally, we compose amortization arguments in the indexed category of coalgebras to implement one amortized data structure in terms of others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03641v3</guid>
      <category>cs.PL</category>
      <category>cs.DS</category>
      <category>math.CT</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Grodin, Robert Harper</dc:creator>
    </item>
    <item>
      <title>Breaking the $T^{2/3}$ Barrier for Sequential Calibration</title>
      <link>https://arxiv.org/abs/2406.13668</link>
      <description>arXiv:2406.13668v3 Announce Type: replace-cross 
Abstract: A set of probabilistic forecasts is calibrated if each prediction of the forecaster closely approximates the empirical distribution of outcomes on the subset of timesteps where that prediction was made. We study the fundamental problem of online calibrated forecasting of binary sequences, which was initially studied by Foster &amp; Vohra (1998). They derived an algorithm with $O(T^{2/3})$ calibration error after $T$ time steps, and showed a lower bound of $\Omega(T^{1/2})$. These bounds remained stagnant for two decades, until Qiao &amp; Valiant (2021) improved the lower bound to $\Omega(T^{0.528})$ by introducing a combinatorial game called sign preservation and showing that lower bounds for this game imply lower bounds for calibration.
  In this paper, we give the first improvement to the $O(T^{2/3})$ upper bound on calibration error of Foster &amp; Vohra. We do this by introducing a variant of Qiao &amp; Valiant's game that we call sign preservation with reuse (SPR). We prove that the relationship between SPR and calibrated forecasting is bidirectional: not only do lower bounds for SPR translate into lower bounds for calibration, but algorithms for SPR also translate into new algorithms for calibrated forecasting. We then give an improved \emph{upper bound} for the SPR game, which implies, via our equivalence, a forecasting algorithm with calibration error $O(T^{2/3 - \varepsilon})$ for some $\varepsilon &gt; 0$, improving Foster &amp; Vohra's upper bound for the first time. Using similar ideas, we then prove a slightly stronger lower bound than that of Qiao &amp; Valiant, namely $\Omega(T^{0.54389})$. Our lower bound is obtained by an oblivious adversary, marking the first $\omega(T^{1/2})$ calibration lower bound for oblivious adversaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13668v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuval Dagan, Constantinos Daskalakis, Maxwell Fishelson, Noah Golowich, Robert Kleinberg, Princewill Okoroafor</dc:creator>
    </item>
    <item>
      <title>Bridging Classical and Quantum: Group-Theoretic Approach to Quantum Circuit Simulation</title>
      <link>https://arxiv.org/abs/2407.19575</link>
      <description>arXiv:2407.19575v2 Announce Type: replace-cross 
Abstract: Efficiently simulating quantum circuits on classical computers is a fundamental challenge in quantum computing. This paper presents a novel theoretical approach that achieves exponential speedups (polynomial runtime) over existing simulators for a wide class of quantum circuits. The technique leverages advanced group theory and symmetry considerations to map quantum circuits to equivalent forms amenable to efficient classical simulation. Several fundamental theorems are proven that establish the mathematical foundations of this approach, including a generalized Gottesman-Knill theorem. The potential of this method is demonstrated through theoretical analysis and preliminary benchmarks. This work contributes to the understanding of the boundary between classical and quantum computation, provides new tools for quantum circuit analysis and optimization, and opens up avenues for further research at the intersection of group theory and quantum computation. The findings may have implications for quantum algorithm design, error correction, and the development of more efficient quantum simulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19575v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math-ph</category>
      <category>math.GR</category>
      <category>math.MP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daksh Shami</dc:creator>
    </item>
    <item>
      <title>Count on Your Elders: Laplace vs Gaussian Noise</title>
      <link>https://arxiv.org/abs/2408.07021</link>
      <description>arXiv:2408.07021v3 Announce Type: replace-cross 
Abstract: In recent years, Gaussian noise has become a popular tool in differentially private algorithms, often replacing Laplace noise which dominated the early literature. Gaussian noise is the standard approach to $\textit{approximate}$ differential privacy, often resulting in much higher utility than traditional (pure) differential privacy mechanisms. In this paper we argue that Laplace noise may in fact be preferable to Gaussian noise in many settings, in particular for $(\varepsilon,\delta)$-differential privacy when $\delta$ is small. We consider two scenarios:
  First, we consider the problem of counting under continual observation and present a new generalization of the binary tree mechanism that uses a $k$-ary number system with $\textit{negative digits}$ to improve the privacy-accuracy trade-off. Our mechanism uses Laplace noise and whenever $\delta$ is sufficiently small it improves the mean squared error over the best possible $(\varepsilon,\delta)$-differentially private factorization mechanisms based on Gaussian noise. Specifically, using $k=19$ we get an asymptotic improvement over the bound given in the work by Henzinger, Upadhyay and Upadhyay (SODA 2023) when $\delta = O(T^{-0.92})$.
  Second, we show that the noise added by the Gaussian mechanism can always be replaced by Laplace noise of comparable variance for the same $(\epsilon, \delta)$-differential privacy guarantee, and in fact for sufficiently small $\delta$ the variance of the Laplace noise becomes strictly better. This challenges the conventional wisdom that Gaussian noise should be used for high-dimensional noise.
  Finally, we study whether counting under continual observation may be easier in an average-case sense. We show that, under pure differential privacy, the expected worst-case error for a random input must be $\Omega(\log(T)/\varepsilon)$, matching the known lower bound for worst-case inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07021v3</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joel Daniel Andersson, Rasmus Pagh, Teresa Anna Steiner, Sahel Torkamani</dc:creator>
    </item>
    <item>
      <title>Statistical-Computational Trade-offs for Recursive Adaptive Partitioning Estimators</title>
      <link>https://arxiv.org/abs/2411.04394</link>
      <description>arXiv:2411.04394v2 Announce Type: replace-cross 
Abstract: Models based on recursive adaptive partitioning such as decision trees and their ensembles are popular for high-dimensional regression as they can potentially avoid the curse of dimensionality. Because empirical risk minimization (ERM) is computationally infeasible, these models are typically trained using greedy algorithms. Although effective in many cases, these algorithms have been empirically observed to get stuck at local optima. We explore this phenomenon in the context of learning sparse regression functions over $d$ binary features, showing that when the true regression function $f^*$ does not satisfy Abbe et al. (2022)'s Merged Staircase Property (MSP), greedy training requires $\exp(\Omega(d))$ to achieve low estimation error. Conversely, when $f^*$ does satisfy MSP, greedy training can attain small estimation error with only $O(\log d)$ samples. This dichotomy mirrors that of two-layer neural networks trained with stochastic gradient descent (SGD) in the mean-field regime, thereby establishing a head-to-head comparison between SGD-trained neural networks and greedy recursive partitioning estimators. Furthermore, ERM-trained recursive partitioning estimators achieve low estimation error with $O(\log d)$ samples irrespective of whether $f^*$ satisfies MSP, thereby demonstrating a statistical-computational trade-off for greedy training. Our proofs are based on a novel interpretation of greedy recursive partitioning using stochastic process theory and a coupling technique that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04394v2</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Shuo Tan, Jason M. Klusowski, Krishnakumar Balasubramanian</dc:creator>
    </item>
  </channel>
</rss>
