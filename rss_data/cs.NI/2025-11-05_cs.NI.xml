<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Nov 2025 02:43:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Nonlinear Instabilities in Computer Network Dynamics</title>
      <link>https://arxiv.org/abs/2511.01886</link>
      <description>arXiv:2511.01886v1 Announce Type: new 
Abstract: This work studies two types of computer networking models. The primary focus is to understand the different dynamical phenomena observed in practice due to the presence of severe nonlinearities, delays and widely varying operating conditions. The first models considered are of senders running TCP (Transmission Control Protocol) and traffic passing through RED (Random Early Detection) gateways. Building on earlier work, a first order nonlinear discrete-time model is developed for the interaction scenario between transport protocols like TCP and UDP (User Datagram Protocol) and Active Queuing Management schemes like RED. It is shown that the dynamics resulting from the interaction with TCP is consistent with various dynamical behaviors and parameter sensitivities observed in practice. Using bifurcation-theoretic ideas it is shown that TCP-RED type networks may lose their stability through a period doubling bifurcation followed by border collision bifurcations. The nonlinear dependence of the throughput function of TCP-type flows on drop probability is found to be responsible for the period doubling bifurcation, whereas limited buffer space and lack of sufficient damping results in border collision bifurcations. A second class of models studied in this work deals with optimal rate control in networks and are based on the rate-control framework proposed by Kelly. Using the results on delay-differential equation stability, the stability and its lack thereof is studied through an underlying map which arises naturally in time delay systems. An invariance property of this map is used to prove delay-independent stability and to compute bounds on periodic oscillations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01886v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>nlin.CD</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Priya Ranjan</dc:creator>
    </item>
    <item>
      <title>A Modular DTaaS Architecture for Predictive Slice Management in 6G Systems</title>
      <link>https://arxiv.org/abs/2511.01989</link>
      <description>arXiv:2511.01989v1 Announce Type: new 
Abstract: The sixth generation (6G) of wireless networks will require fundamentally new orchestration paradigms to meet stringent requirements for ultra-low latency, high reliability, and pervasive intelligence. Network slicing emerges as a key enabler to support diverse services with customized quality-of-service (QoS) guarantees. However, dynamic and fine-grained slice management poses significant challenges in terms of real-time provisioning, SLA assurance, and cross-layer observability. In this paper, we propose a novel Digital Twin as a Service (DTaaS) framework that embeds per-slice digital twins (SDTs) into the orchestration loop. Each SDT maintains a synchronized, real-time representation of its slice, leveraging multi-domain telemetry and deep sequential models to predict traffic evolution and SLA risks. The framework introduces modular intelligence layers, programmable interfaces, and edge-embedded decision-making to enable proactive provisioning, adaptive scaling, and closed-loop SLA assurance. Mathematical formulations for fidelity measurement, predictive control, and optimization objectives are provided to ensure rigor and transparency. Evaluation results demonstrate that DTaaS significantly improves SLA compliance ratio, reduces resource over-provisioning, and lowers average SLA violation probability, offering a scalable and reliable orchestration approach for 6G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01989v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tu\u{g}\c{c}e Bilen, Mehmet \"Ozdem</dc:creator>
    </item>
    <item>
      <title>Permissioned Blockchain in Advanced Air Mobility: A Performance Analisys for UTM</title>
      <link>https://arxiv.org/abs/2511.02171</link>
      <description>arXiv:2511.02171v1 Announce Type: new 
Abstract: The rapid adoption of Uncrewed Aerial Vehicles (UAVs) has driven aviation authorities to propose distributed Uncrewed Traffic Management (UTM) architectures. Several studies have advocated blockchain as a promising technology to meet these requirements. However, since UTM is a safety-critical and highly regulated domain, compliance with standards and regulatory frameworks is as crucial as performance and security. This work benchmarks two distributed architectures aligned with current regulatory frameworks: the Linux Foundation's InterUSS platform and a Hyperledger Fabric-based private ledger. Our findings reveal that blockchain-based systems require architectures specifically designed for aeronautical performance constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02171v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rodrigo Nunes, Andr\'e Melo, Rafael Albarello, Reinaldo Gomes, Cesar Marcondes, Louren\c{c}o Pereira Jr</dc:creator>
    </item>
    <item>
      <title>Optimizing Multi-UAV 3D Deployment for Energy-Efficient Sensing over Uneven Terrains</title>
      <link>https://arxiv.org/abs/2511.02368</link>
      <description>arXiv:2511.02368v1 Announce Type: new 
Abstract: In this work, we consider a multi-unmanned aerial vehicle (UAV) cooperative sensing system where UAVs are deployed to sense multiple targets in terrain-aware line of sight (LoS) conditions in uneven terrain equipped with directional antennas. To mitigate terrain-induced LoS blockages that degrade detection performance, we incorporate a binary LoS indicator and propose a bounding volume hierarchy (BHV)-based adaptive scheme for efficient LoS evaluation. We formulate a bi-objective problem that maximizes the probability of cooperative detection with minimal hover energy constraints governing spatial, orientational, and safety constraints. To address the problem, which is inherently non-convex, we propose a hierarchical heuristic framework that combines exploration through a genetic algorithm (GA) with per-UAV refinement via particle swarm optimization (PSO), where a penalty-based fitness evaluation guides solutions toward feasibility, bounded within constraints. The proposed methodology is an effective trade-off method of traversing through a complex search space and maintaining terrain-aware LoS connectivity and energy aware deployment. Monte Carlo simulations on real-world terrain data show that the proposed GA+PSO framework improves detection probability by 37.02% and 36.5% for 2 and 3 UAVs, respectively, while reducing average excess hover energy by 45.0% and 48.9% compared to the PSO-only baseline. Relative to the non-optimized scheme, it further achieves 59.5% and 54.2% higher detection probability with 59.8% and 65.9% lower excess hover energy, thereby showing its effectiveness with a small number of UAVs over uneven terrain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02368v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rushi Moliya, Dhaval K. Patel, Brijesh Soni, Miguel L\'opez-Ben\'itez</dc:creator>
    </item>
    <item>
      <title>Lightweight Latency Prediction Scheme for Edge Applications: A Rational Modelling Approach</title>
      <link>https://arxiv.org/abs/2511.02501</link>
      <description>arXiv:2511.02501v1 Announce Type: new 
Abstract: Accurately predicting end-to-end network latency is essential for enabling reliable task offloading in real-time edge computing applications. This paper introduces a lightweight latency prediction scheme based on rational modelling that uses features such as frame size, arrival rate, and link utilization, eliminating the need for intrusive active probing. The model achieves state-of-the-art prediction accuracy through extensive experiments and 5-fold cross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference time, offering a substantial trade-off between precision and efficiency compared to traditional regressors and neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02501v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohan Liyanage, Eldiyar Zhantileuov, Ali Kadhum Idrees, Rolf Schuster</dc:creator>
    </item>
    <item>
      <title>Janus: Leveraging Incremental Computation for Efficient DNS Verification</title>
      <link>https://arxiv.org/abs/2511.02559</link>
      <description>arXiv:2511.02559v1 Announce Type: new 
Abstract: Existing DNS configuration verification tools face significant issues (e.g., inefficient and lacking support for incremental verification). Inspired by the advancements in recent work of distributed data plane verification and the resemblance be- tween the data plane and DNS configuration, we tackle the challenge of DNS misconfiguration by introducing Janus, a DNS verification tool. Our key insight is that the process of a nameserver handling queries can be transformed into a matching process on a match-action table. With this insight, Janus consists of (1) an efficient data structure for partition query space based on the behaviors, (2) a symbolic execution algorithm that specifies how a single nameserver can efficiently cover all possible queries and ensure the accuracy of verification, (3) a mechanism to support incremental verification with less computational effort. Extensive experiments on real-world datasets (with over 6 million resource records) show that Janus achieves significant speedups, with peak improvements of up to 255.7x and a maximum 6046x reduction in the number of LECs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02559v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Wang, Kexin Yu, Wenyun Xu, Kaiqiang Hu, Ziyi Wang, Lizhao You, Qiang Su, Dong Guo, Haizhou Du, Wanjian Feng, Qingyu Song, Linghe Kong, Qiao Xiang, Jiwu Shu</dc:creator>
    </item>
    <item>
      <title>Decentralized AI Service Placement, Selection and Routing in Mobile Networks</title>
      <link>https://arxiv.org/abs/2511.02638</link>
      <description>arXiv:2511.02638v1 Announce Type: new 
Abstract: The rapid development and usage of large-scale AI models by mobile users will dominate the traffic load in future communication networks. The advent of AI technology also facilitates a decentralized AI ecosystem where small organizations or even individuals can host AI services. In such scenarios, AI service (models) placement, selection, and request routing decisions are tightly coupled, posing a challenging yet fundamental trade-off between service quality and service latency, especially when considering user mobility. Existing solutions for related problems in mobile edge computing (MEC) and data-intensive networks fall short due to restrictive assumptions about network structure or user mobility. To bridge this gap, we propose a decentralized framework that jointly optimizes AI service placement, selection, and request routing. In the proposed framework, we use traffic tunneling to support user mobility without costly AI service migrations. To account for nonlinear queuing delays, we formulate a nonconvex problem to optimize the trade-off between service quality and end-to-end latency. We derive the node-level KKT conditions and develop a decentralized Frank--Wolfe algorithm with a novel messaging protocol. Numerical evaluations validate the proposed approach and show substantial performance improvements over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02638v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinkun Zhang, Stefan Vlaski, Kin Leung</dc:creator>
    </item>
    <item>
      <title>CRRM: A 5G system-level simulator</title>
      <link>https://arxiv.org/abs/2511.02692</link>
      <description>arXiv:2511.02692v1 Announce Type: new 
Abstract: System-level simulation is indispensable for developing and testing novel algorithms for 5G and future wireless networks, yet a gap persists between the needs of the machine learning re- search community and the available tooling. To address this, we introduce the Cellular Radio Reference Model (CRRM), an open-source, pure Python simulator we designed specifically for speed, usability, and direct integration with modern AI frameworks. The core scientific contribution of CRRM lies in its architecture, which departs from traditional discrete-event simulation. We model the system as a set of inter-dependent computational blocks which form nodes in a directed graph. This enables a compute-on-demand mechanism we term smart update.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02692v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keith Briggs, Ibrahim Nur</dc:creator>
    </item>
    <item>
      <title>On the Optimization of Model Aggregation for Federated Learning at the Network Edge</title>
      <link>https://arxiv.org/abs/2511.02703</link>
      <description>arXiv:2511.02703v1 Announce Type: new 
Abstract: The rapid increase in connected devices has signifi- cantly intensified the computational and communication demands on modern telecommunication networks. To address these chal- lenges, integrating advanced Machine Learning (ML) techniques like Federated Learning (FL) with emerging paradigms such as Multi-access Edge Computing (MEC) and Software-Defined Wide Area Networks (SD-WANs) is crucial. This paper intro- duces online resource management strategies specifically designed for FL model aggregation, utilizing intermediate aggregation at edge nodes. Our analysis highlights the benefits of incorporating edge aggregators to reduce network link congestion and maximize the potential of edge computing nodes. However, the risk of network congestion persists. To mitigate this, we propose a novel aggregation approach that deploys an aggregator overlay network. We present an Integer Linear Programming (ILP) model and a heuristic algorithm to optimize the routing within this overlay network. Our solution demonstrates improved adapt- ability to network resource utilization, significantly reducing FL training round failure rates by up to 15% while also alleviating cloud link congestion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02703v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mengyao Li, Noah Ploch, Sebastian Troia, Carlo Spatocco, Wolfgang Kellerer, Guido Maier</dc:creator>
    </item>
    <item>
      <title>Agentic World Modeling for 6G: Near-Real-Time Generative State-Space Reasoning</title>
      <link>https://arxiv.org/abs/2511.02748</link>
      <description>arXiv:2511.02748v1 Announce Type: new 
Abstract: We argue that sixth-generation (6G) intelligence is not fluent token prediction but the capacity to imagine and choose -- to simulate future scenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe open radio access network (O-RAN) near-real-time (Near-RT) control via counterfactual dynamics and a world modeling (WM) paradigm that learns an action-conditioned generative state space. This enables quantitative "what-if" forecasting beyond large language models (LLMs) as the primary modeling primitive. Actions such as physical resource blocks (PRBs) are treated as first-class control inputs in a causal world model, and both aleatoric and epistemic uncertainty are modeled for prediction and what-if analysis. An agentic, model predictive control (MPC)-based cross-entropy method (CEM) planner operates over short horizons, using prior-mean rollouts within data-driven PRB bounds to maximize a deterministic reward. The model couples multi-scale structured state-space mixtures (MS3M) with a compact stochastic latent to form WM-MS3M, summarizing key performance indicators (KPIs) histories and predicting next-step KPIs under hypothetical PRB sequences. On realistic O-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with 32% fewer parameters and similar latency, and achieves 35-80% lower root mean squared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster inference, enabling rare-event simulation and offline policy screening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02748v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farhad Rezazadeh, Hatim Chergui, Merouane Debbah, Houbing Song, Dusit Niyato, Lingjia Liu</dc:creator>
    </item>
    <item>
      <title>FedSelect-ME: A Secure Multi-Edge Federated Learning Framework with Adaptive Client Scoring</title>
      <link>https://arxiv.org/abs/2511.01898</link>
      <description>arXiv:2511.01898v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative model training without sharing raw data but suffers from limited scalability, high communication costs, and privacy risks due to its centralized architecture. This paper proposes FedSelect-ME, a hierarchical multi-edge FL framework that enhances scalability, security, and energy efficiency. Multiple edge servers distribute workloads and perform score-based client selection, prioritizing participants based on utility, energy efficiency, and data sensitivity. Secure Aggregation with Homomorphic Encryption and Differential Privacy protects model updates from exposure and manipulation. Evaluated on the eICU healthcare dataset, FedSelect-ME achieves higher prediction accuracy, improved fairness across regions, and reduced communication overhead compared to FedAvg, FedProx, and FedSelect. The results demonstrate that the proposed framework effectively addresses the bottlenecks of conventional FL, offering a secure, scalable, and efficient solution for large-scale, privacy-sensitive healthcare applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01898v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanie Vatani, Reza Ebrahimi Atani</dc:creator>
    </item>
    <item>
      <title>Deep Reinforcement Learning for Multi-flow Routing in Heterogeneous Wireless Networks</title>
      <link>https://arxiv.org/abs/2511.02030</link>
      <description>arXiv:2511.02030v1 Announce Type: cross 
Abstract: Due to the rapid growth of heterogeneous wireless networks (HWNs), where devices with diverse communication technologies coexist, there is increasing demand for efficient and adaptive multi-hop routing with multiple data flows. Traditional routing methods, designed for homogeneous environments, fail to address the complexity introduced by links consisting of multiple technologies, frequency-dependent fading, and dynamic topology changes. In this paper, we propose a deep reinforcement learning (DRL)-based routing framework using deep Q-networks (DQN) to establish routes between multiple source-destination pairs in HWNs by enabling each node to jointly select a communication technology, a subband, and a next hop relay that maximizes the rate of the route. Our approach incorporates channel and interference-aware neighbor selection approaches to improve decision-making beyond conventional distance-based heuristics. We further evaluate the robustness and generalizability of the proposed method under varying network dynamics, including node mobility, changes in node density, and the number of data flows. Simulation results demonstrate that our DRL-based routing framework significantly enhances scalability, adaptability, and end-to-end throughput in complex HWN scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02030v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Kim, Justin H. Kong, Terrence J. Moore, Fikadu T. Dagefu</dc:creator>
    </item>
    <item>
      <title>GPoS: Geospatially-aware Proof of Stake</title>
      <link>https://arxiv.org/abs/2511.02034</link>
      <description>arXiv:2511.02034v1 Announce Type: cross 
Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory resilience, robustness, and fairness. We empirically analyze five major Proof of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui, revealing that a few geographic regions dominate consensus voting power, resulting in limited geospatial decentralization. To address this, we propose Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity with stake-based voting power. Experimental evaluation demonstrates an average 45% improvement in geospatial decentralization, as measured by the Gini coefficient of Eigenvector centrality, while incurring minimal performance overhead in BFT protocols, including HotStuff and CometBFT. These results demonstrate that GPoS can improve geospatial decentralization {while, in our experiments, incurring minimal overhead} to consensus performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02034v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3774629</arxiv:DOI>
      <dc:creator>Shashank Motepalli, Naman Garg, Gengrui Zhang, Hans-Arno Jacobsen</dc:creator>
    </item>
    <item>
      <title>Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live</title>
      <link>https://arxiv.org/abs/2511.02230</link>
      <description>arXiv:2511.02230v1 Announce Type: cross 
Abstract: Agentic LLM applications interleave LLM generation requests with tool calls. These tool calls break the continuity of the workflow by creating pauses between LLM requests, bringing many challenges for the serving system, especially under multi-turn scenarios. Each pause potentially causes KV cache eviction and extra waiting time before entering the continuous batch for the following LLM request. Since these pauses happen for each call, this problem becomes increasingly severe as turn number grow for agentic programs. Previous works either fail to incorporate information from the tool call, evicting KV cache that leads to repetitive prefill or loading, or ignore the continuity of a multi-turn program, creating waiting time between turns that increases per-request latency.
  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by combining tool-aware KV cache timeout with program-level scheduling. By predicting tool call durations in agentic workflows, Continuum selectively pins the KV cache in GPU memory with a time-to-live value based on total turn number. When combined with program-level first-come-first-serve, Continuum prevents scheduling bubbles, preserves multi-turn continuity, and optimizes for throughput for complex agentic workflows. By modeling the variability of tool call and agent program continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models shows that Continuum significantly improves the average job completion times, and remains performant across different hardware setups and DRAM offloading schemes. Preview code is available at: https://github.com/Hanchenli/vllm-continuum</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02230v1</guid>
      <category>cs.OS</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanchen Li, Qiuyang Mang, Runyuan He, Qizheng Zhang, Huanzhi Mao, Xiaokun Chen, Alvin Cheung, Joseph Gonzalez, Ion Stoica</dc:creator>
    </item>
    <item>
      <title>dApps: Enabling Real-Time AI-Based Open RAN Control</title>
      <link>https://arxiv.org/abs/2501.16502</link>
      <description>arXiv:2501.16502v2 Announce Type: replace 
Abstract: Open Radio Access Networks (RANs) leverage disaggregated and programmable RAN functions and open interfaces to enable closed-loop, data-driven radio resource management. This is performed through custom intelligent applications on the RAN Intelligent Controllers (RICs), optimizing RAN policy scheduling, network slicing, user session management, and medium access control, among others. In this context, we have proposed dApps as a key extension of the O-RAN architecture into the real-time and user-plane domains. Deployed directly on RAN nodes, dApps access data otherwise unavailable to RICs due to privacy or timing constraints, enabling the execution of control actions within shorter time intervals. In this paper, we propose for the first time a reference architecture for dApps, defining their life cycle from deployment by the Service Management and Orchestration (SMO) to real-time control loop interactions with the RAN nodes where they are hosted. We introduce a new dApp interface, E3, along with an Application Protocol (AP) that supports structured message exchanges and extensible communication for various service models. By bridging E3 with the existing O-RAN E2 interface, we enable dApps, xApps, and rApps to coexist and coordinate. These applications can then collaborate on complex use cases and employ hierarchical control to resolve shared resource conflicts. Finally, we present and open-source a dApp framework based on OpenAirInterface (OAI). We benchmark its performance in two real-time control use cases, i.e., spectrum sharing and positioning in a 5th generation (5G) Next Generation Node Base (gNB) scenario. Our experimental results show that standardized real-time control loops via dApps are feasible, achieving average control latency below 450 microseconds and allowing optimal use of shared spectral resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16502v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.comnet.2025.111342</arxiv:DOI>
      <arxiv:journal_reference>Computer Networks, Volume 269, 2025, 111342, ISSN 1389-1286</arxiv:journal_reference>
      <dc:creator>Andrea Lacava, Leonardo Bonati, Niloofar Mohamadi, Rajeev Gangula, Florian Kaltenberger, Pedram Johari, Salvatore D'Oro, Francesca Cuomo, Michele Polese, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>Towards Timing Isolation for Mixed-Criticality Communication in Software-Defined Vehicles</title>
      <link>https://arxiv.org/abs/2508.13652</link>
      <description>arXiv:2508.13652v2 Announce Type: replace 
Abstract: As the automotive industry transitions toward centralized Linux-based architectures, ensuring the predictable execution of mixed-criticality applications becomes essential. However, concurrent use of the Linux network stack introduces interference, resulting in unpredictable latency and jitter. To address this challenge, we present a layered software architecture that enforces timing isolation for Ethernet-based data exchange between mixed-criticality applications on Linux-based automotive control units. Our approach integrates traffic prioritization strategies at the middleware layer, the network stack layer, and the hardware layer to achieve isolation across the full software stack. At the middleware layer, we implement a fixed-priority, non-preemptive scheduler to manage publishers of varying criticality. At the network layer, we leverage the express data path (XDP) to route high-priority data directly from the network interface driver into critical application memory, bypassing the standard Linux network stack. At the hardware layer, we dedicate a network interface card (NIC) queue exclusively to real-time traffic. We demonstrate how our architecture performs in a Data Distribution Service (DDS)-based system. Our evaluation shows that the approach leads to consistent and predictable latencies for real-time traffic, even under heavy interference from best-effort applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13652v2</guid>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'or\'ant Meszl\'enyi, Julius Kahle, Dominik P\"ullen, Stefan Kowalewski, Stefan Katzenbeisser, Alexandru Kampmann</dc:creator>
    </item>
    <item>
      <title>Drift Plus Optimistic Penalty: A Learning Framework for Stochastic Network Optimization with Improved Regret Bounds</title>
      <link>https://arxiv.org/abs/2509.03762</link>
      <description>arXiv:2509.03762v3 Announce Type: replace 
Abstract: We consider the problem of joint routing and scheduling in queueing networks, where the edge transmission costs are unknown. At each time-slot, the network controller receives noisy observations of transmission costs only for those edges it selects for transmission. The network controller's objective is to make routing and scheduling decisions so that the total expected cost is minimized. This problem exhibits an exploration-exploitation trade-off, however, previous bandit-style solutions cannot be directly applied to this problem due to the queueing dynamics. In order to ensure network stability, the network controller needs to optimize throughput and cost simultaneously. We show that the best achievable cost is lower bounded by the solution to a static optimization problem, and develop a network control policy using techniques from Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show that the policy achieves a sub-linear regret of order $O(\sqrt{T}\log T)$, as compared to the best policy that has complete knowledge of arrivals and costs. Finally, we evaluate the proposed policy using simulations and show that its regret is indeed sub-linear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03762v3</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sathwik Chadaga, Eytan Modiano</dc:creator>
    </item>
    <item>
      <title>Enabling Network Policy Enforcement in Service Meshes</title>
      <link>https://arxiv.org/abs/2510.04052</link>
      <description>arXiv:2510.04052v2 Announce Type: replace 
Abstract: Portable service mesh implementations enable Layer 4 to Layer 7 policy enforcement across heterogeneous infrastructures, yet they depend on the underlying network's connectivity and policies. Layer 3 network policies govern IP traffic regardless of whether upper layers authorize the flow. While these policies are integral to security, correct enforcement often requires coordination across multiple teams, and achieving consistent policy behavior across heterogeneous environments is challenging. Studies show that most Kubernetes clusters do not enforce any network policies. We propose integrating Layer 3 network policy enforcement with service meshes to protect data-plane traffic in a portable, infrastructure-agnostic manner. This integration allows developers to define Layer 3-7 policies and to ensure enforcement across any infrastructure. Our solution builds an overlay Layer 3 network and enforces Layer 3 policies by routing traffic through specific policy enforcement points and applying default-deny principles with authorization keys. We prototyped our approach using Kubernetes and Istio and found that it adds less than 1ms of latency while supporting complex policies comparable to native Kubernetes network policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04052v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Behrooz Farkiani, Fan Liu, Patrick Crowley</dc:creator>
    </item>
    <item>
      <title>A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks</title>
      <link>https://arxiv.org/abs/2510.19973</link>
      <description>arXiv:2510.19973v2 Announce Type: replace 
Abstract: The path to higher network autonomy in 6G lies beyond the mere optimization of key performance indicators (KPIs). While KPIs have enabled automation gains under TM Forum Levels 1--3, they remain numerical abstractions that act only as proxies for the real essence of communication networks: seamless connectivity, fairness, adaptability, and resilience. True autonomy requires perceiving and reasoning over the network environment as it is. Such progress can be achieved through \emph{agentic AI}, where large language model (LLM)-powered agents perceive multimodal telemetry, reason with memory, negotiate across domains, and act via APIs to achieve multi-objective goals. However, deploying such agents introduces the challenge of cognitive biases inherited from human design, which can distort reasoning, negotiation, tool use, and actuation. Between neuroscience and AI, this paper provides a tutorial on a selection of well-known biases, including their taxonomy, definition, mathematical formulation, emergence in telecom systems and the commonly impacted agentic components. The tutorial also presents various mitigation strategies tailored to each type of bias. The article finally provides two practical use-cases, which tackle the emergence, impact and mitigation gain of some famous biases in 6G inter-slice and cross-domain management. In particular, anchor randomization, temporal decay and inflection bonus techniques are introduced to specifically address anchoring, temporal and confirmation biases. This avoids that agents stick to the initial high resource allocation proposal or decisions that are recent and/or confirming a prior hypothesis. By grounding decisions in a richer and fairer set of past experiences, the quality and bravery of the agentic agreements in the second use-case, for instance, are leading to $\times 5$ lower latency and around $40\%$ higher energy saving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19973v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hatim Chergui, Farhad Rezazadeh, Merouane Debbah, Christos Verikoukis</dc:creator>
    </item>
    <item>
      <title>Optimizing Energy and Latency in 6G Smart Cities with Edge CyberTwins</title>
      <link>https://arxiv.org/abs/2511.00955</link>
      <description>arXiv:2511.00955v2 Announce Type: replace 
Abstract: The proliferation of IoT devices in smart cities challenges 6G networks with conflicting energy-latency requirements across heterogeneous slices. Existing approaches struggle with the energy-latency trade-off, particularly for massive scale deployments exceeding 50,000 devices km. This paper proposes an edge-aware CyberTwin framework integrating hybrid federated learning for energy-latency co-optimization in 6G network slicing. Our approach combines centralized Artificial Intelligence scheduling for latency-sensitive slices with distributed federated learning for non-critical slices, enhanced by compressive sensing-based digital twins and renewable energy-aware resource allocation. The hybrid scheduler leverages a three-tier architecture with Physical Unclonable Function (PUF) based security attestation achieving 99.7% attack detection accuracy. Comprehensive simulations demonstrate 52% energy reduction for non-real-time slices compared to Diffusion-Reinforcement Learning baselines while maintaining 0.9ms latency for URLLC applications with 99.1% SLA compliance. The framework scales to 50,000 devices km with CPU overhead below 25%, validated through NS-3 hybrid simulations across realistic smart city scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00955v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amine Abouaomar, Badr Ben Elallid, Nabil Benamar</dc:creator>
    </item>
    <item>
      <title>LoLaFL: Low-Latency Federated Learning via Forward-only Propagation</title>
      <link>https://arxiv.org/abs/2412.14668</link>
      <description>arXiv:2412.14668v3 Announce Type: replace-cross 
Abstract: Federated learning (FL) has emerged as a widely adopted paradigm for enabling edge learning with distributed data while ensuring data privacy. However, the traditional FL with deep neural networks trained via backpropagation can hardly meet the low-latency learning requirements in the sixth generation (6G) mobile networks. This challenge mainly arises from the high-dimensional model parameters to be transmitted and the numerous rounds of communication required for convergence due to the inherent randomness of the training process. To address this issue, we adopt the state-of-the-art principle of maximal coding rate reduction to learn linear discriminative features and extend the resultant white-box neural network into FL, yielding the novel framework of Low-Latency Federated Learning (LoLaFL) via forward-only propagation. LoLaFL enables layer-wise transmissions and aggregation with significantly fewer communication rounds, thereby considerably reducing latency. Additionally, we propose two \emph{nonlinear} aggregation schemes for LoLaFL. The first scheme is based on the proof that the optimal NN parameter aggregation in LoLaFL should be harmonic-mean-like. The second scheme further exploits the low-rank structures of the features and transmits the low-rank-approximated covariance matrices of features to achieve additional latency reduction. Theoretic analysis and experiments are conducted to evaluate the performance of LoLaFL. In comparison with traditional FL, the two nonlinear aggregation schemes for LoLaFL can achieve reductions in latency of over 87\% and 97\%, respectively, while maintaining comparable accuracies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14668v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jierui Zhang, Jianhao Huang, Kaibin Huang</dc:creator>
    </item>
    <item>
      <title>Collective Communication for 100k+ GPUs</title>
      <link>https://arxiv.org/abs/2510.20171</link>
      <description>arXiv:2510.20171v3 Announce Type: replace-cross 
Abstract: The increasing scale of large language models (LLMs) necessitates highly efficient collective communication frameworks, particularly as training workloads extend to hundreds of thousands of GPUs. Traditional communication methods face significant throughput and latency limitations at this scale, hindering both the development and deployment of state-of-the-art models. This paper presents the NCCLX collective communication framework, developed at Meta, engineered to optimize performance across the full LLM lifecycle, from the synchronous demands of large-scale training to the low-latency requirements of inference. The framework is designed to support complex workloads on clusters exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency data exchange. Empirical evaluation on the Llama4 model demonstrates substantial improvements in communication efficiency. This research contributes a robust solution for enabling the next generation of LLMs to operate at unprecedented scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20171v3</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Regina Ren, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Yulun Wang, Bruce Wu, Xinfeng Xie, Jingyi Yang, Mingran Yang, Kenny Yu, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Prashanth Kannan, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Srikanth Sundaresan, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng</dc:creator>
    </item>
  </channel>
</rss>
