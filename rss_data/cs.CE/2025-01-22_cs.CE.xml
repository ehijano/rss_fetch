<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Revisiting Ensemble Methods for Stock Trading and Crypto Trading Tasks at ACM ICAIF FinRL Contest 2023-2024</title>
      <link>https://arxiv.org/abs/2501.10709</link>
      <description>arXiv:2501.10709v1 Announce Type: new 
Abstract: Reinforcement learning has demonstrated great potential for performing financial tasks. However, it faces two major challenges: policy instability and sampling bottlenecks. In this paper, we revisit ensemble methods with massively parallel simulations on graphics processing units (GPUs), significantly enhancing the computational efficiency and robustness of trained models in volatile financial markets. Our approach leverages the parallel processing capability of GPUs to significantly improve the sampling speed for training ensemble models. The ensemble models combine the strengths of component agents to improve the robustness of financial decision-making strategies. We conduct experiments in both stock and cryptocurrency trading tasks to evaluate the effectiveness of our approach. Massively parallel simulation on a single GPU improves the sampling speed by up to $1,746\times$ using $2,048$ parallel environments compared to a single environment. The ensemble models have high cumulative returns and outperform some individual agents, reducing maximum drawdown by up to $4.17\%$ and improving the Sharpe ratio by up to $0.21$.
  This paper describes trading tasks at ACM ICAIF FinRL Contests in 2023 and 2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10709v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaus Holzer, Keyi Wang, Kairong Xiao, Xiao-Yang Liu Yanglet</dc:creator>
    </item>
    <item>
      <title>Optimizing MACD Trading Strategies A Dance of Finance, Wavelets, and Genetics</title>
      <link>https://arxiv.org/abs/2501.10808</link>
      <description>arXiv:2501.10808v1 Announce Type: new 
Abstract: In today's financial markets, quantitative trading has become an essential trading method, with the MACD indicator widely employed in quantitative trading strategies. This paper begins by screening and cleaning the dataset, establishing a model that adheres to the basic buy and sell rules of the MACD, and calculating key metrics such as the win rate, return, Sharpe ratio, and maximum drawdown for each stock. However, the MACD often generates erroneous signals in highly volatile markets. To address this, wavelet transform is applied to reduce noise, smoothing the DIF image, and a model is developed based on this to optimize the identification of buy and sell points. The results show that the annualized return has increased by 5%, verifying the feasibility of the method.
  Subsequently, the divergence principle is used to further optimize the trading strategy, enhancing the model's performance. Additionally, a genetic algorithm is employed to optimize the MACD parameters, tailoring the strategy to the characteristics of different stocks. To improve computational efficiency, the MindSpore framework is used for resource management and parallel computing. The optimized strategy demonstrates improved win rates, returns, Sharpe ratios, and a reduction in maximum drawdown in backtesting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10808v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wangyu Chen, Zhenpeng Zhu</dc:creator>
    </item>
    <item>
      <title>Statistical Design of Thermal Protection System Using Physics-Informed Machine learning</title>
      <link>https://arxiv.org/abs/2501.10825</link>
      <description>arXiv:2501.10825v1 Announce Type: new 
Abstract: Estimating the material properties of thermal protection films is crucial for their effective design and application, particularly in high-temperature environments. This work presents a novel approach to determine the properties using uncertainty quantification simulations. We quantify uncertainty in the material properties for effective insulation by proposing a Bayesian distribution for them. Sampling from this distribution is performed using Monte Carlo simulations, which require repeatedly solving the predictive thermal model. To address the computational inefficiency of conventional numerical simulations, we develop a parametric Physics-Informed Neural Network (PINN) to solve the heat transfer problem. The proposed PINN significantly reduces computational time while maintaining accuracy, as verified against traditional numerical solutions. Additionally, we used the Sequential Monte Carlo (SMC) method to enable vectorized and parallel computations, further enhancing computational speedup. Our results demonstrate that integrating MCMC with PINN decreases computational time substantially compared to using standard numerical methods. Moreover, combining the SMC method with PINN yields multifold computational speedup, making this approach highly effective for the rapid and accurate estimation of material properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10825v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karthik Reddy Lyathakula</dc:creator>
    </item>
    <item>
      <title>Anatomy of a Historic Blackout: Decoding Spatiotemporal Dynamics of Power Outages and Disparities During Hurricane Beryl</title>
      <link>https://arxiv.org/abs/2501.10835</link>
      <description>arXiv:2501.10835v1 Announce Type: new 
Abstract: This study investigates the spatial patterns and temporal variations in outage duration, intensity, and restoration/recovery following the 2024 Hurricane Beryl in Houston, Texas. This historic blackout caused widespread power disruptions across the Houston metropolitan area, leaving more than 2 million customers without power over several days, resulting in more than 143 million total customer-out hours.The findings reveal that areas with higher population density and proximity to the hurricane's path experienced more severe initial impacts. Regions with higher median income showed faster recovery, while lower-income areas exhibited prolonged restoration periods, even with favorable infrastructural conditions, suggesting disparities in restoration speed. The study also highlights how urban development features, such as road density and land elevation, explain spatial disparities in power outage impacts and recovery. This research advances the understanding of power outage dynamics in large metropolitan regions through four key contributions: (1) empirical characterization of outages from a historic hurricane, highlighting infrastructure vulnerabilities in a high-density urban context; (2) comprehensive analysis using multiple metrics to capture spatiotemporal dynamics of outages and restoration; (3) leveraging of high-resolution outage data at fine geographic scales and frequent intervals to quantify and reveal previously masked spatial disparities; and (4) systematic examination of socioeconomic, urban development, and environmental factors in shaping disparities in outage impacts and recovery timelines. These findings provide infrastructure managers, operators, utilities, and decision-makers with crucial empirical insights to quantify power outage impacts, justify resilience investments, and address vulnerability and equity issues in the power infrastructure during hazard events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10835v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiangpeng Li, Junwei Ma, Ali Mostafavi</dc:creator>
    </item>
    <item>
      <title>Open FinLLM Leaderboard: Towards Financial AI Readiness</title>
      <link>https://arxiv.org/abs/2501.10963</link>
      <description>arXiv:2501.10963v1 Announce Type: new 
Abstract: Financial large language models (FinLLMs) with multimodal capabilities are envisioned to revolutionize applications across business, finance, accounting, and auditing. However, real-world adoption requires robust benchmarks of FinLLMs' and agents' performance. Maintaining an open leaderboard of models is crucial for encouraging innovative adoption and improving model effectiveness. In collaboration with Linux Foundation and Hugging Face, we create an open FinLLM leaderboard, which serves as an open platform for assessing and comparing LLMs' performance on a wide spectrum of financial tasks. By demoncratizing access to advanced AI tools and financial knowledge, a chatbot or agent may enhance the analytical capabilities of the general public to a professional-level within a few months of usage. This open leaderboard welcomes contributions from academia, open-source community, industry, and stakeholders. In particular, we encourage contributions of new datasets, tasks, and models for continual update. Through fostering a collaborative and open ecosystem, we seek to ensure the long-term sustainability and relevance of LLMs and agents as they evolve with the financial sector's needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10963v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyuan Colin Lin, Felix Tian, Keyi Wang, Xingjian Zhao, Jimin Huang, Qianqian Xie, Luca Borella, Matt White, Christina Dan Wang, Kairong Xiao, Xiao-Yang Liu Yanglet, Li Deng</dc:creator>
    </item>
    <item>
      <title>Machine Learning Surrogates for Optimizing Transportation Policies with Agent-Based Models</title>
      <link>https://arxiv.org/abs/2501.11057</link>
      <description>arXiv:2501.11057v1 Announce Type: new 
Abstract: Rapid urbanization and growing urban populations worldwide present significant challenges for cities, including increased traffic congestion and air pollution. Effective strategies are needed to manage traffic volumes and reduce emissions. In practice, traditional traffic flow simulations are used to test those strategies. However, high computational intensity usually limits their applicability in investigating a magnitude of different scenarios to evaluate best policies. This paper presents a first approach of using Graph Neural Networks (GNN) as surrogates for large-scale agent-based simulation models. In a case study using the MATSim model of Paris, the GNN effectively learned the impacts of capacity reduction policies on citywide traffic flow. Performance analysis across various road types and scenarios revealed that the GNN could accurately capture policy-induced effects on edge-based traffic volumes, particularly on roads directly affected by the policies and those with higher traffic volumes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11057v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elena Natterer, Roman Engelhardt, Sebastian H\"orl, Klaus Bogenberger</dc:creator>
    </item>
    <item>
      <title>Efficient and accurate simulation of the Smith-Zener pinning mechanism during grain growth using a front-tracking numerical framework</title>
      <link>https://arxiv.org/abs/2501.11130</link>
      <description>arXiv:2501.11130v1 Announce Type: new 
Abstract: This study proposes a new full-field approach for modeling grain boundary pinning by second phase particles in two-dimensional polycrystals. These particles are of great importance during thermomechanical treatments, as they produce deviations from the microstructural evolution that the alloy produces in the absence of particles. This phenomenon, well-known as Smith-Zener pinning, is widely used by metallurgists to control the grain size during the metal forming process of many alloys. Predictive tools are then needed to accurately model this phenomenon. This article introduces a new methodology for the simulation of microstructural evolutions subjected to the presence of second phase particles. The methodology employs a Lagrangian 2D front-tracking methodology, while the particles are modeled using discretized circular shapes or pinning nodes. The evolution of the particles can be considered and modeled using a constant velocity of particle shrinking. This approach has the advantages of improving the limited description made of the phenomenon in vertex approaches, to be usable for a wide range of second-phase particle sizes and to improve calculation times compared to front-capturing type approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11130v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Florez, Marc Bernacki</dc:creator>
    </item>
    <item>
      <title>Kilometer-Scale E3SM Land Model Simulation over North America</title>
      <link>https://arxiv.org/abs/2501.11141</link>
      <description>arXiv:2501.11141v1 Announce Type: new 
Abstract: The development of a kilometer-scale E3SM Land Model (km-scale ELM) is an integral part of the E3SM project, which seeks to advance energy-related Earth system science research with state-of-the-art modeling and simulation capabilities on exascale computing systems. Through the utilization of high-fidelity data products, such as atmospheric forcing and soil properties, the km-scale ELM plays a critical role in accurately modeling geographical characteristics and extreme weather occurrences. The model is vital for enhancing our comprehension and prediction of climate patterns, as well as their effects on ecosystems and human activities.
  This study showcases the first set of full-capability, km-scale ELM simulations over various computational domains, including simulations encompassing 21.6 million land gridcells, reflecting approximately 21.5 million square kilometers of North America at a 1 km x 1 km resolution. We present the largest km-scale ELM simulation using up to 100,800 CPU cores across 2,400 nodes. This continental-scale simulation is 300 times larger than any previous studies, and the computational resources used are about 400 times larger than those used in prior efforts. Both strong and weak scaling tests have been conducted, revealing exceptional performance efficiency and resource utilization.
  The km-scale ELM uses the common E3SM modeling infrastructure and a general data toolkit known as KiloCraft. Consequently, it can be readily adapted for both fully-coupled E3SM simulations and data-driven simulations over specific areas, ranging from a single gridcell to the entire North America.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11141v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dali Wang, Chen Wang, Qinglei Cao, Peter Schwartz, Fengming Yuan, Jayesh Krishna, Danqing Wu, Danial Ricciuto, Peter Thornton, Shih-Chieh Kao, Michele Thornton, Kathryn Mohror</dc:creator>
    </item>
    <item>
      <title>Blockchain and Stablecoin Integration for Crowdfunding: A framework for enhanced efficiency, security, and liquidity</title>
      <link>https://arxiv.org/abs/2501.11145</link>
      <description>arXiv:2501.11145v1 Announce Type: new 
Abstract: Crowdfunding platforms face high transaction fees, need for more transparency, and trust deficits. These issues deter contributors and entrepreneurs from effectively leveraging crowdfunding for innovation and growth. Blockchain technology introduces decentralization, security, and efficiency to address these limitations (1). This paper proposes a blockchain-based crowdfunding framework that integrates stablecoins such as USDT and USDC to mitigate cryptocurrency volatility and ensure seamless fund management. Smart contracts automate compliance processes, including Know Your Customer (KYC) / Anti-Money Laundering (AML) checks, and enhance operational efficiency (2). Furthermore, tokenization enables liquidity by allowing fractional ownership and secondary market trading, which must be effectively implemented on any global market platform. A comparative analysis highlights the superiority of the framework over traditional platforms in terms of cost reduction, transparency, and investor trust. A case study focused on the Turkish market illustrates the practical benefits of blockchain adoption in equity crowdfunding, particularly in navigating local regulatory and financial complexities. This approach provides a scalable, secure, and accessible solution for modern crowdfunding ecosystems, while reducing the costs of platforms and increasing the trust of investors and backers in crowdfunding projects. Keywords Blockchain, stablecoins, crowdfunding, tokenization, and compliance</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11145v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mustafa Savas Unsal</dc:creator>
    </item>
    <item>
      <title>Mapping network structures and dynamics of decentralised cryptocurrencies: The evolution of Bitcoin (2009-2023)</title>
      <link>https://arxiv.org/abs/2501.11416</link>
      <description>arXiv:2501.11416v1 Announce Type: new 
Abstract: Cryptocurrencies have recently been in the spotlight of public debate due to their embrace by the new US President, with crypto fans expecting a 'bull run'. The global cryptocurrency market capitalisation is more than \$3.50 trillion, with 1 Bitcoin exchanging for more than \$97,000 at the end of November 2024. Monitoring the evolution of these systems is key to understanding whether the popular perception of cryptocurrencies as a new, sustainable economic infrastructure is well-founded. In this paper, we have reconstructed the network structures and dynamics of Bitcoin from its launch in January 2009 to December 2023 and identified its key evolutionary phases. Our results show that network centralisation and wealth concentration increased from the very early years, following a richer-get-richer mechanism. This trend was endogenous to the system, beyond any subsequent institutional or exogenous influence. The evolution of Bitcoin is characterised by three periods, Exploration, Adaptation and Maturity, with substantial coherent network patterns. Our findings suggest that Bitcoin is a highly centralised structure, with high levels of wealth inequality and internally crystallised power dynamics, which may have negative implications for its long-term sustainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11416v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Venturini, Daniel Garc\'ia-Costa, Elena \'Alvarez-Garc\'ia, Francisco Grimaldo, Flaminio Squazzoni</dc:creator>
    </item>
    <item>
      <title>Multi-source Multi-level Multi-token Ethereum Dataset and Benchmark Platform</title>
      <link>https://arxiv.org/abs/2501.11906</link>
      <description>arXiv:2501.11906v1 Announce Type: new 
Abstract: This paper introduces 3MEthTaskforce (https://3meth.github.io), a multi-source, multi-level, and multi-token Ethereum dataset addressing the limitations of single-source datasets. Integrating over 300 million transaction records, 3,880 token profiles, global market indicators, and Reddit sentiment data from 2014-2024, it enables comprehensive studies on user behavior, market sentiment, and token performance. 3MEthTaskforce defines benchmarks for user behavior prediction and token price prediction tasks, using 6 dynamic graph networks and 19 time-series models to evaluate performance. Its multimodal design supports risk analysis and market fluctuation modeling, providing a valuable resource for advancing blockchain analytics and decentralized finance research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11906v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyuan Li, Mengxiao Zhang, Maoyuan Li, Jianzheng Li, Junyi Yang, Shuangyan Deng, Zijian Zhang, Jiamou Liu</dc:creator>
    </item>
    <item>
      <title>Select2Drive: Pragmatic Communications for Real-Time Collaborative Autonomous Driving</title>
      <link>https://arxiv.org/abs/2501.12040</link>
      <description>arXiv:2501.12040v1 Announce Type: new 
Abstract: Vehicle-to-Everything communications-assisted Autonomous Driving (V2X-AD) has witnessed remarkable advancements in recent years, with pragmatic communications (PragComm) emerging as a promising paradigm for real-time collaboration among vehicles and other agents.Simultaneously, extensive research has explored the interplay between collaborative perception and decision-making in end-to-end driving frameworks.In this work, we revisit the collaborative driving problem and propose the Select2Drive framework to optimize the utilization of limited computational and communication resources.Particularly, to mitigate cumulative latency in perception and decision-making, Select2Drive introduces Distributed Predictive Perception (DPP) by formulating an active prediction paradigm and simplifies high-dimensional semantic feature prediction into computation cost-efficient, motion-aware reconstruction. Given the "less is more" principle that a broadened perceptual horizon possibly confuses the decision module rather than contributing to it, Select2Drive utilizes Area-of-Importance-based PragComm (APC) to prioritize the communications of critical regions, thus boosting both communication efficiency and decision-making efficacy. Empirical evaluations on the V2Xverse dataset and CARLA driving simulator demonstrate that Select2Drive achieves a 11.31% (resp. 7.69%) improvement in offline perception tasks under limited bandwidth (resp. pose error conditions). Moreover, it delivers at most 14.68% and 31.76% enhancement in closed-loop driving scores and route completion rates, particularly in scenarios characterized by dense traffic and high-speed dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12040v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Huang, Jianhang Zhu, Rongpeng Li, Zhifeng Zhao, Honggang Zhang</dc:creator>
    </item>
    <item>
      <title>PINNsAgent: Automated PDE Surrogation with Large Language Models</title>
      <link>https://arxiv.org/abs/2501.12053</link>
      <description>arXiv:2501.12053v1 Announce Type: new 
Abstract: Solving partial differential equations (PDEs) using neural methods has been a long-standing scientific and engineering research pursuit. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative to traditional numerical methods for solving PDEs. However, the gap between domain-specific knowledge and deep learning expertise often limits the practical application of PINNs. Previous works typically involve manually conducting extensive PINNs experiments and summarizing heuristic rules for hyperparameter tuning. In this work, we introduce PINNsAgent, a novel surrogation framework that leverages large language models (LLMs) and utilizes PINNs as a foundation to bridge the gap between domain-specific knowledge and deep learning. Specifically, PINNsAgent integrates (1) Physics-Guided Knowledge Replay (PGKR), which encodes the essential characteristics of PDEs and their associated best-performing PINNs configurations into a structured format, enabling efficient knowledge transfer from solved PDEs to similar problems and (2) Memory Tree Reasoning, a strategy that effectively explores the search space for optimal PINNs architectures. By leveraging LLMs and exploration strategies, PINNsAgent enhances the automation and efficiency of PINNs-based solutions. We evaluate PINNsAgent on 14 benchmark PDEs, demonstrating its effectiveness in automating the surrogation process and significantly improving the accuracy of PINNs-based solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12053v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingpo Wuwu, Chonghan Gao, Tianyu Chen, Yihang Huang, Yuekai Zhang, Jianing Wang, Jianxin Li, Haoyi Zhou, Shanghang Zhang</dc:creator>
    </item>
    <item>
      <title>Monetary Evolution: How Societies Shaped Money from Antiquity to Cryptocurrencies</title>
      <link>https://arxiv.org/abs/2501.10443</link>
      <description>arXiv:2501.10443v1 Announce Type: cross 
Abstract: With the growing popularity and rising value of cryptocurrencies, skepticism surrounding this groundbreaking innovation persists. Many financial and business experts argue that the value created in the cryptocurrency realm resembles the generation of currency from thin air. However, a historical analysis of the fundamental concepts that have shaped money reveals striking parallels with past transformations in human society. This study extends these historical insights to the present era, demonstrating how enduring monetary concepts are once again redefining our understanding of money and reshaping its form. Additionally, we offer novel interpretations of cryptocurrency by linking the intrinsic nature of money, the communities it fosters, and the cryptographic technologies that have provided the infrastructure for this transformative shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10443v1</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahya Karbalaii</dc:creator>
    </item>
    <item>
      <title>Efficient Traffic Prediction Through Spatio-Temporal Distillation</title>
      <link>https://arxiv.org/abs/2501.10459</link>
      <description>arXiv:2501.10459v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) have gained considerable attention in recent years for traffic flow prediction due to their ability to learn spatio-temporal pattern representations through a graph-based message-passing framework. Although GNNs have shown great promise in handling traffic datasets, their deployment in real-life applications has been hindered by scalability constraints arising from high-order message passing. Additionally, the over-smoothing problem of GNNs may lead to indistinguishable region representations as the number of layers increases, resulting in performance degradation. To address these challenges, we propose a new knowledge distillation paradigm termed LightST that transfers spatial and temporal knowledge from a high-capacity teacher to a lightweight student. Specifically, we introduce a spatio-temporal knowledge distillation framework that helps student MLPs capture graph-structured global spatio-temporal patterns while alleviating the over-smoothing effect with adaptive knowledge distillation. Extensive experiments verify that LightST significantly speeds up traffic flow predictions by 5X to 40X compared to state-of-the-art spatio-temporal GNNs, all while maintaining superior accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10459v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>AAAI'2025</arxiv:journal_reference>
      <dc:creator>Qianru Zhang, Xinyi Gao, Haixin Wang, Siu-Ming Yiu, Hongzhi Yin</dc:creator>
    </item>
    <item>
      <title>Using Domain Knowledge with Deep Learning to Solve Applied Inverse Problems</title>
      <link>https://arxiv.org/abs/2501.10481</link>
      <description>arXiv:2501.10481v1 Announce Type: cross 
Abstract: Advancements in deep learning have improved the ability to model complex, nonlinear relationships, such as those encountered in complex material inverse problems. However, the effectiveness of these methods often depends on large datasets, which are not always available. In this study, the incorporation of domain-specific knowledge of mechanical behavior is investigated to evaluate the impact on the predictive performance of the models in data-scarce scenarios. To demonstrate this, stress-strain curves were used to predict key microstructural features of porous materials, and the performance of models trained with and without domain knowledge was compared using five deep learning models: Convolutional Neural Networks, Extreme Gradient Boosting, K-Nearest Neighbors, Long Short-Term Memory, and Random Forest. The results of the models with domain-specific characteristics consistently achieved higher $R^2$ values and improved learning efficiency compared to models without prior knowledge. When the models did not include domain knowledge, the model results revealed meaningful patterns were not recognized, while those enhanced with mechanical insights showed superior feature extraction and predictions. These findings underscore the critical role of domain knowledge in guiding deep learning models, highlighting the need to combine domain expertise with data-driven approaches to achieve reliable and accurate outcomes in materials science and related fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10481v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinyi Tian, Winston Lindqwister, Manolis Veveakis, Laura E. Dalton</dc:creator>
    </item>
    <item>
      <title>Deep Operator Networks for Bayesian Parameter Estimation in PDEs</title>
      <link>https://arxiv.org/abs/2501.10684</link>
      <description>arXiv:2501.10684v1 Announce Type: cross 
Abstract: We present a novel framework combining Deep Operator Networks (DeepONets) with Physics-Informed Neural Networks (PINNs) to solve partial differential equations (PDEs) and estimate their unknown parameters. By integrating data-driven learning with physical constraints, our method achieves robust and accurate solutions across diverse scenarios. Bayesian training is implemented through variational inference, allowing for comprehensive uncertainty quantification for both aleatoric and epistemic uncertainties. This ensures reliable predictions and parameter estimates even in noisy conditions or when some of the physical equations governing the problem are missing. The framework demonstrates its efficacy in solving forward and inverse problems, including the 1D unsteady heat equation and 2D reaction-diffusion equations, as well as regression tasks with sparse, noisy observations. This approach provides a computationally efficient and generalizable method for addressing uncertainty quantification in PDE surrogate modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10684v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amogh Raj, Carol Eunice Gudumotou, Sakol Bun, Keerthana Srinivasa, Arash Sarshar</dc:creator>
    </item>
    <item>
      <title>An Analysis of the Correctness and Computational Complexity of Path Planning in Payment Channel Networks</title>
      <link>https://arxiv.org/abs/2501.11419</link>
      <description>arXiv:2501.11419v1 Announce Type: cross 
Abstract: Payment Channel Networks (PCNs) are a method for improving the scaling and latency of cryptocurrency transactions. For a payment to be made between two peers in a PCN, a feasible low-fee path in the network must be planned. Many PCN path planning algorithms use a search algorithm that is a variant of Dijkstra's algorithm. In this article, we prove the correctness and computational complexity of this algorithm. Specifically, we show that, if the PCN satisfies a consistency property relating to the fees charged by payment channels, the algorithm is correct and has polynomial computational complexity. However, in the general case, the algorithm is not correct and the path planning problem is NP-hard. These newly developed results can be used to inform the development of new or existing PCNs amenable to path planning. For example, we show that the Lightning Network, which is the most widely used PCN and is built on the Bitcoin cryptocurrency, currently satisfies the above consistency property. As a second contribution, we demonstrate that a small modification to the above path planning algorithm which, although having the same asymptotic computational complexity, empirically shows better performance. This modification involves the use of a bidirectional search and is empirically evaluated by simulating transactions on the Lightning Network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11419v1</guid>
      <category>cs.DM</category>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Padraig Corcoran, Rhyd Lewis</dc:creator>
    </item>
    <item>
      <title>Biomedical Knowledge Graph: A Survey of Domains, Tasks, and Real-World Applications</title>
      <link>https://arxiv.org/abs/2501.11632</link>
      <description>arXiv:2501.11632v1 Announce Type: cross 
Abstract: Biomedical knowledge graphs (BKGs) have emerged as powerful tools for organizing and leveraging the vast and complex data found across the biomedical field. Yet, current reviews of BKGs often limit their scope to specific domains or methods, overlooking the broader landscape and the rapid technological progress reshaping it. In this survey, we address this gap by offering a systematic review of BKGs from three core perspectives: domains, tasks, and applications. We begin by examining how BKGs are constructed from diverse data sources, including molecular interactions, pharmacological datasets, and clinical records. Next, we discuss the essential tasks enabled by BKGs, focusing on knowledge management, retrieval, reasoning, and interpretation. Finally, we highlight real-world applications in precision medicine, drug discovery, and scientific research, illustrating the translational impact of BKGs across multiple sectors. By synthesizing these perspectives into a unified framework, this survey not only clarifies the current state of BKG research but also establishes a foundation for future exploration, enabling both innovative methodological advances and practical implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11632v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.IR</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxing Lu, Sin Yee Goi, Xukai Zhao, Jinzhuo Wang</dc:creator>
    </item>
    <item>
      <title>General Field Evaluation in High-Order Meshes on GPUs</title>
      <link>https://arxiv.org/abs/2501.12349</link>
      <description>arXiv:2501.12349v1 Announce Type: cross 
Abstract: Robust and scalable function evaluation at any arbitrary point in the finite/spectral element mesh is required for querying the partial differential equation solution at points of interest, comparison of solution between different meshes, and Lagrangian particle tracking. This is a challenging problem, particularly for high-order unstructured meshes partitioned in parallel with MPI, as it requires identifying the element that overlaps a given point and computing the corresponding reference space coordinates. We present a robust and efficient technique for general field evaluation in large-scale high-order meshes with quadrilaterals and hexahedra. In the proposed method, a combination of globally partitioned and processor-local maps are used to first determine a list of candidate MPI ranks, and then locally candidate elements that could contain a given point. Next, element-wise bounding boxes further reduce the list of candidate elements. Finally, Newton's method with trust region is used to determine the overlapping element and corresponding reference space coordinates. Since GPU-based architectures have become popular for accelerating computational analyses using meshes with tensor-product elements, specialized kernels have been developed to utilize the proposed methodology on GPUs. The method is also extended to enable general field evaluation on surface meshes. The paper concludes by demonstrating the use of proposed method in various applications ranging from mesh-to-mesh transfer during r-adaptivity to Lagrangian particle tracking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12349v1</guid>
      <category>cs.MS</category>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ketan Mittal, Aditya Parik, Som Dutta, Paul Fischer, Tzanio Kolev, James Lottes</dc:creator>
    </item>
    <item>
      <title>Intraday Power Trading for Imbalance Markets: An Adaptive Risk-Averse Strategy using Mixture Models</title>
      <link>https://arxiv.org/abs/2402.01215</link>
      <description>arXiv:2402.01215v2 Announce Type: replace 
Abstract: Efficient markets are characterised by profit-driven participants continuously refining their positions towards the latest insights. Margins for profit generation are generally small, shaping a difficult landscape for automated trading strategies. This paper introduces a novel intraday power trading strategy tailored for single-price balancing markets. The strategy relies on a strategically devised mixture model to forecast future system imbalance prices and is formulated as a stochastic optimization problem with decision-dependent distributions to address two primary challenges: (i) the impact of trading positions on the system imbalance price and (ii) the uncertainty inherent in the model. The first challenge is tackled by adjusting the model to account for price changes after taking a position. For the second challenge, a coherent risk measure is added to the cost function to take additional uncertainties into account. This paper introduces a methodology to select the tuning parameter of this risk measure adaptively by continuously quantifying the performance of the strategy on a window of recently observed data. The strategy is validated with a simulation on the Belgian electricity market using real-time market data. The adaptive tuning approach leads to higher absolute profits, while also reducing the number of trades.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01215v2</guid>
      <category>cs.CE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Robin Bruneel, Mathijs Schuurmans, Panagiotis Patrinos</dc:creator>
    </item>
    <item>
      <title>Liquidity Fragmentation or Optimization? Analyzing Automated Market Makers Across Ethereum and Rollups</title>
      <link>https://arxiv.org/abs/2410.10324</link>
      <description>arXiv:2410.10324v2 Announce Type: replace 
Abstract: Layer-2 (L2) blockchains offer security guarantees for Ethereum while reducing transaction (gas) fees. Consequently, they are gaining popularity among traders at Automated Market Makers (AMMs), but Liquidity Providers (LPs) are lagging behind. Our empirical results show that AMM liquidity pools on Ethereum are oversubscribed compared to their counterparties on L2s and deliver lower returns than staking ETH. LPs would receive higher rewards by reallocating over 2/3 of the liquidity to AMMs on L2s, or staking. We employ Lagrangian optimization to find the optimal liquidity allocation strategy that maximizes LP's rewards. Moreover, we show that the returns from liquidity provisions converge to the staking rate, and in equilibrium, liquidity provisions to any AMM should provide returns equal to staking rewards. Lastly, we measure the elasticity of trading volume with respect to TVL at AMM pools and found that at the well established blockchains an increase in TVL is not associated with an increase in trading volume.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10324v2</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krzysztof Gogol, Manvir Schneider, Claudio Tessone, Benjamin Livshits</dc:creator>
    </item>
    <item>
      <title>A Theoretical Framework for an Efficient Normalizing Flow-Based Solution to the Electronic Schrodinger Equation</title>
      <link>https://arxiv.org/abs/2406.00047</link>
      <description>arXiv:2406.00047v3 Announce Type: replace-cross 
Abstract: A central problem in quantum mechanics involves solving the Electronic Schrodinger Equation for a molecule or material. The Variational Monte Carlo approach to this problem approximates a particular variational objective via sampling, and then optimizes this approximated objective over a chosen parameterized family of wavefunctions, known as the ansatz. Recently neural networks have been used as the ansatz, with accompanying success. However, sampling from such wavefunctions has required the use of a Markov Chain Monte Carlo approach, which is inherently inefficient. In this work, we propose a solution to this problem via an ansatz which is cheap to sample from, yet satisfies the requisite quantum mechanical properties. We prove that a normalizing flow using the following two essential ingredients satisfies our requirements: (a) a base distribution which is constructed from Determinantal Point Processes; (b) flow layers which are equivariant to a particular subgroup of the permutation group. We then show how to construct both continuous and discrete normalizing flows which satisfy the requisite equivariance. We further demonstrate the manner in which the non-smooth nature ("cusps") of the wavefunction may be captured, and how the framework may be generalized to provide induction across multiple molecules. The resulting theoretical framework entails an efficient approach to solving the Electronic Schrodinger Equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00047v3</guid>
      <category>physics.chem-ph</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Freedman, Eyal Rozenberg, Alex Bronstein</dc:creator>
    </item>
    <item>
      <title>A Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges</title>
      <link>https://arxiv.org/abs/2412.04782</link>
      <description>arXiv:2412.04782v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have transformed numerous domains by providing advanced capabilities in natural language understanding, generation, and reasoning. Despite their groundbreaking applications across industries such as research, healthcare, and creative media, their rapid adoption raises critical concerns regarding sustainability. This survey paper comprehensively examines the environmental, economic, and computational challenges associated with LLMs, focusing on energy consumption, carbon emissions, and resource utilization in data centers. By synthesizing insights from existing literature, this work explores strategies such as resource-efficient training, sustainable deployment practices, and lifecycle assessments to mitigate the environmental impacts of LLMs. Key areas of emphasis include energy optimization, renewable energy integration, and balancing performance with sustainability. The findings aim to guide researchers, practitioners, and policymakers in developing actionable strategies for sustainable AI systems, fostering a responsible and environmentally conscious future for artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04782v2</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aditi Singh, Nirmal Prakashbhai Patel, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei</dc:creator>
    </item>
    <item>
      <title>An Adaptive Collocation Point Strategy For Physics Informed Neural Networks via the QR Discrete Empirical Interpolation Method</title>
      <link>https://arxiv.org/abs/2501.07700</link>
      <description>arXiv:2501.07700v3 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) have gained significant attention for solving forward and inverse problems related to partial differential equations (PDEs). While advancements in loss functions and network architectures have improved PINN accuracy, the impact of collocation point sampling on their performance remains underexplored. Fixed sampling methods, such as uniform random sampling and equispaced grids, can fail to capture critical regions with high solution gradients, limiting their effectiveness for complex PDEs. Adaptive methods, inspired by adaptive mesh refinement from traditional numerical methods, address this by dynamically updating collocation points during training but may overlook residual dynamics between updates, potentially losing valuable information. To overcome this limitation, we propose an adaptive collocation point selection strategy utilizing the QR Discrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling technique for efficiently approximating nonlinear functions. Our results on benchmark PDEs, including the wave, Allen-Cahn, and Burgers' equations, demonstrate that our QR-DEIM-based approach improves PINN accuracy compared to existing methods, offering a promising direction for adaptive collocation point strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07700v3</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian Celaya, David Fuentes, Beatrice Riviere</dc:creator>
    </item>
  </channel>
</rss>
