<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 04:12:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards the Datasets Used in Requirements Engineering of Mobile Apps: Preliminary Findings from a Systematic Mapping Study</title>
      <link>https://arxiv.org/abs/2509.03541</link>
      <description>arXiv:2509.03541v1 Announce Type: new 
Abstract: [Background] Research on requirements engineering (RE) for mobile apps employs datasets formed by app users, developers or vendors. However, little is known about the sources of these datasets in terms of platforms and the RE activities that were researched with the help of the respective datasets. [Aims] The goal of this paper is to investigate the state-of-the-art of the datasets of mobile apps used in existing RE research. [Method] We carried out a systematic mapping study by following the guidelines of Kitchenham et al. [Results] Based on 43 selected papers, we found that Google Play and Apple App Store provide the datasets for more than 90% of published research in RE for mobile apps. We also found that the most investigated RE activities - based on datasets, are requirements elicitation and requirements analysis. [Conclusions] Our most important conclusions are: (1) there is a growth in the use of datasets for RE research of mobile apps since 2012, (2) the RE knowledge for mobile apps might be skewed due to the overuse of Google Play and Apple App Store, (3) there are attempts to supplement reviews of apps from repositories with other data sources, (4) there is a need to expand the alternative sources and experiments with complimentary use of multiple sources, if the community wants more generalizable results. Plus, it is expected to expand the research on other RE activities, beyond elicitation and analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03541v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chong Wang, Haoning Wu, Peng Liang, Maya Daneva, Marten van Sinderen</dc:creator>
    </item>
    <item>
      <title>A Multi-stage Error Diagnosis for APB Transaction</title>
      <link>https://arxiv.org/abs/2509.03554</link>
      <description>arXiv:2509.03554v1 Announce Type: new 
Abstract: Functional verification and debugging are critical bottlenecks in modern System-on-Chip (SoC) design, with manual detection of Advanced Peripheral Bus (APB) transaction errors in large Value Change Dump (VCD) files being inefficient and error-prone. Addressing the 2025 ICCAD Contest Problem D, this study proposes an automated error diagnosis framework using a hierarchical Random Forest-based architecture. The multi-stage error diagnosis employs four pre-trained binary classifiers to sequentially detect Out-of-Range Access, Address Corruption, and Data Corruption errors, prioritizing high-certainty address-related faults before tackling complex data errors to enhance efficiency. Experimental results show an overall accuracy of 91.36%, with near-perfect precision and recall for address errors and robust performance for data errors. Although the final results of the ICCAD 2025 CAD Contest are yet to be announced as of the submission date, our team achieved first place in the beta stage, highlighting the method's competitive strength. This research validates the potential of hierarchical machine learning as a powerful automated tool for hardware debugging in Electronic Design Automation (EDA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03554v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng-Yang Tsai, Tzu-Wei Huang, Jen-Wei Shih, I-Hsiang Wang, Yu-Cheng Lin, Rung-Bin Lin</dc:creator>
    </item>
    <item>
      <title>Parse Tree Tracking Through Time for Programming Process Analysis at Scale</title>
      <link>https://arxiv.org/abs/2509.03668</link>
      <description>arXiv:2509.03668v1 Announce Type: new 
Abstract: Background and Context: Programming process data can be utilized to understand the processes students use to write computer programming assignments. Keystroke- and line-level event logs have been used in the past in various ways, primarily in high-level descriptive statistics (e.g., timings, character deletion rate, etc). Analysis of behavior in context (e.g., how much time students spend working on loops) has been cumbersome because of our inability to automatically track high-level code representations, such as abstract syntax trees, through time and unparseable states.
  Objective: Our study has two goals. The first is to design the first algorithm that tracks parse tree nodes through time. Second, we utilize this algorithm to perform a partial replication study of prior work that used manual tracking of code representations, as well as other novel analyses of student programming behavior that can now be done at scale.
  Method: We use two algorithms presented in this paper to track parse tree nodes through time and construct tree representations for unparseable code states. We apply these algorithms to a public keystroke data from student coursework in a 2021 CS1 course and conduct analysis on the resulting parse trees.
  Findings: We discover newly observable statistics at scale, including that code is deleted at similar rates inside and outside of conditionals and loops, a third of commented out code is eventually restored, and that frequency with which students jump around in their code may not be indicative of struggle.
  Implications: The ability to track parse trees through time opens the door to understanding new dimensions of student programming, such as best practices of structural development of code over time, quantitative measurement of what syntactic constructs students struggle most with, refactoring behavior, and attention shifting within the code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03668v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matt Rau, Chris Brown, John Edwards</dc:creator>
    </item>
    <item>
      <title>Towards an Understanding of Developer Experience-Driven Transparency in Software Ecosystems</title>
      <link>https://arxiv.org/abs/2509.03848</link>
      <description>arXiv:2509.03848v1 Announce Type: new 
Abstract: Software ecosystems (SECO) have become a dominant paradigm in the software industry, enabling third-party developers to co-create value through complementary components and services. While Developer Experience (DX) is increasingly recognized as critical for sustainable SECO, transparency remains an underexplored factor shaping how developers perceive and interact with ecosystems. Existing studies acknowledge transparency as essential for trust, fairness, and engagement, yet its relationship with DX has not been systematically conceptualized. Hence, this work aims to advance the understanding of transparency in SECO from a developer-centered perspective. To this end, we propose SECO-TransDX (Transparency in Software Ecosystems from a Developer Experience Perspective), a conceptual model that introduces the notion of DX-driven transparency. The model identifies 63 interrelated concepts, including conditioning factors, ecosystem procedures, artifacts, and relational dynamics that influence how transparency is perceived and constructed during developer interactions. SECO-TransDX was built upon prior research and refined through a Delphi study with experts from academia and industry. It offers a structured lens to examine how transparency mediates DX across technical, social, and organizational layers. For researchers, it lays the groundwork for future studies and tool development; for practitioners, it supports the design of trustworthy, developer-centered platforms that improve transparency and foster long-term engagement in SECO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03848v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rodrigo Oliveira Zacarias, Rodrigo Pereira dos Santos, Patricia Lago</dc:creator>
    </item>
    <item>
      <title>VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from Rich-Text Issue Report</title>
      <link>https://arxiv.org/abs/2509.03875</link>
      <description>arXiv:2509.03875v1 Announce Type: new 
Abstract: Software vulnerabilities exist in open-source software (OSS), and the developers who discover these vulnerabilities may submit issue reports (IRs) to describe their details. Security practitioners need to spend a lot of time manually identifying vulnerability-related IRs from the community, and the time gap may be exploited by attackers to harm the system. Previously, researchers have proposed automatic approaches to facilitate identifying these vulnerability-related IRs, but these works focus on textual descriptions but lack the comprehensive analysis of IR's rich-text information. In this paper, we propose VulRTex, a reasoning-guided approach to identify vulnerability-related IRs with their rich-text information. In particular, VulRTex first utilizes the reasoning ability of the Large Language Model (LLM) to prepare the Vulnerability Reasoning Database with historical IRs. Then, it retrieves the relevant cases from the prepared reasoning database to generate reasoning guidance, which guides LLM to identify vulnerabilities by reasoning analysis on target IRs' rich-text information. To evaluate the performance of VulRTex, we conduct experiments on 973,572 IRs, and the results show that VulRTex achieves the highest performance in identifying the vulnerability-related IRs and predicting CWE-IDs when the dataset is imbalanced, outperforming the best baseline with +11.0% F1, +20.2% AUPRC, and +10.5% Macro-F1, and 2x lower time cost than baseline reasoning approaches. Furthermore, VulRTex has been applied to identify 30 emerging vulnerabilities across 10 representative OSS projects in 2024's GitHub IRs, and 11 of them are successfully assigned CVE-IDs, which illustrates VulRTex's practicality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03875v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyou Jiang, Mingyang Li, Guowei Yang, Lin Shi, Qing Wang</dc:creator>
    </item>
    <item>
      <title>Vulnerability-Affected Versions Identification: How Far Are We?</title>
      <link>https://arxiv.org/abs/2509.03876</link>
      <description>arXiv:2509.03876v1 Announce Type: new 
Abstract: Identifying which software versions are affected by a vulnerability is critical for patching, risk mitigation.Despite a growing body of tools, their real-world effectiveness remains unclear due to narrow evaluation scopes often limited to early SZZ variants, outdated techniques, and small or coarse-graineddatasets. In this paper, we present the first comprehensive empirical study of vulnerability affected versions identification. We curate a high quality benchmark of 1,128 real-world C/C++ vulnerabilities and systematically evaluate 12 representative tools from both tracing and matching paradigms across four dimensions: effectiveness at both vulnerability and version levels, root causes of false positives and negatives, sensitivity to patch characteristics, and ensemble potential. Our findings reveal fundamental limitations: no tool exceeds 45.0% accuracy, with key challenges stemming from heuristic dependence, limited semantic reasoning, and rigid matching logic. Patch structures such as add-only and cross-file changes further hinder performance. Although ensemble strategies can improve results by up to 10.1%, overall accuracy remains below 60.0%, highlighting the need for fundamentally new approaches. Moreover, our study offers actionable insights to guide tool development, combination strategies, and future research in this critical area. Finally, we release the replicated code and benchmark on our website to encourage future contributions.outdated techniques, and small or coarse grained datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03876v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingchu Chen, Chengwei Liu, Jialun Cao, Yang Xiao, Xinyue Cai, Yeting Li, Jingyi Shi, Tianqi Sun, Haiming Chen ang Wei Huo</dc:creator>
    </item>
    <item>
      <title>Analyzing Variations in Dependency Distributions Due to Code Smell Interactions</title>
      <link>https://arxiv.org/abs/2509.03896</link>
      <description>arXiv:2509.03896v1 Announce Type: new 
Abstract: The existence of dependencies between modules, such as classes, can mean that changing a module triggers ripple effects that make maintenance complex and costly, so the advice is to minimize dependencies between modules. It is therefore important to understand the circumstances that can lead to increased dependencies. Recent studies suggest that code smells, which are characteristics of code that indicate potential design issues, may interact in ways that increase dependencies between modules. In this study, we aim to confirm previous observations and investigate whether and how the distribution of static dependencies changes in the presence of code smell interactions. We conducted a dependency analysis on 116 open-source Java systems to quantify the interactions, comparing interactions among code smells and interactions between code smells and non-code smells. Our results suggest that while interactions between code smell pairs are associated with increases in certain dependencies and decreases in others, overall, they are associated with an increase in total dependencies. For example, the median number of dependencies between Feature Envy methods and Data Classes is seven times as many as when the methods are non-Feature Envy methods, increasing from 1 to 7. This implies that developers should prioritize addressing code smells that interact with each other, rather than code smells that exist only in isolation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03896v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zushuai Zhang, Elliott Wen, Ewan Tempero</dc:creator>
    </item>
    <item>
      <title>The Auth Shim: A Lightweight Architectural Pattern for Integrating Enterprise SSO with Standalone Open-Source Applications</title>
      <link>https://arxiv.org/abs/2509.03900</link>
      <description>arXiv:2509.03900v1 Announce Type: new 
Abstract: Open-source software OSS is widely adopted in enterprise settings, but standalone tools often lack native support for protocols like SAML or OIDC, creating a critical security integration gap. This paper introduces and formalizes the Auth Shim, a lightweight architectural pattern designed to solve this problem. The Auth Shim is a minimal, external proxy service that acts as a compatibility layer, translating requests from an enterprise Identity Provider IdP into the native session management mechanism of a target application. A key prerequisite for this pattern is that the target application must expose a programmatic, secure administrative API. We present a case study of the pattern's implementation at Adobe to integrate a popular OSS BI tool with Okta SAML, which enabled automated Role-Based Access Control RBAC via IAM group mapping and eliminated manual user provisioning. By defining its components, interactions, and production deployment considerations, this paper provides a reusable, secure, and cost-effective blueprint for integrating any standalone OSS tool into an enterprise SSO ecosystem, thereby enabling organizations to embrace open-source innovation without compromising on security governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03900v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuvraj Agrawal</dc:creator>
    </item>
    <item>
      <title>RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models</title>
      <link>https://arxiv.org/abs/2509.04078</link>
      <description>arXiv:2509.04078v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have exhibited significant proficiency in code debugging, especially in automatic program repair, which may substantially reduce the time consumption of developers and enhance their efficiency. Significant advancements in debugging datasets have been made to promote the development of code debugging. However, these datasets primarily focus on assessing the LLM's function-level code repair capabilities, neglecting the more complex and realistic repository-level scenarios, which leads to an incomplete understanding of the LLM's challenges in repository-level debugging. While several repository-level datasets have been proposed, they often suffer from limitations such as limited diversity of tasks, languages, and error types. To mitigate this challenge, this paper introduces RepoDebug, a multi-task and multi-language repository-level code debugging dataset with 22 subtypes of errors that supports 8 commonly used programming languages and 3 debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs, where Claude 3.5 Sonnect, the best-performing model, still cannot perform well in repository-level debugging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04078v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingjing Liu, Zeming Liu, Zihao Cheng, Mengliang He, Xiaoming Shi, Yuhang Guo, Xiangrong Zhu, Yuanfang Guo, Yunhong Wang, Haifeng Wang</dc:creator>
    </item>
    <item>
      <title>An Empirical Study of Vulnerabilities in Python Packages and Their Detection</title>
      <link>https://arxiv.org/abs/2509.04260</link>
      <description>arXiv:2509.04260v1 Announce Type: new 
Abstract: In the rapidly evolving software development landscape, Python stands out for its simplicity, versatility, and extensive ecosystem. Python packages, as units of organization, reusability, and distribution, have become a pressing concern, highlighted by the considerable number of vulnerability reports. As a scripting language, Python often cooperates with other languages for performance or interoperability. This adds complexity to the vulnerabilities inherent to Python packages, and the effectiveness of current vulnerability detection tools remains underexplored. This paper addresses these gaps by introducing PyVul, the first comprehensive benchmark suite of Python-package vulnerabilities. PyVul includes 1,157 publicly reported, developer-verified vulnerabilities, each linked to its affected packages. To accommodate diverse detection techniques, it provides annotations at both commit and function levels. An LLM-assisted data cleansing method is incorporated to improve label accuracy, achieving 100% commit-level and 94% function-level accuracy, establishing PyVul as the most precise large-scale Python vulnerability benchmark. We further carry out a distribution analysis of PyVul, which demonstrates that vulnerabilities in Python packages involve multiple programming languages and exhibit a wide variety of types. Moreover, our analysis reveals that multi-lingual Python packages are potentially more susceptible to vulnerabilities. Evaluation of state-of-the-art detectors using this benchmark reveals a significant discrepancy between the capabilities of existing tools and the demands of effectively identifying real-world security issues in Python packages. Additionally, we conduct an empirical review of the top-ranked CWEs observed in Python packages, to diagnose the fine-grained limitations of current detection tools and highlight the necessity for future advancements in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04260v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haowei Quan, Junjie Wang, Xinzhe Li, Terry Yue Zhuo, Xiao Chen, Xiaoning Du</dc:creator>
    </item>
    <item>
      <title>FaaSGuard: Secure CI/CD for Serverless Applications -- An OpenFaaS Case Study</title>
      <link>https://arxiv.org/abs/2509.04328</link>
      <description>arXiv:2509.04328v1 Announce Type: new 
Abstract: Serverless computing significantly alters software development by abstracting infrastructure management and enabling rapid, modular, event-driven deployments. Despite its benefits, the distinct characteristics of serverless functions, such as ephemeral execution and fine-grained scalability, pose unique security challenges, particularly in open-source platforms like OpenFaaS. Existing approaches typically address isolated phases of the DevSecOps lifecycle, lacking an integrated and comprehensive security strategy. To bridge this gap, we propose FaaSGuard, a unified DevSecOps pipeline explicitly designed for open-source serverless environments. FaaSGuard systematically embeds lightweight, fail-closed security checks into every stage of the development lifecycle-planning, coding, building, deployment, and monitoring-effectively addressing threats such as injection attacks, hard-coded secrets, and resource exhaustion. We validate our approach empirically through a case study involving 20 real-world serverless functions from public GitHub repositories. Results indicate that FaaSGuard effectively detects and prevents critical vulnerabilities, demonstrating high precision (95%) and recall (91%) without significant disruption to established CI/CD practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04328v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amine Barrak, Emna Ksontini, Ridouane Atike, Fehmi Jaafar</dc:creator>
    </item>
    <item>
      <title>Design and Development of a Web Platform for Blood Donation Management</title>
      <link>https://arxiv.org/abs/2509.04423</link>
      <description>arXiv:2509.04423v1 Announce Type: new 
Abstract: Blood donation is a critical component of healthcare, yet locating suitable donors in emergencies often presents significant challenges. This paper presents the design and development of a Blood Donation Web Platform, a web-based system that connects patients, donors, and administrators within a centralized digital space. The platform allows interested donors to register their personal information, including blood group, contact details, and availability. Patients can search for donors based on blood group and location, and the system provides a list of nearby donors who are ready to donate. The platform design was guided by use case, database, class, and sequence diagrams to ensure a well-structured and efficient system architecture. Modern web technologies, including PHP (Laravel framework), HTML, CSS, Bootstrap, and MySQL, supported by XAMPP and Visual Studio Code, were employed to implement a dynamic, interactive, and user-friendly platform. By streamlining donor refgistration, blood requests, and communication, the proposed system reduces delays and complexities in emergencies, improving timely accessibility of blood and enhancing overall efficiency in blood donation services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04423v1</guid>
      <category>cs.SE</category>
      <category>cs.DB</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatima Zulfiqar Ali, Atrooba Ilyas</dc:creator>
    </item>
    <item>
      <title>Reactive Bottom-Up Testing</title>
      <link>https://arxiv.org/abs/2509.03711</link>
      <description>arXiv:2509.03711v1 Announce Type: cross 
Abstract: Modern computing systems remain rife with software vulnerabilities. Engineers apply many means to detect them, of which dynamic testing is one of the most common and effective. However, most dynamic testing techniques follow a top-down paradigm, and struggle to reach and exercise functions deep within the call graph. While recent works have proposed Bottom-Up approaches to address these limitations, they face challenges with false positives and generating valid inputs that adhere to the context of the entire program.
  In this work, we introduce a new paradigm that we call Reactive Bottom-Up Testing. Our insight is that function-level testing is necessary but not sufficient for the validation of vulnerabilities in functions. What we need is a systematic approach that not only tests functions in isolation but also validates their behavior within the broader program context, ensuring that detected vulnerabilities are both reachable and triggerable. We develop a three-stage bottom-up testing scheme: (1) identify likely-vulnerable functions and generate type- and context-aware harnesses; (2) fuzz to find crashes and extract input constraints via symbolic execution; (3) verify crashes by combining constraints to remove false positives. We implemented an automated prototype, which we call Griller. We evaluated Griller in a controlled setting using a benchmark of 48 known vulnerabilities across 5 open-source projects, where we successfully detected 28 known vulnerabilities. Additionally, we evaluated Griller on several real-world applications such as Pacman, and it discovered 6 previously unknown vulnerabilities. Our findings suggest that Reactive Bottom-Up Testing can significantly enhance the detection of vulnerabilities in complex systems, paving the way for more robust security practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03711v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Muralee, Sourag Cherupattamoolayil, James C. Davis, Antonio Bianchi, Aravind Machiry</dc:creator>
    </item>
    <item>
      <title>BIDO: A Unified Approach to Address Obfuscation and Concept Drift Challenges in Image-based Malware Detection</title>
      <link>https://arxiv.org/abs/2509.03807</link>
      <description>arXiv:2509.03807v1 Announce Type: cross 
Abstract: To identify malicious Android applications, various malware detection techniques have been proposed. Among them, image-based approaches are considered potential alternatives due to their efficiency and scalability. Recent studies have reported that these approaches suffer significant performance declines when confronted with obfuscation or concept drift. However, existing solutions often treat these two challenges as different problems, offering independent solutions. These techniques overlook the fact that both challenges share a common statistical root, out-of-distribution, and research from this perspective remains limited. In response, we propose BIDO, a hybrid image-based malware detector designed to enhance robustness against both obfuscation and concept drift simultaneously. Specifically, to improve the discriminative power of image features, we introduce a local feature selection module that identifies informative subregions within malware images. Second, to enhance feature robustness, we model pairwise cross-modal dependencies in an outer product space, enabling the extraction of stable co-occurrence patterns. Third, to ensure feature compactness, we design a learnable metric that pulls samples with identical labels closer while pushing apart those with different labels, regardless of obfuscation or concept drift. Extensive experiments on the real-world datasets demonstrate that BIDO significantly outperforms existing baselines, achieving higher robustness against both concept drift and obfuscation. The source code is available at: https://github.com/whatishope/BIDO/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03807v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Junhui Li, Chengbin Feng, Zhiwei Yang, Qi Mo, Wei Wang</dc:creator>
    </item>
    <item>
      <title>Assessing Large Language Models in Comprehending and Verifying Concurrent Programs across Memory Models</title>
      <link>https://arxiv.org/abs/2501.14326</link>
      <description>arXiv:2501.14326v2 Announce Type: replace 
Abstract: As concurrent programming becomes increasingly prevalent, effectively identifying and addressing concurrency issues such as data races and deadlocks is critical. This study evaluates the performance of several leading large language models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini, and Mistral-AI's Large2, in understanding and analyzing concurrency issues within software programs. Given that relaxed memory models, such as Total Store Order (TSO) and Partial Store Order (PSO), are widely implemented and adapted in modern systems, supported even by commodity architectures like ARM and x86, our evaluation focuses not only on sequentially consistent memory models but also on these relaxed memory models. Specifically, we assess two main aspects: the models' capacity to detect concurrency problems under a sequentially consistent memory model and their ability to verify the correctness conditions of concurrent programs across both sequentially consistent and relaxed memory models. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests designed to evaluate Total Store Order (TSO) and Partial Store Order (PSO) memory models. The experimental results reveal that GPT-4, GPT-4o, and Mistral-AI's Large2 demonstrate a robust understanding of concurrency issues, effectively identifying data races and deadlocks when assessed under a sequentially consistent memory model. However, despite its superior performance, all selected LLMs face significant challenges verifying program correctness under relaxed memory models. These LLMs exhibit limitations in accurately capturing memory ordering constraints, and their current capabilities fall short in verifying even small programs in these complex scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14326v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ridhi Jain, Rahul Purandare</dc:creator>
    </item>
    <item>
      <title>KNighter: Transforming Static Analysis with LLM-Synthesized Checkers</title>
      <link>https://arxiv.org/abs/2503.09002</link>
      <description>arXiv:2503.09002v3 Announce Type: replace 
Abstract: Static analysis is a powerful technique for bug detection in critical systems like operating system kernels. However, designing and implementing static analyzers is challenging, time-consuming, and typically limited to predefined bug patterns. While large language models (LLMs) have shown promise for static analysis, directly applying them to scan large systems remains impractical due to computational constraints and contextual limitations.
  We present KNighter, the first approach that unlocks scalable LLM-based static analysis by automatically synthesizing static analyzers from historical bug patterns. Rather than using LLMs to directly analyze massive systems, our key insight is leveraging LLMs to generate specialized static analyzers guided by historical patch knowledge. KNighter implements this vision through a multi-stage synthesis pipeline that validates checker correctness against original patches and employs an automated refinement process to iteratively reduce false positives. Our evaluation on the Linux kernel demonstrates that KNighter generates high-precision checkers capable of detecting diverse bug patterns overlooked by existing human-written analyzers. To date, KNighter-synthesized checkers have discovered 92 new, critical, long-latent bugs (average 4.3 years) in the Linux kernel; 77 are confirmed, 57 fixed, and 30 have been assigned CVE numbers. This work establishes an entirely new paradigm for scalable, reliable, and traceable LLM-based static analysis for real-world systems via checker synthesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09002v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.OS</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731569.3764827</arxiv:DOI>
      <dc:creator>Chenyuan Yang, Zijie Zhao, Zichen Xie, Haoyu Li, Lingming Zhang</dc:creator>
    </item>
    <item>
      <title>RBT4DNN: Requirements-based Testing of Neural Networks</title>
      <link>https://arxiv.org/abs/2504.02737</link>
      <description>arXiv:2504.02737v3 Announce Type: replace 
Abstract: Testing allows developers to determine whether a system functions as expected. When such systems include deep neural networks (DNNs), Testing becomes challenging, as DNNs approximate functions for which the formalization of functional requirements is intractable. This prevents the application of well-developed approaches to requirements-based testing to DNNs.
  To address this, we propose a requirements-based testing method (RBT4DNN) that uses natural language requirements statements. These statements use a glossary of terms to define a semantic feature space that can be leveraged for test input generation. RBT4DNN formalizes preconditions of functional requirements as logical combinations of those semantic features. Training data matching these feature combinations can be used to fine-tune a generative model to reliably produce test inputs satisfying the precondition. Executing these tests on a trained DNN enables comparing its output to the expected requirement postcondition behavior. We propose two use cases for RBT4DNN: (1) given requirements defining DNN correctness properties, RBT4DNN comprises a novel approach for detecting faults, and (2) during development, requirements-guided exploration of model behavior can provide developers with feedback on model generalization. Our further evaluation shows that RBT4DNN-generated tests are realistic, diverse, and aligned with requirement preconditions, enabling targeted analysis of model behavior and effective fault detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02737v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nusrat Jahan Mozumder, Felipe Toledo, Swaroopa Dola, Matthew B. Dwyer</dc:creator>
    </item>
    <item>
      <title>Larger Is Not Always Better: Exploring Small Open-source Language Models in Logging Statement Generation</title>
      <link>https://arxiv.org/abs/2505.16590</link>
      <description>arXiv:2505.16590v3 Announce Type: replace 
Abstract: Developers use logging statements to create logs that document system behavior and aid in software maintenance. As such, high-quality logging is essential for effective maintenance; however, manual logging often leads to errors and inconsistency. Recent methods emphasize using large language models (LLMs) for automated logging statement generation, but these present privacy and resource issues, hindering their suitability for enterprise use. This paper presents the first large-scale empirical study evaluating small open-source language models (SOLMs) for automated logging statement generation. We evaluate four prominent SOLMs using various prompt strategies and parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA) and Retrieval-Augmented Generation (RAG). Our results show that fine-tuned SOLMs with LoRA and RAG prompts, particularly Qwen2.5-coder-14B, outperform existing tools and LLM baselines in predicting logging locations and generating high-quality statements, with robust generalization across diverse repositories. These findings highlight SOLMs as a privacy-preserving, efficient alternative for automated logging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16590v3</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renyi Zhong, Yichen Li, Guangba Yu, Wenwei Gu, Jinxi Kuang, Yintong Huo, Michael R. Lyu</dc:creator>
    </item>
    <item>
      <title>How to Elicit Explainability Requirements? A Comparison of Interviews, Focus Groups, and Surveys</title>
      <link>https://arxiv.org/abs/2505.23684</link>
      <description>arXiv:2505.23684v4 Announce Type: replace 
Abstract: As software systems grow increasingly complex, explainability has become a crucial non-functional requirement for transparency, user trust, and regulatory compliance. Eliciting explainability requirements is challenging, as different methods capture varying levels of detail and structure. This study examines the efficiency and effectiveness of three commonly used elicitation methods - focus groups, interviews, and online surveys - while also assessing the role of taxonomy usage in structuring and improving the elicitation process. We conducted a case study at a large German IT consulting company, utilizing a web-based personnel management software. A total of two focus groups, 18 interviews, and an online survey with 188 participants were analyzed. The results show that interviews were the most efficient, capturing the highest number of distinct needs per participant per time spent. Surveys collected the most explanation needs overall but had high redundancy. Delayed taxonomy introduction resulted in a greater number and diversity of needs, suggesting that a two-phase approach is beneficial. Based on our findings, we recommend a hybrid approach combining surveys and interviews to balance efficiency and coverage. Future research should explore how automation can support elicitation and how taxonomies can be better integrated into different methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23684v4</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Obaidi, Jakob Droste, Hannah Deters, Marc Herrmann, Raymond Ochsner, Jil Kl\"under, Kurt Schneider</dc:creator>
    </item>
    <item>
      <title>Specification-Guided Repair of Arithmetic Errors in Dafny Programs using LLMs</title>
      <link>https://arxiv.org/abs/2507.03659</link>
      <description>arXiv:2507.03659v2 Announce Type: replace 
Abstract: Debugging and repairing faults when programs fail to formally verify can be complex and time-consuming. Automated Program Repair (APR) can ease this burden by automatically identifying and fixing faults. However, traditional APR techniques often rely on test suites for validation, but these may not capture all possible scenarios. In contrast, formal specifications provide strong correctness criteria, enabling more effective automated repair.
  In this paper, we present an APR tool for Dafny, a verification-aware programming language that uses formal specifications - including pre-conditions, post-conditions, and invariants - as oracles for fault localization and repair. Assuming the correctness of the specifications and focusing on arithmetic bugs, we localize faults through a series of steps, which include using Hoare logic to determine the state of each statement within the program, and applying Large Language Models (LLMs) to synthesize candidate fixes. The models considered are GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.
  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny programs. Our tool achieves 89.6% fault localization coverage and GPT-4o mini yields the highest repair success rate of 74.18%. These results highlight the potential of combining formal reasoning with LLM-based program synthesis for automated program repair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03659v2</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valentina Wu, Alexandra Mendes, Alexandre Abreu</dc:creator>
    </item>
    <item>
      <title>InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching</title>
      <link>https://arxiv.org/abs/2507.08523</link>
      <description>arXiv:2507.08523v2 Announce Type: replace 
Abstract: Modern software systems generate massive volumes of runtime logs, necessitating efficient and accurate log parsing to enable critical downstream tasks such as anomaly detection and root cause analysis. Recently, large language models (LLMs) have achieved advanced accuracy on log parsing, but their deployment in production environments faces two major limitations: (1) the privacy risks associated with commercial LLMs, driving the adoption of local deployment, and (2) the stringent latency and throughput requirements imposed by high-volume log streams, which existing LLM-based parsers fail to meet. Although recent efforts have reduced the number of LLM queries, they overlook the high latency of the LLM invocations, where concurrent log parsing requests can cause serve performance degradation of LLM inference system.
  In this study, we present InferLog, the first LLM inference optimization method for online log parsing. Our key insight is that the inference efficiency emerges as the vital bottleneck in LLM-based online log parsing, rather than parsing accuracy. InferLog accelerates inference by designing (1) A Prefix-aware ICL Refinement policy to refine the examples and permutation of in-context learning to improve the prefix caching efficiency. (2) A rapid and task-specific configuration tuning pipeline based on meta-learning to find the optimal LLM scheduling-related configuration for dynamic log parsing workloads. The experimental results based on Loghub dataset and vLLM demonstrate that InferLog significantly outperforms existing inference optimization methods and markedly accelerates the state-of-the-art LLM-based log parser without compromising parsing accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08523v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilun Wang, Pengfei Chen, Haiyu Huang, Zilong He, Gou Tan, Chuanfu Zhang, Jingkai He, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models</title>
      <link>https://arxiv.org/abs/2406.01359</link>
      <description>arXiv:2406.01359v3 Announce Type: replace-cross 
Abstract: Code completion models have made significant progress in recent years. Recently, repository-level code completion has drawn more attention in modern software development, and several baseline methods and benchmarks have been proposed. However, existing repository-level code completion methods often fall short of fully using the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies. Besides, the existing benchmarks usually focus on limited code completion scenarios, which cannot reflect the repository-level code completion abilities well of existing methods. To address these limitations, we propose the R2C2-Coder to enhance and benchmark the real-world repository-level code completion abilities of code Large Language Models, where the R2C2-Coder includes a code prompt construction method R2C2-Enhance and a well-designed benchmark R2C2-Bench. Specifically, first, in R2C2-Enhance, we first construct the candidate retrieval pool and then assemble the completion prompt by retrieving from the retrieval pool for each completion cursor position. Second, based on R2C2 -Enhance, we can construct a more challenging and diverse R2C2-Bench with training, validation and test splits, where a context perturbation strategy is proposed to simulate the real-world repository-level code completion well. Extensive results on multiple benchmarks demonstrate the effectiveness of our R2C2-Coder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01359v3</guid>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ken Deng, Jiaheng Liu, He Zhu, Congnan Liu, Jingxin Li, Jiakai Wang, Peng Zhao, Chenchen Zhang, Yanan Wu, Xueqiao Yin, Yuanxing Zhang, Zizheng Zhan, Wenbo Su, Bangyu Xiang, Tiezheng Ge, Bo Zheng</dc:creator>
    </item>
  </channel>
</rss>
