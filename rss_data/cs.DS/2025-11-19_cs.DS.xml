<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Nov 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimal Sequential Flows</title>
      <link>https://arxiv.org/abs/2511.13806</link>
      <description>arXiv:2511.13806v1 Announce Type: cross 
Abstract: We provide a new algebraic technique to solve the sequential flow problem in polynomial space. The task is to maximize the flow through a graph where edge capacities can be changed over time by choosing a sequence of capacity labelings from a given finite set. Our method is based on a novel factorization theorem for finite semigroups that, applied to a suitable flow semigroup, allows to derive small witnesses. This generalizes to multiple in/output vertices, as well as regular constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13806v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Gimbert, Corto Mascle, Patrick Totzke</dc:creator>
    </item>
    <item>
      <title>CORGI: Efficient Pattern Matching With Quadratic Guarantees</title>
      <link>https://arxiv.org/abs/2511.13942</link>
      <description>arXiv:2511.13942v1 Announce Type: cross 
Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $\beta$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13942v1</guid>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Conference on Advances in Cognitive Systems (2025) 212-225</arxiv:journal_reference>
      <dc:creator>Daniel Weitekamp</dc:creator>
    </item>
    <item>
      <title>Compression with Privacy-Preserving Random Access</title>
      <link>https://arxiv.org/abs/2511.14524</link>
      <description>arXiv:2511.14524v1 Announce Type: cross 
Abstract: It is shown that an i.i.d. binary source sequence $X_1, \ldots, X_n$ can be losslessly compressed at any rate above entropy such that the individual decoding of any $X_i$ reveals \emph{no} information about the other bits $\{X_j : j \neq i\}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14524v1</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Venkat Chandar, Aslan Tchamkerten, Shashank Vatedka</dc:creator>
    </item>
    <item>
      <title>Lower Bounds on Tree Covers</title>
      <link>https://arxiv.org/abs/2508.10376</link>
      <description>arXiv:2508.10376v2 Announce Type: replace 
Abstract: Given an $n$-point metric space $(X,d_X)$, a tree cover $\mathcal{T}$ is a set of $|\mathcal{T}|=k$ trees on $X$ such that every pair of vertices in $X$ has a low-distortion path in one of the trees in $\mathcal{T}$. Tree covers have been playing a crucial role in graph algorithms for decades, and the research focus is the construction of tree covers with small size $k$ and distortion.
  When $k=1$, the best distortion is known to be $\Theta(n)$. For a constant $k\ge 2$, the best distortion upper bound is $\tilde O(n^{\frac 1 k})$ and the strongest lower bound is $\Omega(\log_k n)$, leaving a gap to be closed. In this paper, we improve the lower bound to $\Omega(n^{\frac{1}{2^{k-1}}})$.
  Our proof is a novel analysis on a structurally simple grid-like graph, which utilizes some combinatorial fixed-point theorems. We believe that they will prove useful for analyzing other tree-like data structures as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10376v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Chen, Zihan Tan, Hangyu Xu</dc:creator>
    </item>
    <item>
      <title>Online Flow Time Minimization: Tight Bounds for Non-Preemptive Algorithms</title>
      <link>https://arxiv.org/abs/2511.03485</link>
      <description>arXiv:2511.03485v2 Announce Type: replace 
Abstract: This paper studies the classical online scheduling problem of minimizing total flow time for $n$ jobs on $m$ identical machines. Prior work often cites the $\Omega(n)$ lower bound for non-preemptive algorithms to argue for the necessity of preemption or resource augmentation, which shows the trivial $O(n)$-competitive greedy algorithm is tight. However, this lower bound applies only to \emph{deterministic} algorithms in the \emph{single-machine} case, leaving several fundamental questions unanswered. Can randomness help in the non-preemptive setting, and what is the optimal online deterministic algorithm when $m \geq 2$? We resolve both questions. We present a polynomial-time randomized algorithm with competitive ratio $\Theta(\sqrt{n/m})$ and prove a matching randomized lower bound, settling the randomized non-preemptive setting for every $m$. This also improves the best-known offline approximation ratio from $O(\sqrt{n/m}\log(n/m))$ to $O(\sqrt{n/m})$. On the deterministic side, we present a non-preemptive algorithm with competitive ratio $O(n/m^{2}+\sqrt{n/m}\log m)$ and prove a nearly matching lower bound.
  Our framework also extends to the kill-and-restart model, where we reveal a sharp transition of deterministic algorithms: we design an asymptotically optimal algorithm with the competitive ratio $O(\sqrt{n/m})$ for $m\ge 2$, yet establish a strong $\Omega(n/\log n)$ lower bound for $m=1$. Moreover, we show that randomization provides no further advantage, as the lower bound coincides with that of the non-preemptive setting.
  While our main results assume prior knowledge of $n$, we also investigate the setting where $n$ is unknown. We show kill-and-restart is powerful enough to break the $O(n)$ barrier for $m \geq 2$ even without knowing $n$. Conversely, we prove randomization alone is insufficient, as no algorithm can achieve an $o(n)$ competitive ratio in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03485v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutong Geng, Enze Sun, Zonghan Yang, Yuhao Zhang</dc:creator>
    </item>
    <item>
      <title>Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics</title>
      <link>https://arxiv.org/abs/2510.26324</link>
      <description>arXiv:2510.26324v2 Announce Type: replace-cross 
Abstract: Given a noisy linear measurement $y = Ax + \xi$ of a distribution $p(x)$, and a good approximation to the prior $p(x)$, when can we sample from the posterior $p(x \mid y)$? Posterior sampling provides an accurate and fair framework for tasks such as inpainting, deblurring, and MRI reconstruction, and several heuristics attempt to approximate it. Unfortunately, approximate posterior sampling is computationally intractable in general.
  To sidestep this hardness, we focus on (local or global) log-concave distributions $p(x)$. In this regime, Langevin dynamics yields posterior samples when the exact scores of $p(x)$ are available, but it is brittle to score--estimation error, requiring an MGF bound (sub-exponential error). By contrast, in the unconditional setting, diffusion models succeed with only an $L^2$ bound on the score error. We prove that combining diffusion models with an annealed variant of Langevin dynamics achieves conditional sampling in polynomial time using merely an $L^4$ bound on the score error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26324v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyang Xun, Shivam Gupta, Eric Price</dc:creator>
    </item>
  </channel>
</rss>
