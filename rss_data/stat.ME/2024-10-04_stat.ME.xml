<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Oct 2024 04:00:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A unified approach to penalized likelihood estimation of covariance matrices in high dimensions</title>
      <link>https://arxiv.org/abs/2410.02403</link>
      <description>arXiv:2410.02403v1 Announce Type: new 
Abstract: We consider the problem of estimation of a covariance matrix for Gaussian data in a high dimensional setting. Existing approaches include maximum likelihood estimation under a pre-specified sparsity pattern, l_1-penalized loglikelihood optimization and ridge regularization of the sample covariance. We show that these three approaches can be addressed in an unified way, by considering the constrained optimization of an objective function that involves two suitably defined penalty terms. This unified procedure exploits the advantages of each individual approach, while bringing novelty in the combination of the three. We provide an efficient algorithm for the optimization of the regularized objective function and describe the relationship between the two penalty terms, thereby highlighting the importance of the joint application of the three methods. A simulation study shows how the sparse estimates of covariance matrices returned by the procedure are stable and accurate, both in low and high dimensional settings, and how their calculation is more efficient than existing approaches under a partially known sparsity pattern. An illustration on sonar data shows is presented for the identification of the covariance structure among signals bounced off a certain material. The method is implemented in the publicly available R package gicf.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02403v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Cibinel, Alberto Roverato, Veronica Vinciotti</dc:creator>
    </item>
    <item>
      <title>Bayesian Calibration and Uncertainty Quantification for a Large Nutrient Load Impact Model</title>
      <link>https://arxiv.org/abs/2410.02448</link>
      <description>arXiv:2410.02448v1 Announce Type: new 
Abstract: Nutrient load simulators are large, deterministic, models that simulate the hydrodynamics and biogeochemical processes in aquatic ecosystems. They are central tools for planning cost efficient actions to fight eutrophication since they allow scenario predictions on impacts of nutrient load reductions to, e.g., harmful algal biomass growth. Due to being computationally heavy, the uncertainties related to these predictions are typically not rigorously assessed though. In this work, we developed a novel Bayesian computational approach for estimating the uncertainties in predictions of the Finnish coastal nutrient load model FICOS. First, we constructed a likelihood function for the multivariate spatiotemporal outputs of the FICOS model. Then, we used Bayes optimization to locate the posterior mode for the model parameters conditional on long term monitoring data. After that, we constructed a space filling design for FICOS model runs around the posterior mode and used it to train a Gaussian process emulator for the (log) posterior density of the model parameters. We then integrated over this (approximate) parameter posterior to produce probabilistic predictions for algal biomass and chlorophyll a concentration under alternative nutrient load reduction scenarios. Our computational algorithm allowed for fast posterior inference and the Gaussian process emulator had good predictive accuracy within the highest posterior probability mass region. The posterior predictive scenarios showed that the probability to reach the EUs Water Framework Directive objectives in the Finnish Archipelago Sea is generally low even under large load reductions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02448v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Karel Kaurila, Risto Lignell, Frede Thingstad, Harri Kuosa, Jarno Vanhatalo</dc:creator>
    </item>
    <item>
      <title>Stochastic Gradient Variational Bayes in the Stochastic Blockmodel</title>
      <link>https://arxiv.org/abs/2410.02649</link>
      <description>arXiv:2410.02649v1 Announce Type: new 
Abstract: Stochastic variational Bayes algorithms have become very popular in the machine learning literature, particularly in the context of nonparametric Bayesian inference. These algorithms replace the true but intractable posterior distribution with the best (in the sense of Kullback-Leibler divergence) member of a tractable family of distributions, using stochastic gradient algorithms to perform the optimization step. stochastic variational Bayes inference implicitly trades off computational speed for accuracy, but the loss of accuracy is highly model (and even dataset) specific. In this paper we carry out an empirical evaluation of this trade off in the context of stochastic blockmodels, which are a widely used class of probabilistic models for network and relational data. Our experiments indicate that, in the context of stochastic blockmodels, relatively large subsamples are required for these algorithms to find accurate approximations of the posterior, and that even then the quality of the approximations provided by stochastic gradient variational algorithms can be highly variable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02649v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Regueiro, Abel Rodr\'iguez, Juan Sosa</dc:creator>
    </item>
    <item>
      <title>Exact Bayesian Inference for Multivariate Spatial Data of Any Size with Application to Air Pollution Monitoring</title>
      <link>https://arxiv.org/abs/2410.02655</link>
      <description>arXiv:2410.02655v1 Announce Type: new 
Abstract: Fine particulate matter and aerosol optical thickness are of interest to atmospheric scientists for understanding air quality and its various health/environmental impacts. The available data are extremely large, making uncertainty quantification in a fully Bayesian framework quite difficult, as traditional implementations do not scale reasonably to the size of the data. We specifically consider roughly 8 million observations obtained from NASA's Moderate Resolution Imaging Spectroradiometer (MODIS) instrument. To analyze data on this scale, we introduce Scalable Multivariate Exact Posterior Regression (SM-EPR) which combines the recently introduced data subset approach and Exact Posterior Regression (EPR). EPR is a new Bayesian hierarchical model where it is possible to sample independent replicates of fixed and random effects directly from the posterior without the use of Markov chain Monte Carlo (MCMC) or approximate Bayesian techniques. We extend EPR to the multivariate spatial context, where the multiple variables may be distributed according to different distributions. The combination of the data subset approach with EPR allows one to perform exact Bayesian inference without MCMC for effectively any sample size. We demonstrate our new SM-EPR method using this motivating big remote sensing data application and provide several simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02655v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Madelyn Clinch, Jonathan R. Bradley</dc:creator>
    </item>
    <item>
      <title>Regression Discontinuity Designs Under Interference</title>
      <link>https://arxiv.org/abs/2410.02727</link>
      <description>arXiv:2410.02727v1 Announce Type: new 
Abstract: We extend the continuity-based framework to Regression Discontinuity Designs (RDDs) to identify and estimate causal effects in the presence of interference when units are connected through a network. In this setting, assignment to an "effective treatment," which comprises the individual treatment and a summary of the treatment of interfering units (e.g., friends, classmates), is determined by the unit's score and the scores of other interfering units, leading to a multiscore RDD with potentially complex, multidimensional boundaries. We characterize these boundaries and derive generalized continuity assumptions to identify the proposed causal estimands, i.e., point and boundary causal effects. Additionally, we develop a distance-based nonparametric estimator, derive its asymptotic properties under restrictions on the network degree distribution, and introduce a novel variance estimator that accounts for network correlation. Finally, we apply our methodology to the PROGRESA/Oportunidades dataset to estimate the direct and indirect effects of receiving cash transfers on children's school attendance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02727v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Dal Torrione, Tiziano Arduini, Laura Forastiere</dc:creator>
    </item>
    <item>
      <title>Shocks-adaptive Robust Minimum Variance Portfolio for a Large Universe of Assets</title>
      <link>https://arxiv.org/abs/2410.01826</link>
      <description>arXiv:2410.01826v1 Announce Type: cross 
Abstract: This paper proposes a robust, shocks-adaptive portfolio in a large-dimensional assets universe where the number of assets could be comparable to or even larger than the sample size. It is well documented that portfolios based on optimizations are sensitive to outliers in return data. We deal with outliers by proposing a robust factor model, contributing methodologically through the development of a robust principal component analysis (PCA) for factor model estimation and a shrinkage estimation for the random error covariance matrix. This approach extends the well-regarded Principal Orthogonal Complement Thresholding (POET) method (Fan et al., 2013), enabling it to effectively handle heavy tails and sudden shocks in data. The novelty of the proposed robust method is its adaptiveness to both global and idiosyncratic shocks, without the need to distinguish them, which is useful in forming portfolio weights when facing outliers. We develop the theoretical results of the robust factor model and the robust minimum variance portfolio. Numerical and empirical results show the superior performance of the new portfolio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01826v1</guid>
      <category>q-fin.PM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingliang Fan, Ruike Wu, Yanrong Yang</dc:creator>
    </item>
    <item>
      <title>A Divide-and-Conquer Approach to Persistent Homology</title>
      <link>https://arxiv.org/abs/2410.01839</link>
      <description>arXiv:2410.01839v1 Announce Type: cross 
Abstract: Persistent homology is a tool of topological data analysis that has been used in a variety of settings to characterize different dimensional holes in data. However, persistent homology computations can be memory intensive with a computational complexity that does not scale well as the data size becomes large. In this work, we propose a divide-and-conquer (DaC) method to mitigate these issues. The proposed algorithm efficiently finds small, medium, and large-scale holes by partitioning data into sub-regions and uses a Vietoris-Rips filtration. Furthermore, we provide theoretical results that quantify the bottleneck distance between DaC and the true persistence diagram and the recovery probability of holes in the data. We empirically verify that the rate coincides with our theoretical rate, and find that the memory and computational complexity of DaC outperforms an alternative method that relies on a clustering preprocessing step to reduce the memory and computational complexity of the persistent homology computations. Finally, we test our algorithm using spatial data of the locations of lakes in Wisconsin, where the classical persistent homology is computationally infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01839v1</guid>
      <category>math.AT</category>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenghui Li, Jessi Cisewski-Kehe</dc:creator>
    </item>
    <item>
      <title>Instrumental variables: A non-asymptotic viewpoint</title>
      <link>https://arxiv.org/abs/2410.02015</link>
      <description>arXiv:2410.02015v1 Announce Type: cross 
Abstract: We provide a non-asymptotic analysis of the linear instrumental variable estimator allowing for the presence of exogeneous covariates. In addition, we introduce a novel measure of the strength of an instrument that can be used to derive non-asymptotic confidence intervals. For strong instruments, these non-asymptotic intervals match the asymptotic ones exactly up to higher order corrections; for weaker instruments, our intervals involve adaptive adjustments to the instrument strength, and thus remain valid even when asymptotic predictions break down. We illustrate our results via an analysis of the effect of PM2.5 pollution on various health conditions, using wildfire smoke exposure as an instrument. Our analysis shows that exposure to PM2.5 pollution leads to statistically significant increases in incidence of health conditions such as asthma, heart disease, and strokes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02015v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Xia, Martin J. Wainwright, Whitney Newey</dc:creator>
    </item>
    <item>
      <title>A Likelihood Based Approach to Distribution Regression Using Conditional Deep Generative Models</title>
      <link>https://arxiv.org/abs/2410.02025</link>
      <description>arXiv:2410.02025v1 Announce Type: cross 
Abstract: In this work, we explore the theoretical properties of conditional deep generative models under the statistical framework of distribution regression where the response variable lies in a high-dimensional ambient space but concentrates around a potentially lower-dimensional manifold. More specifically, we study the large-sample properties of a likelihood-based approach for estimating these models. Our results lead to the convergence rate of a sieve maximum likelihood estimator (MLE) for estimating the conditional distribution (and its devolved counterpart) of the response given predictors in the Hellinger (Wasserstein) metric. Our rates depend solely on the intrinsic dimension and smoothness of the true conditional distribution. These findings provide an explanation of why conditional deep generative models can circumvent the curse of dimensionality from the perspective of statistical foundations and demonstrate that they can learn a broader class of nearly singular conditional distributions. Our analysis also emphasizes the importance of introducing a small noise perturbation to the data when they are supported sufficiently close to a manifold. Finally, in our numerical studies, we demonstrate the effective implementation of the proposed approach using both synthetic and real-world datasets, which also provide complementary validation to our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02025v1</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivam Kumar, Yun Yang, Lizhen Lin</dc:creator>
    </item>
    <item>
      <title>A fast, flexible simulation framework for Bayesian adaptive designs -- the R package BATSS</title>
      <link>https://arxiv.org/abs/2410.02050</link>
      <description>arXiv:2410.02050v1 Announce Type: cross 
Abstract: The use of Bayesian adaptive designs for randomised controlled trials has been hindered by the lack of software readily available to statisticians. We have developed a new software package (Bayesian Adaptive Trials Simulator Software - BATSS for the statistical software R, which provides a flexible structure for the fast simulation of Bayesian adaptive designs for clinical trials. We illustrate how the BATSS package can be used to define and evaluate the operating characteristics of Bayesian adaptive designs for various different types of primary outcomes (e.g., those that follow a normal, binary, Poisson or negative binomial distribution) and can incorporate the most common types of adaptations: stopping treatments (or the entire trial) for efficacy or futility, and Bayesian response adaptive randomisation - based on user-defined adaptation rules. Other important features of this highly modular package include: the use of (Integrated Nested) Laplace approximations to compute posterior distributions, parallel processing on a computer or a cluster, customisability, adjustment for covariates and a wide range of available conditional distributions for the response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02050v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dominique-Laurent Couturier, Elizabeth G Ryan, Rainer Puhr, Thomas Jaki, Stephane Heritier</dc:creator>
    </item>
    <item>
      <title>Fast nonparametric feature selection with error control using integrated path stability selection</title>
      <link>https://arxiv.org/abs/2410.02208</link>
      <description>arXiv:2410.02208v1 Announce Type: cross 
Abstract: Feature selection can greatly improve performance and interpretability in machine learning problems. However, existing nonparametric feature selection methods either lack theoretical error control or fail to accurately control errors in practice. Many methods are also slow, especially in high dimensions. In this paper, we introduce a general feature selection method that applies integrated path stability selection to thresholding to control false positives and the false discovery rate. The method also estimates q-values, which are better suited to high-dimensional data than p-values. We focus on two special cases of the general method based on gradient boosting (IPSSGB) and random forests (IPSSRF). Extensive simulations with RNA sequencing data show that IPSSGB and IPSSRF have better error control, detect more true positives, and are faster than existing methods. We also use both methods to detect microRNAs and genes related to ovarian cancer, finding that they make better predictions with fewer features than other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02208v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Melikechi, David B. Dunson, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>Choosing alpha post hoc: the danger of multiple standard significance thresholds</title>
      <link>https://arxiv.org/abs/2410.02306</link>
      <description>arXiv:2410.02306v1 Announce Type: cross 
Abstract: A fundamental assumption of classical hypothesis testing is that the significance threshold $\alpha$ is chosen independently from the data. The validity of confidence intervals likewise relies on choosing $\alpha$ beforehand. We point out that the independence of $\alpha$ is guaranteed in practice because, in most fields, there exists one standard $\alpha$ that everyone uses -- so that $\alpha$ is automatically independent of everything. However, there have been recent calls to decrease $\alpha$ from $0.05$ to $0.005$. We note that this may lead to multiple accepted standard thresholds within one scientific field. For example, different journals may require different significance thresholds. As a consequence, some researchers may be tempted to conveniently choose their $\alpha$ based on their p-value. We use examples to illustrate that this severely invalidates hypothesis tests, and mention some potential solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02306v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Hemerik, Nick W Koning</dc:creator>
    </item>
    <item>
      <title>Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression</title>
      <link>https://arxiv.org/abs/2410.02629</link>
      <description>arXiv:2410.02629v1 Announce Type: cross 
Abstract: This paper studies the generalization performance of iterates obtained by Gradient Descent (GD), Stochastic Gradient Descent (SGD) and their proximal variants in high-dimensional robust regression problems. The number of features is comparable to the sample size and errors may be heavy-tailed. We introduce estimators that precisely track the generalization error of the iterates along the trajectory of the iterative algorithm. These estimators are provably consistent under suitable conditions. The results are illustrated through several examples, including Huber regression, pseudo-Huber regression, and their penalized variants with non-smooth regularizer. We provide explicit generalization error estimates for iterates generated from GD and SGD, or from proximal SGD in the presence of a non-smooth regularizer. The proposed risk estimates serve as effective proxies for the actual generalization error, allowing us to determine the optimal stopping iteration that minimizes the generalization error. Extensive simulations confirm the effectiveness of the proposed generalization error estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02629v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Tan, Pierre C. Bellec</dc:creator>
    </item>
    <item>
      <title>Functional principal component analysis for longitudinal observations with sampling at random</title>
      <link>https://arxiv.org/abs/2203.14760</link>
      <description>arXiv:2203.14760v2 Announce Type: replace 
Abstract: Functional principal component analysis has been shown to be invaluable for revealing variation modes of longitudinal outcomes, which serves as important building blocks for forecasting and model building. Decades of research have advanced methods for functional principal component analysis often assuming independence between the observation times and longitudinal outcomes. Yet such assumptions are fragile in real-world settings where observation times may be driven by outcome-related reasons. Rather than ignoring the informative observation time process, we explicitly model the observational times by a counting process dependent on time-varying prognostic factors. Identification of the mean, covariance function, and functional principal components ensues via inverse intensity weighting. We propose using weighted penalized splines for estimation and establish consistency and convergence rates for the weighted estimators. Simulation studies demonstrate that the proposed estimators are substantially more accurate than the existing ones in the presence of a correlation between the observation time process and the longitudinal outcome process. We further examine the finite-sample performance of the proposed method using the Acute Infection and Early Disease Research Program study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.14760v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peijun Sang, Dehan Kong, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Recovery and inference of causal effects with sequential adjustment for confounding and attrition</title>
      <link>https://arxiv.org/abs/2401.16990</link>
      <description>arXiv:2401.16990v3 Announce Type: replace 
Abstract: Confounding bias and selection bias are two significant challenges to the validity of conclusions drawn from applied causal inference. The latter can stem from informative missingness, such as in cases of attrition. We introduce the Sequential Adjustment Criteria (SAC), which extend available graphical conditions for recovering causal effects using sequential regressions, allowing for the inclusion of post-exposure and forbidden variables in the admissible adjustment sets. We propose an estimator for the recovered Average Treatment Effect (ATE) based on Targeted Minimum-Loss Estimation (TMLE), which enjoys multiple robustness under certain conditions. This approach ensures consistency even in scenarios where the Double Inverse Probability Weighting (DIPW) and the na\"ive plug-in sequential regressions approaches fall short. Through a simulation study, we assess the performance of the proposed estimator against alternative methods across different graph setups and model specification scenarios. As a motivating application, we examine the effect of pharmacological treatment for Attention-Deficit/Hyperactivity Disorder (ADHD) upon the scores obtained by diagnosed Norwegian schoolchildren in national tests using observational data ($n=9,352$). Our findings align with the accumulated clinical evidence, affirming a positive but small impact of medication on academic achievement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16990v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Johan de Aguas, Johan Pensar, Tom\'as Varnet P\'erez, Guido Biele</dc:creator>
    </item>
    <item>
      <title>Stable Reduced-Rank VAR Identification</title>
      <link>https://arxiv.org/abs/2403.00237</link>
      <description>arXiv:2403.00237v4 Announce Type: replace 
Abstract: The vector autoregression (VAR) has been widely used in system identification, econometrics, natural science, and many other areas. However, when the state dimension becomes large the parameter dimension explodes. So rank reduced modelling is attractive and is well developed. But a fundamental requirement in almost all applications is stability of the fitted model. And this has not been addressed in the rank reduced case. Here, we develop, for the first time, a closed-form formula for an estimator of a rank reduced transition matrix which is guaranteed to be stable. We show that our estimator is consistent and asymptotically statistically efficient and illustrate it in comparative simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00237v4</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.automatica.2024.111961</arxiv:DOI>
      <arxiv:journal_reference>Automatica, vol. 171, p. 111961, 2025</arxiv:journal_reference>
      <dc:creator>Xinhui Rong, Victor Solo</dc:creator>
    </item>
    <item>
      <title>Extrinsic Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2409.03572</link>
      <description>arXiv:2409.03572v2 Announce Type: replace 
Abstract: One develops a fast computational methodology for principal component analysis on manifolds. Instead of estimating intrinsic principal components on an object space with a Riemannian structure, one embeds the object space in a numerical space, and the resulting chord distance is used. This method helps us analyzing high, theoretically even infinite dimensional data, from a new perspective. We define the extrinsic principal sub-manifolds of a random object on a Hilbert manifold embedded in a Hilbert space, and the sample counterparts. The resulting extrinsic principal components are useful for dimension data reduction. For application, one retains a very small number of such extrinsic principal components for a shape of contour data sample, extracted from imaging data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03572v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ka Chun Wong, Vic Patrangenaru, Robert L. Paige, Mihaela Pricop Jeckstadt</dc:creator>
    </item>
    <item>
      <title>Multi-source Stable Variable Importance Measure via Adversarial Machine Learning</title>
      <link>https://arxiv.org/abs/2409.07380</link>
      <description>arXiv:2409.07380v2 Announce Type: replace 
Abstract: As part of enhancing the interpretability of machine learning, it is of renewed interest to quantify and infer the predictive importance of certain exposure covariates. Modern scientific studies often collect data from multiple sources with distributional heterogeneity. Thus, measuring and inferring stable associations across multiple environments is crucial in reliable and generalizable decision-making. In this paper, we propose MIMAL, a novel statistical framework for Multi-source stable Importance Measure via Adversarial Learning. MIMAL measures the importance of some exposure variables by maximizing the worst-case predictive reward over the source mixture. Our framework allows various machine learning methods for confounding adjustment and exposure effect characterization. For inferential analysis, the asymptotic normality of our introduced statistic is established under a general machine learning framework that requires no stronger learning accuracy conditions than those for single source variable importance. Numerical studies with various types of data generation setups and machine learning implementation are conducted to justify the finite-sample performance of MIMAL. We also illustrate our method through a real-world study of Beijing air pollution in multiple locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07380v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zitao Wang, Nian Si, Zijian Guo, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Perturbation-Robust Predictive Modeling of Social Effects by Network Subspace Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2410.01163</link>
      <description>arXiv:2410.01163v2 Announce Type: replace 
Abstract: Network-linked data, where multivariate observations are interconnected by a network, are becoming increasingly prevalent in fields such as sociology and biology. These data often exhibit inherent noise and complex relational structures, complicating conventional modeling and statistical inference. Motivated by empirical challenges in analyzing such data sets, this paper introduces a family of network subspace generalized linear models designed for analyzing noisy, network-linked data. We propose a model inference method based on subspace-constrained maximum likelihood, which emphasizes flexibility in capturing network effects and provides a robust inference framework against network perturbations. We establish the asymptotic distributions of the estimators under network perturbations, demonstrating the method's accuracy through extensive simulations involving random network models and deep-learning-based embedding algorithms. The proposed methodology is applied to a comprehensive analysis of a large-scale study on school conflicts, where it identifies significant social effects, offering meaningful and interpretable insights into student behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01163v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianxiang Wang, Can M. Le, Tianxi Li</dc:creator>
    </item>
    <item>
      <title>Randomized Runge-Kutta-Nystr\"om Methods for Unadjusted Hamiltonian and Kinetic Langevin Monte Carlo</title>
      <link>https://arxiv.org/abs/2310.07399</link>
      <description>arXiv:2310.07399v2 Announce Type: replace-cross 
Abstract: We introduce $5/2$- and $7/2$-order $L^2$-accurate randomized Runge-Kutta-Nystr\"{o}m methods, tailored for approximating Hamiltonian flows within non-reversible Markov chain Monte Carlo samplers, such as unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin Monte Carlo. We establish quantitative $5/2$-order $L^2$-accuracy upper bounds under gradient and Hessian Lipschitz assumptions on the potential energy function. The numerical experiments demonstrate the superior efficiency of the proposed unadjusted samplers on a variety of well-behaved, high-dimensional target distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07399v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Tore Selland Kleppe</dc:creator>
    </item>
    <item>
      <title>Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values</title>
      <link>https://arxiv.org/abs/2311.04855</link>
      <description>arXiv:2311.04855v4 Announce Type: replace-cross 
Abstract: Non-negative matrix factorization (NMF) is a dimensionality reduction technique that has shown promise for analyzing noisy data, especially astronomical data. For these datasets, the observed data may contain negative values due to noise even when the true underlying physical signal is strictly positive. Prior NMF work has not treated negative data in a statistically consistent manner, which becomes problematic for low signal-to-noise data with many negative values. In this paper we present two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and also any introduced negativity. Both of these algorithms use the negative data space without clipping, and correctly recover non-negative signals without any introduced positive offset that occurs when clipping negative data. We demonstrate this numerically on both simple and more realistic examples, and prove that both algorithms have monotonically decreasing update rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04855v4</guid>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Green, Stephen Bailey</dc:creator>
    </item>
    <item>
      <title>Identifying Genetic Variants for Obesity Incorporating Prior Insights: Quantile Regression with Insight Fusion for Ultra-high Dimensional Data</title>
      <link>https://arxiv.org/abs/2406.12212</link>
      <description>arXiv:2406.12212v2 Announce Type: replace-cross 
Abstract: Obesity is widely recognized as a critical and pervasive health concern. We strive to identify important genetic risk factors from hundreds of thousands of single nucleotide polymorphisms (SNPs) for obesity. We propose and apply a novel Quantile Regression with Insight Fusion (QRIF) approach that can integrate insights from established studies or domain knowledge to simultaneously select variables and modeling for ultra-high dimensional genetic data, focusing on high conditional quantiles of body mass index (BMI) that are of most interest. We discover interesting new SNPs and shed new light on a comprehensive view of the underlying genetic risk factors for different levels of BMI. This may potentially pave the way for more precise and targeted treatment strategies. The QRIF approach intends to balance the trade-off between the prior insights and the observed data while being robust to potential false information. We further establish the desirable asymptotic properties under the challenging non-differentiable check loss functions via Huber loss approximation and nonconvex SCAD penalty via local linear approximation. Finally, we develop an efficient algorithm for the QRIF approach. Our simulation studies further demonstrate its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12212v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiantong Wang, Heng Lian, Yan Yu, Heping Zhang</dc:creator>
    </item>
  </channel>
</rss>
