<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Oct 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>State of Brain Emulation Report 2025</title>
      <link>https://arxiv.org/abs/2510.15745</link>
      <description>arXiv:2510.15745v1 Announce Type: new 
Abstract: The State of Brain Emulation Report 2025 provides a comprehensive overview of recent achievements in brain emulation. By analyzing current trends and the state of the art, this report aims to identify key opportunities and challenges facing the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15745v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niccol\`o Zanichelli, Maximilian Schons, Isaak Freeman, Philip Shiu, Anton Arkhipov</dc:creator>
    </item>
    <item>
      <title>GENESIS: A Generative Model of Episodic-Semantic Interaction</title>
      <link>https://arxiv.org/abs/2510.15828</link>
      <description>arXiv:2510.15828v1 Announce Type: new 
Abstract: A central challenge in cognitive neuroscience is to explain how semantic and episodic memory, two major forms of declarative memory, typically associated with cortical and hippocampal processing, interact to support learning, recall, and imagination. Despite significant advances, we still lack a unified computational framework that jointly accounts for core empirical phenomena across both semantic and episodic processing domains. Here, we introduce the Generative Episodic-Semantic Integration System (GENESIS), a computational model that formalizes memory as the interaction between two limited-capacity generative systems: a Cortical-VAE, supporting semantic learning and generalization, and a Hippocampal-VAE, supporting episodic encoding and retrieval within a retrieval-augmented generation (RAG) architecture. GENESIS reproduces hallmark behavioral findings, including generalization in semantic memory, recognition, serial recall effects and gist-based distortions in episodic memory, and constructive episodic simulation, while capturing their dynamic interactions. The model elucidates how capacity constraints shape the fidelity and memorability of experiences, how semantic processing introduces systematic distortions in episodic recall, and how episodic replay can recombine previous experiences. Together, these results provide a principled account of memory as an active, constructive, and resource-bounded process. GENESIS thus advances a unified theoretical framework that bridges semantic and episodic memory, offering new insights into the generative foundations of human cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15828v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco D'Alessandro, Leo D'Amato, Mikel Elkano, Mikel Uriz, Giovanni Pezzulo</dc:creator>
    </item>
    <item>
      <title>SpikeFit: Towards Optimal Deployment of Spiking Networks on Neuromorphic Hardware</title>
      <link>https://arxiv.org/abs/2510.15542</link>
      <description>arXiv:2510.15542v1 Announce Type: cross 
Abstract: This paper introduces SpikeFit, a novel training method for Spiking Neural Networks (SNNs) that enables efficient inference on neuromorphic hardware, considering all its stringent requirements: the number of neurons and synapses that can fit on a single device, and lower bit-width representations (e.g., 4-bit, 8-bit). Unlike conventional compressing approaches that address only a subset of these requirements (limited numerical precision and limited number of neurons in the network), SpikeFit treats the allowed weights' discrete values themselves as learnable parameters co-optimized with the model, allowing for optimal Clusterization-Aware Training (CAT) of the model's weights at low precision (2-, 4-, or 8-bit) which results in higher network compression efficiency, as well as limiting the number of unique synaptic connections to a value required by neuromorphic processor. This joint optimization allows SpikeFit to find a discrete weight set aligned with hardware constraints, enabling the most complete deployment across a broader range of neuromorphic processors than existing methods of SNN compression support. Moreover, SpikeFit introduces a new hardware-friendly Fisher Spike Contribution (FSC) pruning method showing the state-of-the-art performance. We demonstrate that for spiking neural networks constrained to only four unique synaptic weight values (M = 4), our SpikeFit method not only outperforms state-of-the-art SNNs compression methods and conventional baselines combining extreme quantization schemes and clustering algorithms, but also meets a wider range of neuromorphic hardware requirements and provides the lowest energy use in experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15542v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan Kartashov, Mariia Pushkareva, Iakov Karandashev</dc:creator>
    </item>
    <item>
      <title>Amplitude equations of associative memory patterns in spatially distributed systems</title>
      <link>https://arxiv.org/abs/2506.13576</link>
      <description>arXiv:2506.13576v3 Announce Type: replace 
Abstract: Evolution equations are derived for the amplitudes of associative memories: heterogeneous states stored in the connectivity of distributed systems with non-local interactions. The resulting coupled amplitude equations describe the spatio-temporal dynamics of memory recall. They capture pattern completion and selection, and show that short-range connections can sustain spatio-temporal memory pattern dynamics in the form of propagating patterning fronts. The derived amplitude equations are of the same form as those describing classical pattern-forming instabilities, indicating a universality of the dynamics of memory recall and pattern formation in non-equilibrium systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13576v3</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.PS</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akke Mats Houben</dc:creator>
    </item>
    <item>
      <title>Emergence of Functionally Differentiated Structures via Mutual Information Minimization in Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2507.12858</link>
      <description>arXiv:2507.12858v2 Announce Type: replace 
Abstract: Functional differentiation in the brain emerges as distinct regions specialize and is key to understanding brain function as a complex system. Previous research has modeled this process using artificial neural networks with specific constraints. Here, we propose a novel approach that induces functional differentiation in recurrent neural networks by minimizing mutual information between neural subgroups via mutual information neural estimation. We apply our method to a 2-bit working memory task and a chaotic signal separation task involving Lorenz and R\"ossler time series. Analysis of network performance, correlation patterns, and weight matrices reveals that mutual information minimization yields high task performance alongside clear functional modularity and moderate structural modularity. Importantly, our results show that functional differentiation, which is measured through correlation structures, emerges earlier than structural modularity defined by synaptic weights. This suggests that functional specialization precedes and probably drives structural reorganization within developing neural networks. Our findings provide new insights into how information-theoretic principles may govern the emergence of specialized functions and modular structures during artificial and biological brain development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12858v2</guid>
      <category>q-bio.NC</category>
      <category>nlin.AO</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Tomoda, Ichiro Tsuda, Yutaka Yamaguti</dc:creator>
    </item>
    <item>
      <title>Cross-Population Amplitude Coupling in High-Dimensional Oscillatory Neural Time Series</title>
      <link>https://arxiv.org/abs/2105.03508</link>
      <description>arXiv:2105.03508v3 Announce Type: replace-cross 
Abstract: Neural oscillations have long been considered important markers of interaction across brain regions, yet identifying coordinated oscillatory activity from high-dimensional multiple-electrode recordings remains challenging. We sought to quantify time-varying covariation of oscillatory amplitudes across two brain regions, during a memory task, based on local field potentials recorded from 96 electrodes in each region. We extended Canonical Correlation Analysis (CCA) to multiple time series through the cross-correlation of latent time series. This, however, introduces a large number of possible lead-lag cross-correlations across the two regions. To manage that high dimensionality we developed rigorous statistical procedures aimed at finding a small number of dominant lead-lag effects. The method correctly identified ground truth structure in realistic simulation-based settings. When we used it to analyze local field potentials recorded from prefrontal cortex and visual area V4 we obtained highly plausible results. The new statistical methodology could also be applied to other slowly-varying high-dimensional time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.03508v3</guid>
      <category>stat.ME</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Eric A. Yttri, Matthew A. Smith, Robert E. Kass</dc:creator>
    </item>
  </channel>
</rss>
