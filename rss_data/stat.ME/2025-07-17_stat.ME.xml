<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Jul 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Differentially Private Conformal Prediction via Quantile Binary Search</title>
      <link>https://arxiv.org/abs/2507.12497</link>
      <description>arXiv:2507.12497v1 Announce Type: new 
Abstract: Most Differentially Private (DP) approaches focus on limiting privacy leakage from learners based on the data that they are trained on, there are fewer approaches that consider leakage when procedures involve a calibration dataset which is common in uncertainty quantification methods such as Conformal Prediction (CP). Since there is a limited amount of approaches in this direction, in this work we deliver a general DP approach for CP that we call Private Conformity via Quantile Search (P-COQS). The proposed approach adapts an existing randomized binary search algorithm for computing DP quantiles in the calibration phase of CP thereby guaranteeing privacy of the consequent prediction sets. This however comes at a price of slightly under-covering with respect to the desired $(1 - \alpha)$-level when using finite-sample calibration sets (although broad empirical results show that the P-COQS generally targets the required level in the considered cases). Confirming properties of the adapted algorithm and quantifying the approximate coverage guarantees of the consequent CP, we conduct extensive experiments to examine the effects of privacy noise, sample size and significance level on the performance of our approach compared to existing alternatives. In addition, we empirically evaluate our approach on several benchmark datasets, including CIFAR-10, ImageNet and CoronaHack. Our results suggest that the proposed method is robust to privacy noise and performs favorably with respect to the current DP alternative in terms of empirical coverage, efficiency, and informativeness. Specifically, the results indicate that P-COQS produces smaller conformal prediction sets while simultaneously targeting the desired coverage and privacy guarantees in all these experimental settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12497v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ogonnaya M. Romanus, Roberto Molinari</dc:creator>
    </item>
    <item>
      <title>Cross-World Assumption and Refining Prediction Intervals for Individual Treatment Effects</title>
      <link>https://arxiv.org/abs/2507.12581</link>
      <description>arXiv:2507.12581v1 Announce Type: new 
Abstract: While average treatment effects (ATE) and conditional average treatment effects (CATE) provide valuable population- and subgroup-level summaries, they fail to capture uncertainty at the individual level. For high-stakes decision-making, individual treatment effect (ITE) estimates must be accompanied by valid prediction intervals that reflect heterogeneity and unit-specific uncertainty. However, the fundamental unidentifiability of ITEs limits the ability to derive precise and reliable individual-level uncertainty estimates. To address this challenge, we investigate the role of a cross-world correlation parameter, $ \rho(x) = cor(Y(1), Y(0) | X = x) $, which describes the dependence between potential outcomes, given covariates, in the Neyman-Rubin super-population model with i.i.d. units. Although $ \rho $ is fundamentally unidentifiable, we argue that in most real-world applications, it is possible to impose reasonable and interpretable bounds informed by domain-expert knowledge. Given $\rho$, we design prediction intervals for ITE, achieving more stable and accurate coverage with substantially shorter widths; often less than 1/3 of those from competing methods. The resulting intervals satisfy coverage guarantees $P\big(Y(1) - Y(0) \in C_{ITE}(X)\big) \geq 1 - \alpha$ and are asymptotically optimal under Gaussian assumptions. We provide strong theoretical and empirical arguments that cross-world assumptions can make individual uncertainty quantification both practically informative and statistically valid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12581v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik, Yaxuan Huang, Bin Yu</dc:creator>
    </item>
    <item>
      <title>An Efficient Approach to Design Bayesian Platform Trials</title>
      <link>https://arxiv.org/abs/2507.12647</link>
      <description>arXiv:2507.12647v1 Announce Type: new 
Abstract: Platform trials evaluate multiple experimental treatments against a common control group (and/or against each other), which often reduces the trial duration and sample size. Bayesian platform designs offer several practical advantages, including the flexible addition or removal of experimental arms using posterior probabilities and the incorporation of prior/external information. Regulatory agencies require that the operating characteristics of Bayesian designs are assessed by estimating the sampling distribution of posterior probabilities via Monte Carlo simulation. It is computationally intensive to repeat this simulation process for all design configurations considered, particularly for platform trials with complex interim decision procedures. In this paper, we propose an efficient method to assess operating characteristics and determine sample sizes as well as other design parameters for Bayesian platform trials. We prove theoretical results that allow us to model the joint sampling distribution of posterior probabilities across multiple endpoints and trial stages using simulations conducted at only two sample sizes. This work is motivated by design complexities in the SSTARLET trial, an ongoing Bayesian adaptive platform trial for tuberculosis preventive therapies (ClinicalTrials.gov ID: NCT06498414). Our proposed design method is not only computationally efficient but also capable of accommodating intricate, real-world trial constraints like those encountered in SSTARLET.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12647v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Lara Maleyeff, Shirin Golchi, Dick Menzies</dc:creator>
    </item>
    <item>
      <title>Refining the Notion of No Anticipation in Difference-in-Differences Studies</title>
      <link>https://arxiv.org/abs/2507.12891</link>
      <description>arXiv:2507.12891v1 Announce Type: new 
Abstract: We address an ambiguity in identification strategies using difference-in-differences, which are widely applied in empirical research, particularly in economics. The assumption commonly referred to as the "no-anticipation assumption" states that treatment has no effect on outcomes before its implementation. However, because standard causal models rely on a temporal structure in which causes precede effects, such an assumption seems to be inherently satisfied. This raises the question of whether the assumption is repeatedly stated out of redundancy or because the formal statements fail to capture the intended subject-matter interpretation. We argue that confusion surrounding the no-anticipation assumption arises from ambiguity in the intervention considered and that current formulations of the assumption are ambiguous. Therefore, new definitions and identification results are proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12891v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Piccininni, Eric J. Tchetgen Tchetgen, Mats J. Stensrud</dc:creator>
    </item>
    <item>
      <title>Is 1:1 Always Most Powerful? Why Unequal Allocation Merits Broader Consideration</title>
      <link>https://arxiv.org/abs/2507.13036</link>
      <description>arXiv:2507.13036v1 Announce Type: new 
Abstract: The principle of allocating an equal number of patients to each arm in a randomized controlled trial is widely accepted as the standard strategy for maximising the trial's statistical power. However, this long-held belief only holds true if the treatment groups have equal outcome variances, a condition that is often not met in practice. This paper questions the prevalent practice of exclusively defaulting to equal randomisation (ER) and posits that a departure from a 1:1 ratio can be both valid and advantageous. We demonstrate this principle through two simulated case studies, one with a binary endpoint and one with a continuous endpoint, comparing the performance of ER against preplanned Fixed Unequal Randomisation and Response-Adaptive Randomisation targeting Neyman allocation. Our results show that unequal ratios can increase statistical power while simultaneously allocating a substantially larger proportion of patients to the superior treatment arm compared to ER. We conclude that, when unequal variances are suspected, a strategic decision regarding the allocation ratio, rather than a default 1:1, constitutes the superior design choice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13036v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Stef Baas, David S. Robertson, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>Optimal Empirical Risk Minimization under Temporal Distribution Shifts</title>
      <link>https://arxiv.org/abs/2507.13287</link>
      <description>arXiv:2507.13287v1 Announce Type: new 
Abstract: Temporal distribution shifts pose a key challenge for machine learning models trained and deployed in dynamically evolving environments. This paper introduces RIDER (RIsk minimization under Dynamically Evolving Regimes) which derives optimally-weighted empirical risk minimization procedures under temporal distribution shifts. Our approach is theoretically grounded in the random distribution shift model, where random shifts arise as a superposition of numerous unpredictable changes in the data-generating process. We show that common weighting schemes, such as pooling all data, exponentially weighting data, and using only the most recent data, emerge naturally as special cases in our framework. We demonstrate that RIDER consistently improves out-of-sample predictive performance when applied as a fine-tuning step on the Yearbook dataset, across a range of benchmark methods in Wild-Time. Moreover, we show that RIDER outperforms standard weighting strategies in two other real-world tasks: predicting stock market volatility and forecasting ride durations in NYC taxi data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13287v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujin Jeong, Ramesh Johari, Dominik Rothenh\"ausler, Emily Fox</dc:creator>
    </item>
    <item>
      <title>Ranking Vectors Clustering: Theory and Applications</title>
      <link>https://arxiv.org/abs/2507.12583</link>
      <description>arXiv:2507.12583v1 Announce Type: cross 
Abstract: We study the problem of clustering ranking vectors, where each vector represents preferences as an ordered list of distinct integers. Specifically, we focus on the k-centroids ranking vectors clustering problem (KRC), which aims to partition a set of ranking vectors into k clusters and identify the centroid of each cluster. Unlike classical k-means clustering (KMC), KRC constrains both the observations and centroids to be ranking vectors. We establish the NP-hardness of KRC and characterize its feasible set. For the single-cluster case, we derive a closed-form analytical solution for the optimal centroid, which can be computed in linear time. To address the computational challenges of KRC, we develop an efficient approximation algorithm, KRCA, which iteratively refines initial solutions from KMC, referred to as the baseline solution. Additionally, we introduce a branch-and-bound (BnB) algorithm for efficient cluster reconstruction within KRCA, leveraging a decision tree framework to reduce computational time while incorporating a controlling parameter to balance solution quality and efficiency. We establish theoretical error bounds for KRCA and BnB. Through extensive numerical experiments on synthetic and real-world datasets, we demonstrate that KRCA consistently outperforms baseline solutions, delivering significant improvements in solution quality with fast computational times. This work highlights the practical significance of KRC for personalization and large-scale decision making, offering methodological advancements and insights that can be built upon in future studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12583v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Fattahi, Ali Eshragh, Babak Aslani, Meysam Rabiee</dc:creator>
    </item>
    <item>
      <title>NA-DiD: Extending Difference-in-Differences with Capabilities</title>
      <link>https://arxiv.org/abs/2507.12690</link>
      <description>arXiv:2507.12690v1 Announce Type: cross 
Abstract: This paper introduces the Non-Additive Difference-in-Differences (NA-DiD) framework, which extends classical DiD by incorporating non-additive measures the Choquet integral for effect aggregation. It serves as a novel econometric tool for impact evaluation, particularly in settings with non-additive treatment effects. First, we introduce the integral representation of the classial DiD model, and then extend it to non-additive measures, therefore deriving the formulae for NA-DiD estimation. Then, we give its theoretical properties. Applying NA-DiD to a simulated hospital hygiene intervention, we find that classical DiD can overestimate treatment effects, f.e. failing to account for compliance erosion. In contrast, NA-DiD provides a more accurate estimate by incorporating non-linear aggregation. The Julia implementation of the techniques used and introduced in this article is provided in the appendices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12690v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanis{\l}aw M. S. Halkiewicz</dc:creator>
    </item>
    <item>
      <title>Placebo Discontinuity Design</title>
      <link>https://arxiv.org/abs/2507.12693</link>
      <description>arXiv:2507.12693v1 Announce Type: cross 
Abstract: Standard regression discontinuity design (RDD) models rely on the continuity of expected potential outcomes at the cutoff. The standard continuity assumption can be violated by strategic manipulation of the running variable, which is realistic when the cutoff is widely known and when the treatment of interest is a social program or government benefit. In this work, we identify the treatment effect despite such a violation, by leveraging a placebo treatment and a placebo outcome. We introduce a local instrumental variable estimator. Our estimator decomposes into two terms: the standard RDD estimator of the target outcome's discontinuity, and a new adjustment term based on the placebo outcome's discontinuity. We show that our estimator is consistent, and we justify a robust bias-corrected inference procedure. Our method expands the applicability of RDD to settings with strategic behavior around the cutoff, which commonly arise in social science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12693v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Singh, Moses Stewart</dc:creator>
    </item>
    <item>
      <title>Rank-adaptive covariance testing with applications to genomics and neuroimaging</title>
      <link>https://arxiv.org/abs/2309.10284</link>
      <description>arXiv:2309.10284v3 Announce Type: replace 
Abstract: In biomedical studies, testing for differences in covariance offers scientific insights beyond mean differences, especially when differences are driven by complex joint behavior between features. However, when differences in joint behavior are weakly dispersed across many dimensions and arise from differences in low-rank structures within the data, as is often the case in genomics and neuroimaging, existing two-sample covariance testing methods may suffer from power loss. The Ky-Fan(k) norm, defined by the sum of the top Ky-Fan(k) singular values, is a simple and intuitive matrix norm able to capture signals caused by differences in low-rank structures between matrices, but its statistical properties in hypothesis testing have not been studied well. In this paper, we investigate the behavior of the Ky-Fan(k) norm in two-sample covariance testing. Ultimately, we propose a novel methodology, Rank-Adaptive Covariance Testing (RACT), which is able to leverage differences in low-rank structures found in the covariance matrices of two groups in order to maximize power. RACT uses permutation for statistical inference, ensuring an exact Type I error control. We validate RACT in simulation studies and evaluate its performance when testing for differences in gene expression networks between two types of lung cancer, as well as testing for covariance heterogeneity in diffusion tensor imaging (DTI) data taken on two different scanner types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10284v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Veitch, Yinqiu He, Jun Young Park</dc:creator>
    </item>
    <item>
      <title>Flexible Nonparametric Inference for Causal Effects under the Front-Door Model</title>
      <link>https://arxiv.org/abs/2312.10234</link>
      <description>arXiv:2312.10234v2 Announce Type: replace 
Abstract: Evaluating causal treatment effects in observational studies requires addressing confounding. While the back-door criterion enables identification through adjustment for observed covariates, it fails in the presence of unmeasured confounding. The front-door criterion offers an alternative by leveraging variables that fully mediate the treatment effect and are unaffected by unmeasured confounders of the treatment-outcome pair. We develop novel one-step and targeted minimum loss-based estimators for both the average treatment effect and the average treatment effect on the treated under front-door assumptions. Our estimators are built on multiple parameterizations of the observed data distribution, including approaches that avoid modeling the mediator density entirely, and are compatible with flexible, machine learning-based nuisance estimation. We establish conditions for root-$n$ consistency and asymptotic linearity by deriving second-order remainder bounds. We also develop flexible tests for assessing identification assumptions, including a doubly robust testing procedure, within a semiparametric extension of the front-door model that encodes generalized (Verma) independence constraints. We further show how these constraints can be leveraged to improve the efficiency of causal effect estimators. Simulation studies confirm favorable finite-sample performance, and real-data applications in education and emergency medicine illustrate the practical utility of our methods. An accompanying R package, fdcausal, implements all proposed procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10234v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Guo, David Benkeser, Razieh Nabi</dc:creator>
    </item>
    <item>
      <title>Integrated path stability selection</title>
      <link>https://arxiv.org/abs/2403.15877</link>
      <description>arXiv:2403.15877v3 Announce Type: replace 
Abstract: Stability selection is a popular method for improving feature selection algorithms. One of its key attributes is that it provides theoretical upper bounds on the expected number of false positives, E(FP), enabling false positive control in practice. However, stability selection often selects few features because existing bounds on E(FP) are relatively loose. In this paper, we introduce a novel approach to stability selection based on integrating stability paths rather than maximizing over them. This yields upper bounds on E(FP) that are much stronger than previous bounds, leading to significantly more true positives in practice for the same target E(FP). Furthermore, our method requires no more computation than the original stability selection algorithm. We demonstrate the method on simulations and real data from two cancer studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15877v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2525589</arxiv:DOI>
      <dc:creator>Omar Melikechi, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>A Bayesian Joint Modelling for Misclassified Interval-censoring and Competing Risks</title>
      <link>https://arxiv.org/abs/2404.09362</link>
      <description>arXiv:2404.09362v2 Announce Type: replace 
Abstract: In active surveillance of prostate cancer, cancer progression is interval-censored and the examination to detect progression is subject to misclassification, usually false negatives. Meanwhile, patients may initiate early treatment before progression detection, constituting a competing risk. We developed the Misclassification-Corrected Interval-censored Cause-specific Joint Model (MCICJM) to estimate the association between longitudinal biomarkers and cancer progression in this setting. The sensitivity of the examination is considered in the likelihood of this model via a parameter that may be set to a specific value if the sensitivity is known, or for which a prior distribution can be specified if the sensitivity is unknown. Our simulation results show that misspecification of the sensitivity parameter or ignoring it entirely impacts the model parameters, especially the parameter uncertainty and the baseline hazards. Moreover, specification of a prior distribution for the sensitivity parameter may reduce the risk of misspecification in settings where the exact sensitivity is unknown, but may cause identifiability issues. Thus, imposing restrictions on the baseline hazards is recommended. A trade-off between modelling with a sensitivity constant at the risk of misspecification and a sensitivity prior at the cost of flexibility needs to be decided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09362v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenwei Yang, Dimitris Rizopoulos, Eveline A. M. Heijnsdijk, Lisa F. Newcomb, Nicole S. Erler</dc:creator>
    </item>
    <item>
      <title>Formalising causal inference as prediction on a target population</title>
      <link>https://arxiv.org/abs/2407.17385</link>
      <description>arXiv:2407.17385v3 Announce Type: replace 
Abstract: The standard approach to causal modelling especially in social and health sciences is the potential outcomes framework due to Neyman and Rubin. In this framework, observations are thought to be drawn from a distribution over variables of interest, and the goal is to identify parameters of this distribution. Even though the stated goal is often to inform decision making on some target population, there is no straightforward way to include these target populations in the framework. Instead of modelling the relationship between the observed sample and the target population, the inductive assumptions in this framework take the form of abstract sampling and independence assumptions. In this paper, we develop a version of this framework that construes causal inference as treatment-wise predictions for finite populations where all assumptions are testable in retrospect; this means that one can not only test predictions themselves (without any fundamental problem) but also investigate sources of error when they fail. Due to close connections to the original framework, established methods can still be be analysed under the new framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17385v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benedikt H\"oltgen, Robert C. Williamson</dc:creator>
    </item>
    <item>
      <title>From Likelihood to Limit State: A Reliability-Inspired Framework for Bayesian Evidence Estimation and High-dimensional Sampling</title>
      <link>https://arxiv.org/abs/2409.19910</link>
      <description>arXiv:2409.19910v2 Announce Type: replace 
Abstract: Bayesian analysis plays a crucial role in estimating distribution of unknown parameters for given data and model. Due to the curse of dimensionality, it becomes difficult for high-dimensional problems, especially when multiple modes exist. This paper introduces an efficient Bayesian posterior sampling algorithm, based on a new interpretation of evidence from the perspective of structural reliability estimation. That is, the evidence can be equivalently formulated as an integration of failure probabilities, by regarding the likelihood function as a limit state function. The evidence is then evaluated with subset simulation (SuS) algorithm. The posterior samples can be obtained following the principle of importance resampling as a postprocessing procedure. The estimation variance is derived to quantify the inherent uncertainty associated with the SuS estimator of evidence. The effective sample size is introduced to measure the quality of posterior sampling. Three benchmark examples are first considered to illustrate the performance of the proposed algorithm by comparing it with two state-of-art algorithms. It is then used for the finite element model updating, showing its applicability in practical engineering problems. The proposed SuS algorithm exhibits comparable or even better performance in evidence estimation and posterior sampling, compared to the aBUS and MULTINEST algorithms, especially when the dimension of unknown parameters is high.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19910v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Liao, Binbin Li, Hua-Ping Wan</dc:creator>
    </item>
    <item>
      <title>Time-dependent Predictive Accuracy Metrics in the Context of Interval Censoring and Competing Risks</title>
      <link>https://arxiv.org/abs/2501.01280</link>
      <description>arXiv:2501.01280v2 Announce Type: replace 
Abstract: Evaluating the performance of a prediction model is a common task in medical statistics. Standard accuracy metrics require the observation of the true outcomes. This is typically not possible in the setting with time-to-event outcomes due to censoring. Interval censoring, the presence of time-varying covariates, and competing risks present additional challenges in obtaining those accuracy metrics. In this study, we propose two methods to deal with interval censoring in a time-varying competing risk setting: a model-based approach and the inverse probability of censoring weighting (IPCW) approach, focusing on three key time-dependent metrics: area under the receiver-operating characteristic curve (AUC), Brier score, and expected predictive cross-entropy (EPCE). The evaluation is conducted over a medically relevant time interval of interest, $[t, \Delta t)$. The model-based approach includes all subjects in the risk set, using their predicted risks to contribute to the accuracy metrics. In contrast, the IPCW approach only considers the subset of subjects who are known to be event-free or experience the event within the interval of interest. we performed a simulation study to compare the performance of the two approaches with regard to the three metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01280v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhenwei Yang, Dimitris Rizopoulos, Lisa F. Newcomb, Nicole S. Erler</dc:creator>
    </item>
    <item>
      <title>Revisiting the Berkeley Admissions data: Statistical Tests for Causal Hypotheses</title>
      <link>https://arxiv.org/abs/2502.10161</link>
      <description>arXiv:2502.10161v2 Announce Type: replace 
Abstract: Reasoning about fairness through correlation-based notions is rife with pitfalls. The 1973 University of California, Berkeley graduate school admissions case from Bickel et. al. (1975) is a classic example of one such pitfall, namely Simpson's paradox. The discrepancy in admission rates among males and female applicants, in the aggregate data over all departments, vanishes when admission rates per department are examined. We reason about the Berkeley graduate school admissions case through a causal lens. In the process, we introduce a statistical test for causal hypothesis testing based on Pearl's instrumental-variable inequalities (Pearl 1995). We compare different causal notions of fairness that are based on graphical, counterfactual and interventional queries on the causal model, and develop statistical tests for these notions that use only observational data. We study the logical relations between notions, and show that while notions may not be equivalent, their corresponding statistical tests coincide for the case at hand. We believe that a thorough case-based causal analysis helps develop a more principled understanding of both causal hypothesis testing and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10161v2</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourbh Bhadane, Joris M. Mooij, Philip Boeken, Onno Zoeter</dc:creator>
    </item>
    <item>
      <title>Interpretable Transformation and Analysis of Timelines through Learning via Surprisability</title>
      <link>https://arxiv.org/abs/2503.04502</link>
      <description>arXiv:2503.04502v2 Announce Type: replace 
Abstract: The analysis of high-dimensional timeline data and the identification of outliers and anomalies is critical across diverse domains, including sensor readings, biological and medical data, historical records, and global statistics. However, conventional analysis techniques often struggle with challenges such as high dimensionality, complex distributions, and sparsity. These limitations hinder the ability to extract meaningful insights from complex temporal datasets, making it difficult to identify trending features, outliers, and anomalies effectively. Inspired by surprisability -- a cognitive science concept describing how humans instinctively focus on unexpected deviations - we propose Learning via Surprisability (LvS), a novel approach for transforming high-dimensional timeline data. LvS quantifies and prioritizes anomalies in time-series data by formalizing deviations from expected behavior. LvS bridges cognitive theories of attention with computational methods, enabling the detection of anomalies and shifts in a way that preserves critical context, offering a new lens for interpreting complex datasets. We demonstrate the usefulness of LvS on three high-dimensional timeline use cases: a time series of sensor data, a global dataset of mortality causes over multiple years, and a textual corpus containing over two centuries of State of the Union Addresses by U.S. presidents. Our results show that the LvS transformation enables efficient and interpretable identification of outliers, anomalies, and the most variable features along the timeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04502v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Osnat Mokryn, Teddy Lazebnik, Hagit Ben Shoshan</dc:creator>
    </item>
    <item>
      <title>Total/dual correlation/coherence, redundancy/synergy, complexity, and O-information for real and complex valued multivariate data</title>
      <link>https://arxiv.org/abs/2507.08773</link>
      <description>arXiv:2507.08773v3 Announce Type: replace 
Abstract: Firstly, assuming Gaussianity, equations for the following information theory measures are presented: total correlation/coherence (TC), dual total correlation/coherence (DTC), O-information, TSE complexity, and redundancy-synergy index (RSI). Since these measures are functions of the covariance matrix "S" and its inverse "S^-1", the associated Wishart and inverse-Wishart distributions are of note. DTC is shown to be the Kullback-Leibler (KL) divergence for the inverse-Wishart pair "(S^-1)" and its diagonal matrix "D=diag(S^-1)", shedding light on its interpretation as a measure of "total partial correlation", -lndetP, with test hypothesis H0: P=I, where "P" is the standardized inverse covariance (i.e. P=(D^-1/2)(S^-1)(D^-1/2). The second aim of this paper introduces a generalization of all these measures for structured groups of variables. For instance, consider three or more groups, each consisting of three or more variables, with predominant redundancy within each group, but with synergistic interactions between groups. O-information will miss the between group synergy (since redundancy occurs more often in the system). In contrast, the structured O-information measure presented here will correctly report predominant synergy between groups. This is a relevant generalization towards structured multivariate information measures. A third aim is the presentation of a framework for quantifying the contribution of "connections" between variables, to the system's TC, DTC, O-information, and TSE complexity. A fourth aim is to present a generalization of the redundancy-synergy index for quantifying the contribution of a group of variables to the system's redundancy-synergy balance. Finally, it is shown that the expressions derived here directly apply to data from several other elliptical distributions. All program codes, data files, and executables are available (https://osf.io/jd37g/).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08773v3</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roberto D. Pascual-Marqui, Kieko Kochi, Toshihiko Kinoshita</dc:creator>
    </item>
    <item>
      <title>Does $K$-fold CV based penalty perform variable selection or does it lead to $n^{1/2}$-consistency in Lasso?</title>
      <link>https://arxiv.org/abs/2507.12457</link>
      <description>arXiv:2507.12457v2 Announce Type: replace 
Abstract: Least absolute shrinkage and selection operator or Lasso, introduced by Tibshirani (1996), is one of the widely used regularization methods in regression. It is observed that the properties of Lasso vary wildly depending on the choice of the penalty parameter. The recent results of Lahiri (2021) suggest that, depending on the nature of the penalty parameter, Lasso can either be variable selection consistent or be $n^{1/2}-$consistent. However, practitioners generally implement Lasso by choosing the penalty parameter in a data-dependent way, the most popular being the $K$-fold cross-validation. In this paper, we explore the variable selection consistency and $n^{1/2}-$consistency of Lasso when the penalty is chosen based on $K$-fold cross-validation with $K$ being fixed. We consider the fixed-dimensional heteroscedastic linear regression model and show that Lasso with $K$-fold cross-validation based penalty is $n^{1/2}-$consistent, but not variable selection consistent. We also establish the $n^{1/2}-$consistency of the $K$-fold cross-validation based penalty as an intermediate result. Additionally, as a consequence of $n^{1/2}-$consistency, we establish the validity of Bootstrap to approximate the distribution of the Lasso estimator based on $K-$fold cross-validation. We validate the Bootstrap approximation in finite samples based on a moderate simulation study. Thus, our results essentially justify the use of $K$-fold cross-validation in practice to draw inferences based on $n^{1/2}-$scaled pivotal quantities in Lasso regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12457v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mayukh Choudhury, Debraj Das</dc:creator>
    </item>
    <item>
      <title>Optimal designs for discrete choice models via graph Laplacians</title>
      <link>https://arxiv.org/abs/2208.08926</link>
      <description>arXiv:2208.08926v3 Announce Type: replace-cross 
Abstract: In discrete choice experiments, the information matrix depends on the model parameters. Therefore designing optimally informative experiments for arbitrary initial parameters often yields highly nonlinear optimization problems and makes optimal design infeasible. To overcome such challenges, we connect design theory for discrete choice experiments with Laplacian matrices of undirected graphs, resulting in complexity reduction and feasibility of optimal design. We rewrite the $D$-optimality criterion in terms of Laplacians via Kirchhoff's matrix tree theorem, and show that its dual has a simple description via the Cayley-Menger determinant of the Farris transform of the Laplacian matrix. This results in a drastic reduction of complexity and allows us to implement a gradient descent algorithm to find locally $D$-optimal designs. For the subclass of Bradley-Terry paired comparison models, we find a direct link to maximum likelihood estimation for Laplacian-constrained Gaussian graphical models. Finally, we study the performance of our algorithm and demonstrate its application to real and simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.08926v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frank R\"ottger, Thomas Kahle, Rainer Schwabe</dc:creator>
    </item>
    <item>
      <title>Nonparametric IPSS: Fast, flexible feature selection with false discovery control</title>
      <link>https://arxiv.org/abs/2410.02208</link>
      <description>arXiv:2410.02208v3 Announce Type: replace-cross 
Abstract: Feature selection is a critical task in machine learning and statistics. However, existing feature selection methods either (i) rely on parametric methods such as linear or generalized linear models, (ii) lack theoretical false discovery control, or (iii) identify few true positives. Here, we introduce a general feature selection method with finite-sample false discovery control based on applying integrated path stability selection (IPSS) to arbitrary feature importance scores. The method is nonparametric whenever the importance scores are nonparametric, and it estimates q-values, which are better suited to high-dimensional data than p-values. We focus on two special cases using importance scores from gradient boosting (IPSSGB) and random forests (IPSSRF). Extensive nonlinear simulations with RNA sequencing data show that both methods accurately control the false discovery rate and detect more true positives than existing methods. Both methods are also efficient, running in under 20 seconds when there are 500 samples and 5000 features. We apply IPSSGB and IPSSRF to detect microRNAs and genes related to cancer, finding that they yield better predictions with fewer features than existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02208v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/bioinformatics/btaf299</arxiv:DOI>
      <arxiv:journal_reference>Bioinformatics (2025)</arxiv:journal_reference>
      <dc:creator>Omar Melikechi, David B. Dunson, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>Carefree multiple testing with e-processes</title>
      <link>https://arxiv.org/abs/2501.19360</link>
      <description>arXiv:2501.19360v2 Announce Type: replace-cross 
Abstract: E-processes enable hypothesis testing with ongoing data collection while maintaining Type I error control. However, when testing multiple hypotheses simultaneously, current $e$-value based multiple testing methods such as e-BH are not invariant to the order in which data are gathered for the different $e$-processes. This can lead to undesirable situations, e.g., where a hypothesis rejected at time $t$ is no longer rejected at time $t+1$ after choosing to gather more data for one or more $e$-processes unrelated to that hypothesis. We argue that multiple testing methods should always work with suprema of $e$-processes. We provide an example to illustrate that e-BH does not control the FDR, at level $\alpha$ when applied to suprema of $e$-processes. From the same example we see that the FWER is not controlled with averaging, and also closed e-BH does not control the FDR. We show that adjusters can be used to ensure FDR-sup control with e-BH under arbitrary dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19360v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yury Tavyrikov, Jelle J. Goeman, Rianne de Heide</dc:creator>
    </item>
    <item>
      <title>MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment</title>
      <link>https://arxiv.org/abs/2502.18699</link>
      <description>arXiv:2502.18699v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18699v2</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>Edgeworth corrections for the spiked eigenvalues of non-Gaussian sample covariance matrices with applications</title>
      <link>https://arxiv.org/abs/2507.09584</link>
      <description>arXiv:2507.09584v2 Announce Type: replace-cross 
Abstract: Yang and Johnstone (2018) established an Edgeworth correction for the largest sample eigenvalue in a spiked covariance model under the assumption of Gaussian observations, leaving the extension to non-Gaussian settings as an open problem. In this paper, we address this issue by establishing first-order Edgeworth expansions for spiked eigenvalues in both single-spike and multi-spike scenarios with non-Gaussian data. Leveraging these expansions, we construct more accurate confidence intervals for the population spiked eigenvalues and propose a novel estimator for the number of spikes. Simulation studies demonstrate that our proposed methodology outperforms existing approaches in both robustness and accuracy across a wide range of settings, particularly in low-dimensional cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09584v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yashi Wei, Jiang Hu, Zhidong Bai</dc:creator>
    </item>
  </channel>
</rss>
