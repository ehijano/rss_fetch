<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 05:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Beyond Expected Goals: A Probabilistic Framework for Shot Occurrences in Soccer</title>
      <link>https://arxiv.org/abs/2512.00203</link>
      <description>arXiv:2512.00203v1 Announce Type: new 
Abstract: Expected goals (xG) models estimate the probability that a shot results in a goal from its context (e.g., location, pressure), but they operate only on observed shots. We propose xG+, a possession-level framework that first estimates the probability that a shot occurs within the next second and its corresponding xG if it were to occur. We also introduce ways to aggregate this joint probability estimate over the course of a possession. By jointly modeling shot-taking behavior and shot quality, xG+ remedies the conditioning-on-shots limitation of standard xG. We show that this improves predictive accuracy at the team level and produces a more persistent player skill signal than standard xG models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00203v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Pipping, Tianshu Feng, R. Paul Sabin</dc:creator>
    </item>
    <item>
      <title>Bayesian Analysis of Hotel Booking Cancellations: A Hierarchical Modeling Approach</title>
      <link>https://arxiv.org/abs/2512.00240</link>
      <description>arXiv:2512.00240v1 Announce Type: new 
Abstract: This study presents a comprehensive Bayesian analysis of hotel booking cancellations using PyMC, comparing three model specifications of increasing complexity. We investigate how lead time, special requests, and parking requirements affect cancellation probability, and explore interaction effects with hotel type. Using MCMC sampling (NUTS algorithm) on 5,000 booking records, we find strong evidence that longer lead times increase cancellation probability (posterior mean: 0.600, 95\% HDI: [0.532, 0.661]), while special requests (posterior mean: -0.642) and parking (posterior mean: -3.879) significantly reduce cancellation risk. Model comparison via WAIC reveals that the full interaction model provides the best predictive performance, suggesting that the effects of booking characteristics vary systematically between city and resort hotels. This Bayesian approach enables full uncertainty quantification and provides actionable insights for revenue management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00240v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingdong Yang</dc:creator>
    </item>
    <item>
      <title>Robust Wrapped Gaussian Process Inference for Noisy Angular Data</title>
      <link>https://arxiv.org/abs/2512.00277</link>
      <description>arXiv:2512.00277v1 Announce Type: new 
Abstract: Angular data are commonly encountered in settings with a directional or orientational component. Regressing an angular response on real-valued features requires intrinsically capturing the circular or spherical manifold the data lie on, or using an appropriate extrinsic transformation. A popular example of the latter is the technique of distributional wrapping, in which functions are "wrapped" around the unit circle via a modulo-$2{\pi}$ transformation. This approach enables flexible, non-linear models like Gaussian processes (GPs) to properly account for circular structure. While straightforward in concept, the need to infer the latent unwrapped distribution along with its wrapping behavior makes inference difficult in noisy response settings, as misspecification of one can severely hinder estimation of the other. However, applications such as radiowave analysis (Shangguan et al., 2015) and biomedical engineering (Kurz and Hanebeck, 2015) encounter radial data where wrapping occurs in only one direction. We therefore propose a novel wrapped GP (WGP) model formulation that recognizes monotonic wrapping behavior for more accurate inference in these situations. This is achieved by estimating the locations where wrapping occurs and partitioning the input space accordingly. We also specify a more robust Student's t response likelihood, and take advantage of an elliptical slice sampling (ESS) algorithm for rejection-free sampling from the latent GP space. We showcase our model's preferable performance on simulated examples compared to existing WGP methodologies. We then apply our method to the problem of localizing radiofrequency identification (RFID) tags, in which we model the relationship between frequency and phase angle to infer how far away an RFID tag is from an antenna.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00277v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Cooper, Justin Strait, Mary Frances Dorn, Robert B. Gramacy, Brendon Parsons, Alessandro Cattaneo</dc:creator>
    </item>
    <item>
      <title>Kicking for Goal or Touch? An Expected Points Framework for Penalty Decisions in Rugby Union</title>
      <link>https://arxiv.org/abs/2512.00312</link>
      <description>arXiv:2512.00312v1 Announce Type: new 
Abstract: Following a penalty in rugby union, teams typically choose between attempting a shot at goal or kicking to touch to pursue a try. We develop an Expected Points (EP) framework that quantifies the value of each option as a function of both field location and game context. Using phase-level data from the 2018/19 Premiership Rugby season (35,199 phases across 132 matches) and an angle-distance model of penalty kick success estimated from international records, we construct two surfaces: (i) the expected points of a possession beginning with a lineout, and (ii) the expected points of a kick at goal, taking into account the in-game consequences of made and missed kicks. We then compare these surfaces to produce decision maps that indicate where kicking for goal or kicking to touch maximizes expected return, and we analyze how the boundary shifts with game context and the expected meters gained to touch. Our results provide a unified, data-driven method for evaluating penalty decisions and can be tailored to team-specific kickers and lineout units. This study offers, to our knowledge, the first comprehensive EP-based assessment of penalty strategy in rugby union and outlines extensions to win-probability analysis and richer tracking data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00312v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenny Watts, Jonathan Pipping</dc:creator>
    </item>
    <item>
      <title>Exploring Student Interactions with AI-Powered Learning Tools: A Qualitative Study Connecting Interaction Patterns to Educational Learning Theories</title>
      <link>https://arxiv.org/abs/2512.00519</link>
      <description>arXiv:2512.00519v1 Announce Type: new 
Abstract: With the growing use of artificial intelligence in classrooms and online learning, it has become important to understand how students actually interact with AI tools and how such interactions match with traditional ways of learning. In this study, we focused on how students engage with tools like ChatGPT, Grammarly, and Khan Academy, and tried to connect their usage patterns with well known learning theories. A small experiment was carried out where undergraduate students completed different learning tasks using these tools, and later shared their thoughts through semi structured interviews. We looked at four types of interaction directive, assistive, dialogic, and empathetic and compared them with learning approaches like behaviorism, cognitivism, constructivism, and humanism. After analyzing the interviews, we found five main themes Feedback and Reinforcement, Cognitive Scaffolding, Dialogic Engagement, Personalization and Empathy, and Learning Agency. Our findings show that how useful an AI tool feels is not just about its features, but also about how students personally connect with it. By relating these experiences to existing educational theories, we have tried to build a framework that can help design better AI based learning environments. This work aims to support teachers, EdTech designers, and education researchers by giving practical suggestions grounded in real student experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00519v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5296/jet.v13i2.23228</arxiv:DOI>
      <dc:creator>Prathamesh Muzumdar, Sumanth Cheemalapati</dc:creator>
    </item>
    <item>
      <title>A State-Space Approach to Modeling Tire Degradation in Formula 1 Racing</title>
      <link>https://arxiv.org/abs/2512.00640</link>
      <description>arXiv:2512.00640v1 Announce Type: new 
Abstract: Tire degradation plays a critical role in Formula 1 race strategy, influencing both lap times and optimal pit-stop decisions. This paper introduces a Bayesian state-space modeling framework for estimating the latent degradation dynamics of Formula 1 tires using publicly available timing data from the FastF1 Python API. Lap times are modeled as a function of fuel mass and latent tire pace, with pit stops represented as state resets. Several model extensions are explored, including compound-specific degradation rates, time-varying degradation dynamics, and a skewed t observation model to account for asymmetric driver errors. Using Lewis Hamilton's performance in the 2025 Austrian Grand Prix as a case study, the proposed framework demonstrates superior predictive performance over an ARIMA(2,1,2) baseline, particularly under the skewed t specification. Although compound-specific degradation differences were not statistically distinct, the results show that the state-space approach provides interpretable, probabilistic, and computationally efficient estimates of tire degradation. This framework can be generalized to multi-race or multi-driver analyses, offering a foundation for real-time strategy modeling and performance prediction in Formula 1 racing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00640v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cole Cappello, Andrew Hoegh</dc:creator>
    </item>
    <item>
      <title>Model-based indicators for co-clustered environments and species communities</title>
      <link>https://arxiv.org/abs/2512.00678</link>
      <description>arXiv:2512.00678v1 Announce Type: new 
Abstract: Accurate biodiversity monitoring is essential for effective environmental policy, yet current practices often rely on arbitrarily defined ecosystems, communities, and ad-hoc indicator species, limiting cost-efficiency and reproducibility. We present a model-based framework that infers ecological sub-communities and corresponding indicators in terms of habitat and species from species survey data, such as large-scale arthropod abundance data used here as example. Environments and species are co-clustered using Bayesian decoupling for Poisson factorization. Latent, hierarchical regression relates observable habitat features to each subcommunity. Additionally, we propose a novel, model-based ranking of indicator species based on the learned subcommunities, generalizing classical approaches. This integrated approach motivates model-based ecosystem classification and indicator species selection, offering a scalable, reproducible pathway for biodiversity monitoring and informed conservation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00678v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Braden Scherting, Otso Ovaskainen, Tomas Roslin, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>A Clinical Instrument to Measure Patient Anecdotes in Clinical Trials</title>
      <link>https://arxiv.org/abs/2512.01041</link>
      <description>arXiv:2512.01041v1 Announce Type: new 
Abstract: Clinical trials assessing neurological treatment are challenging due to the diversity of brain function, and the difficulty in quantifying it. Traditional treatment studies in epilepsy use seizure frequency as the primary outcome measure, which may overlooking meaningful improvements in patients' quality of life. This paper introduces the Clinical Instrument for Measuring Patient Anecdotes in Clinical Trials (Clinical IMPACT), a novel tool designed to capture qualitative non-seizure improvement across neurological domains.
  The Clinical IMPACT incorporates open-ended inquiries that allow participants or caregivers to identify and select anecdotal evidence of their most significant treatment benefits. A blinded panel of experts ranks these anecdotes, facilitating a rigorous statistical analysis using the Wilcoxon Rank-Sum Test to detect treatment efficacy. The approach is resistant to type 1 error, yet comprehensive in its ability to capture real-world effects on quality of life.
  The potential of the Clinical IMPACT tool to enhance sensitivity while also providing qualitative insights that can inform patients, healthcare providers, and regulatory bodies about treatment effects makes it important to consider in any neurological trial. We describe how it can be used in epilepsy, and advocate for its inclusion as a key secondary endpoint to provide a perspective on non-seizure outcomes, which have previously been challenging to measure, let alone to interpret, even when the clinical trial is positive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01041v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ian Miller, Ann Hyslop, Colin Decker</dc:creator>
    </item>
    <item>
      <title>pvEBayes: An R Package for Empirical Bayes Methods in Pharmacovigilance</title>
      <link>https://arxiv.org/abs/2512.01057</link>
      <description>arXiv:2512.01057v1 Announce Type: new 
Abstract: Monitoring the safety of medical products is a core concern of contemporary pharmacovigilance. To support drug safety assessment, Spontaneous Reporting Systems (SRS) collect reports of suspected adverse events of approved medical products offering a critical resource for identifying potential safety concerns that may not emerge during clinical trials. Modern nonparametric empirical Bayes methods are flexible statistical approaches that can accurately identify and estimate the strength of the association between an adverse event and a drug from SRS data. However, there is currently no comprehensive and easily accessible implementation of these methods. Here, we introduce the R package pvEBayes, which implements a suite of nonparametric empirical Bayes methods for pharmacovigilance, along with post-processing tools and graphical summaries for streamlining the application of these methods. Detailed examples are provided to demonstrate the application of the package through analyses of two real-world SRS datasets curated from the publicly available FDA FAERS database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01057v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihao Tan, Marianthi Markatou, Saptarshi Chakraborty</dc:creator>
    </item>
    <item>
      <title>COVID-19 Forecasting from U.S. Wastewater Surveillance Data: A Retrospective Multi-Model Study (2022-2024)</title>
      <link>https://arxiv.org/abs/2512.01074</link>
      <description>arXiv:2512.01074v1 Announce Type: new 
Abstract: Accurate and reliable forecasting models are critical for guiding public health responses and policy decisions during pandemics such as COVID-19. Retrospective evaluation of model performance is essential for improving epidemic forecasting capabilities. In this study, we used COVID-19 wastewater data from CDC's National Wastewater Surveillance System to generate sequential weekly retrospective forecasts for the United States from March 2022 through September 2024, both at the national level and for four major regions (Northeast, Midwest, South, and West). We produced 133 weekly forecasts using 11 models, including ARIMA, generalized additive models (GAM), simple linear regression (SLR), Prophet, and the n-sub-epidemic framework (top-ranked, weighted-ensemble, and unweighted-ensemble variants). Forecast performance was assessed using mean absolute error (MAE), mean squared error (MSE), weighted interval score (WIS), and 95% prediction interval coverage. The n-sub-epidemic unweighted ensembles outperformed all other models at 3-4-week horizons, particularly at the national level and in the Midwest and West. ARIMA and GAM performed best at 1-2-week horizons in most regions, whereas Prophet and SLR consistently underperformed across regions and horizons. These findings highlight the value of region-specific modeling strategies and demonstrate the utility of the n-sub-epidemic framework for real-time outbreak forecasting using wastewater surveillance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01074v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faharudeen Alhassan, Hamed Karami, Amanda Bleichrodt, James M. Hyman, Isaac C. H. Fung, Ruiyan Luo, Gerardo Chowell</dc:creator>
    </item>
    <item>
      <title>Improved Disease Outbreak Detection from Out-of-sequence measurements Using Markov-switching Fixed-lag Particle Filters</title>
      <link>https://arxiv.org/abs/2512.01639</link>
      <description>arXiv:2512.01639v1 Announce Type: new 
Abstract: Particle filters (PFs) have become an essential tool for disease surveillance, as they can estimate hidden epidemic states in nonlinear and non-Gaussian models. In epidemic modelling, population dynamics may be governed by distinct regimes such as endemic or outbreak phases which can be represented using Markov-switching state-space models. In many real-world surveillance systems, data often arrives with delays or in the wrong temporal order, producing out-of-sequence (OOS) measurements that pertain to past time points rather than the current one. While existing PF methods can incorporate OOS measurements through particle reweighting, these approaches are limited in their ability to fully adjust past latent trajectories. To address this, we introduce a Markov-switching fixed-lag particle filter (FL-PF) that resimulates particle trajectories within a user-specified lag window, allowing OOS measurements to retroactively update both state and model estimates. By explicitly reevaluating historical samples, the FL-PF improves the accuracy and timeliness of outbreak detection and reduces false alarms. We also show how to compute the log-likelihood within the FL-PF framework, enabling parameter estimation using Sequential Monte Carlo squared (SMC$^2$). Together, these contributions extend the applicability of PFs to surveillance systems where retrospective data are common, offering a more robust framework for monitoring disease outbreaks and parameter inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01639v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Conor Rosato, Joshua Murphy, Si\^an E. Jenkins, Paul Horridge, Alessandro Varsi, Martyn Bull, Alessandro Gerada, Alex Howard, Veronica Bowman, Simon Maskell</dc:creator>
    </item>
    <item>
      <title>Bayesian Statistical Inversion for High-Dimensional Computer Model Output and Spatially Distributed Counts</title>
      <link>https://arxiv.org/abs/2512.01927</link>
      <description>arXiv:2512.01927v1 Announce Type: new 
Abstract: Data collected by the Interstellar Boundary Explorer (IBEX) satellite, recording heliospheric energetic neutral atoms (ENAs), exhibit a phenomenon that has caused space scientists to revise hypotheses about the physical processes, and computer simulations under those models, in play at the boundary of our solar system. Evaluating the fit of these computer models involves tuning their parameters to observational data from IBEX. This would be a classic (Bayesian) inverse problem if not for three challenges: (1) the computer simulations are slow, limiting the size of campaigns of runs; so (2) surrogate modeling is essential, but outputs are high-resolution images, thwarting conventional methods; and (3) IBEX observations are counts, whereas most inverse problem techniques assume Gaussian field data. To fill that gap we propose a novel approach to Bayesian inverse problems coupling a Poisson response with a sparse Gaussian process surrogate using the Vecchia approximation. We demonstrate the capabilities of our proposed framework, which compare favorably to alternatives, through multiple simulated examples in terms of recovering "true" computer model parameters and accurate out-of-sample prediction. We then apply this new technology to IBEX satellite data and associated computer models developed at Los Alamos National Laboratory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01927v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steven D. Barnett, Robert B. Gramacy, Lauren J. Beesley, Dave Osthus, Yifan Huang, Fan Guo, Daniel B. Reisenfeld</dc:creator>
    </item>
    <item>
      <title>Predicting Onsets and Dry Spells of the West African Monsoon Season Using Machine Learning Methods</title>
      <link>https://arxiv.org/abs/2512.01965</link>
      <description>arXiv:2512.01965v1 Announce Type: new 
Abstract: The beginning of the rainy season and the occurrence of dry spells in West Africa is notoriously difficult to predict, however these are the key indicators farmers use to decide when to plant crops, having a major influence on their overall yield. While many studies have shown correlations between global sea surface temperatures and characteristics of the West African monsoon season, there are few that effectively implementing this information into machine learning (ML) prediction models. In this study we investigated the best ways to define our target variables, onset and dry spell, and produced methods to predict them for upcoming seasons using sea surface temperature teleconnections. Defining our target variables required the use of a combination of two well known definitions of onset. We then applied custom statistical techniques -- like total variation regularization and predictor selection -- to the two models we constructed, the first being a linear model and the other an adaptive-threshold logistic regression model. We found mixed results for onset prediction, with spatial verification showing signs of significant skill, while temporal verification showed little to none. For dry spell though, we found significant accuracy through the analysis of multiple binary classification metrics. These models overcome some limitations that current approaches have, such as being computationally intensive and needing bias correction. We also introduce this study as a framework to use ML methods for targeted prediction of certain weather phenomenon using climatologically relevant variables. As we apply ML techniques to more problems, we see clear benefits for fields like meteorology and lay out a few new directions for further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01965v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Colin Bobocea, Yves Atchad\'e</dc:creator>
    </item>
    <item>
      <title>Predicting COVID-19 Prevalence Using Wastewater RNA Surveillance: A Semi-Supervised Learning Approach with Temporal Feature Trust</title>
      <link>https://arxiv.org/abs/2512.00100</link>
      <description>arXiv:2512.00100v1 Announce Type: cross 
Abstract: As COVID-19 transitions into an endemic disease that remains constantly present in the population at a stable level, monitoring its prevalence without invasive measures becomes increasingly important. In this paper, we present a deep neural network estimator for the COVID-19 daily case count based on wastewater surveillance data and other confounding factors. This work builds upon the study by Jiang, Kolozsvary, and Li (2024), which connects the COVID-19 case counts with testing data collected early in the pandemic. Using the COVID-19 testing data and the wastewater surveillance data during the period when both data were highly reliable, one can train an artificial neural network that learns the nonlinear relation between the COVID-19 daily case count and the wastewater viral RNA concentration. From a machine learning perspective, the main challenge lies in addressing temporal feature reliability, as the training data has different reliability over different time periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00100v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Chen, Eric Liang</dc:creator>
    </item>
    <item>
      <title>Network Curvature as a Structural Driver of Dengue Spread: A Bayesian Spatial Analysis of Recife (2015-2024)</title>
      <link>https://arxiv.org/abs/2512.00315</link>
      <description>arXiv:2512.00315v1 Announce Type: cross 
Abstract: We investigate whether the structural connectivity of urban road networks helps explain dengue incidence in Recife, Brazil (2015--2024). For each neighborhood, we compute the average \emph{communicability curvature}, a graph-theoretic measure capturing the ability of a locality to influence others through multiple network paths. We integrate this metric into Negative Binomial models, fixed-effects regressions, SAR/SAC spatial models, and a hierarchical INLA/BYM2 specification. Across all frameworks, curvature is the strongest and most stable predictor of dengue risk. In the BYM2 model, the structured spatial component collapses ($\phi \approx 0$), indicating that functional network connectivity explains nearly all spatial dependence typically attributed to adjacency-based CAR terms. The results show that dengue spread in Recife is driven less by geographic contiguity and more by network-mediated structural flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00315v1</guid>
      <category>physics.soc-ph</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc\'ilio Ferreira dos Santos, Cleiton de Lima Ricardo, Andreza dos Santos Rodrigues de Melo</dc:creator>
    </item>
    <item>
      <title>On Statistical Inference for High-Dimensional Binary Time Series</title>
      <link>https://arxiv.org/abs/2512.00338</link>
      <description>arXiv:2512.00338v1 Announce Type: cross 
Abstract: The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00338v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dehao Dai, Yunyi Zhang</dc:creator>
    </item>
    <item>
      <title>Explainable Machine Learning for Macroeconomic and Financial Nowcasting: A Decision-Grade Framework for Business and Policy</title>
      <link>https://arxiv.org/abs/2512.00399</link>
      <description>arXiv:2512.00399v1 Announce Type: cross 
Abstract: Macroeconomic nowcasting sits at the intersection of traditional econometrics, data-rich information systems, and AI applications in business, economics, and policy. Machine learning (ML) methods are increasingly used to nowcast quarterly GDP growth, but adoption in high-stakes settings requires that predictive accuracy be matched by interpretability and robust uncertainty quantification. This article reviews recent developments in macroeconomic nowcasting and compares econometric benchmarks with ML approaches in data-rich and shock-prone environments, emphasizing the use of nowcasts as decision inputs rather than as mere error-minimization exercises. The discussion is organized along three axes. First, we contrast penalized regressions, dimension-reduction techniques, tree ensembles, and neural networks with autoregressive models, Dynamic Factor Models, and Random Walks, emphasizing how each family handles small samples, collinearity, mixed frequencies, and regime shifts. Second, we examine explainability tools (intrinsic measures and model-agnostic XAI methods), focusing on temporal stability, sign coherence, and their ability to sustain credible economic narratives and nowcast revisions. Third, we analyze non-parametric uncertainty quantification via block bootstrapping for predictive intervals and confidence bands on feature importance under serial dependence and ragged edge. We translate these elements into a reference workflow for "decision-grade" nowcasting systems, including vintage management, time-aware validation, and automated reliability audits, and we outline a research agenda on regime-dependent model comparison, bootstrap design for latent components, and temporal stability of explanations. Explainable ML and uncertainty quantification emerge as structural components of a responsible forecasting pipeline, not optional refinements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00399v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Attolico</dc:creator>
    </item>
    <item>
      <title>Correlated Confounding Variables Are Not Easily Controlled for in Large Survey Research</title>
      <link>https://arxiv.org/abs/2512.01003</link>
      <description>arXiv:2512.01003v1 Announce Type: cross 
Abstract: Results in epidemiology and social science often require the removal of confounding effects from measurements of the pairwise correlation of variables in survey data. This is typically accomplished by some variant of linear regression (e.g., ``logistic" or ``Cox proportional"). But, knowing whether all possible confounders have been identified, or are even visible (not latent), is in general impossible. Here, we exhibit two examples that frame the issue. The first example proposes a highly unlikely hypothesis on drug use, draws data from a large, respected survey, and succeeds in ``proving" the implausible hypothesis, despite regressing out more than 20 confounding variables. The second constructs a ``metamodel" in which a single (by hypothesis unmeasurable) latent variable affects many mutually correlated confounders. From simulations, we derive formulas for the magnitude of spurious association that persists even as increasing numbers of confounders are regressed out. The intent of these examples is for them to serve as cautionary tales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01003v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William H. Press</dc:creator>
    </item>
    <item>
      <title>A mixture of distributed lag non-linear models to account for spatially heterogeneous exposure-lag-response associations</title>
      <link>https://arxiv.org/abs/2512.01508</link>
      <description>arXiv:2512.01508v1 Announce Type: cross 
Abstract: Environmental exposures, such as air pollution and extreme temperatures, have complex effects on human health. These effects are often characterized by non-linear exposure-lag-response relationships and delayed impacts over time. Accurately capturing these dynamics is crucial for informing public health interventions. The Distributed Lag Non-Linear Model (DLNM) is a flexible statistical framework for estimating such effects in epidemiological research. However, standard DLNM implementations typically assume a homogeneous exposure-lag-response association across the study region, overlooking potential spatial heterogeneity, which can lead to biased risk estimates. To address this limitation, we introduce DLNM-Clust: a novel mixture of DLNMs that extends the traditional DLNM. Within a Bayesian framework, DLNM-Clust probabilistically assigns each geographic unit to one of $C$ latent spatial clusters, each of which is defined by a distinct DLNM specification. This approach allows capturing both common patterns and singular deviations in the exposure-lag-response surface. We demonstrate the method using municipality-level time-series data on the relationship between air pollution and the incidence of COVID-19 in Belgium. Our results emphasize the importance of spatially aware modeling strategies in environmental epidemiology, facilitating region-specific risk assessment and supporting the development of targeted public health initiatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01508v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Alvaro Briz-Red\'on, Ana Corber\'an-Vallet, Adina Iftimi, Carmen \'I\~niguez</dc:creator>
    </item>
    <item>
      <title>Common Structure Discovery in Collections of Bipartite Networks: Application to Pollination Systems</title>
      <link>https://arxiv.org/abs/2512.01716</link>
      <description>arXiv:2512.01716v1 Announce Type: cross 
Abstract: Bipartite networks are widely used to encode the ecological interactions. Being able to compare the organization of bipartite networks is a first step toward a better understanding of how environmental factors shape community structure and resilience. Yet current methods for structure detection in bipartite networks overlook shared patterns across collections of networks. We introduce the \emph{colBiSBM}, a family of probabilistic models for collections of bipartite networks that extends the classical Latent Block Model (LBM). The proposed framework assumes that networks are independent realizations of a shared mesoscale structure, encoded through common inter-block connectivity parameters. We establish identifiability conditions for the different variants of \emph{colBiSBM} and develop a variational EM algorithm for parameter estimation, coupled with an adaptation of the Integrated Classification Likelihood (ICL) criterion for model selection. We demonstrate how our approach can be used to classify networks based on their topology or organization. Simulation studies highlight the ability of \emph{colBiSBM} to recover common structures, improve clustering performance, and enhance link prediction by borrowing strength across networks. An application to plant--pollinator networks highlights how the method uncovers shared ecological roles and partitions networks into sub-collections with similar connectivity patterns. These results illustrate the methodological and practical advantages of joint modeling over separate network analyses in the study of bipartite systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01716v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Louis Lacoste, Pierre Barbillon, Sophie Donnet</dc:creator>
    </item>
    <item>
      <title>Can the hot hand phenomenon be modelled? A Bayesian hidden Markov approach</title>
      <link>https://arxiv.org/abs/2303.17863</link>
      <description>arXiv:2303.17863v2 Announce Type: replace 
Abstract: Sports data analytics is a relevant topic in applied statistics that has been growing in importance in recent years. In basketball, a player or team has a hot hand when their performance during a match is better than expected or they are on a streak of making consecutive shots. This phenomenon has generated a great deal of controversy with detractors claiming its non-existence while other authors indicate its evidence. In this work, we present a Bayesian longitudinal hidden Markov model that analyses the hot hand phenomenon in consecutive basketball shots, each of which can be either missed or made. Two possible states (cold or hot) are assumed in the hidden Markov chains of events, and the probability of success for each throw is modelled by considering both the corresponding hidden state and the distance to the basket. This model is applied to a real data set, the Miami Heat team in the season 2005-2006 of the USA National Basketball Association. We show that this model is a powerful tool for assessing the overall performance of a team during a match or a season, and, in particular, for quantifying the magnitude of the team streaks in probabilistic terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.17863v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00180-024-01560-8</arxiv:DOI>
      <dc:creator>Gabriel Calvo, Carmen Armero, Luigi Spezia</dc:creator>
    </item>
    <item>
      <title>Evaluating Gender Wage Inequality in Academia using Causal Inference Methods for Observational Data</title>
      <link>https://arxiv.org/abs/2505.24078</link>
      <description>arXiv:2505.24078v2 Announce Type: replace 
Abstract: Observational studies often present challenges for causal inference due to confounding and heterogeneity. In this paper, we illustrate how modern causal inference methods can be applied to large-scale academic salary data. Using records from 12,039 tenure-track faculty in the University of North Carolina system, linked with bibliometric indicators and institutional classifications, we estimate the causal effect of gender on faculty salaries. Our analysis combines propensity score matching with causal forests to adjust for rank, discipline, research productivity, and career experience. Results indicate that female faculty earn approximately 6% less than comparable male colleagues, with variation in the gap across career stages and levels of research productivity. This case study demonstrates how causal inference methods for observational data can provide insight into structural disparities in complex social systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24078v2</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihan Zhang, Jan Hannig</dc:creator>
    </item>
    <item>
      <title>Design loads for wave impacts -- introducing the Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures</title>
      <link>https://arxiv.org/abs/2511.23156</link>
      <description>arXiv:2511.23156v2 Announce Type: replace 
Abstract: Wave impact loads on a maritime structure can cause casualties, damage, pollution of the sea and operational delays. Their extreme (or design) values should therefore be considered in the design of these structures. However, this is challenging because these events are both rare and complex, requiring high-fidelity computation and long analysis durations to obtain such design loads. Existing extreme value prediction methods are not tailored or validated for wave impacts. We therefore introduce the new Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures. The method introduces a probabilistic approach to multi-fidelity screening, allowing efficient linear potential flow indicators to be used in the low-fidelity stage, even for strongly non-linear load cases. The method is validated against a range of cases, including non-linear waves, ship vertical bending moments, green water impact loads, and slamming loads. It can be concluded that PAS accurately estimates both the short-term distributions and extreme values in these test cases, with most probable maximum (MPM) values within 10% of the available full brute-force Monte-Carlo Simulation (MCS) results. In addition, PAS achieves this performance very efficiently, requiring less than 4% of the high-fidelity simulation time needed for conventional MCS. These results demonstrate that PAS can reliably reproduce the statistics of both weakly and strongly non-linear extreme load problems, while significantly reducing the associated computational cost. The present study validates the statistical PAS framework; further work should focus on validating the full procedure including CFD load simulations, and on validating it for long-term extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23156v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanne M. van Essen, Harleigh C. Seyffert</dc:creator>
    </item>
    <item>
      <title>Can-SAVE: Deploying Low-Cost and Population-Scale Cancer Screening via Survival Analysis Variables and EHR</title>
      <link>https://arxiv.org/abs/2309.15039</link>
      <description>arXiv:2309.15039v3 Announce Type: replace-cross 
Abstract: Conventional medical cancer screening methods are costly, labor-intensive, and extremely difficult to scale. Although AI can improve cancer detection, most systems rely on complex or specialized medical data, making them impractical for large-scale screening. We introduce Can-SAVE, a lightweight AI system that ranks population-wide cancer risks solely based on medical history events. By integrating survival model outputs into a gradient-boosting framework, our approach detects subtle, long-term patient risk patterns - often well before clinical symptoms manifest. Can-SAVE was rigorously evaluated on a real-world dataset of 2.5 million adults spanning five Russian regions, marking the study as one of the largest and most comprehensive deployments of AI-driven cancer risk assessment. In a retrospective oncologist-supervised study over 1.9M patients, Can-SAVE achieves a 4-10x higher detection rate at identical screening volumes and an Average Precision (AP) of 0.228 vs. 0.193 for the best baseline (LoRA-tuned Qwen3-Embeddings via DeepSeek-R1 summarization). In a year-long prospective pilot (426K patients), our method almost doubled the cancer detection rate (+91%) and increased population coverage by 36% over the national screening protocol. The system demonstrates practical scalability: a city-wide population of 1 million patients can be processed in under three hours using standard hardware, enabling seamless clinical integration. This work proves that Can-SAVE achieves nationally significant cancer detection improvements while adhering to real-world public healthcare constraints, offering immediate clinical utility and a replicable framework for population-wide screening. Code for training and feature engineering is available at https://github.com/sb-ai-lab/Can-SAVE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15039v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Philonenko, Vladimir Kokh, Pavel Blinov</dc:creator>
    </item>
    <item>
      <title>Interpret the estimand framework from a causal inference perspective</title>
      <link>https://arxiv.org/abs/2407.00292</link>
      <description>arXiv:2407.00292v2 Announce Type: replace-cross 
Abstract: The estimand framework proposed by ICH in 2017 has brought fundamental changes in the pharmaceutical industry. It clearly describes how a treatment effect in a clinical question should be precisely defined and estimated, through attributes including treatments, endpoints and intercurrent events. However, ideas around the estimand framework are commonly in text, and different interpretations on this framework may exist. This article aims to interpret the estimand framework through its underlying theories, the causal inference framework based on potential outcomes. The statistical origin and formula of an estimand is given through the causal inference framework, with all attributes translated into statistical terms. We describe how five strategies proposed by ICH to analyze intercurrent events are incorporated in the statistical formula of an estimand, and we also suggest a new strategy to analyze intercurrent events. The roles of target populations and analysis sets in the estimand framework are compared and discussed based on the statistical formula of an estimand. This article recommends continuing studying causal inference theories behind the estimand framework and improving the estimand framework with greater methodological comprehensibility and availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00292v2</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinghong Zeng</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Polychoric Correlation</title>
      <link>https://arxiv.org/abs/2407.18835</link>
      <description>arXiv:2407.18835v5 Announce Type: replace-cross 
Abstract: Polychoric correlation is often an important building block in the analysis of rating data, particularly for structural equation models. However, the commonly employed maximum likelihood (ML) estimator is highly susceptible to misspecification of the polychoric correlation model, for instance through violations of latent normality assumptions. We propose a novel estimator that is designed to be robust against partial misspecification of the polychoric model, that is, when the model is misspecified for an unknown fraction of observations, such as careless respondents. To this end, the estimator minimizes a robust loss function based on the divergence between observed frequencies and theoretical frequencies implied by the polychoric model. In contrast to existing literature, our estimator makes no assumption on the type or degree of model misspecification. It furthermore generalizes ML estimation, is consistent as well as asymptotically normally distributed, and comes at no additional computational cost. We demonstrate the robustness and practical usefulness of our estimator in simulation studies and an empirical application on a Big Five administration. In the latter, the polychoric correlation estimates of our estimator and ML differ substantially, which, after further inspection, is likely due to the presence of careless respondents that the estimator helps identify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18835v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1017/psy.2025.10066</arxiv:DOI>
      <arxiv:journal_reference>Psychometrika (2025+), forthcoming</arxiv:journal_reference>
      <dc:creator>Max Welz, Patrick Mair, Andreas Alfons</dc:creator>
    </item>
    <item>
      <title>Using a Two-Parameter Sensitivity Analysis Framework to Efficiently Combine Randomized and Non-randomized Studies</title>
      <link>https://arxiv.org/abs/2412.03731</link>
      <description>arXiv:2412.03731v2 Announce Type: replace-cross 
Abstract: Causal inference is vital for informed decision-making across fields such as biomedical research and social sciences. Randomized controlled trials (RCTs) are considered the gold standard for internal validity of inferences, whereas observational studies (OSs) often provide the opportunity for greater external validity. However, both data sources have inherent limitations preventing their use for broadly valid statistical inferences: RCTs may lack generalizability due to their selective eligibility criterion, and OSs are vulnerable to unobserved confounding. This paper proposes an innovative approach to integrate RCT and OS that borrows the other study's strengths to remedy each study's limitations. The method uses a novel triplet matching algorithm to align RCT and OS samples and a new two-parameter sensitivity analysis framework to quantify internal and external validity biases. This combined approach yields causal estimates that are more robust to hidden biases than OSs alone and provides reliable inferences about the treatment effect in the general population. We apply this method to investigate the effects of lactation on maternal health using a small RCT and a long-term observational health records dataset from the California National Primate Research Center. This application demonstrates the practical utility of our approach in generating scientifically sound and actionable causal estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03731v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoqi Yu, Bikram Karmakar, Jessica Vandeleest, Eleanor Bimla Schwarz</dc:creator>
    </item>
    <item>
      <title>Bayesian Synthetic Control with a Soft Simplex Constraint</title>
      <link>https://arxiv.org/abs/2503.06454</link>
      <description>arXiv:2503.06454v2 Announce Type: replace-cross 
Abstract: The challenges posed by high-dimensional data and use of the simplex constraint are two major concerns in the empirical application of the synthetic control method (SCM) in econometric studies. To address both issues simultaneously, we propose a Bayesian SCM that integrates a soft simplex constraint within spike-and-slab variable selection. The hierarchical prior structure captures the extent to which the data supports the simplex constraint, allowing for more efficient and data-adaptive counterfactual estimation. The intractable marginal likelihood induced by the soft simplex constraint presents a major computational challenge, which we resolve by developing a novel Metropolis-within-Gibbs algorithm that updates the regression coefficients of two predictors simultaneously. Our main theoretical contribution is a high-dimensional selection consistency result for the spike-and-slab variable selection under the simplex constraint, which significantly extends the current theory for high-dimensional Bayesian variable selection. Simulation studies demonstrate that our method performs well across diverse settings. To illustrate its practical values, we apply it to two empirical examples for estimating the effect of economic policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06454v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Xu, Quan Zhou</dc:creator>
    </item>
    <item>
      <title>AI-Assisted Conversational Interviewing: Effects on Data Quality and Respondent Experience</title>
      <link>https://arxiv.org/abs/2504.13908</link>
      <description>arXiv:2504.13908v3 Announce Type: replace-cross 
Abstract: Standardized surveys scale efficiently but sacrifice depth, while conversational interviews improve response quality at the cost of scalability and consistency. This study bridges the gap between these methods by introducing a framework for AI-assisted conversational interviewing. To evaluate this framework, we conducted a web survey experiment where 1,800 participants were randomly assigned to AI 'chatbots' which use large language models (LLMs) to dynamically probe respondents for elaboration and interactively code open-ended responses to fixed questions developed by human researchers. We assessed the AI chatbot's performance in terms of coding accuracy, response quality, and respondent experience. Our findings reveal that AI chatbots perform moderately well in live coding even without survey-specific fine-tuning, despite slightly inflated false positive errors due to respondent acquiescence bias. Open-ended responses were more detailed and informative, but this came at a slight cost to respondent experience. Our findings highlight the feasibility of using AI methods such as chatbots enhanced by LLMs to enhance open-ended data collection in web surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13908v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soubhik Barari, Jarret Angbazo, Natalie Wang, Leah M. Christian, Elizabeth Dean, Zoe Slowinski, Brandon Sepulvado</dc:creator>
    </item>
    <item>
      <title>Bridging Prediction and Intervention Problems in Social Systems</title>
      <link>https://arxiv.org/abs/2507.05216</link>
      <description>arXiv:2507.05216v2 Announce Type: replace-cross 
Abstract: Many automated decision systems (ADS) are designed to solve prediction problems -- where the goal is to learn patterns from a sample of the population and apply them to individuals from the same population. In reality, these prediction systems operationalize holistic policy interventions in deployment. Once deployed, ADS can shape impacted population outcomes through an effective policy change in how decision-makers operate, while also being defined by past and present interactions between stakeholders and the limitations of existing organizational, as well as societal, infrastructure and context. In this work, we consider the ways in which we must shift from a prediction-focused paradigm to an interventionist paradigm when considering the impact of ADS within social systems. We argue this requires a new default problem setup for ADS beyond prediction, to instead consider predictions as decision support, final decisions, and outcomes. We highlight how this perspective unifies modern statistical frameworks and other tools to study the design, implementation, and evaluation of ADS systems, and point to the research directions necessary to operationalize this paradigm shift. Using these tools, we characterize the limitations of focusing on isolated prediction tasks, and lay the foundation for a more intervention-oriented approach to developing and deploying ADS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05216v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lydia T. Liu, Inioluwa Deborah Raji, Angela Zhou, Luke Guerdan, Jessica Hullman, Daniel Malinsky, Bryan Wilder, Simone Zhang, Hammaad Adam, Amanda Coston, Ben Laufer, Ezinne Nwankwo, Michael Zanger-Tishler, Eli Ben-Michael, Solon Barocas, Avi Feller, Marissa Gerchick, Talia Gillis, Shion Guha, Daniel Ho, Lily Hu, Kosuke Imai, Sayash Kapoor, Joshua Loftus, Razieh Nabi, Arvind Narayanan, Ben Recht, Juan Carlos Perdomo, Matthew Salganik, Mark Sendak, Alexander Tolbert, Berk Ustun, Suresh Venkatasubramanian, Angelina Wang, Ashia Wilson</dc:creator>
    </item>
    <item>
      <title>Beyond Linearity and Time-Homogeneity: Relational Hyper Event Models with Time-Varying Non-Linear Effects</title>
      <link>https://arxiv.org/abs/2509.05289</link>
      <description>arXiv:2509.05289v3 Announce Type: replace-cross 
Abstract: Recent technological advances have made it easier to collect large and complex networks of time-stamped relational events connecting two or more entities. Relational hyper-event models (RHEMs) aim to explain the dynamics of these events by modeling the event rate as a function of statistics based on past history and external information.
  However, despite the complexity of the data, most current RHEM approaches still rely on a linearity assumption to model this relationship. In this work, we address this limitation by introducing a more flexible model that allows the effects of statistics to vary non-linearly and over time. While time-varying and non-linear effects have been used in relational event modeling, we take this further by modeling joint time-varying and non-linear effects using tensor product smooths.
  We validate our methodology on both synthetic and empirical data. In particular, we use RHEMs to study how patterns of scientific collaboration and impact evolve over time. Our approach provides deeper insights into the dynamic factors driving relational hyper-events, allowing us to evaluate potential non-monotonic patterns that cannot be identified using linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05289v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Boschi, J\"urgen Lerner, Ernst C. Wit</dc:creator>
    </item>
  </channel>
</rss>
