<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 02:18:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adaptive Anomaly Detection in the Presence of Concept Drift: Extended Report</title>
      <link>https://arxiv.org/abs/2506.15831</link>
      <description>arXiv:2506.15831v1 Announce Type: new 
Abstract: Data changes to reflect evolving user behaviour, preferences, and changes in the environment. Such changes may occur due to expected shifts in the data distribution, i.e., concept drift, or unexpected anomalous changes. The presence of concept drift poses challenges for anomaly detection in time series. While anomalies are caused by undesirable changes in the data, differentiating abnormal changes from varying normal behaviours is difficult due to differing frequencies of occurrence, varying time intervals when normal patterns occur. Differentiating between concept drift and anomalies is critical for accurate analysis as studies have shown that the compounding effects of error propagation in downstream data analysis tasks lead to lower detection accuracy and increased overhead due to unnecessary model updates. Unfortunately, existing work has largely explored anomaly detection and concept drift detection in isolation. We develop AnDri, a system for Anomaly detection in the presence of Drift, which adjusts the normal patterns temporally, and distinguish abnormal subsequences and new concepts. Moreover, it introduces a new clustering method, Adjacent Hierarchical Clustering (AHC), which groups similar subsequences while respecting their temporal locality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15831v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jongjun Park, Fei Chiang, Mostafa Milani</dc:creator>
    </item>
    <item>
      <title>Delta: A Learned Mixed Cost-based Query Optimization Framework</title>
      <link>https://arxiv.org/abs/2506.15848</link>
      <description>arXiv:2506.15848v1 Announce Type: new 
Abstract: Query optimizer is a crucial module for database management systems. Existing optimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic programming with cost models but face search space explosion and heuristic pruning constraints; (2) value-based ones train value networks to enable efficient beam search, but incur higher training costs and lower accuracy. They also lack mechanisms to detect queries where they may perform poorly. To determine more efficient plans, we propose Delta, a mixed cost-based query optimization framework that consists of a compatible query detector and a two-stage planner. Delta first employs a Mahalanobis distancebased detector to preemptively filter out incompatible queries where the planner might perform poorly. For compatible queries, Delta activates its two-stage mixed cost-based planner. Stage I serves as a coarse-grained filter to generate high-quality candidate plans based on the value network via beam search, relaxing precision requirements and narrowing the search space. Stage II employs a fine-grained ranker to determine the best plan from the candidate plans based on a learned cost model. Moreover, to reduce training costs, we reuse and augment the training data from stage I to train the model in stage II. Experimental results on three workloads demonstrate that Delta identifies higher-quality plans, achieving an average 2.34x speedup over PostgreSQL and outperforming the state-of-the-art learned methods by 2.21x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15848v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiazhen Peng, Zheng Qu, Xiaoye Miao, Rong Zhu</dc:creator>
    </item>
    <item>
      <title>Empowering Graph-based Approximate Nearest Neighbor Search with Adaptive Awareness Capabilities</title>
      <link>https://arxiv.org/abs/2506.15986</link>
      <description>arXiv:2506.15986v1 Announce Type: new 
Abstract: Approximate Nearest Neighbor Search (ANNS) in high-dimensional spaces finds extensive applications in databases, information retrieval, recommender systems, etc. While graph-based methods have emerged as the leading solution for ANNS due to their superior query performance, they still face several challenges, such as struggling with local optima and redundant computations. These issues arise because existing methods (i) fail to fully exploit the topological information underlying the proximity graph G, and (ii) suffer from severe distribution mismatches between the base data and queries in practice.
  To this end, this paper proposes GATE, high-tier proximity Graph with Adaptive Topology and Query AwarEness, as a lightweight and adaptive module atop the graph-based indexes to accelerate ANNS. Specifically, GATE formulates the critical problem to identify an optimal entry point in the proximity graph for a given query, facilitating faster online search. By leveraging the inherent clusterability of high-dimensional data, GATE first extracts a small set of hub nodes V as candidate entry points. Then, resorting to a contrastive learning-based two-tower model, GATE encodes both the structural semantics underlying G and the query-relevant features into the latent representations of these hub nodes V. A navigation graph index on V is further constructed to minimize the model inference overhead. Extensive experiments demonstrate that GATE achieves a 1.2-2.0X speed-up in query performance compared to state-of-the-art graph-based indexes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15986v1</guid>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiancheng Ruan, Tingyang Chen, Renchi Yang, Xiangyu Ke, Yunjun Gao</dc:creator>
    </item>
    <item>
      <title>Filter-Centric Vector Indexing: Geometric Transformation for Efficient Filtered Vector Search</title>
      <link>https://arxiv.org/abs/2506.15987</link>
      <description>arXiv:2506.15987v1 Announce Type: new 
Abstract: The explosive growth of vector search applications demands efficient handling of combined vector similarity and attribute filtering; a challenge where current approaches force an unsatisfying choice between performance and accuracy. We introduce Filter-Centric Vector Indexing (FCVI), a novel framework that transforms this fundamental trade-off by directly encoding filter conditions into the vector space through a mathematically principled transformation $\psi(v, f, \alpha)$. Unlike specialized solutions, FCVI works with any existing vector index (HNSW, FAISS, ANNOY) while providing theoretical guarantees on accuracy. Our comprehensive evaluation demonstrates that FCVI achieves 2.6-3.0 times higher throughput than state-of-the-art methods while maintaining comparable recall. More remarkably, FCVI exhibits exceptional stability under distribution shifts; maintaining consistent performance when filter patterns or vector distributions change, unlike traditional approaches that degrade significantly. This combination of performance, compatibility, and resilience positions FCVI as an immediately applicable solution for production vector search systems requiring flexible filtering capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15987v1</guid>
      <category>cs.DB</category>
      <category>math.MG</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3735403.3735996</arxiv:DOI>
      <dc:creator>Alireza Heidari, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Data-Agnostic Cardinality Learning from Imperfect Workloads</title>
      <link>https://arxiv.org/abs/2506.16007</link>
      <description>arXiv:2506.16007v1 Announce Type: new 
Abstract: Cardinality estimation (CardEst) is a critical aspect of query optimization. Traditionally, it leverages statistics built directly over the data. However, organizational policies (e.g., regulatory compliance) may restrict global data access. Fortunately, query-driven cardinality estimation can learn CardEst models using query workloads. However, existing query-driven models often require access to data or summaries for best performance, and they assume perfect training workloads with complete and balanced join templates (or join graphs). Such assumptions rarely hold in real-world scenarios, in which join templates are incomplete and imbalanced. We present GRASP, a data-agnostic cardinality learning system designed to work under these real-world constraints. GRASP's compositional design generalizes to unseen join templates and is robust to join template imbalance. It also introduces a new per-table CardEst model that handles value distribution shifts for range predicates, and a novel learned count sketch model that captures join correlations across base relations. Across three database instances, we demonstrate that GRASP consistently outperforms existing query-driven models on imperfect workloads, both in terms of estimation accuracy and query latency. Remarkably, GRASP achieves performance comparable to, or even surpassing, traditional approaches built over the underlying data on the complex CEB-IMDb-full benchmark -- despite operating without any data access and using only 10% of all possible join templates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16007v1</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.14778/3742728.3742745</arxiv:DOI>
      <dc:creator>Peizhi Wu, Rong Kang, Tieying Zhang, Jianjun Chen, Ryan Marcus, Zachary G. Ives</dc:creator>
    </item>
    <item>
      <title>PBench: Workload Synthesizer with Real Statistics for Cloud Analytics Benchmarking</title>
      <link>https://arxiv.org/abs/2506.16379</link>
      <description>arXiv:2506.16379v1 Announce Type: new 
Abstract: Cloud service providers commonly use standard benchmarks like TPC-H and TPC-DS to evaluate and optimize cloud data analytics systems. However, these benchmarks rely on fixed query patterns and fail to capture the real execution statistics of production cloud workloads. Although some cloud database vendors have recently released real workload traces, these traces alone do not qualify as benchmarks, as they typically lack essential components like the original SQL queries and their underlying databases. To overcome this limitation, this paper introduces a new problem of workload synthesis with real statistics, which aims to generate synthetic workloads that closely approximate real execution statistics, including key performance metrics and operator distributions, in real cloud workloads. To address this problem, we propose PBench, a novel workload synthesizer that constructs synthetic workloads by judiciously selecting and combining workload components (i.e., queries and databases) from existing benchmarks. This paper studies the key challenges in PBench. First, we address the challenge of balancing performance metrics and operator distributions by introducing a multi-objective optimization-based component selection method. Second, to capture the temporal dynamics of real workloads, we design a timestamp assignment method that progressively refines workload timestamps. Third, to handle the disparity between the original workload and the candidate workload, we propose a component augmentation approach that leverages large language models (LLMs) to generate additional workload components while maintaining statistical fidelity. We evaluate PBench on real cloud workload traces, demonstrating that it reduces approximation error by up to 6x compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16379v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Zhou, Chunwei Liu, Bhuvan Urgaonkar, Zhengle Wang, Magnus Mueller, Chao Zhang, Songyue Zhang, Pascal Pfeil, Dominik Horn, Zhengchun Liu, Davide Pagano, Tim Kraska, Samuel Madden, Ju Fan</dc:creator>
    </item>
    <item>
      <title>LDI: Localized Data Imputation</title>
      <link>https://arxiv.org/abs/2506.16616</link>
      <description>arXiv:2506.16616v1 Announce Type: new 
Abstract: Missing values are a common challenge in real-world tabular data and can significantly impair downstream analysis. While Large Language Models (LLMs) have recently shown promise in data imputation, existing methods often rely on broad, unfiltered prompts that compromise accuracy, scalability, and explainability. We introduce LDI (Localized Data Imputation), a novel framework that improves both the accuracy and transparency of LLM-based imputation by selecting a compact, contextually relevant subset of attributes and tuples for each missing value. This localized prompting reduces noise, enables traceability by revealing which data influenced each prediction, and is effective across both hosted LLMs and lightweight local models. Our extensive experiments on four real-world datasets show that LDI outperforms state-of-the-art methods, achieving up to 8% higher accuracy when using hosted LLMs. The gains are more substantial with lightweight local models, reaching nearly 17% and 97% accuracy on some datasets when using 3 and 10 examples, respectively. In addition to higher accuracy, LDI offers improved interpretability and robustness to data inconsistencies, making it well-suited for high-stakes and privacy-sensitive applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16616v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soroush Omidvartehrani, Davood Rafiei</dc:creator>
    </item>
    <item>
      <title>Advancing Fact Attribution for Query Answering: Aggregate Queries and Novel Algorithms</title>
      <link>https://arxiv.org/abs/2506.16923</link>
      <description>arXiv:2506.16923v1 Announce Type: new 
Abstract: In this paper, we introduce a novel approach to computing the contribution of input tuples to the result of the query, quantified by the Banzhaf and Shapley values. In contrast to prior algorithmic work that focuses on Select-Project-Join-Union queries, ours is the first practical approach for queries with aggregates. It relies on two novel optimizations that are essential for its practicality and significantly improve the runtime performance already for queries without aggregates. The first optimization exploits the observation that many input tuples have the same contribution to the query result, so it is enough to compute the contribution of one of them. The second optimization uses the gradient of the query lineage to compute the contributions of all tuples with the same complexity as for one of them. Experiments with a million instances over 3 databases show that our approach achieves up to 3 orders of magnitude runtime improvements over the state-of-the-art for queries without aggregates, and that it is practical for aggregate queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16923v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Omer Abramovich, Daniel Deutch, Nave Frost, Ahmet Kara, Dan Olteanu</dc:creator>
    </item>
    <item>
      <title>PUL: Pre-load in Software for Caches Wouldn't Always Play Along</title>
      <link>https://arxiv.org/abs/2506.16976</link>
      <description>arXiv:2506.16976v1 Announce Type: new 
Abstract: Memory latencies and bandwidth are major factors, limiting system performance and scalability. Modern CPUs aim at hiding latencies by employing large caches, out-of-order execution, or complex hardware prefetchers. However, software-based prefetching exhibits higher efficiency, improving with newer CPU generations.
  In this paper we investigate software-based, post-Moore systems that offload operations to intelligent memories. We show that software-based prefetching has even higher potential in near-data processing settings by maximizing compute utilization through compute/IO interleaving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16976v1</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arthur Bernhardt, Sajjad Tamimi, Florian Stock, Andreas Koch, Ilia Petrov</dc:creator>
    </item>
    <item>
      <title>Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning</title>
      <link>https://arxiv.org/abs/2506.16015</link>
      <description>arXiv:2506.16015v1 Announce Type: cross 
Abstract: The exponential expansion of scientific literature has surpassed the epistemic processing capabilities of both human experts and current artificial intelligence systems. This paper introduces Bayesian Epistemology with Weighted Authority (BEWA), a formally structured architecture that operationalises belief as a dynamic, probabilistically coherent function over structured scientific claims. Each claim is contextualised, author-attributed, and evaluated through a system of replication scores, citation weighting, and temporal decay. Belief updates are performed via evidence-conditioned Bayesian inference, contradiction processing, and epistemic decay mechanisms. The architecture supports graph-based claim propagation, authorial credibility modelling, cryptographic anchoring, and zero-knowledge audit verification. By formalising scientific reasoning into a computationally verifiable epistemic network, BEWA advances the foundation for machine reasoning systems that promote truth utility, rational belief convergence, and audit-resilient integrity across dynamic scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16015v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Craig S. Wright</dc:creator>
    </item>
    <item>
      <title>From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience</title>
      <link>https://arxiv.org/abs/2506.16051</link>
      <description>arXiv:2506.16051v1 Announce Type: cross 
Abstract: Reproducibility remains a central challenge in machine learning (ML), especially in collaborative eScience projects where teams iterate over data, features, and models. Current ML workflows are often dynamic yet fragmented, relying on informal data sharing, ad hoc scripts, and loosely connected tools. This fragmentation impedes transparency, reproducibility, and the adaptability of experiments over time. This paper introduces a data-centric framework for lifecycle-aware reproducibility, centered around six structured artifacts: Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These artifacts formalize the relationships between data, code, and decisions, enabling ML experiments to be versioned, interpretable, and traceable over time. The approach is demonstrated through a clinical ML use case of glaucoma detection, illustrating how the system supports iterative exploration, improves reproducibility, and preserves the provenance of collaborative decisions across the ML lifecycle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16051v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <category>cs.DL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiwei Li, Carl Kesselman, Tran Huy Nguyen, Benjamin Yixing Xu, Kyle Bolo, Kimberley Yu</dc:creator>
    </item>
    <item>
      <title>REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing</title>
      <link>https://arxiv.org/abs/2506.16444</link>
      <description>arXiv:2506.16444v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is confined to the data that they have been trained on. To overcome this issue, Retrieval-Augmented Generation (RAG) complements the static training-derived knowledge of LLMs with an external knowledge repository. RAG consists of three stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes a significant bottleneck in inference pipelines. In this stage, a user query is mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS) algorithm searches for similar vectors in the database to identify relevant items. Due to the large database sizes, ANNS incurs significant data movement overheads between the host and the storage system. To alleviate these overheads, prior works propose In-Storage Processing (ISP) techniques that accelerate ANNS by performing computations inside storage. However, existing works that leverage ISP for ANNS (i) employ algorithms that are not tailored to ISP systems, (ii) do not accelerate data retrieval operations for data selected by ANNS, and (iii) introduce significant hardware modifications, limiting performance and hindering their adoption. We propose REIS, the first ISP system tailored for RAG that addresses these limitations with three key mechanisms. First, REIS employs a database layout that links database embedding vectors to their associated documents, enabling efficient retrieval. Second, it enables efficient ANNS by introducing an ISP-tailored data placement technique that distributes embeddings across the planes of the storage system and employs a lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that uses the existing computational resources inside the storage system. Compared to a server-grade system, REIS improves the performance (energy efficiency) of retrieval by an average of 13x (55x).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16444v1</guid>
      <category>cs.CL</category>
      <category>cs.AR</category>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kangqi Chen, Andreas Kosmas Kakolyris, Rakesh Nadig, Manos Frouzakis, Nika Mansouri Ghiasi, Yu Liang, Haiyu Mao, Jisung Park, Mohammad Sadrosadati, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures</title>
      <link>https://arxiv.org/abs/2506.16654</link>
      <description>arXiv:2506.16654v1 Announce Type: cross 
Abstract: Graph machine learning has led to a significant increase in the capabilities of models that learn on arbitrary graph-structured data and has been applied to molecules, social networks, recommendation systems, and transportation, among other domains. Data in multi-tabular relational databases can also be constructed as 'relational entity graphs' for Relational Deep Learning (RDL) - a new blueprint that enables end-to-end representation learning without traditional feature engineering. Compared to arbitrary graph-structured data, relational entity graphs have key properties: (i) their structure is defined by primary-foreign key relationships between entities in different tables, (ii) the structural connectivity is a function of the relational schema defining a database, and (iii) the graph connectivity is temporal and heterogeneous in nature. In this paper, we provide a comprehensive review of RDL by first introducing the representation of relational databases as relational entity graphs, and then reviewing public benchmark datasets that have been used to develop and evaluate recent GNN-based RDL models. We discuss key challenges including large-scale multi-table integration and the complexities of modeling temporal dynamics and heterogeneous data, while also surveying foundational neural network methods and recent architectural advances specialized for relational entity graphs. Finally, we explore opportunities to unify these distinct modeling challenges, highlighting how RDL converges multiple sub-fields in graph machine learning towards the design of foundation models that can transform the processing of relational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16654v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vijay Prakash Dwivedi, Charilaos Kanatsoulis, Shenyang Huang, Jure Leskovec</dc:creator>
    </item>
    <item>
      <title>Selective Use of Yannakakis' Algorithm to Improve Query Performance: Machine Learning to the Rescue</title>
      <link>https://arxiv.org/abs/2502.20233</link>
      <description>arXiv:2502.20233v2 Announce Type: replace 
Abstract: Query optimization has played a central role in database research for decades. However, more often than not, the proposed optimization techniques lead to a performance improvement in some, but not in all, situations. Therefore, we urgently need a methodology for designing a decision procedure that decides for a given query whether the optimization technique should be applied or not.
  In this work, we propose such a methodology with a focus on Yannakakis-style query evaluation as our optimization technique of interest. More specifically, we formulate this decision problem as an algorithm selection problem and we present a Machine Learning based approach for its solution. Empirical results with several benchmarks on a variety of database systems show that our approach indeed leads to a statistically significant performance improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20233v2</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniela B\"ohm, Georg Gottlob, Matthias Lanzinger, Davide Longo, Cem Okulmus, Reinhard Pichler, Alexander Selzer</dc:creator>
    </item>
    <item>
      <title>AQETuner: Reliable Query-level Configuration Tuning for Analytical Query Engines</title>
      <link>https://arxiv.org/abs/2504.11756</link>
      <description>arXiv:2504.11756v2 Announce Type: replace 
Abstract: Modern analytical query engines (AQEs) are essential for large-scale data analysis and processing. These systems usually provide numerous query-level tunable knobs that significantly affect individual query performance. While several studies have explored automatic DBMS configuration tuning, they have several limitations to handle query-level tuning. Firstly, they fail to capture how knobs influence query plans, which directly affect query performance. Secondly, they overlook query failures during the tuning processing, resulting in low tuning efficiency. Thirdly, they struggle with cold-start problems for new queries, leading to prolonged tuning time. To address these challenges, we propose AQETuner, a novel Bayesian Optimization-based system tailored for reliable query-level knob tuning in AQEs. AQETuner first applies the attention mechanisms to jointly encode the knobs and plan query, effectively identifying the impact of knobs on plan nodes. Then, AQETuner employs a dual-task Neural Process to predict both query performance and failures, leveraging their interactions to guide the tuning process. Furthermore, AQETuner utilizes Particle Swarm Optimization to efficiently generate high-quality samples in parallel during the initial tuning stage for the new queries. Experimental results show that AQETuner significantly outperforms existing methods, reducing query latency by up to 23.7% and query failures by up to 51.2%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11756v2</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lixiang Chen, Yuxing Han, Yu Chen, Xing Chen, Chengcheng Yang, Weining Qian</dc:creator>
    </item>
    <item>
      <title>ProvSQL: A General System for Keeping Track of the Provenance and Probability of Data</title>
      <link>https://arxiv.org/abs/2504.12058</link>
      <description>arXiv:2504.12058v2 Announce Type: replace 
Abstract: We present the data model, design choices, and performance of ProvSQL, a general and easy-to-deploy provenance tracking and probabilistic database system implemented as a PostgreSQL extension. ProvSQL's data and query models closely reflect that of a large core of SQL, including multiset semantics, the full relational algebra, and aggregation. A key part of its implementation relies on generic provenance circuits stored in memory-mapped files. We propose benchmarks to measure the overhead of provenance and probabilistic evaluation and demonstrate its scalability and competitiveness with respect to other state-of-the-art systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12058v2</guid>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aryak Sen, Silviu Maniu, Pierre Senellart</dc:creator>
    </item>
    <item>
      <title>SimBank: from Simulation to Solution in Prescriptive Process Monitoring</title>
      <link>https://arxiv.org/abs/2506.14772</link>
      <description>arXiv:2506.14772v2 Announce Type: replace 
Abstract: Prescriptive Process Monitoring (PresPM) is an emerging area within Process Mining, focused on optimizing processes through real-time interventions for effective decision-making. PresPM holds significant promise for organizations seeking enhanced operational performance. However, the current literature faces two key limitations: a lack of extensive comparisons between techniques and insufficient evaluation approaches. To address these gaps, we introduce SimBank: a simulator designed for accurate benchmarking of PresPM methods. Modeled after a bank's loan application process, SimBank enables extensive comparisons of both online and offline PresPM methods. It incorporates a variety of intervention optimization problems with differing levels of complexity and supports experiments on key causal machine learning challenges, such as assessing a method's robustness to confounding in data. SimBank additionally offers a comprehensive evaluation capability: for each test case, it can generate the true outcome under each intervention action, which is not possible using recorded datasets. The simulator incorporates parallel activities and loops, drawing from common logs to generate cases that closely resemble real-life process instances. Our proof of concept demonstrates SimBank's benchmarking capabilities through experiments with various PresPM methods across different interventions, highlighting its value as a publicly available simulator for advancing research and practice in PresPM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14772v2</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob De Moor, Hans Weytjens, Johannes De Smedt, Jochen De Weerdt</dc:creator>
    </item>
    <item>
      <title>Influential Slot and Tag Selection in Billboard Advertisement</title>
      <link>https://arxiv.org/abs/2401.10601</link>
      <description>arXiv:2401.10601v2 Announce Type: replace-cross 
Abstract: The selection of influential billboard slots remains an important problem in billboard advertisements. Existing studies on this problem have not considered the case of context-specific influence probability. To bridge this gap, in this paper, we introduce the Context Dependent Influential Billboard Slot Selection Problem. First, we show that the problem is NP-hard. We also show that the influence function holds the bi-monotonicity, bi-submodularity, and non-negativity properties. We propose an orthant-wise Stochastic Greedy approach to solve this problem. We show that this method leads to a constant-factor approximation guarantee. Subsequently, we propose an orthant-wise Incremental and Lazy Greedy approach. In a generic sense, this is a method for maximizing a bi-submodular function under the cardinality constraint, which may also be of independent interest. We analyze the performance guarantee of this algorithm as well as time and space complexity. The proposed solution approaches have been implemented with real-world billboard and trajectory datasets. We compare the performance of our method with several baseline methods, and the results are reported. Our proposed orthant-wise stochastic greedy approach leads to significant results when the parameters are set properly with reasonable computational overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10601v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dildar Ali, Suman Banerjee, Yamuna Prasad</dc:creator>
    </item>
    <item>
      <title>Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL Generation</title>
      <link>https://arxiv.org/abs/2502.12911</link>
      <description>arXiv:2502.12911v2 Announce Type: replace-cross 
Abstract: Generating SQLs from user queries is a long-standing challenge, where the accuracy of initial schema linking significantly impacts subsequent SQL generation performance. However, current schema linking models still struggle with missing relevant schema elements or an excess of redundant ones. A crucial reason for this is that commonly used metrics, recall and precision, fail to capture relevant element missing and thus cannot reflect actual schema linking performance. Motivated by this, we propose enhanced schema linking metrics by introducing a restricted missing indicator. Accordingly, we introduce Knapsack optimization-based Schema Linking Approach (KaSLA), a plug-in schema linking method designed to prevent the missing of relevant schema elements while minimizing the inclusion of redundant ones. KaSLA employs a hierarchical linking strategy that first identifies the optimal table linking and subsequently links columns within the selected table to reduce linking candidate space. In each linking process, it utilizes a knapsack optimization approach to link potentially relevant elements while accounting for a limited tolerance of potentially redundant ones. With this optimization, KaSLA-1.6B achieves superior schema linking results compared to large-scale LLMs, including deepseek-v3 with the state-of-the-art (SOTA) schema linking method. Extensive experiments on Spider and BIRD benchmarks verify that KaSLA can significantly improve the SQL generation performance of SOTA Text2SQL models by substituting their schema linking processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12911v2</guid>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Yuan, Hao Chen, Zijin Hong, Qinggang Zhang, Feiran Huang, Qing Li, Xiao Huang</dc:creator>
    </item>
    <item>
      <title>Linking Data Citation to Repository Visibility: An Empirical Study</title>
      <link>https://arxiv.org/abs/2506.09530</link>
      <description>arXiv:2506.09530v2 Announce Type: replace-cross 
Abstract: In today's data-driven research landscape, dataset visibility and accessibility play a crucial role in advancing scientific knowledge. At the same time, data citation is essential for maintaining academic integrity, acknowledging contributions, validating research outcomes, and fostering scientific reproducibility. As a critical link, it connects scholarly publications with the datasets that drive scientific progress. This study investigates whether repository visibility influences data citation rates. We hypothesize that repositories with higher visibility, as measured by search engine metrics, are associated with increased dataset citations. Using OpenAlex data and repository impact indicators (including the visibility index from Sistrix, the h-index of repositories, and citation metrics such as mean and median citations), we analyze datasets in Social Sciences and Economics to explore their relationship. Our findings suggest that datasets hosted on more visible web domains tend to receive more citations, with a positive correlation observed between web domain visibility and dataset citation counts, particularly for datasets with at least one citation. However, when analyzing domain-level citation metrics, such as the h-index, mean, and median citations, the correlations are inconsistent and weaker. While higher visibility domains tend to host datasets with greater citation impact, the distribution of citations across datasets varies significantly. These results suggest that while visibility plays a role in increasing citation counts, it is not the sole factor influencing dataset citation impact. Other elements, such as dataset quality, research trends, and disciplinary norms, can also contribute to citation patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09530v2</guid>
      <category>cs.DL</category>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fakhri Momeni, Janete Saldanha Bach, Brigitte Mathiak, Peter Mutschke</dc:creator>
    </item>
  </channel>
</rss>
