<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Sep 2024 04:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Maximum And- vs. Even-SAT</title>
      <link>https://arxiv.org/abs/2409.07837</link>
      <description>arXiv:2409.07837v1 Announce Type: new 
Abstract: A (multi)set of literals, called a clause, is strongly satisfied by an assignment if no literal evaluates to false. Finding an assignment that maximises the number of strongly satisfied clauses is NP-hard. We present a simple algorithm that finds, given a set of clauses that admits an assignment that strongly satisfies a $\rho$-fraction of the clauses, an assignment in which at least a $\rho$-fraction of the clauses is weakly satisfied, in the sense that an even number of literals evaluates to false. In particular, this implies an efficient algorithm for finding an undirected cut of value $\rho$ in a graph given that a directed cut of value $\rho$ in the graph is promised to exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07837v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tamio-Vesa Nakajima, Stanislav \v{Z}ivn\'y</dc:creator>
    </item>
    <item>
      <title>Computing the LZ-End parsing: Easy to implement and practically efficient</title>
      <link>https://arxiv.org/abs/2409.07840</link>
      <description>arXiv:2409.07840v1 Announce Type: new 
Abstract: The LZ-End parsing [Kreft &amp; Navarro, 2011] of an input string yields compression competitive with the popular Lempel-Ziv 77 scheme, but also allows for efficient random access. Kempa and Kosolobov showed that the parsing can be computed in time and space linear in the input length [Kempa &amp; Kosolobov, 2017], however, the corresponding algorithm is hardly practical. We put the spotlight on their suboptimal algorithm that computes the parsing in time $\mathcal{O}(n \lg\lg n)$. It requires a comparatively small toolset and is therefore easy to implement, but at the same time very efficient in practice. We give a detailed and simplified description with a full listing that incorporates undocumented tricks from the original implementation, but also uses lazy evaluation to reduce the workload in practice and requires less working memory by removing a level of indirection. We legitimize our algorithm in a brief benchmark, obtaining the parsing faster than the state of the art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07840v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Dinklage</dc:creator>
    </item>
    <item>
      <title>An Optimal Algorithm for Sorting Pattern-Avoiding Sequences</title>
      <link>https://arxiv.org/abs/2409.07868</link>
      <description>arXiv:2409.07868v1 Announce Type: new 
Abstract: We present a deterministic comparison-based algorithm that sorts sequences avoiding a fixed permutation $\pi$ in linear time, even if $\pi$ is a priori unkown. Moreover, the dependence of the multiplicative constant on the pattern $\pi$ matches the information-theoretic lower bound. A crucial ingredient is an algorithm for performing efficient multi-way merge based on the Marcus-Tardos theorem. As a direct corollary, we obtain a linear-time algorithm for sorting permutations of bounded twin-width.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07868v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michal Opler</dc:creator>
    </item>
    <item>
      <title>Fine-Grained Complexity of Multiple Domination and Dominating Patterns in Sparse Graphs</title>
      <link>https://arxiv.org/abs/2409.08037</link>
      <description>arXiv:2409.08037v1 Announce Type: new 
Abstract: The study of domination in graphs has led to a variety of domination problems studied in the literature. Most of these follow the following general framework: Given a graph $G$ and an integer $k$, decide if there is a set $S$ of $k$ vertices such that (1) some inner property $\phi(S)$ (e.g., connectedness) is satisfied, and (2) each vertex $v$ satisfies some domination property $\rho(S, v)$ (e.g., there is an $s\in S$ that is adjacent to $v$).
  Since many real-world graphs are sparse, we seek to determine the optimal running time of such problems in both the number $n$ of vertices and the number $m$ of edges in $G$. While the classic dominating set problem admits a rather limited improvement in sparse graphs (Fischer, K\"unnemann, Redzic SODA'24), we show that natural variants studied in the literature admit much larger speed-ups, with a diverse set of possible running times. Specifically, we obtain conditionally optimal algorithms for:
  1) $r$-Multiple $k$-Dominating Set (each vertex must be adjacent to at least $r$ vertices in $S$): If $r\le k-2$, we obtain a running time of $(m/n)^{r} n^{k-r+o(1)}$ that is conditionally optimal assuming the 3-uniform hyperclique hypothesis. In sparse graphs, this fully interpolates between $n^{k-1\pm o(1)}$ and $n^{2\pm o(1)}$, depending on $r$. Curiously, when $r=k-1$, we obtain a randomized algorithm beating $(m/n)^{k-1} n^{1+o(1)}$ and we show that this algorithm is close to optimal under the $k$-clique hypothesis.
  2) $H$-Dominating Set ($S$ must induce a pattern $H$). We conditionally settle the complexity of three such problems: (a) Dominating Clique ($H$ is a $k$-clique), (b) Maximal Independent Set of size $k$ ($H$ is an independent set on $k$ vertices), (c) Dominating Induced Matching ($H$ is a perfect matching on $k$ vertices).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08037v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marvin K\"unnemann, Mirza Redzic</dc:creator>
    </item>
    <item>
      <title>Static Pricing for Single Sample Multi-unit Prophet Inequalities</title>
      <link>https://arxiv.org/abs/2409.07719</link>
      <description>arXiv:2409.07719v1 Announce Type: cross 
Abstract: In this paper, we study $k$-unit single sample prophet inequalities. A seller has $k$ identical, indivisible items to sell. A sequence of buyers arrive one-by-one, with each buyer's private value for the item, $X_i$, revealed to the seller when they arrive. While the seller is unaware of the distribution from which $X_i$ is drawn, they have access to a single sample, $Y_i$ drawn from the same distribution as $X_i$. What strategies can the seller adopt so as to maximize social welfare?
  Previous work has demonstrated that when $k = 1$, if the seller sets a price equal to the maximum of the samples, they can achieve a competitive ratio of $\frac{1}{2}$ of the social welfare, and recently Pashkovich and Sayutina established an analogous result for $k = 2$. In this paper, we prove that for $k \geq 3$, setting a (static) price equal to the $k^{\text{th}}$ largest sample also obtains a competitive ratio of $\frac{1}{2}$, resolving a conjecture Pashkovich and Sayutina pose.
  We then consider the situation where $k$ is large. We demonstrate that setting a price equal to the $(k-\sqrt{2k\log k})^{\text{th}}$ largest sample obtains a competitive ratio of $1 - \sqrt{\frac{2\log k}{k}} - o\left(\sqrt{\frac{\log k}{k}}\right)$, and that this is the optimal possible ratio achievable with a static pricing scheme that sets one of the samples as a price.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07719v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pranav Nuti, Peter Westbrook</dc:creator>
    </item>
    <item>
      <title>Graph Inspection for Robotic Motion Planning: Do Arithmetic Circuits Help?</title>
      <link>https://arxiv.org/abs/2409.08219</link>
      <description>arXiv:2409.08219v1 Announce Type: cross 
Abstract: We investigate whether algorithms based on arithmetic circuits are a viable alternative to existing solvers for Graph Inspection, a problem with direct application in robotic motion planning. Specifically, we seek to address the high memory usage of existing solvers. Aided by novel theoretical results enabling fast solution recovery, we implement a circuit-based solver for Graph Inspection which uses only polynomial space and test it on several realistic robotic motion planning datasets. In particular, we provide a comprehensive experimental evaluation of a suite of engineered algorithms for three key subroutines. While this evaluation demonstrates that circuit-based methods are not yet practically competitive for our robotics application, it also provides insights which may guide future efforts to bring circuit-based algorithms from theory to practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08219v1</guid>
      <category>cs.RO</category>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, Daniel Coimbra Salomao, Alex Crane, Yosuke Mizutani, Felix Reidl, Blair D. Sullivan</dc:creator>
    </item>
    <item>
      <title>Towards Instance-Optimal Euclidean Spanners</title>
      <link>https://arxiv.org/abs/2409.08227</link>
      <description>arXiv:2409.08227v1 Announce Type: cross 
Abstract: Euclidean spanners are important geometric objects that have been extensively studied since the 1980s. The two most basic "compactness'' measures of a Euclidean spanner $E$ are the size (number of edges) $|E|$ and the weight (sum of edge weights) $\|E\|$. In this paper, we initiate the study of instance optimal Euclidean spanners. Our results are two-fold.
  We demonstrate that the greedy spanner is far from being instance optimal, even when allowing its stretch to grow. More concretely, we design two hard instances of point sets in the plane, where the greedy $(1+x \epsilon)$-spanner (for basically any parameter $x \geq 1$) has $\Omega_x(\epsilon^{-1/2}) \cdot |E_\mathrm{spa}|$ edges and weight $\Omega_x(\epsilon^{-1}) \cdot \|E_\mathrm{light}\|$, where $E_\mathrm{spa}$ and $E_\mathrm{light}$ denote the per-instance sparsest and lightest $(1+\epsilon)$-spanners, respectively, and the $\Omega_x$ notation suppresses a polynomial dependence on $1/x$.
  As our main contribution, we design a new construction of Euclidean spanners, which is inherently different from known constructions, achieving the following bounds: a stretch of $1+\epsilon\cdot 2^{O(\log^*(d/\epsilon))}$ with $O(1) \cdot |E_\mathrm{spa}|$ edges and weight $O(1) \cdot \|E_\mathrm{light}\|$. In other words, we show that a slight increase to the stretch suffices for obtaining instance optimality up to an absolute constant for both sparsity and lightness. Remarkably, there is only a log-star dependence on the dimension in the stretch, and there is no dependence on it whatsoever in the number of edges and weight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08227v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung Le, Shay Solomon, Cuong Than, Csaba D. T\'oth, Tianyi Zhang</dc:creator>
    </item>
    <item>
      <title>The Competitive Ratio of Threshold Policies for Online Unit-density Knapsack Problems</title>
      <link>https://arxiv.org/abs/1907.08735</link>
      <description>arXiv:1907.08735v3 Announce Type: replace 
Abstract: We study a supply chain ordering problem faced by a wholesale supplier serving unpredictable demand. In this problem, the supplier has an initial stock, and faces a stream of orders for different amounts that are unknown a priori. Each order must be either accepted or rejected immediately, and must respect the knapsack constraint, that is, an order is only acceptable if its amount can be fully served by the remaining stock. The objective is to maximize the total stock spent serving orders.
  We investigate randomized threshold algorithms that accept an item as long as its size exceeds the threshold. We derive two optimal threshold distributions, the first is 0.4324-competitive relative to the optimal offline integral packing, and the second is 0.4285-competitive relative to the optimal offline fractional packing. Both results require optimizing the cumulative distribution function of the random threshold, which are challenging infinite-dimensional optimization problems. We also consider the generalization to multiple knapsacks, where an arriving item has a different size in each knapsack and must be placed in at most one knapsack. We derive a 0.2142-competitive algorithm for this problem. We also show that any randomized algorithm for this problem cannot be more than 0.4605-competitive. This is the first upper bound strictly less than 0.5, which implies the intrinsic challenge of knapsack constraint.
  We show how to naturally implement our optimal threshold distributions in the warehouses of a Latin American chain department store. We run simulations on their order data, which demonstrate the efficacy of our proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:1907.08735v3</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Will Ma, David Simchi-Levi, Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Simpler Optimal Sorting from a Directed Acyclic Graph</title>
      <link>https://arxiv.org/abs/2407.21591</link>
      <description>arXiv:2407.21591v5 Announce Type: replace 
Abstract: Fredman proposed in 1976 the following algorithmic problem: Given are a ground set $X$, some partial order $P$ over $X$, and some comparison oracle $O_L$ that specifies a linear order $L$ over $X$ that extends $P$. A query to $O_L$ has as input distinct $x, x' \in X$ and outputs whether $x &lt;_L x'$ or vice versa. If we denote by $e(P)$ the number of linear extensions of $P$, then $\log e(P)$ is a worst-case lower bound on the number of queries needed to output the sorted order of $X$.
  Fredman did not specify in what form the partial order is given. Haeupler, Hlad\'ik, Iacono, Rozhon, Tarjan, and T\v{e}tek ('24) propose to assume as input a directed acyclic graph, $G$, with $m$ edges and $n=|X|$ vertices. Denote by $P_G$ the partial order induced by $G$. Algorithmic performance is measured in running time and the number of queries used, where they use $\Theta(m + n + \log e(P_G))$ time and $\Theta(\log e(P_G))$ queries to output $X$ in its sorted order. Their algorithm is worst-case optimal in terms of running time and queries, both. Their algorithm combines topological sorting with heapsort. Their analysis relies upon sophisticated counting arguments using entropy, recursively defined sets defined over the run of their algorithm, and vertices in the graph that they identify as bottlenecks for sorting.
  In this paper, we do away with sophistication. We show that when the input is a directed acyclic graph then the problem admits a simple solution using $\Theta(m + n + \log e(P_G))$ time and $\Theta(\log e(P_G))$ queries. Especially our proofs are much simpler as we avoid the usage of advanced charging arguments and data structures, and instead rely upon two brief observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21591v5</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivor van der Hoog, Eva Rotenberg, Daniel Rutschmann</dc:creator>
    </item>
    <item>
      <title>Upward Pointset Embeddings of Planar st-Graphs</title>
      <link>https://arxiv.org/abs/2408.17369</link>
      <description>arXiv:2408.17369v3 Announce Type: replace 
Abstract: We study upward pointset embeddings (UPSEs) of planar $st$-graphs. Let $G$ be a planar $st$-graph and let $S \subset \mathbb{R}^2$ be a pointset with $|S|= |V(G)|$. An UPSE of $G$ on $S$ is an upward planar straight-line drawing of $G$ that maps the vertices of $G$ to the points of $S$. We consider both the problem of testing the existence of an UPSE of $G$ on $S$ (UPSE Testing) and the problem of enumerating all UPSEs of $G$ on $S$. We prove that UPSE Testing is NP-complete even for $st$-graphs that consist of a set of directed $st$-paths sharing only $s$ and $t$. On the other hand, for $n$-vertex planar $st$-graphs whose maximum $st$-cutset has size $k$, we prove that it is possible to solve UPSE Testing in $O(n^{4k})$ time with $O(n^{3k})$ space, and to enumerate all UPSEs of $G$ on $S$ with $O(n)$ worst-case delay, using $O(k n^{4k} \log n)$ space, after $O(k n^{4k} \log n)$ set-up time. Moreover, for an $n$-vertex $st$-graph whose underlying graph is a cycle, we provide a necessary and sufficient condition for the existence of an UPSE on a given poinset, which can be tested in $O(n \log n)$ time. Related to this result, we give an algorithm that, for a set $S$ of $n$ points, enumerates all the non-crossing monotone Hamiltonian cycles on $S$ with $O(n)$ worst-case delay, using $O(n^2)$ space, after $O(n^2)$ set-up time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17369v3</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Alegria, Susanna Caroppo, Giordano Da Lozzo, Marco D'Elia, Giuseppe Di Battista, Fabrizio Frati, Fabrizio Grosso, Maurizio Patrignani</dc:creator>
    </item>
    <item>
      <title>Structured Downsampling for Fast, Memory-efficient Curation of Online Data Streams</title>
      <link>https://arxiv.org/abs/2409.06199</link>
      <description>arXiv:2409.06199v2 Announce Type: replace 
Abstract: Operations over data streams typically hinge on efficient mechanisms to aggregate or summarize history on a rolling basis. For high-volume data steams, it is critical to manage state in a manner that is fast and memory efficient -- particularly in resource-constrained or real-time contexts. Here, we address the problem of extracting a fixed-capacity, rolling subsample from a data stream. Specifically, we explore ``data stream curation'' strategies to fulfill requirements on the composition of sample time points retained. Our ``DStream'' suite of algorithms targets three temporal coverage criteria: (1) steady coverage, where retained samples should spread evenly across elapsed data stream history; (2) stretched coverage, where early data items should be proportionally favored; and (3) tilted coverage, where recent data items should be proportionally favored. For each algorithm, we prove worst-case bounds on rolling coverage quality. We focus on the more practical, application-driven case of maximizing coverage quality given a fixed memory capacity. As a core simplifying assumption, we restrict algorithm design to a single update operation: writing from the data stream to a calculated buffer site -- with data never being read back, no metadata stored (e.g., sample timestamps), and data eviction occurring only implicitly via overwrite. Drawing only on primitive, low-level operations and ensuring full, overhead-free use of available memory, this ``DStream'' framework ideally suits domains that are resource-constrained, performance-critical, and fine-grained (e.g., individual data items as small as single bits or bytes). The proposed approach supports $\mathcal{O}(1)$ data ingestion via concise bit-level operations. To further practical applications, we provide plug-and-play open-source implementations targeting both scripted and compiled application domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06199v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Andres Moreno, Luis Zaman, Emily Dolson</dc:creator>
    </item>
    <item>
      <title>A Gap-ETH-Tight Approximation Scheme for Euclidean TSP</title>
      <link>https://arxiv.org/abs/2011.03778</link>
      <description>arXiv:2011.03778v3 Announce Type: replace-cross 
Abstract: We revisit the classic task of finding the shortest tour of $n$ points in $d$-dimensional Euclidean space, for any fixed constant $d \geq 2$. We determine the optimal dependence on $\varepsilon$ in the running time of an algorithm that computes a $(1+\varepsilon)$-approximate tour, under a plausible assumption. Specifically, we give an algorithm that runs in $2^{\mathcal{O}(1/\varepsilon^{d-1})} n\log n$ time. This improves the previously smallest dependence on $\varepsilon$ in the running time $(1/\varepsilon)^{\mathcal{O}(1/\varepsilon^{d-1})}n \log n$ of the algorithm by Rao and Smith~(STOC 1998). We also show that a $2^{o(1/\varepsilon^{d-1})}\text{poly}(n)$ algorithm would violate the Gap-Exponential Time Hypothesis (Gap-ETH).
  Our new algorithm builds upon the celebrated quadtree-based methods initially proposed by Arora (J. ACM 1998), but it adds a new idea that we call \emph{sparsity-sensitive patching}. On a high level this lets the granularity with which we simplify the tour depend on how sparse it is locally. We demonstrate that our technique extends to other problems, by showing that for Steiner Tree and Rectilinear Steiner Tree it yields the same running time. We complement our results with a matching Gap-ETH lower bound for Rectilinear Steiner Tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.03778v3</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>S\'andor Kisfaludi-Bak, Jesper Nederlof, Karol W\k{e}grzycki</dc:creator>
    </item>
    <item>
      <title>Quantum Realization of the Finite Element Method</title>
      <link>https://arxiv.org/abs/2403.19512</link>
      <description>arXiv:2403.19512v2 Announce Type: replace-cross 
Abstract: This paper presents a quantum algorithm for the solution of prototypical second-order linear elliptic partial differential equations discretized by $d$-linear finite elements on Cartesian grids of a bounded $d$-dimensional domain. An essential step in the construction is a BPX preconditioner, which transforms the linear system into a sufficiently well-conditioned one, making it amenable to quantum computation. We provide a constructive proof demonstrating that, for any fixed dimension, our quantum algorithm can compute suitable functionals of the solution to a given tolerance $\mathtt{tol}$ with an optimal complexity of order $\mathtt{tol}^{-1}$ up to logarithmic terms, significantly improving over existing approaches. Notably, this approach does not rely on regularity of the solution and achieves quantum advantage over classical solvers in two dimensions, whereas prior quantum methods required at least four dimensions for asymptotic benefits. We further detail the design and implementation of a quantum circuit capable of executing our algorithm, present simulator results, and report numerical experiments on current quantum hardware, confirming the feasibility of preconditioned finite element methods for near-term quantum computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19512v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Deiml, Daniel Peterseim</dc:creator>
    </item>
  </channel>
</rss>
