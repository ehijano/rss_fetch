<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 May 2025 01:45:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>ARC-V: Vertical Resource Adaptivity for HPC Workloads in Containerized Environments</title>
      <link>https://arxiv.org/abs/2505.02964</link>
      <description>arXiv:2505.02964v1 Announce Type: new 
Abstract: Existing state-of-the-art vertical autoscalers for containerized environments are traditionally built for cloud applications, which might behave differently than HPC workloads with their dynamic resource consumption. In these environments, autoscalers may create an inefficient resource allocation. This work analyzes nine representative HPC applications with different memory consumption patterns. Our results identify the limitations and inefficiencies of the Kubernetes Vertical Pod Autoscaler (VPA) for enabling memory elastic execution of HPC applications. We propose, implement, and evaluate ARC-V. This policy leverages both in-flight resource updates of pods in Kubernetes and the knowledge of memory consumption patterns of HPC applications for achieving elastic memory resource provisioning at the node level. Our results show that ARC-V can effectively save memory while eliminating out-of-memory errors compared to the standard Kubernetes VPA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02964v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Medeiros, Jeremy J. Williams, Jacob Wahlgren, Leonardo Saud Maia Leite, Ivy Peng</dc:creator>
    </item>
    <item>
      <title>Parallel GPU-Accelerated Randomized Construction of Approximate Cholesky Preconditioners</title>
      <link>https://arxiv.org/abs/2505.02977</link>
      <description>arXiv:2505.02977v1 Announce Type: new 
Abstract: We introduce a parallel algorithm to construct a preconditioner for solving a large, sparse linear system where the coefficient matrix is a Laplacian matrix (a.k.a., graph Laplacian). Such a linear system arises from applications such as discretization of a partial differential equation, spectral graph partitioning, and learning problems on graphs. The preconditioner belongs to the family of incomplete factorizations and is purely algebraic. Unlike traditional incomplete factorizations, the new method employs randomization to determine whether or not to keep fill-ins, i.e., newly generated nonzero elements during Gaussian elimination. Since the sparsity pattern of the randomized factorization is unknown, computing such a factorization in parallel is extremely challenging, especially on many-core architectures such as GPUs. Our parallel algorithm dynamically computes the dependency among row/column indices of the Laplacian matrix to be factorized and processes the independent indices in parallel. Furthermore, unlike previous approaches, our method requires little pre-processing time. We implemented the parallel algorithm for multi-core CPUs and GPUs, and we compare their performance to other state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02977v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Liang, Chao Chen, Yotam Yaniv, Hengrui Luo, David Tench, Xiaoye S. Li, Aydin Buluc, James Demmel</dc:creator>
    </item>
    <item>
      <title>"Two-Stagification": Job Dispatching in Large-Scale Clusters via a Two-Stage Architecture</title>
      <link>https://arxiv.org/abs/2505.03032</link>
      <description>arXiv:2505.03032v1 Announce Type: new 
Abstract: A continuing effort is devoted to devising effective dispatching policies for clusters of First Come First Served servers. Although the optimal solution for dispatchers aware of both job size and server state remains elusive, lower bounds and strong heuristics are known. In this paper, we introduce a two-stage cluster architecture that applies classical Round Robin, Join Idle Queue, and Least Work Left dispatching schemes, coupled with an optimized service-time threshold to separate large jobs from shorter ones. Using both synthetic (Weibull) workloads and real Google data center traces, we demonstrate that our two-stage approach greatly improves upon the corresponding single-stage policies and closely approaches the performance of advanced size- and state-aware methods. Our results highlight that careful architectural design-rather than increased complexity at the dispatcher-can yield significantly better mean response times in large-scale computing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03032v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mert Yildiz, Alexey Rolich, Andrea Baiocchi</dc:creator>
    </item>
    <item>
      <title>The Tensor-Core Beamformer: A High-Speed Signal-Processing Library for Multidisciplinary Use</title>
      <link>https://arxiv.org/abs/2505.03269</link>
      <description>arXiv:2505.03269v1 Announce Type: new 
Abstract: Beamforming is a well-known technique to combine signals from multiple sensors. It has a wide range of application domains. This paper introduces the Tensor-Core Beamformer: a generic, optimized beamformer library that harnesses the computational power of GPU tensor cores to accelerate beamforming computations. The library hides the complexity of tensor cores from the user, and supports 16-bit and 1-bit precision. An extensive performance evaluation on NVIDIA and AMD GPUs shows that the library outperforms traditional beamforming on regular GPU cores by a wide margin, at much higher energy efficiency. In the 16-bit mode, it achieves over 600 TeraOps/s on an AMD MI300X GPU, while approaching 1 TeraOp/J. In the 1-bit mode, it breaks the 3 PetaOps/s barrier and achieves over 10 TeraOps/J on an NVIDIA A100 GPU. The beamforming library can be easily integrated into existing pipelines. We demonstrate its use for medical ultrasound and radio-astronomical instruments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03269v1</guid>
      <category>cs.DC</category>
      <category>astro-ph.IM</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Leon Oostrum, Bram Veenboer, Ronald Rook, Michael Brown, Pieter Kruizinga, John W. Romein</dc:creator>
    </item>
    <item>
      <title>Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories</title>
      <link>https://arxiv.org/abs/2505.03443</link>
      <description>arXiv:2505.03443v1 Announce Type: new 
Abstract: Centralized and distributed systems are two main approaches to organizing ICT infrastructure, each with its pros and cons. Centralized systems concentrate resources in one location, making management easier but creating single points of failure. Distributed systems, on the other hand, spread resources across multiple nodes, offering better scalability and fault tolerance, but requiring more complex management. The choice between them depends on factors like application needs, scalability, and data sensitivity. Centralized systems suit applications with limited scalability and centralized control, while distributed systems excel in large-scale environments requiring high availability and performance. This paper explores a distributed document repository system developed for the Italian Ministry of Justice, using edge repositories to analyze textual data and metadata, enhancing semantic exploration capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03443v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valerio Bellandi</dc:creator>
    </item>
    <item>
      <title>TailBench++: Flexible Multi-Client, Multi-Server Benchmarking for Latency-Critical Workloads</title>
      <link>https://arxiv.org/abs/2505.03600</link>
      <description>arXiv:2505.03600v1 Announce Type: new 
Abstract: Cloud systems have rapidly expanded worldwide in the last decade, shifting computational tasks to cloud servers where clients submit their requests. Among cloud workloads, latency-critical applications -- characterized by high-percentile response times -- have gained special interest. These applications are present in modern services, representing an important fraction of cloud workloads. This work analyzes common cloud benchmarking suites and identifies TailBench as the most suitable to assess cloud performance with latency-critical workloads. Unfortunately, this suite presents key limitations, especially in multi-server scenarios or environments with variable client arrival patterns and fluctuating loads. To address these limitations, we propose TailBench++, an enhanced benchmark suite that extends TailBench to enable cloud evaluation studies to be performed in dynamic multi-client, multi-server environments. It allows reproducing experiments with varying client arrival times, dynamic query per second (QPS) fluctuations, and multiple servers handling requests. Case studies show that TailBench++ enables more realistic evaluations by capturing a wider range of real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03600v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhilin Li, Lucia Pons, Salvador Petit, Julio Sahuquillo, Julio Pons</dc:creator>
    </item>
    <item>
      <title>Revisiting Lower Bounds for Two-Step Consensus</title>
      <link>https://arxiv.org/abs/2505.03627</link>
      <description>arXiv:2505.03627v1 Announce Type: new 
Abstract: A seminal result by Lamport shows that at least $\max\{2e+f+1,2f+1\}$ processes are required to implement partially synchronous consensus that tolerates $f$ process failures and can furthermore decide in two message delays under $e$ failures. This lower bound is matched by the classical Fast Paxos protocol. However, more recent practical protocols, such as Egalitarian Paxos, provide two-step decisions with fewer processes, seemingly contradicting the lower bound. We show that this discrepancy arises because the classical bound requires two-step decisions under a wide range of scenarios, not all of which are relevant in practice. We propose a more pragmatic condition for which we establish tight bounds on the number of processes required. Interestingly, these bounds depend on whether consensus is implemented as an atomic object or a decision task. For consensus as an object, $\max\{2e+f-1,2f+1\}$ processes are necessary and sufficient for two-step decisions, while for a task the tight bound is $\max\{2e+f, 2f+1\}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03627v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fedor Ryabinin, Alexey Gotsman, Pierre Sutra</dc:creator>
    </item>
    <item>
      <title>Multiscale Parallel Simulation of Malignant Pleural Mesothelioma via Adaptive Domain Partitioning -- an Efficiency Analysis Study</title>
      <link>https://arxiv.org/abs/2505.03067</link>
      <description>arXiv:2505.03067v1 Announce Type: cross 
Abstract: A novel parallel efficiency analysis on a framework for simulating the growth of Malignant Pleural Mesothelioma (MPM) tumours is presented. Proliferation of MPM tumours in the pleural space is simulated using a Cellular Potts Model (CPM) coupled with partial differential equations (PDEs). Using segmented lung data from CT scans, an environment is set up with artificial tumour data in the pleural space, representing the simulation domain, onto which a dynamic bounding box is applied to restrict computations to the region of interest, dramatically reducing memory and CPU overhead. This adaptive partitioning of the domain enables efficient use of computational resources by reducing the three-dimensional (3D) domain over which the PDEs are to be solved. The PDEs, representing oxygen, nutrients, and cytokines, are solved using the finite-volume method with a first-order implicit Euler scheme. Parallelization is realized using the public Python library mpi4py in combination with LinearGMRESSolver and PETSc for efficient convergence. Performance analyses have shown that parallelization achieves a reduced solving time compared to serial computation. Also, optimizations enable efficient use of available memory and improved load balancing amongst the cores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03067v1</guid>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anton Dolganov, Valeria Krzhizhanovskaya, Stefano Trebeschi, Vivek M. Sheraton</dc:creator>
    </item>
    <item>
      <title>A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning</title>
      <link>https://arxiv.org/abs/2505.03553</link>
      <description>arXiv:2505.03553v1 Announce Type: cross 
Abstract: Inconsistent outputs and hallucinations from large language models (LLMs) are major obstacles to reliable AI systems. When different proprietary reasoning models (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI, are given the same complex request, they often produce divergent results due to variations in training and inference. This paper proposes a novel consensus mechanism, inspired by distributed ledger technology, to validate and converge these outputs, treating each RM as a black-box peer. Building on the Hashgraph consensus algorithm, our approach employs gossip-about-gossip communication and virtual voting to achieve agreement among an ensemble of RMs. We present an architectural design for a prototype system in which RMs iteratively exchange and update their answers, using information from each round to improve accuracy and confidence in subsequent rounds. This approach goes beyond simple majority voting by incorporating the knowledge and cross-verification content of every model. We justify the feasibility of this Hashgraph-inspired consensus for AI ensembles and outline its advantages over traditional ensembling techniques in reducing nonfactual outputs. Preliminary considerations for implementation, evaluation criteria for convergence and accuracy, and potential challenges are discussed. The proposed mechanism demonstrates a promising direction for multi-agent AI systems to self-validate and deliver high-fidelity responses in complex tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03553v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kolawole E. Ogunsina, Morayo A. Ogunsina</dc:creator>
    </item>
    <item>
      <title>Decentralized Nonconvex Optimization under Heavy-Tailed Noise: Normalization and Optimal Convergence</title>
      <link>https://arxiv.org/abs/2505.03736</link>
      <description>arXiv:2505.03736v1 Announce Type: cross 
Abstract: Heavy-tailed noise in nonconvex stochastic optimization has garnered increasing research interest, as empirical studies, including those on training attention models, suggest it is a more realistic gradient noise condition. This paper studies first-order nonconvex stochastic optimization under heavy-tailed gradient noise in a decentralized setup, where each node can only communicate with its direct neighbors in a predefined graph. Specifically, we consider a class of heavy-tailed gradient noise that is zero-mean and has only $p$-th moment for $p \in (1, 2]$. We propose GT-NSGDm, Gradient Tracking based Normalized Stochastic Gradient Descent with momentum, that utilizes normalization, in conjunction with gradient tracking and momentum, to cope with heavy-tailed noise on distributed nodes. We show that, when the communication graph admits primitive and doubly stochastic weights, GT-NSGDm guarantees, for the \textit{first} time in the literature, that the expected gradient norm converges at an optimal non-asymptotic rate $O\big(1/T^{(p-1)/(3p-2)}\big)$, which matches the lower bound in the centralized setup. When tail index $p$ is unknown, GT-NSGDm attains a non-asymptotic rate $O\big( 1/T^{(p-1)/(2p)} \big)$ that is, for $p &lt; 2$, topology independent and has a speedup factor $n^{1-1/p}$ in terms of the number of nodes $n$. Finally, experiments on nonconvex linear regression with tokenized synthetic data and decentralized training of language models on a real-world corpus demonstrate that GT-NSGDm is more robust and efficient than baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03736v1</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhua Yu, Dusan Jakovetic, Soummya Kar</dc:creator>
    </item>
    <item>
      <title>Clock Distribution with Gradient TRIX</title>
      <link>https://arxiv.org/abs/2301.05073</link>
      <description>arXiv:2301.05073v2 Announce Type: replace 
Abstract: Gradient clock synchronization (GCS) algorithms minimize the worst-case clock offset between the nodes in a distributed network of diameter $D$ and size $n$. They achieve optimal offsets of $\Theta(\log D)$ locally, i.e. between adjacent nodes as shown by Lenzen et al., and $\Theta(D)$ globally as shown by Biaz and Welch. As demonstrated in the work of Bund et al., this is a highly promising approach for improved clocking schemes for large-scale synchronous Systems-on-Chip (SoC). Unfortunately, in large systems, faults hinder their practical use. State of the art fault-tolerant, as presented by Bund et al., has a drawback that is fatal in this setting: It relies on node and edge replication. For $f=1$, this translates to at least $16$-fold edge replication and high degree nodes, far from the optimum of $2f+1=3$ for tolerating up to $f$ faulty neighbors.
  In this work, we present a self-stabilizing GCS algorithm for a grid-like directed graph with optimal node in- and out-degrees of $3$ that tolerates $1$ faulty in-neighbor. If nodes fail with independent probability $p\in o(n^{-1/2})$, it achieves asymptotically optimal local skew of $\Theta(\log D)$ with probability $1-o(1)$; this holds under general worst-case assumptions on link delay and clock speed variations, provided they change slowly relative to the speed of the system. The failure probability is the largest possible ensuring that with probabity $1-o(1)$ for each node at most one in-neighbor fails. As modern hardware is clocked at gigahertz speeds and the algorithm can simultaneously sustain a constant number of arbitrary changes due to faults in each clock cycle, this results in sufficient robustness to dramatically increase the size of reliable synchronously clocked SoCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.05073v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Lenzen, Shreyas Srinivas</dc:creator>
    </item>
    <item>
      <title>Synthesizing Proxy Applications for MPI Programs</title>
      <link>https://arxiv.org/abs/2301.06062</link>
      <description>arXiv:2301.06062v3 Announce Type: replace 
Abstract: Proxy applications (proxy-apps) are basic tools for evaluating the performance of specific workloads on high-performance computing (HPC) systems. Since the development of high-fidelity proxy-apps, which exhibit similar performance characteristics as corresponding production applications, is labor-intensive, synthetic proxy-apps are created as a useful supplement to manually developed proxy-apps. To thoroughly resemble performance characteristics of HPC applications represented by Message Passing Interface (MPI) programs, we propose Siesta, a novel framework to automatically synthesize proxy-apps based on communication-computation traces. Given an MPI program, Siesta synthesizes parameterized code snippets to mimic computation behaviors in different execution periods, and combines the code snippets and MPI function records into an event trace. It then extracts program behavior patterns from the trace as grammars and finally transforms the grammars into a synthetic proxy-app. We evaluate the proposed methods on representative MPI programs with various environments. The results show that our synthetic proxy-apps can precisely approximate the performance characteristics of MPI programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06062v3</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiyu Luo, Tao Yan, Qingguo Xu, Jingwei Sun, Guangzhong Sun</dc:creator>
    </item>
    <item>
      <title>ESTA: An Efficient Spatial-Temporal Range Aggregation Query Processing Algorithm for UAV Networks</title>
      <link>https://arxiv.org/abs/2308.11977</link>
      <description>arXiv:2308.11977v3 Announce Type: replace 
Abstract: Unmanned Aerial Vehicle (UAV) networks are increasingly deployed in military and civilian applications, serving as critical platforms for data collection. Users frequently require aggregated statistical information derived from historical sensory data within specific spatial and temporal boundaries. To address this, users submit aggregation query requests with spatial-temporal constraints to target UAVs that store the relevant data. These UAVs process and return the query results, which can be aggregated within the network during transmission to conserve energy and bandwidth-resources that are inherently limited in UAV networks. However,the dynamic topology caused by UAV mobility, coupled with these resource constraints, makes efficient in-network aggregation challenging without compromising user query delay. To the best of our knowledge, existing research has yet to adequately explore spatial-temporal range aggregation queries in the context of UAV networks. In this paper, we propose ESTA, an Efficient Spatial-Temporal range Aggregation query processing algorithm tailored for UAV networks. ESTA leverages pre-planned UAV trajectories to construct a topology change graph that models the network's evolving connectivity. It then employs an efficient shortest path algorithm to determine the minimum query response delay. Subsequently, while adhering to user-specified delay constraints, ESTA transforms the in-network aggregation process into a series of set cover problems, which are solved recursively to build a Spatial-Temporal Aggregation Tree (STAT). This tree enables the identification of an energy-efficient routing path for aggregating and delivering query results. Extensive simulations demonstrate that ESTA reduces energy consumption by more than 50% compared to a baseline algorithm, all while satisfying the required query delay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11977v3</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Liu, Wenbin Zhai, Xin Li, Youwei Ding, Wanying Lu, Ran Wang</dc:creator>
    </item>
    <item>
      <title>Malleus: Straggler-Resilient Hybrid Parallel Training of Large-scale Models via Malleable Data and Model Parallelization</title>
      <link>https://arxiv.org/abs/2410.13333</link>
      <description>arXiv:2410.13333v3 Announce Type: replace 
Abstract: As the scale of models and training data continues to grow, there is an expanding reliance on more GPUs to train large-scale models, which inevitably increases the likelihood of encountering dynamic stragglers that some devices lag behind in performance occasionally. However, hybrid parallel training, one of the de facto paradigms to train large models, is typically sensitive to the stragglers.
  This paper presents Malleus, a straggler-resilient hybrid parallel training framework for large-scale models. Malleus quantifies the stragglers at the nuanced, per-GPU granularity during training, and develops a novel planning algorithm to deduce the optimal parallelization of GPU devices, pipeline stages, model layers, and training data, maximizing training efficiency when stragglers exist. In addition, once a shift in the straggler situation is detected, Malleus adaptively adjusts the parallelization via a re-planning process, and seamlessly and efficiently migrates the model states on the fly, without sacrificing the stability of the training tasks. Empirical results on large language models with up to 110B parameters show that Malleus consistently outperforms existing parallel training frameworks under various straggler situations, delivering on average 2.63-5.28 times of efficiency improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13333v3</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Li, Fangcheng Fu, Hao Ge, Sheng Lin, Xuanyu Wang, Jiawen Niu, Yujie Wang, Hailin Zhang, Xiaonan Nie, Bin Cui</dc:creator>
    </item>
    <item>
      <title>EPOCH: Enabling Preemption Operation for Context Saving in Heterogeneous FPGA Systems</title>
      <link>https://arxiv.org/abs/2501.16205</link>
      <description>arXiv:2501.16205v3 Announce Type: replace 
Abstract: FPGAs are increasingly used in multi-tenant cloud environments to offload compute-intensive tasks from the main CPU. The operating system (OS) plays a vital role in identifying tasks suitable for offloading and coordinating between the CPU and FPGA for seamless task execution. The OS leverages preemption to manage CPU efficiently and balance CPU time; however, preempting tasks running on FPGAs without context loss remains challenging. Despite growing reliance on FPGAs, vendors have yet to deliver a solution that fully preserves and restores task context. This paper presents EPOCH, the first out-of-the-box framework to seamlessly preserve the state of tasks running on multi-tenant cloud FPGAs. EPOCH enables interrupting a tenant's execution at any arbitrary clock cycle, capturing its state, and saving this 'state snapshot' in off-chip memory with fine-grain granularity. Subsequently, when task resumption is required, EPOCH can resume execution from the saved 'state snapshot', eliminating the need to restart the task from scratch. EPOCH automates intricate processes, shields users from complexities, and synchronizes all underlying logic in a common clock domain, mitigating timing violations and ensuring seamless handling of interruptions. EPOCH proficiently captures the state of fundamental FPGA elements, such as look-up tables, flip-flops, block--RAMs, and digital signal processing units. On real hardware, ZynQ-XC7Z020 SoC, the proposed solution achieves context save and restore operations per frame in 62.2us and 67.4us, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16205v3</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arsalan Ali Malik, Emre Karabulut, Aydin Aysu</dc:creator>
    </item>
    <item>
      <title>Towards Enterprise-Ready Computer Using Generalist Agent</title>
      <link>https://arxiv.org/abs/2503.01861</link>
      <description>arXiv:2503.01861v2 Announce Type: replace 
Abstract: This paper presents our ongoing work toward developing an enterprise-ready Computer Using Generalist Agent (CUGA) system. Our research highlights the evolutionary nature of building agentic systems suitable for enterprise environments. By integrating state-of-the-art agentic AI techniques with a systematic approach to iterative evaluation, analysis, and refinement, we have achieved rapid and cost-effective performance gains, notably reaching a new state-of-the-art performance on the WebArena benchmark. We detail our development roadmap, the methodology and tools that facilitated rapid learning from failures and continuous system refinement, and discuss key lessons learned and future challenges for enterprise adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01861v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov, Ido Levy, Aviad Sela, Asaf Adi, Nir Mashkif</dc:creator>
    </item>
    <item>
      <title>UnifyFL: Enabling Decentralized Cross-Silo Federated Learning</title>
      <link>https://arxiv.org/abs/2504.18916</link>
      <description>arXiv:2504.18916v2 Announce Type: replace 
Abstract: Federated Learning (FL) is a decentralized machine learning (ML) paradigm in which models are trained on private data across several devices called clients and combined at a single node called an aggregator rather than aggregating the data itself. Many organizations employ FL to have better privacy-aware ML-driven decision-making capabilities. However, organizations often operate independently rather than collaborate to enhance their FL capabilities due to the lack of an effective mechanism for collaboration. The challenge lies in balancing trust and resource efficiency. One approach relies on trusting a third-party aggregator to consolidate models from all organizations (multilevel FL), but this requires trusting an entity that may be biased or unreliable. Alternatively, organizations can bypass a third party by sharing their local models directly, which requires significant computational resources for validation. Both approaches reflect a fundamental trade-off between trust and resource constraints, with neither offering an ideal solution. In this work, we develop a trust-based cross-silo FL framework called UnifyFL, which uses decentralized orchestration and distributed storage. UnifyFL provides flexibility to the participating organizations and presents synchronous and asynchronous modes to handle stragglers. Our evaluation on a diverse testbed shows that UnifyFL achieves a performance comparable to the ideal multilevel centralized FL while allowing trust and optimal use of resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18916v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarang S, Druva Dhakshinamoorthy, Aditya Shiva Sharma, Yuvraj Singh Bhadauria, Siddharth Chaitra Vivek, Arihant Bansal, Arnab K. Paul</dc:creator>
    </item>
    <item>
      <title>Grassroots Democratic Federation: Fair Governance of Large-Scale, Decentralized, Sovereign Digital Communities</title>
      <link>https://arxiv.org/abs/2505.02208</link>
      <description>arXiv:2505.02208v3 Announce Type: replace 
Abstract: Grassroots Democratic Federation aims to address the egalitarian formation and the fair democratic governance of large-scale, decentralized, sovereign digital communities, the size of the EU, the US, existing social networks, and even humanity at large. A grassroots democratic federation evolves via the grassroots formation of digital communities and their consensual federation. Such digital communities may form according to geography, jurisdiction, affiliations, relations, interests, or causes. Small communities (say up to 100 members) govern themselves; larger communities -- no matter how large -- are governed by a small assembly elected by sortition among its members. Earlier work on Grassroots Democratic Federation explored the fair sortition of the assemblies of a federation in a static setting: Given a federation, populate its assemblies with members satisfying ex ante and ex post fairness conditions on the participation of members of a community in its assembly, and on the representation of child communities in the assembly of their parent community.
  In practice, we expect a grassroots democratic federation to grow and evolve dynamically and in all directions -- bottom-up, top-down, and middle-out. To address that, we formally specify this dynamic setting and adapt the static fairness conditions to it: The ex post condition on the fair representation of a child community becomes a condition that must always hold; the ex ante conditions in expectation on the fair participation of an individual and on the fair representation of a child community become conditions satisfied in actuality in the limit, provided the federation structure eventually stabilizes. We then present a protocol that satisfies these fairness conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02208v3</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ehud Shapiro, Nimrod Talmon</dc:creator>
    </item>
    <item>
      <title>THEMIS: Time, Heterogeneity, and Energy Minded Scheduling for Fair Multi-Tenant Use in FPGAs</title>
      <link>https://arxiv.org/abs/2404.00507</link>
      <description>arXiv:2404.00507v3 Announce Type: replace-cross 
Abstract: Using correct design metrics and understanding the limitations of the underlying technology is critical to developing effective scheduling algorithms. Unfortunately, existing scheduling techniques used \emph{incorrect} metrics and had \emph{unrealistic} assumptions for fair scheduling of multi-tenant FPGAs where each tenant is aimed to share approximately the same number of resources both spatially and temporally.
  This paper introduces an enhanced fair scheduling algorithm for multi-tenant FPGA use, addressing previous metric and assumption issues, with three specific improvements claimed First, our method ensures spatiotemporal fairness by considering both spatial and temporal aspects, addressing the limitation of prior work that assumed uniform task latency. Second, we incorporate energy considerations into fairness by adjusting scheduling intervals and accounting for energy overhead, thereby balancing energy efficiency with fairness. Third, we acknowledge overlooked aspects of FPGA multi-tenancy, including heterogeneous regions and the constraints on dynamically merging/splitting partially reconfigurable regions. We develop and evaluate our improved fair scheduling algorithm with these three enhancements. Inspired by the Greek goddess of law and personification of justice, we name our fair scheduling solution THEMIS: \underline{T}ime, \underline{H}eterogeneity, and \underline{E}nergy \underline{Mi}nded \underline{S}cheduling.
  We used the Xilinx Zedboard XC7Z020 to quantify our approach's savings. Compared to previous algorithms, our improved scheduling algorithm enhances fairness between 24.2--98.4\% and allows a trade-off between 55.3$\times$ in energy vs. 69.3$\times$ in fairness. The paper thus informs cloud providers about future scheduling optimizations for fairness with related challenges and opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00507v3</guid>
      <category>cs.OS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TC.2025.3566874</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Computers Early Access May 2025</arxiv:journal_reference>
      <dc:creator>Emre Karabulut, Arsalan Ali Malik, Amro Awad, Aydin Aysu</dc:creator>
    </item>
    <item>
      <title>Unleashing the Power of Continual Learning on Non-Centralized Devices: A Survey</title>
      <link>https://arxiv.org/abs/2412.13840</link>
      <description>arXiv:2412.13840v2 Announce Type: replace-cross 
Abstract: Non-Centralized Continual Learning (NCCL) has become an emerging paradigm for enabling distributed devices such as vehicles and servers to handle streaming data from a joint non-stationary environment. To achieve high reliability and scalability in deploying this paradigm in distributed systems, it is essential to conquer challenges stemming from both spatial and temporal dimensions, manifesting as distribution shifts, catastrophic forgetting, heterogeneity, and privacy issues. This survey focuses on a comprehensive examination of the development of the non-centralized continual learning algorithms and the real-world deployment across distributed devices. We begin with an introduction to the background and fundamentals of non-centralized learning and continual learning. Then, we review existing solutions from three levels to represent how existing techniques alleviate the catastrophic forgetting and distribution shift. Additionally, we delve into the various types of heterogeneity issues, security, and privacy attributes, as well as real-world applications across three prevalent scenarios. Furthermore, we establish a large-scale benchmark to revisit this problem and analyze the performance of the state-of-the-art NCCL approaches. Finally, we discuss the important challenges and future research directions in NCCL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13840v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichen Li, Haozhao Wang, Wenchao Xu, Tianzhe Xiao, Hong Liu, Minzhu Tu, Yuying Wang, Xin Yang, Rui Zhang, Shui Yu, Song Guo, Ruixuan Li</dc:creator>
    </item>
    <item>
      <title>Unveiling the Mechanisms of DAI: A Logic-Based Approach to Stablecoin Analysis</title>
      <link>https://arxiv.org/abs/2412.15814</link>
      <description>arXiv:2412.15814v3 Announce Type: replace-cross 
Abstract: Stablecoins are digital assets designed to maintain a stable value, typically pegged to traditional currencies. Despite their growing prominence, many stablecoins have struggled to consistently meet stability expectations, and their underlying mechanisms often remain opaque and challenging to analyze. This paper focuses on the DAI stablecoin, which combines crypto-collateralization and algorithmic mechanisms. We propose a formal logic-based framework for representing the policies and operations of DAI, implemented in Prolog and released as open-source software. Our framework enables detailed analysis and simulation of DAI's stability mechanisms, providing a foundation for understanding its robustness and identifying potential vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15814v3</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LO</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco De Sclavis, Giuseppe Galano, Aldo Glielmo, Matteo Nardelli</dc:creator>
    </item>
    <item>
      <title>Optimal Distributed Replacement Paths</title>
      <link>https://arxiv.org/abs/2502.15378</link>
      <description>arXiv:2502.15378v3 Announce Type: replace-cross 
Abstract: We study the replacement paths problem in the $\mathsf{CONGEST}$ model of distributed computing. Given an $s$-$t$ shortest path $P$, the goal is to compute, for every edge $e$ in $P$, the shortest-path distance from $s$ to $t$ avoiding $e$. For unweighted directed graphs, we establish the tight randomized round complexity bound for this problem as $\widetilde{\Theta}(n^{2/3} + D)$ by showing matching upper and lower bounds. Our upper bound extends to $(1+\epsilon)$-approximation for weighted directed graphs. Our lower bound applies even to the second simple shortest path problem, which asks only for the smallest replacement path length. These results improve upon the very recent work of Manoharan and Ramachandran (SIROCCO 2024), who showed a lower bound of $\widetilde{\Omega}(n^{1/2} + D)$ and an upper bound of $\widetilde{O}(n^{2/3} + \sqrt{n h_{st}} + D)$, where $h_{st}$ is the number of hops in the given $s$-$t$ shortest path $P$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15378v3</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi-Jun Chang, Yanyu Chen, Dipan Dey, Gopinath Mishra, Hung Thuan Nguyen, Bryce Sanchez</dc:creator>
    </item>
    <item>
      <title>SemSpaceFL: A Collaborative Hierarchical Federated Learning Framework for Semantic Communication in 6G LEO Satellites</title>
      <link>https://arxiv.org/abs/2505.00966</link>
      <description>arXiv:2505.00966v2 Announce Type: replace-cross 
Abstract: The advent of the sixth-generation (6G) wireless networks, enhanced by artificial intelligence, promises ubiquitous connectivity through Low Earth Orbit (LEO) satellites. These satellites are capable of collecting vast amounts of geographically diverse and real-time data, which can be immensely valuable for training intelligent models. However, limited inter-satellite communication and data privacy constraints hinder data collection on a single server for training. Therefore, we propose SemSpaceFL, a novel hierarchical federated learning (HFL) framework for LEO satellite networks, with integrated semantic communication capabilities. Our framework introduces a two-tier aggregation architecture where satellite models are first aggregated at regional gateways before final consolidation at a cloud server, which explicitly accounts for satellite mobility patterns and energy constraints. The key innovation lies in our novel aggregation approach, which dynamically adjusts the contribution of each satellite based on its trajectory and association with different gateways, which ensures stable model convergence despite the highly dynamic nature of LEO constellations. To further enhance communication efficiency, we incorporate semantic encoding-decoding techniques trained through the proposed HFL framework, which enables intelligent data compression while maintaining signal integrity. Our experimental results demonstrate that the proposed aggregation strategy achieves superior performance and faster convergence compared to existing benchmarks, while effectively managing the challenges of satellite mobility and energy limitations in dynamic LEO networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00966v2</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Loc X. Nguyen, Sheikh Salman Hassan, Yu Min Park, Yan Kyaw Tun, Zhu Han, Choong Seon Hong</dc:creator>
    </item>
  </channel>
</rss>
