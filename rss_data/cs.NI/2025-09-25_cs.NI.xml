<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Sep 2025 01:39:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Radio Propagation Modelling: To Differentiate or To Deep Learn, That Is The Question</title>
      <link>https://arxiv.org/abs/2509.19337</link>
      <description>arXiv:2509.19337v1 Announce Type: new 
Abstract: Differentiable ray tracing has recently challenged the status quo in radio propagation modelling and digital twinning. Promising unprecedented speed and the ability to learn from real-world data, it offers a real alternative to conventional deep learning (DL) models. However, no experimental evaluation on production-grade networks has yet validated its assumed scalability or practical benefits. This leaves mobile network operators (MNOs) and the research community without clear guidance on its applicability. In this paper, we fill this gap by employing both differentiable ray tracing and DL models to emulate radio coverage using extensive real-world data collected from the network of a major MNO, covering 13 cities and more than 10,000 antennas. Our results show that, while differentiable ray-tracing simulators have contributed to reducing the efficiency-accuracy gap, they struggle to generalize from real-world data at a large scale, and they remain unsuitable for real-time applications. In contrast, DL models demonstrate higher accuracy and faster adaptation than differentiable ray-tracing simulators across urban, suburban, and rural deployments, achieving accuracy gains of up to 3 dB. Our experimental results aim to provide timely insights into a fundamental open question with direct implications on the wireless ecosystem and future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19337v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stefanos Bakirtzis, Paul Almasan, Jos\'e Su\'arez-Varela, Gabriel O. Ferreira, Michail Kalntis, Andr\'e Felipe Zanella, Ian Wassell, Andra Lutu</dc:creator>
    </item>
    <item>
      <title>Fine-Grained AI Model Caching and Downloading With Coordinated Multipoint Broadcasting in Multi-Cell Edge Networks</title>
      <link>https://arxiv.org/abs/2509.19341</link>
      <description>arXiv:2509.19341v1 Announce Type: new 
Abstract: 6G networks are envisioned to support on-demand AI model downloading to accommodate diverse inference requirements of end users. By proactively caching models at edge nodes, users can retrieve the requested models with low latency for on-device AI inference. However, the substantial size of contemporary AI models poses significant challenges for edge caching under limited storage capacity, as well as for the concurrent delivery of heterogeneous models over wireless channels. To address these challenges, we propose a fine-grained AI model caching and downloading system that exploits parameter reusability, stemming from the common practice of fine-tuning task-specific models from a shared pre-trained model with frozen parameters. This system selectively caches model parameter blocks (PBs) at edge nodes, eliminating redundant storage of reusable parameters across different cached models. Additionally, it incorporates coordinated multipoint (CoMP) broadcasting to simultaneously deliver reusable PBs to multiple users, thereby enhancing downlink spectrum utilization. Under this arrangement, we formulate a model downloading delay minimization problem to jointly optimize PB caching, migration (among edge nodes), and broadcasting beamforming. To tackle this intractable problem, we develop a distributed multi-agent learning framework that enables edge nodes to explicitly learn mutual influence among their actions, thereby facilitating cooperation. Furthermore, a data augmentation approach is proposed to adaptively generate synthetic training samples through a predictive model, boosting sample efficiency and accelerating policy learning. Both theoretical analysis and simulation experiments validate the superior convergence performance of the proposed learning framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19341v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Fu, Peng Qin, Yueyue Zhang, Yifei Wang</dc:creator>
    </item>
    <item>
      <title>TinyAC: Bringing Autonomic Computing Principles to Resource-Constrained Systems</title>
      <link>https://arxiv.org/abs/2509.19350</link>
      <description>arXiv:2509.19350v1 Announce Type: new 
Abstract: Autonomic Computing (AC) is a promising approach for developing intelligent and adaptive self-management systems at the deep network edge. In this paper, we present the problems and challenges related to the use of AC for IoT devices. Our proposed hybrid approach bridges bottom-up intelligence (TinyML and on-device learning) and top-down guidance (LLMs) to achieve a scalable and explainable approach for developing intelligent and adaptive self-management tiny systems. Moreover, we argue that TinyAC systems require self-adaptive features to handle problems that may occur during their operation. Finally, we identify gaps, discuss existing challenges and future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19350v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wojciech Kalka, Ruitao Xue, Kamil Faber, Aleksander Slominski, Devki Jha, Rajiv Ranjan, Tomasz Szydlo</dc:creator>
    </item>
    <item>
      <title>A User-to-User Resource Reselling Game in Open RAN with Buffer Rollover</title>
      <link>https://arxiv.org/abs/2509.19392</link>
      <description>arXiv:2509.19392v1 Announce Type: new 
Abstract: The development of the Open RAN (O-RAN) framework helps enable network slicing through its virtualization, interoperability, and flexibility. To improve spectral efficiency and better meet users' dynamic and heterogeneous service demands, O-RAN's flexibility further presents an opportunity for resource reselling of unused physical resource blocks (PRBs) across users. In this work, we propose a novel game-based user-to-user PRB reselling model in the O-RAN setting, which models the carryover of unmet demand across time slots, along with how users' internal buffer states relate to any PRBs purchased. We formulate the interplay between the users as a strategic game, with each participant aiming to maximize their own payoffs, and we prove the existence and uniqueness of the Nash equilibrium (NE) in the game. We furthermore propose an iterative bidding mechanism that converges to this NE. Extensive simulations show that our best approach reduces data loss by 30.5% and spectrum resource wastage by 50.7% while significantly improving social welfare, compared to its absence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19392v1</guid>
      <category>cs.NI</category>
      <category>cs.GT</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruide Cao, Marie Siew, David Yau</dc:creator>
    </item>
    <item>
      <title>FedOC: Multi-Server FL with Overlapping Client Relays in Wireless Edge Networks</title>
      <link>https://arxiv.org/abs/2509.19398</link>
      <description>arXiv:2509.19398v1 Announce Type: new 
Abstract: Multi-server Federated Learning (FL) has emerged as a promising solution to mitigate communication bottlenecks of single-server FL. We focus on a typical multi-server FL architecture, where the regions covered by different edge servers (ESs) may overlap. A key observation of this architecture is that clients located in the overlapping areas can access edge models from multiple ESs. Building on this insight, we propose FedOC (Federated learning with Overlapping Clients), a novel framework designed to fully exploit the potential of these overlapping clients. In FedOC, overlapping clients could serve dual roles: (1) as Relay Overlapping Clients (ROCs), they forward edge models between neighboring ESs in real time to facilitate model sharing among different ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically select their initial model for local training based on the edge model delivery time, which enables indirect data fusion among different regions of ESs. The overall FedOC workflow proceeds as follows: in every round, each client trains local model based on the earliest received edge model and transmits to the respective ESs for model aggregation. Then each ES transmits the aggregated edge model to neighboring ESs through ROC relaying. Upon receiving the relayed models, each ES performs a second aggregation and subsequently broadcasts the updated model to covered clients. The existence of ROCs enables the model of each ES to be disseminated to the other ESs in a decentralized manner, which indirectly achieves intercell model and speeding up the training process, making it well-suited for latency-sensitive edge environments. Extensive experimental results show remarkable performance gains of our scheme compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19398v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yun Ji, Zeyu Chen, Xiaoxiong Zhong, Yanan Ma, Sheng Zhang, Yuguang Fang</dc:creator>
    </item>
    <item>
      <title>Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile Data Augmentation</title>
      <link>https://arxiv.org/abs/2509.19405</link>
      <description>arXiv:2509.19405v1 Announce Type: new 
Abstract: Accurate outdoor positioning in cellular networks is hindered by sparse, heterogeneous measurement collections and the high cost of exhaustive site surveys. This paper introduces a lightweight, modular mobile data augmentation framework designed to enhance multi-cell fingerprinting-based positioning using operator-collected minimization of drive test (MDT) records. The proposed approach decouples spatial and radio-feature synthesis: kernel density estimation (KDE) models the empirical spatial distribution to generate geographically coherent synthetic locations, while a k-nearest-neighbor (KNN)-based block produces augmented per-cell radio fingerprints. The architecture is intentionally training-free, interpretable, and suitable for distributed or on-premise operator deployments, supporting privacy-aware workflows. We both validate each augmentation module independently and assess its end-to-end impact on fingerprinting-based positioning using a real-world MDT dataset provided by an Italian mobile network operator across diverse urban and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation consistently improves positioning performance, with the largest benefits in sparsely sampled or structurally complex regions; we also observe region-dependent saturation effects as augmentation increases. The framework offers a practical, low-complexity path to enhance operator positioning services using existing mobile data traces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19405v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tony Chahoud, Lorenzo Mario Amorosa, Riccardo Marini, Luca De Nardis</dc:creator>
    </item>
    <item>
      <title>Poster: ChatIYP: Enabling Natural Language Access to the Internet Yellow Pages Database</title>
      <link>https://arxiv.org/abs/2509.19411</link>
      <description>arXiv:2509.19411v1 Announce Type: new 
Abstract: The Internet Yellow Pages (IYP) aggregates information from multiple sources about Internet routing into a unified, graph-based knowledge base. However, querying it requires knowledge of the Cypher language and the exact IYP schema, thus limiting usability for non-experts. In this paper, we propose ChatIYP, a domain-specific Retrieval-Augmented Generation (RAG) system that enables users to query IYP through natural language questions. Our evaluation demonstrates solid performance on simple queries, as well as directions for improvement, and provides insights for selecting evaluation metrics that are better fit for IYP querying AI agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19411v1</guid>
      <category>cs.NI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasilis Andritsoudis, Pavlos Sermpezis, Ilias Dimitriadis, Athena Vakali</dc:creator>
    </item>
    <item>
      <title>Where 6G Stands Today: Evolution, Enablers, and Research Gaps</title>
      <link>https://arxiv.org/abs/2509.19646</link>
      <description>arXiv:2509.19646v1 Announce Type: new 
Abstract: As the fifth-generation (5G) mobile communication system continues its global deployment, both industry and academia have started conceptualizing the 6th generation (6G) to address the growing need for a progressively advanced and digital society. Even while 5G offers considerable advancements over LTE, it could struggle to be sufficient to meet all of the requirements, including ultra-high reliability, seamless automation, and ubiquitous coverage. In response, 6G is supposed to bring out a highly intelligent, automated, and ultra-reliable communication system that can handle a vast number of connected devices. This paper offers a comprehensive overview of 6G, beginning with its main stringent requirements while focusing on key enabling technologies such as terahertz (THz) communications, intelligent reflecting surfaces, massive MIMO and AI-driven networking that will shape the 6G networks. Furthermore, the paper lists various 6G applications and usage scenarios that will benefit from these advancements. At the end, we outline the potential challenges that must be addressed to achieve the 6G promises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19646v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Salma Tika, Abdelkrim Haqiq, Essaid Sabir, Elmahdi Driouch</dc:creator>
    </item>
    <item>
      <title>RIS-assisted Data Collection and Wireless Power Transfer in Low-altitude Wireless Networks</title>
      <link>https://arxiv.org/abs/2509.19651</link>
      <description>arXiv:2509.19651v1 Announce Type: new 
Abstract: Low-altitude wireless networks (LAWNs) have become effective solutions for collecting data from low-power Internet-of-Things devices (IoTDs) in remote areas with limited communication infrastructure. However, some outdoor IoTDs deployed in such areas face both energy constraints and low-channel quality challenges, making it challenging to ensure timely data collection from these IoTDs in LAWNs. In this work, we investigate a reconfigurable intelligent surface (RIS)-assisted uncrewed aerial vehicle (UAV)-enabled data collection and wireless power transfer system in LAWN. Specifically, IoTDs first harvest energy from a low-altitude UAV, and then upload their data to the UAV by applying the time division multiple access (TDMA) protocol, supported by an RIS to improve the channel quality. To maintain satisfactory data freshness of the IoTDs and save energy for an energy-constrained UAV, we aim to minimize the age of information (AoI) and energy consumption of the UAV by jointly optimizing the RIS phase shits, UAV trajectory, charging time allocation, and binary IoTD scheduling. We propose a deep reinforcement learning (DRL)-based approach, namely the alternating optimization-improved parameterized deep Q-network (AO-IPDQN). Specifically, considering that RIS typically contains a large number of reflecting elements, we first adopt an alternating optimization (AO) method to optimize the RIS phase shifts to reduce the dimension of the action space. Then, we propose the improved parameterized deep Q-network (IPDQN) method to deal with the hybrid action space. Simulation results indicate that AO-IPDQN approach achieves excellent performance relative to multiple comparison methods across various simulation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19651v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenwen Xie, Geng Sun, Jiahui Li, Jiacheng Wang, Yinqiu Liu, Dusit Niyato, Dong In Kim, Shiwen Mao</dc:creator>
    </item>
    <item>
      <title>Games Are Not Equal: Classifying Cloud Gaming Contexts for Effective User Experience Measurement</title>
      <link>https://arxiv.org/abs/2509.19669</link>
      <description>arXiv:2509.19669v1 Announce Type: new 
Abstract: To tap into the growing market of cloud gaming, whereby game graphics is rendered in the cloud and streamed back to the user as a video feed, network operators are creating monetizable assurance services that dynamically provision network resources. However, without accurately measuring cloud gaming user experience, they cannot assess the effectiveness of their provisioning methods. Basic measures such as bandwidth and frame rate by themselves do not suffice, and can only be interpreted in the context of the game played and the player activity within the game. This paper equips the network operator with a method to obtain a real-time measure of cloud gaming experience by analyzing network traffic, including contextual factors such as the game title and player activity stage. Our method is able to classify the game title within the first five seconds of game launch, and continuously assess the player activity stage as being active, passive, or idle. We deploy it in an ISP hosting NVIDIA cloud gaming servers for the region. We provide insights from hundreds of thousands of cloud game streaming sessions over a three-month period into the dependence of bandwidth consumption and experience level on the gameplay contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19669v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3730567.3764455</arxiv:DOI>
      <dc:creator>Yifan Wang, Minzhao Lyu, Vijay Sivaraman</dc:creator>
    </item>
    <item>
      <title>SPARQ: An Optimization Framework for the Distribution of AI-Intensive Applications under Non-Linear Delay Constraints</title>
      <link>https://arxiv.org/abs/2509.19913</link>
      <description>arXiv:2509.19913v1 Announce Type: new 
Abstract: Next-generation real-time compute-intensive applications, such as extended reality, multi-user gaming, and autonomous transportation, are increasingly composed of heterogeneous AI-intensive functions with diverse resource requirements and stringent latency constraints. While recent advances have enabled very efficient algorithms for joint service placement, routing, and resource allocation for increasingly complex applications, current models fail to capture the non-linear relationship between delay and resource usage that becomes especially relevant in AI-intensive workloads. In this paper, we extend the cloud network flow optimization framework to support queuing-delay-aware orchestration of distributed AI applications over edge-cloud infrastructures. We introduce two execution models, Guaranteed-Resource (GR) and Shared-Resource (SR), that more accurately capture how computation and communication delays emerge from system-level resource constraints. These models incorporate M/M/1 and M/G/1 queue dynamics to represent dedicated and shared resource usage, respectively. The resulting optimization problem is non-convex due to the non-linear delay terms. To overcome this, we develop SPARQ, an iterative approximation algorithm that decomposes the problem into two convex sub-problems, enabling joint optimization of service placement, routing, and resource allocation under nonlinear delay constraints. Simulation results demonstrate that the SPARQ not only offers a more faithful representation of system delays, but also substantially improves resource efficiency and the overall cost-delay tradeoff compared to existing state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19913v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pietro Spadaccino, Paolo Di Lorenzo, Sergio Barbarossa, Antonia M. Tulino, Jaime Llorca</dc:creator>
    </item>
    <item>
      <title>A Novel Short-Term Anomaly Prediction for IIoT with Software Defined Twin Network</title>
      <link>https://arxiv.org/abs/2509.20068</link>
      <description>arXiv:2509.20068v1 Announce Type: new 
Abstract: Secure monitoring and dynamic control in an IIoT environment are major requirements for current development goals. We believe that dynamic, secure monitoring of the IIoT environment can be achieved through integration with the Software-Defined Network (SDN) and Digital Twin (DT) paradigms. The current literature lacks implementation details for SDN-based DT and time-aware intelligent model training for short-term anomaly detection against IIoT threats. Therefore, we have proposed a novel framework for short-term anomaly detection that uses an SDN-based DT. Using a comprehensive dataset, time-aware labeling of features, and a comprehensive evaluation of various machine learning models, we propose a novel SD-TWIN-based anomaly detection algorithm. According to the performance of a new real-time SD-TWIN deployment, the GPU- accelerated LightGBM model is particularly effective, achieving a balance of high recall and strong classification performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20068v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilal Dalgic (Manisa Celal Bayar University, Turkey), Betul Sen (Manisa Celal Bayar University, Turkey), Muge Erel-Ozcevik (Manisa Celal Bayar University, Turkey)</dc:creator>
    </item>
    <item>
      <title>Can LLMs Forecast Internet Traffic from Social Media?</title>
      <link>https://arxiv.org/abs/2509.20123</link>
      <description>arXiv:2509.20123v1 Announce Type: new 
Abstract: Societal events shape the Internet's behavior. The death of a prominent public figure, a software launch, or a major sports match can trigger sudden demand surges that overwhelm peering points and content delivery networks. Although these events fall outside regular traffic patterns, forecasting systems still rely solely on those patterns and therefore miss these critical anomalies.
  Thus, we argue for socio-technical systems that supplement technical measurements with an active understanding of the underlying drivers, including how events and collective behavior shape digital demands. We propose traffic forecasting using signals from public discourse, such as headlines, forums, and social media, as early demand indicators.
  To validate our intuition, we present a proof-of-concept system that autonomously scrapes online discussions, infers real-world events, clusters and enriches them semantically, and correlates them with traffic measurements at a major Internet Exchange Point. This prototype predicted between 56-92% of society-driven traffic spikes after scraping a moderate amount of online discussions.
  We believe this approach opens new research opportunities in cross-domain forecasting, scheduling, demand anticipation, and society-informed decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20123v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonatan Langlet, Mariano Scazzariello, Flavio Luciani, Marta Burocchi, Dejan Kosti\'c, Marco Chiesa</dc:creator>
    </item>
    <item>
      <title>A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks</title>
      <link>https://arxiv.org/abs/2509.19306</link>
      <description>arXiv:2509.19306v1 Announce Type: cross 
Abstract: Edge intelligence has emerged as a promising strategy to deliver low-latency and ubiquitous services for mobile devices. Recent advances in fine-tuning mechanisms of foundation models have enabled edge intelligence by integrating low-rank adaptation (LoRA) with federated learning. However, in wireless networks, the device heterogeneity and resource constraints on edge devices pose great threats to the performance of federated fine-tuning. To tackle these issues, we propose to optimize federated fine-tuning in heterogenous wireless networks via online learning. First, the framework of switching-based federated fine-tuning in wireless networks is provided. The edge devices switches to LoRA modules dynamically for federated fine-tuning with base station to jointly mitigate the impact of device heterogeneity and transmission unreliability. Second, a tractable upper bound on the inference risk gap is derived based on theoretical analysis. To improve the generalization capability, we formulate a non-convex mixed-integer programming problem with long-term constraints, and decouple it into model switching, transmit power control, and bandwidth allocation subproblems. An online optimization algorithm is developed to solve the problems with polynomial computational complexity. Finally, the simulation results on the SST-2 and QNLI data sets demonstrate the performance gains in test accuracy and energy efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19306v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyi Wang, Zhongyuan Zhao, Qingtian Wang, Zexu Li, Yue Wang, Tony Q. S. Quek</dc:creator>
    </item>
    <item>
      <title>Joint Channel Estimation and Computation Offloading in Fluid Antenna-assisted MEC Networks</title>
      <link>https://arxiv.org/abs/2509.19340</link>
      <description>arXiv:2509.19340v1 Announce Type: cross 
Abstract: With the emergence of fluid antenna (FA) in wireless communications, the capability to dynamically adjust port positions offers substantial benefits in spatial diversity and spectrum efficiency, which are particularly valuable for mobile edge computing (MEC) systems. Therefore, we propose an FA-assisted MEC offloading framework to minimize system delay. This framework faces two severe challenges, which are the complexity of channel estimation due to dynamic port configuration and the inherent non-convexity of the joint optimization problem. Firstly, we propose Information Bottleneck Metric-enhanced Channel Compressed Sensing (IBM-CCS), which advances FA channel estimation by integrating information relevance into the sensing process and capturing key features of FA channels effectively. Secondly, to address the non-convex and high-dimensional optimization problem in FA-assisted MEC systems, which includes FA port selection, beamforming, power control, and resource allocation, we propose a game theory-assisted Hierarchical Twin-Dueling Multi-agent Algorithm (HiTDMA) based offloading scheme, where the hierarchical structure effectively decouples and coordinates the optimization tasks between the user side and the base station side. Crucially, the game theory effectively reduces the dimensionality of power control variables, allowing deep reinforcement learning (DRL) agents to achieve improved optimization efficiency. Numerical results confirm that the proposed scheme significantly reduces system delay and enhances offloading performance, outperforming benchmarks. Additionally, the IBM-CCS channel estimation demonstrates superior accuracy and robustness under varying port densities, contributing to efficient communication under imperfect CSI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19340v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Ju, Mingdong Li, Haoyu Wang, Lei Liu, Youyang Qu, Mianxiong Dong, Victor C. M. Leung, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>InterQnet: A Heterogeneous Full-Stack Approach to Co-designing Scalable Quantum Networks</title>
      <link>https://arxiv.org/abs/2509.19503</link>
      <description>arXiv:2509.19503v1 Announce Type: cross 
Abstract: Quantum communications have progressed significantly, moving from a theoretical concept to small-scale experiments to recent metropolitan-scale demonstrations. As the technology matures, it is expected to revolutionize quantum computing in much the same way that classical networks revolutionized classical computing. Quantum communications will also enable breakthroughs in quantum sensing, metrology, and other areas. However, scalability has emerged as a major challenge, particularly in terms of the number and heterogeneity of nodes, the distances between nodes, the diversity of applications, and the scale of user demand. This paper describes InterQnet, a multidisciplinary project that advances scalable quantum communications through a comprehensive approach that improves devices, error handling, and network architecture. InterQnet has a two-pronged strategy to address scalability challenges: InterQnet-Achieve focuses on practical realizations of heterogeneous quantum networks by building and then integrating first-generation quantum repeaters with error mitigation schemes and centralized automated network control systems. The resulting system will enable quantum communications between two heterogeneous quantum platforms through a third type of platform operating as a repeater node. InterQnet-Scale focuses on a systems study of architectural choices for scalable quantum networks by developing forward-looking models of quantum network devices, advanced error correction schemes, and entanglement protocols. Here we report our current progress toward achieving our scalability goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19503v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joaquin Chung, Daniel Dilley, Ely Eastman, Alvin Gonzales, Kara Hokenstad, Md Shariful Islam, Varun Jorapur, Joseph Petrullo, Andy C. Y. Li, Bikun Li, Vasileios Niaouris, Anirudh Ramesh, Ansh Singal, Caitao Zhan, Michael Bishof, Eric Chitambar, Jacob P. Covey, Alan Dibos, Xu Han, Liang Jiang, Prem Kumar, Jeffrey Larson, Zain H. Saleem, Rajkumar Kettimuthu</dc:creator>
    </item>
    <item>
      <title>To Stream or Not to Stream: Towards A Quantitative Model for Remote HPC Processing Decisions</title>
      <link>https://arxiv.org/abs/2509.19532</link>
      <description>arXiv:2509.19532v1 Announce Type: cross 
Abstract: Modern scientific instruments generate data at rates that increasingly exceed local compute capabilities and, when paired with the staging and I/O overheads of file-based transfers, also render file-based use of remote HPC resources impractical for time-sensitive analysis and experimental steering. Real-time streaming frameworks promise to reduce latency and improve system efficiency, but lack a principled way to assess their feasibility. In this work, we introduce a quantitative framework and an accompanying Streaming Speed Score to evaluate whether remote high-performance computing (HPC) resources can provide timely data processing compared to local alternatives. Our model incorporates key parameters including data generation rate, transfer efficiency, remote processing power, and file input/output overhead to compute total processing completion time and identify operational regimes where streaming is beneficial. We motivate our methodology with use cases from facilities such as APS, FRIB, LCLS-II, and the LHC, and validate our approach through an illustrative case study based on LCLS-II data. Our measurements show that streaming can achieve up to 97% lower end-to-end completion time than file-based methods under high data rates, while worst-case congestion can increase transfer times by over an order of magnitude, underscoring the importance of tail latency in streaming feasibility decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19532v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flavio Castro, Weijian Zheng, Joaquin Chung, Ian Foster, Rajkumar Kettimuthu</dc:creator>
    </item>
    <item>
      <title>BALANCE: Bitrate-Adaptive Limit-Aware Netcast Content Enhancement Utilizing QUBO and Quantum Annealing</title>
      <link>https://arxiv.org/abs/2509.19616</link>
      <description>arXiv:2509.19616v1 Announce Type: cross 
Abstract: In an era of increasing data cap constraints, optimizing video streaming quality while adhering to user-defined data caps remains a significant challenge. This paper introduces Bitrate-Adaptive Limit-Aware Netcast Content Enhancement (BALANCE), a novel Quantum framework aimed at addressing this issue. BALANCE intelligently pre-selects video segments based on visual complexity and anticipated data consumption, utilizing the Video Multimethod Assessment Fusion (VMAF) metric to enhance Quality of Experience (QoE). We compare our method against traditional bitrate ladders used in Adaptive Bitrate (ABR) streaming, demonstrating a notable improvement in QoE under equivalent data constraints. We compare the Slack variable approach with the Dynamic Penalization Approach (DPA) by framing the bitrate allocation problem through Quadratic Unconstrained Binary Optimization (QUBO) to effectively enforce data limits. Our results indicate that the DPA consistently outperforms the Slack Variable Method, delivering more valid and optimal solutions as data limits increase. This new quantum approach significantly enhances streaming satisfaction for users with limited data plans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19616v1</guid>
      <category>eess.IV</category>
      <category>cs.MM</category>
      <category>cs.NI</category>
      <category>quant-ph</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/WCNC61545.2025.10978311</arxiv:DOI>
      <arxiv:journal_reference>Proc. 2025 IEEE Wireless Communications and Networking Conference (WCNC), 2025, pp. 1-6</arxiv:journal_reference>
      <dc:creator>Animesh Rajpurohit, Michael Kelley, Wei Wang, Krishna Murthy Kattiyan Ramamoorthy</dc:creator>
    </item>
    <item>
      <title>A Deep Dive into the Impact of Solar Storms on LEO Satellite Networks</title>
      <link>https://arxiv.org/abs/2509.19647</link>
      <description>arXiv:2509.19647v1 Announce Type: cross 
Abstract: Low Earth Orbit (LEO) satellite networks are an important part of the global communication infrastructure today. Despite ongoing efforts to improve their resilience, they remain vulnerable to component damage and deorbiting under harsh space weather conditions. Prior work identified a modest but noticeable impact on LEO satellite network performance during solar storms, typically manifesting as an immediate rise in packet loss and a sustained increase in round-trip time (RTT). However, these studies offer only coarse-grained insights and do not capture the nuanced spatial and temporal patterns of disruption across the LEO network.
  In this paper, we conduct a deep dive into the impact of solar storms on LEO satellite communications. By localizing the impact of increased atmospheric drag at the level of individual satellites and orbits, we reveal significant heterogeneity in how different parts of the network are affected. We find that the degree of performance degradation varies significantly across geographic regions, depending on satellite positioning during the storm. Specifically, we find that (i) not all satellite orbits are equally vulnerable, (ii) within a given orbit, certain satellites experience disproportionate impact depending on their position relative to geomagnetic conditions, and (iii) autonomous maneuvering of satellites might be a cause of the sustained increase in RTT. Our findings uncover previously overlooked patterns of vulnerability in LEO satellite constellations and highlight the need for more adaptive, region-aware mitigation strategies to address space weather-induced network disruptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19647v1</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>astro-ph.SR</category>
      <category>cs.NI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3748749.3749094</arxiv:DOI>
      <dc:creator>Eunju Kang, Alagappan Ramanathan, Sangeetha Abdu Jyothi</dc:creator>
    </item>
    <item>
      <title>CollaPipe: Adaptive Segment-Optimized Pipeline Parallelism for Collaborative LLM Training in Heterogeneous Edge Networks</title>
      <link>https://arxiv.org/abs/2509.19855</link>
      <description>arXiv:2509.19855v1 Announce Type: cross 
Abstract: The increasing demand for intelligent mobile applications has made multi-agent collaboration with Transformer-based large language models (LLMs) essential in mobile edge computing (MEC) networks. However, training LLMs in such environments remains challenging due to heavy computation, high end-to-end latency, and limited model generalization. We introduce CollaPipe, a hybrid distributed learning framework that integrates collaborative pipeline parallelism with federated aggregation to support self-evolving intelligent networks. In CollaPipe, the encoder part is adaptively partitioned into variable-sized segments and deployed across mobile devices for pipeline-parallel training, while the decoder is deployed on edge servers to handle generative tasks. Then we perform global model update via federated aggregation. To enhance training efficiency, we formulate a joint optimization problem that adaptively allocates model segments, micro-batches, bandwidth, and transmission power. We derive and use a closed-form convergence bound to design an Dynamic Segment Scheduling and Resource Allocation (DSSDA) algorithm based on Lyapunov optimization, ensuring system stability under long-term constraints. Extensive experiments on downstream tasks with Transformer and BERT models show that CollaPipe improves computation efficiency by up to 15.09%, reduces end-to-end latency by at least 48.98%, and cuts single device memory usage by more than half, enabling online learning in heterogeneous and dynamic communication environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19855v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiewei Chen, Xiumei Deng, Zehui Xiong, Shaoyong Guo, Xuesong Qiu, Ping Wang, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>Joint Ex-Post Location Calibration and Radio Map Construction under Biased Positioning Errors</title>
      <link>https://arxiv.org/abs/2509.20059</link>
      <description>arXiv:2509.20059v1 Announce Type: cross 
Abstract: This paper proposes a high-accuracy radio map construction method tailored for environments where location information is affected by bursty errors. Radio maps are an effective tool for visualizing wireless environments. Although extensive research has been conducted on accurate radio map construction, most existing approaches assume noise-free location information during sensing. In practice, however, positioning errors ranging from a few to several tens of meters can arise due to device-based positioning systems (e.g., GNSS). Ignoring such errors during inference can lead to significant degradation in radio map accuracy. This study highlights that these errors often tend to be biased when using mobile devices as sensors. We introduce a novel framework that models these errors together with spatial correlation in radio propagation by embedding them as tunable parameters in the marginal log-likelihood function. This enables ex-post calibration of location uncertainty during radio map construction. Numerical results based on practical human mobility data demonstrate that the proposed method can limit RMSE degradation to approximately 0.25-0.29 dB, compared with Gaussian process regression using noise-free location data, whereas baseline methods suffer performance losses exceeding 1 dB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20059v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koki Kanzaki (The University of Electro-Communications), Koya Sato (The University of Electro-Communications)</dc:creator>
    </item>
    <item>
      <title>Enabling immersive experiences in challenging network conditions</title>
      <link>https://arxiv.org/abs/2304.03732</link>
      <description>arXiv:2304.03732v2 Announce Type: replace 
Abstract: Immersive experiences, such as remote collaboration and augmented and virtual reality, require delivery of large volumes of data with consistent ultra-low latency across wireless networks in fluctuating network conditions. We describe the high-level design behind a data delivery solution that meets these requirements and provide synthetic simulations and test results running in network conditions based on real-world measurements demonstrating the efficacy of the solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03732v2</guid>
      <category>cs.NI</category>
      <category>cs.MM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pooja Aggarwal, Michael Luby, Lorenz Minder</dc:creator>
    </item>
    <item>
      <title>InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps</title>
      <link>https://arxiv.org/abs/2507.23177</link>
      <description>arXiv:2507.23177v2 Announce Type: replace 
Abstract: Ultra-dense fifth generation (5G) and beyond networks leverage spectrum sharing and frequency reuse to enhance throughput, but face unpredictable in-band uplink (UL) interference challenges that significantly degrade Signal to Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases (gNBs). This is particularly problematic at cell edges, where overlapping regions force User Equipments (UEs) to increase transmit power, and in directional millimeter wave systems, where beamforming sidelobes can create unexpected interference. The resulting signal degradation disrupts protocol operations, including scheduling and resource allocation, by distorting quality indicators like Reference Signal Received Power (RSRP) and Received Signal Strength Indicator (RSSI), and can compromise critical functions such as channel state reporting and Hybrid Automatic Repeat Request (HARQ) acknowledgments. To address this problem, this article introduces InterfO-RAN, a real-time programmable solution that leverages a Convolutional Neural Network (CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical layer, detecting in-band interference with accuracy exceeding 91% in under 650 us. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics Processing Unit (GPU), coexisting with the 5G NR physical layer processing of NVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial Radio Units (RUs) and smartphones, our solution was trained and tested on more than 7 million NR UL slots collected from real-world environments, demonstrating robust interference detection capabilities essential for maintaining network performance in dense deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23177v2</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neagin Neasamoni Santhi, Davide Villa, Michele Polese, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>RG-Attn: Radian Glue Attention for Multi-modality Multi-agent Cooperative Perception</title>
      <link>https://arxiv.org/abs/2501.16803</link>
      <description>arXiv:2501.16803v3 Announce Type: replace-cross 
Abstract: Cooperative perception enhances autonomous driving by leveraging Vehicle-to-Everything (V2X) communication for multi-agent sensor fusion. However, most existing methods rely on single-modal data sharing, limiting fusion performance, particularly in heterogeneous sensor settings involving both LiDAR and cameras across vehicles and roadside units (RSUs). To address this, we propose Radian Glue Attention (RG-Attn), a lightweight and generalizable cross-modal fusion module that unifies intra-agent and inter-agent fusion via transformation-based coordinate alignment and a unified sampling/inversion strategy. RG-Attn efficiently aligns features through a radian-based attention constraint, operating column-wise on geometrically consistent regions to reduce overhead and preserve spatial coherence, thereby enabling accurate and robust fusion. Building upon RG-Attn, we propose three cooperative architectures. The first, Paint-To-Puzzle (PTP), prioritizes communication efficiency but assumes all agents have LiDAR, optionally paired with cameras. The second, Co-Sketching-Co-Coloring (CoS-CoCo), offers maximal flexibility, supporting any sensor setup (e.g., LiDAR-only, camera-only, or both) and enabling strong cross-modal generalization for real-world deployment. The third, Pyramid-RG-Attn Fusion (PRGAF), aims for peak detection accuracy with the highest computational overhead. Extensive evaluations on simulated and real-world datasets show our framework delivers state-of-the-art detection accuracy with high flexibility and efficiency. GitHub Link: https://github.com/LantaoLi/RG-Attn</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16803v3</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>cs.NI</category>
      <category>eess.IV</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lantao Li, Kang Yang, Wenqi Zhang, Xiaoxue Wang, Chen Sun</dc:creator>
    </item>
    <item>
      <title>Vulnerability Disclosure or Notification? Best Practices for Reaching Stakeholders at Scale</title>
      <link>https://arxiv.org/abs/2506.14323</link>
      <description>arXiv:2506.14323v2 Announce Type: replace-cross 
Abstract: Security researchers are interested in security vulnerabilities, but these security vulnerabilities create risks for stakeholders. Coordinated Vulnerability Disclosure has been an accepted best practice for many years in disclosing newly discovered vulnerabilities. This practice has mostly worked, but it can become challenging when there are many different parties involved.
  There has also been research into known vulnerabilities, using datasets or active scans to discover how many machines are still vulnerable. The ethical guidelines suggest that researchers also make an effort to notify the owners of these machines. We identify that this differs from vulnerability disclosure, but rather the practice of vulnerability notification. This practice has some similarities with vulnerability disclosure but should be distinguished from it, providing other challenges and requiring a different approach.
  Based on our earlier disclosure experience and on prior work documenting their disclosure and notification operations, we provide a meta-review on vulnerability disclosure and notification to observe the shifts in strategies in recent years. We assess how researchers initiated their messaging and examine the outcomes. We then compile the best practices for the existing disclosure guidelines and for notification operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14323v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ting-Han Chen, Jeroen van der Ham-de Vos</dc:creator>
    </item>
  </channel>
</rss>
