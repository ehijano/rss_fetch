<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Dec 2025 05:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Broader View on Clustering under Cluster-Aware Norm Objectives</title>
      <link>https://arxiv.org/abs/2512.06211</link>
      <description>arXiv:2512.06211v1 Announce Type: new 
Abstract: We revisit the $(f,g)$-clustering problem that we introduced in a recent work [SODA'25], and which subsumes fundamental clustering problems such as $k$-Center, $k$-Median, Min-Sum of Radii, and Min-Load $k$-Clustering. This problem assigns each of the $k$ clusters a cost determined by the monotone, symmetric norm $f$ applied to the vector distances in the cluster, and aims at minimizing the norm $g$ applied to the vector of cluster costs. Previously, we focused on certain special cases for which we designed constant-factor approximation algorithms. Our bounds for more general settings left, however, large gaps to the known bounds for the basic problems they capture.
  In this work, we provide a clearer picture of the approximability of these more general settings. First, we design an $O(\log^2 n)$-approximation algorithm for $(f, L_{1})$-clustering for any $f$. This improves upon our previous $\widetilde{O}(\sqrt{n})$-approximation. Second, we provide an $O(k)$-approximation for the general $(f,g)$-clustering problem, which improves upon our previous $\widetilde{O}(\sqrt{kn})$-approximation algorithm and matches the best-known upper bound for Min-Load $k$-Clustering.
  We then design an approximation algorithm for $(f,g)$-clustering that interpolates, up to polylog factors, between the best known bounds for $k$-Center, $k$-Median, Min-Sum of Radii, Min-Load $k$-Clustering, (Top, $L_{1}$)-clustering, and $(L_{\infty},g)$-clustering based on a newly defined parameter of $f$ and $g$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06211v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin G. Herold, Evangelos Kipouridis, Joachim Spoerhase</dc:creator>
    </item>
    <item>
      <title>Finding a Maximum Common (Induced) Subgraph: Structural Parameters Revisited</title>
      <link>https://arxiv.org/abs/2512.06383</link>
      <description>arXiv:2512.06383v1 Announce Type: new 
Abstract: We study the parameterized complexity of the problems of finding a maximum common (induced) subgraph of two given graphs. Since these problems generalize several NP-complete problems, they are intractable even when parameterized by strongly restricted structural parameters. Our contribution in this paper is to sharply complement the hardness of the problems by showing fixed-parameter tractable cases: both induced and non-induced problems parameterized by max-leaf number and by neighborhood diversity, and the induced problem parameterized by twin cover number. These results almost completely determine the complexity of the problems with respect to well-studied structural parameters. Also, the result on the twin cover number presents a rather rare example where the induced and non-induced cases have different complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06383v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tesshu Hanaka, Yuto Okada, Yota Otachi, Lena Volk</dc:creator>
    </item>
    <item>
      <title>Instance Dependent Testing of Samplers using Interval Conditioning</title>
      <link>https://arxiv.org/abs/2512.06458</link>
      <description>arXiv:2512.06458v1 Announce Type: new 
Abstract: Sampling algorithms play a pivotal role in probabilistic AI. However, verifying if a sampler program indeed samples from the claimed distribution is a notoriously hard problem. Provably correct testers like Barbarik, Teq, Flash, CubeProbe for testing of different kinds of samplers were proposed only in the last few years. All these testers focus on the worst-case efficiency, and do not support verification of samplers over infinite domains, a case occurring frequently in Astronomy, Finance, Network Security, etc.
  In this work, we design the first tester of samplers with instance-dependent efficiency, allowing us to test samplers over natural numbers. Our tests are developed via a novel distance estimation algorithm between an unknown and a known probability distribution using an interval conditioning framework. The core technical contribution is a new connection with probability mass estimation of a continuous distribution. The practical gains are also substantial: our experiments establish up to 1000x speedup over state-of-the-art testers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06458v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishiraj Bhattacharyya, Sourav Chakraborty, Yash Pote, Uddalok Sarkar, Sayantan Sen</dc:creator>
    </item>
    <item>
      <title>The $k$-Fold Matroid Secretary Problem</title>
      <link>https://arxiv.org/abs/2512.06611</link>
      <description>arXiv:2512.06611v1 Announce Type: new 
Abstract: In the matroid secretary problem, elements $N := [n]$ of a matroid $\mathcal{M} \subseteq 2^N$ arrive in random order. When an element arrives, its weight is revealed and a choice must be made to accept or reject the element, subject to the constraint that the accepted set $S \in \mathcal{M}$. Kleinberg'05 gives a $(1-O(1/\sqrt{k}))$-competitive algorithm when $\mathcal{M}$ is a $k$-uniform matroid. We generalize their result, giving a $(1-O(\sqrt{\log(n)/k}))$-competitive algorithm when $\mathcal{M}$ is a $k$-fold matroid union.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06611v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishi Gujjar, Kevin Hua, Robert Kleinberg, Frederick V. Qiu</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Bayesian Online Assortment of Reusable Resources</title>
      <link>https://arxiv.org/abs/2512.06997</link>
      <description>arXiv:2512.06997v1 Announce Type: new 
Abstract: Motivated by the applications of rental services in e-commerce, we consider revenue maximization in online assortment of reusable resources for a stream of arriving consumers with different types. We design competitive online algorithms with respect to the optimum online policy in the Bayesian setting, in which types are drawn independently from known heterogeneous distributions over time. In the regime where the minimum of initial inventories $c_0$ is large, our main result is a near-optimal $1-\min\left(\frac{1}{2},\sqrt{\log(c_0)/c_0}\right)$ competitive algorithm for the general case of reusable resources. Our algorithm relies on an expected LP benchmark for the problem, solves this LP, and simulates the solution through an independent randomized rounding. The main challenge is obtaining point-wise inventory feasibility in a computationally efficient fashion from these simulation-based algorithms. To this end, we use several technical ingredients to design $\textit{discarding policies}$ -- one for each resource. These policies handle the trade-off between the inventory feasibility under reusability and the revenue loss of each of the resources. However, discarding a unit of a resource changes the future consumption of other resources. To handle this new challenge, we also introduce $\textit{post-processing}$ assortment procedures that help with designing and analyzing our discarding policies as they run in parallel, which might be of independent interest. As a side result, by leveraging techniques from the literature on prophet inequality, we further show an improved near-optimal $1-1/\sqrt{c_0+3}$ competitive algorithm for the special case of non-reusable resources. We finally evaluate the performance of our algorithms using the numerical simulations on the synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06997v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1287/opre.2020.0687</arxiv:DOI>
      <dc:creator>Yiding Feng, Rad Niazadeh, Amin Saberi</dc:creator>
    </item>
    <item>
      <title>Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications</title>
      <link>https://arxiv.org/abs/2512.07120</link>
      <description>arXiv:2512.07120v1 Announce Type: new 
Abstract: We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k &gt;= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07120v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Allagan, G. Morgan, S. Langley, R. Lopez-Bonilla, V. Deriglazov</dc:creator>
    </item>
    <item>
      <title>Property Testing of Computational Networks</title>
      <link>https://arxiv.org/abs/2512.07577</link>
      <description>arXiv:2512.07577v1 Announce Type: new 
Abstract: In this paper we initiate the study of \emph{property testing of weighted computational networks viewed as computational devices}. Our goal is to design property testing algorithms that for a given computational network with oracle access to the weights of the network, accept (with probability at least $\frac23$) any network that computes a certain function (or a function with a certain property) and reject (with probability at least $\frac23$) any network that is \emph{far} from computing the function (or any function with the given property). We parameterize the notion of being far and want to reject networks that are \emph{$(\epsilon,\delta)$-far}, which means that one needs to change an $\epsilon$-fraction of the description of the network to obtain a network that computes a function that differs in at most a $\delta$-fraction of inputs from the desired function (or any function with a given property).
  To exemplify our framework, we present a case study involving simple neural Boolean networks with ReLU activation function. As a highlight, we demonstrate that for such networks, any near constant function is testable in query complexity independent of the network's size. We also show that a similar result cannot be achieved in a natural generalization of the distribution-free model to our setting, and also in a related vanilla testing model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07577v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Artur Czumaj, Christian Sohler</dc:creator>
    </item>
    <item>
      <title>Approximation Algorithms for the $b$-Matching and List-Restricted Variants of MaxQAP</title>
      <link>https://arxiv.org/abs/2512.07618</link>
      <description>arXiv:2512.07618v1 Announce Type: new 
Abstract: We study approximation algorithms for two natural generalizations of the Maximum Quadratic Assignment Problem (MaxQAP). In the Maximum List-Restricted Quadratic Assignment Problem, each node in one partite set may only be matched to nodes from a prescribed list. For instances on $n$ nodes where every list has size at least $n - O(\sqrt{n})$, we design a randomized $O(\sqrt{n})$-approximation algorithm based on the linear-programming relaxation and randomized rounding framework of Makarychev, Manokaran, and Sviridenko. In the Maximum Quadratic $b$-Matching Assignment Problem, we seek a $b$-matching that maximizes the MaxQAP objective. We refine the standard MaxQAP relaxation and combine randomized rounding over $b$ independent iterations with a polynomial-time algorithm for maximum-weight $b$-matching problem to obtain an $O(\sqrt{bn})$-approximation. When $b$ is constant and all lists have size $n - O(\sqrt{n})$, our guarantees asymptotically match the best known approximation factor for MaxQAP, yielding the first approximation algorithms for these two variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07618v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiratchaphat Nanta, Vorapong Suppakitpaisarn, Piyashat Sripratak</dc:creator>
    </item>
    <item>
      <title>Tight Universal Bounds for Partially Presorted Pareto Front and Convex Hull</title>
      <link>https://arxiv.org/abs/2512.06559</link>
      <description>arXiv:2512.06559v1 Announce Type: cross 
Abstract: TimSort is a well-established sorting algorithm whose running time depends on how sorted the input already is. Recently, Eppstein, Goodrich, Illickan, and To designed algorithms inspired by TimSort for Pareto front, planar convex hull, and two other problems. For each of these problems, they define a Range Partition Entropy; a function $H$ mapping lists $I$ that store $n$ points to a number between $0$ and $\log n$. Their algorithms have, for each list of points $I$, a running time of $O(n(1 + H(I)))$.
  In this paper, we provide matching lower bounds for the Pareto front and convex hull algorithms by Eppstein, Goodrich, Illickan, and To. In particular, we show that their algorithm does not correspond to TimSort (or related stack-based MergeSort variants) but rather to a variant of QuickSort. From this, we derive an intuitive notion of universal optimality. We show comparison-based lower bounds that prove that the algorithms by Eppstein, Goodrich, Illickan and To are universally optimal under this notion of universal optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06559v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivor van der Hoog, Eva Rotenberg, Daniel Rutschmann</dc:creator>
    </item>
    <item>
      <title>Prediction with Expert Advice under Local Differential Privacy</title>
      <link>https://arxiv.org/abs/2512.06971</link>
      <description>arXiv:2512.06971v1 Announce Type: cross 
Abstract: We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \textit{central} DP algorithm by 1.5-3$\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06971v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Jacobsen, Kassem Fawaz</dc:creator>
    </item>
    <item>
      <title>Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach</title>
      <link>https://arxiv.org/abs/2512.07313</link>
      <description>arXiv:2512.07313v1 Announce Type: cross 
Abstract: We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07313v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bosun Kang, Hyejun Park, Chenglin Fan</dc:creator>
    </item>
    <item>
      <title>Exact recovery of planted cliques in semi-random graphs</title>
      <link>https://arxiv.org/abs/2011.08447</link>
      <description>arXiv:2011.08447v5 Announce Type: replace 
Abstract: In this paper, we study the Planted Clique problem in a semi-random model. Our model is inspired from the Feige-Kilian model [16] which has been studied in many other works [8,11,17,26,35,38] for a variety of graph problems. Our algorithm and analysis is on similar lines to the one studied for the Densest $k$-subgraph problem in the work of Khanna and Louis [25].
  As a by-product of our main result, we give an alternate SDP-based rounding algorithm (with similar guarantees) for solving the Planted Clique problem in a random graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.08447v5</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yash Khanna</dc:creator>
    </item>
    <item>
      <title>Static Pricing Guarantees for Queueing Systems</title>
      <link>https://arxiv.org/abs/2305.09168</link>
      <description>arXiv:2305.09168v5 Announce Type: replace 
Abstract: We consider a general queueing system with price-sensitive customers in which the service provider seeks to balance two objectives, maximizing the average revenue rate and minimizing the average queue length. Customers arrive according to a Poisson process, observe an offered price, and decide to join the queue if their valuation exceeds the price. The queue is operated first-in first-out, can have multiple servers, and the service times are exponential. Our model represents applications in areas like make-to-order manufacturing, cloud computing, and food delivery.
  The optimal solution for our model is dynamic; the price changes as the state of the system changes. However, such dynamic pricing policies may be undesirable for a variety of reasons. In this work, we provide non-asymptotic performance guarantees for a simple and natural class of static pricing policies which charge a fixed price up to a certain occupancy threshold and then allow no more customers into the system. Despite the mixed-sign objective, we are able to show our policy can guarantee a constant fraction of the optimal dynamic pricing policy in the worst-case. We also show that our policy yields a family of bi-criteria approximations that simultaneously guarantee a constant fraction of the optimal revenue with at most a constant factor increase in expected queue length. For instance, our policy for the M/M/1 setting can be set so that its worst-case guarantees is at least 50, 66, 75, or 80% of the optimal revenue and at most a 0, 16, 54, or 100% increase in the optimal queue length, respectively. We also provide guarantees for settings with multiple servers as well as the expected sojourn time objective. In a large simulation, we show that our class of policies is at most 4% sub-optimal on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09168v5</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Bergquist, Adam N. Elmachtoub</dc:creator>
    </item>
    <item>
      <title>An Objective Improvement Approach to Solving Discounted Payoff Games</title>
      <link>https://arxiv.org/abs/2404.04124</link>
      <description>arXiv:2404.04124v5 Announce Type: replace 
Abstract: While discounted payoff games and classic games that reduce to them, like parity and mean-payoff games, are symmetric, their solutions are not. We have taken a fresh view on the properties that optimal solutions need to have, and devised a novel way to converge to them, which is entirely symmetric. We achieve this by building a constraint system that uses every edge to define an inequation, and update the objective function by taking a single outgoing edge for each vertex into account. These edges loosely represent strategies of both players, where the objective function intuitively asks to make the inequation to these edges sharp. In fact, where they are not sharp, there is an `error' represented by the difference between the two sides of the inequation, which is 0 where the inequation is sharp. Hence, the objective is to minimise the sum of these errors. For co-optimal strategies, and only for them, it can be achieved that all selected inequations are sharp or, equivalently, that the sum of these errors is zero. While no co-optimal strategies have been found, we step-wise improve the error by improving the solution for a given objective function or by improving the objective function for a given solution. This also challenges the gospel that methods for solving payoff games are either based on strategy improvement or on value iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04124v5</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Dell'Erba, Arthur Dumas, Sven Schewe</dc:creator>
    </item>
    <item>
      <title>Commitment Gap via Correlation Gap</title>
      <link>https://arxiv.org/abs/2508.20246</link>
      <description>arXiv:2508.20246v2 Announce Type: replace 
Abstract: Selection problems with costly information, dating back to Weitzman's Pandora's Box problem, have received much attention recently. We study the general model of Costly Information Combinatorial Selection (CICS) that was recently introduced by Chawla et al. [2024] and Bowers et al. [2025]. In this problem, a decision maker needs to select a feasible subset of stochastic variables, and can only learn information about their values through a series of costly steps, modeled by a Markov decision process. The algorithmic objective is to maximize the total value of the selection minus the cost of information acquisition. However, determining the optimal algorithm is known to be a computationally challenging problem.
  To address this challenge, previous approaches have turned to approximation algorithms by considering a restricted class of committing policies that simplify the decision-making aspects of the problem and allow for efficient optimization. This motivates the question of bounding the commitment gap, measuring the worst case ratio in the performance of the optimal committing policy and the overall optimal. In this work, we obtain improved bounds on the commitment gap of CICS through a reduction to a simpler problem of Bayesian Combinatorial Selection where information is free. By establishing a close relationship between these problems, we are able to relate the commitment gap of CICS to ex ante free-order prophet inequalities. As a consequence, we obtain improved efficient approximations for arbitrary instances of the CICS under various feasibility constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20246v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuchi Chawla, Dimitris Christou, Trung Dang</dc:creator>
    </item>
    <item>
      <title>JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford</title>
      <link>https://arxiv.org/abs/2512.01802</link>
      <description>arXiv:2512.01802v2 Announce Type: replace 
Abstract: We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01802v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Wang, Xi Chen</dc:creator>
    </item>
  </channel>
</rss>
