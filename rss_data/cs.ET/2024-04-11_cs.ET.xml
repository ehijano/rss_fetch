<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Apr 2024 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fast System Technology Co-Optimization Framework for Emerging Technology Based on Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2404.06939</link>
      <description>arXiv:2404.06939v1 Announce Type: new 
Abstract: This paper proposes a fast system technology co-optimization (STCO) framework that optimizes power, performance, and area (PPA) for next-generation IC design, addressing the challenges and opportunities presented by novel materials and device architectures. We focus on accelerating the technology level of STCO using AI techniques, by employing graph neural network (GNN)-based approaches for both TCAD simulation and cell library characterization, which are interconnected through a unified compact model, collectively achieving over a 100X speedup over traditional methods. These advancements enable comprehensive STCO iterations with runtime speedups ranging from 1.9X to 14.1X and supports both emerging and traditional technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06939v1</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianliang Ma, Guangxi Fan, Xuguang Sun, Zhihui Deng, Kainlu Low, Leilai Shao</dc:creator>
    </item>
    <item>
      <title>Quantum Tunneling: From Theory to Error-Mitigated Quantum Simulation</title>
      <link>https://arxiv.org/abs/2404.07034</link>
      <description>arXiv:2404.07034v1 Announce Type: cross 
Abstract: Ever since the discussions about a possible quantum computer arised, quantum simulations have been at the forefront of possible utilities and the task of quantum simulations is one that promises quantum advantage. In recent years, simulations of large molecules through VQE or dynamics of many-body spin Hamiltonians may be possible, and even able to achieve useful results with the use of error mitigation techniques. Simulating smaller models is also important, and currently, in the NISQ (Noisy intermediate-scale quantum) era, it is easier and less prone to errors. This current study encompasses the theoretical background and the hardware aware circuit implementation of a quantum tunneling simulation. Specifically, this study presents the theoretical background needed for such implementation and highlights the main steps of development. Building on classic approaches of quantum tunneling simulations, this study improves the result of such simulations by employing error mitigation techniques (ZNE and REM) and uses them in conjunction with multiprogramming of the quantum chip for solving the hardware under-utilization problem that arises in such contexts. Moreover, we highlight the need for hardware-aware circuit implementations and discuss these considerations in detail to give an end-to-end workflow overview of quantum simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07034v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sorana Catrina, Alexandra B\u{a}icoianu</dc:creator>
    </item>
    <item>
      <title>Photonic next-generation reservoir computer based on distributed feedback in optical fiber</title>
      <link>https://arxiv.org/abs/2404.07116</link>
      <description>arXiv:2404.07116v1 Announce Type: cross 
Abstract: Reservoir computing (RC) is a machine learning paradigm that excels at dynamical systems analysis. Photonic RCs, which perform implicit computation through optical interactions, have attracted increasing attention due to their potential for low latency predictions. However, most existing photonic RCs rely on a nonlinear physical cavity to implement system memory, limiting control over the memory structure and requiring long warm-up times to eliminate transients. In this work, we resolve these issues by demonstrating a photonic next-generation reservoir computer (NG-RC) using a fiber optic platform. Our photonic NG-RC eliminates the need for a cavity by generating feature vectors directly from nonlinear combinations of the input data with varying delays. Our approach uses Rayleigh backscattering to produce output feature vectors by an unconventional nonlinearity resulting from coherent, interferometric mixing followed by a quadratic readout. Performing linear optimization on these feature vectors, our photonic NG-RC demonstrates state-of-the-art performance for the observer (cross-prediction) task applied to the R\"ossler, Lorenz, and Kuramoto-Sivashinsky systems. In contrast to digital NG-RC implementations, this scheme is easily scalable to high-dimensional systems while maintaining low latency and low power consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07116v1</guid>
      <category>physics.optics</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Cox, Joseph Murray, Joseph Hart, Brandon Redding</dc:creator>
    </item>
    <item>
      <title>Enabling data-driven and bidirectional model development in Verilog-A for photonic devices</title>
      <link>https://arxiv.org/abs/2402.10971</link>
      <description>arXiv:2402.10971v2 Announce Type: replace 
Abstract: We present a method to model photonic components in Verilog-A by introducing bidirectional signaling through a single port. To achieve this, the concept of power waves and scattering parameters from electromagnetism are employed. As a consequence, one can simultaneously transmit forward and backward propagating waves on a single wire while also capturing realistic, measurement-backed response of photonic components in Verilog-A. We demonstrate examples to show the efficacy of the proposed technique in accounting for critical effects in photonic integrated circuits such as Fabry-Perot cavity resonance, reflections to lasers, etc. Our solution makes electronic-photonic co-simulation more intuitive and accurate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10971v2</guid>
      <category>cs.ET</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dias Azhigulov, Zeqin Lu, James Pond, Lukas Chrostowski, Sudip Shekhar</dc:creator>
    </item>
    <item>
      <title>Tridiagonal matrix decomposition for Hamiltonian simulation on a quantum computer</title>
      <link>https://arxiv.org/abs/2310.00121</link>
      <description>arXiv:2310.00121v2 Announce Type: replace-cross 
Abstract: The construction of quantum circuits to simulate Hamiltonian evolution is central to many quantum algorithms. State-of-the-art circuits are based on oracles whose implementation is often omitted, and the complexity of the algorithm is estimated by counting oracle queries. However, in practical applications, an oracle implementation contributes a large constant factor to the overall complexity of the algorithm. The key finding of this work is the efficient procedure for representation of a tridiagonal matrix in the Pauli basis, which allows one to construct a Hamiltonian evolution circuit without the use of oracles. The procedure represents a general tridiagonal matrix $2^n \times 2^n$ by systematically determining all Pauli strings present in the decomposition, dividing them into commuting subsets. The efficiency is in the number of commuting subsets $O(n)$. The method is demonstrated using the one-dimensional wave equation, verifying numerically that the gate complexity as function of the number of qubits is lower than the oracle based approach for $n &lt; 15$ and requires half the number of qubits. This method is applicable to other Hamiltonians based on the tridiagonal matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00121v2</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boris Arseniev, Dmitry Guskov, Richik Sengupta, Igor Zacharov</dc:creator>
    </item>
    <item>
      <title>AI-Enabled System for Efficient and Effective Cyber Incident Detection and Response in Cloud Environments</title>
      <link>https://arxiv.org/abs/2404.05602</link>
      <description>arXiv:2404.05602v2 Announce Type: replace-cross 
Abstract: The escalating sophistication and volume of cyber threats in cloud environments necessitate a paradigm shift in strategies. Recognising the need for an automated and precise response to cyber threats, this research explores the application of AI and ML and proposes an AI-powered cyber incident response system for cloud environments. This system, encompassing Network Traffic Classification, Web Intrusion Detection, and post-incident Malware Analysis (built as a Flask application), achieves seamless integration across platforms like Google Cloud and Microsoft Azure. The findings from this research highlight the effectiveness of the Random Forest model, achieving an accuracy of 90% for the Network Traffic Classifier and 96% for the Malware Analysis Dual Model application. Our research highlights the strengths of AI-powered cyber security. The Random Forest model excels at classifying cyber threats, offering an efficient and robust solution. Deep learning models significantly improve accuracy, and their resource demands can be managed using cloud-based TPUs and GPUs. Cloud environments themselves provide a perfect platform for hosting these AI/ML systems, while container technology ensures both efficiency and scalability. These findings demonstrate the contribution of the AI-led system in guaranteeing a robust and scalable cyber incident response solution in the cloud.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05602v2</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Ashfaaq M. Farzaan, Mohamed Chahine Ghanem, Ayman El-Hajjar, Deepthi N. Ratnayake</dc:creator>
    </item>
  </channel>
</rss>
