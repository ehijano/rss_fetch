<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Nov 2025 05:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects</title>
      <link>https://arxiv.org/abs/2511.02132</link>
      <description>arXiv:2511.02132v1 Announce Type: new 
Abstract: The rise of disaggregated AI GPUs has exposed a critical bottleneck in large-scale attention workloads: non-uniform memory access (NUMA). As multi-chiplet designs become the norm for scaling compute capabilities, memory latency and bandwidth vary sharply across compute regions, undermining the performance of traditional GPU kernel scheduling strategies that assume uniform memory access. We identify how these NUMA effects distort locality in multi-head attention (MHA) and present Swizzled Head-first Mapping, a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our method achieves up to 50% higher performance over state-of-the-art attention algorithms using conventional scheduling techniques and sustains consistently high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware scheduling is now fundamental to achieving full efficiency on next-generation disaggregated GPUs, offering a path forward for scalable AI training and inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02132v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mansi Choudhary, Karthik Sangaiah, Sonali Singh, Muhammad Osama, Lisa Wu Wills, Ganesh Dasika</dc:creator>
    </item>
    <item>
      <title>BoolSkeleton: Boolean Network Skeletonization via Homogeneous Pattern Reduction</title>
      <link>https://arxiv.org/abs/2511.02196</link>
      <description>arXiv:2511.02196v1 Announce Type: new 
Abstract: Boolean equivalence allows Boolean networks with identical functionality to exhibit diverse graph structures. This gives more room for exploration in logic optimization, while also posing a challenge for tasks involving consistency between Boolean networks. To tackle this challenge, we introduce BoolSkeleton, a novel Boolean network skeletonization method that improves the consistency and reliability of design-specific evaluations. BoolSkeleton comprises two key steps: preprocessing and reduction. In preprocessing, the Boolean network is transformed into a defined Boolean dependency graph, where nodes are assigned the functionality-related status. Next, the homogeneous and heterogeneous patterns are defined for the node-level pattern reduction step. Heterogeneous patterns are preserved to maintain critical functionality-related dependencies, while homogeneous patterns can be reduced. Parameter K of the pattern further constrains the fanin size of these patterns, enabling fine-tuned control over the granularity of graph reduction. To validate BoolSkeleton's effectiveness, we conducted four analysis/downstream tasks around the Boolean network: compression analysis, classification, critical path analysis, and timing prediction, demonstrating its robustness across diverse scenarios. Furthermore, it improves above 55% in the average accuracy compared to the original Boolean network for the timing prediction task. These experiments underscore the potential of BoolSkeleton to enhance design consistency in logic synthesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02196v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liwei Ni, Jiaxi Zhang, Shenggen Zheng, Junfeng Liu, Xingyu Meng, Biwei Xie, Xingquan Li, Huawei Li</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA</title>
      <link>https://arxiv.org/abs/2511.02269</link>
      <description>arXiv:2511.02269v1 Announce Type: new 
Abstract: The rise of generative AI for tasks like Automatic Speech Recognition (ASR) has created a critical energy consumption challenge. While ASICs offer high efficiency, they lack the programmability to adapt to evolving algorithms. To address this trade-off, we implement and evaluate Whisper's core computational kernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs) accelerator. To our knowledge, this is the first work to execute a Whisper kernel on a CGRA and compare its performance against CPUs and GPUs. Using hardware/software co-design, we evaluate our system via an FPGA prototype and project performance for a 28 nm ASIC. Our results demonstrate superior energy efficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA Jetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This work positions CGLA as a promising platform for sustainable ASR on power-constrained edge devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02269v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuto Ando, Yu Eto, Ayumu Takeuchi, Yasuhiko Nakashima</dc:creator>
    </item>
    <item>
      <title>VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning</title>
      <link>https://arxiv.org/abs/2511.02285</link>
      <description>arXiv:2511.02285v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive potential in generating Verilog codes, but ensuring functional correctness remains a challenge. Existing approaches often rely on self-consistency or simulation feedback to select the best candidate, but they miss opportunities to focus LLM reasoning on the most informative parts of the design. We propose VFocus, a three-stage framework that enhances Verilog generation by sharpening the focus of LLM reasoning onto critical decision points in the code generation process. In the \textbf{pre-ranking stage}, VFocus generates multiple code candidates through LLM prompting, retries for syntactically valid outputs, and introduces a \textit{Density-guided Filtering} to retain candidates that fall within the "reasoning sweet spot" for functional correctness. In the \textbf{ranking stage}, we simulate each code candidate using an automatically generated testbench and apply self-consistency-based clustering to identify the most consistent outputs. Finally, in the \textbf{post-ranking refinement stage}, VFocus performs inconsistency mining on top-ranked candidates and invokes reasoning-augmented LLM prompts for candidate refinement. Experiments on the VerilogEval-Human benchmark show that VFocus significantly improves the pass@1 correctness across multiple reasoning LLMs, demonstrating its effectiveness in enhancing Verilog generation for complex hardware design tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02285v1</guid>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhuorui Zhao, Bing Li, Grace Li Zhang, Ulf Schlichtmann</dc:creator>
    </item>
    <item>
      <title>Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA</title>
      <link>https://arxiv.org/abs/2511.02408</link>
      <description>arXiv:2511.02408v1 Announce Type: new 
Abstract: In this paper, we implement a stand-alone facial expression recognition system on an SoC FPGA with multi-threading using a Deep learning Processor Unit (DPU). The system consists of two steps: one for face detection step and one for facial expression recognition. In the previous work, the Haar Cascade detector was run on a CPU in the face detection step due to FPGA resource limitations, but this detector is less accurate for profile and variable illumination condition images. Moreover, the previous work used a dedicated circuit accelerator, so running a second DNN inference for face detection on the FPGA would require the addition of a new accelerator. As an alternative to this approach, we run the two inferences by DNN on a DPU, which is a general-purpose CNN accelerator of the systolic array type. Our method for face detection using DenseBox and facial expression recognition using CNN on the same DPU enables the efficient use of FPGA resources while maintaining a small circuit size. We also developed a multi-threading technique that improves the overall throughput while increasing the DPU utilization efficiency. With this approach, we achieved an overall system throughput of 25 FPS and a throughput per power consumption of 2.4 times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02408v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/CANDARW64572.2024.00025</arxiv:DOI>
      <arxiv:journal_reference>2024 Twelfth International Symposium on Computing and Networking Workshops (CANDARW)</arxiv:journal_reference>
      <dc:creator>Takuto Ando, Yusuke Inoue</dc:creator>
    </item>
    <item>
      <title>Digit-Recurrence Posit Division</title>
      <link>https://arxiv.org/abs/2511.02494</link>
      <description>arXiv:2511.02494v1 Announce Type: new 
Abstract: Posit arithmetic has emerged as a promising alternative to IEEE 754 floating-point representation, offering enhanced accuracy and dynamic range. However, division operations in posit systems remain challenging due to their inherent hardware complexity. In this work, we present posit division units based on the digit-recurrence algorithm, marking the first implementation of radix-4 digit-recurrence techniques within this context. Our approach incorporates hardware-centric optimizations including redundant arithmetic, on-the-fly quotient conversion, and operand scaling to streamline the division process while mitigating latency, area, and power overheads. Comprehensive synthesis evaluations across multiple posit configurations demonstrate significant performance improvements, including more than 80% energy reduction with small area overhead compared to existing methods, and a substantial decrease in the number of iterations. These results underscore the potential of our adapted algorithm to enhance the efficiency of posit-based arithmetic units.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02494v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raul Murillo, Julio Villalba-Moreno, Alberto A. Del Barrio, Guillermo Botella</dc:creator>
    </item>
    <item>
      <title>Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator</title>
      <link>https://arxiv.org/abs/2511.02530</link>
      <description>arXiv:2511.02530v1 Announce Type: new 
Abstract: This paper presents the first implementation and in-depth evaluation of the primary computational kernels from the stable-diffusion.cpp image generation framework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array (CGRA) accelerator. We designed IMAX3 as a versatile computational platform, and this work assesses its capabilities by executing a demanding image generation workload. We evaluate its performance on a current Field-Programmable Gate Array (FPGA) prototype to establish a baseline and project its potential for a future Application-Specific Integrated Circuit (ASIC) implementation. Our results demonstrate that, despite its general-purpose architecture, IMAX3 achieves promising performance and power efficiency, particularly in its projected ASIC form. This work provides concrete guidelines for future IMAX architectural designs and establishes a foundation for developing next-generation, AI-specialized Coarse-Grained Linear Array (CGLA) accelerators by refining this versatile platform. Ultimately, this achievement contributes to the realization of energy-efficient, on-device, multi-modal AI platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02530v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuto Ando, Yu Eto, Yasuhiko Nakashima</dc:creator>
    </item>
    <item>
      <title>EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs</title>
      <link>https://arxiv.org/abs/2511.01866</link>
      <description>arXiv:2511.01866v1 Announce Type: cross 
Abstract: Edge intelligence paradigm is increasingly demanded by the emerging autonomous systems, such as robotics. Beyond ensuring privacy-preserving operation and resilience in connectivity-limited environments, edge deployment offers significant energy and cost advantages over cloud-based solutions. However, deploying large language models (LLMs) for reasoning tasks on edge GPUs faces critical challenges from strict latency constraints and limited computational resources. To navigate these constraints, developers must balance multiple design factors - choosing reasoning versus non-reasoning architectures, selecting appropriate model sizes, allocating token budgets, and applying test-time scaling strategies - to meet target latency and optimize accuracy. Yet guidance on optimal combinations of these variables remains scarce. In this work, we present EdgeReasoning, a comprehensive study characterizing the deployment of reasoning LLMs on edge GPUs. We systematically quantify latency-accuracy tradeoffs across various LLM architectures and model sizes. We systematically evaluate prompt-based and model-tuning-based techniques for reducing reasoning token length while maintaining performance quality. We further profile test-time scaling methods with varying degrees of parallelism to maximize accuracy under strict latency budgets. Through these analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency configurations, offering systematic guidance for optimal edge deployment of reasoning LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01866v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Kubwimana, Qijing Huang</dc:creator>
    </item>
    <item>
      <title>FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN Inference</title>
      <link>https://arxiv.org/abs/2503.04426</link>
      <description>arXiv:2503.04426v2 Announce Type: replace 
Abstract: The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical applications brings their reliability to the front. High performance demands of DNNs require the use of specialized hardware accelerators. Systolic array architecture is widely used in DNN accelerators due to its parallelism and regular structure. This work presents a run-time reconfigurable systolic array architecture with three execution modes and four implementation options. All four implementations are evaluated in terms of resource utilization, throughput, and fault tolerance improvement. The proposed architecture is used for reliability enhancement of DNN inference on systolic array through heterogeneous mapping of different network layers to different execution modes. The approach is supported by a novel reliability assessment method based on fault propagation analysis. It is used for the exploration of the appropriate execution mode--layer mapping for DNN inference. The proposed architecture efficiently protects registers and MAC units of systolic array PEs from transient and permanent faults. The reconfigurability feature enables a speedup of up to $3\times$, depending on layer vulnerability. Furthermore, it requires $6\times$ fewer resources compared to static redundancy and $2.5\times$ fewer resources compared to the previously proposed solution for transient faults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04426v2</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.micpro.2025.105222</arxiv:DOI>
      <arxiv:journal_reference>Cherezova, N., Jutman, A., &amp; Jenihhin, M. (2025). FORTALESA: Fault-tolerant reconfigurable systolic array for DNN inference. Microprocessors and Microsystems, 119, 105222</arxiv:journal_reference>
      <dc:creator>Natalia Cherezova, Artur Jutman, Maksim Jenihhin</dc:creator>
    </item>
    <item>
      <title>CPU-Based Layout Design for Picker-to-Parts Pallet Warehouses</title>
      <link>https://arxiv.org/abs/2506.04266</link>
      <description>arXiv:2506.04266v2 Announce Type: replace-cross 
Abstract: Picker-to-parts pallet warehouses often face inefficiencies due to conventional layouts causing excessive travel distances and high labor requirements. This study introduces a novel layout design inspired by CPU architecture, partitioning warehouse space into specialized zones, namely Performance (P), Efficiency (E), and Shared (S). Discrete-event simulation is used to evaluate this design against traditional rectangular (random and ABC storage) and Flying-V layouts. Results demonstrate significant improvements in throughput time and reduced labor requirements, highlighting the potential for CPU-based layouts in optimizing warehouse operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04266v2</guid>
      <category>cs.MA</category>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Looms, Lin Xie</dc:creator>
    </item>
    <item>
      <title>Controlling quantum chaos via Parrondo strategies on noisy intermediate-scale quantum hardware</title>
      <link>https://arxiv.org/abs/2506.11225</link>
      <description>arXiv:2506.11225v2 Announce Type: replace-cross 
Abstract: Advancements in Noisy Intermediate-Scale Quantum (NISQ) computing are steadily pushing these systems toward outperforming classical supercomputers on specific, well-defined computational tasks. In this work, we explore and control quantum chaos in NISQ systems using discrete-time quantum walks (DTQW) on cyclic graphs. To efficiently implement quantum walks on NISQ hardware, we employ the quantum Fourier transform (QFT) to diagonalize the conditional shift operator, optimizing circuit depth and fidelity. We experimentally realize the transition from quantum chaos to order via DTQW dynamics on both odd and even cyclic graphs, specifically 3- and 4-cycle graphs, using the counterintuitive Parrondo's paradox strategy across three different NISQ devices. While the 4-cycle graphs exhibit high-fidelity quantum evolution, the 3-cycle implementation shows significant fidelity improvement when augmented with dynamical decoupling pulses. Our results demonstrate a practical approach to probing and harnessing controlled chaotic dynamics on real quantum hardware, laying the groundwork for future quantum algorithms and cryptographic protocols based on quantum walks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11225v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AR</category>
      <category>nlin.CD</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1103/m89r-2dy5</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. E (2025)</arxiv:journal_reference>
      <dc:creator>Aditi Rath, Dinesh Kumar Panda, Colin Benjamin</dc:creator>
    </item>
    <item>
      <title>QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation</title>
      <link>https://arxiv.org/abs/2510.19296</link>
      <description>arXiv:2510.19296v2 Announce Type: replace-cross 
Abstract: The remarkable progress of Large Language Models (LLMs) presents promising opportunities for Verilog code generation which is significantly important for automated circuit design. The lacking of meaningful functional rewards hinders the preference optimization based on Reinforcement Learning (RL) for producing functionally correct Verilog code. In this paper, we propose Signal-Aware Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments of functionally correct output signal to optimize RL training. Considering Verilog code specifies the structural interconnection of hardware gates and wires so that different output signals are independent, the key insight of QiMeng-SALV is to extract verified signal-aware implementations in partially incorrect modules, so as to enhance the extraction of meaningful functional rewards. Roughly, we verify the functional correctness of signals in generated module by comparing with that of reference module in the training data. Then abstract syntax tree (AST) is employed to identify signal-aware code segments which can provide meaningful functional rewards from erroneous modules. Finally, we introduce signal-aware DPO which is optimized on the correct signal-level code segments, thereby preventing noise and interference from incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from conventional module-level to fine-grained signal-level optimization in Verilog code generation, addressing the issue of insufficient functional rewards. Experiments demonstrate that our method achieves state-of-the-art performance on VerilogEval and RTLLM, with a 7B parameter model matching the performance of the DeepSeek v3 671B model and significantly outperforming the leading open-source model CodeV trained on the same dataset. Our code is available at https://github.com/zy1xxx/SALV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19296v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Zhang, Rui Zhang, Jiaming Guo, Lei Huang, Di Huang, Yunpu Zhao, Shuyao Cheng, Pengwei Jin, Chongxiao Li, Zidong Du, Xing Hu, Qi Guo, Yunji Chen</dc:creator>
    </item>
  </channel>
</rss>
