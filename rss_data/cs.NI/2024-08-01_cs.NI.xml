<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Aug 2024 01:40:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimal Sampling under Cost for Remote Estimation of the Wiener Process over a Channel with Delay</title>
      <link>https://arxiv.org/abs/2407.21181</link>
      <description>arXiv:2407.21181v1 Announce Type: new 
Abstract: In this paper, we address the optimal sampling of a Wiener process under sampling and transmission costs, with the samples being forwarded to a remote estimator over a channel with random IID delay. The goal of the estimator is to reconstruct an estimate of the real-time signal value from causally received samples. Our study focuses on the optimal online strategy for both sampling and transmission, aiming to minimize the mean square estimation error. We establish that the optimal strategy involves threshold policies for both sampling and transmission, and we derive the optimal thresholds. We utilize Lagrange relaxation and backward induction as our methodology, revealing the problem of minimizing estimation error, under the assumption that sampling and transmission times are independent of the observed Wiener process. Our comparative analysis demonstrates that the estimation error achieved by the optimal joint sampling and transmission policy is significantly lower than that of age-optimal sampling, zero-wait sampling, periodic sampling, and policies that optimize only the sampling times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21181v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Orhan T. Yava\c{s}can, S\"uleyman \c{C}{\i}t{\i}r, Elif Uysal</dc:creator>
    </item>
    <item>
      <title>Towards Variable-Length In-Network Caching</title>
      <link>https://arxiv.org/abs/2407.21324</link>
      <description>arXiv:2407.21324v2 Announce Type: new 
Abstract: We present StarCache, a new in-network caching architecture that can cache variable-length items to balance a wide range of key-value workloads. Unlike existing works, StarCache does not cache hot items in the switch memory. Instead, we make hot items revisit the switch data plane continuously by exploiting packet recirculation. Our approach keeps cached key-value pairs in the switch data plane while freeing them from item size limitations caused by hardware constraints. We implement a StarCache prototype on an Intel Tofino switch. Our experimental results show that StarCache can balance highly skewed workloads with various key and value sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21324v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gyuyeong Kim</dc:creator>
    </item>
    <item>
      <title>Priority and Stackelberg Game-Based Incentive Task Allocation for Device-Assisted MEC Networks</title>
      <link>https://arxiv.org/abs/2407.21352</link>
      <description>arXiv:2407.21352v1 Announce Type: new 
Abstract: Mobile edge computing (MEC) is a promising computing paradigm that offers users proximity and instant computing services for various applications, and it has become an essential component of the Internet of Things (IoT). However, as compute-intensive services continue to emerge and the number of IoT devices explodes, MEC servers are confronted with resource limitations. In this work, we investigate a task-offloading framework for device-assisted edge computing, which allows MEC servers to assign certain tasks to auxiliary IoT devices (ADs) for processing. To facilitate efficient collaboration among task IoT devices (TDs), the MEC server, and ADs, we propose an incentive-driven pricing and task allocation scheme. Initially, the MEC server employs the Vickrey auction mechanism to recruit ADs. Subsequently, based on the Stackelberg game, we analyze the interactions between TDs and the MEC server. Finally, we establish the optimal service pricing and task allocation strategy, guided by the Stackelberg model and priority settings. Simulation results show that the proposed scheme dramatically improves the utility of the MEC server while safeguarding the interests of TDs and ADs, achieving a triple-win scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21352v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Li, Xing Zhang, Bo Lei, Zheyan Qu, Wenbo Wang</dc:creator>
    </item>
    <item>
      <title>Kuramoto oscillators in random networks</title>
      <link>https://arxiv.org/abs/2407.21513</link>
      <description>arXiv:2407.21513v1 Announce Type: new 
Abstract: By means of numerical analysis conducted with the aid of the computer, the collective synchronization of coupled phase oscillators in the Kuramoto model in the connected regime of random networks of various sizes is studied. The oscillators synchronize and achieve phase coherence, and this process is not significantly affected by the level of connectivity of the network. If the probability that two oscillators are coupled is around the network connectivity threshold synchronization still occurs, although in a more attenuated way. If the size of the network is sufficiently large the oscillators have a phase transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21513v1</guid>
      <category>cs.NI</category>
      <category>nlin.CD</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agostino Funel</dc:creator>
    </item>
    <item>
      <title>REPS: Recycling Entropies for Packet Spraying to Adaptively Explore Paths and Mitigate Failures</title>
      <link>https://arxiv.org/abs/2407.21625</link>
      <description>arXiv:2407.21625v1 Announce Type: new 
Abstract: Most existing datacenter transport protocols rely on in-order packet delivery, a design choice rooted in legacy systems and simplicity. However, advancements in technology, such as RDMA, have made it feasible to relax this requirement, allowing for more effective use of modern datacenter topologies like FatTree and Dragonfly. The rise of AI/ML workloads underscores the necessity for enhanced link utilization, a challenge for single-path load balancers due to issues like ECMP collisions.
  In this paper, we introduce REPS, a novel per-packet traffic load-balancing algorithm that integrates seamlessly with existing congestion control mechanisms. REPS reroutes packets around congested hotspots and unreliable or failing links with remarkable simplicity and minimal state requirements.
  Our evaluation demonstrates that REPS significantly outperforms traditional packet spraying and other state-of-the-art solutions in datacenter networks, offering substantial improvements in performance and link utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21625v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommaso Bonato, Abdul Kabbani, Ahmad Ghalayini, Mohammad Dohadwala, Michael Papamichael, Daniele De Sensi, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>Discovery of 6G Services and Resources in Edge-Cloud-Continuum</title>
      <link>https://arxiv.org/abs/2407.21751</link>
      <description>arXiv:2407.21751v1 Announce Type: new 
Abstract: The advent of 6G networks will present a pivotal juncture in the evolution of telecommunications, marked by the proliferation of devices, dynamic service requests, and the integration of edge and cloud computing. In response to these transformative shifts, this paper proposes a service and resource discovery architecture as part of service provisioning for the future 6G edge-cloud-continuum. Through the architecture's orchestration and platform components, users will have access to services efficiently and on time. Blockchain underpins trust in this inherently trustless environment, while semantic networking dynamically extracts context from service requests, fostering efficient communication and service delivery. A key innovation lies in dynamic overlay zoning, which not only optimizes resource allocation but also endows our architecture with scalability, adaptability, and resilience. Notably, our architecture excels at predictive capabilities, harnessing learning algorithms to anticipate user and service instance behavior, thereby enhancing network responsiveness and preserving service continuity. This comprehensive architecture paves the way for unparalleled resource optimization, latency reduction, and seamless service delivery, positioning it as an instrumental pillar in the unfolding 6G landscape. Simulation results show that our architecture provides near-optimal timely responses that significantly improve the network's potential, offering scalable and efficient service and resource discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21751v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Farhoudi, Masoud Shokrnezhad, Tarik Taleb, Richard Li, JaeSeung Song</dc:creator>
    </item>
    <item>
      <title>E-Commerce Product Recommendation System based on ML Algorithms</title>
      <link>https://arxiv.org/abs/2407.21026</link>
      <description>arXiv:2407.21026v1 Announce Type: cross 
Abstract: Algorithms are used in eCommerce product recommendation systems. These systems just recently began utilizing machine learning algorithms due to the development and growth of the artificial intelligence research community. This project aspires to transform how eCommerce platforms communicate with their users. We have created a model that can customize product recommendations and offers for each unique customer using cutting-edge machine learning techniques, we used PCA to reduce features and four machine learning algorithms like Gaussian Naive Bayes (GNB), Random Forest (RF), Logistic Regression (LR), Decision Tree (DT), the Random Forest algorithms achieve the highest accuracy of 99.6% with a 96.99 r square score, 1.92% MSE score, and 0.087 MAE score. The outcome is advantageous for both the client and the business. In this research, we will examine the model's development and training in detail and show how well it performs using actual data. Learning from machines can change of eCommerce world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21026v1</guid>
      <category>cs.IR</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md. Zahurul Haque</dc:creator>
    </item>
    <item>
      <title>LoRaWAN Based Dynamic Noise Mapping with Machine Learning for Urban Noise Enforcement</title>
      <link>https://arxiv.org/abs/2407.21204</link>
      <description>arXiv:2407.21204v1 Announce Type: cross 
Abstract: Static noise maps depicting long-term noise levels over wide areas are valuable urban planning assets for municipalities in decreasing noise exposure of residents. However, non-traffic noise sources with transient behavior, which people complain frequently, are usually ignored by static maps. We propose here a dynamic noise mapping approach using the data collected via low-power wide-area network (LPWAN, specifically LoRaWAN) based internet of things (IoT) infrastructure, which is one of the most common communication backbones for smart cities. Noise mapping based on LPWAN is challenging due to the low data rates of these protocols. The proposed dynamic noise mapping approach diminishes the negative implications of data rate limitations using machine learning (ML) for event and location prediction of non-traffic sources based on the scarce data. The strength of these models lies in their consideration of the spatial variance in acoustic behavior caused by the buildings in urban settings. The effectiveness of the proposed method and the accuracy of the resulting dynamic maps are evaluated in field tests. The results show that the proposed system can decrease the map error caused by non-traffic sources up to 51% and can stay effective under significant packet losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21204v1</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>H. Emre Erdem, Henry Leung</dc:creator>
    </item>
    <item>
      <title>Predicting Software Reliability in Softwarized Networks</title>
      <link>https://arxiv.org/abs/2407.21224</link>
      <description>arXiv:2407.21224v1 Announce Type: cross 
Abstract: Providing high quality software and evaluating the software reliability in softwarized networks are crucial for vendors and customers. These networks rely on open source code, which are sensitive to contain high number of bugs. Both, the knowledge about the code of previous releases as well as the bug history of the particular project can be used to evaluate the software reliability of a new software release based on SRGM. In this work a framework to predict the number of the bugs of a new release, as well as other reliability parameters, is proposed. An exemplary implementation of this framework to two particular open source projects, is described in detail. The difference between the prediction accuracy of the two projects is presented. Different alternatives to increase the prediction accuracy are proposed and compared in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21224v1</guid>
      <category>cs.SE</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasan Yagiz Ozkan, Madeleine Kaufmann, Wolfgang Kellerer, Carmen Mas-Machuca</dc:creator>
    </item>
    <item>
      <title>Meili: Enabling SmartNIC as a Service in the Cloud</title>
      <link>https://arxiv.org/abs/2312.11871</link>
      <description>arXiv:2312.11871v3 Announce Type: replace 
Abstract: SmartNICs are touted as an attractive substrate for network application offloading, offering benefits in programmability, host resource saving, and energy efficiency. The current usage restricts offloading to local hosts and confines SmartNIC ownership to individual application teams, resulting in poor resource efficiency and scalability. This paper presents Meili, a novel system that realizes SmartNIC as a service to address these issues. Meili organizes heterogeneous SmartNIC resources as a pool and offers a unified one-NIC abstraction to application developers. This allows developers to focus solely on the application logic while dynamically optimizing their performance needs. Our evaluation on NVIDIA BlueField series and AMD Pensando SmartNICs demonstrates that Meili achieves scalable single-flow throughput with a maximum 8 {\mu}s latency overhead and enhances resource efficiency by 3.07$\times$ compared to standalone deployments and 1.44$\times$ compared to state-of-the-art microservice deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11871v3</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiang Su, Shaofeng Wu, Zhixiong Niu, Ran Shu, Peng Cheng, Yongqiang Xiong, Zaoxing Liu, Hong Xu</dc:creator>
    </item>
    <item>
      <title>Human-Centric Decision-Making in Cell-Less 6G Networks</title>
      <link>https://arxiv.org/abs/2402.14344</link>
      <description>arXiv:2402.14344v2 Announce Type: replace 
Abstract: In next-generation networks, cells will be replaced by a collection of points-of-access (PoAs), with overlapping coverage areas and/or different technologies. Along with a promise for greater performance and flexibility, this creates further pressure on network management algorithms, which must make joint decisions on (i) PoA-to-user association and (ii) PoA management. We solve this challenging problem through an efficient and effective solution concept called Cluster-then-Match (CtM). Importantly, CtM makes human-centric decisions, where pure network performance is balanced against metrics like energy consumption and electromagnetic field exposure, which concern all humans in the network area -- including those who are not network users. Through our performance evaluation, which leverages detailed models for EMF exposure estimation and standard-specified signal propagation models, we show that CtM outperforms state-of-the-art network management schemes, including those utilizing machine learning, reducing energy consumption by over 80%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14344v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma Chiaramello, Carla Fabiana Chiasserini, Francesco Malandrino, Alessandro Nordio, Marta Parazzini, Alvaro Valcarce</dc:creator>
    </item>
    <item>
      <title>Can LLMs Understand Computer Networks? Towards a Virtual System Administrator</title>
      <link>https://arxiv.org/abs/2404.12689</link>
      <description>arXiv:2404.12689v2 Announce Type: replace 
Abstract: Recent advancements in Artificial Intelligence, and particularly Large Language Models (LLMs), offer promising prospects for aiding system administrators in managing the complexity of modern networks. However, despite this potential, a significant gap exists in the literature regarding the extent to which LLMs can understand computer networks. Without empirical evidence, system administrators might rely on these models without assurance of their efficacy in performing network-related tasks accurately.
  In this paper, we are the first to conduct an exhaustive study on LLMs' comprehension of computer networks. We formulate several research questions to determine whether LLMs can provide correct answers when supplied with a network topology and questions on it. To assess them, we developed a thorough framework for evaluating LLMs' capabilities in various network-related tasks. We evaluate our framework on multiple computer networks employing proprietary (e.g., GPT4) and open-source (e.g., Llama2) models. Our findings in general purpose LLMs using a zero-shot scenario demonstrate promising results, with the best model achieving an average accuracy of 79.3%. Proprietary LLMs achieve noteworthy results in small and medium networks, while challenges persist in comprehending complex network topologies, particularly for open-source models. Moreover, we provide insight into how prompt engineering can enhance the accuracy of some tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12689v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Denis Donadel, Francesco Marchiori, Luca Pajola, Mauro Conti</dc:creator>
    </item>
    <item>
      <title>Slice-aware Resource Allocation and Admission Control for Smart Factory Wireless Networks</title>
      <link>https://arxiv.org/abs/2405.08976</link>
      <description>arXiv:2405.08976v3 Announce Type: replace 
Abstract: The 5th generation (5G) and beyond network offers substantial promise as the ideal wireless technology to replace the existing inflexible wired connections in traditional factories of today. 5G network slicing allows for tailored allocation of resources to different network services, each with unique Quality of Service (QoS) requirements. This paper presents a novel solution for slice-aware radio resource allocation based on a convex optimisation and control framework for applications in smart factory wireless networks. The proposed framework dynamically allocates minimum power and sub-channels to downlink mixed service type industrial users categorised into three slices: Capacity Limited (CL), Ultra Reliable Low Latency Communication (URLLC), and Time Sensitive (TS) slices. Given that the base station (BS) has limited transmission power, we enforce admission control by effectively relaxing the target rate constraints for current connections in the CL slice. This rate readjustment occurs whenever power consumption exceeds manageable levels. Simulation results show that our approach minimises power, allocates sub-channels to users, maintains slice isolation, and delivers QoS-specific communications to users in all the slices despite time-varying number of users and changing network conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08976v3</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Regina Ochonu, Josep Vidal</dc:creator>
    </item>
    <item>
      <title>Learning-based Big Data Sharing Incentive in Mobile AIGC Networks</title>
      <link>https://arxiv.org/abs/2407.10980</link>
      <description>arXiv:2407.10980v2 Announce Type: replace 
Abstract: Rapid advancements in wireless communication have led to a dramatic upsurge in data volumes within mobile edge networks. These substantial data volumes offer opportunities for training Artificial Intelligence-Generated Content (AIGC) models to possess strong prediction and decision-making capabilities. AIGC represents an innovative approach that utilizes sophisticated generative AI algorithms to automatically generate diverse content based on user inputs. Leveraging mobile edge networks, mobile AIGC networks enable customized and real-time AIGC services for users by deploying AIGC models on edge devices. Nonetheless, several challenges hinder the provision of high-quality AIGC services, including issues related to the quality of sensing data for AIGC model training and the establishment of incentives for big data sharing from mobile devices to edge devices amidst information asymmetry. In this paper, we initially define a Quality of Data (QoD) metric based on the age of information to quantify the quality of sensing data. Subsequently, we propose a contract theoretic model aimed at motivating mobile devices for big data sharing. Furthermore, we employ a Proximal Policy Optimization (PPO) algorithm to determine the optimal contract. Numerical results demonstrate the efficacy and reliability of the proposed PPO-based contract model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10980v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Wen, Yang Zhang, Yulin Chen, Weifeng Zhong, Xumin Huang, Lei Liu, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>Naeural AI OS -- Decentralized ubiquitous computing MLOps execution engine</title>
      <link>https://arxiv.org/abs/2306.08708</link>
      <description>arXiv:2306.08708v3 Announce Type: replace-cross 
Abstract: Over the past few years, ubiquitous, or pervasive computing has gained popularity as the primary approach for a wide range of applications, including enterprise-grade systems, consumer applications, and gaming systems. Ubiquitous computing refers to the integration of computing technologies into everyday objects and environments, creating a network of interconnected devices that can communicate with each other and with humans. By using ubiquitous computing technologies, communities can become more connected and efficient, with members able to communicate and collaborate more easily. This enabled interconnectedness and collaboration can lead to a more successful and sustainable community. The spread of ubiquitous computing, however, has emphasized the importance of automated learning and smart applications in general. Even though there have been significant strides in Artificial Intelligence and Deep Learning, large scale adoption has been hesitant due to mounting pressure on expensive and highly complex cloud numerical-compute infrastructures. Adopting, and even developing, practical machine learning systems can come with prohibitive costs, not only in terms of complex infrastructures but also of solid expertise in Data Science and Machine Learning. In this paper we present an innovative approach for low-code development and deployment of end-to-end AI cooperative application pipelines. We address infrastructure allocation, costs, and secure job distribution in a fully decentralized global cooperative community based on tokenized economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08708v3</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beatrice Milik, Stefan Saraev, Cristian Bleotiu, Radu Lupaescu, Bogdan Hobeanu, Andrei Ionut Damian</dc:creator>
    </item>
    <item>
      <title>Green Edge AI: A Contemporary Survey</title>
      <link>https://arxiv.org/abs/2312.00333</link>
      <description>arXiv:2312.00333v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) technologies have emerged as pivotal enablers across a multitude of industries largely due to their significant resurgence over the past decade. The transformative power of AI is primarily derived from the utilization of deep neural networks (DNNs), which require extensive data for training and substantial computational resources for processing. Consequently, DNN models are typically trained and deployed on resource-rich cloud servers. However, due to potential latency issues associated with cloud communications, deep learning (DL) workflows are increasingly being transitioned to wireless edge networks in proximity to end-user devices (EUDs). This shift is designed to support latency-sensitive applications and has given rise to a new paradigm of edge AI, which will play a critical role in upcoming sixth-generation (6G) networks to support ubiquitous AI applications. Despite its considerable potential, edge AI faces substantial challenges, mostly due to the dichotomy between the resource limitations of wireless edge networks and the resource-intensive nature of DL. Specifically, the acquisition of large-scale data, as well as the training and inference processes of DNNs, can rapidly deplete the battery energy of EUDs. This necessitates an energy-conscious approach to edge AI to ensure both optimal and sustainable performance. In this paper, we present a contemporary survey on green edge AI. We commence by analyzing the principal energy consumption components of edge AI systems to identify the fundamental design principles of green edge AI. Guided by these principles, we then explore energy-efficient design methodologies for the three critical tasks in edge AI systems, including training data acquisition, edge training, and edge inference. Finally, we underscore potential future research directions to further enhance the energy efficiency of edge AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00333v2</guid>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuyi Mao, Xianghao Yu, Kaibin Huang, Ying-Jun Angela Zhang, Jun Zhang</dc:creator>
    </item>
  </channel>
</rss>
