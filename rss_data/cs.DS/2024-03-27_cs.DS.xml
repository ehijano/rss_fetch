<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Mar 2024 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Note On Lookahead In Real Life And Computing</title>
      <link>https://arxiv.org/abs/2403.17942</link>
      <description>arXiv:2403.17942v1 Announce Type: new 
Abstract: Past, Present and Future are considered to be three temporal and logical concepts which are well defined by human beings for their existence and growth. We, as human beings, have the privilege of using our intelligence to mentally execute an activity before physical occurrence of the same in the real world. Knowledge of the past, aplomb of present and visualisation for the future correspond to three concepts such as look-back, look-at and look-ahead respectively in real life as well as in diversified domains of computing. Look-Ahead(LA) deals with the future prediction of information and processing of input to produce the output in advance. In this article, our main objective is to learn, understand and explore the concept of LA and design novel models as solution for real world problems. We present three well known algorithmic frameworks used in practice based on availability of input information such as offline, online and semi-online. We introduce interesting real life applications and well known computing problems where LA plays a significant role for making a process, system or algorithm efficient. We define new types of LA and propose a taxonomy for LA based on literature review for designing novel LA models in future. Using the concept of LA, We identify and present many interesting and non-trivial research challenges as future potential research directions. Intuitively, we observe that LA can be used as a powerful tool and framework for future researchers in design of efficient computational models and algorithms for solving non-trivial and challenging optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17942v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Burle Sharma, Rakesh Mohanty, Sucheta Panda</dc:creator>
    </item>
    <item>
      <title>Online Submodular Welfare Maximization Meets Post-Allocation Stochasticity and Reusability</title>
      <link>https://arxiv.org/abs/2403.18059</link>
      <description>arXiv:2403.18059v1 Announce Type: new 
Abstract: We generalize the problem of online submodular welfare maximization to incorporate a variety of new elements arising from reusability, stochastic rewards, combinatorial actions and similar features that have received significant attention in recent years. For our general formulation, we show that a non-adaptive Greedy algorithm achieves the highest possible competitive ratio against an adaptive offline benchmark in the adversarial arrival model and in the unknown IID stochastic arrival model. In addition to generalizing several previous results, this shows that, in general, adaptivity to stochastic rewards (and similar features) offers no theoretical (worst-case) benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18059v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajan Udwani</dc:creator>
    </item>
    <item>
      <title>Minimum sum vertex cover: kernelization and parameterized algorithms</title>
      <link>https://arxiv.org/abs/2403.18497</link>
      <description>arXiv:2403.18497v1 Announce Type: new 
Abstract: Given an ordering of the vertices of a graph, the cost of covering an edge is the smaller number of its two ends. The minimum sum vertex cover problem asks for an ordering that minimizes the total cost of covering all edges. We consider parameterized complexity of this problem, using the largest cost~$k$ of covering a single edge as the parameter. Note that the first $k$ vertices form a (not necessarily minimal) vertex cover of the graph, and ordering of vertices after $k$ is irrelevant. We present a $(2k^2 + 3k)$-vertex kernel and an $O(|E(G)| + 2^kk! k^4)$-time algorithm for the minimum sum vertex cover problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18497v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixin Cao, Jingyi Liu, Jianxin Wang</dc:creator>
    </item>
    <item>
      <title>Realizing temporal transportation trees</title>
      <link>https://arxiv.org/abs/2403.18513</link>
      <description>arXiv:2403.18513v1 Announce Type: new 
Abstract: In this paper, we study the complexity of the \textit{periodic temporal graph realization} problem with respect to upper bounds on the fastest path durations among its vertices. This constraint with respect to upper bounds appears naturally in transportation network design applications where, for example, a road network is given, and the goal is to appropriately schedule periodic travel routes, while not exceeding some desired upper bounds on the travel times. This approach is in contrast to verification applications of the graph realization problems, where exact values for the distances (respectively, fastest travel times) are given, following some kind of precise measurement. In our work, we focus only on underlying tree topologies, which are fundamental in many transportation network applications.
  As it turns out, the periodic upper-bounded temporal tree realization problem (TTR) has a very different computational complexity behavior than both (i) the classic graph realization problem with respect to shortest path distances in static graphs and (ii) the periodic temporal graph realization problem with exact given fastest travel times (which was recently introduced). First, we prove that, surprisingly, TTR is NP-hard, even for a constant period $\Delta$ and when the input tree $G$ satisfies at least one of the following conditions: (a) $G$ has a constant diameter, or (b) $G$ has constant maximum degree. In contrast, when we are given exact values of the fastest travel delays, the problem is known to be solvable in polynomial time. Second, we prove that TTR is fixed-parameter tractable (FPT) with respect to the number of leaves in the input tree $G$, via a novel combination of techniques for totally unimodular matrices and mixed integer linear programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18513v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George B. Mertzios, Hendrik Molter, Paul G. Spirakis</dc:creator>
    </item>
    <item>
      <title>JumpBackHash: Say Goodbye to the Modulo Operation to Distribute Keys Uniformly to Buckets</title>
      <link>https://arxiv.org/abs/2403.18682</link>
      <description>arXiv:2403.18682v1 Announce Type: new 
Abstract: The distribution of keys to a given number of buckets is a fundamental task in distributed data processing and storage. A simple, fast, and therefore popular approach is to map the hash values of keys to buckets based on the remainder after dividing by the number of buckets. Unfortunately, these mappings are not stable when the number of buckets changes, which can lead to severe spikes in system resource utilization, such as network or database requests. Consistent hash algorithms can minimize remappings, but are either significantly slower than the modulo-based approach, require floating-point arithmetic, or are based on a family of hash functions rarely available in standard libraries. This paper introduces JumpBackHash, which uses only integer arithmetic and a standard pseudorandom generator. Due to its speed and simple implementation, it can safely replace the modulo-based approach to improve assignment and system stability. A production-ready Java implementation of JumpBackHash has been released as part of the Hash4j open source library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18682v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Otmar Ertl</dc:creator>
    </item>
    <item>
      <title>New Graph and Hypergraph Container Lemmas with Applications in Property Testing</title>
      <link>https://arxiv.org/abs/2403.18777</link>
      <description>arXiv:2403.18777v1 Announce Type: new 
Abstract: The graph and hypergraph container methods are powerful tools with a wide range of applications across combinatorics. Recently, Blais and Seth (FOCS 2023) showed that the graph container method is particularly well-suited for the analysis of the natural canonical tester for two fundamental graph properties: having a large independent set and $k$-colorability. In this work, we show that the connection between the container method and property testing extends further along two different directions.
  First, we show that the container method can be used to analyze the canonical tester for many other properties of graphs and hypergraphs. We introduce a new hypergraph container lemma and use it to give an upper bound of $\widetilde{O}(kq^3/\epsilon)$ on the sample complexity of $\epsilon$-testing satisfiability, where $q$ is the number of variables per constraint and $k$ is the size of the alphabet. This is the first upper bound for the problem that is polynomial in all of $k$, $q$ and $1/\epsilon$. As a corollary, we get new upper bounds on the sample complexity of the canonical testers for hypergraph colorability and for every semi-homogeneous graph partition property.
  Second, we show that the container method can also be used to study the query complexity of (non-canonical) graph property testers. This result is obtained by introducing a new container lemma for the class of all independent set stars, a strict superset of the class of all independent sets. We use this container lemma to give a new upper bound of $\widetilde{O}(\rho^5/\epsilon^{7/2})$ on the query complexity of $\epsilon$-testing the $\rho$-independent set property. This establishes for the first time the non-optimality of the canonical tester for a non-homogeneous graph partition property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18777v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Blais, Cameron Seth</dc:creator>
    </item>
    <item>
      <title>Hypergraph Unreliability in Quasi-Polynomial Time</title>
      <link>https://arxiv.org/abs/2403.18781</link>
      <description>arXiv:2403.18781v1 Announce Type: new 
Abstract: The hypergraph unreliability problem asks for the probability that a hypergraph gets disconnected when every hyperedge fails independently with a given probability. For graphs, the unreliability problem has been studied over many decades, and multiple fully polynomial-time approximation schemes are known starting with the work of Karger (STOC 1995). In contrast, prior to this work, no non-trivial result was known for hypergraphs (of arbitrary rank).
  In this paper, we give quasi-polynomial time approximation schemes for the hypergraph unreliability problem. For any fixed $\varepsilon \in (0, 1)$, we first give a $(1+\varepsilon)$-approximation algorithm that runs in $m^{O(\log n)}$ time on an $m$-hyperedge, $n$-vertex hypergraph. Then, we improve the running time to $m\cdot n^{O(\log^2 n)}$ with an additional exponentially small additive term in the approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18781v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruoxu Cen, Jason Li, Debmalya Panigrahi</dc:creator>
    </item>
    <item>
      <title>On the Communication Complexity of Approximate Pattern Matching</title>
      <link>https://arxiv.org/abs/2403.18812</link>
      <description>arXiv:2403.18812v1 Announce Type: new 
Abstract: The decades-old Pattern Matching with Edits problem, given a length-$n$ string $T$ (the text), a length-$m$ string $P$ (the pattern), and a positive integer $k$ (the threshold), asks to list all fragments of $T$ that are at edit distance at most $k$ from $P$. The one-way communication complexity of this problem is the minimum amount of space needed to encode the answer so that it can be retrieved without accessing the input strings $P$ and $T$.
  The closely related Pattern Matching with Mismatches problem (defined in terms of the Hamming distance instead of the edit distance) is already well understood from the communication complexity perspective: Clifford, Kociumaka, and Porat [SODA 2019] proved that $\Omega(n/m \cdot k \log(m/k))$ bits are necessary and $O(n/m \cdot k\log (m|\Sigma|/k))$ bits are sufficient; the upper bound allows encoding not only the occurrences of $P$ in $T$ with at most $k$ mismatches but also the substitutions needed to make each $k$-mismatch occurrence exact.
  Despite recent improvements in the running time [Charalampopoulos, Kociumaka, and Wellnitz; FOCS 2020 and 2022], the communication complexity of Pattern Matching with Edits remained unexplored, with a lower bound of $\Omega(n/m \cdot k\log(m/k))$ bits and an upper bound of $O(n/m \cdot k^3\log m)$ bits stemming from previous research. In this work, we prove an upper bound of $O(n/m \cdot k \log^2 m)$ bits, thus establishing the optimal communication complexity up to logarithmic factors. We also show that $O(n/m \cdot k \log m \log (m|\Sigma|))$ bits allow encoding, for each $k$-error occurrence of $P$ in $T$, the shortest sequence of edits needed to make the occurrence exact.
  We leverage the techniques behind our new result on the communication complexity to obtain quantum algorithms for Pattern Matching with Edits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18812v1</guid>
      <category>cs.DS</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz Kociumaka, Jakob Nogler, Philip Wellnitz</dc:creator>
    </item>
    <item>
      <title>A Polynomial-time Algorithm for the Large Scale of Airplane Refueling Problem</title>
      <link>https://arxiv.org/abs/2210.11634</link>
      <description>arXiv:2210.11634v4 Announce Type: replace 
Abstract: Airplane refueling problem is a nonlinear unconstrained optimization problem with $n!$ feasible solutions. Given a fleet of $n$ airplanes with mid-air refueling technique, the question is to find the best refueling policy to make the last remaining airplane travels the farthest. In order to deal with the large scale of airplanes refueling instances, we proposed the definition of sequential feasible solution by employing the refueling properties of data structure. We proved that if an airplanes refueling instance has feasible solutions, it must have the sequential feasible solutions; and the optimal feasible solution must be the optimal sequential feasible solution. Then we proposed the sequential search algorithm which consists of two steps. The first step of the sequential search algorithm aims to seek out all of the sequential feasible solutions. When the input size of $n$ is greater than an index number, we proved that the number of the sequential feasible solutions will change to grow at a polynomial rate. The second step of the sequential search algorithm aims to search for the maximal sequential feasible solution by bubble sorting all of the sequential feasible solutions. Moreover, we built an efficient computability scheme, according to which we could forecast within a polynomial time the computational complexity of the sequential search algorithm that runs on any given airplanes refueling instance. Thus we could provide a computational strategy for decision makers or algorithm users by considering with their available computing resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.11634v4</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinchuan Cui, Xiaoya Li</dc:creator>
    </item>
    <item>
      <title>Computing Longest Common Subsequence under Cartesian-Tree Matching Model</title>
      <link>https://arxiv.org/abs/2402.19146</link>
      <description>arXiv:2402.19146v2 Announce Type: replace 
Abstract: Two strings of the same length are said to Cartesian-tree match (CT-match) if their Cartesian-trees are isomorphic [Park et al., TCS 2020]. Cartesian-tree matching is a natural model that allows for capturing similarities of numerical sequences. Oizumi et al. [CPM 2022] showed that subsequence pattern matching under CT-matching model can be solved in polynomial time. This current article follows and extends this line of research: We present the first polynomial-time algorithm that finds the longest common subsequence under CT-matching of two given strings $S$ and $T$ of length $n$, in $O(n^6)$ time and $O(n^4)$ space for general ordered alphabets. We then show that the problem has a faster solution in the binary case, by presenting an $O(n^2 / \log n)$-time and space algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.19146v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taketo Tsujimoto, Hiroki Shibata, Takuya Mieno, Yuto Nakashima, Shunsuke Inenaga</dc:creator>
    </item>
    <item>
      <title>Polyamorous Scheduling</title>
      <link>https://arxiv.org/abs/2403.00465</link>
      <description>arXiv:2403.00465v2 Announce Type: replace 
Abstract: Finding schedules for pairwise meetings between the members of a complex social group without creating interpersonal conflict is challenging, especially when different relationships have different needs. We formally define and study the underlying optimisation problem: Polyamorous Scheduling.
  In Polyamorous Scheduling, we are given an edge-weighted graph and try to find a periodic schedule of matchings in this graph such that the maximal weighted waiting time between consecutive occurrences of the same edge is minimised. We show that the problem is NP-hard and that there is no efficient approximation algorithm with a better ratio than 4/3 unless P = NP. On the positive side, we obtain an $O(\log n)$-approximation algorithm; indeed, a $O(\log \Delta)$-approximation for $\Delta$ the maximum degree, i.e., the largest number of relationships of any individual. We also define a generalisation of density from the Pinwheel Scheduling Problem, "poly density", and ask whether there exists a poly-density threshold similar to the 5/6-density threshold for Pinwheel Scheduling [Kawamura, STOC 2024]. Polyamorous Scheduling is a natural generalisation of Pinwheel Scheduling with respect to its optimisation variant, Bamboo Garden Trimming.
  Our work contributes the first nontrivial hardness-of-approximation reduction for any periodic scheduling problem, and opens up numerous avenues for further study of Polyamorous Scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00465v2</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Leszek G\k{a}sieniec, Benjamin Smith, Sebastian Wild</dc:creator>
    </item>
    <item>
      <title>COPR -- Efficient, large-scale log storage and retrieval</title>
      <link>https://arxiv.org/abs/2402.18355</link>
      <description>arXiv:2402.18355v2 Announce Type: replace-cross 
Abstract: Modern, large scale monitoring systems have to process and store vast amounts of log data in near real-time. At query time the systems have to find relevant logs based on the content of the log message using support structures that can scale to these amounts of data while still being efficient to use. We present our novel Compressed Probabilistic Retrieval algorithm (COPR), capable of answering Multi-Set Multi-Membership-Queries, that can be used as an alternative to existing indexing structures for streamed log data. In our experiments, COPR required up to 93% less storage space than the tested state-of-the-art inverted index and had up to four orders of magnitude less false-positives than the tested state-of-the-art membership sketch. Additionally, COPR achieved up to 250 times higher query throughput than the tested inverted index and up to 240 times higher query throughput than the tested membership sketch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18355v2</guid>
      <category>cs.IR</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Reichinger, Thomas Krismayer, Jan Rellermeyer</dc:creator>
    </item>
  </channel>
</rss>
