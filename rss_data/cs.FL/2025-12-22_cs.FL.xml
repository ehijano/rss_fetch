<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.FL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.FL</link>
    <description>cs.FL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.FL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Dec 2025 05:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>About Time: Model-free Reinforcement Learning with Timed Reward Machines</title>
      <link>https://arxiv.org/abs/2512.17637</link>
      <description>arXiv:2512.17637v1 Announce Type: cross 
Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17637v1</guid>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Majumdar, Ritam Raha, Rajarshi Roy, David Parker, Marta Kwiatkowska</dc:creator>
    </item>
    <item>
      <title>Prefix Trees Improve Memory Consumption in Large-Scale Continuous-Time Stochastic Models</title>
      <link>https://arxiv.org/abs/2512.17892</link>
      <description>arXiv:2512.17892v1 Announce Type: cross 
Abstract: Highly-concurrent system models with vast state spaces like Chemical Reaction Networks (CRNs) that model biological and chemical systems pose a formidable challenge to cutting-edge formal analysis tools. Although many symbolic approaches have been presented, transient probability analysis of CRNs, modeled as Continuous-Time Markov Chains (CTMCs), requires explicit state representation. For that purpose, current cutting-edge methods use hash maps, which boast constant average time complexity and linear memory complexity. However, hash maps often suffer from severe memory limitations on models with immense state spaces. To address this, we propose using prefix trees to store states for large, highly concurrent models (particularly CRNs) for memory savings. We present theoretical analyses and benchmarks demonstrating the favorability of prefix trees over hash maps for very large state spaces. Additionally, we propose using a Bounded Model Checking (BMC) pre-processing step to impose a variable ordering to further improve memory usage along with preliminary evaluations suggesting its effectiveness. We remark that while our work is motivated primarily by the challenges posed by CRNs, it is generalizable to all CTMC models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17892v1</guid>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Landon Taylor, Joshua Jeppson, Ahmed Irfan, Lukas Buecherl, Chris Myers, Zhen Zhang</dc:creator>
    </item>
    <item>
      <title>Computing measures of weak-MSO definable sets of trees</title>
      <link>https://arxiv.org/abs/2410.13479</link>
      <description>arXiv:2410.13479v2 Announce Type: replace 
Abstract: This work addresses the problem of computing measures of recognisable sets of infinite trees. An algorithm is provided to compute the probability measure of a tree language recognisable by a weak alternating automaton, or equivalently definable in weak monadic second-order logic. The measure is the uniform coin-flipping measure or more generally it is generated by a~branching stochastic process. The class of tree languages in consideration, although smaller than all regular tree languages, comprises in particular the languages definable in the alternation-free mu-calculus or in temporal logic CTL. Thus, the new algorithm may enhance the toolbox of probabilistic model checking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13479v2</guid>
      <category>cs.FL</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damian Niwi\'nski, Marcin Przyby{\l}ko, Micha{\l} Skrzypczak</dc:creator>
    </item>
    <item>
      <title>Deterministic Suffix-reading Automata</title>
      <link>https://arxiv.org/abs/2505.09353</link>
      <description>arXiv:2505.09353v3 Announce Type: replace 
Abstract: We introduce deterministic suffix-reading automata (DSA), a new automaton model over finite words. Transitions in a DSA are labeled with words. From a state, a DSA triggers an outgoing transition on seeing a word ending with the transition's label. Therefore, rather than moving along an input word letter by letter, a DSA can jump along blocks of letters, with each block ending in a suitable suffix. This feature allows DSAs to recognize regular languages more concisely, compared to DFAs. In this work, we focus on questions around finding a minimal DSA for a regular language. The number of states is not a faithful measure of the size of a DSA, since the transition-labels contain strings of arbitrary length. Hence, we consider total-size (number of states + number of edges + total length of transition-labels) as the size measure of DSAs.
  We start by formally defining the model and providing a DSA-to-DFA conversion that allows to compare the expressiveness and succinctness of DSA with related automata models. Our main technical contribution is a method to derive DSAs from a given DFA: a DFA-to-DSA conversion. We make a surprising observation that the smallest DSA derived from the canonical DFA of a regular language L need not be a minimal DSA for L. This observation leads to a fundamental bottleneck in deriving a minimal DSA for a regular language. In fact, we prove that given a DFA and a number k, the problem of deciding if there exists an equivalent DSA of total-size atmost k is NP-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09353v3</guid>
      <category>cs.FL</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R Keerthan, B Srivathsan, R Venkatesh, Sagar Verma</dc:creator>
    </item>
  </channel>
</rss>
