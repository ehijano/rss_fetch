<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 May 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical comparison of Hidden Markov Models via Fragment Analysis</title>
      <link>https://arxiv.org/abs/2504.21046</link>
      <description>arXiv:2504.21046v1 Announce Type: new 
Abstract: Standard practice in Hidden Markov Model (HMM) selection favors the candidate with the highest full-sequence likelihood, although this is equivalent to making a decision based on a single realization. We introduce a \emph{fragment-based} framework that redefines model selection as a formal statistical comparison. For an unknown true model $\mathrm{HMM}_0$ and a candidate $\mathrm{HMM}_j$, let $\mu_j(r)$ denote the probability that $\mathrm{HMM}_j$ and $\mathrm{HMM}_0$ generate the same sequence of length~$r$. We show that if $\mathrm{HMM}_i$ is closer to $\mathrm{HMM}_0$ than $\mathrm{HMM}_j$, there exists a threshold $r^{*}$ -- often small -- such that $\mu_i(r)&gt;\mu_j(r)$ for all $r\geq r^{*}$. Sampling $k$ independent fragments yields unbiased estimators $\hat{\mu}_j(r)$ whose differences are asymptotically normal, enabling a straightforward $Z$-test for the hypothesis $H_0\!:\,\mu_i(r)=\mu_j(r)$. By evaluating only short subsequences, the procedure circumvents full-sequence likelihood computation and provides valid $p$-values for model comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21046v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos M. Hernandez-Suarez, Osval A. Montesinos-L\'opez</dc:creator>
    </item>
    <item>
      <title>A Hybrid Mixture of $t$-Factor Analyzers for Clustering High-dimensional Data</title>
      <link>https://arxiv.org/abs/2504.21120</link>
      <description>arXiv:2504.21120v1 Announce Type: new 
Abstract: This paper develops a novel hybrid approach for estimating the mixture model of $t$-factor analyzers (MtFA) that employs multivariate $t$-distribution and factor model to cluster and characterize grouped data. The traditional estimation method for MtFA faces computational challenges, particularly in high-dimensional settings, where the eigendecomposition of large covariance matrices and the iterative nature of Expectation-Maximization (EM) algorithms lead to scalability issues. We propose a computational scheme that integrates a profile likelihood method into the EM framework to efficiently obtain the model parameter estimates. The effectiveness of our approach is demonstrated through simulations showcasing its superior computational efficiency compared to the existing method, while preserving clustering accuracy and resilience against outliers. Our method is applied to cluster the Gamma-ray bursts, reinforcing several claims in the literature that Gamma-ray bursts have heterogeneous subpopulations and providing characterizations of the estimated groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21120v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.HE</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazeem Kareem, Fan Dai</dc:creator>
    </item>
    <item>
      <title>Graphical models with marginals in the exponential family</title>
      <link>https://arxiv.org/abs/2504.21122</link>
      <description>arXiv:2504.21122v1 Announce Type: new 
Abstract: Graphical models encode conditional independence statements of a multivariate distribution via a graph. Traditionally, the marginal distributions in a graphical model are assumed to be Gaussian. In this paper, we propose a three-level hierarchical model that functions as the hyper-Markov law that enables a graphical model with marginals in the exponential family with quadratic variance function. Inference on the model parameters is made using a Bayesian approach. We perform a simulation study and real data analyses to illustrate the usefulness of our models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21122v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis E. Nieto-Barajas, Sim\'on Lunag\'omez</dc:creator>
    </item>
    <item>
      <title>Simultaneous Nonparametric Confidence Bands for Load-Sharing Systems</title>
      <link>https://arxiv.org/abs/2504.21219</link>
      <description>arXiv:2504.21219v1 Announce Type: new 
Abstract: Load-sharing systems arise in many different reliability applications, for instance, when modeling tensile strength of fibrous composites in textile industry or lifetimes of redundant technical systems in engineering. Sequential order statistics serve as a flexible model for the ordered component failure times of such systems and allow the residual lifetime distribution of the components to change after each component failure. In a proportional hazard rate setting, the model consists of some baseline distribution function and several model parameters describing successive adjustments of the hazard rates of the operating components. This work provides nonparametric confidence bands for the baseline distribution function, where the model parameters may be known or unknown. In case of known model parameters, we show how to construct exact confidence bands based on Kolmogorov-Smirnov type statistics, which are distribution-free with respect to the baseline distribution. If the model parameters are unknown, finite sample inference turns out to be infeasible, and asymptotic confidence bands for the baseline distribution function are derived. As a technical tool, we extend the existing asymptotic theory of semiparametric estimators based on the profile-likelihood approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21219v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Bedbur, Johann K\"ohne, Fabian Mies</dc:creator>
    </item>
    <item>
      <title>Hurdle Network Model With Latent Dynamic Shrinkage For Enhanced Edge Prediction in Zero-Inflated Directed Network Time Series</title>
      <link>https://arxiv.org/abs/2504.21275</link>
      <description>arXiv:2504.21275v1 Announce Type: new 
Abstract: This article aims to model international trade relationships among 29 countries in the apparel industry between 1994 and 2013. Bilateral trade flows can be represented as a directed network, where nodes correspond to countries and directed edges indicate trade flows (i.e., whether one country exported to another in a given year). Additionally, node (e.g., GDP) and edge-specific (e.g., labor provision) covariates are also available. The study focuses on two key challenges: (1) capturing multiple forms of temporal and network dependence, and dependence on covariates; and (2) accounting for potential trade volume as an important but partially observed edge-specific covariate, which is only available for country pairs that engaged in trade.
  To address these challenges, we introduce the dynamic hurdle network model (Hurdle-Net) for zero-inflated directed network time series that incorporates several novel features. First, it represents the time series as a paired binary and continuous time series and utilizes a hurdle model that effectively handles sparsity in edge occurrence. Second, the model captures evolving network dependencies using node-specific latent variables governed by a dynamic shrinkage process. Third, it leverages a shared latent structure across the binary and continuous components, reflecting the fact that both networks involve the same nodes. Finally, the model employs a generalized logistic link function to relate edge occurrence to edge weight, allowing for a parsimonious and coherent hierarchical Bayesian framework that jointly models both network components. Compared to static or independent models, Hurdle-Net provides improved model selection, estimation, and prediction performance for analyzing international trade patterns. Its effectiveness is demonstrated through simulation studies and an application to bilateral trade flow data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21275v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandipan Pramanik, Raymond Robertson, Yang Ni</dc:creator>
    </item>
    <item>
      <title>Conditional inference for multi-omics data with right censored data</title>
      <link>https://arxiv.org/abs/2504.21324</link>
      <description>arXiv:2504.21324v1 Announce Type: new 
Abstract: Recent advances in multi-omics technology highlight the need for statistical methods that account for complex dependencies among biological layers. In this paper, we propose a novel Multi-Omics Factor-Adjusted Cox (MOFA-Cox) model to handle multi-omics survival data, addressing the intricate correlation structures across different omics layers. Building upon this model, we introduce a decorrelated score test for the Cox model in high-dimensional survival analysis. We establish the theoretical properties of our test statistic, which show that it admits a closed-form asymptotic distribution, eliminating the need for resampling. We further analyze its local power under local alternatives. Importantly, our test statistic does not require a sparsity assumption on the covariates of interest, broadening its applicability. Numerical studies and an applaication to the TCGA breast cancer dataset demonstrate the effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21324v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heyuan Zhang, Meiling Hao, Lianqiang Qu, Liuquan Sun</dc:creator>
    </item>
    <item>
      <title>Bayesian Wasserstein Repulsive Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2504.21391</link>
      <description>arXiv:2504.21391v1 Announce Type: new 
Abstract: We develop the Bayesian Wasserstein repulsive Gaussian mixture model that promotes well-separated clusters. Unlike existing repulsive mixture approaches that focus on separating the component means, our method encourages separation between mixture components based on the Wasserstein distance. We establish posterior contraction rates within the framework of nonparametric density estimation. Posterior sampling is performed using a blocked-collapsed Gibbs sampler. Through simulation studies and real data applications, we demonstrate the effectiveness of the proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21391v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weipeng Huang, Tin Lok James Ng</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to sharing information on sensitivity of a Multi-Cancer Early Detection test across and within tumour types and stages</title>
      <link>https://arxiv.org/abs/2504.21517</link>
      <description>arXiv:2504.21517v1 Announce Type: new 
Abstract: The Galleri (R) (GRAIL) multi-cancer early detection test measures circulating tumour DNA (ctDNA) to predict the presence of more than 50 different cancers, from a blood test. If sensitivity of the test to detect early-stage cancers is high, using it as part of a screening programme may lead to better cancer outcomes, but available evidence indicates there is heterogeneity in sensitivity between cancer types and stages. We describe a framework for sharing evidence on test sensitivity between cancer types and/or stages, examining whether models with different sharing assumptions are supported by the evidence and considering how further data could be used to strengthen inference. Bayesian hierarchical models were fitted, and the impact of information sharing in increasing precision of the estimates of test sensitivity for different cancer types and stages was examined. Assumptions on sharing were informed by evidence from a review of the literature on the determinants of ctDNA shedding and its detection in a blood test. Support was strongest for the assumption that sensitivity can be shared only across stage 4 for all cancer types. There was also support for the assumption that sensitivities can be shared across cancer types for each stage, if cancer types expected to have low sensitivity are excluded which increased precision of early-stage cancer sensitivity estimates and was considered the most appropriate model. High heterogeneity limited improvements in precision. For future research, elicitation of expert opinion could inform more realistic sharing assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21517v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sofia Dias, Yiwen Liu, Stephen Palmer, Marta O Soares</dc:creator>
    </item>
    <item>
      <title>Powerful randomization tests for subgroup analysis</title>
      <link>https://arxiv.org/abs/2504.21572</link>
      <description>arXiv:2504.21572v1 Announce Type: new 
Abstract: Randomization tests are widely used to generate valid $p$-values for testing sharp null hypotheses in finite-population causal inference. This article extends their application to subgroup analysis. We show that directly testing subgroup null hypotheses may lack power due to small subgroup sizes. Incorporating an estimator of the conditional average treatment effect (CATE) can substantially improve power but requires splitting the treatment variables between estimation and testing to preserve finite-sample validity. To this end, we propose BaR-learner, a Bayesian extension of the popular method R-learner for CATE estimation. BaR-learner imputes the treatment variables reserved for randomization tests, reducing information loss due to sample-splitting. Furthermore, we show that the treatment variables most informative for training BaR-learner are different from those most valuable for increasing test power. Motivated by this insight, we introduce AdaSplit, a sample-splitting procedure that adaptively allocates units between estimation and testing. Simulation studies demonstrate that our method yields more powerful randomization tests than baselines that omit CATE estimation or rely on random sample-splitting. We also apply our method to a blood pressure intervention trial, identifying patient subgroups with significant treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21572v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Zhang, Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Clustered Observational Studies with an Application to the Effectiveness of Magnet Nursing Hospitals</title>
      <link>https://arxiv.org/abs/2504.21617</link>
      <description>arXiv:2504.21617v1 Announce Type: new 
Abstract: In a clustered observational study, treatment is assigned to groups and all units within the group are exposed to the treatment. Here, we use a clustered observational study (COS) design to estimate the effectiveness of Magnet Nursing certificates for emergency surgery patients. Recent research has introduced specialized weighting estimators for the COS design that balance baseline covariates at the unit and cluster level. These methods allow researchers to adjust for observed confounders, but are sensitive to unobserved confounding. In this paper, we develop new sensitivity analysis methods tailored to weighting estimators for COS designs. We provide several key contributions. First, we introduce a key bias decomposition, tailored to the specific confounding structure that arises in a COS. Second, we develop a sensitivity framework for weighted COS designs that constrain the error in the underlying weights. We introduce both a marginal sensitivity model and a variance-based sensitivity model, and extend both to accommodate multiple estimands. Finally, we propose amplification and benchmarking methods to better interpret the results. Throughout, we illustrate our proposed methods by analyzing the effectiveness of Magnet nursing hospitals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21617v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melody Huang, Eli Ben-Michael, Matthew McHugh, Luke Keele</dc:creator>
    </item>
    <item>
      <title>Conditional independence testing with a single realization of a multivariate nonstationary nonlinear time series</title>
      <link>https://arxiv.org/abs/2504.21647</link>
      <description>arXiv:2504.21647v1 Announce Type: new 
Abstract: Identifying relationships among stochastic processes is a key goal in disciplines that deal with complex temporal systems, such as economics. While the standard toolkit for multivariate time series analysis has many advantages, it can be difficult to capture nonlinear dynamics using linear vector autoregressive models. This difficulty has motivated the development of methods for variable selection, causal discovery, and graphical modeling for nonlinear time series, which routinely employ nonparametric tests for conditional independence. In this paper, we introduce the first framework for conditional independence testing that works with a single realization of a nonstationary nonlinear process. The key technical ingredients are time-varying nonlinear regression, time-varying covariance estimation, and a distribution-uniform strong Gaussian approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21647v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Wieck-Sosa, Michel F. C. Haddad, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>While-alive regression analysis of composite survival endpoints</title>
      <link>https://arxiv.org/abs/2504.21710</link>
      <description>arXiv:2504.21710v1 Announce Type: new 
Abstract: Composite endpoints, which combine two or more distinct outcomes, are frequently used in clinical trials to enhance the event rate and improve the statistical power. In the recent literature, the while-alive cumulative frequency measure offers a strong alternative to define composite survival outcomes, by relating the average event rate to the survival time. Although non-parametric methods have been proposed for two-sample comparisons between cumulative frequency measures in clinical trials, limited attention has been given to regression methods that directly address time-varying effects in while-alive measures for composite survival outcomes. Motivated by an individually randomized trial (HF-ACTION) and a cluster randomized trial (STRIDE), we address this gap by developing a regression framework for while-alive measures for composite survival outcomes that include a terminal component event. Our regression approach uses splines to model time-varying association between covariates and a while-alive loss rate of all component events, and can be applied to both independent and clustered data. We derive the asymptotic properties of the regression estimator in each setting and evaluate its performance through simulations. Finally, we apply our regression method to analyze data from the HF-ACTION individually randomized trial and the STRIDE cluster randomized trial. The proposed methods are implemented in the WAreg R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21710v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Fang, Hajime Uno, Fan Li</dc:creator>
    </item>
    <item>
      <title>Easily Computed Marginal Likelihoods for Multivariate Mixture Models Using the THAMES Estimator</title>
      <link>https://arxiv.org/abs/2504.21812</link>
      <description>arXiv:2504.21812v1 Announce Type: new 
Abstract: We present a new version of the truncated harmonic mean estimator (THAMES) for univariate or multivariate mixture models. The estimator computes the marginal likelihood from Markov chain Monte Carlo (MCMC) samples, is consistent, asymptotically normal and of finite variance. In addition, it is invariant to label switching, does not require posterior samples from hidden allocation vectors, and is easily approximated, even for an arbitrarily high number of components. Its computational efficiency is based on an asymptotically optimal ordering of the parameter space, which can in turn be used to provide useful visualisations. We test it in simulation settings where the true marginal likelihood is available analytically. It performs well against state-of-the-art competitors, even in multivariate settings with a high number of components. We demonstrate its utility for inference and model selection on univariate and multivariate data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21812v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Metodiev, Nicholas J. Irons, Marie Perrot-Dock\`es, Pierre Latouche, Adrian E. Raftery</dc:creator>
    </item>
    <item>
      <title>An Axiomatic Approach to Comparing Sensitivity Parameters</title>
      <link>https://arxiv.org/abs/2504.21106</link>
      <description>arXiv:2504.21106v1 Announce Type: cross 
Abstract: Many methods are available for assessing the importance of omitted variables. These methods typically make different, non-falsifiable assumptions. Hence the data alone cannot tell us which method is most appropriate. Since it is unreasonable to expect results to be robust against all possible robustness checks, researchers often use methods deemed "interpretable", a subjective criterion with no formal definition. In contrast, we develop the first formal, axiomatic framework for comparing and selecting among these methods. Our framework is analogous to the standard approach for comparing estimators based on their sampling distributions. We propose that sensitivity parameters be selected based on their covariate sampling distributions, a design distribution of parameter values induced by an assumption on how covariates are assigned to be observed or unobserved. Using this idea, we define a new concept of parameter consistency, and argue that a reasonable sensitivity parameter should be consistent. We prove that the literature's most popular approach is inconsistent, while several alternatives are consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21106v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Diegert, Matthew A. Masten, Alexandre Poirier</dc:creator>
    </item>
    <item>
      <title>Multi-Domain Causal Discovery in Bijective Causal Models</title>
      <link>https://arxiv.org/abs/2504.21261</link>
      <description>arXiv:2504.21261v1 Announce Type: cross 
Abstract: We consider the problem of causal discovery (a.k.a., causal structure learning) in a multi-domain setting. We assume that the causal functions are invariant across the domains, while the distribution of the exogenous noise may vary. Under causal sufficiency (i.e., no confounders exist), we show that the causal diagram can be discovered under less restrictive functional assumptions compared to previous work. What enables causal discovery in this setting is bijective generation mechanisms (BGM), which ensures that the functional relation between the exogenous noise $E$ and the endogenous variable $Y$ is bijective and differentiable in both directions at every level of the cause variable $X = x$. BGM generalizes a variety of models including additive noise model, LiNGAM, post-nonlinear model, and location-scale noise model. Further, we derive a statistical test to find the parents set of the target variable. Experiments on various synthetic and real-world datasets validate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21261v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kasra Jalaldoust, Saber Salehkaleybar, Negar Kiyavash</dc:creator>
    </item>
    <item>
      <title>Real-time Program Evaluation using Anytime-valid Rank Tests</title>
      <link>https://arxiv.org/abs/2504.21595</link>
      <description>arXiv:2504.21595v1 Announce Type: cross 
Abstract: Counterfactual mean estimators such as difference-in-differences and synthetic control have grown into workhorse tools for program evaluation. Inference for these estimators is well-developed in settings where all post-treatment data is available at the time of analysis. However, in settings where data arrives sequentially, these tests do not permit real-time inference, as they require a pre-specified sample size T. We introduce real-time inference for program evaluation through anytime-valid rank tests. Our methodology relies on interpreting the absence of a treatment effect as exchangeability of the treatment estimates. We then convert these treatment estimates into sequential ranks, and construct optimal finite-sample valid sequential tests for exchangeability. We illustrate our methods in the context of difference-in-differences and synthetic control. In simulations, they control size even under mild exchangeability violations. While our methods suffer slight power loss at T, they allow for early rejection (before T) and preserve the ability to reject later (after T).</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21595v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam van Meer, Nick W. Koning</dc:creator>
    </item>
    <item>
      <title>On the Robustness of Mixture Models in the Presence of Hidden Markov Regimes with Covariate-Dependent Transition Probabilities</title>
      <link>https://arxiv.org/abs/2504.21669</link>
      <description>arXiv:2504.21669v1 Announce Type: cross 
Abstract: This paper studies the robustness of quasi-maximum-likelihood (QML) estimation in hidden Markov models (HMMs) when the regime-switching structure is misspecified. Specifically, we examine the case where the true data-generating process features a hidden Markov regime sequence with covariate-dependent transition probabilities, but estimation proceeds under a simplified mixture model that assumes regimes are independent and identically distributed. We show that the parameters governing the conditional distribution of the observables can still be consistently estimated under this misspecification, provided certain regularity conditions hold. Our results highlight a practical benefit of using computationally simpler mixture models in settings where regime dependence is complex or difficult to model directly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21669v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Demian Pouzo, Martin Sola, Zacharias Psaradakis</dc:creator>
    </item>
    <item>
      <title>Assessing Racial Disparities in Healthcare Expenditures Using Causal Path-Specific Effects</title>
      <link>https://arxiv.org/abs/2504.21688</link>
      <description>arXiv:2504.21688v1 Announce Type: cross 
Abstract: Racial disparities in healthcare expenditures are well-documented, yet the underlying drivers remain complex and require further investigation. This study employs causal and counterfactual path-specific effects to quantify how various factors, including socioeconomic status, insurance access, health behaviors, and health status, mediate these disparities. Using data from the Medical Expenditures Panel Survey, we estimate how expenditures would differ under counterfactual scenarios in which the values of specific mediators were aligned across racial groups along selected causal pathways. A key challenge in this analysis is ensuring robustness against model misspecification while addressing the zero-inflation and right-skewness of healthcare expenditures. For reliable inference, we derive asymptotically linear estimators by integrating influence function-based techniques with flexible machine learning methods, including super learners and a two-part model tailored to the zero-inflated, right-skewed nature of healthcare expenditures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21688v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaxian Ou, Xinwei He, David Benkeser, Razieh Nabi</dc:creator>
    </item>
    <item>
      <title>Flexible Bayesian Multiple Comparison Adjustment Using Dirichlet Process and Beta-Binomial Model Priors</title>
      <link>https://arxiv.org/abs/2208.07086</link>
      <description>arXiv:2208.07086v3 Announce Type: replace 
Abstract: Researchers frequently wish to assess the equality or inequality of groups, but this poses the challenge of adequately adjusting for multiple comparisons. Statistically, all possible configurations of equality and inequality constraints can be uniquely represented as partitions of groups, where any number of groups are equal if they are in the same subset of the partition. In a Bayesian framework, one can adjust for multiple comparisons by constructing a suitable prior distribution over all possible partitions. Inspired by work on variable selection in regression, we propose a class of flexible beta-binomial priors for multiple comparison adjustment. We compare this prior setup to the Dirichlet process prior suggested by Gopalan and Berry (1998) and multiple comparison adjustment methods that do not specify a prior over partitions directly. Our approach not only allows researchers to assess pairwise equality constraints but simultaneously all possible equalities among all groups. Since the space of possible partitions grows rapidly -- for ten groups, there are already 115,975 possible partitions -- we use a stochastic search algorithm to efficiently explore the space. Our method is implemented in the Julia package EqualitySampler, and we illustrate it on examples related to the comparison of means, standard deviations, and proportions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07086v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Don van den Bergh, Fabian Dablander</dc:creator>
    </item>
    <item>
      <title>Conic Sparsity: Estimation of Regression Parameters in Closed Convex Polyhedral Cones</title>
      <link>https://arxiv.org/abs/2302.01974</link>
      <description>arXiv:2302.01974v3 Announce Type: replace 
Abstract: Statistical problems often involve linear equality and inequality constraints on model parameters. Direct estimation of parameters restricted to general polyhedral cones, particularly when one is interested in estimating low dimensional features, may be challenging. We use a dual form parameterization to characterize parameter vectors restricted to lower dimensional faces of polyhedral cones and use the characterization to define a notion of 'sparsity' on such cones. We show that the proposed notion agrees with the usual notion of sparsity in the unrestricted case and prove the validity of the proposed definition as a measure of sparsity. The identifiable parameterization of the lower dimensional faces allows a generalization of popular spike-and-slab priors to a closed convex polyhedral cone. The prior measure utilizes the geometry of the cone by defining a Markov random field over the adjacency graph of the extreme rays of the cone. We describe an efficient way of computing the posterior of the parameters in the restricted case. We illustrate the usefulness of the proposed methodology for imposing linear equality and inequality constraints by using wearables data from the National Health and Nutrition Examination Survey (NHANES) actigraph study where the daily average activity profiles of participants exhibit patterns that seem to obey such constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01974v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neha Agarwala, Arkaprava Roy, Anindya Roy</dc:creator>
    </item>
    <item>
      <title>Nonlinear Causal Discovery with Confounders</title>
      <link>https://arxiv.org/abs/2302.03178</link>
      <description>arXiv:2302.03178v3 Announce Type: replace 
Abstract: This article introduces a causal discovery method to learn nonlinear relationships in a directed acyclic graph with correlated Gaussian errors due to confounding. First, we derive model identifiability under the sublinear growth assumption. Then, we propose a novel method, named the Deconfounded Functional Structure Estimation (DeFuSE), consisting of a deconfounding adjustment to remove the confounding effects and a sequential procedure to estimate the causal order of variables. We implement DeFuSE via feedforward neural networks for scalable computation. Moreover, we establish the consistency of DeFuSE under an assumption called the strong causal minimality. In simulations, DeFuSE compares favorably against state-of-the-art competitors that ignore confounding or nonlinearity. Finally, we demonstrate the utility and effectiveness of the proposed approach with an application to gene regulatory network analysis. The Python implementation is available at https://github.com/chunlinli/defuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03178v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2023.2179490</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association, 2023</arxiv:journal_reference>
      <dc:creator>Chunlin Li, Xiaotong Shen, Wei Pan</dc:creator>
    </item>
    <item>
      <title>Consistent and Scalable Composite Likelihood Estimation of Probit Models with Crossed Random Effects</title>
      <link>https://arxiv.org/abs/2308.15681</link>
      <description>arXiv:2308.15681v4 Announce Type: replace 
Abstract: Estimation of crossed random effects models commonly requires computational costs that grow faster than linearly in the sample size $N$, often as fast as $\Omega(N^{3/2})$, making them unsuitable for large data sets. For non-Gaussian responses, integrating out the random effects to get a marginal likelihood brings significant challenges, especially for high dimensional integrals where the Laplace approximation might not be accurate. We develop a composite likelihood approach to probit models that replaces the crossed random effects model with some hierarchical models that require only one-dimensional integrals. We show how to consistently estimate the crossed effects model parameters from the hierarchical model fits. We find that the computation scales linearly in the sample size. We illustrate the method on about five million observations from Stitch Fix where the crossed effects formulation would require an integral of dimension larger than $700{,}000$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15681v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruggero Bellio, Swarnadip Ghosh, Art B. Owen, Cristiano Varin</dc:creator>
    </item>
    <item>
      <title>SARMA: Scalable Low-Rank High-Dimensional Autoregressive Moving Averages via Tensor Decomposition</title>
      <link>https://arxiv.org/abs/2405.00626</link>
      <description>arXiv:2405.00626v2 Announce Type: replace 
Abstract: Existing models for high-dimensional time series are overwhelmingly developed within the finite-order vector autoregressive (VAR) framework. However, the more flexible vector autoregressive moving averages (VARMA) have been much less considered. This paper introduces a Tucker-low-rank framework to efficiently capture VARMA-type dynamics for high-dimensional time series, named the Scalable ARMA (SARMA) model. It generalizes the Tucker-low-rank finite-order VAR model to the infinite-order case via flexible parameterizations of the AR coefficient tensor along the temporal dimension. The resulting model enables dynamic factor extraction across response and predictor variables, facilitating interpretation of group patterns. Additionally, we consider sparsity assumptions on the factor loadings to accomplish automatic variable selection and greater estimation efficiency. Both rank-constrained and sparsity-inducing estimators are developed for the proposed model, along with algorithms and model selection methods. The validity of our theory and empirical advantages of our approach are confirmed by simulation studies and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00626v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feiqing Huang, Kexin Lu, Yao Zheng</dc:creator>
    </item>
    <item>
      <title>A comparison between copula-based, mixed model, and estimating equation methods for analysis of bivariate correlated data</title>
      <link>https://arxiv.org/abs/2410.11892</link>
      <description>arXiv:2410.11892v2 Announce Type: replace 
Abstract: Regression analysis of non-normal correlated data is commonly performed using generalized linear mixed models (GLMM) and generalized estimating equations (GEE). The recent development of generalized joint regression models (GJRM) provide an alternative to these approaches by using copulas to flexibly model response variables and their dependence structures.
  This paper presents a simulation study comparing GJRM with alternative methods. We find that for the normal model with identity link, all models provide accurate estimates of marginal population parameters with comparable fit. However, for non-normal marginal distributions and when a non-identity link function is used, we highlight a major pitfall in the use of GLMMs: without significant adjustment they provide highly biased estimates of marginal population parameters. GLMM bias is more pronounced when the marginal distributions are more skewed or highly correlated. In addition, we highlight discrepancies between the estimates from different GLMM packages. In contrast, we find that GJRM provides unbiased estimates across all distributions with accurate standard errors when the copula is correctly specified. In addition, we highlight the advantages of the likelihood-based structure of the GJRM and show that it provides a model fit comparable, and often favorable to, GLMMs and GLMs. In a longitudinal study of doctor visits, we show that the GJRM provides better model fits than a comparable non-GAMLSS GLMM, GEE or GLM, due to its greater selection of marginal distributions. We conclude that the GJRM provides a superior approach to current popular models for regression of non-normal correlated data when population parameters are of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11892v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aydin Sareff-Hibbert, Gillian Z. Heller</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Models for Multiple Raters: a General Statistical Framework</title>
      <link>https://arxiv.org/abs/2410.21498</link>
      <description>arXiv:2410.21498v3 Announce Type: replace 
Abstract: Rating procedure is crucial in many applied fields (e.g., educational, clinical, emergency). It implies that a rater (e.g., teacher, doctor) rates a subject (e.g., student, doctor) on a rating scale. Given raters variability, several statistical methods have been proposed for assessing and improving the quality of ratings. Model estimation in the presence of heterogeneity has been one of the recent challenges in this research line. Consequently, several methods have been proposed to address this issue under a parametric multilevel modelling framework, in which strong distributional assumptions are made. We propose a more flexible model under the Bayesian nonparametric (BNP) framework, in which most of those assumptions are relaxed. By eliciting hierarchical discrete nonparametric priors, the model accommodates clusters among raters and subjects, naturally accounts for heterogeneity, and improves estimates accuracy. We propose a general BNP heteroscedastic framework to analyse continuous and coarse rating data and possible latent differences among subjects and raters. The estimated densities are used to make inferences about the rating process and the quality of the ratings. By exploiting a stick-breaking representation of the Dirichlet Process, a general class of Intraclass Correlation Coefficient (ICC) indices might be derived for these models. Our method allows us to independently identify latent similarities between subjects and raters and can be applied in precise education to improve personalised teaching programs or interventions. Theoretical results about the ICC are provided together with computational strategies. Simulations and a real-world application are presented, and possible future directions are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21498v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Mignemi, Ioanna Manolopoulou</dc:creator>
    </item>
    <item>
      <title>Assessing the replicability of RCTs in RWE emulations</title>
      <link>https://arxiv.org/abs/2412.09334</link>
      <description>arXiv:2412.09334v2 Announce Type: replace 
Abstract: Background: The standard regulatory approach to assess replication success is the two-trials rule, requiring both the original and the replication study to be significant with effect estimates in the same direction. The sceptical p-value was recently presented as an alternative method for the statistical assessment of the replicability of study results. Methods: We review the statistical properties of the sceptical p-value and compare those to the two-trials rule. We extend the methodology to non-inferiority trials and describe how to invert the sceptical p-value to obtain confidence intervals. We illustrate the performance of the different methods using real-world evidence emulations of randomized, controlled trials (RCTs) conducted within the RCT DUPLICATE initiative. Results: The sceptical p-value depends not only on the two p-values, but also on sample size and effect size of the two studies. It can be calibrated to have the same Type-I error rate as the two-trials rule, but has larger power to detect an existing effect. In the application to the results from the RCT DUPLICATE initiative, the sceptical p- value leads to qualitatively similar results than the two-trials rule, but tends to show more evidence for treatment effects compared to the two-trials rule. Conclusion: The sceptical p-value represents a valid statistical measure to assess the replicability of study results and is especially useful in the context of real-world evidence emulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09334v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeanette K\"oppe, Charlotte Micheloud, Stella Erdmann, Rachel Heyard, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric mixtures of Archimedean copulas</title>
      <link>https://arxiv.org/abs/2412.09539</link>
      <description>arXiv:2412.09539v3 Announce Type: replace 
Abstract: Copula-based dependence modeling often relies on parametric formulations. This is mathematically convenient, but can be statistically inefficient when the parametric families are not suitable for the data and model in focus. A Bayesian nonparametric mixture of Archimedean copulas is introduced to increase the flexibility of copula-based dependence modeling. Specifically, the Poisson-Dirichlet process is used as a mixing distribution over the Archimedean copulas' parameter. Properties of the mixture model are studied for the main Archimedean families, and posterior distributions are sampled via their full conditional distributions. The performance of the model is illustrated via numerical experiments involving simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09539v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruyi Pan, Luis E. Nieto-Barajas, Radu V. Craiu</dc:creator>
    </item>
    <item>
      <title>An Efficient Dual ADMM for Huber Regression with Fused Lasso Penalty</title>
      <link>https://arxiv.org/abs/2501.05676</link>
      <description>arXiv:2501.05676v2 Announce Type: replace 
Abstract: The ordinary least squares estimate in linear regression is sensitive to the influence of errors with large variance, which reduces its robustness, especially when dealing with heavy-tailed errors or outliers frequently encountered in real-world scenarios. To address this issue and accommodate the sparsity of coefficients along with their sequential disparities, we combine the adaptive robust Huber loss function with a fused lasso penalty. This combination yields a robust estimator capable of simultaneously achieving estimation and variable selection. Furthermore, we utilize an efficient alternating direction method of multipliers to solve this regression model from a dual perspective. The effectiveness and efficiency of our proposed approach is demonstrated through numerical experiments carried out on both simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05676v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengjiao Shi, Yunhai Xiao</dc:creator>
    </item>
    <item>
      <title>A New Approach to Radiocarbon Summarisation: Rigorous Identification of Variations/Changepoints in the Occurrence Rate of Radiocarbon Samples using a Poisson Process</title>
      <link>https://arxiv.org/abs/2501.15980</link>
      <description>arXiv:2501.15980v3 Announce Type: replace 
Abstract: A commonly-used paradigm to estimate changes in the frequency of past events or the size of populations is to consider the occurrence rate of archaeological/environmental samples found at a site over time. The reliability of such a "dates-as-data" approach is highly dependent upon how the occurrence rates are estimated from the underlying samples, particularly when calendar age information for the samples is obtained from radiocarbon (14C). The most frequently used "14C-dates-as-data" approach of creating Summed Probability Distributions (SPDs) is not statistically valid, or coherent, and can provide highly misleading inference. Here, we provide an alternative method with a rigorous statistical underpinning that also provides valuable additional information on potential changepoints in the rate of events. Furthermore, unlike current SPD alternatives, our summarisation approach does not restrict users to pre-specified, rigid, summary formats (e.g., exponential or logistic growth) but instead flexibly adapts to the dates themselves. Our methodology ensures more reliable "14C-dates-as-data" analyses, allowing us to better assess and identify potential signals present. We model the occurrence of events, each assumed to leave a radiocarbon sample in the archaeological/environmental record, as an inhomogeneous Poisson process. The varying rate of samples over time is then estimated within a fully-Bayesian framework using reversible-jump Markov Chain Monte Carlo (RJ-MCMC). Given a set of radiocarbon samples, we reconstruct how their occurrence rate varies over calendar time and identify if that rate contains statistically-significant changes, i.e., specific times at which the rate of events abruptly changes. We illustrate our method with both a simulation study and a practical example concerning late-Pleistocene megafaunal population changes in Alaska and Yukon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15980v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy J Heaton, Sara Al-assam, Edouard Bard</dc:creator>
    </item>
    <item>
      <title>Anytime-valid FDR control with the stopped e-BH procedure</title>
      <link>https://arxiv.org/abs/2502.08539</link>
      <description>arXiv:2502.08539v2 Announce Type: replace 
Abstract: The recent e-Benjamini-Hochberg (e-BH) procedure for multiple hypothesis testing is known to control the false discovery rate (FDR) under arbitrary dependence between the input e-values. This paper points out an important subtlety when applying the e-BH procedure with e-processes, which are sequential generalizations of e-values (where the data are observed sequentially). Since adaptively stopped e-processes are e-values, the e-BH procedure can be repeatedly applied at every time step, and one can continuously monitor the e-processes and the rejection sets obtained. One would hope that the "stopped e-BH procedure" (se-BH) has an FDR guarantee for the rejection set obtained at any stopping time. However, while this is true if the data in different streams are independent, it is not true in full generality, because each stopped e-process is an e-value only for stopping times in its own local filtration, but the se-BH procedure employs a stopping time with respect to a global filtration. This can cause information to leak across time, allowing one stream to know its future by knowing past data of another stream. This paper formulates a simple causal condition under which local e-processes are also global e-processes and thus the se-BH procedure does indeed control the FDR. The condition excludes unobserved confounding from the past and is met under most reasonable scenarios including genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08539v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Relative Efficiency (BRE): Examining its Performance in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2503.04775</link>
      <description>arXiv:2503.04775v3 Announce Type: replace 
Abstract: Traditional Relative Efficiency (RE), based solely on variance, has limitations in evaluating estimator performance, particularly in planned missing data designs. We introduce Bhirkuti's Relative Efficiency (BRE), a novel metric that integrates precision and accuracy to provide a more robust assessment of efficiency. To compute BRE, we use interquartile range (IQR) overlap to measure precision and apply a bias adjustment factor based on the absolute median relative bias (AMRB). Monte Carlo simulations using a Latent Growth Model (LGM) with planned missing data illustrate that BRE maintains theoretically consistency and interpretability, avoiding paradoxes such as RE exceeding 100%. Visualizations via boxplots and ridgeline plots confirm that BRE provides a stable and meaningful estimator efficiency evaluation, making it a valuable advancement in psychometric and statistical modeling. By addressing fundamental weaknesses in traditional RE, BRE provides a superior, theoretically justified alternative for relative efficiency in psychometric modeling, structural equation modeling, and missing data research. This advancement enhances data-driven decision-making and offers a methodologically rigorous tool for researchers analyzing incomplete datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04775v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>An Improved Satterthwaite Effective Degrees of Freedom Correction for Weighted Syntheses of Variance</title>
      <link>https://arxiv.org/abs/2503.22080</link>
      <description>arXiv:2503.22080v3 Announce Type: replace 
Abstract: This article presents an improved approximation for the effective degrees of freedom in the Satterthwaite (1941, 1946) method which estimates the distribution of a weighted combination of variance components The standard Satterthwaite approximation assumes a scaled chisquare distribution for the composite variance estimator but is known to be biased downward when component degrees of freedom are small. Building on recent work by von Davier (2025), we propose an adjusted estimator that corrects this bias by modifying both the numerator and denominator of the traditional formula. The new approximation incorporates a weighted average of component degrees of freedom and a scaling factor that ensures consistency as the number of components or their degrees of freedom increases. We demonstrate the utility of this adjustment in practical settings, including Rubin's (1987) total variance estimation in multiple imputations, where weighted variance combinations are common. The proposed estimator generalizes and further improves von Davier's (2025) unweighted case and more accurately approximates synthetic variance estimators with arbitrary weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22080v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matthias von Davier</dc:creator>
    </item>
    <item>
      <title>Estimating Wage Disparities Using Foundation Models</title>
      <link>https://arxiv.org/abs/2409.09894</link>
      <description>arXiv:2409.09894v2 Announce Type: replace-cross 
Abstract: The rise of foundation models marks a paradigm shift in machine learning: instead of training specialized models from scratch, foundation models are first trained on massive datasets before being adapted or fine-tuned to make predictions on smaller datasets. Initially developed for text, foundation models have also excelled at making predictions about social science data. However, while many estimation problems in the social sciences use prediction as an intermediate step, they ultimately require different criteria for success. In this paper, we develop methods for fine-tuning foundation models to perform these estimation problems. We first characterize an omitted variable bias that can arise when a foundation model is only fine-tuned to maximize predictive accuracy. We then provide a novel set of conditions for fine-tuning under which estimates derived from a foundation model are root-n-consistent. Based on this theory, we develop new fine-tuning algorithms that empirically mitigate this omitted variable bias. To demonstrate our ideas, we study gender wage decomposition. This is a statistical estimation problem from econometrics where the goal is to decompose the gender wage gap into components that can and cannot be explained by career histories of workers. Classical methods for decomposing the wage gap employ simple predictive models of wages which condition on coarse summaries of career history that may omit factors that are important for explaining the gap. Instead, we use a custom-built foundation model to decompose the gender wage gap, which captures a richer representation of career history. Using data from the Panel Study of Income Dynamics, we find that career history explains more of the gender wage gap than standard econometric models can measure, and we identify elements of career history that are omitted by standard models but are important for explaining the wage gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09894v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keyon Vafa, Susan Athey, David M. Blei</dc:creator>
    </item>
    <item>
      <title>Deep Generative Quantile Bayes</title>
      <link>https://arxiv.org/abs/2410.08378</link>
      <description>arXiv:2410.08378v2 Announce Type: replace-cross 
Abstract: We develop a multivariate posterior sampling procedure through deep generative quantile learning. Simulation proceeds implicitly through a push-forward mapping that can transform i.i.d. random vector samples from the posterior. We utilize Monge-Kantorovich depth in multivariate quantiles to directly sample from Bayesian credible sets, a unique feature not offered by typical posterior sampling methods. To enhance the training of the quantile mapping, we design a neural network that automatically performs summary statistic extraction. This additional neural network structure has performance benefits, including support shrinkage (i.e., contraction of our posterior approximation) as the observation sample size increases. We demonstrate the usefulness of our approach on several examples where the absence of likelihood renders classical MCMC infeasible. Finally, we provide the following frequentist theoretical justifications for our quantile learning framework: {consistency of the estimated vector quantile, of the recovered posterior distribution, and of the corresponding Bayesian credible sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08378v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jungeum Kim, Percy S. Zhai, Veronika Ro\v{c}kov\'a</dc:creator>
    </item>
    <item>
      <title>Contextual Online Uncertainty-Aware Preference Learning for Human Feedback</title>
      <link>https://arxiv.org/abs/2504.19342</link>
      <description>arXiv:2504.19342v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a pivotal paradigm in artificial intelligence to align large models with human preferences. In this paper, we propose a novel statistical framework to simultaneously conduct the online decision-making and statistical inference on the optimal model using human preference data based on dynamic contextual information. Our approach introduces an efficient decision strategy that achieves both the optimal regret bound and the asymptotic distribution of the estimators. A key challenge in RLHF is handling the dependent online human preference outcomes with dynamic contexts. To address this, in the methodological aspect, we propose a two-stage algorithm starting with $\epsilon$-greedy followed by exploitations; in the theoretical aspect, we tailor anti-concentration inequalities and matrix martingale concentration techniques to derive the uniform estimation rate and asymptotic normality of the estimators using dependent samples from both stages. Extensive simulation results demonstrate that our method outperforms state-of-the-art strategies. We apply the proposed framework to analyze the human preference data for ranking large language models on the Massive Multitask Language Understanding dataset, yielding insightful results on the performance of different large language models for medical anatomy knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19342v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nan Lu, Ethan X. Fang, Junwei Lu</dc:creator>
    </item>
  </channel>
</rss>
