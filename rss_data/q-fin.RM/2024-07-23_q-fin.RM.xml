<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.RM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.RM</link>
    <description>q-fin.RM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.RM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jul 2024 01:41:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Analyzing selected cryptocurrencies spillover effects on global financial indices: Comparing risk measures using conventional and eGARCH-EVT-Copula approaches</title>
      <link>https://arxiv.org/abs/2407.15766</link>
      <description>arXiv:2407.15766v1 Announce Type: new 
Abstract: This study examines the interdependence between cryptocurrencies and international financial indices, such as MSCI World and MSCI Emerging Markets. We compute the value at risk, expected shortfall (ES), and range value at risk (RVaR) and investigate the dynamics of risk spillover. We employ a hybrid approach to derive these risk measures that integrate GARCH models, extreme value models, and copula functions. This framework uses a bivariate portfolio approach involving cryptocurrency data and traditional financial indices. To estimate the above risks of these portfolio structures, we employ symmetric and asymmetric GARCH and both tail flexible EVT models as marginal to model the marginal distribution of each return series and apply different copula functions to connect the pairs of marginal distributions into a multivariate distribution. The empirical findings indicate that the eGARCH EVT-based copula model adeptly captures intricate dependencies, surpassing conventional methodologies like Historical simulations and t-distributed parametric in VaR estimation. At the same time, the HS method proves superior for ES, and the t-distributed parametric method outperforms RVaR. Eventually, the Diebold-Yilmaz approach will be applied to compute risk spillovers between four sets of asset sequences. This phenomenon implies that cryptocurrencies reveal substantial spillover effects among themselves but minimal impact on other assets. From this, it can be concluded that cryptocurrencies propose diversification benefits and do not provide hedging advantages within an investor's portfolio. Our results underline RVaR superiority over ES regarding regulatory arbitrage and model misspecification. The conclusions of this study will benefit investors and financial market professionals who aspire to comprehend digital currencies as a novel asset class and attain perspicuity in regulatory arbitrage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15766v1</guid>
      <category>q-fin.RM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shafique Ur Rehman, Touqeer Ahmad, Wu Dash Desheng, Amirhossein Karamoozian</dc:creator>
    </item>
    <item>
      <title>Is the difference between deep hedging and delta hedging a statistical arbitrage?</title>
      <link>https://arxiv.org/abs/2407.14736</link>
      <description>arXiv:2407.14736v1 Announce Type: cross 
Abstract: The recent work of Horikawa and Nakagawa (2024) explains that there exist complete market models in which the difference between the hedging position provided by deep hedging and that of the replicating portfolio is a statistical arbitrage. This raises concerns as it entails that deep hedging can include a speculative component aimed simply at exploiting the structure of the risk measure guiding the hedging optimisation problem. We test whether such finding remains true in a GARCH-based market model. We observe that the difference between deep hedging and delta hedging can be a statistical arbitrage if the risk measure considered does not put sufficient relative weight on adverse outcomes. Nevertheless, a suitable choice of risk measure can prevent the deep hedging agent from including a speculative overlay within its hedging strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14736v1</guid>
      <category>q-fin.CP</category>
      <category>q-fin.RM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pascal Fran\c{c}ois, Genevi\`eve Gauthier, Fr\'ed\'eric Godin, Carlos Octavio P\'erez Mendoza</dc:creator>
    </item>
    <item>
      <title>A new paradigm of mortality modeling via individual vitality dynamics</title>
      <link>https://arxiv.org/abs/2407.15388</link>
      <description>arXiv:2407.15388v2 Announce Type: cross 
Abstract: The significance of mortality modeling extends across multiple research areas, including life insurance valuation, longevity risk management, life-cycle hypothesis, and retirement income planning. Despite the variety of existing approaches, such as mortality laws and factor-based models, they often lack compatibility or fail to meet specific research needs. To address these shortcomings, this study introduces a novel approach centered on modeling the dynamics of individual vitality and defining mortality as the depletion of vitality level to zero. More specifically, we develop a four-component framework to analyze the initial value, trend, diffusion, and sudden changes in vitality level over an individual's lifetime. We demonstrate the framework's estimation and analytical capabilities in various settings and discuss its practical implications in actuarial problems and other research areas. The broad applicability and interpretability of our vitality-based modeling approach offer an enhanced paradigm for mortality modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15388v2</guid>
      <category>stat.AP</category>
      <category>q-fin.RM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaobai Zhu, Kenneth Q. Zhou, Zijia Wang</dc:creator>
    </item>
    <item>
      <title>Large-scale Time-Varying Portfolio Optimisation using Graph Attention Networks</title>
      <link>https://arxiv.org/abs/2407.15532</link>
      <description>arXiv:2407.15532v1 Announce Type: cross 
Abstract: Apart from assessing individual asset performance, investors in financial markets also need to consider how a set of firms performs collectively as a portfolio. Whereas traditional Markowitz-based mean-variance portfolios are widespread, network-based optimisation techniques have built upon these developments. However, most studies do not contain firms at risk of default and remove any firms that drop off indices over a certain time. This is the first study to incorporate risky firms and use all the firms in portfolio optimisation. We propose and empirically test a novel method that leverages Graph Attention networks (GATs), a subclass of Graph Neural Networks (GNNs). GNNs, as deep learning-based models, can exploit network data to uncover nonlinear relationships. Their ability to handle high-dimensional features and accommodate customised layers for specific purposes makes them particularly appealing for large-scale problems such as mid- and small-cap portfolio optimization. This study utilises 30 years of data on mid-cap firms, creating graphs of firms using distance correlation and the Triangulated Maximally Filtered Graph approach. These graphs are the inputs to a GAT model that we train using custom layers which impose weight and allocation constraints and a loss function derived from the Sharpe ratio, thus directly maximising portfolio risk-adjusted returns. This new model is benchmarked against a network characteristic-based portfolio, a mean variance-based portfolio, and an equal-weighted portfolio. The results show that the portfolio produced by the GAT-based model outperforms all benchmarks and is consistently superior to other strategies over a long period while also being informative of market dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15532v1</guid>
      <category>q-fin.PM</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kamesh Korangi, Christophe Mues, Cristi\'an Bravo</dc:creator>
    </item>
    <item>
      <title>Quantifying distribution system resilience from utility data: large event risk and benefits of investments</title>
      <link>https://arxiv.org/abs/2407.10773</link>
      <description>arXiv:2407.10773v2 Announce Type: replace-cross 
Abstract: We focus on large blackouts in electric distribution systems caused by extreme winds. Such events have a large cost and impact on customers. To quantify resilience to these events, we formulate large event risk and show how to calculate it from the historical outage data routinely collected by utilities' outage management systems. Risk is defined using an event cost exceedance curve. The tail of this curve and the large event risk is described by the probability of a large cost event and the slope magnitude of the tail on a log-log plot. Resilience can be improved by planned investments to upgrade system components or speed up restoration. The benefits that these investments would have had if they had been made in the past can be quantified by "rerunning history" with the effects of the investment included, and then recalculating the large event risk to find the improvement in resilience. An example using utility data shows a 12% and 22% reduction in the probability of a large cost event due to 10% wind hardening and 10% faster restoration respectively. This new data-driven approach to quantify resilience and resilience investments is realistic and much easier to apply than complicated approaches based on modeling all the phases of resilience. Moreover, an appeal to improvements to past lived experience may well be persuasive to customers and regulators in making the case for resilience investments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10773v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arslan Ahmad, Ian Dobson</dc:creator>
    </item>
  </channel>
</rss>
