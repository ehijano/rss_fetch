<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 May 2025 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Space evaluation at the starting point of soccer transitions</title>
      <link>https://arxiv.org/abs/2505.14711</link>
      <description>arXiv:2505.14711v1 Announce Type: new 
Abstract: Soccer is a sport played on a pitch where effective use of space is crucial. Decision-making during transitions, when possession switches between teams, has been increasingly important, but research on space evaluation in these moments has been limited. Recent space evaluation methods such as OBSO (Off-Ball Scoring Opportunity) use scoring probability, so it is not well-suited for assessing areas far from the goal, where transitions typically occur. In this paper, we propose OBPV (Off-Ball Positioning Value) to evaluate space across the pitch, including the starting points of transitions. OBPV extends OBSO by introducing the field value model, which evaluates the entire pitch, and by employing the transition kernel model, which reflects positional specificity through kernel density estimation of pass distributions. Experiments using La Liga 2023/24 season tracking and event data show that OBPV highlights effective space utilization during counter-attacks and reveals team-specific characteristics in how the teams utilize space after positive and negative transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14711v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yohei Ogawa, Rikuhei Umemoto, Keisuke Fujii</dc:creator>
    </item>
    <item>
      <title>ComBAT Harmonization for diffusion MRI: Challenges and Best Practices</title>
      <link>https://arxiv.org/abs/2505.14722</link>
      <description>arXiv:2505.14722v1 Announce Type: new 
Abstract: Over the years, ComBAT has become the standard method for harmonizing MRI-derived measurements, with its ability to compensate for site-related additive and multiplicative biases while preserving biological variability. However, ComBAT relies on a set of assumptions that, when violated, can result in flawed harmonization. In this paper, we thoroughly review ComBAT's mathematical foundation, outlining these assumptions, and exploring their implications for the demographic composition necessary for optimal results.
  Through a series of experiments involving a slightly modified version of ComBAT called Pairwise-ComBAT tailored for normative modeling applications, we assess the impact of various population characteristics, including population size, age distribution, the absence of certain covariates, and the magnitude of additive and multiplicative factors. Based on these experiments, we present five essential recommendations that should be carefully considered to enhance consistency and supporting reproducibility, two essential factors for open science, collaborative research, and real-life clinical deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14722v1</guid>
      <category>stat.AP</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.med-ph</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre-Marc Jodoin, Manon Edde, Gabriel Girard, F\'elix Dumais, Guillaume Theaud, Matthieu Dumont, Jean-Christophe Houde, Yoan David, Maxime Descoteaux</dc:creator>
    </item>
    <item>
      <title>Effective climate policies for major emission reductions of ozone precursors: Global evidence from two decades</title>
      <link>https://arxiv.org/abs/2505.14731</link>
      <description>arXiv:2505.14731v1 Announce Type: new 
Abstract: Despite policymakers deploying various tools to mitigate emissions of ozone (O\textsubscript{3}) precursors, such as nitrogen oxides (NO\textsubscript{x}), carbon monoxide (CO), and volatile organic compounds (VOCs), the effectiveness of policy combinations remains uncertain. We employ an integrated framework that couples structural break detection with machine learning to pinpoint effective interventions across the building, electricity, industrial, and transport sectors, identifying treatment effects as abrupt changes without prior assumptions about policy treatment assignment and timing. Applied to two decades of global O\textsubscript{3} precursor emissions data, we detect 78, 77, and 78 structural breaks for NO\textsubscript{x}, CO, and VOCs, corresponding to cumulative emission reductions of 0.96-0.97 Gt, 2.84-2.88 Gt, and 0.47-0.48 Gt, respectively. Sector-level analysis shows that electricity sector structural policies cut NO\textsubscript{x} by up to 32.4\%, while in buildings, developed countries combined adoption subsidies with carbon taxes to achieve 42.7\% CO reductions and developing countries used financing plus fuel taxes to secure 52.3\%. VOCs abatement peaked at 38.5\% when fossil-fuel subsidy reforms were paired with financial incentives. Finally, hybrid strategies merging non-price measures (subsidies, bans, mandates) with pricing instruments delivered up to an additional 10\% co-benefit. These findings guide the sequencing and complementarity of context-specific policy portfolios for O\textsubscript{3} precursor mitigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14731v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ningning Yao, Huan Xi, Lang Chen, Zhe Song, Jian Li, Yulei Chen, Baocai Guo, Yuanhang Zhang, Tong Zhu, Pengfei Li, Daniel Rosenfeld, John H. Seinfeld, Shaocai Yu</dc:creator>
    </item>
    <item>
      <title>Predicting ICU Readmission in Acute Pancreatitis Patients Using a Machine Learning-Based Model with Enhanced Clinical Interpretability</title>
      <link>https://arxiv.org/abs/2505.14850</link>
      <description>arXiv:2505.14850v1 Announce Type: new 
Abstract: Acute pancreatitis (AP) is a common and potentially life-threatening gastrointestinal disease that imposes a significant burden on healthcare systems. ICU readmissions among AP patients are common, especially in severe cases, with rates exceeding 40%. Identifying high-risk patients for readmission is crucial for improving outcomes. This study used the MIMIC-III database to identify ICU admissions for AP based on diagnostic codes.
  We applied a preprocessing pipeline including missing data imputation, correlation analysis, and hybrid feature selection. Recursive Feature Elimination with Cross-Validation (RFECV) and LASSO regression, supported by expert review, reduced over 50 variables to 20 key predictors, covering demographics, comorbidities, lab tests, and interventions. To address class imbalance, we used the Synthetic Minority Over-sampling Technique (SMOTE) in a five-fold cross-validation framework.
  We developed and optimized six machine learning models-Logistic Regression, k-Nearest Neighbors, Naive Bayes, Random Forest, LightGBM, and XGBoost-using grid search. Model performance was evaluated with AUROC, accuracy, F1 score, sensitivity, specificity, PPV, and NPV. XGBoost performed best, with an AUROC of 0.862 (95% CI: 0.800-0.920) and accuracy of 0.889 (95% CI: 0.858-0.923) on the test set.
  An ablation study showed that removing any feature decreased performance. SHAP analysis identified platelet count, age, and SpO2 as key predictors of readmission. This study shows that ensemble learning, informed feature selection, and handling class imbalance can improve ICU readmission prediction in AP patients, supporting targeted post-discharge interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14850v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuheng Chen, Yong Si, Junyi Fan, Li Sun, Elham Pishgar, Kamiar Alaei, Greg Placencia, Maryam Pishgar</dc:creator>
    </item>
    <item>
      <title>Bayesian Multivariate Approach to Subnational mortality graduation with Age-Varying Smoothness</title>
      <link>https://arxiv.org/abs/2505.14955</link>
      <description>arXiv:2505.14955v1 Announce Type: new 
Abstract: This work introduces a Bayesian smoothing approach for the joint graduation of mortality rates across multiple populations. In particular, dynamical linear models are used to induce smoothness across ages through structured dependence, analogously to how temporal correlation is accommodated in state-space time-indexed models. An essential issue in subnational mortality probabilistic modelling is the lack or sparseness of information for some subpopulations. For many countries, mortality data is severely limited, and approaches based on a single population model can result in high uncertainty in the adjusted mortality tables. Here, we recognize the interdependence within a group of mortality data and pursue the pooling of information across several curves that ideally share common characteristics, such as the influence of epidemics or major economic shifts. Our proposal considers multivariate Bayesian dynamical models with common parameters, allowing for borrowing of information across mortality tables and enabling tests of convergence across populations. We also employ discount factors, typical in DLMs, to regulate smoothness, with varying discounting across ages, ensuring less smoothness at younger ages and greater stability at adult ages. This setup implies a trade-off between stability and adaptability. The discount parameter controls the responsiveness of the fit at older ages to new data. The estimation is fully Bayesian, accommodating all uncertainties in modelling and prediction. To illustrate the effectiveness of our model, we analyse male and female mortality data from England and Wales between 2010 and 2012, obtained from the Office for National Statistics. In scenarios with simulated missing data, our approach showed strong performance and flexibility in pooling information from related populations with more complete data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14955v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luiz F. V. Figueiredo, Viviana G. R. Lobo, Mariane B. Alves, Thais C. O. Fonseca</dc:creator>
    </item>
    <item>
      <title>A systematic review of sample size determination in Bayesian randomized clinical trials: full Bayesian methods are rarely used</title>
      <link>https://arxiv.org/abs/2505.15735</link>
      <description>arXiv:2505.15735v1 Announce Type: new 
Abstract: Utilizing Bayesian methods in clinical trials has become increasingly popular, as they can incorporate historical data and expert opinions into the design and allow for smaller sample sizes to reduce costs while providing reliable and robust statistical results. Sample size determination (SSD) is a key aspect of clinical trial design and various methods for Bayesian sample size determination are available. However, it is unclear how these methods are being used in practice. A systematic literature review was conducted to understand how sample sizes for Bayesian randomized clinical trials (RCTs) are determined and inform the design of future Bayesian trials. We searched five databases in May 2023, and updated in January 2025, including efficacy RCTs in humans which utilized a Bayesian framework for the primary data analysis, published in English, and enrolled participants between 2009 and 2024. The literature search produced 19,182 records, of which 105 studies were selected for data extraction. Results show that the most common method for SSD in Bayesian RCTs was a hybrid approach in which elements of Bayesian and frequentist theory are combined. Many RCTs did not provide a justification for SSD, while fully Bayesian methods were rarely used in practice, despite significant theoretical development. Our review also revealed a lack of standardized reporting, making it challenging to review the SSD. The CONSORT statement for reporting RCTs states that sample size calculations must be reported, which was poorly adhered to. Among RCTs that reported SSD, relevant information was frequently omitted from the reports and discussed in poorly structured supplementary materials. Thus, there is a critical need for greater transparency, standardization and translation of relevant methodology in Bayesian RCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15735v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yanara Marks, Jessie Cunningham, Arlene Jiang, Linke Li, Yi-Shu Lin, Abigail McGrory, Yongdong Ouyang, Nam-Anh Tran, Yuning Wang, Anna Heath</dc:creator>
    </item>
    <item>
      <title>A two-stage model for factors influencing citation counts</title>
      <link>https://arxiv.org/abs/2505.15384</link>
      <description>arXiv:2505.15384v1 Announce Type: cross 
Abstract: This work aims to study a count response random variable, the number of citations of a research paper, affected by some explanatory variables through a suitable regression model. Due to the fact that the count variable exhibits substantial variation since the sample variance is larger than the sample mean, the classical Poisson regression model seems not to be appropriate. We concentrate attention on the negative binomial regression model, which allows the variance of each measurement to be a function of its predicted value. Nevertheless, the process of citations of papers may be divided into two parts. In the first stage, the paper has no citations, and the second part provides the intensity of the citations. A hurdle model for separating the documents with citations and those without citations is considered. The dataset for the empirical application consisted of 43,190 research papers in the field of Economics and Business from 2014-2021, obtained from The Lens database. Citation counts and social attention scores for each article were gathered from Altmetric database. The main findings indicate that both collaboration and funding have a positive impact on citation counts and reduce the likelihood of receiving zero citations. Higher journal impact factors lead to higher citation counts, while lower peer review ratings lead to fewer citations and a higher probability of zero citations. Mentions in news, blogs, and social media have varying but generally limited effects on citation counts. Open access via repositories (green OA) correlates with higher citation counts and a lower probability of zero citations. In contrast, OA via the publisher's website without an explicit open license (bronze OA) is associated with higher citation counts but also with a higher probability of zero citations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15384v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Dorta-Gonz\'alez, Emilio G\'omez-D\'eniz</dc:creator>
    </item>
    <item>
      <title>SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding</title>
      <link>https://arxiv.org/abs/2505.15423</link>
      <description>arXiv:2505.15423v1 Announce Type: cross 
Abstract: Capturing nonlinear relationships without sacrificing interpretability remains a persistent challenge in regression modeling. We introduce SplitWise, a novel framework that enhances stepwise regression. It adaptively transforms numeric predictors into threshold-based binary features using shallow decision trees, but only when such transformations improve model fit, as assessed by the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC). This approach preserves the transparency of linear models while flexibly capturing nonlinear effects. Implemented as a user-friendly R package, SplitWise is evaluated on both synthetic and real-world datasets. The results show that it consistently produces more parsimonious and generalizable models than traditional stepwise and penalized regression techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15423v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcell T. Kurbucz, Nikolaos Tzivanakis, Nilufer Sari Aslam, Adam M. Sykulski</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach for Inference on Mixed Graphical Models</title>
      <link>https://arxiv.org/abs/2505.15464</link>
      <description>arXiv:2505.15464v1 Announce Type: cross 
Abstract: Mixed data refers to a type of data in which variables can be of multiple types, such as continuous, discrete, or categorical. This data is routinely collected in various fields, including healthcare and social sciences. A common goal in the analysis of such data is to identify dependence relationships between variables, for an understanding of their associations. In this paper, we propose a Bayesian pairwise graphical model that estimates conditional independencies between any type of data. We implement a flexible modeling construction, that includes zero-inflated count data and can also handle missing data. We show that the model maintains both global and local Markov properties. We employ a spike-and-slab prior for the estimation of the graph and implement an MCMC algorithm for posterior inference based on conditional likelihoods. We assess performances on four simulation scenarios with distinct dependence structures, that also include cases with data missing at random, and compare results with existing methods. Finally, we present an analysis of real data from adolescents diagnosed with an eating disorder. Estimated graphs show differences in the associations estimated at intake and discharge, suggesting possible effects of the treatment on cognitive and behavioral measures in the adolescents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15464v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mauro Florez, Anna Gottard, Carrie McAdams, Michele Guindani, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>A Multi-Tiered Bayesian Network Coastal Compound Flood Analysis Framework</title>
      <link>https://arxiv.org/abs/2505.15520</link>
      <description>arXiv:2505.15520v1 Announce Type: cross 
Abstract: Coastal compound floods (CCFs) are triggered by the interaction of multiple mechanisms, such as storm surges, storm rainfall, tides, and river flow. These events can bring significant damage to communities, and there is an increasing demand for accurate and efficient probabilistic analyses of CCFs to support risk assessments and decision-making. In this study, a multi-tiered Bayesian network (BN) CCF analysis framework is established. In this framework, multiple tiers of BN models with different complexities are designed for application with varying levels of data availability and computational resources. A case study is conducted in New Orleans, LA, to demonstrate this framework. In the Tier-1 BN model, storm surges and river flow are incorporated based on hydrodynamic simulations. A seasonality node is used to capture the dependence between concurrent river flow and tropical cyclone (TC) parameters. In the Tier-2 BN model, joint distribution models of TC parameters are built for separate TC intensity categories. TC-induced rainfall is modeled as input to hydraulic simulations. In the Tier-3 BN model, potential variations of meteorological conditions are incorporated by quantifying their effects on TC activity and coastal water level. Flood antecedent conditions are also incorporated to more completely represent the conditions contributing to flood severity. In this case study, a series of joint distribution, numerical, machine learning, and experimental models are used to compute conditional probability tables needed for BNs. A series of probabilistic analyses is performed based on these BN models, including CCF hazard curve construction and CCF deaggregation. The results of the analysis demonstrate the promise of this framework in performing CCF hazard analysis under varying levels of resource availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15520v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyue Liu, Meredith L. Carr, Norberto C. Nadal-Caraballo, Luke A. Aucoin, Madison C. Yawn, Michelle T. Bensi</dc:creator>
    </item>
    <item>
      <title>Are machine learning interpretations reliable? A stability study on global interpretations</title>
      <link>https://arxiv.org/abs/2505.15728</link>
      <description>arXiv:2505.15728v1 Announce Type: cross 
Abstract: As machine learning systems are increasingly used in high-stakes domains, there is a growing emphasis placed on making them interpretable to improve trust in these systems. In response, a range of interpretable machine learning (IML) methods have been developed to generate human-understandable insights into otherwise black box models. With these methods, a fundamental question arises: Are these interpretations reliable? Unlike with prediction accuracy or other evaluation metrics for supervised models, the proximity to the true interpretation is difficult to define. Instead, we ask a closely related question that we argue is a prerequisite for reliability: Are these interpretations stable? We define stability as findings that are consistent or reliable under small random perturbations to the data or algorithms. In this study, we conduct the first systematic, large-scale empirical stability study on popular machine learning global interpretations for both supervised and unsupervised tasks on tabular data. Our findings reveal that popular interpretation methods are frequently unstable, notably less stable than the predictions themselves, and that there is no association between the accuracy of machine learning predictions and the stability of their associated interpretations. Moreover, we show that no single method consistently provides the most stable interpretations across a range of benchmark datasets. Overall, these results suggest that interpretability alone does not warrant trust, and underscores the need for rigorous evaluation of interpretation stability in future work. To support these principles, we have developed and released an open source IML dashboard and Python package to enable researchers to assess the stability and reliability of their own data-driven interpretations and discoveries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15728v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luqin Gan, Tarek M. Zikry, Genevera I. Allen</dc:creator>
    </item>
    <item>
      <title>Neural Conditional Transport Maps</title>
      <link>https://arxiv.org/abs/2505.15808</link>
      <description>arXiv:2505.15808v1 Announce Type: cross 
Abstract: We present a neural framework for learning conditional optimal transport (OT) maps between probability distributions. Our approach introduces a conditioning mechanism capable of processing both categorical and continuous conditioning variables simultaneously. At the core of our method lies a hypernetwork that generates transport layer parameters based on these inputs, creating adaptive mappings that outperform simpler conditioning methods. Comprehensive ablation studies demonstrate the superior performance of our method over baseline configurations. Furthermore, we showcase an application to global sensitivity analysis, offering high performance in computing OT-based sensitivity indices. This work advances the state-of-the-art in conditional optimal transport, enabling broader application of optimal transport principles to complex, high-dimensional domains such as generative modeling and black-box model explainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15808v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Carlos Rodriguez-Pardo, Leonardo Chiani, Emanuele Borgonovo, Massimo Tavoni</dc:creator>
    </item>
    <item>
      <title>Mediation analysis in longitudinal intervention studies with an ordinal treatment-dependent confounder</title>
      <link>https://arxiv.org/abs/2501.04581</link>
      <description>arXiv:2501.04581v2 Announce Type: replace 
Abstract: In interventional health studies, causal mediation analysis can be employed to investigate mechanisms through which the intervention affects the targeted health outcome. Identifying direct and indirect (i.e. mediated) effects from empirical data become complicated, however, when the mediator-outcome association is confounded by a variable itself affected by the treatment. Here, we investigate identification of mediational effects under such post-treatment confounding in a setting with a longitudinal mediator, time-to-event outcome and a trichotomous ordinal treatment-dependent confounder. If the intervention always affects the treatment-dependent confounder only in one direction (monotonicity), we show that the mediational effects are identified up to a stratum-specific sensitivity parameter and derive their empirical non-parametric expressions. The feasibility of the monotonicity assumption can be assessed using empirical data, based on restrictions on the marginal distributions of counterfactuals of the treatment-dependent confounder. We avoid pitfalls related to post-treatment conditioning by treating the mediator as a functional entity and defining the time-to-event outcome as a restricted disease-free time. In an empirical analysis, we use data from the Finnish Diabetes Prevention Study to assess the extent to which the effect of a lifestyle intervention on avoiding type 2 diabetes is mediated through weight reduction in a high-risk population, with other health-related changes acting as treatment-dependent confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04581v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikko Valtanen, Tommi H\"ark\"anen, Matti Uusitupa, Jaakko Tuomilehto, Jaana Lindstr\"om, Kari Auranen</dc:creator>
    </item>
    <item>
      <title>BlockingPy: approximate nearest neighbours for blocking of records for entity resolution</title>
      <link>https://arxiv.org/abs/2504.04266</link>
      <description>arXiv:2504.04266v2 Announce Type: replace 
Abstract: Entity resolution (probabilistic record linkage, deduplication) is a key step in scientific analysis and data science pipelines involving multiple data sources. The objective of entity resolution is to link records without common unique identifiers that refer to the same entity (e.g., person, company). However, without identifiers, researchers need to specify which records to compare in order to calculate matching probability and reduce computational complexity. One solution is to deterministically block records based on some common variables, such as names, dates of birth or sex or use phonetic algorithms. However, this approach assumes that these variables are free of errors and completely observed, which is often not the case. To address this challenge, we have developed a Python package, BlockingPy, which uses blocking via modern approximate nearest neighbour search and graph algorithms to reduce the number of comparisons. In this paper, we present the design of the package, its functionalities and two case studies related to official statistics. The presented software will be useful for researchers interested in linking data from various sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04266v2</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.CO</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tymoteusz Strojny, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Improving the statistical efficiency of cross-conformal prediction</title>
      <link>https://arxiv.org/abs/2503.01495</link>
      <description>arXiv:2503.01495v2 Announce Type: replace-cross 
Abstract: Vovk (2015) introduced cross-conformal prediction, a modification of split conformal designed to improve the width of prediction sets. The method, when trained with a miscoverage rate equal to $\alpha$ and $n \gg K$, ensures a marginal coverage of at least $1 - 2\alpha - 2(1-\alpha)(K-1)/(n+K)$, where $n$ is the number of observations and $K$ denotes the number of folds. A simple modification of the method achieves coverage of at least $1-2\alpha$. In this work, we propose new variants of both methods that yield smaller prediction sets without compromising the latter theoretical guarantees. The proposed methods are based on recent results deriving more statistically efficient combination of p-values that leverage exchangeability and randomization. Simulations confirm the theoretical findings and bring out some important tradeoffs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01495v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Gasparin, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A Review of Stop-and-Go Traffic Wave Suppression Strategies: Variable Speed Limit vs. Jam-Absorption Driving</title>
      <link>https://arxiv.org/abs/2504.11372</link>
      <description>arXiv:2504.11372v2 Announce Type: replace-cross 
Abstract: The main form of freeway traffic congestion is the familiar stop-and-go wave, characterized by wide moving jams that propagate indefinitely upstream provided enough traffic demand. They cause severe, long-lasting adverse effects, such as reduced traffic efficiency, increased driving risks, and higher vehicle emissions. This underscores the crucial importance of artificial intervention in the propagation of stop-and-go waves. Over the past two decades, two prominent strategies for stop-and-go wave suppression have emerged: variable speed limit (VSL) and jam-absorption driving (JAD). Although they share similar research motivations, objectives, and theoretical foundations, the development of these strategies has remained relatively disconnected. To synthesize fragmented advances and drive the field forward, this paper first provides a comprehensive review of the achievements in the stop-and-go wave suppression-oriented VSL and JAD, respectively. It then focuses on bridging the two areas and identifying research opportunities from the following perspectives: fundamental diagrams, secondary waves, generalizability, traffic state estimation and prediction, robustness to randomness, scenarios for strategy validation, and field tests and practical deployment. We expect that through this review, one area can effectively address its limitations by identifying and leveraging the strengths of the other, thus promoting the overall research goal of freeway stop-and-go wave suppression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11372v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengbing He, Jorge Laval, Yu Han, Andreas Hegyi, Ryosuke Nishi, Cathy Wu</dc:creator>
    </item>
  </channel>
</rss>
