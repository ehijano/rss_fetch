<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Nov 2024 08:29:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Political Hegemony, Imitation Isomorphism, and Project Familiarity: Instrumental Variables to Understand Funding Impact on Scholar Performance</title>
      <link>https://arxiv.org/abs/2411.04426</link>
      <description>arXiv:2411.04426v1 Announce Type: new 
Abstract: This paper contributes a new idea for exploring research funding effects on scholar performance. By collecting details of 9,501 research grants received by principal investigators from universities in the U.S. social sciences from 2000 to 2019 and data on their publications and citations in the Microsoft Academic Graph and Web of Science bibliographic collections, we build a novel dataset of grants and article counts, citations, and journal CiteScore. Based on this dataset, we first introduce three instrumental variables (IVs) suitable for isolating endogeneity issues in the study of competing grant effects, namely scholars' political hegemony in academia, imitation isomorphic behavior among scholars, and project familiarity. Then, this study explains the research funding effects by combining the three IVs with a two-stage least square (2SLS) model and by considering gaps from individual academics, universities, and research fields. Also, we provide validity and robustness tests of these three IVs and research funding effects. We find that our IVs serve the function of exogenizing and isolating endogeneity in capturing the research funding effect. Furthermore, empirical findings show that receiving research funding increases a scholar's research output and impact, but does not result in the scholar's research being published in more prestigious academic journals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04426v1</guid>
      <category>cs.DL</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Ding, Yi Bu</dc:creator>
    </item>
    <item>
      <title>VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and Benchmark Models</title>
      <link>https://arxiv.org/abs/2411.04825</link>
      <description>arXiv:2411.04825v1 Announce Type: cross 
Abstract: Existing text simplification or paraphrase datasets mainly focus on sentence-level text generation in a general domain. These datasets are typically developed without using domain knowledge. In this paper, we release a novel dataset, VTechAGP, which is the first academic-to-general-audience text paraphrase dataset consisting of 4,938 document-level these and dissertation academic and general-audience abstract pairs from 8 colleges authored over 25 years. We also propose a novel dynamic soft prompt generative language model, DSPT5. For training, we leverage a contrastive-generative loss function to learn the keyword vectors in the dynamic prompt. For inference, we adopt a crowd-sampling decoding strategy at both semantic and structural levels to further select the best output candidate. We evaluate DSPT5 and various state-of-the-art large language models (LLMs) from multiple perspectives. Results demonstrate that the SOTA LLMs does not provide satisfactory outcomes, while the lightweight DSPT5 can achieve competitive results. To the best of our knowledge, we are the first to build a benchmark dataset and solutions for academic-to-general-audience text paraphrase dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04825v1</guid>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Cheng, Jiaying Gong, Chenhan Yuan, William A. Ingram, Edward Fox, Hoda Eldardiry</dc:creator>
    </item>
    <item>
      <title>Peer Reviews of Peer Reviews: A Randomized Controlled Trial and Other Experiments</title>
      <link>https://arxiv.org/abs/2311.09497</link>
      <description>arXiv:2311.09497v2 Announce Type: replace 
Abstract: Is it possible to reliably evaluate the quality of peer reviews? We study this question driven by two primary motivations -- incentivizing high-quality reviewing using assessed quality of reviews and measuring changes to review quality in experiments. We conduct a large scale study at the NeurIPS 2022 conference, a top-tier conference in machine learning, in which we invited (meta)-reviewers and authors to evaluate reviews given to submitted papers. First, we conduct a RCT to examine bias due to the length of reviews. We generate elongated versions of reviews by adding substantial amounts of non-informative content. Participants in the control group evaluate the original reviews, whereas participants in the experimental group evaluate the artificially lengthened versions. We find that lengthened reviews are scored (statistically significantly) higher quality than the original reviews. In analysis of observational data we find that authors are positively biased towards reviews recommending acceptance of their own papers, even after controlling for confounders of review length, quality, and different numbers of papers per author. We also measure disagreement rates between multiple evaluations of the same review of 28%-32%, which is comparable to that of paper reviewers at NeurIPS. Further, we assess the amount of miscalibration of evaluators of reviews using a linear model of quality scores and find that it is similar to estimates of miscalibration of paper reviewers at NeurIPS. Finally, we estimate the amount of variability in subjective opinions around how to map individual criteria to overall scores of review quality and find that it is roughly the same as that in the review of papers. Our results suggest that the various problems that exist in reviews of papers -- inconsistency, bias towards irrelevant factors, miscalibration, subjectivity -- also arise in reviewing of reviews.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09497v2</guid>
      <category>cs.DL</category>
      <category>cs.GT</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Goldberg, Ivan Stelmakh, Kyunghyun Cho, Alice Oh, Alekh Agarwal, Danielle Belgrave, Nihar B. Shah</dc:creator>
    </item>
  </channel>
</rss>
