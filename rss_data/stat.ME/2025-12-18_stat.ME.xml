<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Dec 2025 05:00:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scaling Causal Mediation for Complex Systems: A Framework for Root Cause Analysis</title>
      <link>https://arxiv.org/abs/2512.14764</link>
      <description>arXiv:2512.14764v1 Announce Type: new 
Abstract: Modern operational systems ranging from logistics and cloud infrastructure to industrial IoT, are governed by complex, interdependent processes. Understanding how interventions propagate through such systems requires causal inference methods that go beyond direct effects to quantify mediated pathways. Traditional mediation analysis, while effective in simple settings, fails to scale to the high-dimensional directed acyclic graphs (DAGs) encountered in practice, particularly when multiple treatments and mediators interact. In this paper, we propose a scalable mediation analysis framework tailored for large causal DAGs involving multiple treatments and mediators. Our approach systematically decomposes total effects into interpretable direct and indirect components. We demonstrate its practical utility through applied case studies in fulfillment center logistics, where complex dependencies and non-controllable factors often obscure root causes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14764v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Casadei, Sreyoshi Bhaduri, Rohit Malshe, Pavan Mullapudi, Raj Ratan, Ankush Pole, Arkajit Rakshit</dc:creator>
    </item>
    <item>
      <title>Bayesian Latent Class Regression and Variable Selection with Applications to Sleep Patterns Data</title>
      <link>https://arxiv.org/abs/2512.14903</link>
      <description>arXiv:2512.14903v1 Announce Type: new 
Abstract: Sleep difficulties in children are heterogeneous in presentation, yet conventional assessment tools like the Children's Sleep Habits Questionnaire (CSHQ) reduce this complexity to a single cumulative score, obscuring distinct patterns of sleep disturbance that require different interventions. Latent Class Regression (LCR) models offer a principled approach to identify subgroups with shared sleep behaviour profiles whilst incorporating predictors of group membership, but Bayesian inference for these models has been hindered by computational challenges and the absence of variable selection methods. We propose a fully Bayesian framework for LCR that uses P\'olya-Gamma data augmentation, enabling efficient sampling of regression coefficients. We extend this framework to include variable selection for both predictors and item responses: predictor variable selection via latent inclusion indicators and item selection through a partially collapsed approach. Through simulation studies, we show that the proposed methods yield accurate parameter estimates, resolve identifiability issues arising in full models and successfully identify informative predictors and items while excluding noise variables. Applying this methodology to CSHQ data from 148 children reveals distinct latent subgroups with different sleep behaviour profiles, anxious nighttime sleepers, short/light sleepers and those with more pervasive sleep problems, with each carrying distinct implications for intervention. Results also highlight the predictive role of Autism Spectrum Disorder diagnosis in subgroup membership. These findings demonstrate the limitations of conventional CSHQ scoring and illustrate the benefits of a probabilistic subgroup-based approach as an alternative for understanding paediatric sleep difficulties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14903v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Heaney, Olive Healy, Jason Wyse, Arthur White</dc:creator>
    </item>
    <item>
      <title>Conditional Expert Kaplan-Meier Estimation: Asymptotic Theory and an Application to Loan Default Modelling</title>
      <link>https://arxiv.org/abs/2512.14959</link>
      <description>arXiv:2512.14959v1 Announce Type: new 
Abstract: We study the conditional expert Kaplan-Meier estimator, an extension of the classical Kaplan--Meier estimator designed for time-to-event data subject to both right-censoring and contamination. Such contamination, where observed events may not reflect true outcomes, is common in applied settings, including insurance and credit risk, where expert opinion is often used to adjudicate uncertain events. Building on previous work, we develop a comprehensive asymptotic theory for the conditional version incorporating covariates through kernel smoothing. We establish functional consistency and weak convergence under suitable regularity conditions and quantify the bias induced by imperfect expert information. The results show that unbiased expert judgments ensure consistency, while systematic deviations lead to a deterministic asymptotic bias that can be explicitly characterized. We examine finite-sample properties through simulation studies and illustrate the practical use of the estimator with an application to loan default data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14959v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Kristian Vilhelm Dinesen</dc:creator>
    </item>
    <item>
      <title>Gamma family characterization and an alternative proof of Gini estimator unbiasedness</title>
      <link>https://arxiv.org/abs/2512.14983</link>
      <description>arXiv:2512.14983v1 Announce Type: new 
Abstract: In this paper, we derive a general representation for the expectation of the (upward adjusted) Gini coefficient estimator in terms of the Laplace transform of the underlying distribution. This representation leads to a characterization of the gamma family within the class of nonnegative scale families, based on a stability property of exponentially tilted distributions. As an application, we provide an alternative proof of the unbiasedness of the Gini coefficient estimator under gamma populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14983v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila</dc:creator>
    </item>
    <item>
      <title>Measuring Nonlinear Relationships and Spatial Heterogeneity of Influencing Factors on Traffic Crash Density Using GeoXAI</title>
      <link>https://arxiv.org/abs/2512.14985</link>
      <description>arXiv:2512.14985v1 Announce Type: new 
Abstract: This study applies a Geospatial Explainable AI (GeoXAI) framework to analyze the spatially heterogeneous and nonlinear determinants of traffic crash density in Florida. By combining a high-performing machine learning model with GeoShapley, the framework provides interpretable, tract-level insights into how roadway characteristics and socioeconomic factors contribute to crash risk. Specifically, results show that variables such as road density, intersection density, neighborhood compactness, and educational attainment exhibit complex nonlinear relationships with crashes. Extremely dense urban areas, such as Miami, show sharply elevated crash risk due to intensified pedestrian activities and roadway complexity. The GeoShapley approach also captures strong spatial heterogeneity in the influence of these factors. Major metropolitan areas including Miami, Orlando, Tampa, and Jacksonville display significantly higher intrinsic crash contributions, while rural tracts generally have lower baseline risk. Each factor exhibits pronounced spatial variation across the state. Based on these findings, the study proposes targeted, geography-sensitive policy recommendations, including traffic calming in compact neighborhoods, adaptive intersection design, speed management on high-volume corridors such as I-95 in Miami, and equity-focused safety interventions in disadvantaged rural areas of central and northern Florida. Moreover, this paper compares the results obtained from GeoShapley framework against other established methods (e.g., SHAP and MGWR), demonstrating its powerful ability to explain nonlinearity and spatial heterogeneity simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14985v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqing Lu, Ziqi Li, Lei Han, Qianwen Guo</dc:creator>
    </item>
    <item>
      <title>Stratified Bootstrap Test Package</title>
      <link>https://arxiv.org/abs/2512.15057</link>
      <description>arXiv:2512.15057v1 Announce Type: new 
Abstract: The Stratified Bootstrap Test (SBT) provides a nonparametric, resampling-based framework for assessing the stability of group-specific ranking patterns in multivariate survey or rating data. By repeatedly resampling observations and examining whether a group's top-ranked items remain among the highest-scoring categories across bootstrap samples, SBT quantifies ranking robustness through a non-containment index. In parallel, the stratified bootstrap test extends this framework to formal statistical inference by testing ordering hypotheses among population means. Through resampling within groups, the method approximates the null distribution of ranking-based test statistics without relying on distributional assumptions. Together, these techniques enable both descriptive and inferential evaluation of ranking consistency, detection of aberrant or adversarial response patterns, and rigorous comparison of groups in applications such as survey analysis, item response assessment, and fairness auditing in AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15057v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ehsan Mohammadi, Fanghua Chen, Yizhou Cai, Yun Yang, Ting Fung Ma, Lu Zhou</dc:creator>
    </item>
    <item>
      <title>On non-stationarity of the Poisson gamma state space models</title>
      <link>https://arxiv.org/abs/2512.15128</link>
      <description>arXiv:2512.15128v1 Announce Type: new 
Abstract: The Poisson-gamma state space (PGSS) models have been utilized in the analysis of non-negative integer-valued time series to sequentially obtain closed form filtering and predictive densities. In this study, we show the underlying mechanics and non-stationary properties of multi-step ahead predictive distributions for the PGSS family of models. By exploiting the non-stationary structure of the PGSS model, we establish that the predictive mean remains constant while the predictive variance diverges with the forecast horizon, a property also found in Gaussian random walk models. We show that, in the long run, the predictive distribution converges to a zero-degenerated distribution, such that both point and interval forecasts eventually converge towards zero. In doing so, we comment on the effect of hyper-parameters and the discount factor on the long-run behavior of the forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15128v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaoru Irie, Tevfik Aktekin</dc:creator>
    </item>
    <item>
      <title>Non-parametric Causal Inference in Dynamic Thresholding Designs</title>
      <link>https://arxiv.org/abs/2512.15244</link>
      <description>arXiv:2512.15244v1 Announce Type: new 
Abstract: Consider a setting where we regularly monitor patients' fasting blood sugar, and declare them to have prediabetes (and encourage preventative care) if this number crosses a pre-specified threshold. The sharp, threshold-based treatment policy suggests that we should be able to estimate the long-term benefit of this preventative care by comparing the health trajectories of patients with blood sugar measurements right above and below the threshold. A naive regression-discontinuity analysis, however, is not applicable here, as it ignores the temporal dynamics of the problem where, e.g., a patient just below the threshold on one visit may become prediabetic (and receive treatment) following their next visit. Here, we study thresholding designs in general dynamic systems, and show that simple reduced-form characterizations remain available for a relevant causal target, namely a dynamic marginal policy effect at the treatment threshold. We develop a local-linear-regression approach for estimation and inference of this estimand, and demonstrate promise of our approach in numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15244v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Ghosh, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Interpretable Multivariate Conformal Prediction with Fast Transductive Standardization</title>
      <link>https://arxiv.org/abs/2512.15383</link>
      <description>arXiv:2512.15383v1 Announce Type: new 
Abstract: We propose a conformal prediction method for constructing tight simultaneous prediction intervals for multiple, potentially related, numerical outputs given a single input. This method can be combined with any multi-target regression model and guarantees finite-sample coverage. It is computationally efficient and yields informative prediction intervals even with limited data. The core idea is a novel \emph{coordinate-wise} standardization procedure that makes residuals across output dimensions directly comparable, estimating suitable scaling parameters using the calibration data themselves. This does not require modeling of cross-output dependence nor auxiliary sample splitting. Implementing this idea requires overcoming technical challenges associated with transductive or full conformal prediction. Experiments on simulated and real data demonstrate this method can produce tighter prediction intervals than existing baselines while maintaining valid simultaneous coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15383v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunjie Fan, Matteo Sesia</dc:creator>
    </item>
    <item>
      <title>Accounting for missing data when modelling block maxima</title>
      <link>https://arxiv.org/abs/2512.15429</link>
      <description>arXiv:2512.15429v1 Announce Type: new 
Abstract: Modelling block maxima using the generalised extreme value (GEV) distribution is a classical and widely used method for studying univariate extremes. It allows for theoretically motivated estimation of return levels, including extrapolation beyond the range of observed data. A frequently overlooked challenge in applying this methodology comes from handling datasets containing missing values. In this case, one cannot be sure whether the true maximum has been recorded in each block, and simply ignoring the issue can lead to biased parameter estimators and, crucially, underestimated return levels. We propose an extension of the standard block maxima approach to overcome such missing data issues. This is achieved by explicitly accounting for the proportion of missing values in each block within the GEV model. Inference is carried out using likelihood-based techniques, and we propose an update to commonly used diagnostic plots to assess model fit. We assess the performance of our method via a simulation study, with results that are competitive with the "ideal" case of having no missing values. The practical use of our methodology is demonstrated on sea surge data from Brest, France, and air pollution data from Plymouth, U.K.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15429v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma S. Simpson, Paul J. Northrop</dc:creator>
    </item>
    <item>
      <title>Optimal Transport-Based Clustering of Attributed Graphs with an Application to Road Traffic Data</title>
      <link>https://arxiv.org/abs/2512.15570</link>
      <description>arXiv:2512.15570v1 Announce Type: new 
Abstract: In many real-world contexts, such as social or transport networks, data exhibit both structural connectivity and node-level attributes. For example, roads in a transport network can be characterized not only by their connectivity but also by traffic flow or speed profiles. Understanding such systems therefore requires jointly analyzing the network structure and node attributes, a challenge addressed by attributed graph partitioning, which clusters nodes based on both connectivity and attributes. In this work, we adapt distance-based methods for this task, including Fr\'echet $k$-means and optimal transport-based approaches based on Gromov--Wasserstein (GW) discrepancy. We investigate how GW methods, traditionally used for general-purpose tasks such as graph matching, can be specifically adapted for node partitioning, an area that has been relatively underexplored. In the context of node-attributed graphs, we introduce an adaptation of the Fused GW method, offering theoretical guarantees and the ability to handle heterogeneous attribute types. Additionally, we propose to incorporate distance-based embeddings to enhance performance. The proposed approaches are systematically evaluated using a dedicated simulation framework and illustrated on a real-world transportation dataset. Experiments investigate the influence of target choice, assess robustness to noise, and provide practical guidance for attributed graph clustering. In the context of road networks, our results demonstrate that these methods can effectively leverage both structural and attribute information to reveal meaningful clusters, offering insights for improved network understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15570v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ioana Gavra, Ketsia Guichard-Sustowski, Lo\"ic Le Marrec</dc:creator>
    </item>
    <item>
      <title>Inference for Forecasting Accuracy: Pooled versus Individual Estimators in High-dimensional Panel Data</title>
      <link>https://arxiv.org/abs/2512.15592</link>
      <description>arXiv:2512.15592v1 Announce Type: new 
Abstract: Panels with large time $(T)$ and cross-sectional $(N)$ dimensions are a key data structure in social sciences and other fields. A central question in panel data analysis is whether to pool data across individuals or to estimate separate models. Pooled estimators typically have lower variance but may suffer from bias, creating a fundamental trade-off for optimal estimation. We develop a new inference method to compare the forecasting performance of pooled and individual estimators. Specifically, we propose a confidence interval for the difference between their forecasting errors and establish its asymptotic validity. Our theory allows for complex temporal and cross-sectional dependence in the model errors and covers scenarios where $N$ can be much larger than $T$-including the independent case under the classical condition $N/T^2 \to 0$. The finite-sample properties of the proposed method are examined in an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15592v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Kutta, Martin Schumann, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Fully Bayesian Spectral Clustering and Benchmarking with Uncertainty Quantification for Small Area Estimation</title>
      <link>https://arxiv.org/abs/2512.15643</link>
      <description>arXiv:2512.15643v1 Announce Type: new 
Abstract: In this work, inspired by machine learning techniques, we propose a new Bayesian model for Small Area Estimation (SAE), the Fay-Herriot model with Spectral Clustering (FH-SC). Unlike traditional approaches, clustering in FH-SC is based on spectral clustering algorithms that utilize external covariates, rather than geographical or administrative criteria. A major advantage of the FH-SC model is its flexibility in integrating existing SAE approaches, with or without clustering random effects. To enable benchmarking, we leverage the theoretical framework of posterior projections for constrained Bayesian inference and derive closed form expressions for the new Rao-Blackwell (RB) estimators of the posterior mean under the FH-SC model. Additionally, we introduce a novel measure of uncertainty for the benchmarked estimator, the Conditional Posterior Mean Square Error (CPMSE), which is generalizable to other Bayesian SAE estimators. We conduct model-based and data-based simulation studies to evaluate the frequentist properties of the CPMSE. The proposed methodology is motivated by a real case study involving the estimation of the proportion of households with internet access in the municipalities of Colombia. Finally, we also illustrate the advantages of FH-SC over existing Bayesian and frequentist approaches through our case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15643v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jairo F\'uquene-Pati\~no</dc:creator>
    </item>
    <item>
      <title>Data-driven controlled subgroup selection in clinical trials</title>
      <link>https://arxiv.org/abs/2512.15676</link>
      <description>arXiv:2512.15676v1 Announce Type: new 
Abstract: Subgroup selection in clinical trials is essential for identifying patient groups that react differently to a treatment, thereby enabling personalised medicine. In particular, subgroup selection can identify patient groups that respond particularly well to a treatment or that encounter adverse events more often. However, this is a post-selection inference problem, which may pose challenges for traditional techniques used for subgroup analysis, such as increased Type I error rates and potential biases from data-driven subgroup identification. In this paper, we present two methods for subgroup selection in regression problems: one based on generalised linear modelling and another on isotonic regression. We demonstrate how these methods can be used for data-driven subgroup identification in the analysis of clinical trials, focusing on two distinct tasks: identifying patient groups that are safe from manifesting adverse events and identifying patient groups with high treatment effect, while controlling for Type I error in both cases. A thorough simulation study is conducted to evaluate the strengths and weaknesses of each method, providing detailed insight into the sensitivity of the Type I error rate control to modelling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15676v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel M. M\"uller, Bj\"orn Bornkamp, Frank Bretz, Timothy I. Cannings, Wei Liu, Henry W. J. Reeve, Richard J. Samworth, Nikolaos Sfikas, Fang Wan, Konstantinos Sechidis</dc:creator>
    </item>
    <item>
      <title>Layer-2 Adoption and Ethereum Mainnet Congestion: Regime-Aware Causal Evidence Across London, the Merge, and Dencun (2021-2024)</title>
      <link>https://arxiv.org/abs/2512.14724</link>
      <description>arXiv:2512.14724v1 Announce Type: cross 
Abstract: Do Ethereum's Layer-2 (L2) rollups actually decongest the Layer-1 (L1) mainnet once protocol upgrades and demand are held constant? Using a 1245-day daily panel from August 5, 2021 to December 31, 2024 that spans the London, Merge, and Dencun upgrades, we link Ethereum fee and congestion metrics to L2 user activity, macro-demand proxies, and targeted event indicators. We estimate a regime-aware error-correction model that treats posting-clean L2 user share as a continuous treatment. Over the pre-Dencun (London+Merge) window, a 10 percentage point increase in L2 adoption lowers median base fees by about 13% -- roughly 5 Gwei at pre-Dencun levels -- and deviations from the long-run relation decay with an 11-day half-life. Block utilization and a scarcity index show similar congestion relief. After Dencun, L2 adoption is already high and treatment support narrows, so blob-era estimates are statistically imprecise and we treat them as exploratory. The pre-Dencun window therefore delivers the first cross-regime causal estimate of how aggregate L2 adoption decongests Ethereum, together with a reusable template for monitoring rollup-centric scaling strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14724v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aysajan Eziz</dc:creator>
    </item>
    <item>
      <title>A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications</title>
      <link>https://arxiv.org/abs/2512.14727</link>
      <description>arXiv:2512.14727v1 Announce Type: cross 
Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14727v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Klaus-Rudolf Kladny, Bernhard Sch\"olkopf, Lisa Koch, Christian F. Baumgartner, Michael Muehlebach</dc:creator>
    </item>
    <item>
      <title>Nonparametric Stochastic Subspaces via the Bootstrap for Characterizing Model Error</title>
      <link>https://arxiv.org/abs/2512.15624</link>
      <description>arXiv:2512.15624v1 Announce Type: cross 
Abstract: Reliable forward uncertainty quantification in engineering requires methods that account for aleatory and epistemic uncertainties. In many applications, epistemic effects arising from uncertain parameters and model form dominate prediction error and strongly influence engineering decisions. Because distinguishing and representing each source separately is often infeasible, their combined effect is typically analyzed using a unified model-error framework. Model error directly affects model credibility and predictive reliability; yet its characterization remains challenging. To address this need, we introduce a bootstrap-based stochastic subspace model for characterizing model error in the stochastic reduced-order modeling framework. Given a snapshot matrix of state vectors, the method leverages the empirical data distribution to induce a sampling distribution over principal subspaces for reduced order modeling. The resulting stochastic model enables improved characterization of model error in computational mechanics compared with existing approaches. The method offers several advantages: (1) it is assumption-free and leverages the empirical data distribution; (2) it enforces linear constraints (such as boundary conditions) by construction; (3) it requires only one hyperparameter, significantly simplifying the training process; and (4) its algorithm is straightforward to implement. We evaluate the method's performance against existing approaches using numerical examples in computational mechanics and structural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15624v1</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akash Yadav, Ruda Zhang</dc:creator>
    </item>
    <item>
      <title>A novel decomposition to explain heterogeneity in observational and randomized studies of causality</title>
      <link>https://arxiv.org/abs/2208.05543</link>
      <description>arXiv:2208.05543v5 Announce Type: replace 
Abstract: This paper introduces a novel decomposition framework to explain heterogeneity in causal effects observed across different studies, considering both observational and randomized settings. We present a formal decomposition of between-study heterogeneity, identifying sources of variability in treatment effects across studies. The proposed methodology allows for robust estimation of causal parameters under various assumptions, addressing differences in pre-treatment covariate distributions, mediating variables, and the outcome mechanism. Our approach is validated through a simulation study and applied to data from the Moving to Opportunity (MTO) study, demonstrating its practical relevance. This work contributes to the broader understanding of causal inference in multi-study environments, with potential applications in evidence synthesis and policy-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.05543v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Gilbert, Ivan D{\i}az, Kara E. Rudolph, Nicholas Williams, Tat-Thang Vo</dc:creator>
    </item>
    <item>
      <title>On the Graphical Rules for Recovering the Average Treatment Effect Under Selection Bias</title>
      <link>https://arxiv.org/abs/2502.00924</link>
      <description>arXiv:2502.00924v5 Announce Type: replace 
Abstract: Selection bias is a major obstacle toward valid causal inference in epidemiology. Over the past decade, several graphical rules based on causal diagrams have been proposed as the sufficient identification conditions for addressing selection bias and recovering causal effects. However, these simple graphical rules are typically coupled with specific identification strategies and estimators. In this article, we show two important cases of selection bias that cannot be addressed by these existing simple rules and their estimators: one case where selection is a descendant of a collider of the treatment and the outcome, and the other case where selection is affected by the mediator. To address selection bias and recover average treatment effect in these two cases, we propose an alternative set of graphical rules and construct identification formulas by the g-computation and the inverse probability weighting (IPW) methods based on single-world intervention graphs (SWIGs). We conduct simulation studies to verify the performance of the estimators when the traditional crude selected-sample analysis (i.e., complete-case analysis) yields erroneous conclusions contradictory to the truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00924v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yichi Zhang, Haidong Lu</dc:creator>
    </item>
    <item>
      <title>Conformal Survival Bands for Risk Screening under Right-Censoring</title>
      <link>https://arxiv.org/abs/2505.04568</link>
      <description>arXiv:2505.04568v4 Announce Type: replace 
Abstract: We propose a method to quantify uncertainty around individual survival distribution estimates using right-censored data, compatible with any survival model. Unlike classical confidence intervals, the survival bands produced by this method offer predictive rather than population-level inference, making them useful for personalized risk screening. For example, in a low-risk screening scenario, they can be applied to flag patients whose survival band at 12 months lies entirely above 50\%, while ensuring that at least half of flagged individuals will survive past that time on average. Our approach builds on recent advances in conformal inference and integrates ideas from inverse probability of censoring weighting and multiple testing with false discovery rate control. We provide asymptotic guarantees and show promising performance in finite samples with both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04568v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Sesia, Vladimir Svetnik</dc:creator>
    </item>
    <item>
      <title>Functional Periodic ARMA Processes</title>
      <link>https://arxiv.org/abs/2507.18962</link>
      <description>arXiv:2507.18962v2 Announce Type: replace 
Abstract: Periodicity is a common feature of time series. For finite-dimensional data, periodic autoregressive moving average (ARMA) models have been extensively studied. In functional time series analysis, AR models have been extended to incorporate periodicity, but existing approaches remain incomplete and do not cover the ARMA setting. This paper develops a rigorous theoretical framework for functional periodic ARMA (fPARMA) processes in general separable Hilbert spaces. The proposed model class accommodates periodically varying dependence structures. We derive sufficient conditions for periodic stationarity, the existence of finite moments, and weak dependence. Moreover, we study Yule-Walker-type estimators for the fPAR operators and, in a specific setting, estimators for the fPARMA operators, and establish convergence rates under Sobolev-type regularity assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18962v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian K\"uhnert, Juhyun Park</dc:creator>
    </item>
    <item>
      <title>Nonparametric Linear Discriminant Analysis for High Dimensional Matrix-Valued Data</title>
      <link>https://arxiv.org/abs/2507.19028</link>
      <description>arXiv:2507.19028v3 Announce Type: replace 
Abstract: This paper addresses classification problems with matrix-valued data, which commonly arise in applications such as neuroimaging and signal processing. Building on the assumption that the data from each class follows a matrix normal distribution, we propose a novel extension of Fisher's Linear Discriminant Analysis (LDA) tailored for matrix-valued observations. To effectively capture structural information while maintaining estimation flexibility, we adopt a nonparametric empirical Bayes framework based on Nonparametric Maximum Likelihood Estimation (NPMLE), applied to vectorized and scaled matrices. The NPMLE method has been shown to provide robust, flexible, and accurate estimates for vector-valued data with various structures in the mean vector or covariance matrix. By leveraging its strengths, our method is effectively generalized to the matrix setting, thereby improving classification performance. Through extensive simulation studies and real data applications, including electroencephalography (EEG) and magnetic resonance imaging (MRI) analysis, we demonstrate that the proposed method tends to outperform existing approaches across a variety of data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19028v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seungyeon Oh, Seongoh Park, Hoyoung Park</dc:creator>
    </item>
    <item>
      <title>Conformalized Polynomial Chaos Expansion for Uncertainty-aware Surrogate Modeling</title>
      <link>https://arxiv.org/abs/2510.22375</link>
      <description>arXiv:2510.22375v3 Announce Type: replace 
Abstract: This work introduces a method to equip data-driven polynomial chaos expansion surrogate models with intervals that quantify the predictive uncertainty of the surrogate. To that end, jackknife-based conformal prediction is integrated into regression-based polynomial chaos expansions. The jackknife algorithm uses leave-one-out residuals to generate predictive intervals around the predictions of the polynomial chaos surrogate. The jackknife+ extension additionally requires leave-one-out model predictions. Both methods allow to use the entire dataset for model training and do not require a hold-out dataset for prediction interval calibration. The key to efficient implementation is to leverage the linearity of the polynomial chaos regression model, so that leave-one-out residuals and, if necessary, leave-one-out model predictions can be computed with analytical, closed-form expressions. This eliminates the need for repeated model re-training. The conformalized polynomial chaos expansion method is first validated on four benchmark models and then applied to two electrical engineering design use-cases. The method produces predictive intervals that provide the target coverages, even for low-accuracy models trained with small datasets. At the same time, training data availability plays a crucial role in improving the empirical coverage and narrowing the predictive interval, as well as in reducing their variability over different training datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22375v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dimitrios Loukrezis, Dimitris G. Giovanis</dc:creator>
    </item>
    <item>
      <title>Balancing Evidentiary Value and Sample Size of Adaptive Designs with Application to Animal Experiments</title>
      <link>https://arxiv.org/abs/2511.17292</link>
      <description>arXiv:2511.17292v2 Announce Type: replace 
Abstract: Reducing the number of experimental units is one of the three pillars of the 3R principles (Replace, Reduce, Refine) in animal research. At the same time, statistical error rates need to be controlled to enable reliable inferences and decisions. This paper proposes a novel measure to quantify the evidentiary value of one experimental unit for a given study design. The experimental unit information index (EUII) is based on power, Type-I error and sample size, and has attractive interpretations both in terms of frequentist error rates and Bayesian posterior odds. We introduce the EUII in simple statistical test settings and show that its asymptotic value depends only on the assumed relative effect size under the alternative. We then extend the definition to adaptive designs where early stopping for efficacy or futility may cause reductions in sample size. Applications to group-sequential designs and a recently proposed adaptive statistical test procedure show the usefulness of the approach when the goal is to maximize the evidentiary value of one experimental unit. A reanalysis of 2738 animal experiments with simulated results from (post-hoc) interim analyses illustrates the possible savings in sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17292v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonhard Held, Fadoua Balabdaoui, Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>Model-Based Clustering of Functional Data Via Random Projection Ensembles</title>
      <link>https://arxiv.org/abs/2512.01450</link>
      <description>arXiv:2512.01450v2 Announce Type: replace 
Abstract: Clustering functional data is a challenging task due to intrinsic infinite-dimensionality and the need for stable, data-adaptive partitioning. In this work, we propose a clustering framework based on Random Projections, which simultaneously performs dimensionality reduction and generates multiple stochastic representations of the original functions. Each projection is clustered independently, and the resulting partitions are then aggregated through an ensemble consensus procedure, enhancing robustness and mitigating the influence of any single projection. To focus on the most informative representations, projections are ranked according to clustering quality criteria, and only a selected subset is retained. In particular, we adopt Gaussian Mixture Models as base clusterers and employ the Kullback-Leibler divergence to order the random projections; these choices enable fast computation and eliminate the need to specify the number of clusters a priori. The performance of the proposed methodology is assessed through an extensive simulation study and two real-data applications, one from spectroscopy data for food authentication and one from log-periodograms of speech recording; the obtained results suggest that the proposal represents an effective tool for the clustering of functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01450v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Mori, Laura Anderlucci</dc:creator>
    </item>
    <item>
      <title>A Metadata-Only Feature-Augmented Method Factor for Ex-Post Correction and Attribution of Common Method Variance</title>
      <link>https://arxiv.org/abs/2512.13446</link>
      <description>arXiv:2512.13446v2 Announce Type: replace 
Abstract: Common Method Variance (CMV) is a recurring problem that reduces survey accuracy. Popular fixes such as the Harman single-factor test, correlated uniquenesses, common latent factor models, and marker variable approaches have well known flaws. These approaches either poorly identify issues, rely too heavily on researchers' choices, omit real information, or require special marker items that many datasets lack. This paper introduces a metadata-only Feature-Augmented Method Factor (FAMF-SEM): a single extra method factor with fixed, item-specific weights based on questionnaire details like reverse coding, page and item order, scale width, wording direction, and item length. These weights are set using ridge regression, based on residual correlations in a basic CFA, and remain fixed in the model. The method avoids the need for additional data or marker variables and provides CMV-adjusted results with clear links to survey design features. An AMOS/LISREL-friendly, no-code Excel workflow demonstrates the method. The paper explains the rationale, provides model details, outlines setup, presents step-by-step instructions, describes checks and reliability tests, and notes limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13446v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Murat Yaslioglu</dc:creator>
    </item>
    <item>
      <title>On the E(s^2)-optimality of two-level supersaturated designs constructed using Wu's method of partially aliased interactions on certain two-level orthogonal arrays</title>
      <link>https://arxiv.org/abs/2512.14378</link>
      <description>arXiv:2512.14378v2 Announce Type: replace 
Abstract: Wu [10] proposed a method for constructing two-level supersaturated designs by using a Hadamard design with n runs and n-1 columns as a staring design and by supplementing it with two-column interactions, as long as they are partially aliased. Bulutoglu and Cheng [2] proved that this method results in E(s^2)-optimal supersaturated designs when certain interaction columns are selected. In this paper, we extend these results and prove E(s^2)-optimality for supersaturated designs that are constructed using Wu's method when the starting design is any orthogonal array with n runs and n-1, n-2 or n-3 columns, as long as its main effects and two-column interactions are partially aliased with two-column interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14378v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emmanouil Androulakis, Kashinath Chatterjee, Haralambos Evangelaras</dc:creator>
    </item>
    <item>
      <title>Spatial-Network Treatment Effects: A Continuous Functional Approach</title>
      <link>https://arxiv.org/abs/2512.12653</link>
      <description>arXiv:2512.12653v3 Announce Type: replace-cross 
Abstract: This paper develops a continuous functional framework for treatment effects that propagate through geographic space and economic networks. We derive a master equation governing propagation from three economic foundations -- heterogeneous agent aggregation, market equilibrium, and cost minimization -- establishing that the framework rests on fundamental principles rather than ad hoc specifications. A key result shows that the spatial-network interaction coefficient equals the mutual information between geographic and market coordinates. The Feynman-Kac representation decomposes effects into inherited and accumulated components along stochastic paths representing economic linkages. The framework nests the no-spillover case as a testable restriction. Monte Carlo simulations demonstrate that conventional estimators -- two-way fixed effects, difference-in-differences, and generalized propensity score -- exhibit 25-38% bias and severe undercoverage when spillovers exist, while our estimator maintains correct inference regardless of whether spillovers are present. Applying the framework to U.S. minimum wage policy, we reject the no-spillover null and find total effects at state borders four times larger than direct effects -- conventional methods capture only one-quarter of policy impact. Structural estimates reveal spatial diffusion consistent with commuting-distance labor mobility, network diffusion consistent with quarterly supply chain adjustment, and significant spatial-network interaction reflecting geographic clustering of industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12653v3</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
  </channel>
</rss>
