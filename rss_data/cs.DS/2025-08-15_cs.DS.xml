<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Aug 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Output-Sparse Matrix Multiplication Using Compressed Sensing</title>
      <link>https://arxiv.org/abs/2508.10250</link>
      <description>arXiv:2508.10250v1 Announce Type: new 
Abstract: We give two algorithms for output-sparse matrix multiplication (OSMM), the problem of multiplying two $n \times n$ matrices $A, B$ when their product $AB$ is promised to have at most $O(n^{\delta})$ many non-zero entries for a given value $\delta \in [0, 2]$. We then show how to speed up these algorithms in the fully sparse setting, where the input matrices $A, B$ are themselves sparse. All of our algorithms work over arbitrary rings.
  Our first, deterministic algorithm for OSMM works via a two-pass reduction to compressed sensing. It runs in roughly $n^{\omega(\delta/2, 1, 1)}$ time, where $\omega(\cdot, \cdot, \cdot)$ is the rectangular matrix multiplication exponent. This substantially improves on prior deterministic algorithms for output-sparse matrix multiplication.
  Our second, randomized algorithm for OSMM works via a reduction to compressed sensing and a variant of matrix multiplication verification, and runs in roughly $n^{\omega(\delta - 1, 1, 1)}$ time. This algorithm and its extension to the fully sparse setting have running times that match those of the (randomized) algorithms for OSMM and FSMM, respectively, in recent work of Abboud, Bringmann, Fischer, and K\"{u}nnemann (SODA, 2024). Our algorithm uses different techniques and is arguably simpler.
  Finally, we observe that the running time of our randomized algorithm and the algorithm of Abboud et al. are optimal via a simple reduction from rectangular matrix multiplication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10250v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huck Bennett, Karthik Gajulapalli, Alexander Golovnev, Evelyn Warton</dc:creator>
    </item>
    <item>
      <title>Lower Bounds on Tree Covers</title>
      <link>https://arxiv.org/abs/2508.10376</link>
      <description>arXiv:2508.10376v1 Announce Type: new 
Abstract: Given an $n$-point metric space $(X,d_X)$, a tree cover $\mathcal{T}$ is a set of $|\mathcal{T}|=k$ trees on $X$ such that every pair of vertices in $X$ has a low-distortion path in one of the trees in $\mathcal{T}$. Tree covers have been playing a crucial role in graph algorithms for decades, and the research focus is the construction of tree covers with small size $k$ and distortion.
  When $k=1$, the best distortion is known to be $\Theta(n)$. For a constant $k\ge 2$, the best distortion upper bound is $\tilde O(n^{\frac 1 k})$ and the strongest lower bound is $\Omega(\log_k n)$, leaving a gap to be closed. In this paper, we improve the lower bound to $\Omega(n^{\frac{1}{2^{k-1}}})$.
  Our proof is a novel analysis on a structurally simple grid-like graph, which utilizes some combinatorial fixed-point theorems. We believe that they will prove useful for analyzing other tree-like data structures as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10376v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Chen, Zihan Tan, Hangyu Xu</dc:creator>
    </item>
    <item>
      <title>On Fixed-Parameter Tractability of Weighted 0-1 Timed Matching Problem on Temporal Graphs</title>
      <link>https://arxiv.org/abs/2508.10562</link>
      <description>arXiv:2508.10562v1 Announce Type: new 
Abstract: Temporal graphs are introduced to model systems where the relationships among the entities of the system evolve over time. In this paper, we consider the temporal graphs where the edge set changes with time and all the changes are known a priori. The underlying graph of a temporal graph is a static graph consisting of all the vertices and edges that exist for at least one timestep in the temporal graph. The concept of 0-1 timed matching in temporal graphs was introduced by Mandal and Gupta [DAM2022] as an extension of the matching problem in static graphs. A 0-1 timed matching of a temporal graph is a non-overlapping subset of the edge set of that temporal graph. The problem of finding the maximum 0-1 timed matching is proved to be NP-complete on multiple classes of temporal graphs. We study the fixed-parameter tractability of the maximum 0-1 timed matching problem. We prove that the problem remains to be NP-complete even when the underlying static graph of the temporal graph has a bounded treewidth. Furthermore, we establish that the problem is W[1]-hard when parameterized by the solution size. Finally, we present a fixed-parameter tractable (FPT) algorithm to address the problem when the problem is parameterized by the maximum vertex degree and the treewidth of the underlying graph of the temporal graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10562v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rinku Kumar, Bodhisatwa Mazumdar, Subhrangsu Mandal</dc:creator>
    </item>
    <item>
      <title>Spirals and Beyond: Competitive Plane Search with Multi-Speed Agents</title>
      <link>https://arxiv.org/abs/2508.10793</link>
      <description>arXiv:2508.10793v1 Announce Type: new 
Abstract: We consider the problem of minimizing the worst-case search time for a hidden point target in the plane using multiple mobile agents of differing speeds, all starting from a common origin. The search time is normalized by the target's distance to the origin, following the standard convention in competitive analysis. The goal is to minimize the maximum such normalized time over all target locations, the search cost. As a base case, we extend the known result for a single unit-speed agent, which achieves an optimal cost of about $\mathcal{U}_1 = 17.28935$ via a logarithmic spiral, to $n$ unit-speed agents. We give a symmetric spiral-based algorithm where each agent follows a logarithmic spiral offset by equal angular phases. This yields a search cost independent of which agent finds the target. We provide a closed-form upper bound $\mathcal{U}_n$ for this setting, which we use in our general result. Our main contribution is an upper bound on the worst-case normalized search time for $n$ agents with arbitrary speeds. We give a framework that selects a subset of agents and assigns spiral-type trajectories with speed-dependent angular offsets, again making the search cost independent of which agent reaches the target. A corollary shows that $n$ multi-speed agents (fastest speed 1) can beat $k$ unit-speed agents (cost below $\mathcal{U}_k$) if the geometric mean of their speeds exceeds $\mathcal{U}_n / \mathcal{U}_k$. This means slow agents may be excluded if they lower the mean too much, motivating non-spiral algorithms. We also give new upper bounds for point search in cones and conic complements using a single unit-speed agent. These are then used to design hybrid spiral-directional strategies, which outperform the spiral-based algorithms when some agents are slow. This suggests that spiral-type trajectories may not be optimal in the general multi-speed setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10793v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantinos Georgiou, Caleb Jones, Matthew Madej</dc:creator>
    </item>
    <item>
      <title>Competitively Consistent Clustering</title>
      <link>https://arxiv.org/abs/2508.10800</link>
      <description>arXiv:2508.10800v1 Announce Type: new 
Abstract: In fully-dynamic consistent clustering, we are given a finite metric space $(M,d)$, and a set $F\subseteq M$ of possible locations for opening centers. Data points arrive and depart, and the goal is to maintain an approximately optimal clustering solution at all times while minimizing the recourse, the total number of additions/deletions of centers over time. Specifically, we study fully dynamic versions of the classical $k$-center, facility location, and $k$-median problems. We design algorithms that, given a parameter $\beta\geq 1$, maintain an $O(\beta)$-approximate solution at all times, and whose total recourse is bounded by $O(\log |F| \log \Delta) \cdot \text{OPT}_\text{rec}^{\beta}$. Here $\text{OPT}_\text{rec}^{\beta}$ is the minimal recourse of an offline algorithm that maintains a $\beta$-approximate solution at all times, and $\Delta$ is the metric aspect ratio. Finally, while we compare the performance of our algorithms to an optimal solution that maintains $k$ centers, our algorithms are allowed to use slightly more than $k$ centers. We obtain our results via a reduction to the recently proposed Positive Body Chasing framework of [Bhattacharya, Buchbinder, Levin, Saranurak, FOCS 2023], which we show gives fractional solutions to our clustering problems online. Our contribution is to round these fractional solutions while preserving the approximation and recourse guarantees. We complement our positive results with logarithmic lower bounds which show that our bounds are nearly tight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10800v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niv Buchbinder, Roie Levin, Yue Yang</dc:creator>
    </item>
    <item>
      <title>Welfare-Centric Clustering</title>
      <link>https://arxiv.org/abs/2508.10345</link>
      <description>arXiv:2508.10345v1 Announce Type: cross 
Abstract: Fair clustering has traditionally focused on ensuring equitable group representation or equalizing group-specific clustering costs. However, Dickerson et al. (2025) recently showed that these fairness notions may yield undesirable or unintuitive clustering outcomes and advocated for a welfare-centric clustering approach that models the utilities of the groups. In this work, we model group utilities based on both distances and proportional representation and formalize two optimization objectives based on welfare-centric clustering: the Rawlsian (Egalitarian) objective and the Utilitarian objective. We introduce novel algorithms for both objectives and prove theoretical guarantees for them. Empirical evaluations on multiple real-world datasets demonstrate that our methods significantly outperform existing fair clustering baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10345v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claire Jie Zhang, Seyed A. Esmaeili, Jamie Morgenstern</dc:creator>
    </item>
    <item>
      <title>Decoded Quantum Interferometry Under Noise</title>
      <link>https://arxiv.org/abs/2508.10725</link>
      <description>arXiv:2508.10725v1 Announce Type: cross 
Abstract: Decoded Quantum Interferometry (DQI) is a recently proposed quantum optimization algorithm that exploits sparsity in the Fourier spectrum of objective functions, with the potential for exponential speedups over classical algorithms on suitably structured problems. While highly promising in idealized settings, its resilience to noise has until now been largely unexplored. To address this, we conduct a rigorous analysis of DQI under noise, focusing on local depolarizing noise. For the maximum linear satisfiability problem, we prove that, in the presence of noise, performance is governed by a noise-weighted sparsity parameter of the instance matrix, with solution quality decaying exponentially as sparsity decreases. We demonstrate this decay through numerical simulations on two special cases: the Optimal Polynomial Intersection problem and the Maximum XOR Satisfiability problem. The Fourier-analytic methods we develop can be readily adapted to other classes of random Pauli noise, making our framework applicable to a broad range of noisy quantum settings and offering guidance on preserving DQI's potential quantum advantage under realistic noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10725v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaifeng Bu, Weichen Gu, Dax Enshan Koh, Xiang Li</dc:creator>
    </item>
    <item>
      <title>A Polynomial-Time Deterministic Algorithm for an NP-Complete Problem</title>
      <link>https://arxiv.org/abs/2108.03877</link>
      <description>arXiv:2108.03877v4 Announce Type: replace 
Abstract: We introduce an NP-complete graph decision problem, the "Multi-stage graph Simple Path" (abbr. MSP) problem, which focuses on determining the existence of specific "global paths" in a graph $G$. We show that the MSP problem can be solved in polynomial ($O(|E|^9)$) time, by proposing a polynomial-time graph algorithm and the proof of its correctness. Our result implies NP$=$P. The algorithm leverages the data structure of reachable-path edge-set $R(e)$. By establishing the interplay between preceding decisions and subsequent decisions, the information computed for $R(e)$ (in a monotonically decreasing manner) carries all necessary contextual information, and can be utilized to summarize the "history" and to detect the "future" for searching "global paths". The relation of $R(e)$ of different stages in the multi-stage graph resembles the state-transition equation in dynamic programming, though it is much more convoluted. To avoid exponential complexity, paths are always treated as a collection of edge sets. Our proof of the algorithm is built upon a mathematical induction - based proving framework, which relies on a crucial structural property of the MSP problem: all MSP instances are arranged into the sequence {$G_0,G_1,G_2,...$}, and each $G_{j}(j&gt;0)$ in the sequence must have some $G_{i}(0\leq i&lt;j)$ that is completely consistent with $G_{j}$ on the existence of "global paths". As an auxiliary method, we have conducted tests using multiple AI systems. With the help of a suggested query list that covers the entire content of the paper, the paper has been verified by Doubao, DeepSeek, Kimi, iFlytek Spark, ERNIE Bot, Gemini, and GPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.03877v4</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinwen Jiang, Holden Wool</dc:creator>
    </item>
    <item>
      <title>Interval-Constrained Bipartite Matching over Time</title>
      <link>https://arxiv.org/abs/2402.18469</link>
      <description>arXiv:2402.18469v5 Announce Type: replace 
Abstract: Interval-constrained online bipartite matching problem frequently occurs in medical appointment scheduling: Unit-time jobs representing patients arrive online and are assigned to a time slot within their given feasible time interval. We consider a variant of this problem where reassignments are allowed and extend it by a notion of time that is decoupled from the job arrival events. As jobs appear, the current point in time gradually advances, and once the time of a slot is passed, the job assigned to it is fixed and cannot be reassigned anymore.
  We analyze two algorithms for the problem with respect to the resulting matching size and the number of reassignments they make. We show that FirstFit with reassignments according to the shortest augmenting path rule is $\frac{2}{3}$-competitive with respect to the matching cardinality, and that the bound is tight. For the number of reassignments performed by the algorithm, we show that it is in $\Omega(n \log n)$ in the worst case, where $n$ is the number of patients or jobs on the online side. The competitive ratio remains bounded by $\frac{2}{3}$ if we restrict the algorithm to make only up to a constant number $k \geq 1$ of reassignments per job arrival. This fills the gap between the known optimal algorithm that makes no reassignments, which is $\frac{1}{2}$-competitive, on the one hand, and an earliest-deadline-first strategy (EDF), which we prove to obtain a maximum matching in this over-time framework, but which suffers $\Omega(n^2)$ reassignments in the worst case, on the other hand.
  Further, we consider the setting in which the sets of feasible slots per job that are not intervals. We show that FirstFit remains $\frac{2}{3}$-competitive in this case, and that this is the best possible deterministic competitive ratio, while EDF loses its optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18469v5</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Abels, Mariia Anapolska, Christina B\"using</dc:creator>
    </item>
    <item>
      <title>Sampling permutations satisfying constraints within the lopsided local lemma regime</title>
      <link>https://arxiv.org/abs/2411.02750</link>
      <description>arXiv:2411.02750v3 Announce Type: replace 
Abstract: Sampling a random permutation with restricted positions, or equivalently approximating the permanent of a 0-1 matrix, is a fundamental problem in computer science, with several notable results achieved over the years. However, existing algorithms typically exhibit high computational complexity. Achieving the optimal running time remains elusive, even for nontrivial subsets of the problem. Furthermore, existing algorithms primarily focus on a single permutation, leaving many combinatorial problems involving multiple constrained permutations unaddressed.
  For a single permutation, we achieve the optimal running time $O(n^2)$ for approximating the permanent of a very dense $n \times n$ 0-1 matrix, where each row and column contains at most $\sqrt{(n-2)/20}$ zeros. This result serves as a fundamental building block in our sampling algorithm for multiple permutations.
  We further introduce a general model called permutations with disjunctive constraints (PDC) for handling multiple constrained permutations. We propose a novel Markov chain-based algorithm for sampling nearly uniform solutions of PDC within a lopsided Lov\'asz Local Lemma (LLL) regime. For uniform PDC formulas, where all constraints are of the same width and all permutations are of the same size, our algorithm runs in nearly linear time with respect to the number of variables.
  Previous approaches for sampling LLL relied on the variable model. In contrast, the sampling problem of PDC encounters a fundamental challenge: the random variables within each permutation in the joint probability space are not mutually independent, leading to long-range correlations. To tackle this challenge, we introduce a novel sampling framework called correlated factorization and a new concept in the path coupling analysis, termed the inactive vertex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02750v3</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun He, Guoliang Qiu, Xiaoming Sun</dc:creator>
    </item>
    <item>
      <title>On the Two Paths Theorem and the Two Disjoint Paths Problem</title>
      <link>https://arxiv.org/abs/2505.16431</link>
      <description>arXiv:2505.16431v2 Announce Type: replace 
Abstract: A tuple (s1,t1,s2,t2) of vertices in a simple undirected graph is 2-linked when there are two vertex-disjoint paths respectively from s1 to t1 and s2 to t2. A graph is 2-linked when all such tuples are 2-linked. We give a new and simple proof of the ``two paths theorem'', a characterisation of edge-maximal graphs which are not 2-linked as webs: particular near triangulations filled with cliques. Our proof works by generalising the theorem, replacing the four vertices above by an arbitrary tuple; it does not require major theorems such as Kuratowski's or Menger's theorems. Instead it follows an inductive characterisation of generalised webs via parallel composition, a graph operation consisting in taking a disjoint union before identifying some pairs of vertices. We use the insights provided by this proof to design a simple O(nm) recursive algorithm for the ``two vertex-disjoint paths'' problem. This algorithm is constructive in that it returns either two disjoint paths, or an embedding of the input graph into a web.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16431v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Humeau (ENS de Lyon, LIP, PLUME), Damien Pous (PLUME, LIP, ENS de Lyon)</dc:creator>
    </item>
    <item>
      <title>Faster Multi-Source Reachability and Approximate Distances via Shortcuts, Hopsets and Matrix Multiplication</title>
      <link>https://arxiv.org/abs/2507.13470</link>
      <description>arXiv:2507.13470v2 Announce Type: replace 
Abstract: Given an $n$-vertex $m$-edge digraph $G = (V,E)$ and a subset $S \subseteq V$ of $|S| = n^{\sigma}$ (for some $0 \le \sigma \le 1$) designated sources, the $S \times V$ reachability problem is to compute the sets $\mathcal V_s$ of vertices reachable from $s$, for every $s \in S$. Naive centralized algorithms run BFS/DFS from each source in $O(m \cdot n^{\sigma})$ time or compute $G$'s transitive closure in $\hat O(n^{\omega})$ time, where $\omega \le 2.371552\ldots$ is the matrix multiplication exponent. Thus, the best known bound is $\hat O(n^{\min \{ 2 + \sigma, \omega\}})$. Leveraging shortcut constructions by Kogan and Parter [SODA 2022, ICALP 2022], we develop a centralized algorithm with running time $\hat O(n^{1 + \frac{2}{3} \omega(\sigma)})$, where $\omega(\sigma)$ is the rectangular matrix multiplication exponent. Using current estimates on $\omega(\sigma)$, our exponent improves upon $\min \{2 + \sigma, \omega \}$ for $\tilde \sigma \leq \sigma \leq 0.53$, where $1/3 &lt; \tilde \sigma &lt; 0.3336$ is a universal constant.
  In a classical result, Cohen [Journal of Algorithms, 1996] devised parallel algorithms for $S \times V$ reachability on graphs admitting balanced recursive separators of size $n^{\rho}$ for $\rho &lt; 1$, requiring polylogarithmic time and work $n^{\max \{\omega \rho, 2\rho + \sigma \} + o(1)}$. We significantly improve, extend, and generalize Cohen's result. First, our parallel algorithm for graphs with small recursive separators has lower work complexity than Cohen's in boraod paramater ranges. Second, we generalize our algorithm to graphs of treewidth at most $n^{\rho}$ ($\rho &lt; 1$) and provide a centralized algorithm that outperforms existing bounds for $S \times V$ reachability on such graphs. We also do this for some other graph familes with small separators. Finally, we extend these results to $(1 + \epsilon)$-approximate distance computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13470v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Elkin, Chhaya Trehan</dc:creator>
    </item>
    <item>
      <title>Learning to Schedule in Parallel-Server Queues with Stochastic Bilinear Rewards</title>
      <link>https://arxiv.org/abs/2112.06362</link>
      <description>arXiv:2112.06362v4 Announce Type: replace-cross 
Abstract: We consider the problem of scheduling in multi-class, parallel-server queuing systems with uncertain rewards from job-server assignments. In this scenario, jobs incur holding costs while awaiting completion, and job-server assignments yield observable stochastic rewards with unknown mean values. The mean rewards for job-server assignments are assumed to follow a bilinear model with respect to features that characterize jobs and servers. Our objective is to minimize regret by maximizing the cumulative reward of job-server assignments over a time horizon, while keeping the total job holding cost bounded to ensure the stability of the queueing system. This problem is motivated by applications requiring resource allocation in network systems. In this problem, it is essential to control the tradeoff between reward maximization and fair allocation for the stability of the underlying queuing system (i.e., maximizing network throughput). To address this problem, we propose a scheduling algorithm based on a weighted proportional fair criteria augmented with marginal costs for reward maximization, incorporating a bandit algorithm tailored for bilinear rewards. Our algorithm achieves a sub-linear regret bound and a sub-linear mean holding cost (and queue length bound) of $\tilde{O}(\sqrt{T})$, respectively, with respect to the time horizon $T$, thus guaranteeing queuing system stability. Additionally, we establish stability conditions for distributed iterative algorithms for computing allocations, which are relevant to large-scale system applications. Finally, we demonstrate the efficiency of our algorithm through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.06362v4</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jung-hun Kim, Milan Vojnovic</dc:creator>
    </item>
    <item>
      <title>A bargain for mergesorts -- How to prove your mergesort correct and stable, almost for free</title>
      <link>https://arxiv.org/abs/2403.08173</link>
      <description>arXiv:2403.08173v3 Announce Type: replace-cross 
Abstract: We present a novel characterization of stable mergesort functions using relational parametricity, and show that it implies the functional correctness of mergesort. As a result, one can prove the correctness of several variations of mergesort (e.g., top-down, bottom-up, tail-recursive, non-tail-recursive, smooth, and non-smooth mergesorts) by proving the characteristic property for each variation. Thanks to our characterization and the parametricity translation, we deduced the correctness results, including stability, of various implementations of mergesort for lists, including highly optimized ones, in the Rocq Prover (formerly the Coq Proof Assistant).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08173v3</guid>
      <category>cs.LO</category>
      <category>cs.DS</category>
      <category>cs.PL</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3747505</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Program. Lang. 9, ICFP, Article 236 (August 2025)</arxiv:journal_reference>
      <dc:creator>Cyril Cohen, Kazuhiko Sakaguchi</dc:creator>
    </item>
  </channel>
</rss>
