<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.OS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.OS</link>
    <description>cs.OS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.OS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Nov 2024 02:44:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>xNVMe: Unleashing Storage Hardware-Software Co-design</title>
      <link>https://arxiv.org/abs/2411.06980</link>
      <description>arXiv:2411.06980v1 Announce Type: new 
Abstract: NVMe SSD hardware has witnessed widespread deployment as commodity and enterprise hardware due to its high performance and rich feature set. Despite the open specifications of various NVMe protocols by the NVMe Express group and NVMe being of software abstractions to program the underlying hardware. The myriad storage I/O paths such as POSIX storage API, ad-hoc OS mechanisms, and userspace I/O libraries have different syntax and semantics that complicate software development and stand in the way of mass adoption and evolution of the NVMe ecosystem. To unify the diverse I/O storage paths, we built xNVMe that exposes a single message-passing API to support both asynchronous and synchronous communication with NVMe devices. xNVMe provides various command sets to support diverse storage I/O paths in different OS (e.g., Linux, FreeBSD, Windows, and MacOS) and userspace libraries (e.g., SPDK) with minimal overhead. xNVMe is an Open Source project and has gained traction amongst various industry stakeholders. In this paper, we elaborate on the lessons that we have learned in the project during its evolution. We also provide some ongoing and future work planned for the project. We hope the database and storage systems community can join in the effort to both extend xNVMe and leverage it as a building block for innovative co-design of storage systems on modern NVMe hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06980v1</guid>
      <category>cs.OS</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon A. F. Lund, Vivek Shah</dc:creator>
    </item>
    <item>
      <title>Virtuoso: High Resource Utilization and {\mu}s-scale Performance Isolation in a Shared Virtual Machine TCP Network Stack</title>
      <link>https://arxiv.org/abs/2309.14016</link>
      <description>arXiv:2309.14016v3 Announce Type: replace-cross 
Abstract: Virtualization improves resource efficiency and ensures security and performance isolation for cloud applications. Today, operators use a layered architecture with separate network stack instances in each VM and container connected to a virtual switch. Decoupling through layering reduces complexity, but induces performance and resource overheads at odds with increasing demands for network bandwidth, connection scalability, and low latency.
  We present Virtuoso, a new software network stack for VMs and containers. Virtuoso re-organizes the network stack to maximize CPU utilization, enforce isolation, and minimize processing overheads. We maximize utilization by running one elastically shared network stack instance on dedicated cores; we enforce isolation by performing central and fine-grained per-packet resource accounting and scheduling; we reduce overheads by building a single-layer data path with a one-shot fast-path incorporating all processing from the TCP transport layer through network virtualization and virtual switching. Virtuoso improves resource efficiency by up to 82%, latencies by up to 58% compared to other virtualized network stacks without sacrificing isolation, and keeps processing overhead within 6.7% of unvirtualized stacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14016v3</guid>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matheus Stolet, Liam Arzola, Simon Peter, Antoine Kaufmann</dc:creator>
    </item>
  </channel>
</rss>
