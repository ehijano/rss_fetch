<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Nov 2025 02:36:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Weighted Asymptotically Optimal Sequential Testing</title>
      <link>https://arxiv.org/abs/2511.07588</link>
      <description>arXiv:2511.07588v1 Announce Type: new 
Abstract: This paper develops a framework for incorporating prior information into sequential multiple testing procedures while maintaining asymptotic optimality. We define a weighted log-likelihood ratio (WLLR) as an additive modification of the standard LLR and use it to construct two new sequential tests: the Weighted Gap and Weighted Gap-Intersection procedures. We prove that both procedures provide strong control of the family-wise error rate. Our main theoretical contribution is to show that these weighted procedures are asymptotically optimal; their expected stopping times achieve the theoretical lower bound as the error probabilities vanish. This first-order optimality is shown to be robust, holding in high-dimensional regimes where the number of null hypotheses grows and in settings with random weights, provided that mild, interpretable conditions on the weight distribution are met.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07588v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soumyabrata Bose, Jay Bartroff</dc:creator>
    </item>
    <item>
      <title>Asymmetric Space-Time Covariance Functions via Hierarchical Mixtures</title>
      <link>https://arxiv.org/abs/2511.07959</link>
      <description>arXiv:2511.07959v1 Announce Type: new 
Abstract: This work is focused on constructing space-time covariance functions through a hierarchical mixture approach that can serve as building blocks for capturing complex dependency structures. This hierarchical mixture approach provides a unified modeling framework that not only constructs a new class of asymmetric space-time covariance functions with closed-form expressions, but also provides corresponding space-time process representations, which further unify constructions for many existing space-time covariance models. This hierarchical mixture framework decomposes the complexity of model specification at different levels of hierarchy, for which parsimonious covariance models can be specified with simple mixing measures to yield flexible properties and closed-form derivation. A characterization theorem is provided for the hierarchical mixture approach on how the mixing measures determine the statistical properties of covariance functions. Several new covariance models resulting from this hierarchical mixture approach are discussed in terms of their practical usefulness. A theorem is also provided to construct a general class of valid asymmetric space-time covariance functions with arbitrary and possibly different degrees of smoothness in space and in time and flexible long-range dependence. The proposed covariance class also bridges a theoretical gap in using the Lagrangian reference framework. The superior performance of several new parsimonious covariance models over existing models is verified with the well-known Irish wind data and the U.S. air temperature data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07959v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pulong Ma</dc:creator>
    </item>
    <item>
      <title>Inference on multiple quantiles in regression models by a rank-score approach</title>
      <link>https://arxiv.org/abs/2511.07999</link>
      <description>arXiv:2511.07999v1 Announce Type: new 
Abstract: This paper tackles the challenge of performing multiple quantile regressions across different quantile levels and the associated problem of controlling the familywise error rate, an issue that is generally overlooked in practice. We propose a multivariate extension of the rank-score test and embed it within a closed-testing procedure to efficiently account for multiple testing. Theoretical foundations and simulation studies demonstrate that our method effectively controls the familywise error rate while achieving higher power than traditional corrections, such as Bonferroni.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07999v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo De Santis, Anna Vesely, Angela Andreella</dc:creator>
    </item>
    <item>
      <title>ANOVATS: A subsampling-based test to detect differences among short time series in marine studies</title>
      <link>https://arxiv.org/abs/2511.08070</link>
      <description>arXiv:2511.08070v1 Announce Type: new 
Abstract: Assessing marine ecosystems is important for understanding the impacts of climate change and human activity, as well as for maintaining healthy oceans and ecosystems. In marine science, it is common for biologists and geologists to identify regional differences based on expert knowledge, frequently through data visualization. However, time series data collected through surveys in marine studies typically span only a few decades, limiting the applicability of classical time series methods. Additionally, without expert knowledge, detecting significant differences becomes challenging. To address these issues, we introduce ANOVATS (ANOVA for small-sample time series data), a subsampling-based method to detect regional differences in small-sample time series data with a fixed number of groups. This method bypasses the need for spectral density estimation, which requires a large number of time points in the data. Furthermore, after detecting differences in homogeneity across all areas using the ANOVATS procedure, we devised a simple ANOVATS post hoc procedure to group the areas. Finally, we demonstrate the effectiveness of our method by analyzing zooplankton biomass data collected in different strata of the North Sea, showing its ability to quantify differences in species between geographical areas without relying on prior biological or geographical knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08070v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuichi Goto, Hiroko Kato Solvang, Masanobu Taniguchi, Tone Falkenhaug</dc:creator>
    </item>
    <item>
      <title>Who's Afraid of the Wallenius Distribution?</title>
      <link>https://arxiv.org/abs/2511.08088</link>
      <description>arXiv:2511.08088v1 Announce Type: new 
Abstract: This paper is about the use of the Wallenius noncentral hypergeometric distribution for analysing contingency tables with two or more groups and two categories and with row margins and sample size, that is both margins, fixed. The parameters of the distribution are taken to be weights which are positive and sum to one and are thus defined on a regular simplex. The approach to analysis is presented for likelihood-based and Bayesian inference and is illustrated by example, with datasets taken from the literature and, in one case, used to generate semi-synthetic data. The analysis of two-by-two contingency tables using the univariate Wallenius distribution is shown to be straightforward, with the parameter a single weight which translates immediately to the requisite odds and the odds ratio. The analysis of contingency tables with more than two groups based on the multivariate Wallenius distribution was however more nuanced than that of the two-group tables. Specifically, some numerical subtleties were required in order to implement the necessary calculations. In particular, optimisation with respect to the weights was performed by transforming the weights to yield an unconstrained optimisation problem and likelihoods which are extremely small were scaled by an appropriate multiplying factor without compromising the elements of inference. Furthermore, a novel Markov chain Monte Carlo algorithm for Bayesian inference, termed the sphere walk Metropolis, was constructed. The proposal is implemented in Cartesian coordinates on the reference simplex and the Metropolis filter in barycentric coordinates on the regular simplex, with the transition between barycentric and Cartesian coordinates effected seamlessly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08088v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linda M. Haines</dc:creator>
    </item>
    <item>
      <title>Simulation-Based Fitting of Intractable Models via Sequential Sampling and Local Smoothing</title>
      <link>https://arxiv.org/abs/2511.08180</link>
      <description>arXiv:2511.08180v1 Announce Type: new 
Abstract: This paper presents a comprehensive algorithm for fitting generative models whose likelihood, moments, and other quantities typically used for inference are not analytically or numerically tractable. The proposed method aims to provide a general solution that requires only limited prior information on the model parameters. The algorithm combines a global search phase, aimed at identifying the region of the solution, with a local search phase that mimics a trust region version of the Fisher scoring algorithm for computing a quasi-likelihood estimator. Comparisons with alternative methods demonstrate the strong performance of the proposed approach. An R package implementing the algorithm is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08180v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guido Masarotto</dc:creator>
    </item>
    <item>
      <title>Reclustering: A New Method to Test the Appropriate Level of Clustering</title>
      <link>https://arxiv.org/abs/2511.08184</link>
      <description>arXiv:2511.08184v1 Announce Type: new 
Abstract: When scholars suspect units are dependent on each other within clusters but independent of each other across clusters, they employ cluster-robust standard errors (CRSEs). Nevertheless, what to cluster over is sometimes unknown. For instance, in the case of cross-sectional survey samples, clusters may be households, municipalities, counties, or states. A few approaches have been proposed, although they are based on asymptotics. I propose a new method to address this issue that works in a finite sample: reclustering. That is, we randomly and repeatedly group fine clusters into new gross clusters and calculate a statistic such as CRSEs. Under the null hypothesis that fine clusters are independent of each other, how they are grouped into gross clusters should not matter for any cluster-sensitive statistic. Thus, if the statistic based on the original clustering is a significant outlier against the distributions of the statistics induced by reclustering, it is reasonable to reject the null hypothesis and employ gross clusters. I compare the performance of reclustering with that of a few previous tests using Monte Carlo simulation and application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08184v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kentaro Fukumoto</dc:creator>
    </item>
    <item>
      <title>Geometric modelling of spatial extremes</title>
      <link>https://arxiv.org/abs/2511.08192</link>
      <description>arXiv:2511.08192v1 Announce Type: new 
Abstract: Recent developments in extreme value statistics have established the so-called geometric approach as a powerful modelling tool for multivariate extremes. We tailor these methods to the case of spatial modelling and examine their efficacy at inferring extremal dependence and performing extrapolation. The geometric approach is based around a limit set described by a gauge function, which is a key target for inference. We consider a variety of spatially-parameterised gauge functions and perform inference on them by building on the framework of Wadsworth and Campbell (2024), where extreme radii are modelled via a truncated gamma distribution. We also consider spatial modelling of the angular distribution, for which we propose two candidate models. Estimation of extreme event probabilities is possible by combining draws from the radial and angular models respectively. We compare our method with two other established frameworks for spatial extreme value analysis and show that our approach generally allows for unbiased, albeit more uncertain, inference compared to the more classical models. We apply the methodology to a space weather dataset of daily geomagnetic field fluctuations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08192v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lydia Kakampakou, Jennifer L. Wadsworth</dc:creator>
    </item>
    <item>
      <title>Clinicians' Interpretation and Preferences for Survival Data Visualisation: A Pre-Post Study Comparing Kaplan-Meier and Mean Residual Life Plots</title>
      <link>https://arxiv.org/abs/2511.08332</link>
      <description>arXiv:2511.08332v1 Announce Type: new 
Abstract: Effective visualization of survival data is essential for clinician interpretation and patient communication. While Kaplan-Meier (KM) plots are widely used, Mean Residual Life (MRL) plots may offer a more intuitive display of prognosis over time. However, little is known about clinicians' knowledge and preferences regarding these alternatives. This pre-post pilot cross-sectional survey assessed 32 medical students and doctors who interpreted four survival plot types (KM, survival difference, MRL, and MRL difference) before and after a brief learning section. Interpretation accuracy, learning gain, and ranking preferences were analyzed. Overall accuracy improved from 50.0 percent pre-learning to 81.2 percent post-learning (p = 0.002), with the largest improvement for MRL plots (+37.5 percentage points). KM plots remained the most preferred for ease of clinical use (59 percent), while MRL plots were valued for patient communication (9 percent). Participants with lower self-rated survival knowledge showed the greatest learning gains. These findings suggest that with minimal instruction, clinicians can interpret MRL plots as effectively as KM plots. Incorporating MRL visualizations into clinical dashboards and medical education could improve understanding of survival outcomes and patient-centered communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08332v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Pacifique Rwandarwacu</dc:creator>
    </item>
    <item>
      <title>Principal nested spheres for high-dimensional data</title>
      <link>https://arxiv.org/abs/2511.08398</link>
      <description>arXiv:2511.08398v1 Announce Type: new 
Abstract: The method of Principal Nested Spheres (PNS) is a non-linear dimension reduction technique for spherical data. The method is a backwards fitting procedure, starting with fitting a high-dimensional sphere and then successively reducing dimension at each stage. After reviewing the PNS method in detail, we introduce some new methods for model selection at each stage between great and small subspheres, based on the Kolmogorov-Smirnov test, a variance test and a likelihood ratio test. The current PNS fitting method is slow for high-dimensional spherical data, and so we introduce a fast PNS method which involves an initial principal components analysis decomposition to select a basis for lower dimensional PNS. A new visual method called the PNS biplot is introduced for examining the effects of the original variables on the PNS, and this involves procedures for back-fitting from the PNS scores back to the original variables. The methodology is illustrated with two high-dimensional datasets from cancer research: Melanoma proteomics data with 500 variables and 205 patients, and a Pan Cancer dataset with 12,478 genes and 300 patients. In both applications the PNS biplot is used to select variables for effective classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08398v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mymuna Monem, Ian L. Dryden, Florence George</dc:creator>
    </item>
    <item>
      <title>Multi-level Latent Variable Models for Coheritability Analysis in Electronic Health Records</title>
      <link>https://arxiv.org/abs/2511.08532</link>
      <description>arXiv:2511.08532v1 Announce Type: new 
Abstract: Electronic health records (EHRs) linked with familial relationship data offer a unique opportunity to investigate the genetic architecture of complex phenotypes at scale. However, existing heritability and coheritability estimation methods often fail to account for the intricacies of familial correlation structures, heterogeneity across phenotype types, and computational scalability. We propose a robust and flexible statistical framework for jointly estimating heritability and genetic correlation among continuous and binary phenotypes in EHR-based family studies. Our approach builds on multi-level latent variable models to decompose phenotypic covariance into interpretable genetic and environmental components, incorporating both within- and between-family variations. We derive iteration algorithms based on generalized equation estimations (GEE) for estimation. Simulation studies under various parameter configurations demonstrate that our estimators are consistent and yield valid inference across a range of realistic settings. Applying our methods to real-world EHR data from a large, urban health system, we identify significant genetic correlations between mental health conditions and endocrine/metabolic phenotypes, supporting hypotheses of shared etiology. This work provides a scalable and rigorous framework for coheritability analysis in high-dimensional EHR data and facilitates the identification of shared genetic influences in complex disease networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08532v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinjun Zhao, Nicholas Tatonetti, Yuanjia Wang</dc:creator>
    </item>
    <item>
      <title>Reluctant Transfer Learning in Penalized Regressions for Individualized Treatment Rules under Effect Heterogeneity</title>
      <link>https://arxiv.org/abs/2511.08559</link>
      <description>arXiv:2511.08559v1 Announce Type: new 
Abstract: Estimating individualized treatment rules (ITRs) is fundamental to precision medicine, where the goal is to tailor treatment decisions to individual patient characteristics. While numerous methods have been developed for ITR estimation, there is limited research on model updating that accounts for shifted treatment-covariate relationships in the ITR setting. In real-world practice, models trained on source data must be updated for new (target) datasets that exhibit shifts in treatment effects. To address this challenge, we propose a Reluctant Transfer Learning (RTL) framework that enables efficient model adaptation by selectively transferring essential model components (e.g., regression coefficients) from source to target data, without requiring access to individual-level source data. Leveraging the principle of reluctant modeling, the RTL approach incorporates model adjustments only when they improve performance on the target dataset, thereby controlling complexity and enhancing generalizability. Our method supports multi-armed treatment settings, performs variable selection for interpretability, and provides theoretical guarantees for the value convergence. Through simulation studies and an application to a real data example from the Best Apnea Interventions for Research (BestAIR) trial, we demonstrate that RTL outperforms existing alternatives. The proposed framework offers an efficient, practically feasible approach to adaptive treatment decision-making under evolving treatment effect conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08559v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eun Jeong Oh, Min Qian</dc:creator>
    </item>
    <item>
      <title>Two Datasets Are Better Than One: Method of Double Moments for 3-D Reconstruction in Cryo-EM</title>
      <link>https://arxiv.org/abs/2511.07438</link>
      <description>arXiv:2511.07438v1 Announce Type: cross 
Abstract: Cryo-electron microscopy (cryo-EM) is a powerful imaging technique for reconstructing three-dimensional molecular structures from noisy tomographic projection images of randomly oriented particles. We introduce a new data fusion framework, termed the method of double moments (MoDM), which reconstructs molecular structures from two instances of the second-order moment of projection images obtained under distinct orientation distributions--one uniform, the other non-uniform and unknown. We prove that these moments generically uniquely determine the underlying structure, up to a global rotation and reflection, and we develop a convex-relaxation-based algorithm that achieves accurate recovery using only second-order statistics. Our results demonstrate the advantage of collecting and modeling multiple datasets under different experimental conditions, illustrating that leveraging dataset diversity can substantially enhance reconstruction quality in computational imaging tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07438v1</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joe Kileel, Oscar Mickelin, Amit Singer, Sheng Xu</dc:creator>
    </item>
    <item>
      <title>Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs</title>
      <link>https://arxiv.org/abs/2511.07484</link>
      <description>arXiv:2511.07484v1 Announce Type: cross 
Abstract: This study presents a novel framework for counterfactual user behavior forecasting that combines structural causal models with transformer-based generative artificial intelligence. To model fictitious situations, the method creates causal graphs that map the connections between user interactions, adoption metrics, and product features. The framework generates realistic behavioral trajectories under counterfactual conditions by using generative models that are conditioned on causal variables. Tested on datasets from web interactions, mobile applications, and e-commerce, the methodology outperforms conventional forecasting and uplift modeling techniques. Product teams can effectively simulate and assess possible interventions prior to deployment thanks to the framework improved interpretability through causal path visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07484v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.17762/ijisae.v12i21s.7628</arxiv:DOI>
      <dc:creator>Dharmateja Priyadarshi Uddandarao, Ravi Kiran Vadlamani</dc:creator>
    </item>
    <item>
      <title>Confidence Intervals for Linear Models with Arbitrary Noise Contamination</title>
      <link>https://arxiv.org/abs/2511.07605</link>
      <description>arXiv:2511.07605v1 Announce Type: cross 
Abstract: We study confidence interval construction for linear regression under Huber's contamination model, where an unknown fraction of noise variables is arbitrarily corrupted. While robust point estimation in this setting is well understood, statistical inference remains challenging, especially because the contamination proportion is not identifiable from the data. We develop a new algorithm that constructs confidence intervals for individual regression coefficients without any prior knowledge of the contamination level. Our method is based on a Z-estimation framework using a smooth estimating function. The method directly quantifies the uncertainty of the estimating equation after a preprocessing step that decorrelates covariates associated with the nuisance parameters. We show that the resulting confidence interval has valid coverage uniformly over all contamination distributions and attains an optimal length of order $O(1/\sqrt{n(1-\epsilon)^2})$, matching the rate achievable when the contamination proportion $\epsilon$ is known. This result stands in sharp contrast to the adaptation cost of robust interval estimation observed in the simpler Gaussian location model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07605v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dong Xie, Chao Gao, John Lafferty</dc:creator>
    </item>
    <item>
      <title>PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure</title>
      <link>https://arxiv.org/abs/2511.07997</link>
      <description>arXiv:2511.07997v1 Announce Type: cross 
Abstract: We revisit the problem of generating synthetic data under differential privacy. To address the core limitations of marginal-based methods, we propose the Private Adaptive Generative Adversarial Network with Bayes Network Structure (PrAda-GAN), which integrates the strengths of both GAN-based and marginal-based approaches. Our method adopts a sequential generator architecture to capture complex dependencies among variables, while adaptively regularizing the learned structure to promote sparsity in the underlying Bayes network. Theoretically, we establish diminishing bounds on the parameter distance, variable selection error, and Wasserstein distance. Our analysis shows that leveraging dependency sparsity leads to significant improvements in convergence rates. Empirically, experiments on both synthetic and real-world datasets demonstrate that PrAda-GAN outperforms existing tabular data synthesis methods in terms of the privacy-utility trade-off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07997v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Jia, Yuheng Ma, Yang Li, Feifei Wang</dc:creator>
    </item>
    <item>
      <title>Parameter Estimation and Seasonal Modification of the Fractional Poisson Process with Application to Vorticity Extremes over the North Atlantic</title>
      <link>https://arxiv.org/abs/2511.08081</link>
      <description>arXiv:2511.08081v1 Announce Type: cross 
Abstract: The fractional Poisson process (FPP) generalizes the standard Poisson process by replacing exponentially distributed return times with Mittag-Leffler distributed ones with an extra tail parameter, allowing for greater flexibility. The FPP has been applied in various fields, such as modeling occurrences of extratropical cyclones in meteorology and solar flares in physics. We propose a new estimation method for the parameters of the FPP, based on minimizing the distance between the empirical and the theoretical distribution at selected quantiles. We conduct an extensive simulation study to evaluate the advantages and limitations of the new estimation method and to compare it with several competing estimators, some of which have not yet been examined in the Mittag-Leffler setting. To enhance the applicability of the FPP in real-world scenarios, particularly in meteorology, we propose a method for incorporating seasonality into the FPP through distance-based weighting. We then analyze the return times of relative vorticity extremes in the North Atlantic-European region using our seasonal modeling approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08081v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Merle Mendel, Roland Fried</dc:creator>
    </item>
    <item>
      <title>Semi-Supervised Treatment Effect Estimation with Unlabeled Covariates via Generalized Riesz Regression</title>
      <link>https://arxiv.org/abs/2511.08303</link>
      <description>arXiv:2511.08303v1 Announce Type: cross 
Abstract: This study investigates treatment effect estimation in the semi-supervised setting, where we can use not only the standard triple of covariates, treatment indicator, and outcome, but also unlabeled auxiliary covariates. For this problem, we develop efficiency bounds and efficient estimators whose asymptotic variance aligns with the efficiency bound. In the analysis, we introduce two different data-generating processes: the one-sample setting and the two-sample setting. The one-sample setting considers the case where we can observe treatment indicators and outcomes for a part of the dataset, which is also called the censoring setting. In contrast, the two-sample setting considers two independent datasets with labeled and unlabeled data, which is also called the case-control setting or the stratified setting. In both settings, we find that by incorporating auxiliary covariates, we can lower the efficiency bound and obtain an estimator with an asymptotic variance smaller than that without such auxiliary covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08303v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Neumann-series corrections for regression adjustment in randomized experiments</title>
      <link>https://arxiv.org/abs/2511.08539</link>
      <description>arXiv:2511.08539v1 Announce Type: cross 
Abstract: We study average treatment effect (ATE) estimation under complete randomization with many covariates in a design-based, finite-population framework. In randomized experiments, regression adjustment can improve precision of estimators using covariates, without requiring a correctly specified outcome model. However, existing design-based analyses establish asymptotic normality only up to $p = o(n^{1/2})$, extendable to $p = o(n^{2/3})$ with a single de-biasing. We introduce a novel theoretical perspective on the asymptotic properties of regression adjustment through a Neumann-series decomposition, yielding a systematic higher-degree corrections and a refined analysis of regression adjustment. Specifically, for ordinary least squares regression adjustment, the Neumann expansion sharpens analysis of the remainder term, relative to the residual difference-in-means. Under mild leverage regularity, we show that the degree-$d$ Neumann-corrected estimator is asymptotically normal whenever $p^{ d+3}(\log p)^{ d+1}=o(n^{ d+2})$, strictly enlarging the admissible growth of $p$. The analysis is purely randomization-based and does not impose any parametric outcome models or super-population assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08539v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dogyoon Song</dc:creator>
    </item>
    <item>
      <title>Statistical Inferences and Predictions for Areal Data and Spatial Data Fusion with Hausdorff--Gaussian Processes</title>
      <link>https://arxiv.org/abs/2208.07900</link>
      <description>arXiv:2208.07900v4 Announce Type: replace 
Abstract: Accurate modeling of spatial dependence is pivotal in analyzing spatial data, influencing parameter estimation and predictions. The spatial structure of the data significantly impacts valid statistical inference. Existing models for areal data often rely on adjacency matrices, struggling to differentiate between polygons of varying sizes and shapes. Conversely, data fusion models rely on computationally intensive numerical integrals, presenting challenges for moderately large datasets. In response to these issues, we propose the Hausdorff-Gaussian process (HGP), a versatile model utilizing the Hausdorff distance to capture spatial dependence in both point and areal data. Integration into generalized linear mixed-effects models enhances its applicability, particularly in addressing data fusion challenges. We validate our approach through a comprehensive simulation study and application to two real-world scenarios: one involving areal data and another demonstrating its effectiveness in data fusion. The results suggest that the HGP is competitive with specialized models regarding goodness-of-fit and prediction performances. In summary, the HGP offers a flexible and robust solution for modeling spatial data of various types and shapes, with potential applications spanning fields such as public health and climate science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07900v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas da Cunha Godoy, Marcos Oliveira Prates, Jun Yan</dc:creator>
    </item>
    <item>
      <title>Identifying treatment response subgroups in observational time-to-event data</title>
      <link>https://arxiv.org/abs/2408.03463</link>
      <description>arXiv:2408.03463v5 Announce Type: replace 
Abstract: Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for treatment effect estimation primarily rely on Randomised Controlled Trials (RCTs), which tend to feature more homogeneous patient groups, making them less relevant for uncovering subgroups in the population encountered in real-world clinical practice. Subgroup analyses established for RCTs suffer from significant statistical biases when applied to observational studies, which benefit from larger and more representative populations. Our work introduces a novel, outcome-guided, subgroup analysis strategy for identifying subgroups of treatment response in both RCTs and observational studies alike. It hence positions itself in-between individualised and average treatment effect estimation to uncover patient subgroups with distinct treatment responses, critical for actionable insights that may influence treatment guidelines. In experiments, our approach significantly outperforms the current state-of-the-art method for subgroup analysis in both randomised and observational treatment regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03463v5</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Jeanselme, Chang Ho Yoon, Fabian Falck, Brian Tom, Jessica Barrett</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of product feature allocation models</title>
      <link>https://arxiv.org/abs/2408.15806</link>
      <description>arXiv:2408.15806v2 Announce Type: replace 
Abstract: Feature allocation models are an extension of Bayesian nonparametric clustering models, where individuals can share multiple features. We study a broad class of models whose probability distribution has a product form, which includes the popular Indian buffet process. This class plays a prominent role among existing priors, and it shares structural characteristics with Gibbs-type priors in the species sampling framework. We develop a general theory for the entire class, obtaining closed form expressions for the predictive structure and the posterior law of the underlying stochastic process. Additionally, we describe the distribution for the number of features and the number of hitherto unseen features in a future sample, leading to the $\alpha$-diversity for feature models. We also examine notable novel examples, such as mixtures of Indian buffet processes and beta Bernoulli models, where the latter entails a finite random number of features. This methodology finds significant applications in ecology, allowing the estimation of species richness for incidence data, as we demonstrate by analyzing plant diversity in Danish forests and trees in Barro Colorado Island.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15806v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkaf058</arxiv:DOI>
      <dc:creator>Lorenzo Ghilotti, Federico Camerlenghi, Tommaso Rigon</dc:creator>
    </item>
    <item>
      <title>Regression Discontinuity Designs Under Interference</title>
      <link>https://arxiv.org/abs/2410.02727</link>
      <description>arXiv:2410.02727v4 Announce Type: replace 
Abstract: We extend the continuity-based framework to Regression Discontinuity Designs (RDDs) to identify and estimate causal effects under interference when units are connected through a network. Assignment to an "effective treatment," combining the individual treatment and a summary of neighbors' treatments, is determined by the unit's score and those of interfering units, yielding a multiscore RDD with complex, multidimensional boundaries. We characterize these boundaries and derive assumptions to identify boundary causal effects. We develop a distance-based nonparametric estimator and establish its asymptotic properties under restrictions on the network degree distribution. We show that while direct effects converge at the standard rate, the rate for indirect effects depends on the number of scores fixed at the cutoff. Finally, we propose a variance estimator accounting for network correlation and apply our method to PROGRESA data to estimate the direct and indirect effects of cash transfers on school attendance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02727v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Dal Torrione, Tiziano Arduini, Laura Forastiere</dc:creator>
    </item>
    <item>
      <title>Partial identification of principal causal effects under violations of principal ignorability</title>
      <link>https://arxiv.org/abs/2412.06628</link>
      <description>arXiv:2412.06628v3 Announce Type: replace 
Abstract: Principal stratification is a general framework for studying causal mechanisms involving post-treatment variables. When estimating principal causal effects, the principal ignorability assumption is commonly invoked, which we study in detail in this manuscript. Our first key contribution is studying a commonly used strategy of using parametric models to jointly model the outcome and principal strata without requiring the principal ignorability assumption. We show that even if the joint distribution of principal strata is known, this strategy necessarily leads to only partial identification of causal effects, even under very simple and correctly specified outcome models. While principal ignorability leads to point identification in this setting, we discuss alternative, weaker assumptions and show how they can lead to informative partial identification regions. An additional contribution is that we provide theoretical support to strategies used in the literature for identifying association parameters that govern the joint distribution of principal strata. We prove that this is possible, but only if the principal ignorability assumption is violated. Additionally, due to partial identifiability of causal effects even when these association parameters are known, we show that these association parameters are only identifiable under strong parametric constraints. Lastly, we extend these results to more flexible semiparametric and nonparametric Bayesian models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06628v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minxuan Wu, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Constructing Evidence-Based Tailoring Variables for Adaptive Interventions</title>
      <link>https://arxiv.org/abs/2506.03054</link>
      <description>arXiv:2506.03054v2 Announce Type: replace 
Abstract: Background: Adaptive interventions provide a guide for how ongoing information about individuals should be used to decide whether and how to modify type, amount, delivery modality or timing of treatment, to improve intervention effectiveness while reducing cost and burden. The variables that inform treatment modification decisions are called tailoring variables. Specifying a tailoring variable for an intervention requires describing what should be measured, when to measure it, when this measure should be used to make decisions, and what cutoffs should be used in making decisions. These questions are causal and prescriptive (what to do, when), not merely predictive, raising important tradeoffs between specificity versus sensitivity, and between waiting for sufficient information versus intervening quickly. Purpose: There is little specific guidance in the literature on how to empirically choose tailoring variables, including cutoffs, measurement times, and decision times. Methods: We review possible approaches for comparing potential tailoring variables and propose a framework for systematically developing tailoring variables. Results: Although secondary observational data can be used to select tailoring variables, additional assumptions are needed. A specifically designed experiment for optimization (an optimization randomized clinical trial), e.g., a multi-arm randomized trial, sequential multiple assignment randomized trial, factorial experiment, or hybrid design, may provide a more direct way to answer these questions. Conclusions: Using randomization directly to inform tailoring variables provides the most direct causal evidence, but requires more effort and resources than secondary data analysis. More research on how best to design tailoring variables for effective, scalable interventions is needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03054v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John J. Dziak, Inbal Nahum-Shani</dc:creator>
    </item>
    <item>
      <title>Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences</title>
      <link>https://arxiv.org/abs/2507.08629</link>
      <description>arXiv:2507.08629v2 Announce Type: replace 
Abstract: This article is motivated by challenges in conducting Bayesian inferences on unknown discrete distributions, with a particular focus on count data. To avoid the computational disadvantages of traditional mixture models, we develop a novel Bayesian predictive approach. In particular, our Metropolis-adjusted Dirichlet (MAD) sequence model characterizes the predictive measure as a mixture of a base measure and Metropolis-Hastings kernels centered on previous data points. The resulting MAD sequence is asymptotically exchangeable and the posterior on the data generator takes the form of a martingale posterior. This structure leads to straightforward algorithms for inference on count distributions, with easy extensions to multivariate, regression, and binary data cases. We obtain a useful asymptotic Gaussian approximation and illustrate the methodology on a variety of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08629v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Agnoletto, Tommaso Rigon, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Inference with General Missingness Patterns and Machine Learning Imputation</title>
      <link>https://arxiv.org/abs/2508.15162</link>
      <description>arXiv:2508.15162v3 Announce Type: replace 
Abstract: Pre-trained machine learning (ML) predictions have been increasingly used to complement incomplete data to enable downstream scientific inquiries, but their naive integration risks biased inferences. Recently, multiple methods have been developed to provide valid inference with ML imputations regardless of prediction quality and to enhance efficiency relative to complete-case analyses. However, existing approaches are often limited to missing outcomes under a missing-completely-at-random (MCAR) assumption, failing to handle general missingness patterns (missing in both the outcome and exposures) under the more realistic missing-at-random (MAR) assumption. This paper develops a novel method that delivers a valid statistical inference framework for general Z-estimation problems using ML imputations under the MAR assumption and for general missingness patterns. The core technical idea is to stratify observations by distinct missingness patterns and construct an estimator by appropriately weighting and aggregating pattern-specific information through a masking-and-imputation procedure on the complete cases. We provide theoretical guarantees of asymptotic normality of the proposed estimator and efficiency dominance over weighted complete-case analyses. Practically, the method affords simple implementations by leveraging existing weighted complete-case analysis software. Extensive simulations are carried out to validate theoretical results. A real data example is provided to further illustrate the practical utility of the proposed method. The paper concludes with a brief discussion on practical implications, limitations, and potential future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15162v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingran Chen, Tyler McCormick, Bhramar Mukherjee, Zhenke Wu</dc:creator>
    </item>
    <item>
      <title>Cox Regression on the Plane</title>
      <link>https://arxiv.org/abs/2509.12473</link>
      <description>arXiv:2509.12473v2 Announce Type: replace 
Abstract: The Cox proportional hazards model is the most widely used regression model in univariate survival analysis. Extensions of the Cox model to bivariate survival data, however, remain scarce. We propose two novel extensions based on a Lehmann-type representation of the survival function. The first, the simple Lehmann model, is a direct extension that retains a straightforward structure. The second, the generalized Lehmann model, allows greater flexibility by incorporating three distinct regression parameters and includes the simple Lehmann model as a special case. For both models, we derive the corresponding regression formulations for the three bivariate hazard functions and discuss their interpretation and model validity. To estimate the regression parameters, we adopt a bivariate pseudo-observations approach. For the generalized Lehmann model, we extend this approach to accommodate a trivariate structure: trivariate pseudo-observations and a trivariate link function. We then propose a two-step estimation procedure, where the marginal regression parameters are estimated in the first step, and the remaining parameters are estimated in the second step. Finally, we establish the consistency and asymptotic normality of the resulting estimators. We illustrate the approach using data from the Global Retinoblastoma Outcome Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12473v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yael Travis-Lumer, Micha Mandel, Ido Didi Fabian, Rebecca A. Betensky, Malka Gorfine</dc:creator>
    </item>
    <item>
      <title>Core-elements Subsampling for Alternating Least Squares</title>
      <link>https://arxiv.org/abs/2509.18024</link>
      <description>arXiv:2509.18024v2 Announce Type: replace 
Abstract: In this paper, we propose a novel element-wise subset selection method for the alternating least squares (ALS) algorithm, focusing on low-rank matrix factorization involving matrices with missing values, as commonly encountered in recommender systems. While ALS is widely used for providing personalized recommendations based on user-item interaction data, its high computational cost, stemming from repeated regression operations, poses significant challenges for large-scale datasets. To enhance the efficiency of ALS, we propose a core-elements subsampling method that selects a representative subset of data and leverages sparse matrix operations to approximate ALS estimations efficiently. We establish theoretical guarantees for the approximation and convergence of the proposed approach, showing that it achieves similar accuracy with significantly reduced computational time compared to full-data ALS. Extensive simulations and real-world applications demonstrate the effectiveness of our method in various scenarios, emphasizing its potential in large-scale recommendation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18024v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dunyao Xue, Mengyu Li, Cheng Meng, Jingyi Zhang</dc:creator>
    </item>
    <item>
      <title>Cluster Catch Digraphs with the Nearest Neighbor Distance</title>
      <link>https://arxiv.org/abs/2501.06268</link>
      <description>arXiv:2501.06268v2 Announce Type: replace-cross 
Abstract: We introduce a new method for clustering based on Cluster Catch Digraphs (CCDs). The new method addresses the limitations of RK-CCDs by employing a new variant of spatial randomness test that employs the nearest neighbor distance (NND) instead of the Ripley's K function used by RK-CCDs. We conduct a comprehensive Monte Carlo analysis to assess the performance of our method, considering factors such as dimensionality, data set size, number of clusters, cluster volumes, and inter-cluster distance. Our method is particularly effective for high-dimensional data sets, comparable to or outperforming KS-CCDs and RK-CCDs that rely on a KS-type statistic or the Ripley's K function. We also evaluate our methods using real and complex data sets, comparing them to well-known clustering methods. Again, our methods exhibit competitive performance, producing high-quality clusters with desirable properties.
  Keywords: Graph-based clustering, Cluster catch digraphs, High-dimensional data, The nearest neighbor distance, Spatial randomness test</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06268v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Shi, Elvan Ceyhan, Nedret Billor</dc:creator>
    </item>
    <item>
      <title>Potato Potahto in the FAO-GAEZ Productivity Measures? Nonclassical Measurement Error with Multiple Proxies</title>
      <link>https://arxiv.org/abs/2502.12141</link>
      <description>arXiv:2502.12141v5 Announce Type: replace-cross 
Abstract: The FAO-GAEZ productivity data are widely used in Economics. However, the empirical literature rarely discusses measurement error. We use two proxies to derive novel analytical bounds around the effect of agricultural productivity in a setting with nonclassical measurement error. These bounds rely on assumptions that are weaker than the ones imposed in empirical studies and exhaust the information contained in the first two moments of the data. We reevaluate three influential studies, documenting that measurement error matters and that the impact of agricultural productivity may be smaller than previously reported. Our methodology has broad applications in empirical research involving mismeasured variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12141v5</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rafael Araujo, Vitor Possebom</dc:creator>
    </item>
    <item>
      <title>GMM with Many Weak Moment Conditions and Nuisance Parameters: General Theory and Applications to Causal Inference</title>
      <link>https://arxiv.org/abs/2505.07295</link>
      <description>arXiv:2505.07295v3 Announce Type: replace-cross 
Abstract: Weak identification arises in many statistical problems when key variables exhibit weak correlations-for example, when instrumental variables correlate weakly with treatment, or when proxy variables correlate weakly with unmeasured confounders. Under weak identification, standard estimation methods such as the generalized method of moments (GMM) can produce substantial bias, both in finite samples and asymptotically. This challenge is compounded in modern applications that require estimating many nuisance parameters. This paper develops a framework for estimation and inference of a finite-dimensional target parameter in general moment models with the number of weak moment conditions and nuisance parameters growing with sample size. We analyze a general two-step debiasing estimator that accommodates flexible, possibly nonparametric first-step estimation of nuisance parameters, in which Neyman orthogonality plays a more critical role in obtaining debiased inference than in conventional settings with strong identification. Under a many-weak-moment asymptotic regime, we establish the estimator's consistency and asymptotic normality. We provide high-level conditions for the general setting and demonstrate their application to two important special cases: inference with weak instruments and inference with weak proxies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07295v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Kwun Chuen Gary Chan, Ting Ye</dc:creator>
    </item>
  </channel>
</rss>
