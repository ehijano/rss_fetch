<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 May 2025 01:55:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Performance of prior event rate ratio method in the presence of differential mortality or dropout</title>
      <link>https://arxiv.org/abs/2505.20757</link>
      <description>arXiv:2505.20757v1 Announce Type: new 
Abstract: Purpose: Prior event rate ratio (PERR) method was proposed to control for unmeasured confounding in real-world evaluation of effectiveness and safety of pharmaceutical products. A widely cited simulation study showed that PERR estimate of treatment effect was biased in the presence of differential morality/dropout. However, the study only considered one specific PERR estimator of treatment effect and one specific scenario of differential mortality/dropout. To enhance understanding of the method, we replicated and extended the simulation to consider an alternative PERR estimator and multiple scenarios. Methods: Simulation studies were performed with varying rate of mortality/dropout, including the same scenario in the previous study in which mortality/dropout was simultaneously influenced by treatment, confounder and prior event and scenarios that differed in the determinants of mortality/dropout. In addition to the PERR estimator used in the previous study (PERR_Prev) that involved data form both completers and non-completers, we also evaluated an alternative PERR estimator (PERR_Comp) that used data only from completers. Results: The bias of PERR_Prev in the previously considered mortality/dropout scenario was replicated. Bias of PERR_Comp was only about one-third in magnitude as compared to that of PERR_Prev in this scenario. Furthermore, PERR_Prev did but PERR_Comp did not give biased estimates of treatment effect in scenarios that mortality/dropout was influenced by treatment or confounder but not prior event. Conclusions: The PERR is better seen as a methodological framework. Its performance depends on the specifications within the framework. PERR_Comp provides unbiased estimates unless mortality/dropout is affected by prior event.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20757v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Bun Cheung, Xiangmei Ma</dc:creator>
    </item>
    <item>
      <title>A New View to Mission Profiles</title>
      <link>https://arxiv.org/abs/2505.20792</link>
      <description>arXiv:2505.20792v1 Announce Type: new 
Abstract: Mission profiles cover the conditions that a component, e.g., an electronic component of a vehicle, is exposed to during its lifecycle. Currently, these profiles typically provide descriptive summaries, such as histograms, of single stress parameters like temperature, humidity, or voltage. This is highly aggregated information. New requirements for electric and autonomous driving cars require much more information how applications are used. In this work, we present a new approach for mission profiles which contains detailed usage information. We suggest a functional description over time, which allows joint modeling of various characteristics such as temperature, humidity, and voltage. The entire lifecycle history is covered, and the method can control the temporal resolution, i.e., the level of details of a mission profile. As a result, more accurate mission profiles can be generated, user quantiles can be derived, and usage outliers can be identified. This model establishes a framework to exchange usage data between suppliers, original equipment manufacturers (OEMs), and end customers while data integrity and protection are assured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20792v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/RAMS48127.2025.10935003</arxiv:DOI>
      <arxiv:journal_reference>Lewitschnig, H. J., Mayrhofer, M., &amp; Filzmoser, P. (2025, January). A New View to Mission Profiles. In 2025 Annual Reliability and Maintainability Symposium (RAMS) (pp. 1-6). IEEE</arxiv:journal_reference>
      <dc:creator>Horst Lewitschnig, Marcus Mayrhofer, Peter Filzmoser</dc:creator>
    </item>
    <item>
      <title>Named Entity Swapping for Metadata Anonymization in a Text Corpus</title>
      <link>https://arxiv.org/abs/2505.21128</link>
      <description>arXiv:2505.21128v1 Announce Type: new 
Abstract: This work introduces an anonymization scheme for a corpus of texts to safeguard metadata from disclosure. It specifically aims to prevent large language models from identifying metadata associated with texts, thereby avoiding their influence on query responses. The core mechanism is called named entity swapping, a technique inspired by data swapping in statistical disclosure control. Our method randomly selects pairs of semantically similar substrings from different texts based on the similarity of their embedding vectors and interchanges some named entities between them. This prevents certain combinations of named entities from being uniquely associated with the metadata of individual texts. Our approach offers two key advantages. First, it enables users to determine the optimal level of anonymization that balances data utility and data risk through a calibration of several key decision variables. Second, it leverages text embeddings both to compute swapping weights and to assess data utility, enabling a high degree of flexibility and customization in the overall workflow. The effectiveness of the proposed method is demonstrated with an application that prevents the disclosure of company names in a cross-sectional dataset of earnings call transcripts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21128v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Greve, Lukas Sablica</dc:creator>
    </item>
    <item>
      <title>An Integrated Time-Varying Ornstein-Uhlenbeck Process for Jointly Modeling Individual and Population-Level Dynamics of Golden Eagles</title>
      <link>https://arxiv.org/abs/2505.21453</link>
      <description>arXiv:2505.21453v1 Announce Type: new 
Abstract: With technological advancements, the quantity and quality of animal movement data has increased greatly. Currently, there is no existing movement model that can be used to describe full year of migratory species data that leverages both individual movement data and species distribution data. Herein we propose a full-year stochastic differential equation model for jointly modeling both individual movement data and species distribution data. We show that this joint model, under certain assumptions, results in efficient computation of the spatio-temporal dynamics of the entire population, and thus provides straightforward inference on the species distribution data. We illustrate this model with 215 bird-years of golden eagle movement in western North America and data from eBird for the species distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21453v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael L. Shull, Ephraim M. Hanks, James C. Russell, Robert K. Murphy, Frances E. Buderman</dc:creator>
    </item>
    <item>
      <title>Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series</title>
      <link>https://arxiv.org/abs/2505.20697</link>
      <description>arXiv:2505.20697v1 Announce Type: cross 
Abstract: The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20697v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary C. Brown, David Carlson</dc:creator>
    </item>
    <item>
      <title>Causal inference with dyadic data in randomized experiments</title>
      <link>https://arxiv.org/abs/2505.20780</link>
      <description>arXiv:2505.20780v1 Announce Type: cross 
Abstract: Estimating the treatment effect within network structures is a key focus in online controlled experiments, particularly for social media platforms. We investigate a scenario where the unit-level outcome of interest comprises a series of dyadic outcomes, which is pervasive in many social network sources, spanning from microscale point-to-point messaging to macroscale international trades. Dyadic outcomes are of particular interest in online controlled experiments, capturing pairwise interactions as basic units for analysis. The dyadic nature of the data induces interference, as treatment assigned to one unit may affect outcomes involving connected pairs. We propose a novel design-based causal inference framework for dyadic outcomes in randomized experiments, develop estimators of the global average causal effect, and establish their asymptotic properties under different randomization designs. We prove the central limit theorem for the estimators and propose variance estimators to quantify the estimation uncertainty. The advantages of integrating dyadic data in randomized experiments are manifested in a variety of numerical experiments, especially in correcting interference bias. We implement our proposed method in a large-scale experiment on WeChat Channels, assessing the impact of a recommendation algorithm on users' interaction metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20780v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilin Li, Lu Deng, Yong Wang, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Larger cities, more commuters, more crime? The role of inter-city commuting in the scaling of urban crime</title>
      <link>https://arxiv.org/abs/2505.20822</link>
      <description>arXiv:2505.20822v1 Announce Type: cross 
Abstract: Cities attract a daily influx of non-resident commuters, reflecting their role in wider urban networks -- not as isolated places. However, it remains unclear how this inter-connectivity shapes the way crime scales with population, given that larger cities tend to receive more commuters and experience more crime. Here, we investigate how inter-city commuting relates to the population--crime relationship. We find that larger cities receive proportionately more commuters, which in turn is associated with higher crime levels. Specifically, each 1% increase in inbound commuters corresponds to a 0.32% rise in theft and 0.20% rise in burglary, holding population constant. We show that models incorporating both population and commuter inflows better explain crime variation than population-only models. These findings underscore the importance of considering how cities are connected -- not just their population size -- in disentangling the population--crime relationship.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20822v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Puttock, Umberto Barros, Diego Pinheiro, Marcos Oliveira</dc:creator>
    </item>
    <item>
      <title>A longitudinal Bayesian framework for estimating causal dose-response relationships</title>
      <link>https://arxiv.org/abs/2505.20893</link>
      <description>arXiv:2505.20893v1 Announce Type: cross 
Abstract: Existing causal methods for time-varying exposure and time-varying confounding focus on estimating the average causal effect of a time-varying binary treatment on an end-of-study outcome. Methods for estimating the effects of a time-varying continuous exposure at any dose level on the outcome are limited. We introduce a scalable, non-parametric Bayesian framework for estimating longitudinal causal dose-response relationships with repeated measures. We incorporate the generalized propensity score either as a covariate or through inverse-probability weighting, formulating two Bayesian dose-response estimators. The proposed approach embeds a double non-parametric generalized Bayesian bootstrap which enables a flexible Dirichlet process specification within a generalized estimating equations structure, capturing temporal correlation while making minimal assumptions about the functional form of the continuous exposure. We applied our proposed approach to a motivating study of monthly metro-ridership data and COVID-19 case counts from major international cities, identifying causal relationships and the dynamic dose-response patterns between higher ridership and increased case counts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20893v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Luo, Kuan Liu, Ramandeep Singh, Daniel J. Graham</dc:creator>
    </item>
    <item>
      <title>Two-step dimensionality reduction of human mobility data: From potential landscapes to spatiotemporal insights</title>
      <link>https://arxiv.org/abs/2505.20929</link>
      <description>arXiv:2505.20929v1 Announce Type: cross 
Abstract: Understanding the spatiotemporal patterns of human mobility is crucial for addressing societal challenges, such as epidemic control and urban transportation optimization. Despite advancements in data collection, the complexity and scale of mobility data continue to pose significant analytical challenges. Existing methods often result in losing location-specific details and fail to fully capture the intricacies of human movement. This study proposes a two-step dimensionality reduction framework to overcome existing limitations. First, we construct a potential landscape of human flow from origin-destination (OD) matrices using combinatorial Hodge theory, preserving essential spatial and structural information while enabling an intuitive visualization of flow patterns. Second, we apply principal component analysis (PCA) to the potential landscape, systematically identifying major spatiotemporal patterns. By implementing this two-step reduction method, we reveal significant shifts during a pandemic, characterized by an overall declines in mobility and stark contrasts between weekdays and holidays. These findings underscore the effectiveness of our framework in uncovering complex mobility patterns and provide valuable insights into urban planning and public health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20929v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunhan Du, Takaaki Aoki, Naoya Fujiwara</dc:creator>
    </item>
    <item>
      <title>Parameter Effects in ReCom Ensembles</title>
      <link>https://arxiv.org/abs/2505.21326</link>
      <description>arXiv:2505.21326v1 Announce Type: cross 
Abstract: Ensemble analysis has become central to redistricting litigation, but parameter effects remain understudied. We analyze 315 ReCom ensembles across the three legislative chambers in 7 states, systematically varying the population tolerance, county preservation strength, and algorithm variant. To validate convergence, we introduce new methods to approximate effective sample size and measure redundancy. We find that varying the population tolerance has a negligible effect on all scores, whereas the algorithm and county-preservation parameters can significantly affect some metrics, inconsistently in some cases but surprisingly consistently in others across jurisdictions. These findings suggest parameter choices should be thoughtfully considered when using ReCom ensembles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21326v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristopher Tapp, Todd Proebsting, Alec Ramsay</dc:creator>
    </item>
    <item>
      <title>Simultaneous Estimation of Piecewise Constant Coefficients in Elliptic PDEs via Bayesian Level-Set Methods</title>
      <link>https://arxiv.org/abs/2404.11552</link>
      <description>arXiv:2404.11552v2 Announce Type: replace 
Abstract: In this article, we propose a non-parametric Bayesian level-set method for simultaneous reconstruction of two different piecewise constant coefficients in an elliptic partial differential equation. We show that the Bayesian formulation of the corresponding inverse problem is well-posed and that the posterior measure as a solution to the inverse problem satisfies a Lipschitz estimate with respect to the measured data in terms of Hellinger distance. We reduce the problem to a shape-reconstruction problem and use level-set priors for the parameters of interest. We demonstrate the efficacy of the proposed method using numerical simulations by performing reconstructions of the original phantom using two reconstruction methods. Posing the inverse problem in a Bayesian paradigm allows us to do statistical inference for the parameters of interest, whereby we are able to quantify the uncertainty in the reconstructions for both methods. This illustrates a key advantage of Bayesian methods over traditional algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11552v2</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anuj Abhishek, Thilo Strauss, Taufiquar Khan</dc:creator>
    </item>
    <item>
      <title>New financial ratios based on the compositional data methodology</title>
      <link>https://arxiv.org/abs/2210.11138</link>
      <description>arXiv:2210.11138v2 Announce Type: replace-cross 
Abstract: Due to their type of mathematical construction, the use of standard financial ratios in studies analysing the financial health of a group of firms leads to a series of statistical problems that can invalidate the results obtained. These problems are originated by the asymmetry of financial ratios. The present article justifies the use of a new methodology using compositional data (CoDa) to analyse the financial statements of a sector, improving analyses using conventional ratios since the new methodology enables statistical techniques to be applied without encountering any serious drawbacks such as skewness and outliers, and without the results depending on the arbitrary choice as to which of the accounting figures is the numerator of the ratio and which is the denominator. An example with data of the wine sector is provided. The results show that when using CoDa, outliers and skewness are much reduced and results are invariant to numerator and denominator permutation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.11138v2</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.3390/axioms11120694</arxiv:DOI>
      <arxiv:journal_reference>Axioms, 11, 12 (2022), 694</arxiv:journal_reference>
      <dc:creator>Salvador Linares-Mustar\'os (University of Girona), Maria \`Angels Farreras-Noguer (University of Girona), N\'uria Arimany-Serrat (University of Vic-Central University of Catalonia), Germ\`a Coenders (University of Girona)</dc:creator>
    </item>
    <item>
      <title>Skew-elliptical copula based mixed models for non-Gaussian longitudinal data with application to an HIV-AIDS study</title>
      <link>https://arxiv.org/abs/2402.00651</link>
      <description>arXiv:2402.00651v4 Announce Type: replace-cross 
Abstract: This study was sparked by an extensive longitudinal dataset focusing on HIV CD4 T$^+$ cell counts from Livingstone district, Zambia. Analysis of the corresponding histogram plots reveals an absence of symmetry in the marginal distributions, while pairwise scatter plots uncover non-elliptical dependence patterns. Traditional linear mixed models designed for longitudinal data fail to capture these complexities adequately. Therefore, it appears prudent to explore a broader framework for modeling such data. In this article, we delve into generalized linear mixed models (GLMM) for the marginals (e.g., the Gamma mixed model), and we address the temporal dependency of repeated measurements by utilizing copulas associated with skew-elliptical distributions (such as the skew-normal/skew-$t$). Our proposed class of copula-based mixed models simultaneously accommodates asymmetry, between-subject variability, and non-standard temporal dependence, thus offering extensions to the standard linear mixed model based on multivariate normality. We estimate the model parameters using the IFM (inference function of margins) method and outline the process of obtaining standard errors for parameter estimates. Through extensive simulation studies covering skewed and symmetric marginal distributions and various copula choices, we assess the finite sample performance of our approach. Finally, we apply these models to the HIV dataset and present our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00651v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Skill vs. Chance Quantification for Popular Card &amp; Board Games</title>
      <link>https://arxiv.org/abs/2410.14363</link>
      <description>arXiv:2410.14363v2 Announce Type: replace-cross 
Abstract: This paper presents a data-driven statistical framework to quantify the role of skill in games, addressing the long-standing question of whether success in a game is predominantly driven by skill or chance. We analyze player level data from four popular games Chess, Rummy, Ludo, and Teen Patti, using empirical win statistics across varying levels of experience. By modeling win rate as a function of experience through a regression framework and employing empirical bootstrap resampling, we estimate the degree to which outcomes improve with repeated play. To summarize these dynamics, we propose a flexible skill score that emphasizes learning over initial performance, aligning with practical and regulatory interpretations of skill. Our results reveal a clear ranking, with Chess showing the highest skill component and Teen Patti the lowest, while Rummy and Ludo fall in between. The proposed framework is transparent, reproducible, and adaptable to other game formats and outcome metrics, offering potential applications in legal classification, game design, and player performance analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14363v2</guid>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tathagata Banerjee, Anushka De, Subhamoy Maitra, Diganta Mukherjee</dc:creator>
    </item>
    <item>
      <title>Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework</title>
      <link>https://arxiv.org/abs/2502.14479</link>
      <description>arXiv:2502.14479v2 Announce Type: replace-cross 
Abstract: The lifetime behaviour of loans is notoriously difficult to model, which can compromise a bank's financial reserves against future losses, if modelled poorly. Therefore, we present a data-driven comparative study amongst three techniques in modelling a series of default risk estimates over the lifetime of each loan, i.e., its term-structure. The behaviour of loans can be described using a nonstationary and time-dependent semi-Markov model, though we model its elements using a multistate regression-based approach. As such, the transition probabilities are explicitly modelled as a function of a rich set of input variables, including macroeconomic and loan-level inputs. Our modelling techniques are deliberately chosen in ascending order of complexity: 1) a Markov chain; 2) beta regression; and 3) multinomial logistic regression. Using residential mortgage data, our results show that each successive model outperforms the previous, likely as a result of greater sophistication. This finding required devising a novel suite of simple model diagnostics, which can itself be reused in assessing sampling representativeness and the performance of other modelling techniques. These contributions surely advance the current practice within banking when conducting multistate modelling. Consequently, we believe that the estimation of loss reserves will be more timeous and accurate under IFRS 9.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14479v2</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster, Roland Breedt</dc:creator>
    </item>
  </channel>
</rss>
