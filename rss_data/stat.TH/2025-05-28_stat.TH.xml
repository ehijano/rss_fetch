<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 May 2025 01:59:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Strong Low Degree Hardness for the Number Partitioning Problem</title>
      <link>https://arxiv.org/abs/2505.20607</link>
      <description>arXiv:2505.20607v1 Announce Type: new 
Abstract: In the number partitioning problem (NPP) one aims to partition a given set of $N$ real numbers into two subsets with approximately equal sum. The NPP is a well-studied optimization problem and is famous for possessing a statistical-to-computational gap: when the $N$ numbers to be partitioned are i.i.d. standard gaussian, the optimal discrepancy is $2^{-\Theta(N)}$ with high probability, but the best known polynomial-time algorithms only find solutions with a discrepancy of $2^{-\Theta(\log^2 N)}$. This gap is a common feature in optimization problems over random combinatorial structures, and indicates the need for a study that goes beyond worst-case analysis.
  We provide evidence of a nearly tight algorithmic barrier for the number partitioning problem. Namely we consider the family of low coordinate degree algorithms (with randomized rounding into the Boolean cube), and show that degree $D$ algorithms fail to solve the NPP to accuracy beyond $2^{-\widetilde O(D)}$. According to the low degree heuristic, this suggests that simple brute-force search algorithms are nearly unimprovable, given any allotted runtime between polynomial and exponential in $N$. Our proof combines the isolation of solutions in the landscape with a conditional form of the overlap gap property: given a good solution to an NPP instance, slightly noising the NPP instance typically leaves no good solutions near the original one. In fact our analysis applies whenever the $N$ numbers to be partitioned are independent with uniformly bounded density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20607v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rushil Mallarapu, Mark Sellke</dc:creator>
    </item>
    <item>
      <title>Eigenstructure inference for high-dimensional covariance with generalized shrinkage inverse-Wishart prior</title>
      <link>https://arxiv.org/abs/2505.20668</link>
      <description>arXiv:2505.20668v1 Announce Type: new 
Abstract: In multivariate statistics, estimating the covariance matrix is essential for understanding the interdependence among variables. In high-dimensional settings, where the number of covariates increases with the sample size, it is well known that the eigenstructure of the sample covariance matrix is inconsistent. The inverse-Wishart prior, a standard choice for covariance estimation in Bayesian inference, also suffers from posterior inconsistency. To address the issue of eigenvalue dispersion in high-dimensional settings, the shrinkage inverse-Wishart (SIW) prior has recently been proposed. Despite its conceptual appeal and empirical success, the asymptotic justification for the SIW prior has remained limited. In this paper, we propose a generalized shrinkage inverse-Wishart (gSIW) prior for high-dimensional covariance modeling. By extending the SIW framework, the gSIW prior accommodates a broader class of prior distributions and facilitates the derivation of theoretical properties under specific parameter choices. In particular, under the spiked covariance assumption, we establish the asymptotic behavior of the posterior distribution for both eigenvalues and eigenvectors by directly evaluating the posterior expectations for two sets of parameter choices. This direct evaluation provides insights into the large-sample behavior of the posterior that cannot be obtained through general posterior asymptotic theorems. Finally, simulation studies illustrate that the proposed prior provides accurate estimation of the eigenstructure, particularly for spiked eigenvalues, achieving narrower credible intervals and higher coverage probabilities compared to existing methods. For spiked eigenvectors, the performance is generally comparable to that of competing approaches, including the sample covariance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20668v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seongmin Kim, Kwangmin Lee, Sewon Park, Jaeyong Lee</dc:creator>
    </item>
    <item>
      <title>Almost Unbiased Liu Type Estimator in Bell Regression Model: Theory, Simulation and Application</title>
      <link>https://arxiv.org/abs/2505.20946</link>
      <description>arXiv:2505.20946v1 Announce Type: new 
Abstract: In this research, we propose a novel regression estimator as an alternative to the Liu estimator for addressing multicollinearity in the Bell regression model, referred to as the almost unbiased Liu estimator. Moreover, the theoretical characteristics of the proposed estimator are analyzed, along with several theorems that specify the conditions under which the almost unbiased Liu estimator outperforms its alternatives. A comprehensive simulation study is conducted to demonstrate the superiority of the almost unbiased Liu estimator and to compare it against the Bell Liu estimator and the maximum likelihood estimator. The practical applicability and advantage of the proposed regression estimator are illustrated through a real-world dataset. The results from both the simulation study and the real-world data application indicate that the new almost unbiased Liu regression estimator outperforms its counterparts based on the mean square error criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20946v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caner Tan{\i}\c{s}, Yasin Asar</dc:creator>
    </item>
    <item>
      <title>Linearity-Inducing Priors for Poisson Parameter Estimation Under $L^{1}$ Loss</title>
      <link>https://arxiv.org/abs/2505.21102</link>
      <description>arXiv:2505.21102v1 Announce Type: new 
Abstract: We study prior distributions for Poisson parameter estimation under $L^1$ loss. Specifically, we construct a new family of prior distributions whose optimal Bayesian estimators (the conditional medians) can be any prescribed increasing function that satisfies certain regularity conditions. In the case of affine estimators, this family is distinct from the usual conjugate priors, which are gamma distributions. Our prior distributions are constructed through a limiting process that matches certain moment conditions. These results provide the first explicit description of a family of distributions, beyond the conjugate priors, that satisfy the affine conditional median property; and more broadly for the Poisson noise model they can give any arbitrarily prescribed conditional median.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21102v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leighton P. Barnes, Alex Dytso, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>Kernel Quantile Embeddings and Associated Probability Metrics</title>
      <link>https://arxiv.org/abs/2505.20433</link>
      <description>arXiv:2505.20433v1 Announce Type: cross 
Abstract: Embedding probability distributions into reproducing kernel Hilbert spaces (RKHS) has enabled powerful nonparametric methods such as the maximum mean discrepancy (MMD), a statistical distance with strong theoretical and computational properties. At its core, the MMD relies on kernel mean embeddings to represent distributions as mean functions in RKHS. However, it remains unclear if the mean function is the only meaningful RKHS representation. Inspired by generalised quantiles, we introduce the notion of kernel quantile embeddings (KQEs). We then use KQEs to construct a family of distances that: (i) are probability metrics under weaker kernel conditions than MMD; (ii) recover a kernelised form of the sliced Wasserstein distance; and (iii) can be efficiently estimated with near-linear cost. Through hypothesis testing, we show that these distances offer a competitive alternative to MMD and its fast approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20433v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masha Naslidnyk, Siu Lun Chau, Fran\c{c}ois-Xavier Briol, Krikamol Muandet</dc:creator>
    </item>
    <item>
      <title>Learning with Expected Signatures: Theory and Applications</title>
      <link>https://arxiv.org/abs/2505.20465</link>
      <description>arXiv:2505.20465v1 Announce Type: cross 
Abstract: The expected signature maps a collection of data streams to a lower dimensional representation, with a remarkable property: the resulting feature tensor can fully characterize the data generating distribution. This "model-free" embedding has been successfully leveraged to build multiple domain-agnostic machine learning (ML) algorithms for time series and sequential data. The convergence results proved in this paper bridge the gap between the expected signature's empirical discrete-time estimator and its theoretical continuous-time value, allowing for a more complete probabilistic interpretation of expected signature-based ML methods. Moreover, when the data generating process is a martingale, we suggest a simple modification of the expected signature estimator with significantly lower mean squared error and empirically demonstrate how it can be effectively applied to improve predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20465v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Lucchese, Mikko S. Pakkanen, Almut E. D. Veraart</dc:creator>
    </item>
    <item>
      <title>Moment Expansions of the Energy Distance</title>
      <link>https://arxiv.org/abs/2505.20647</link>
      <description>arXiv:2505.20647v1 Announce Type: cross 
Abstract: The energy distance is used to test distributional equality, and as a loss function in machine learning. While $D^2(X, Y)=0$ only when $X\sim Y$, the sensitivity to different moments is of practical importance. This work considers $D^2(X, Y)$ in the case where the distributions are close. In this regime, $D^2(X, Y)$ is more sensitive to differences in the means $\bar{X}-\bar{Y}$, than differences in the covariances $\Delta$. This is due to the structure of the energy distance and is independent of dimension. The sensitivity to on versus off diagonal components of $\Delta$ is examined when $X$ and $Y$ are close to isotropic. Here a dimension dependent averaging occurs and, in many cases, off diagonal correlations contribute significantly less. Numerical results verify these relationships hold even when distributional assumptions are not strictly met.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20647v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Langmore</dc:creator>
    </item>
    <item>
      <title>Hybrid Bayesian Estimation in the additive hazards model</title>
      <link>https://arxiv.org/abs/2505.20681</link>
      <description>arXiv:2505.20681v1 Announce Type: cross 
Abstract: Hereby we propose a Bayesian method of estimation for the semiparametric Additive Hazards Model (AHM) from Survival Analysis under right-censoring. With this aim, we review the AHM revisiting the likelihood function, so as to comment on the challenges posed by Bayesian estimation from the full likelihood. Through an algorithmic reformulation of that likelihood, we present an alternative method based on a hybrid Bayesian treatment that exploits Lin and Ying (1994) estimating equation approach and which chooses tractable priors for the parameters. We obtain the estimators from the posterior distributions in closed form, we perform a small simulation experiment, and lastly, we illustrate our method with the classical Nickels Miners dataset and a brief simulation experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20681v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Enrique Ernesto \'Alvarez, Maximiliano Luis Riddick</dc:creator>
    </item>
    <item>
      <title>Berk-Nash Rationalizability</title>
      <link>https://arxiv.org/abs/2505.20708</link>
      <description>arXiv:2505.20708v1 Announce Type: cross 
Abstract: We introduce Berk--Nash rationalizability, a new solution concept for misspecified learning environments. It parallels rationalizability in games and captures all actions that are optimal given beliefs formed using the model that best fits the data in the agent's misspecified model class. Our main result shows that, with probability one, every \emph{limit action} -- any action played or approached infinitely often -- is Berk--Nash rationalizable. This holds regardless of whether behavior converges. We apply the concept to known examples and identify classes of environments where it is easily characterized. The framework provides a general tool for bounding long-run behavior without assuming convergence to a Berk--Nash equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20708v1</guid>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ignacio Esponda, Demian Pouzo</dc:creator>
    </item>
    <item>
      <title>Scalable and adaptive prediction bands with kernel sum-of-squares</title>
      <link>https://arxiv.org/abs/2505.21039</link>
      <description>arXiv:2505.21039v1 Announce Type: cross 
Abstract: Conformal Prediction (CP) is a popular framework for constructing prediction bands with valid coverage in finite samples, while being free of any distributional assumption. A well-known limitation of conformal prediction is the lack of adaptivity, although several works introduced practically efficient alternate procedures. In this work, we build upon recent ideas that rely on recasting the CP problem as a statistical learning problem, directly targeting coverage and adaptivity. This statistical learning problem is based on reproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS) methods. First, we extend previous results with a general representer theorem and exhibit the dual formulation of the learning problem. Crucially, such dual formulation can be solved efficiently by accelerated gradient methods with several hundreds or thousands of samples, unlike previous strategies based on off-the-shelf semidefinite programming algorithms. Second, we introduce a new hyperparameter tuning strategy tailored specifically to target adaptivity through bounds on test-conditional coverage. This strategy, based on the Hilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune kernel lengthscales in our framework, but has broader applicability since it could be used in any CP algorithm where the score function is learned. Finally, extensive experiments are conducted to show how our method compares to related work. All figures can be reproduced with the accompanying code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21039v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Allain (ENSAI, CREST), S\'ebastien da Veiga (ENSAI, CREST), Brian Staber</dc:creator>
    </item>
    <item>
      <title>Sample complexity of optimal transport barycenters with discrete support</title>
      <link>https://arxiv.org/abs/2505.21274</link>
      <description>arXiv:2505.21274v1 Announce Type: cross 
Abstract: Computational implementation of optimal transport barycenters for a set of target probability measures requires a form of approximation, a widespread solution being empirical approximation of measures. We provide an $O(\sqrt{N/n})$ statistical generalization bounds for the empirical sparse optimal transport barycenters problem, where $N$ is the maximum cardinality of the barycenter (sparse support) and $n$ is the sample size of the target measures empirical approximation. Our analysis includes various optimal transport divergences including Wasserstein, Sinkhorn and Sliced-Wasserstein. We discuss the application of our result to specific settings including K-means, constrained K-means, free and fixed support Wasserstein barycenters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21274v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'eo Portales, Edouard Pauwels, Elsa Cazelles</dc:creator>
    </item>
    <item>
      <title>A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective</title>
      <link>https://arxiv.org/abs/2505.21400</link>
      <description>arXiv:2505.21400v1 Announce Type: cross 
Abstract: Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models enable parallel token sampling, leading to faster generation and eliminating left-to-right generation constraints. Despite their empirical success, the theoretical understanding of diffusion model approaches remains underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. In particular, we establish matching upper and lower bounds, up to some constant factor, to demonstrate the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21400v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gen Li, Changxiao Cai</dc:creator>
    </item>
    <item>
      <title>Error analysis for a statistical finite element method</title>
      <link>https://arxiv.org/abs/2201.07543</link>
      <description>arXiv:2201.07543v3 Announce Type: replace 
Abstract: The recently proposed statistical finite element (statFEM) approach synthesises measurement data with finite element models and allows for making predictions about the unknown true system response. We provide a probabilistic error analysis for a prototypical statFEM setup based on a Gaussian process prior under the assumption that the noisy measurement data are generated by a deterministic true system response function that satisfies a second-order elliptic partial differential equation for an unknown true source term. In certain cases, properties such as the smoothness of the source term may be misspecified by the Gaussian process model. The error estimates we derive are for the expectation with respect to the measurement noise of the $L^2$-norm of the difference between the true system response and the mean of the statFEM posterior. The estimates imply polynomial rates of convergence in the numbers of measurement points and finite element basis functions and depend on the Sobolev smoothness of the true source term and the Gaussian process model. A numerical example for Poisson's equation is used to illustrate these theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.07543v3</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toni Karvonen, Fehmi Cirak, Mark Girolami</dc:creator>
    </item>
    <item>
      <title>Clustering risk in Non-parametric Hidden Markov and I.I.D. Models</title>
      <link>https://arxiv.org/abs/2309.12238</link>
      <description>arXiv:2309.12238v4 Announce Type: replace 
Abstract: We conduct an in-depth analysis of the Bayes risk of clustering in the context of Hidden Markov and i.i.d. models. In both settings, we identify the situations where this risk is comparable to the Bayes risk of classification and those where its minimizer, the Bayes clusterer, can be derived from the Bayes classifier. While we demonstrate that clustering based on the Bayes classifier does not always match the optimal Bayes clusterer, we show that this difference is primarily theoretical and that the Bayes classifier remains nearly optimal for clustering. A key quantity emerges, capturing the fundamental difficulty of both classification and clustering tasks. Furthermore, by leveraging the identifiability of HMMs, we establish bounds on the clustering excess risk of a plug-in Bayes classifier in the general nonparametric setting, offering theoretical justification for its widespread use in practice. Simulations further illustrate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12238v4</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elisabeth Gassiat, Ibrahim Kaddouri, Zacharie Naulet</dc:creator>
    </item>
    <item>
      <title>Subscedastic weighted least squares estimates</title>
      <link>https://arxiv.org/abs/2404.00753</link>
      <description>arXiv:2404.00753v4 Announce Type: replace 
Abstract: In the heteroscedastic linear model, the weighted least squares (WLS) estimate of the model coefficients is more efficient than the ordinary least squares (OLS) esti- mate. However, the practical application of WLS is challenging because it requires knowledge of the error variances. Feasible weighted least squares (FLS) estimates, which use approximations of the variances when they are unknown, may either be more or less efficient than the OLS estimate depending on the quality of the approx- imation. A direct comparison between FLS and OLS has significant implications for the application of regression analysis in varied fields, yet such a comparison remains an unresolved challenge. In this study, we address this challenge by identifying the conditions under which FLS estimates using fixed weights demonstrate greater effi- ciency than the OLS estimate. These conditions provide guidance for the design of feasible estimates using random weights. They also shed light on how certain robust regression estimates behave with respect to the linear model with normal errors of unequal variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00753v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Bryan, Haibo Zhou, Didong Li</dc:creator>
    </item>
    <item>
      <title>Nonparametric Diffusivity Estimation for the Stochastic Heat Equation from Noisy Observations</title>
      <link>https://arxiv.org/abs/2410.00677</link>
      <description>arXiv:2410.00677v2 Announce Type: replace 
Abstract: We estimate nonparametrically the spatially varying diffusivity of a stochastic heat equation from observations perturbed by additional noise. To that end, we employ a two-step localization procedure, more precisely, we combine local state estimates into a locally linear regression approach. Our analysis relies on quantitative Trotter--Kato type approximation results for the heat semigroup that are of independent interest. The presence of observational noise leads to non-standard scaling behaviour of the model. Numerical simulations illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00677v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregor Pasemann, Markus Rei{\ss}</dc:creator>
    </item>
    <item>
      <title>Non-identifiability distinguishes Neural Networks among Parametric Models</title>
      <link>https://arxiv.org/abs/2504.18017</link>
      <description>arXiv:2504.18017v2 Announce Type: replace 
Abstract: One of the enduring problems surrounding neural networks is to identify the factors that differentiate them from traditional statistical models. We prove a pair of results which distinguish feedforward neural networks among parametric models at the population level, for regression tasks. Firstly, we prove that for any pair of random variables $(X,Y)$, neural networks always learn a nontrivial relationship between $X$ and $Y$, if one exists. Secondly, we prove that for reasonable smooth parametric models, under local and global identifiability conditions, there exists a nontrivial $(X,Y)$ pair for which the parametric model learns the constant predictor $\mathbb{E}[Y]$. Together, our results suggest that a lack of identifiability distinguishes neural networks among the class of smooth parametric models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18017v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Timothy Sudijono</dc:creator>
    </item>
    <item>
      <title>Minimax Rates of Estimation for Optimal Transport Map between Infinite-Dimensional Spaces</title>
      <link>https://arxiv.org/abs/2505.13570</link>
      <description>arXiv:2505.13570v2 Announce Type: replace 
Abstract: We investigate the estimation of an optimal transport map between probability measures on an infinite-dimensional space and reveal its minimax optimal rate. Optimal transport theory defines distances within a space of probability measures, utilizing an optimal transport map as its key component. Estimating the optimal transport map from samples finds several applications, such as simulating dynamics between probability measures and functional data analysis. However, some transport maps on infinite-dimensional spaces require exponential-order data for estimation, which undermines their applicability. In this paper, we investigate the estimation of an optimal transport map between infinite-dimensional spaces, focusing on optimal transport maps characterized by the notion of $\gamma$-smoothness. Consequently, we show that the order of the minimax risk is polynomial rate in the sample size even in the infinite-dimensional setup. We also develop an estimator whose estimation error matches the minimax optimal rate. With these results, we obtain a class of reasonably estimable optimal transport maps on infinite-dimensional spaces and a method for their estimation. Our experiments validate the theory and practical utility of our approach with application to functional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13570v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donlapark Ponnoprat, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Testing High-Dimensional Mediation Effect with Arbitrary Exposure-Mediator Coefficients</title>
      <link>https://arxiv.org/abs/2310.05539</link>
      <description>arXiv:2310.05539v3 Announce Type: replace-cross 
Abstract: In response to the unique challenge created by high-dimensional mediators in mediation analysis, this paper presents a novel procedure for testing the nullity of the mediation effect in the presence of high-dimensional mediators. The procedure incorporates two distinct features. Firstly, the test remains valid under all cases of the composite null hypothesis, including the challenging scenario where both exposure-mediator and mediator-outcome coefficients are zero. Secondly, it does not impose structural assumptions on the exposure-mediator coefficients, thereby allowing for an arbitrarily strong exposure-mediator relationship. To the best of our knowledge, the proposed test is the first of its kind to provably possess these two features in high-dimensional mediation analysis. The validity and consistency of the proposed test are established, and its numerical performance is showcased through simulation studies. The application of the proposed test is demonstrated by examining the mediation effect of DNA methylation between smoking status and lung cancer development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05539v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Lin, Zijian Guo, Baoluo Sun, Zhenhua Lin</dc:creator>
    </item>
    <item>
      <title>Simultaneous Estimation of Piecewise Constant Coefficients in Elliptic PDEs via Bayesian Level-Set Methods</title>
      <link>https://arxiv.org/abs/2404.11552</link>
      <description>arXiv:2404.11552v2 Announce Type: replace-cross 
Abstract: In this article, we propose a non-parametric Bayesian level-set method for simultaneous reconstruction of two different piecewise constant coefficients in an elliptic partial differential equation. We show that the Bayesian formulation of the corresponding inverse problem is well-posed and that the posterior measure as a solution to the inverse problem satisfies a Lipschitz estimate with respect to the measured data in terms of Hellinger distance. We reduce the problem to a shape-reconstruction problem and use level-set priors for the parameters of interest. We demonstrate the efficacy of the proposed method using numerical simulations by performing reconstructions of the original phantom using two reconstruction methods. Posing the inverse problem in a Bayesian paradigm allows us to do statistical inference for the parameters of interest, whereby we are able to quantify the uncertainty in the reconstructions for both methods. This illustrates a key advantage of Bayesian methods over traditional algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11552v2</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anuj Abhishek, Thilo Strauss, Taufiquar Khan</dc:creator>
    </item>
    <item>
      <title>Evaluating Quantumness, Efficiency and Cost of Quantum Random Number Generators via Photon Statistics</title>
      <link>https://arxiv.org/abs/2405.14085</link>
      <description>arXiv:2405.14085v5 Announce Type: replace-cross 
Abstract: This work presents two significant contributions from the perspectives of quantum random number generator (QRNG) manufacturers and users. For manufacturers, the conventional method of assessing the quantumness of single-photon-based QRNGs through mean and variance comparisons of photon counts is statistically unreliable due to finite sample sizes. Given the sub-Poissonian statistics of single photons, confirming the underlying distribution is crucial for validating a QRNG's quantumness. We propose a more efficient two-fold statistical approach to ensure the quantumness of optical sources with the desired confidence level. Additionally, we demonstrate that the output of QRNGs from exponential and uniform distributions exhibit similarity under device noise, deriving corresponding photon statistics and conditions for $\epsilon$-randomness.
  From the user's perspective, the fundamental parameters of a QRNG are quantumness, efficiency (random entropy and random number generation rate), and cost. Our analysis reveals that these parameters depend on three factors, namely, expected photon count per unit time, external reference cycle duration, and detection efficiency. A lower expected photon count enhances entropy but increases cost and decreases the generation rate. A shorter external reference cycle boosts entropy but must exceed a minimum threshold to minimize timing errors, with minor impacts on cost and rate. Lower detection efficiency enhances entropy and lowers cost but reduces the generation rate. Finally, to validate our results, we perform statistical tests like NIST, Dieharder, AIS-31, ENT etc. over the data simulated with different values of the above parameters. Our findings can empower manufacturers to customize QRNGs to meet user needs effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14085v5</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Goutam Paul, Nirupam Basak, Soumya Das</dc:creator>
    </item>
    <item>
      <title>Simple Relative Deviation Bounds for Covariance and Gram Matrices</title>
      <link>https://arxiv.org/abs/2410.05754</link>
      <description>arXiv:2410.05754v3 Announce Type: replace-cross 
Abstract: We provide non-asymptotic, relative deviation bounds for the eigenvalues of empirical covariance and Gram matrices in general settings. Unlike typical uniform bounds, which may fail to capture the behavior of smaller eigenvalues, our results provide sharper control across the spectrum. Our analysis is based on a general-purpose theorem that allows one to convert existing uniform bounds into relative ones. The theorems and techniques emphasize simplicity and should be applicable across various settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05754v3</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Barzilai, Ohad Shamir</dc:creator>
    </item>
    <item>
      <title>Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the role of model complexity</title>
      <link>https://arxiv.org/abs/2411.02184</link>
      <description>arXiv:2411.02184v2 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability and safety of machine learning systems. In recent years, it has received increasing attention, particularly through post-hoc detection and training-based methods. In this paper, we focus on post-hoc OOD detection, which enables identifying OOD samples without altering the model's training procedure or objective. Our primary goal is to investigate the relationship between model capacity and its OOD detection performance. Specifically, we aim to answer the following question: Does the Double Descent phenomenon manifest in post-hoc OOD detection? This question is crucial, as it can reveal whether overparameterization, which is already known to benefit generalization, can also enhance OOD detection. Despite the growing interest in these topics by the classic supervised machine learning community, this intersection remains unexplored for OOD detection. We empirically demonstrate that the Double Descent effect does indeed appear in post-hoc OOD detection. Furthermore, we provide theoretical insights to explain why this phenomenon emerges in such setting. Finally, we show that the overparameterized regime does not yield superior results consistently, and we propose a method to identify the optimal regime for OOD detection based on our observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02184v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mou\"in Ben Ammar, David Brellmann, Arturo Mendoza, Antoine Manzanera, Gianni Franchi</dc:creator>
    </item>
    <item>
      <title>Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices</title>
      <link>https://arxiv.org/abs/2502.01512</link>
      <description>arXiv:2502.01512v3 Announce Type: replace-cross 
Abstract: Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks. A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution. In this work, we tackle those issue by focusing on the manifold of symmetric positive definite (SPD) matrices, a key focus in information geometry. We introduce a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model. Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis. This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01512v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibault de Surrel, Fabien Lotte, Sylvain Chevallier, Florian Yger</dc:creator>
    </item>
    <item>
      <title>Hallucinations are inevitable but can be made statistically negligible. The "innate" inevitability of hallucinations cannot explain practical LLM issues</title>
      <link>https://arxiv.org/abs/2502.12187</link>
      <description>arXiv:2502.12187v2 Announce Type: replace-cross 
Abstract: Hallucinations, a phenomenon where a language model (LM) generates nonfactual content, pose a significant challenge to the practical deployment of LMs. While many empirical methods have been proposed to mitigate hallucinations, recent studies established a computability-theoretic result showing that any LM will inevitably generate hallucinations on an infinite set of inputs, regardless of the quality and quantity of training datasets and the choice of the language model architecture and training and inference algorithms. Although the computability-theoretic result may seem pessimistic, its significance in practical viewpoints has remained unclear. This paper claims that those "innate" inevitability results from computability theory and diagonal argument, in principle, cannot explain practical issues of LLMs. We demonstrate this claim by presenting a positive theoretical result from a probabilistic perspective. Specifically, we prove that hallucinations can be made statistically negligible, provided that the quality and quantity of the training data are sufficient. Interestingly, our positive result coexists with the computability-theoretic result, implying that while hallucinations on an infinite set of inputs cannot be entirely eliminated, their probability can always be reduced by improving algorithms and training data. By evaluating the two seemingly contradictory results through the lens of information theory, we argue that our probability-theoretic positive result better reflects practical considerations than the computability-theoretic negative result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12187v2</guid>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atsushi Suzuki, Yulan He, Feng Tian, Zhongyuan Wang</dc:creator>
    </item>
  </channel>
</rss>
