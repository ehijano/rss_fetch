<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Oct 2025 01:45:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Shifting norms in scholarly publications: trends in readability, objectivity, authorship, and AI use</title>
      <link>https://arxiv.org/abs/2510.21725</link>
      <description>arXiv:2510.21725v1 Announce Type: new 
Abstract: Academic and scientific publishing practices have changed significantly in recent years. This paper presents an analysis of 17 million research papers published since 2000 to explore changes in authorship and content practices. It shows a clear trend towards more authors, more references and longer abstracts. While increased authorship has been reported elsewhere, the present analysis shows that it is pervasive across many major fields of study. We also identify a decline in author productivity which suggests that `gift' authorship (the inclusion of authors who have not contributed significantly to a work) may be a significant factor. We further report on a tendency for authors to use more hyperbole, perhaps exaggerating their contributions to compete for the limited attention of reviewers, and often at the expense of readability. This has been especially acute since 2023, as AI has been increasingly used across many fields of study, but particularly in fields such as Computer Science, Engineering and Business. In summary, many of these changes are causes of significant concern. Increased authorship counts and gift authorship have the potential to distort impact metrics such as field-weighted citation impact andh-index, while increased AI usage may compromise readability and objectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21725v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Padraig Cunningham, Padhraic Smyth, Barry Smyth</dc:creator>
    </item>
    <item>
      <title>10 Simple Rules for Improving Your Standardized Fields and Terms</title>
      <link>https://arxiv.org/abs/2510.21825</link>
      <description>arXiv:2510.21825v1 Announce Type: new 
Abstract: Contextual metadata is the unsung hero of research data. When done right, standardized and structured vocabularies make your data findable, shareable, and reusable. When done wrong, they turn a well intended effort into data cleanup and curation nightmares. In this paper we tackle the surprisingly tricky process of vocabulary standardization with a mix of practical advice and grounded examples. Drawing from real-world experience in contextual data harmonization, we highlight common challenges (e.g., semantic noise and concept bombs) and provide actionable strategies to address them. Our rules emphasize alignment with Findability, Accessibility, Interoperability, and Reusability (FAIR) principles while remaining adaptable to evolving user and research needs. Whether you are curating datasets, designing a schema, or contributing to a standards body, these rules aim to help you create metadata that is not only technically sound but also meaningful to users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21825v1</guid>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rhiannon Cameron (Centre for Infectious Disease Genomics and One Health, Faculty of Health Sciences, Simon Fraser University, Burnaby, BC, Canada), Emma Griffiths (Centre for Infectious Disease Genomics and One Health, Faculty of Health Sciences, Simon Fraser University, Burnaby, BC, Canada), Damion Dooley (Centre for Infectious Disease Genomics and One Health, Faculty of Health Sciences, Simon Fraser University, Burnaby, BC, Canada), William Hsiao (Centre for Infectious Disease Genomics and One Health, Faculty of Health Sciences, Simon Fraser University, Burnaby, BC, Canada)</dc:creator>
    </item>
    <item>
      <title>A Quantitative Approach to Estimating Bias, Favouritism and Distortion in Scientific Journalism</title>
      <link>https://arxiv.org/abs/2510.21838</link>
      <description>arXiv:2510.21838v1 Announce Type: new 
Abstract: While traditionally not considered part of the scientific method, science communication is increasingly playing a pivotal role in shaping scientific practice. Researchers are now frequently compelled to publicise their findings in response to institutional impact metrics and competitive grant environments. This shift underscores the growing influence of media narratives on both scientific priorities and public perception. In a current trend of personality-driven reporting, we examine patterns in science communication that may indicate biases of different types, towards topics and researchers. We focused and applied our methodology to a corpus of media coverage from three of the most prominent scientific media outlets: Wired, Quanta, and The New Scientist -- spanning the past 5 to 10 years. By mapping linguistic patterns, citation flows, and topical convergence, our objective was to quantify the dimensions and degree of bias that influence the credibility of scientific journalism. In doing so, we seek to illuminate the systemic features that shape science communication today and to interrogate their broader implications for epistemic integrity and public accountability in science. We present our results with anonymised journalist names but conclude that personality-driven media coverage distorts science and the practice of science flattening rather than expanding scientific coverage perception. Keywords : selective sourcing, bias, scientific journalism, Quanta, Wired, New Scientist, fairness, balance, neutrality, standard practices, distortion, personal promotion, communication, media outlets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21838v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raghavendra Koushik, Hector Zenil</dc:creator>
    </item>
    <item>
      <title>Can Small and Reasoning Large Language Models Score Journal Articles for Research Quality and Do Averaging and Few-shot Help?</title>
      <link>https://arxiv.org/abs/2510.22389</link>
      <description>arXiv:2510.22389v1 Announce Type: new 
Abstract: Assessing published academic journal articles is a common task for evaluations of departments and individuals. Whilst it is sometimes supported by citation data, Large Language Models (LLMs) may give more useful indications of article quality. Evidence of this capability exists for two of the largest LLM families, ChatGPT and Gemini, and the medium sized LLM Gemma3 27b, but it is unclear whether smaller LLMs and reasoning models have similar abilities. This is important because larger models may be slow and impractical in some situations, and reasoning models may perform differently. Four relevant questions are addressed with Gemma3 variants, Llama4 Scout, Qwen3, Magistral Small and DeepSeek R1, on a dataset of 2,780 medical, health and life science papers in 6 fields, with two different gold standards, one novel. The results suggest that smaller (open weights) and reasoning LLMs have similar performance to ChatGPT 4o-mini and Gemini 2.0 Flash, but that 1b parameters may often, and 4b sometimes, be too few. Moreover, averaging scores from multiple identical queries seems to be a universally successful strategy, and few-shot prompts (four examples) tended to help but the evidence was equivocal. Reasoning models did not have a clear advantage. Overall, the results show, for the first time, that smaller LLMs &gt;4b, including reasoning models, have a substantial capability to score journal articles for research quality, especially if score averaging is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22389v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Thelwall, Ehsan Mohammadi</dc:creator>
    </item>
    <item>
      <title>Can ChatGPT be a good follower of academic paradigms? Research quality evaluations in conflicting areas of sociology</title>
      <link>https://arxiv.org/abs/2510.22426</link>
      <description>arXiv:2510.22426v1 Announce Type: new 
Abstract: Purpose: It has become increasingly likely that Large Language Models (LLMs) will be used to score the quality of academic publications to support research assessment goals in the future. This may cause problems for fields with competing paradigms since there is a risk that one may be favoured, causing long term harm to the reputation of the other. Design/methodology/approach: To test whether this is plausible, this article uses 17 ChatGPTs to evaluate up to 100 journal articles from each of eight pairs of competing sociology paradigms (1490 altogether). Each article was assessed by prompting ChatGPT to take one of five roles: paradigm follower, opponent, antagonistic follower, antagonistic opponent, or neutral. Findings: Articles were scored highest by ChatGPT when it followed the aligning paradigm, and lowest when it was told to devalue it and to follow the opposing paradigm. Broadly similar patterns occurred for most of the paradigm pairs. Follower ChatGPTs displayed only a small amount of favouritism compared to neutral ChatGPTs, but articles evaluated by an opposing paradigm ChatGPT had a substantial disadvantage. Research limitations: The data covers a single field and LLM. Practical implications: The results confirm that LLM instructions for research evaluation should be carefully designed to ensure that they are paradigm-neutral to avoid accidentally resolving conflicts between paradigms on a technicality by devaluing one side's contributions. Originality/value: This is the first demonstration that LLMs can be prompted to show a partiality for academic paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22426v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Thelwall, Ralph Schroeder, Meena Dhanda</dc:creator>
    </item>
    <item>
      <title>Fraudulent Publishing in Mathematics: A European Call to Action and How Information Infrastructure Can Help</title>
      <link>https://arxiv.org/abs/2510.22576</link>
      <description>arXiv:2510.22576v1 Announce Type: new 
Abstract: The IMU-ICIAM working group's new report on Fraudulent Publishing in the Mathematical Sciences documents how gaming of bibliometrics, predatory outlets and paper-mill activity are eroding trust in research, mathematics included. This short EMS note brings that analysis home to Europe. We urge readers to recognise the warning signs of fraudulent publishing, to report serious irregularities so that they can be investigated and sanctioned, and to reflect critically on their own editorial and reviewing practices. We then sketch why Europe is well placed to lead a structural response: a decade of policy development on open science; mature infrastructures for data, software and scholarly communication; and new capacity for community-led diamond open access. Finally, we outline developments towards non-print contributions across member countries including the growth of formal proofs (e.g. with Lean and Isabelle) and we highlight the role of zbMATH Open as a European quality signal that can help editors, reviewers and authors steer clear of problematic venues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22576v1</guid>
      <category>cs.DL</category>
      <category>math.HO</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Schubotz, Jan Philip SoloveJ</dc:creator>
    </item>
    <item>
      <title>Web Archives for Verifying Attribution in Twitter Screenshots</title>
      <link>https://arxiv.org/abs/2510.22939</link>
      <description>arXiv:2510.22939v1 Announce Type: new 
Abstract: Screenshots of social media posts are a common approach for information sharing. Unfortunately, before sharing a screenshot, users rarely verify whether the attribution of the post is fake or real. There are numerous legitimate reasons to share screenshots. However, sharing screenshots of social media posts is also a vector for mis-/disinformation spread on social media. We are exploring methods to verify the attribution of a social media post shown in a screenshot, using resources found on the live web and in web archives. We focus on the use of web archives, since the attribution of non-deleted posts can be relatively easily verified using the live web. We show how information from a Twitter screenshot (Twitter handle, timestamp, and tweet text) can be extracted and used for locating potential archived tweets in the Internet Archive's Wayback Machine. We evaluate our method on a dataset of 1,571 single tweet screenshots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22939v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tarannum Zaki, Michael L. Nelson, Michele C. Weigle</dc:creator>
    </item>
    <item>
      <title>Fake scientific journals are here to stay</title>
      <link>https://arxiv.org/abs/2510.23146</link>
      <description>arXiv:2510.23146v1 Announce Type: new 
Abstract: Scientific publishing is facing an alarming proliferation of fraudulent practices that threaten the integrity of research communication. The production and dissemination of fake research have become a profitable business, undermining trust in scientific journals and distorting the evaluation processes that depend on them. This brief piece examines the problem of fake journals through a three-level typology. The first level concerns predatory journals, which prioritise financial gain over scholarly quality by charging authors publication fees while providing superficial or fabricated peer review. The second level analyses hijacked journals, in which counterfeit websites impersonate legitimate titles to deceive authors into submitting and paying for publication. The third level addresses hacked journals, where legitimate platforms are compromised through cyberattacks or internal manipulation, enabling the distortion of review and publication processes. Together, these forms of misconduct expose deep vulnerabilities in the scientific communication ecosystem, exacerbated by the pressure to publish and the marketisation of research outputs. The manuscript concludes that combating these practices requires structural reforms in scientific evaluation and governance. Only by reducing the incentives that sustain the business of fraudulent publishing can the scholarly community restore credibility and ensure that scientific communication fulfils the essential purpose of reliable advancement of knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23146v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enrique Ordu\~na-Malea</dc:creator>
    </item>
    <item>
      <title>A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications</title>
      <link>https://arxiv.org/abs/2510.21762</link>
      <description>arXiv:2510.21762v1 Announce Type: cross 
Abstract: We present a dataset of 833k paragraphs extracted from CC-BY licensed scientific publications, classified into four categories: acknowledgments, data mentions, software/code mentions, and clinical trial mentions. The paragraphs are primarily in English and French, with additional European languages represented. Each paragraph is annotated with language identification (using fastText) and scientific domain (from OpenAlex). This dataset, derived from the French Open Science Monitor corpus and processed using GROBID, enables training of text classification models and development of named entity recognition systems for scientific literature mining. The dataset is publicly available on HuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21762v1</guid>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Jeangirard</dc:creator>
    </item>
    <item>
      <title>The 1979 Iranian Revolution and the Lost Decade of Science: A Counterfactual Scientometric Analysis</title>
      <link>https://arxiv.org/abs/2510.21826</link>
      <description>arXiv:2510.21826v1 Announce Type: cross 
Abstract: This paper presents a comprehensive scientometric analysis of the long-term impact of the 1979 Iranian Revolution on the nation scientific development. Using Scopus-indexed data from 1960 to 2024, we benchmark Iran publication trajectory against a carefully selected peer group representing diverse development models, established scientific leaders, Netherlands, stable regional powers, Israel, and high-growth, Asian Tigers, South Korea, Taiwan, Singapore alongside Greece and China. The analysis reveals a stark divergence, in the late 1970s, Iran scientific output surpassed that of South Korea, China and Taiwan. The revolution, however, precipitated a collapse, followed by a lost decade of stagnation, precisely when its Asian peers began an unprecedented, state driven ascent. We employ counterfactual models based on pre revolutionary growth trends to quantify the resulting knowledge deficit. The findings suggest that, in an alternate, stable timeline, Iran scientific output could have rivaled South Korea today. We further outline a research agenda to analyze normalized impact metrics, such as FWCI, and collaboration patterns, complementing our findings on publication volume. By contextualizing Iran unique trajectory, this study contributes to a broader understanding of the divergent recovery patterns exhibited by national scientific systems following profound political shocks, offering insights into the enduring consequences of historical disruptions on the global scientific landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21826v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.DL</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ehsan Roohi</dc:creator>
    </item>
    <item>
      <title>A Stylometric Application of Large Language Models</title>
      <link>https://arxiv.org/abs/2510.21958</link>
      <description>arXiv:2510.21958v1 Announce Type: cross 
Abstract: We show that large language models (LLMs) can be used to distinguish the writings of different authors. Specifically, an individual GPT-2 model, trained from scratch on the works of one author, will predict held-out text from that author more accurately than held-out text from other authors. We suggest that, in this way, a model trained on one author's works embodies the unique writing style of that author. We first demonstrate our approach on books written by eight different (known) authors. We also use this approach to confirm R. P. Thompson's authorship of the well-studied 15th book of the Oz series, originally attributed to F. L. Baum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21958v1</guid>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harrison F. Stropkay, Jiayi Chen, Mohammad J. Latifi, Daniel N. Rockmore, Jeremy R. Manning</dc:creator>
    </item>
    <item>
      <title>Measuring Research Interest Similarity with Transition Probabilities</title>
      <link>https://arxiv.org/abs/2409.18240</link>
      <description>arXiv:2409.18240v2 Announce Type: replace 
Abstract: We introduce a family of paper and author similarity measures based on the concept that papers are more similar if they are more likely to be retrieved during a literature search following backward and forward citations. Since this browsing process resembles a walk in a citation network, we operationalize the concept using the transition probability (TP) of random walkers. The proposed measures are continuous, symmetric, and can be implemented on any citation network. We conduct validation tests of the TP concept and other extant alternatives to gauge which metric can classify papers and predict future co-authors most consistently across different scales of analysis (co-authorships, journals, and disciplines). Our results show that the proposed basic TP measure outperforms alternative metrics such as personalized PageRank and the Node2vec machine-learning technique in classification tasks at various scales. Additionally, we discuss how publication-level data can be leveraged to approximate the research interest similarity of individual scientists. This paper is accompanied by a Python package that implements all the tested metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18240v2</guid>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/qss.a.13</arxiv:DOI>
      <arxiv:journal_reference>Quantitative Science Studies, 6, 922-939 (2025)</arxiv:journal_reference>
      <dc:creator>Attila Varga, Sadamori Kojaku, Filipi Nascimento Silva</dc:creator>
    </item>
    <item>
      <title>The Role of AI in Facilitating Interdisciplinary Collaboration: Evidence from AlphaFold</title>
      <link>https://arxiv.org/abs/2508.13234</link>
      <description>arXiv:2508.13234v2 Announce Type: replace 
Abstract: The acceleration of artificial intelligence (AI) in science is recognized and many scholars have begun to explore its role in interdisciplinary collaboration. However, the mechanisms and extent of this impact are still unclear. This study, using AlphaFold's impact on structural biologists, examines how AI technologies influence interdisciplinary collaborative patterns. By analyzing 1,247 AlphaFold-related papers and 7,700 authors from Scopus, we employ bibliometric analysis and causal inference to compare interdisciplinary collaboration between AlphaFold adopters and non-adopters. Contrary to the widespread belief that AI facilitates interdisciplinary collaboration, our findings show that AlphaFold increased structural biology-computer science collaborations by just 0.48%, with no measurable effect on other disciplines. Specifically, AI creates interdisciplinary collaboration demands with specific disciplines due to its technical characteristics, but this demand is weakened by technological democratization and other factors. These findings demonstrate that artificial intelligence (AI) alone has limited efficacy in bridging disciplinary divides or fostering meaningful interdisciplinary collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13234v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naixuan Zhao, Chunli Wei, Xinyan Zhang, Jiang Li</dc:creator>
    </item>
  </channel>
</rss>
