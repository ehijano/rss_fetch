<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 04:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Empirical Evidence That There Is No Such Thing As A Validated Prediction Model</title>
      <link>https://arxiv.org/abs/2406.08628</link>
      <description>arXiv:2406.08628v1 Announce Type: new 
Abstract: Background: External validations are essential to assess clinical prediction models (CPMs) before deployment. Apart from model misspecification, differences in patient population and other factors influence a model's AUC (c-statistic). We aimed to quantify variation in AUCs across external validation studies and adjust expectations of a model's performance in a new setting.
  Methods: The Tufts-PACE CPM Registry contains CPMs for cardiovascular disease prognosis. We analyzed the AUCs of 469 CPMs with a total of 1,603 external validations. For each CPM, we performed a random effects meta-analysis to estimate the between-study standard deviation $\tau$ among the AUCs. Since the majority of these meta-analyses has only a handful of validations, this leads to very poor estimates of $\tau$. So, we estimated a log normal distribution of $\tau$ across all CPMs and used this as an empirical prior. We compared this empirical Bayesian approach with frequentist meta-analyses using cross-validation.
  Results: The 469 CPMs had a median of 2 external validations (IQR: [1-3]). The estimated distribution of $\tau$ had a mean of 0.055 and a standard deviation of 0.015. If $\tau$ = 0.05, the 95% prediction interval for the AUC in a new setting is at least +/- 0.1, regardless of the number of validations. Frequentist methods underestimate the uncertainty about the AUC in a new setting. Accounting for $\tau$ in a Bayesian approach achieved near nominal coverage.
  Conclusion: Due to large heterogeneity among the validated AUC values of a CPM, there is great irreducible uncertainty in predicting the AUC in a new setting. This uncertainty is underestimated by existing methods. The proposed empirical Bayes approach addresses this problem which merits wide application in judging the validity of prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08628v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Florian D. van Leeuwen, Ewout W. Steyerberg, David van Klaveren, Ben Wessler, David M. Kent, Erik W. van Zwet</dc:creator>
    </item>
    <item>
      <title>Causal Inference on Missing Exposure via Robust Estimation</title>
      <link>https://arxiv.org/abs/2406.08668</link>
      <description>arXiv:2406.08668v1 Announce Type: new 
Abstract: How to deal with missing data in observational studies is a common concern for causal inference. When the covariates are missing at random (MAR), multiple approaches have been provided to help solve the issue. However, if the exposure is MAR, few approaches are available and careful adjustments on both missingness and confounding issues are required to ensure a consistent estimate of the true causal effect on the response. In this article, a new inverse probability weighting (IPW) estimator based on weighted estimating equations (WEE) is proposed to incorporate weights from both the missingness and propensity score (PS) models, which can reduce the joint effect of extreme weights in finite samples. Additionally, we develop a triple robust (TR) estimator via WEE to further protect against the misspecification of the missingness model. The asymptotic properties of WEE estimators are proved using properties of estimating equations. Based on the simulation studies, WEE methods outperform others including imputation-based approaches in terms of bias and variability. Finally, an application study is conducted to identify the causal effect of the presence of cardiovascular disease on mortality for COVID-19 patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08668v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuliang Shi, Yeying Zhu, Joel A. Dubin</dc:creator>
    </item>
    <item>
      <title>Variational Bayes Inference for Spatial Error Models with Missing Data</title>
      <link>https://arxiv.org/abs/2406.08685</link>
      <description>arXiv:2406.08685v1 Announce Type: new 
Abstract: The spatial error model (SEM) is a type of simultaneous autoregressive (SAR) model for analysing spatially correlated data. Markov chain Monte Carlo (MCMC) is one of the most widely used Bayesian methods for estimating SEM, but it has significant limitations when it comes to handling missing data in the response variable due to its high computational cost. Variational Bayes (VB) approximation offers an alternative solution to this problem. Two VB-based algorithms employing Gaussian variational approximation with factor covariance structure are presented, joint VB (JVB) and hybrid VB (HVB), suitable for both missing at random and not at random inference. When dealing with many missing values, the JVB is inaccurate, and the standard HVB algorithm struggles to achieve accurate inferences. Our modified versions of HVB enable accurate inference within a reasonable computational time, thus improving its performance. The performance of the VB methods is evaluated using simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08685v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjana Wijayawardhana, David Gunawan, Thomas Suesse</dc:creator>
    </item>
    <item>
      <title>Learning Joint and Individual Structure in Network Data with Covariates</title>
      <link>https://arxiv.org/abs/2406.08776</link>
      <description>arXiv:2406.08776v1 Announce Type: new 
Abstract: Datasets consisting of a network and covariates associated with its vertices have become ubiquitous. One problem pertaining to this type of data is to identify information unique to the network, information unique to the vertex covariates and information that is shared between the network and the vertex covariates. Existing techniques for network data and vertex covariates focus on capturing structure that is shared but are usually not able to differentiate structure that is unique to each dataset. This work formulates a low-rank model that simultaneously captures joint and individual information in network data with vertex covariates. A two-step estimation procedure is proposed, composed of an efficient spectral method followed by a refinement optimization step. Theoretically, we show that the spectral method is able to consistently recover the joint and individual components under a general signal-plus-noise model.
  Simulations and real data examples demonstrate the ability of the methods to recover accurate and interpretable components. In particular, the application of the methodology to a food trade network between countries with economic, developmental and geographical country-level indicators as covariates yields joint and individual factors that explain the trading patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08776v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carson James, Dongbang Yuan, Irina Gaynanova, Jes\'us Arroyo</dc:creator>
    </item>
    <item>
      <title>Improved methods for empirical Bayes multivariate multiple testing and effect size estimation</title>
      <link>https://arxiv.org/abs/2406.08784</link>
      <description>arXiv:2406.08784v1 Announce Type: new 
Abstract: Estimating the sharing of genetic effects across different conditions is important to many statistical analyses of genomic data. The patterns of sharing arising from these data are often highly heterogeneous. To flexibly model these heterogeneous sharing patterns, Urbut et al. (2019) proposed the multivariate adaptive shrinkage (MASH) method to jointly analyze genetic effects across multiple conditions. However, multivariate analyses using MASH (as well as other multivariate analyses) require good estimates of the sharing patterns, and estimating these patterns efficiently and accurately remains challenging. Here we describe new empirical Bayes methods that provide improvements in speed and accuracy over existing methods. The two key ideas are: (1) adaptive regularization to improve accuracy in settings with many conditions; (2) improving the speed of the model fitting algorithms by exploiting analytical results on covariance estimation. In simulations, we show that the new methods provide better model fits, better out-of-sample performance, and improved power and accuracy in detecting the true underlying signals. In an analysis of eQTLs in 49 human tissues, our new analysis pipeline achieves better model fits and better out-of-sample performance than the existing MASH analysis pipeline. We have implemented the new methods, which we call ``Ultimate Deconvolution'', in an R package, udr, available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08784v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunqi Yang, Peter Carbonetto, David Gerard, Matthew Stephens</dc:creator>
    </item>
    <item>
      <title>A Robust Bayesian approach for reliability prognosis of one-shot devices under cumulative risk model</title>
      <link>https://arxiv.org/abs/2406.08867</link>
      <description>arXiv:2406.08867v1 Announce Type: new 
Abstract: The reliability prognosis of one-shot devices is drawing increasing attention because of their wide applicability. The present study aims to determine the lifetime prognosis of highly durable one-shot device units under a step-stress accelerated life testing (SSALT) experiment applying a cumulative risk model (CRM). In an SSALT experiment, CRM retains the continuity of hazard function by allowing the lag period before the effects of stress change emerge. In an analysis of such lifetime data, plentiful datasets might have outliers where conventional methods like maximum likelihood estimation or likelihood-based Bayesian estimation frequently fail. This work develops a robust estimation method based on density power divergence in classical and Bayesian frameworks. The hypothesis is tested by implementing the Bayes factor based on a robustified posterior. In Bayesian estimation, we exploit Hamiltonian Monte Carlo, which has certain advantages over the conventional Metropolis-Hastings algorithms. Further, the influence functions are examined to evaluate the robust behaviour of the estimators and the Bayes factor. Finally, the analytical development is validated through a simulation study and a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08867v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shanya Baghel, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>Covariate Selection for Optimizing Balance with Covariate-Adjusted Response-Adaptive Randomization</title>
      <link>https://arxiv.org/abs/2406.08968</link>
      <description>arXiv:2406.08968v1 Announce Type: new 
Abstract: Balancing influential covariates is crucial for valid treatment comparisons in clinical studies. While covariate-adaptive randomization is commonly used to achieve balance, its performance can be inadequate when the number of baseline covariates is large. It is therefore essential to identify the influential factors associated with the outcome and ensure balance among these critical covariates. In this article, we propose a novel covariate-adjusted response-adaptive randomization that integrates the patients' responses and covariates information to select sequentially significant covariates and maintain their balance. We establish theoretically the consistency of our covariate selection method and demonstrate that the improved covariate balancing, as evidenced by a faster convergence rate of the imbalance measure, leads to higher efficiency in estimating treatment effects. Furthermore, we provide extensive numerical and empirical studies to illustrate the benefits of our proposed method across various settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08968v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziqing Guo, Yang Liu, Lucy Xia</dc:creator>
    </item>
    <item>
      <title>A geometric approach to informed MCMC sampling</title>
      <link>https://arxiv.org/abs/2406.09010</link>
      <description>arXiv:2406.09010v1 Announce Type: new 
Abstract: A Riemannian geometric framework for Markov chain Monte Carlo (MCMC) is developed where using the Fisher-Rao metric on the manifold of probability density functions (pdfs) informed proposal densities for Metropolis-Hastings (MH) algorithms are constructed. We exploit the square-root representation of pdfs under which the Fisher-Rao metric boils down to the standard $L^2$ metric on the positive orthant of the unit hypersphere. The square-root representation allows us to easily compute the geodesic distance between densities, resulting in a straightforward implementation of the proposed geometric MCMC methodology. Unlike the random walk MH that blindly proposes a candidate state using no information about the target, the geometric MH algorithms effectively move an uninformed base density (e.g., a random walk proposal density) towards different global/local approximations of the target density. We compare the proposed geometric MH algorithm with other MCMC algorithms for various Markov chain orderings, namely the covariance, efficiency, Peskun, and spectral gap orderings. The superior performance of the geometric algorithms over other MH algorithms like the random walk Metropolis, independent MH and variants of Metropolis adjusted Langevin algorithms is demonstrated in the context of various multimodal, nonlinear and high dimensional examples. In particular, we use extensive simulation and real data applications to compare these algorithms for analyzing mixture models, logistic regression models and ultra-high dimensional Bayesian variable selection models. A publicly available R package accompanies the article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09010v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivekananda Roy</dc:creator>
    </item>
    <item>
      <title>Relational event models with global covariates</title>
      <link>https://arxiv.org/abs/2406.09055</link>
      <description>arXiv:2406.09055v1 Announce Type: new 
Abstract: Traditional inference in relational event models from dynamic network data involves only dyadic and node-specific variables, as anything that is global, i.e. constant across dyads, drops out of the partial likelihood. We address this with the use of nested case-control sampling on a time-shifted version of the event process. This leads to a partial likelihood of a degenerate logistic additive model, enabling efficient estimation of global and non-global covariate effects. The method's effectiveness is demonstrated through a simulation study. An application to bike sharing data reveals significant influences of global covariates like weather and time of day on bike-sharing dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09055v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melania Lembo, R\=uta Juozaitien\.e, Veronica Vinciotti, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>Covariate balancing with measurement error</title>
      <link>https://arxiv.org/abs/2406.09163</link>
      <description>arXiv:2406.09163v1 Announce Type: new 
Abstract: In recent years, there is a growing body of causal inference literature focusing on covariate balancing methods. These methods eliminate observed confounding by equalizing covariate moments between the treated and control groups. The validity of covariate balancing relies on an implicit assumption that all covariates are accurately measured, which is frequently violated in observational studies. Nevertheless, the impact of measurement error on covariate balancing is unclear, and there is no existing work on balancing mismeasured covariates adequately. In this article, we show that naively ignoring measurement error reversely increases the magnitude of covariate imbalance and induces bias to treatment effect estimation. We then propose a class of measurement error correction strategies for the existing covariate balancing methods. Theoretically, we show that these strategies successfully recover balance for all covariates, and eliminate bias of treatment effect estimation. We assess the proposed correction methods in simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09163v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xialing Wen, Ying Yan</dc:creator>
    </item>
    <item>
      <title>When Pearson $\chi^2$ and other divisible statistics are not goodness-of-fit tests</title>
      <link>https://arxiv.org/abs/2406.09195</link>
      <description>arXiv:2406.09195v1 Announce Type: new 
Abstract: Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data. While this area of statistics is often perceived - somewhat naively - as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored. Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands.
  Motivated by this need, the article introduces a unifying approach to the analysis of grouped data which allows us to study the class of divisible statistics - that includes Pearson's $\chi^2$, the likelihood ratio as special cases - with a fresh perspective. The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests.
  Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by a class of weighted linear statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09195v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Algeri, Estate V. Khmaladze</dc:creator>
    </item>
    <item>
      <title>General Bayesian Predictive Synthesis</title>
      <link>https://arxiv.org/abs/2406.09254</link>
      <description>arXiv:2406.09254v1 Announce Type: new 
Abstract: This study investigates Bayesian ensemble learning for improving the quality of decision-making. We consider a decision-maker who selects an action from a set of candidates based on a policy trained using observations. In our setting, we assume the existence of experts who provide predictive distributions based on their own policies. Our goal is to integrate these predictive distributions within the Bayesian framework. Our proposed method, which we refer to as General Bayesian Predictive Synthesis (GBPS), is characterized by a loss minimization framework and does not rely on parameter estimation, unlike existing studies. Inspired by Bayesian predictive synthesis and general Bayes frameworks, we evaluate the performance of our proposed method through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09254v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Interventional Causal Discovery in a Mixture of DAGs</title>
      <link>https://arxiv.org/abs/2406.08666</link>
      <description>arXiv:2406.08666v1 Announce Type: cross 
Abstract: Causal interactions among a group of variables are often modeled by a single causal graph. In some domains, however, these interactions are best described by multiple co-existing causal graphs, e.g., in dynamical systems or genomics. This paper addresses the hitherto unknown role of interventions in learning causal interactions among variables governed by a mixture of causal systems, each modeled by one directed acyclic graph (DAG). Causal discovery from mixtures is fundamentally more challenging than single-DAG causal discovery. Two major difficulties stem from (i) inherent uncertainty about the skeletons of the component DAGs that constitute the mixture and (ii) possibly cyclic relationships across these component DAGs. This paper addresses these challenges and aims to identify edges that exist in at least one component DAG of the mixture, referred to as true edges. First, it establishes matching necessary and sufficient conditions on the size of interventions required to identify the true edges. Next, guided by the necessity results, an adaptive algorithm is designed that learns all true edges using ${\cal O}(n^2)$ interventions, where $n$ is the number of nodes. Remarkably, the size of the interventions is optimal if the underlying mixture model does not contain cycles across its components. More generally, the gap between the intervention size used by the algorithm and the optimal size is quantified. It is shown to be bounded by the cyclic complexity number of the mixture model, defined as the size of the minimal intervention that can break the cycles in the mixture, which is upper bounded by the number of cycles among the ancestors of a node.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08666v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Burak Var{\i}c{\i}, Dmitriy Katz-Rogozhnikov, Dennis Wei, Prasanna Sattigeri, Ali Tajer</dc:creator>
    </item>
    <item>
      <title>Orthogonalized Estimation of Difference of $Q$-functions</title>
      <link>https://arxiv.org/abs/2406.08697</link>
      <description>arXiv:2406.08697v1 Announce Type: cross 
Abstract: Offline reinforcement learning is important in many settings with available observational data but the inability to deploy new policies online due to safety, cost, and other concerns. Many recent advances in causal inference and machine learning target estimation of causal contrast functions such as CATE, which is sufficient for optimizing decisions and can adapt to potentially smoother structure. We develop a dynamic generalization of the R-learner (Nie and Wager 2021, Lewis and Syrgkanis 2021) for estimating and optimizing the difference of $Q^\pi$-functions, $Q^\pi(s,1)-Q^\pi(s,0)$ (which can be used to optimize multiple-valued actions). We leverage orthogonal estimation to improve convergence rates in the presence of slower nuisance estimation rates and prove consistency of policy optimization under a margin condition. The method can leverage black-box nuisance estimators of the $Q$-function and behavior policy to target estimation of a more structured $Q$-function contrast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08697v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angela Zhou</dc:creator>
    </item>
    <item>
      <title>Introducing Diminutive Causal Structure into Graph Representation Learning</title>
      <link>https://arxiv.org/abs/2406.08709</link>
      <description>arXiv:2406.08709v1 Announce Type: cross 
Abstract: When engaging in end-to-end graph representation learning with Graph Neural Networks (GNNs), the intricate causal relationships and rules inherent in graph data pose a formidable challenge for the model in accurately capturing authentic data relationships. A proposed mitigating strategy involves the direct integration of rules or relationships corresponding to the graph data into the model. However, within the domain of graph representation learning, the inherent complexity of graph data obstructs the derivation of a comprehensive causal structure that encapsulates universal rules or relationships governing the entire dataset. Instead, only specialized diminutive causal structures, delineating specific causal relationships within constrained subsets of graph data, emerge as discernible. Motivated by empirical insights, it is observed that GNN models exhibit a tendency to converge towards such specialized causal structures during the training process. Consequently, we posit that the introduction of these specific causal structures is advantageous for the training of GNN models. Building upon this proposition, we introduce a novel method that enables GNN models to glean insights from these specialized diminutive causal structures, thereby enhancing overall performance. Our method specifically extracts causal knowledge from the model representation of these diminutive causal structures and incorporates interchange intervention to optimize the learning process. Theoretical analysis serves to corroborate the efficacy of our proposed method. Furthermore, empirical experiments consistently demonstrate significant performance improvements across diverse datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08709v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hang Gao, Peng Qiao, Yifan Jin, Fengge Wu, Jiangmeng Li, Changwen Zheng</dc:creator>
    </item>
    <item>
      <title>Volatility Forecasting Using Similarity-based Parameter Correction and Aggregated Shock Information</title>
      <link>https://arxiv.org/abs/2406.08738</link>
      <description>arXiv:2406.08738v1 Announce Type: cross 
Abstract: We develop a procedure for forecasting the volatility of a time series immediately following a news shock. Adapting the similarity-based framework of Lin and Eck (2020), we exploit series that have experienced similar shocks. We aggregate their shock-induced excess volatilities by positing the shocks to be affine functions of exogenous covariates. The volatility shocks are modeled as random effects and estimated as fixed effects. The aggregation of these estimates is done in service of adjusting the $h$-step-ahead GARCH forecast of the time series under study by an additive term. The adjusted and unadjusted forecasts are evaluated using the unobservable but easily-estimated realized volatility (RV). A real-world application is provided, as are simulation results suggesting the conditions and hyperparameters under which our method thrives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08738v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Lundquist, Daniel Eck</dc:creator>
    </item>
    <item>
      <title>Empirical Networks are Sparse: Enhancing Multi-Edge Models with Zero-Inflation</title>
      <link>https://arxiv.org/abs/2406.09169</link>
      <description>arXiv:2406.09169v1 Announce Type: cross 
Abstract: Real-world networks are sparse. As we show in this article, even when a large number of interactions is observed most node pairs remain disconnected. We demonstrate that classical multi-edge network models, such as the $G(N,p)$, configuration models, and stochastic block models, fail to accurately capture this phenomenon. To mitigate this issue, zero-inflation must be integrated into these traditional models. Through zero-inflation, we incorporate a mechanism that accounts for the excess number of zeroes (disconnected pairs) observed in empirical data. By performing an analysis on all the datasets from the Sociopatterns repository, we illustrate how zero-inflated models more accurately reflect the sparsity and heavy-tailed edge count distributions observed in empirical data. Our findings underscore that failing to account for these ubiquitous properties in real-world networks inadvertently leads to biased models which do not accurately represent complex systems and their dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09169v1</guid>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giona Casiraghi, Georges Andres</dc:creator>
    </item>
    <item>
      <title>Generative vs. Discriminative modeling under the lens of uncertainty quantification</title>
      <link>https://arxiv.org/abs/2406.09172</link>
      <description>arXiv:2406.09172v1 Announce Type: cross 
Abstract: Learning a parametric model from a given dataset indeed enables to capture intrinsic dependencies between random variables via a parametric conditional probability distribution and in turn predict the value of a label variable given observed variables. In this paper, we undertake a comparative analysis of generative and discriminative approaches which differ in their construction and the structure of the underlying inference problem. Our objective is to compare the ability of both approaches to leverage information from various sources in an epistemic uncertainty aware inference via the posterior predictive distribution. We assess the role of a prior distribution, explicit in the generative case and implicit in the discriminative case, leading to a discussion about discriminative models suffering from imbalanced dataset. We next examine the double role played by the observed variables in the generative case, and discuss the compatibility of both approaches with semi-supervised learning. We also provide with practical insights and we examine how the modeling choice impacts the sampling from the posterior predictive distribution. With regard to this, we propose a general sampling scheme enabling supervised learning for both approaches, as well as semi-supervised learning when compatible with the considered modeling approach. Throughout this paper, we illustrate our arguments and conclusions using the example of affine regression, and validate our comparative analysis through classification simulations using neural network based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09172v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elouan Argouarc'h, Fran\c{c}ois Desbouvries, Eric Barat, Eiji Kawasaki</dc:creator>
    </item>
    <item>
      <title>Oblivious subspace embeddings for compressed Tucker decompositions</title>
      <link>https://arxiv.org/abs/2406.09387</link>
      <description>arXiv:2406.09387v1 Announce Type: cross 
Abstract: Emphasis in the tensor literature on random embeddings (tools for low-distortion dimension reduction) for the canonical polyadic (CP) tensor decomposition has left analogous results for the more expressive Tucker decomposition comparatively lacking. This work establishes general Johnson-Lindenstrauss (JL) type guarantees for the estimation of Tucker decompositions when an oblivious random embedding is applied along each mode. When these embeddings are drawn from a JL-optimal family, the decomposition can be estimated within $\varepsilon$ relative error under restrictions on the embedding dimension that are in line with recent CP results. We implement a higher-order orthogonal iteration (HOOI) decomposition algorithm with random embeddings to demonstrate the practical benefits of this approach and its potential to improve the accessibility of otherwise prohibitive tensor analyses. On moderately large face image and fMRI neuroimaging datasets, empirical results show that substantial dimension reduction is possible with minimal increase in reconstruction error relative to traditional HOOI ($\leq$5% larger error, 50%-60% lower computation time for large models with 50% dimension reduction along each mode). Especially for large tensors, our method outperforms traditional higher-order singular value decomposition (HOSVD) and recently proposed TensorSketch methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09387v1</guid>
      <category>stat.ML</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthew Pietrosanu, Bei Jiang, Linglong Kong</dc:creator>
    </item>
    <item>
      <title>A procedure for multiple testing of partial conjunction hypotheses based on a hazard rate inequality</title>
      <link>https://arxiv.org/abs/2110.06692</link>
      <description>arXiv:2110.06692v4 Announce Type: replace 
Abstract: The partial conjunction null hypothesis is tested in order to discover a signal that is present in multiple studies. The standard approach of carrying out a multiple test procedure on the partial conjunction (PC) $p$-values can be extremely conservative. We suggest alleviating this conservativeness, by eliminating many of the conservative PC $p$-values prior to the application of a multiple test procedure. This leads to the following two step procedure: first, select the set with PC $p$-values below a selection threshold; second, within the selected set only, apply a family-wise error rate or false discovery rate controlling procedure on the conditional PC $p$-values. The conditional PC $p$-values are valid if the null p-values are uniform and the combining method is Fisher. The proof of their validity is based on a novel inequality in hazard rate order of partial sums of order statistics which may be of independent interest. We also provide the conditions for which the false discovery rate controlling procedures considered will be below the nominal level. We demonstrate the potential usefulness of our novel method, CoFilter (conditional testing after filtering), for analyzing multiple genome wide association studies of Crohn's disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.06692v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thorsten Dickhaus, Ruth Heller, Anh-Tuan Hoang, Yosef Rinott</dc:creator>
    </item>
    <item>
      <title>Conformal prediction with local weights: randomization enables local guarantees</title>
      <link>https://arxiv.org/abs/2310.07850</link>
      <description>arXiv:2310.07850v2 Announce Type: replace 
Abstract: In this work, we consider the problem of building distribution-free prediction intervals with finite-sample conditional coverage guarantees. Conformal prediction (CP) is an increasingly popular framework for building prediction intervals with distribution-free guarantees, but these guarantees only ensure marginal coverage: the probability of coverage is averaged over a random draw of both the training and test data, meaning that there might be substantial undercoverage within certain subpopulations. Instead, ideally, we would want to have local coverage guarantees that hold for each possible value of the test point's features. While the impossibility of achieving pointwise local coverage is well established in the literature, many variants of conformal prediction algorithm show favorable local coverage properties empirically. Relaxing the definition of local coverage can allow for a theoretical understanding of this empirical phenomenon. We aim to bridge this gap between theoretical validation and empirical performance by proving achievable and interpretable guarantees for a relaxed notion of local coverage. Building on the localized CP method of Guan (2023) and the weighted CP framework of Tibshirani et al. (2019), we propose a new method, randomly-localized conformal prediction (RLCP), which returns prediction intervals that are not only marginally valid but also achieve a relaxed local coverage guarantee and guarantees under covariate shift. Through a series of simulations and real data experiments, we validate these coverage guarantees of RLCP while comparing it with the other local conformal prediction methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07850v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Hore, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Efficient and Globally Robust Causal Excursion Effect Estimation</title>
      <link>https://arxiv.org/abs/2311.16529</link>
      <description>arXiv:2311.16529v3 Announce Type: replace 
Abstract: Causal excursion effect (CEE) characterizes the effect of an intervention under policies that deviate from the experimental policy. It is widely used to study the effect of time-varying interventions that have the potential to be frequently adaptive, such as those delivered through smartphones. We study the semiparametric efficient estimation of CEE and we derive a semiparametric efficiency bound for CEE with identity or log link functions under working assumptions, in the context of micro-randomized trials. We propose a class of two-stage estimators that achieve the efficiency bound and are robust to misspecified nuisance models. In deriving the asymptotic property of the estimators, we establish a general theory for globally robust Z-estimators with either cross-fitted or non-cross-fitted nuisance parameters. We demonstrate substantial efficiency gain of the proposed estimator compared to existing ones through simulations and a real data application using the Drink Less micro-randomized trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16529v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxi Cheng, Lauren Bell, Tianchen Qian</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: V. Non-asymptotic</title>
      <link>https://arxiv.org/abs/2403.18951</link>
      <description>arXiv:2403.18951v2 Announce Type: replace 
Abstract: Due to the complexity of order statistics, the finite sample behaviour of robust statistics is generally not analytically solvable. While the Monte Carlo method can provide approximate solutions, its convergence rate is typically very slow, making the computational cost to achieve the desired accuracy unaffordable for ordinary users. In this paper, we propose an approach analogous to the Fourier transformation to decompose the finite sample structure of the uniform distribution. By obtaining sets of sequences that are consistent with parametric distributions for the first four sample moments, we can approximate the finite sample behavior of other estimators with significantly reduced computational costs. This article reveals the underlying structure of randomness and presents a novel approach to integrate multiple assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18951v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuobang Li</dc:creator>
    </item>
    <item>
      <title>Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets</title>
      <link>https://arxiv.org/abs/2406.08390</link>
      <description>arXiv:2406.08390v2 Announce Type: replace 
Abstract: Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery's cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08390v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul E. Seifert, Emil Kraft, Steffen Bakker, Stein-Erik Fleten</dc:creator>
    </item>
    <item>
      <title>Approximate Message Passing for orthogonally invariant ensembles: Multivariate non-linearities and spectral initialization</title>
      <link>https://arxiv.org/abs/2110.02318</link>
      <description>arXiv:2110.02318v2 Announce Type: replace-cross 
Abstract: We study a class of Approximate Message Passing (AMP) algorithms for symmetric and rectangular spiked random matrix models with orthogonally invariant noise. The AMP iterates have fixed dimension $K \geq 1$, a multivariate non-linearity is applied in each AMP iteration, and the algorithm is spectrally initialized with $K$ super-critical sample eigenvectors. We derive the forms of the Onsager debiasing coefficients and corresponding AMP state evolution, which depend on the free cumulants of the noise spectral distribution. This extends previous results for such models with $K=1$ and an independent initialization.
  Applying this approach to Bayesian principal components analysis, we introduce a Bayes-OAMP algorithm that uses as its non-linearity the posterior mean conditional on all preceding AMP iterates. We describe a practical implementation of this algorithm, where all debiasing and state evolution parameters are estimated from the observed data, and we illustrate the accuracy and stability of this approach in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.02318v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Zhong, Tianhao Wang, Zhou Fan</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: I. Mean</title>
      <link>https://arxiv.org/abs/2403.12110</link>
      <description>arXiv:2403.12110v2 Announce Type: replace-cross 
Abstract: As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12110v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuobang Li</dc:creator>
    </item>
    <item>
      <title>Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models</title>
      <link>https://arxiv.org/abs/2404.19707</link>
      <description>arXiv:2404.19707v2 Announce Type: replace-cross 
Abstract: Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. We also discuss labelling of the shocks, maximum likelihood estimation of the parameters, and stationarity the model. The introduced methods are implemented to the accompanying R package sstvars. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production in times of low economic policy uncertainty but slightly increases it in times of high economic policy uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19707v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Savi Virolainen</dc:creator>
    </item>
  </channel>
</rss>
