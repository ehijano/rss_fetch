<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Apr 2024 19:06:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sampling error mitigation through spectrum smoothing in ensemble data assimilation</title>
      <link>https://arxiv.org/abs/2404.00154</link>
      <description>arXiv:2404.00154v1 Announce Type: new 
Abstract: In data assimilation, an ensemble provides a nonintrusive way to evolve a probability density described by a nonlinear prediction model. Although a large ensemble size is required for statistical accuracy, the ensemble size is typically limited to a small number due to the computational cost of running the prediction model, which leads to a sampling error. Several methods, such as localization, exist to mitigate the sampling error, often requiring problem-dependent fine-tuning and design. This work introduces another sampling error mitigation method using a smoothness constraint in the Fourier space. In particular, this work smoothes out the spectrum of the system to increase the stability and accuracy even under a small ensemble size. The efficacy of the new idea is validated through a suite of stringent test problems, including Lorenz 96 and Kuramoto-Sivashinsky turbulence models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00154v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bosu Choi, Yoonsang Lee</dc:creator>
    </item>
    <item>
      <title>Numerical Simulations for Fractional Differential Equations of Higher Order and a Wright-Type Transformation</title>
      <link>https://arxiv.org/abs/2404.00248</link>
      <description>arXiv:2404.00248v1 Announce Type: new 
Abstract: In this work, a new relationship is established between the solutions of higher fractional differential equations and a Wright-type transformation. Solutions could be interpreted as expected values of functions in a random time process. As applications, we solve the fractional beam equation, fractional electric circuits with special functions as external sources, and derive dAlemberts formula for the fractional wave equation. Due to this relationship, we present two methods for simulating solutions of fractional differential equations. The two approaches use the interpretation of the Caputo derivative of a function as a Wright-type transformation of the higher derivative of the function. In the first approach, we use the Runge-Kutta method of hybrid orders 4 and 5 to solve ordinary differential equations combined with the Monte Carlo integration to conduct the Wrighttype transformation. The second method uses a feedforward neural network to simulate the fractional differential equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00248v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Nacianceno, T. Oraby, H. Rodrigo, Y. Sepulveda, J. Sifuentes, E. Suazo, T. Stuck, J. Williams</dc:creator>
    </item>
    <item>
      <title>Geometric mean for T-positive definite tensors and associated Riemannian geometry</title>
      <link>https://arxiv.org/abs/2404.00255</link>
      <description>arXiv:2404.00255v1 Announce Type: new 
Abstract: In this paper, we generalize the geometric mean of two positive definite matrices to that of third-order tensors using the notion of T-product. Specifically, we define the geometric mean of two T-positive definite tensors and verify several properties that "mean" should satisfy including the idempotence and the commutative property, and so on. Moreover, it is shown that the geometric mean is a unique T-positive definite solution of an algebraic Riccati tensor equation and can be expressed as solutions of algebraic Riccati matrix equations. In addition, we investigate the Riemannian manifold associated with the geometric mean for T-positive definite tensors, considering it as a totally geodesic embedded submanifold of the Riemannian manifold associated with the case of matrices. It is particularly shown that the geometric mean of two T-positive definite tensors is the midpoint of a unique geodesic joining the tensors, and the manifold is a Cartan-Hadamard-Riemannian manifold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00255v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.FA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeong-Hoon Ju, Taehyeong Kim, Yeongrak Kim, Hayoung Choi</dc:creator>
    </item>
    <item>
      <title>Implicit-explicit schemes for compressible Cahn-Hilliard-Navier-Stokes equations</title>
      <link>https://arxiv.org/abs/2404.00326</link>
      <description>arXiv:2404.00326v1 Announce Type: new 
Abstract: The isentropic compressible Cahn-Hilliard-Navier-Stokes equations is a system of fourth-order partial differential equations that model the evolution of some binary fluids under convection.
  The purpose of this paper is the design of efficient numerical schemes to approximate the solution of initial-boundary value problems with these equations. The efficiency stems from the implicit treatment of the high-order terms in the equations. Our proposal is a second-order linearly implicit-explicit time stepping scheme applied in a method of lines approach, in which the convective terms are treated explicitly and only linear systems have to be solved.
  Some experiments are performed to assess the validity and efficiency of this proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00326v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pep Mulet</dc:creator>
    </item>
    <item>
      <title>Robust and structure-preserving time-discretisation and linearisation schemes for singular and degenerate evolution systems arising in models for biofilm growth</title>
      <link>https://arxiv.org/abs/2404.00391</link>
      <description>arXiv:2404.00391v1 Announce Type: new 
Abstract: We propose and analyse numerical schemes for a system of quasilinear, degenerate evolution equations modelling biofilm growth as well as other processes such as flow through porous media and the spreading of wildfires. The first equation in the system is parabolic and exhibits degenerate and singular diffusion, while the second is either uniformly parabolic or an ordinary differential equation. First, we introduce a semi-implicit time discretisation that has the benefit of decoupling the equations. We prove the positivity, boundedness, and convergence of the time-discrete solutions to the time-continuous solution. Then, we introduce an iterative linearisation scheme to solve the resulting nonlinear time-discrete problems. Under weak assumptions on the time-step size, we prove that the scheme converges irrespective of the space discretisation and mesh. Moreover, if the problem is non-degenerate, the convergence becomes faster as the time-step size decreases. Finally, employing the finite element method for the spatial discretisation, we study the behaviour of the scheme, and compare its performance to other commonly used schemes. These tests confirm that the proposed scheme is robust and fast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00391v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. K. H. Smeets, K. Mitra, I. S. Pop, S. Sonner</dc:creator>
    </item>
    <item>
      <title>Chebyshev and The Fast Fourier Transform Methods for Signal Interpolation</title>
      <link>https://arxiv.org/abs/2404.00414</link>
      <description>arXiv:2404.00414v1 Announce Type: new 
Abstract: Approximation theorem is one of the most important aspects of numerical analysis that has evolved over the years with many different approaches. Some of the most popular approximation methods include the Lebesgue approximation theorem, the Weierstrass approximation, and the Fourier approximation theorem. The limitations associated with various approximation methods are too crucial to ignore, and thus, the nature of a specific dataset may require using a specific approximation method for such estimates. In this report, we shall delve into Chebyshev's polynomials interpolation in detail as an alternative approach to reconstructing signals and compare the reconstruction to that of the Fourier polynomials. We will also explore the advantages and limitations of the Chebyshev polynomials and discuss in detail their mathematical formulation and equivalence to the cosine function over a given interval [a, b].</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00414v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ishmael N. Amartey</dc:creator>
    </item>
    <item>
      <title>Characterizing GSVD by singular value expansion of linear operators and its computation</title>
      <link>https://arxiv.org/abs/2404.00655</link>
      <description>arXiv:2404.00655v1 Announce Type: new 
Abstract: The generalized singular value decomposition (GSVD) of a matrix pair $\{A, L\}$ with $A\in\mathbb{R}^{m\times n}$ and $L\in\mathbb{R}^{p\times n}$ generalizes the singular value decomposition (SVD) of a single matrix. In this paper, we provide a new understanding of GSVD from the viewpoint of SVD, based on which we propose a new iterative method for computing nontrivial GSVD components of a large-scale matrix pair. By introducing two linear operators $\mathcal{A}$ and $\mathcal{L}$ induced by $\{A, L\}$ between two finite-dimensional Hilbert spaces and applying the theory of singular value expansion (SVE) for linear compact operators, we show that the GSVD of $\{A, L\}$ is nothing but the SVEs of $\mathcal{A}$ and $\mathcal{L}$. This result characterizes completely the structure of GSVD for any matrix pair with the same number of columns. As a direct application of this result, we generalize the standard Golub-Kahan bidiagonalization (GKB) that is a basic routine for large-scale SVD computation such that the resulting generalized GKB (gGKB) process can be used to approximate nontrivial extreme GSVD components of $\{A, L\}$, which is named the gGKB\_GSVD algorithm. We use the GSVD of $\{A, L\}$ to study several basic properties of gGKB and also provide preliminary results about convergence and accuracy of gGKB\_GSVD for GSVD computation. Numerical experiments are presented to demonstrate the effectiveness of this method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00655v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haibo Li</dc:creator>
    </item>
    <item>
      <title>Off-the-grid regularisation for Poisson inverse problems</title>
      <link>https://arxiv.org/abs/2404.00810</link>
      <description>arXiv:2404.00810v1 Announce Type: new 
Abstract: Off-the-grid regularisation has been extensively employed over the last decade in the context of ill-posed inverse problems formulated in the continuous setting of the space of Radon measures $\mathcal{M}(\mathcal{X})$. These approaches enjoy convexity and counteract the discretisation biases as well the numerical instabilities typical of their discrete counterparts. In the framework of sparse reconstruction of discrete point measures (sum of weighted Diracs), a Total Variation regularisation norm in $\mathcal{M}(\mathcal{X})$ is typically combined with an $L^2$ data term modelling additive Gaussian noise. To asses the framework of off-the-grid regularisation in the presence of signal-dependent Poisson noise, we consider in this work a variational model coupling the Total Variation regularisation with a Kullback-Leibler data term under a non-negativity constraint. Analytically, we study the optimality conditions of the composite functional and analyse its dual problem. Then, we consider an homotopy strategy to select an optimal regularisation parameter and use it within a Sliding Frank-Wolfe algorithm. Several numerical experiments on both 1D/2D simulated and real 3D fluorescent microscopy data are reported.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00810v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta Lazzaretti, Claudio Estatico, Alejandro Melero Carrillo, Luca Calatroni</dc:creator>
    </item>
    <item>
      <title>Randomized Nystr\"om approximation of non-negative self-adjoint operators</title>
      <link>https://arxiv.org/abs/2404.00960</link>
      <description>arXiv:2404.00960v1 Announce Type: new 
Abstract: The randomized singular value decomposition (SVD) has become a popular approach to computing cheap, yet accurate, low-rank approximations to matrices due to its efficiency and strong theoretical guarantees. Recent work by Boull\'e and Townsend (FoCM, 2023) presents an infinite-dimensional analog of the randomized SVD to approximate Hilbert-Schmidt operators. However, many applications involve computing low-rank approximations to symmetric positive semi-definite matrices. In this setting, it is well-established that the randomized Nystr{\"o}m approximation is usually preferred over the randomized SVD. This paper explores an infinite-dimensional analog of the Nystr{\"o}m approximation to compute low-rank approximations to non-negative self-adjoint trace-class operators. We present an analysis of the method and, along the way, improve the existing infinite-dimensional bounds for the randomized SVD. Our analysis yields bounds on the expected value and tail bounds for the Nystr{\"o}m approximation error in the operator, trace, and Hilbert-Schmidt norms. Numerical experiments for simple integral operators validate the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00960v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Persson, Nicolas Boull\'e, Daniel Kressner</dc:creator>
    </item>
    <item>
      <title>Sequential-in-time training of nonlinear parametrizations for solving time-dependent partial differential equations</title>
      <link>https://arxiv.org/abs/2404.01145</link>
      <description>arXiv:2404.01145v1 Announce Type: new 
Abstract: Sequential-in-time methods solve a sequence of training problems to fit nonlinear parametrizations such as neural networks to approximate solution trajectories of partial differential equations over time. This work shows that sequential-in-time training methods can be understood broadly as either optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which are well known concepts in numerical analysis. The unifying perspective leads to novel stability and a posteriori error analysis results that provide insights into theoretical and numerical aspects that are inherent to either OtD or DtO schemes such as the tangent space collapse phenomenon, which is a form of over-fitting. Additionally, the unified perspective facilitates establishing connections between variants of sequential-in-time training methods, which is demonstrated by identifying natural gradient descent methods on energy functionals as OtD schemes applied to the corresponding gradient flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01145v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Zhang, Yifan Chen, Eric Vanden-Eijnden, Benjamin Peherstorfer</dc:creator>
    </item>
    <item>
      <title>Capturing Shock Waves by Relaxation Neural Networks</title>
      <link>https://arxiv.org/abs/2404.01163</link>
      <description>arXiv:2404.01163v1 Announce Type: new 
Abstract: In this paper, we put forward a neural network framework to solve the nonlinear hyperbolic systems. This framework, named relaxation neural networks(RelaxNN), is a simple and scalable extension of physics-informed neural networks(PINN). It is shown later that a typical PINN framework struggles to handle shock waves that arise in hyperbolic systems' solutions. This ultimately results in the failure of optimization that is based on gradient descent in the training process. Relaxation systems provide a smooth asymptotic to the discontinuity solution, under the expectation that macroscopic problems can be solved from a microscopic perspective. Based on relaxation systems, the RelaxNN framework alleviates the conflict of losses in the training process of the PINN framework. In addition to the remarkable results demonstrated in numerical simulations, most of the acceleration techniques and improvement strategies aimed at the standard PINN framework can also be applied to the RelaxNN framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01163v1</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nan Zhou, Zheng Ma</dc:creator>
    </item>
    <item>
      <title>Adaptive hybrid high-order method for guaranteed lower eigenvalue bounds</title>
      <link>https://arxiv.org/abs/2404.01228</link>
      <description>arXiv:2404.01228v1 Announce Type: new 
Abstract: The higher-order guaranteed lower eigenvalue bounds of the Laplacian in the recent work by Carstensen, Ern, and Puttkammer [Numer. Math. 149, 2021] require a parameter $C_{\mathrm{st},1}$ that is found $\textit{not}$ robust as the polynomial degree $p$ increases. This is related to the $H^1$ stability bound of the $L^2$ projection onto polynomials of degree at most $p$ and its growth $C_{\rm st, 1}\propto (p+1)^{1/2}$ as $p \to \infty$. A similar estimate for the Galerkin projection holds with a $p$-robust constant $C_{\mathrm{st},2}$ and $C_{\mathrm{st},2} \le 2$ for right-isosceles triangles. This paper utilizes the new inequality with the constant $C_{\mathrm{st},2}$ to design a modified hybrid high-order (HHO) eigensolver that directly computes guaranteed lower eigenvalue bounds under the idealized hypothesis of exact solve of the generalized algebraic eigenvalue problem and a mild explicit condition on the maximal mesh-size in the simplicial mesh. A key advance is a $p$-robust parameter selection.
  The analysis of the new method with a different fine-tuned volume stabilization allows for a priori quasi-best approximation and improved $L^2$ error estimates as well as a stabilization-free reliable and efficient a posteriori error control. The associated adaptive mesh-refining algorithm performs superior in computer benchmarks with striking numerical evidence for optimal higher empirical convergence rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01228v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carsten Carstensen, Benedikt Gr\"a{\ss}le, Ngoc Tien Tran</dc:creator>
    </item>
    <item>
      <title>Duality based error control for the Signorini problem</title>
      <link>https://arxiv.org/abs/2404.01251</link>
      <description>arXiv:2404.01251v1 Announce Type: new 
Abstract: In this paper we study the a posteriori bounds for a conforming piecewise linear finite element approximation of the Signorini problem. We prove new rigorous a posteriori estimates of residual type in $L^{p}$, for $p \in (4,\infty)$ in two spatial dimensions. This new analysis treats the positive and negative parts of the discretisation error separately, requiring a novel sign- and bound-preserving interpolant, which is shown to have optimal approximation properties. The estimates rely on the sharp dual stability results on the problem in $W^{2,(4 - \varepsilon)/3}$ for any $\varepsilon \ll 1$. We summarise extensive numerical experiments aimed at testing the robustness of the estimator to validate the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01251v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben S. Ashby, Tristan Pryer</dc:creator>
    </item>
    <item>
      <title>A finite operator learning technique for mapping the elastic properties of microstructures to their mechanical deformations</title>
      <link>https://arxiv.org/abs/2404.00074</link>
      <description>arXiv:2404.00074v1 Announce Type: cross 
Abstract: To develop faster solvers for governing physical equations in solid mechanics, we introduce a method that parametrically learns the solution to mechanical equilibrium. The introduced method outperforms traditional ones in terms of computational cost while acceptably maintaining accuracy. Moreover, it generalizes and enhances the standard physics-informed neural networks to learn a parametric solution with rather sharp discontinuities. We focus on micromechanics as an example, where the knowledge of the micro-mechanical solution, i.e., deformation and stress fields for a given heterogeneous microstructure, is crucial. The parameter under investigation is the Young modulus distribution within the heterogeneous solid system. Our method, inspired by operator learning and the finite element method, demonstrates the ability to train without relying on data from other numerical solvers. Instead, we leverage ideas from the finite element approach to efficiently set up loss functions algebraically, particularly based on the discretized weak form of the governing equations. Notably, our investigations reveal that physics-based training yields higher accuracy compared to purely data-driven approaches for unseen microstructures. In essence, this method achieves independence from data and enhances accuracy for predictions beyond the training range. The aforementioned observations apply here to heterogeneous elastic microstructures. Comparisons are also made with other well-known operator learning algorithms, such as DeepOnet, to further emphasize the advantages of the newly proposed architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00074v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shahed Rezaei, Shirko Faroughi, Mahdi Asgharzadeh, Ali Harandi, Gottfried Laschet, Stefanie Reese, Markus Apel</dc:creator>
    </item>
    <item>
      <title>Modeling Large-Scale Walking and Cycling Networks: A Machine Learning Approach Using Mobile Phone and Crowdsourced Data</title>
      <link>https://arxiv.org/abs/2404.00162</link>
      <description>arXiv:2404.00162v1 Announce Type: cross 
Abstract: Walking and cycling are known to bring substantial health, environmental, and economic advantages. However, the development of evidence-based active transportation planning and policies has been impeded by significant data limitations, such as biases in crowdsourced data and representativeness issues of mobile phone data. In this study, we develop and apply a machine learning based modeling approach for estimating daily walking and cycling volumes across a large-scale regional network in New South Wales, Australia that includes 188,999 walking links and 114,885 cycling links. The modeling methodology leverages crowdsourced and mobile phone data as well as a range of other datasets on population, land use, topography, climate, etc. The study discusses the unique challenges and limitations related to all three aspects of model training, testing, and inference given the large geographical extent of the modeled networks and relative scarcity of observed walking and cycling count data. The study also proposes a new technique to identify model estimate outliers and to mitigate their impact. Overall, the study provides a valuable resource for transportation modelers, policymakers and urban planners seeking to enhance active transportation infrastructure planning and policies with advanced emerging data-driven modeling methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00162v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Meead Saberi, Tanapon Lilasathapornkit</dc:creator>
    </item>
    <item>
      <title>Predicting the statistical error of analog particle tracing Monte Carlo</title>
      <link>https://arxiv.org/abs/2404.00315</link>
      <description>arXiv:2404.00315v1 Announce Type: cross 
Abstract: Large particle systems are often described by high-dimensional (linear) kinetic equations that are simulated using Monte Carlo methods for which the asymptotic convergence rate is independent of the dimensionality. Even though the asymptotic convergence rate is known, predicting the actual value of the statistical error remains a challenging problem. In this paper, we show how the statistical error of an analog particle tracing Monte Carlo method can be calculated (expensive) and predicted a priori (cheap) when estimating quantities of interest (QoI) on a histogram. We consider two types of QoI estimators: point estimators for which each particle provides one independent contribution to the QoI estimates, and analog estimators for which each particle provides multiple correlated contributions to the QoI estimates. The developed statistical error predictors can be applied to other QoI estimators and nonanalog simulation routines as well. The error analysis is based on interpreting the number of particle visits to a histogram bin as the result of a (correlated) binomial experiment. The resulting expressions can be used to optimize (non)analog particle tracing Monte Carlo methods and hybrid simulation methods involving a Monte Carlo component, as well as to select an optimal particle tracing Monte Carlo method from several available options. Additionally, the cheap statistical error predictors can be used to determine a priori the number of particles N that is needed to reach a desired accuracy. We illustrate the theory using a linear kinetic equation describing neutral particles in the plasma edge of a fusion device and show numerical results. The code used to perform the numerical experiments is openly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00315v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vince Maes, Ignace Bossuyt, Hannes Vandecasteele, Wouter Dekeyser, Julian Koellermeier, Martine Baelmans, Giovanni Samaey</dc:creator>
    </item>
    <item>
      <title>Learning truly monotone operators with applications to nonlinear inverse problems</title>
      <link>https://arxiv.org/abs/2404.00390</link>
      <description>arXiv:2404.00390v1 Announce Type: cross 
Abstract: This article introduces a novel approach to learning monotone neural networks through a newly defined penalization loss. The proposed method is particularly effective in solving classes of variational problems, specifically monotone inclusion problems, commonly encountered in image processing tasks. The Forward-Backward-Forward (FBF) algorithm is employed to address these problems, offering a solution even when the Lipschitz constant of the neural network is unknown. Notably, the FBF algorithm provides convergence guarantees under the condition that the learned operator is monotone. Building on plug-and-play methodologies, our objective is to apply these newly learned operators to solving non-linear inverse problems. To achieve this, we initially formulate the problem as a variational inclusion problem. Subsequently, we train a monotone neural network to approximate an operator that may not inherently be monotone. Leveraging the FBF algorithm, we then show simulation examples where the non-linear inverse problem is successfully solved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00390v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younes Belkouchi, Jean-Christophe Pesquet, Audrey Repetti, Hugues Talbot</dc:creator>
    </item>
    <item>
      <title>Extracting Manifold Information from Point Clouds</title>
      <link>https://arxiv.org/abs/2404.00427</link>
      <description>arXiv:2404.00427v1 Announce Type: cross 
Abstract: A kernel based method is proposed for the construction of signature (defining) functions of subsets of $\mathbb{R}^d$. The subsets can range from full dimensional manifolds (open subsets) to point clouds (a finite number of points) and include bounded smooth manifolds of any codimension. The interpolation and analysis of point clouds are the main application. Two extreme cases in terms of regularity are considered, where the data set is interpolated by an analytic surface, at the one extreme, and by a H\"older continuous surface, at the other. The signature function can be computed as a linear combination of translated kernels, the coefficients of which are the solution of a finite dimensional linear problem. Once it is obtained, it can be used to estimate the dimension as well as the normal and the curvatures of the interpolated surface. The method is global and does not require explicit knowledge of local neighborhoods or any other structure present in the data set. It admits a variational formulation with a natural ``regularized'' counterpart, that proves to be useful in dealing with data sets corrupted by numerical error or noise. The underlying analytical structure of the approach is presented in general before it is applied to the case of point clouds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00427v1</guid>
      <category>cs.CV</category>
      <category>cs.CG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Patrick Guidotti</dc:creator>
    </item>
    <item>
      <title>Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in Quantifying Uncertainty Propagation</title>
      <link>https://arxiv.org/abs/2404.00502</link>
      <description>arXiv:2404.00502v1 Announce Type: cross 
Abstract: We introduce a conditional pseudo-reversible normalizing flow for constructing surrogate models of a physical model polluted by additive noise to efficiently quantify forward and inverse uncertainty propagation. Existing surrogate modeling approaches usually focus on approximating the deterministic component of physical model. However, this strategy necessitates knowledge of noise and resorts to auxiliary sampling methods for quantifying inverse uncertainty propagation. In this work, we develop the conditional pseudo-reversible normalizing flow model to directly learn and efficiently generate samples from the conditional probability density functions. The training process utilizes dataset consisting of input-output pairs without requiring prior knowledge about the noise and the function. Our model, once trained, can generate samples from any conditional probability density functions whose high probability regions are covered by the training set. Moreover, the pseudo-reversibility feature allows for the use of fully-connected neural network architectures, which simplifies the implementation and enables theoretical analysis. We provide a rigorous convergence analysis of the conditional pseudo-reversible normalizing flow model, showing its ability to converge to the target conditional probability density function using the Kullback-Leibler divergence. To demonstrate the effectiveness of our method, we apply it to several benchmark tests and a real-world geologic carbon storage problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00502v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minglei Yang, Pengjun Wang, Ming Fan, Dan Lu, Yanzhao Cao, Guannan Zhang</dc:creator>
    </item>
    <item>
      <title>The algebra of hyperinterpolation-class on the sphere</title>
      <link>https://arxiv.org/abs/2404.00523</link>
      <description>arXiv:2404.00523v1 Announce Type: cross 
Abstract: This paper considers the so-called concept of hyperinterpolation-class, i.e., the set of all operators derived from the hyperinterpolation operator on the unit sphere. Concretely, we select four different elements in the hyperinterpolation-class, namely filtered hyperinterpolation, Lasso hyperinterpolation, hard thresholding hyperinterpolation and generalized hyperinterpolation introduced by Dai [10], to explore their algebraic properties on the sphere. Based on the idea of a discrete (semi) inner product, we propose the concepts of hyper self-adjoint operator, hyper projection operator and hyper algebra. Next, we prove generalized hyperinterpolation is hyper self-adjoint and commutative with hyperinterpolation. Then we establish corresponding results of the product, sum and difference of hyper projection operators. Last but not least, we present the results of ideals between hard thresholding hyperinterpolation and hyperinterpolation. We also give a preliminary result of involutions, and also introduce the concepts of hyper $C^{\ast}$-algebra and hyper homomorphism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00523v1</guid>
      <category>math.FA</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congpei An, Jiashu Ran</dc:creator>
    </item>
    <item>
      <title>Numerical modelling of flame spread over thin circular ducts</title>
      <link>https://arxiv.org/abs/2404.01016</link>
      <description>arXiv:2404.01016v1 Announce Type: cross 
Abstract: This paper presents a numerical investigation into the phenomenon of flame spread over thin circular ducts in normal gravity and microgravity environments. Flame spread over such geometry is of significant interest due to its relevance in various practical applications, including tubes for flow purpose in medical system, fire safety in spacecrafts, ducts as well as wiring tubes. This study comprises of a comprehensive investigation of key parameters affecting flame spread rate, including fuel radius and opposed flow speed in normal gravity and microgravity environments. A 2-D axisymmetric flame spread model accounted for char and numerical simulations were performed which revealed valuable insights into the underlying mechanisms governing flame spread over such geometry. The results computed from the numerical model is compared with the experimentally observed flame spread rate to validate the numerical model which can be used to gain a comprehensive understanding of the underlying physical phenomena. As the radius of circular duct increases the flame spread rate increases both in normal gravity and microgravity environments. The conduction heat feedback and radiation heat gain coming from hot char through gas phase at inner core region are the two major mechanisms which controls the flame spread phenomena over the circular duct fuels. The flame spread rate at different flow ranging from quiescent (0 cm/s) to 30 cm/s is also evaluated and 21 % oxygen and found a non-monotonic increasing decreasing trend of flame spread rate at different opposed flow speed in both normal gravity and microgravity environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01016v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vipin Kumar, Kambam Naresh, Amit Kumar</dc:creator>
    </item>
    <item>
      <title>Gelation in input-driven aggregation</title>
      <link>https://arxiv.org/abs/2404.01032</link>
      <description>arXiv:2404.01032v1 Announce Type: cross 
Abstract: We investigate irreversible aggregation processes driven by a source of small mass clusters. In the spatially homogeneous situation, a well-mixed system is consists of clusters of various masses whose concentrations evolve according to an infinite system of nonlinear ordinary differential equations. We focus on the cluster mass distribution in the long time limit. An input-driven aggregation with rates proportional to the product of merging partners undergoes a percolation transition. We examine this process analytically and numerically. There are two theoretical schemes and two natural ways of numerical integration on the level of a truncated system with a finite number of equations. After the percolation transition, the behavior depends on the adopted approach: The giant component quickly engulfs the entire system (Flory approach), or a non-trivial stationary mass distribution emerges (Stockmayer approach). We also outline generalization to ternary aggregation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01032v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.NA</category>
      <category>math.CA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. L. Krapivsky, Sergey A. Matveev</dc:creator>
    </item>
    <item>
      <title>Covering convection with thermal blankets: formation of supercontinents</title>
      <link>https://arxiv.org/abs/2404.01172</link>
      <description>arXiv:2404.01172v1 Announce Type: cross 
Abstract: The continental plates of Earth are known to drift over a geophysical timescale, and their interactions have lead to some of the most spectacular geoformations of our planet while also causing natural disasters such as earthquakes and volcanic activity. Understanding the dynamics of interacting continental plates is thus significant. In this work, we present a fluid mechanical investigation of the plate motion, interaction, and dynamics. Through numerical experiments, we examine the coupling between a convective fluid and plates floating on top of it. With physical modeling, we show the coupling is both mechanical and thermal, leading to the thermal blanket effect: the floating plate is not only transported by the fluid flow beneath, it also prevents the heat from leaving the fluid, leading to a convective flow that further affects the plate motion. By adding several plates to such a coupled fluid-structure interaction, we also investigate how floating plates interact with each other and show that, under proper conditions, small plates can converge into a supercontinent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01172v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinzi Mac Huang</dc:creator>
    </item>
    <item>
      <title>Spectral Neural Operators</title>
      <link>https://arxiv.org/abs/2205.10573</link>
      <description>arXiv:2205.10573v2 Announce Type: replace 
Abstract: A plentitude of applications in scientific computing requires the approximation of mappings between Banach spaces. Recently introduced Fourier Neural Operator (FNO) and Deep Operator Network (DeepONet) can provide this functionality. For both of these neural operators, the input function is sampled on a given grid (uniform for FNO), and the output function is parametrized by a neural network. We argue that this parametrization leads to 1) opaque output that is hard to analyze and 2) systematic bias caused by aliasing errors in the case of FNO. The alternative, advocated in this article, is to use Chebyshev and Fourier series for both domain and codomain. The resulting Spectral Neural Operator (SNO) has transparent output, never suffers from aliasing, and may include many exact (lossless) operations on functions. The functionality is based on well-developed fast, and stable algorithms from spectral methods. The implementation requires only standard numerical linear algebra. Our benchmarks show that for many operators, SNO is superior to FNO and DeepONet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.10573v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>V. Fanaskov, I. Oseledets</dc:creator>
    </item>
    <item>
      <title>Higher-order Far-field Boundary Conditions for Crystalline Defects</title>
      <link>https://arxiv.org/abs/2210.05573</link>
      <description>arXiv:2210.05573v2 Announce Type: replace 
Abstract: Crystalline materials exhibit long-range elastic fields due to the presence of defects, leading to significant domain size effects in atomistic simulations. A rigorous far-field expansion of these long-range fields identifies low-rank structure in the form of a sum of discrete multipole terms and continuum correctors. We propose a novel numerical scheme that exploits this low-rank structure to accelerate material defect simulations by minimizing the domain size effects. Our approach iteratively improves the boundary condition, systematically following the asymptotic expansion of the far field. We provide both rigorous error estimates for the method and a range of empirical numerical tests, to assess it's convergence and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.05573v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Julian Braun, Christoph Ortner, Yangshuai Wang, Lei Zhang</dc:creator>
    </item>
    <item>
      <title>A Streamline upwind Petrov-Galerkin Reduced Order Method for Advection-Dominated Partial Differential Equations under Optimal Control</title>
      <link>https://arxiv.org/abs/2301.01973</link>
      <description>arXiv:2301.01973v2 Announce Type: replace 
Abstract: In this paper we will consider distributed Linear-Quadratic Optimal Control Problems dealing with Advection-Diffusion PDEs for high values of the P\'eclet number. In this situation, computational instabilities occur, both for steady and unsteady cases. A Streamline Upwind Petrov-Galerkin technique is used in the optimality system to overcome these unpleasant effects. We will apply a finite element method discretization in an optimize-then-discretize approach. Concerning the parabolic case, a stabilized space-time framework will be considered and stabilization will also occur in both bilinear forms involving time derivatives. Then we will build Reduced Order Models on this discretization procedure and two possible settings can be analyzed: whether or not stabilization is needed in the online phase, too. In order to build the reduced bases for state, control, and adjoint variables we will consider a Proper Orthogonal Decomposition algorithm in a partitioned approach. It is the first time that Reduced Order Models are applied to stabilized parabolic problems in this setting. The discussion is supported by computational experiments, where relative errors between the FEM and ROM solutions are studied together with the respective computational times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.01973v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Zoccolan, Maria Strazzullo, Gianluigi Rozza</dc:creator>
    </item>
    <item>
      <title>Inverse Volume Scaling of Finite-Size Error in Periodic Coupled Cluster Theory</title>
      <link>https://arxiv.org/abs/2304.03330</link>
      <description>arXiv:2304.03330v2 Announce Type: replace 
Abstract: Coupled cluster theory is one of the most popular post-Hartree-Fock methods for ab initio molecular quantum chemistry. The finite-size error of the correlation energy in periodic coupled cluster calculations for three-dimensional insulating systems has been observed to satisfy the inverse volume scaling, even in the absence of any correction schemes. This is surprising, as simpler theories that utilize only a subset of the coupled cluster diagrams exhibit much slower decay of the finite-size error, which scales inversely with the length of the system. In this study, we review the current understanding of finite-size error in quantum chemistry methods for periodic systems. We introduce new tools that elucidate the mechanisms behind this phenomenon in the context of coupled cluster doubles calculations. This reconciles some seemingly paradoxical statements related to finite-size scaling. Our findings also show that singularity subtraction can be a powerful method to effectively reduce finite-size errors in practical quantum chemistry calculations for periodic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03330v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.chem-ph</category>
      <category>physics.comp-ph</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevX.14.011059</arxiv:DOI>
      <dc:creator>Xin Xing, Lin Lin</dc:creator>
    </item>
    <item>
      <title>NLTGCR: A class of Nonlinear Acceleration Procedures based on Conjugate Residuals</title>
      <link>https://arxiv.org/abs/2306.00325</link>
      <description>arXiv:2306.00325v3 Announce Type: replace 
Abstract: This paper develops a new class of nonlinear acceleration algorithms based on extending conjugate residual-type procedures from linear to nonlinear equations. The main algorithm has strong similarities with Anderson acceleration as well as with inexact Newton methods - depending on which variant is implemented. We prove theoretically and verify experimentally, on a variety of problems from simulation experiments to deep learning applications, that our method is a powerful accelerated iterative algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00325v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1137/23M1576360</arxiv:DOI>
      <arxiv:journal_reference>SIAM Journal on Matrix Analysis and Applications, Volume 45, Issue 1, pp. 1-827 (2024)</arxiv:journal_reference>
      <dc:creator>Huan He, Ziyuan Tang, Shifan Zhao, Yousef Saad, Yuanzhe Xi</dc:creator>
    </item>
    <item>
      <title>Shape Optimization by Constrained First-Order Least Mean Approximation</title>
      <link>https://arxiv.org/abs/2309.13595</link>
      <description>arXiv:2309.13595v2 Announce Type: replace 
Abstract: In this work, the problem of shape optimization, subject to PDE constraints, is reformulated as an $L^p$ best approximation problem under divergence constraints to the shape tensor introduced in Laurain and Sturm: ESAIM Math. Model. Numer. Anal. 50 (2016). More precisely, the main result of this paper states that the $L^p$ distance of the above approximation problem is equal to the dual norm of the shape derivative considered as a functional on $W^{1,p^\ast}$ (where $1/p + 1/p^\ast = 1$). This implies that for any given shape, one can evaluate its distance from being a stationary one with respect to the shape derivative by simply solving the associated $L^p$-type least mean approximation problem. Moreover, the Lagrange multiplier for the divergence constraint turns out to be the shape deformation of steepest descent. This provides a way, as an alternative to the approach by Deckelnick, Herbert and Hinze: ESAIM Control Optim. Calc. Var. 28 (2022), for computing shape gradients in $W^{1,p^\ast}$ for $p^\ast \in ( 2 , \infty )$. The discretization of the least mean approximation problem is done with (lowest-order) matrix-valued Raviart-Thomas finite element spaces leading to piecewise constant approximations of the shape deformation acting as Lagrange multiplier. Admissible deformations in $W^{1,p^\ast}$ to be used in a shape gradient iteration are reconstructed locally. Our computational results confirm that the $L^p$ distance of the best approximation does indeed measure the distance of the considered shape to optimality. Also confirmed by our computational tests are the observations that choosing $p^\ast$ (much) larger than 2 (which means that $p$ must be close to 1 in our best approximation problem) decreases the chance of encountering mesh degeneracy during the shape gradient iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13595v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gerhard Starke</dc:creator>
    </item>
    <item>
      <title>Adaptive Computation of Elliptic Eigenvalue Optimization with a Phase-Field Approach</title>
      <link>https://arxiv.org/abs/2310.03970</link>
      <description>arXiv:2310.03970v2 Announce Type: replace 
Abstract: In this paper, we discuss adaptive approximations of an elliptic eigenvalue optimization problem in a phase-field setting by a conforming finite element method. An adaptive algorithm is proposed and implemented in several two dimensional numerical examples for illustration of efficiency and accuracy. Theoretical findings consist in the vanishing limit of a subsequence of estimators and the convergence of the relevant subsequence of adaptively-generated solutions to a solution to the continuous optimality system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03970v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jing Li, Yifeng Xu, Shengfeng Zhu</dc:creator>
    </item>
    <item>
      <title>Effect of temperature-dependent material properties on thermal regulation in microvascular composites</title>
      <link>https://arxiv.org/abs/2401.03103</link>
      <description>arXiv:2401.03103v2 Announce Type: replace 
Abstract: Fiber-reinforced composites (FRC) provide structural systems with unique features that appeal to various civilian and military sectors. Often, one needs to modulate the temperature field to achieve the intended functionalities (e.g., self-healing) in these lightweight structures. Vascular-based active cooling offers one efficient way of thermal regulation in such material systems. However, the thermophysical properties (e.g., thermal conductivity, specific heat capacity) of FRC and their base constituents depend on temperature, and such structures are often subject to a broad spectrum of temperatures. Notably, prior active cooling modeling studies did not account for such temperature dependence. Thus, the primary aim of this paper is to reveal the effect of temperature-dependent material properties -- obtained via material characterization -- on the qualitative and quantitative behaviors of active cooling. By applying mathematical analysis and conducting numerical simulations, we show this dependence does not affect qualitative attributes, such as minimum and maximum principles (in the same spirit as \textsc{Hopf}'s results for elliptic partial differential equations). However, the dependence slightly affects quantitative results, such as the mean surface temperature and thermal efficiency. The import of our study is that it provides a deeper understanding of thermal regulation systems under practical scenarios and can guide researchers and practitioners in perfecting associated designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03103v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Adhikari, J. F. Patrick, K. B. Nakshatrala</dc:creator>
    </item>
    <item>
      <title>Uniqueness, stability and algorithm for an inverse wave-number-dependent source problems</title>
      <link>https://arxiv.org/abs/2402.12088</link>
      <description>arXiv:2402.12088v3 Announce Type: replace 
Abstract: This paper is concerned with an inverse wavenumber/frequency-dependent source problem for the Helmholtz equation. In two and three dimensions, the unknown source term is supposed to be compactly supported in spatial variables but independent on one spatial variable. The dependence of the source function on wavenumber/frequency is supposed to be unknown. Based on the Dirichlet-Laplacian and Fourier-Transform methods, we develop two effcient non-iterative numerical algorithms to recover the wavenumber-dependent source. Uniqueness proof and increasing stability analysis are carried out in terms of the boundary measurement data of Dirichlet kind. Numerical experiments are conducted to illustrate the effectiveness and efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12088v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mengjie Zhao, Suliang Si, Guanghui Hu</dc:creator>
    </item>
    <item>
      <title>Unbiased Markov chain quasi-Monte Carlo for Gibbs samplers</title>
      <link>https://arxiv.org/abs/2403.04407</link>
      <description>arXiv:2403.04407v2 Announce Type: replace 
Abstract: In statistical analysis, Monte Carlo (MC) stands as a classical numerical integration method. When encountering challenging sample problem, Markov chain Monte Carlo (MCMC) is a commonly employed method. However, the MCMC estimator is biased after a fixed number of iterations. Unbiased MCMC, an advancement achieved through coupling techniques, addresses this bias issue in MCMC. It allows us to run many short chains in parallel. Quasi-Monte Carlo (QMC), known for its high order of convergence, is an alternative of MC. By incorporating the idea of QMC into MCMC, Markov chain quasi-Monte Carlo (MCQMC) effectively reduces the variance of MCMC, especially in Gibbs samplers. This work presents a novel approach that integrates unbiased MCMC with MCQMC, called as an unbiased MCQMC method. This method renders unbiased estimators while improving the rate of convergence significantly. Numerical experiments demonstrate that unbiased MCQMC with a sample size of $N$ achieves convergence rates of approximately $O(N^{-1})$ in moderate dimensions for Gibbs sampling problems. In the setting of parallelization, unbiased MCQMC also performs better than unbiased MCMC, even running with short chains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04407v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiarui Du, Zhijian He</dc:creator>
    </item>
    <item>
      <title>Eigenvalues of Dual Hermitian Matrices with Application in Formation Control</title>
      <link>https://arxiv.org/abs/2403.10308</link>
      <description>arXiv:2403.10308v2 Announce Type: replace 
Abstract: We propose a supplement matrix method for computing eigenvalues of a dual Hermitian matrix, and discuss its application in multi-agent formation control. Suppose we have a ring, which can be the real field, the complex field, or the quaternion ring. We study dual number symmetric matrices, dual complex Hermitian matrices and dual quaternion Hermitian matrices in a unified frame of dual Hermitian matrices. An $n \times n$ dual Hermitian matrix has $n$ dual number eigenvalues. We define determinant, characteristic polynomial and supplement matrices for a dual Hermitian matrix. Supplement matrices are Hermitian matrices in the original ring. The standard parts of the eigenvalues of that dual Hermitian matrix are the eigenvalues of the standard part Hermitian matrix in the original ring, while the dual parts of the eigenvalues of that dual Hermitian matrix are the eigenvalues of those supplement matrices. Hence, by applying any practical method for computing eigenvalues of Hermitian matrices in the original ring, we have a practical method for computing eigenvalues of a dual Hermitian matrix. We call this method the supplement matrix method. In multi-agent formation control, a desired relative configuration scheme may be given. People need to know if this scheme is reasonable such that a feasible solution of configurations of these multi-agents exists. By exploring the eigenvalue problem of dual Hermitian matrices, and its link with the unit gain graph theory, we open a cross-disciplinary approach to solve the relative configuration problem. Numerical experiments are reported.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10308v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.RA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liqun Qi, Chunfeng Cui</dc:creator>
    </item>
    <item>
      <title>Higher order multi-dimension reduction methods via Einstein-product</title>
      <link>https://arxiv.org/abs/2403.18171</link>
      <description>arXiv:2403.18171v2 Announce Type: replace 
Abstract: This paper explores the extension of dimension reduction (DR) techniques to the multi-dimension case by using the Einstein product. Our focus lies on graph-based methods, encompassing both linear and nonlinear approaches, within both supervised and unsupervised learning paradigms. Additionally, we investigate variants such as repulsion graphs and kernel methods for linear approaches. Furthermore, we present two generalizations for each method, based on single or multiple weights. We demonstrate the straightforward nature of these generalizations and provide theoretical insights. Numerical experiments are conducted, and results are compared with original methods, highlighting the efficiency of our proposed methods, particularly in handling high-dimensional data such as color images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18171v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alaeddine Zahir, Khalide Jbilou, Ahmed Ratnani</dc:creator>
    </item>
    <item>
      <title>A unified SHTC multiphase model of continuum mechanics</title>
      <link>https://arxiv.org/abs/2403.19298</link>
      <description>arXiv:2403.19298v2 Announce Type: replace 
Abstract: In this paper, we present a unified nonequilibrium model of continuum mechanics for compressible multiphase flows. The model, which is formulated within the framework of Symmetric Hyperbolic Thermodynamically Compatible (SHTC) equations, can describe the arbitrary number of phases that can be heat-conducting inviscid and viscous fluids, as well as elastoplastic solids. The phases are allowed to have different velocities, pressures, temperatures, and shear stresses, while the material interfaces are treated as diffuse interfaces with the volume fraction playing the role of the interface field. To relate our model to other multiphase approaches, we reformulate the SHTC governing equations in terms of the phase state parameters and put them in the form of Baer-Nunziato-type models. It is the Baer-Nunziato form of the SHTC equations which is then solved numerically using a robust second-order path-conservative MUSCL-Hancock finite volume method on Cartesian meshes. Due to the fact that the obtained governing equations are very challenging, we restrict our numerical examples to a simplified version of the model, focusing on the isentropic limit for three-phase mixtures. To address the stiffness properties of the relaxation source terms present in the model, the implemented scheme incorporates a semi-analytical time integration method specifically designed for the non-linear stiff source terms governing the strain relaxation. The validation process involves a wide range of benchmarks and several applications for compressible multiphase problems. Notably, results are presented for multiphase flows in all the relaxation limit cases of the model, including inviscid and viscous Newtonian fluids, as well as non-linear hyperelastic and elastoplastic solids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19298v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Ferrari, Ilya Peshkov, Evgeniy Romenski, Michael Dumbser</dc:creator>
    </item>
    <item>
      <title>Puzzle game: Prediction and Classification of Wordle Solution Words</title>
      <link>https://arxiv.org/abs/2403.19433</link>
      <description>arXiv:2403.19433v2 Announce Type: replace 
Abstract: In MCM/ICM 2023, we proposed a new result prediction model for the popular game Wordle launched by The New York Times. We first preprocessed the raw data and then established a prediction model based on ARIMA to predict the number of report results as of March 1, 2023. We selected word usage frequency, word information entropy, and the number of repeated letters contained in the word as the attributes of the word, and conducted a correlation analysis between these three attributes and the percentage of seven attempts. We also established a regression model based on the XGBoost algorithm, predicted the distribution of reported results, and predicted the correlation percentage of "EERIE". In addition, we also constructed a word classification model that classified words into "simple", "moderate", and "difficult", and explored the relationship between the three attributes and the classification results. Finally, we calculated the percentage of players in the dataset who needed 3 or more attempts for each word. The appendix provides relevant information and problems to be solved for the mathematical modeling competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19433v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haidong Xin, Fang Wu, Zhitong Zhou, Shujuan Wang</dc:creator>
    </item>
    <item>
      <title>Iterative Sketching for Secure Coded Regression</title>
      <link>https://arxiv.org/abs/2308.04185</link>
      <description>arXiv:2308.04185v2 Announce Type: replace-cross 
Abstract: Linear regression is a fundamental and primitive problem in supervised machine learning, with applications ranging from epidemiology to finance. In this work, we propose methods for speeding up distributed linear regression. We do so by leveraging randomized techniques, while also ensuring security and straggler resiliency in asynchronous distributed computing systems. Specifically, we randomly rotate the basis of the system of equations and then subsample blocks, to simultaneously secure the information and reduce the dimension of the regression problem. In our setup, the basis rotation corresponds to an encoded encryption in an approximate gradient coding scheme, and the subsampling corresponds to the responses of the non-straggling servers in the centralized coded computing framework. This results in a distributive iterative stochastic approach for matrix compression and steepest descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04185v2</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neophytos Charalambides, Hessam Mahdavifar, Mert Pilanci, Alfred O. Hero III</dc:creator>
    </item>
    <item>
      <title>The Collisional Particle-In-Cell Method for the Vlasov-Maxwell-Landau Equations</title>
      <link>https://arxiv.org/abs/2401.01689</link>
      <description>arXiv:2401.01689v2 Announce Type: replace-cross 
Abstract: We introduce an extension of the particle-in-cell (PIC) method that captures the Landau collisional effects in the Vlasov-Maxwell-Landau equations. The method arises from a regularisation of the variational formulation of the Landau equation, leading to a discretisation of the collision operator that conserves mass, charge, momentum, and energy, while increasing the (regularised) entropy. The collisional effects appear as a fully deterministic effective force, thus the method does not require any transport-collision splitting. The scheme can be used in arbitrary dimension, and for a general interaction, including the Coulomb case. We validate the scheme on scenarios such as the Landau damping, the two-stream instability, and the Weibel instability, demonstrating its effectiveness in the numerical simulation of plasma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01689v2</guid>
      <category>physics.plasm-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rafael Bailo, Jos\'e A. Carrillo, Jingwei Hu</dc:creator>
    </item>
  </channel>
</rss>
