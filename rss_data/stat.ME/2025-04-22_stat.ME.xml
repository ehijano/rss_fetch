<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Apr 2025 04:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A New Multiple Correlation Coefficient without Specifying the Dependent Variable</title>
      <link>https://arxiv.org/abs/2504.15372</link>
      <description>arXiv:2504.15372v1 Announce Type: new 
Abstract: Multiple correlation is a fundamental concept with broad applications. The classical multiple correlation coefficient is developed to assess how strongly a dependent variable is associated with a linear combination of independent variables. To compute this coefficient, the dependent variable must be chosen in advance. In many applications, however, it is difficult and even infeasible to specify the dependent variable, especially when some variables of interest are equally important. To overcome this difficulty, we propose a new coefficient of multiple correlation which (a) does not require the specification of the dependent variable, (b) has a simple formula and shares connections with the classical correlation coefficients, (c) consistently measures the linear correlation between continuous variables, which is 0 if and only if variables are uncorrelated and 1 if and only if one variable is a linear function of others, and (d) has an asymptotic distribution which can be used for hypothesis testing. We study the asymptotic behavior of the sample coefficient under mild regularity conditions. Given that the asymptotic bias of the sample coefficient is not negligible when the data dimension and the sample size are comparable, we propose a bias-corrected estimator that consistently performs well in such cases. Moreover, we develop an efficient strategy for making inferences on multiple correlation based on either the limiting distribution or the resampling methods and the stochastic approximation Monte Carlo algorithm, depending on whether the regularity assumptions are valid or not. Theoretical and numerical studies demonstrate that our coefficient provides a useful tool for evaluating multiple correlation in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15372v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Yang, Yuhong Zhou, Wei Xu, Kirsten Beyer</dc:creator>
    </item>
    <item>
      <title>Assessing Surrogate Heterogeneity in Real World Data Using Meta-Learners</title>
      <link>https://arxiv.org/abs/2504.15386</link>
      <description>arXiv:2504.15386v1 Announce Type: new 
Abstract: Surrogate markers are most commonly studied within the context of randomized clinical trials. However, the need for alternative outcomes extends beyond these settings and may be more pronounced in real-world public health and social science research, where randomized trials are often impractical. Research on identifying surrogates in real-world non-randomized data is scarce, as available statistical approaches for evaluating surrogate markers tend to rely on the assumption that treatment is randomized. While the few methods that allow for non-randomized treatment/exposure appropriately handle confounding individual characteristics, they do not offer a way to examine surrogate heterogeneity with respect to patient characteristics. In this paper, we propose a framework to assess surrogate heterogeneity in real-world, i.e., non-randomized, data and implement this framework using various meta-learners. Our approach allows us to quantify heterogeneity in surrogate strength with respect to patient characteristics while accommodating confounders through the use of flexible, off-the-shelf machine learning methods. In addition, we use our framework to identify individuals for whom the surrogate is a valid replacement of the primary outcome. We examine the performance of our methods via a simulation study and application to examine heterogeneity in the surrogacy of hemoglobin A1c as a surrogate for fasting plasma glucose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15386v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rebecca Knowlton, Layla Parast</dc:creator>
    </item>
    <item>
      <title>Deep learning with missing data</title>
      <link>https://arxiv.org/abs/2504.15388</link>
      <description>arXiv:2504.15388v1 Announce Type: new 
Abstract: In the context of multivariate nonparametric regression with missing covariates, we propose Pattern Embedded Neural Networks (PENNs), which can be applied in conjunction with any existing imputation technique. In addition to a neural network trained on the imputed data, PENNs pass the vectors of observation indicators through a second neural network to provide a compact representation. The outputs are then combined in a third neural network to produce final predictions. Our main theoretical result exploits an assumption that the observation patterns can be partitioned into cells on which the Bayes regression function behaves similarly, and belongs to a compositional H\"older class. It provides a finite-sample excess risk bound that holds for an arbitrary missingness mechanism, and in combination with a complementary minimax lower bound, demonstrates that our PENN estimator attains in typical cases the minimax rate of convergence as if the cells of the partition were known in advance, up to a poly-logarithmic factor in the sample size. Numerical experiments on simulated, semi-synthetic and real data confirm that the PENN estimator consistently improves, often dramatically, on standard neural networks without pattern embedding. Code to reproduce our experiments, as well as a tutorial on how to apply our method, is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15388v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Ma, Tengyao Wang, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>A stochastic method to estimate a zero-inflated two-part mixed model for human microbiome data</title>
      <link>https://arxiv.org/abs/2504.15411</link>
      <description>arXiv:2504.15411v1 Announce Type: new 
Abstract: Human microbiome studies based on genetic sequencing techniques produce compositional longitudinal data of the relative abundances of microbial taxa over time, allowing to understand, through mixed-effects modeling, how microbial communities evolve in response to clinical interventions, environmental changes, or disease progression. In particular, the Zero-Inflated Beta Regression (ZIBR) models jointly and over time the presence and abundance of each microbe taxon, considering the compositional nature of the data, its skewness, and the over-abundance of zeros. However, as for other complex random effects models, maximum likelihood estimation suffers from the intractability of likelihood integrals. Available estimation methods rely on log-likelihood approximation, which is prone to potential limitations such as biased estimates or unstable convergence. In this work we develop an alternative maximum likelihood estimation approach for the ZIBR model, based on the Stochastic Approximation Expectation Maximization (SAEM) algorithm. The proposed methodology allows to model unbalanced data, which is not always possible in existing approaches. We also provide estimations of the standard errors and the log-likelihood of the fitted model. The performance of the algorithm is established through simulation, and its use is demonstrated on two microbiome studies, showing its ability to detect changes in both presence and abundance of bacterial taxa over time and in response to treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15411v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Barrera, Cristian Meza, Ana Arribas-Gil</dc:creator>
    </item>
    <item>
      <title>Micro-randomized Trials with Categorical Treatments: Causal Effect Estimation and Sample Size Calculation</title>
      <link>https://arxiv.org/abs/2504.15484</link>
      <description>arXiv:2504.15484v1 Announce Type: new 
Abstract: Micro-randomized trials (MRTs) are widely used to assess the marginal and moderated effect of mobile health (mHealth) treatments delivered via mobile devices. In many applications, the mHealth treatments are categorical with multiple levels such as different types of message contents, but existing analysis and sample size calculation methods for MRTs only focus on binary treatment options (i.e., prompt vs. no prompt). We extended the causal excursion effect definition and the weighted and centered least squares estimator to MRTs with categorical treatments. Furthermore, we developed a sample size formula for comparing categorical treatment levels, and proved the type I error and power guarantee under working assumptions. We conducted extensive simulations to assess type I error and power under assumption violations, and we provided practical guidelines for using the sample size formula to ensure adequate power in most real-world scenarios. We illustrated the proposed estimator and sample size formula using the HeartSteps MRT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15484v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Lin, Tianchen Qian</dc:creator>
    </item>
    <item>
      <title>Intra-Class Correlation Coefficient Ignorable Clustered Randomized Trials for Detecting Treatment Effect Heterogeneity</title>
      <link>https://arxiv.org/abs/2504.15503</link>
      <description>arXiv:2504.15503v1 Announce Type: new 
Abstract: Accurately estimating the intra-class correlation coefficient (ICC) is crucial for adequately powering clustered randomized trials (CRTs). Challenges arise due to limited prior data on the specific outcome within the target population, making accurate ICC estimation difficult. Furthermore, ICC can vary significantly across studies, even for the same outcome, influenced by factors like study design, participant characteristics, and the specific intervention. Power calculations are extremely sensitive to ICC assumptions. Minor variation in the assumed ICC can lead to large differences in the number of clusters needed, potentially impacting trial feasibility and cost.
  This paper identifies a special class of CRTs aiming to detect the treatment effect heterogeneity, wherein the ICC can be completely disregarded in calculation of power and sample size. This result offers a solution for research projects lacking preliminary estimates of the ICC or facing challenges in their estimate. Moreover, this design facilitates power improvement through increasing the cluster sizes rather than the number of clusters, making it particular advantageous in the situations where expanding the number of clusters is difficult or costly.
  This paper provides a rigorous theoretical foundation for this class of ICC-ignorable CRTs, including mathematical proofs and practical guidance for implementation. We also present illustrative examples to demonstrate the practical implications of this approach in various research contexts in healthcare delivery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15503v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Yang, M\'arcio A. Diniz, Deukwoo Kwon, Madhu Mazumdar</dc:creator>
    </item>
    <item>
      <title>Ridge-Regularized Largest Root Test for General High-dimensional Double Wishart Problems</title>
      <link>https://arxiv.org/abs/2504.15510</link>
      <description>arXiv:2504.15510v1 Announce Type: new 
Abstract: In multivariate analysis, many core problems involve the eigen-analysis of an \(F\)-matrix, \(\bF = \bW_1\bW_2^{-1}\), constructed from two Wishart matrices, \(\bW_1\) and \(\bW_2\). These so-called \textit{Double Wishart problems} arise in contexts such as MANOVA, covariance matrix equality testing, and hypothesis testing in multivariate linear regression. A prominent classical approach, Roy's largest root test, relies on the largest eigenvalue of \(\bF\) for inference. However, in high-dimensional settings, this test becomes impractical due to the singularity or near-singularity of \(\bW_2\). To address this challenge, we propose a ridge-regularization framework by introducing a ridge term to \(\bW_2\). Specifically, we develop a family of ridge-regularized largest root tests, leveraging the largest eigenvalue of \(\bF_\lambda = \bW_1(\bW_2 + \lambda I)^{-1}\), where \(\lambda &gt; 0\) is the regularization parameter. Under mild assumptions, we establish the asymptotic Tracy-Widom distribution of the largest eigenvalue of \(\bF_\lambda\) after appropriate scaling. An efficient method for estimating the scaling parameters is proposed using the Mar\v{c}enko-Pastur equation, and the consistency of these estimators is proven. The proposed framework is applied to illustrative Double Wishart problems, and simulation studies are conducted to evaluate the numerical performance of the methods. Finally, the proposed method is applied to the \emph{Human Connectome Project} data to test for the presence of associations between volumetric measurements of human brain and behavioral variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15510v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoran Li</dc:creator>
    </item>
    <item>
      <title>Bayesian information theoretic model-averaging stochastic item selection for computer adaptive testing: compromise-free item exposure</title>
      <link>https://arxiv.org/abs/2504.15543</link>
      <description>arXiv:2504.15543v1 Announce Type: new 
Abstract: The goal of Computer Adaptive Testing (CAT) is to reliably estimate an individual's ability as modeled by an item response theory (IRT) instrument using only a subset of the instrument's items. A secondary goal is to vary the items presented across different testing sessions so that the sequence of items does not become overly stereotypical -- we want all items to have an exposure rate sufficiently far from zero. We formulate the optimization problem for CAT in terms of Bayesian information theory, where one chooses the item at each step based on the criterion of the ability model discrepancy -- the statistical distance between the ability estimate at the next step and the full-test ability estimate. This viewpoint of CAT naturally motivates a stochastic selection procedure that equates choosing the next item to sampling from a model-averaging ensemble ability model. Using the NIH Work Disability Functional Assessment Battery (WD-FAB), we evaluate our new methods in comparison to pre-existing methods found in the literature. We find that our stochastic selector has superior properties in terms of both item exposure and test accuracy/efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15543v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua C. Chang, Edison Choe</dc:creator>
    </item>
    <item>
      <title>Joint leave-group-out cross-validation in Bayesian spatial models</title>
      <link>https://arxiv.org/abs/2504.15586</link>
      <description>arXiv:2504.15586v1 Announce Type: new 
Abstract: Cross-validation (CV) is a widely-used method of predictive assessment based on repeated model fits to different subsets of the available data. CV is applicable in a wide range of statistical settings. However, in cases where data are not exchangeable, the design of CV schemes should account for suspected correlation structures within the data. CV scheme designs include the selection of left-out blocks and the choice of scoring function for evaluating predictive performance.
  This paper focuses on the impact of two scoring strategies for block-wise CV applied to spatial models with Gaussian covariance structures. We investigate, through several experiments, whether evaluating the predictive performance of blocks of left-out observations jointly, rather than aggregating individual (pointwise) predictions, improves model selection performance. Extending recent findings for data with serial correlation (such as time-series data), our experiments suggest that joint scoring reduces the variability of CV estimates, leading to more reliable model selection, particularly when spatial dependence is strong and model differences are subtle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15586v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Cooper, Aki Vehtari, Catherine Forbes</dc:creator>
    </item>
    <item>
      <title>Interacting Immediate Neighbour Interpolation for Geoscientific Data</title>
      <link>https://arxiv.org/abs/2504.15781</link>
      <description>arXiv:2504.15781v1 Announce Type: new 
Abstract: A diverse range of interpolation methods, including Kriging, spline/minimum curvature and radial basis function interpolation exist for interpolating spatially incomplete geoscientific data. Such methods use various spatial properties of the observed data to infer its local and global behaviour. In this study, we exploit the adaptability of locally interacting systems from statistical physics and develop an interpolation framework for numerical geoscientific data called Interacting Immediate Neighbour Interpolation (IINI), which solely relies on local and immediate neighbour correlations. In the IINI method, medium-to-long range correlations are constructed from the collective local interactions of grid centroids. To demonstrate the functionality and strengths of IINI, we apply our methodology to the interpolation of ground gravity, airborne magnetic and airborne radiometric datasets. We further compare the performance of IINI to conventional methods such as minimum curvature surface fitting. Results show that IINI is competitive with conventional interpolation techniques in terms of validation accuracy, while being significantly simpler in terms of algorithmic complexity and data pre-processing requirements. IINI demonstrates the broader applicability of statistical physics concepts within the field of geostatistics, highlighting their potential to enrich and expand traditional geostatistical methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15781v1</guid>
      <category>stat.ME</category>
      <category>physics.geo-ph</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arya Kimiaghalam, Andrei Swidinsky, Mohammad Parsa</dc:creator>
    </item>
    <item>
      <title>Residual lifetime prediction for heterogeneous degradation data by Bayesian semi-parametric method</title>
      <link>https://arxiv.org/abs/2504.15794</link>
      <description>arXiv:2504.15794v1 Announce Type: new 
Abstract: Degradation data are considered for assessing reliability in highly reliable systems. The usual assumption is that degradation units come from a homogeneous population. But in presence of high variability in the manufacturing process, this assumption is not true in general; that is different sub-populations are involved in the study. Predicting residual lifetime of a functioning unit is a major challenge in the degradation modeling especially in heterogeneous environment. To account for heterogeneous degradation data, we have proposed a Bayesian semi-parametric approach to relax the conventional modeling assumptions. We model the degradation path using Dirichlet process mixture of normal distributions. Based on the samples obtained from posterior distribution of model parameters we obtain residual lifetime distribution for individual unit. Transformation based MCMC technique is used for simulating values from the derived residual lifetime distribution for prediction of residual lifetime. A simulation study is undertaken to check performance of the proposed semi-parametric model compared with parametric model. Fatigue Crack Size data is analyzed to illustrate the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15794v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Barin Karmakar, Biswabrata Pradhan</dc:creator>
    </item>
    <item>
      <title>Causal machine learning for high-dimensional mediation analysis using interventional effects mapped to a target trial</title>
      <link>https://arxiv.org/abs/2504.15834</link>
      <description>arXiv:2504.15834v1 Announce Type: new 
Abstract: Causal mediation analysis examines causal pathways linking exposures to disease. The estimation of interventional effects, which are mediation estimands that overcome certain identifiability problems of natural effects, has been advanced through causal machine learning methods, particularly for high-dimensional mediators. Recently, it has been proposed interventional effects can be defined in each study by mapping to a target trial assessing specific hypothetical mediator interventions. This provides an appealing framework to directly address real-world research questions about the extent to which such interventions might mitigate an increased disease risk in the exposed. However, existing estimators for interventional effects mapped to a target trial rely on singly-robust parametric approaches, limiting their applicability in high-dimensional settings. Building upon recent developments in causal machine learning for interventional effects, we address this gap by developing causal machine learning estimators for three interventional effect estimands, defined by target trials assessing hypothetical interventions inducing distinct shifts in joint mediator distributions. These estimands are motivated by a case study within the Longitudinal Study of Australian Children, used for illustration, which assessed how intervening on high inflammatory burden and other non-inflammatory adverse metabolomic markers might mitigate the adverse causal effect of overweight or obesity on high blood pressure in adolescence. We develop one-step and (partial) targeted minimum loss-based estimators based on efficient influence functions of those estimands, demonstrating they are root-n consistent, efficient, and multiply robust under certain conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15834v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tong Chen, Stijn Vansteelandt, David Burgner, Toby Mansell, Margarita Moreno-Betancur</dc:creator>
    </item>
    <item>
      <title>Multiscale detection of practically significant changes in a gradually varying time series</title>
      <link>https://arxiv.org/abs/2504.15872</link>
      <description>arXiv:2504.15872v1 Announce Type: new 
Abstract: In many change point problems it is reasonable to assume that compared to a benchmark at a given time point $t_0$ the properties of the observed stochastic process change gradually over time for $t &gt;t_0$. Often, these gradual changes are not of interest as long as they are small (nonrelevant), but one is interested in the question if the deviations are practically significant in the sense that the deviation of the process compared to the time $t_0$ (measured by an appropriate metric) exceeds a given threshold, which is of practical significance (relevant change).
  In this paper we develop novel and powerful change point analysis for detecting such deviations in a sequence of gradually varying means, which is compared with the average mean from a previous time period. Current approaches to this problem suffer from low power, rely on the selection of smoothing parameters and require a rather regular (smooth) development for the means. We develop a multiscale procedure that alleviates all these issues, validate it theoretically and demonstrate its good finite sample performance on both synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15872v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Bastian, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Multivariate Poisson intensity estimation via low-rank tensor decomposition</title>
      <link>https://arxiv.org/abs/2504.15879</link>
      <description>arXiv:2504.15879v1 Announce Type: new 
Abstract: In this work, we introduce new matrix- and tensor-based methodologies for estimating multivariate intensity functions of spatial point processes. By modeling intensity functions as infinite-rank tensors within function spaces, we develop new algorithms to reveal optimal bias-variance trade-off for infinite-rank tensor estimation. Our methods dramatically enhance estimation accuracy while simultaneously reducing computational complexity. To our knowledge, this work marks the first application of matrix and tensor techinques to spatial point processes. Extensive numerical experiments further demonstrate that our techniques consistently outperform current state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15879v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haotian Xu, Carlos Misael Madrid Padilla, Oscar Hernan Madrid Padilla, Daren Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian sample size calculations for external validation studies of risk prediction models</title>
      <link>https://arxiv.org/abs/2504.15923</link>
      <description>arXiv:2504.15923v1 Announce Type: cross 
Abstract: Summary: Contemporary sample size calculations for external validation of risk prediction models require users to specify fixed values of assumed model performance metrics alongside target precision levels (e.g., 95% CI widths). However, due to the finite samples of previous studies, our knowledge of true model performance in the target population is uncertain, and so choosing fixed values represents an incomplete picture. As well, for net benefit (NB) as a measure of clinical utility, the relevance of conventional precision-based inference is doubtful. In this work, we propose a general Bayesian algorithm for constructing the joint distribution of predicted risks and response values based on summary statistics of model performance in previous studies. For statistical metrics of performance, we propose sample size determination rules that either target desired expected precision, or a desired assurance probability that the precision criteria will be satisfied. For NB, we propose rules based on optimality assurance (the probability that the planned study correctly identifies the most beneficial strategy) and the Expected Value of Sample Information (EVSI), the expected gain in NB from the planned validation study. We showcase these developments in a case study on the validation of a risk prediction model for deterioration of hospitalized COVID-19 patients. Compared to the conventional sample size calculation methods, a Bayesian approach requires explicit quantification of uncertainty around model performance, but thereby enables various sample size rules based on expected precision, assurance probabilities, and value of information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15923v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadatsafavi, Paul Gustafson, Solmaz Setayeshgar, Laure Wynants, Richard Riley</dc:creator>
    </item>
    <item>
      <title>Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series</title>
      <link>https://arxiv.org/abs/2304.03069</link>
      <description>arXiv:2304.03069v4 Announce Type: replace 
Abstract: The real life time series are usually nonstationary, bringing a difficult question of model adaptation. Classical approaches like ARMA-ARCH assume arbitrary type of dependence. To avoid their bias, we will focus on recently proposed agnostic philosophy of moving estimator: in time $t$ finding parameters optimizing e.g. $F_t=\sum_{\tau&lt;t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$ moving log-likelihood, evolving in time. It allows for example to estimate parameters using inexpensive exponential moving averages (EMA), like absolute central moments $m_p=E[|x-\mu|^p]$ evolving for one or multiple powers $p\in\mathbb{R}^+$ using $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$. Application of such general adaptive methods of moments will be presented on Student's t-distribution, popular especially in economical applications, here applied to log-returns of DJIA companies. While standard ARMA-ARCH approaches provide evolution of $\mu$ and $\sigma$, here we also get evolution of $\nu$ describing $\rho(x)\sim |x|^{-\nu-1}$ tail shape, probability of extreme events - which might turn out catastrophic, destabilizing the market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03069v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jarek Duda</dc:creator>
    </item>
    <item>
      <title>Guidance on Individualized Treatment Rule Estimation in High Dimensions</title>
      <link>https://arxiv.org/abs/2306.16402</link>
      <description>arXiv:2306.16402v3 Announce Type: replace 
Abstract: Individualized treatment rules, cornerstones of precision medicine, inform patient treatment decisions with the goal of optimizing patient outcomes. These rules are generally unknown functions of patients' pre-treatment covariates, meaning they must be estimated from clinical or observational study data. Myriad methods have been developed to learn these rules, and these procedures are demonstrably successful in traditional asymptotic settings with moderate number of covariates. The finite-sample performance of these methods in high-dimensional covariate settings, which are increasingly the norm in modern clinical trials, has not been well characterized, however. We perform a comprehensive comparison of state-of-the-art individualized treatment rule estimators, assessing performance on the basis of the estimators' accuracy, interpretability, and computational efficiency. Sixteen data-generating processes with continuous outcomes and binary treatment assignments are considered, reflecting a diversity of randomized and observational studies. We summarize our findings and provide succinct advice to practitioners needing to estimate individualized treatment rules in high dimensions. All code is made publicly available, facilitating modifications and extensions to our simulation study. A novel pre-treatment covariate filtering procedure is also proposed and is shown to improve estimators' accuracy and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16402v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Boileau, Ning Leng, Sandrine Dudoit</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Lasso in Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2403.19515</link>
      <description>arXiv:2403.19515v3 Announce Type: replace 
Abstract: Generalized linear models or GLM constitute plethora of sub-models which extends the ordinary linear regression by connecting the mean of response variable with the covariates through appropriate link functions. On the other hand, Lasso is a popular and easy-to-implement penalization method in regression when not all covariates are relevant. However, Lasso does not generally have a tractable asymptotic distribution (Knight and Fu (2000)). In this paper, we develop a Bootstrap method which works as an alternative to the asymptotic distribution of Lasso for all the submodels of GLM. We support our theoretical findings by showing good finite-sample properties of the proposed Bootstrap method through a moderately large simulation study. We also implement our method on a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19515v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mayukh Choudhury, Debraj Das</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Genomic Data with Multiple Heterogeneous Outcomes</title>
      <link>https://arxiv.org/abs/2404.09119</link>
      <description>arXiv:2404.09119v4 Announce Type: replace 
Abstract: With the evolution of single-cell RNA sequencing techniques into a standard approach in genomics, it has become possible to conduct cohort-level causal inferences based on single-cell-level measurements. However, the individual gene expression levels of interest are not directly observable; instead, only repeated proxy measurements from each individual's cells are available, providing a derived outcome to estimate the underlying outcome for each of many genes. In this paper, we propose a generic semiparametric inference framework for doubly robust estimation with multiple derived outcomes, which also encompasses the usual setting of multiple outcomes when the response of each unit is available. To reliably quantify the causal effects of heterogeneous outcomes, we specialize the analysis to standardized average treatment effects and quantile treatment effects. Through this, we demonstrate the use of the semiparametric inferential results for doubly robust estimators derived from both Von Mises expansions and estimating equations. A multiple testing procedure based on Gaussian multiplier bootstrap is tailored for doubly robust estimators to control the false discovery exceedance rate. Applications in single-cell CRISPR perturbation analysis and individual-level differential expression analysis demonstrate the utility of the proposed methods and offer insights into the usage of different estimands for causal inference in genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09119v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Zhenghao Zeng, Edward H. Kennedy, Larry Wasserman, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Factor pre-training in Bayesian multivariate logistic models</title>
      <link>https://arxiv.org/abs/2409.17441</link>
      <description>arXiv:2409.17441v2 Announce Type: replace 
Abstract: This article focuses on inference in logistic regression for high-dimensional binary outcomes. A popular approach induces dependence across the outcomes by including latent factors in the linear predictor. Bayesian approaches are useful for characterizing uncertainty in inferring the regression coefficients, factors and loadings, while also incorporating hierarchical and shrinkage structure. However, Markov chain Monte Carlo algorithms for posterior computation face challenges in scaling to high-dimensional outcomes. Motivated by applications in ecology, we exploit a blessing of dimensionality to motivate pre-estimation of the latent factors. Conditionally on the factors, the outcomes are modeled via independent logistic regressions. We implement Gaussian approximations in parallel in inferring the posterior on the regression coefficients and loadings, including a simple adjustment to obtain credible intervals with valid frequentist coverage. We show posterior concentration properties and excellent empirical performance in simulations. The methods are applied to insect biodiversity data in Madagascar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17441v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Mauri, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>On testing for independence between generalized error models of several time series</title>
      <link>https://arxiv.org/abs/2410.24003</link>
      <description>arXiv:2410.24003v2 Announce Type: replace 
Abstract: We define generalized innovations associated with generalized error models having arbitrary distributions, that is, distributions that can be mixtures of continuous and discrete distributions. These models include stochastic volatility models and regime-switching models. We also propose statistics for testing independence between the generalized errors of these models, extending previous results of Duchesne, Ghoudi and Remillard (2012) obtained for stochastic volatility models. We define families of empirical processes constructed from lagged generalized errors, and we show that their joint asymptotic distributions are Gaussian and independent of the estimated parameters of the individual time series. Moebius transformations of the empirical processes are used to obtain tractable covariances. Several tests statistics are then proposed, based on Cramer-von Mises statistics and dependence measures, as well as graphical methods to visualize the dependence. In addition, numerical experiments are performed to assess the power of the proposed tests. Finally, to show the usefulness of our methodologies, examples of applications for financial data and crime data are given to cover both discrete and continuous cases. ll developed methodologies are implemented in the CRAN package IndGenErrors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24003v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kilani Ghoudi, Bouchra R. Nasri, Bruno N. Remillard</dc:creator>
    </item>
    <item>
      <title>A conceptual synthesis of causal assumptions for causal discovery and inference</title>
      <link>https://arxiv.org/abs/2504.11035</link>
      <description>arXiv:2504.11035v2 Announce Type: replace 
Abstract: This work presents a conceptual synthesis of causal discovery and inference frameworks, with a focus on how foundational assumptions -- causal sufficiency, causal faithfulness, and the causal Markov condition -- are formalized and operationalized across methodological traditions. Through structured tables and comparative summaries, I map core assumptions, tasks, and analytical choices from multiple causal frameworks, highlighting their connections and differences. The synthesis provides practical guidance for researchers designing causal studies, especially in settings where observational or experimental constraints challenge standard approaches. This guide spans all phases of causal analysis, including question formulation, formalization of background knowledge, selection of appropriate frameworks, choice of study design or algorithm, and interpretation. It is intended as a tool to support rigorous causal reasoning across diverse empirical domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11035v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hannah E. Correia</dc:creator>
    </item>
    <item>
      <title>Bringing closure to FDR control: beating the e-Benjamini-Hochberg procedure</title>
      <link>https://arxiv.org/abs/2504.11759</link>
      <description>arXiv:2504.11759v2 Announce Type: replace 
Abstract: False discovery rate (FDR) has been a key metric for error control in multiple hypothesis testing, and many methods have developed for FDR control across a diverse cross-section of settings and applications. We develop a closure principle for all FDR controlling procedures, i.e., we provide a characterization based on e-values for all admissible FDR controlling procedures. A general version of this closure principle can recover any multiple testing error metric and allows one to choose the error metric post-hoc. We leverage this idea to formulate the closed eBH procedure, a (usually strict) improvement over the eBH procedure for FDR control when provided with e-values. This also yields a closed BY procedure that dominates the Benjamini-Yekutieli (BY) procedure for FDR control with arbitrarily dependent p-values, thus proving that the latter is inadmissibile. We demonstrate the practical performance of our new procedures in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11759v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A Scoping Review of Earth Observation and Machine Learning for Causal Inference: Implications for the Geography of Poverty</title>
      <link>https://arxiv.org/abs/2406.02584</link>
      <description>arXiv:2406.02584v4 Announce Type: replace-cross 
Abstract: Earth observation (EO) data such as satellite imagery can have far-reaching impacts on our understanding of the geography of poverty, especially when coupled with machine learning (ML) and computer vision. Early research used computer vision to predict living conditions in areas with limited data, but recent studies increasingly focus on causal analysis. Despite this shift, the use of EO-ML methods for causal inference lacks thorough documentation, and best practices are still developing. Through a comprehensive scoping review, we catalog the current literature on EO-ML methods in causal analysis. We synthesize five principal approaches to incorporating EO data in causal workflows: (1) outcome imputation for downstream causal analysis, (2) EO image deconfounding, (3) EO-based treatment effect heterogeneity, (4) EO-based transportability analysis, and (5) image-informed causal discovery. Building on these findings, we provide a detailed protocol guiding researchers in integrating EO data into causal analysis -- covering data requirements, computer vision model selection, and evaluation metrics. While our focus centers on health and living conditions outcomes, our protocol is adaptable to other sustainable development domains utilizing EO data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02584v4</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kazuki Sakamoto, Connor T. Jerzak, Adel Daoud</dc:creator>
    </item>
    <item>
      <title>Estimation of conditional inequality curves and measures via estimating the conditional quantile function</title>
      <link>https://arxiv.org/abs/2412.20228</link>
      <description>arXiv:2412.20228v3 Announce Type: replace-cross 
Abstract: The classical concept of inequality curves and measures is extended to conditional inequality curves and measures and a curve of conditional inequality measures is introduced. This extension provides a more nuanced analysis of inequality in relation to covariates. In particular, this enables comparison of inequalities between subpopulations, conditioned on certain values of covariates. To estimate the curves and measures, a novel method for estimating the conditional quantile function is proposed. The method incorporates a modified quantile regression framework that employs isotonic regression to ensure that there is no quantile crossing. The consistency of the proposed estimators is proved while their finite sample performance is evaluated through simulation studies and compared with existing quantile regression approaches. Finally, practical application is demonstrated by analysing salary inequality across different employee age groups, highlighting the potential of conditional inequality measures in empirical research. The code used to prepare the results presented in this article is available in a dedicated GitHub repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20228v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicja Jokiel-Rokita, Sylwester Pi\k{a}tek, Rafa{\l} Topolnicki</dc:creator>
    </item>
    <item>
      <title>Debiasing Functions of Private Statistics in Postprocessing</title>
      <link>https://arxiv.org/abs/2502.13314</link>
      <description>arXiv:2502.13314v2 Announce Type: replace-cross 
Abstract: Given a differentially private unbiased estimate $\tilde{q}=q(D) +\nu$ of a statistic $q(D)$, we wish to obtain unbiased estimates of functions of $q(D)$, such as $1/q(D)$, solely through post-processing of $\tilde{q}$, with no further access to the confidential dataset $D$. To this end, we adapt the deconvolution method used for unbiased estimation in the statistical literature, deriving unbiased estimators for a broad family of twice-differentiable functions when the privacy-preserving noise $\nu$ is drawn from the Laplace distribution (Dwork et al., 2006). We further extend this technique to a more general class of functions, deriving approximately optimal estimators that are unbiased for values in a user-specified interval (possibly extending to $\pm \infty$). We use these results to derive an unbiased estimator for private means when the size $n$ of the dataset is not publicly known. In a numerical application, we find that a mechanism that uses our estimator to return an unbiased sample size and mean outperforms a mechanism that instead uses the previously known unbiased privacy mechanism for such means (Kamath et al., 2023). We also apply our estimators to develop unbiased transformation mechanisms for per-record differential privacy, a privacy concept in which the privacy guarantee is a public function of a record's value (Seeman et al., 2024). Our mechanisms provide stronger privacy guarantees than those in prior work (Finley et al., 2024) by using Laplace, rather than Gaussian, noise. Finally, using a different approach, we go beyond Laplace noise by deriving unbiased estimators for polynomials under the weak condition that the noise distribution has sufficiently many moments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13314v2</guid>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Flavio Calmon, Elbert Du, Cynthia Dwork, Brian Finley, Grigory Franguridi</dc:creator>
    </item>
    <item>
      <title>How Much Weak Overlap Can Doubly Robust T-Statistics Handle?</title>
      <link>https://arxiv.org/abs/2504.13273</link>
      <description>arXiv:2504.13273v2 Announce Type: replace-cross 
Abstract: In the presence of sufficiently weak overlap, it is known that no regular root-n-consistent estimators exist and standard estimators may fail to be asymptotically normal. This paper shows that a thresholded version of the standard doubly robust estimator is asymptotically normal with well-calibrated Wald confidence intervals even when constructed using nonparametric estimates of the propensity score and conditional mean outcome. The analysis implies a cost of weak overlap in terms of black-box nuisance rates, borne when the semiparametric bound is infinite, and the contribution of outcome smoothness to the outcome regression rate, which is incurred even when the semiparametric bound is finite. As a byproduct of this analysis, I show that under weak overlap, the optimal global regression rate is the same as the optimal pointwise regression rate, without the usual polylogarithmic penalty. The high-level conditions yield new rules of thumb for thresholding in practice. In simulations, thresholded AIPW can exhibit moderate overrejection in small samples, but I am unable to reject a null hypothesis of exact coverage in large samples. In an empirical application, the clipped AIPW estimator that targets the standard average treatment effect yields similar precision to a heuristic 10% fixed-trimming approach that changes the target sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13273v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Dorn</dc:creator>
    </item>
  </channel>
</rss>
