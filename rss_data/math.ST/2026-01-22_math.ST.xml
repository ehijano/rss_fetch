<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jan 2026 02:37:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Cluster size distributions of discrete random fields</title>
      <link>https://arxiv.org/abs/2601.14586</link>
      <description>arXiv:2601.14586v1 Announce Type: new 
Abstract: We study discrete random fields $\{X_t: t\in \mathbb{Z}^d\}$ parameterized on the $d$-dimensional integer lattice $\mathbb{Z}^d$. For a fixed threshold $u$, the excursion set $\{t \in \mathbb{Z}^d : X_t &gt; u\}$ decomposes into connected components or clusters, whose size, defined as the number of lattice points they contain, are random. This paper investigates the probability distribution of these cluster sizes. For stationary random fields, we derive exact expressions for the cluster size distribution. To address nonstationary settings, we introduce a peak-based cluster size distribution, which characterizes the distribution of cluster sizes conditional on the presence of a local maximum above $u$. This formulation provides a tractable alternative when exact cluster size distributions are analytically inaccessible. The proposed framework applies broadly to Gaussian and non-Gaussian random fields, relying only on their joint dependence structure. Our results provide a theoretical foundation for quantifying spatial extent in discretely sampled data, with applications to medical imaging, geoscience, environmental monitoring, and other scientific areas where thresholded random fields naturally arise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14586v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Cheng, John Ginos</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Inference for Sparsely Permuted Linear Regression</title>
      <link>https://arxiv.org/abs/2601.14872</link>
      <description>arXiv:2601.14872v2 Announce Type: new 
Abstract: We study a linear observation model with an unknown permutation called \textit{permuted/shuffled linear regression}, where responses and covariates are mismatched and the permutation forms a discrete, factorial-size parameter. The permutation is a key component of the data-generating process, yet its statistical investigation remains challenging due to its discrete nature. We develop a general statistical inference framework on the permutation and regression coefficients. First, we introduce a localization step that reduces the permutation space to a small candidate set building on recent advances in the repro samples method, whose miscoverage decays polynomially with the number of Monte Carlo samples. Then, based on this localized set, we provide statistical inference procedures: a conditional Monte Carlo test of permutation structures with valid finite-sample Type-I error control. We also develop coefficient inference that remains valid under alignment uncertainty of permutations. For computational purposes, we develop a linear assignment problem computable in polynomial time and demonstrate that, with high probability, the solution is equivalent to that of the conventional least squares with large computational cost. Extensions to partially permuted designs and ridge regularization are further discussed. Extensive simulations and an application to air-quality data corroborate finite-sample validity, strong power to detect mismatches, and practical scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14872v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hirofumi Ota, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Central subspace data depth</title>
      <link>https://arxiv.org/abs/2601.14947</link>
      <description>arXiv:2601.14947v1 Announce Type: new 
Abstract: Statistical data depth plays an important role in the analysis of multivariate data sets. The main outcome is a center-outward ordering of the observations that can be used both to highlight features of the underlying distribution of the data and as input to further statistical analysis. An important property of data depth is related to symmetric distributions as the point with the highest depth value, the center, coincides with the point of symmetry. However, there are applications in which it is more natural to consider symmetry with respect to a subspace of a certain dimension rather than to a point, i.e. a subspace of dimension zero. We provide a general framework to construct statistical data depths which attain maximum value in a subspace, providing a center-outward ordering from that subspace. We refer to these data depths as central subspace data depths. Moreover, if the distribution is symmetric with respect to a subspace, then the depth is maximized at that subspace. We introduce general notions of symmetry about a subspace for distributions, study the properties of central subspace data depths and provide asymptotic convergence for the corresponding sample versions. Additionally, we discuss connections with projection pursuit and dimension reduction. An application based on custom data fraud detection shows the importance of the proposed approach and strengthens its potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14947v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giacomo Francisci, Claudio Agostinelli</dc:creator>
    </item>
    <item>
      <title>Statistical Learning Theory for Distributional Classification</title>
      <link>https://arxiv.org/abs/2601.14818</link>
      <description>arXiv:2601.14818v1 Announce Type: cross 
Abstract: In supervised learning with distributional inputs in the two-stage sampling setup, relevant to applications like learning-based medical screening or causal learning, the inputs (which are probability distributions) are not accessible in the learning phase, but only samples thereof. This problem is particularly amenable to kernel-based learning methods, where the distributions or samples are first embedded into a Hilbert space, often using kernel mean embeddings (KMEs), and then a standard kernel method like Support Vector Machines (SVMs) is applied, using a kernel defined on the embedding Hilbert space. In this work, we contribute to the theoretical analysis of this latter approach, with a particular focus on classification with distributional inputs using SVMs. We establish a new oracle inequality and derive consistency and learning rate results. Furthermore, for SVMs using the hinge loss and Gaussian kernels, we formulate a novel variant of an established noise assumption from the binary classification literature, under which we can establish learning rates. Finally, some of our technical tools like a new feature space for Gaussian kernels on Hilbert spaces are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14818v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Fiedler</dc:creator>
    </item>
    <item>
      <title>Geostatistics from Elliptic Boundary-Value Problems: Green Operators, Transmission Conditions, and Schur Complements</title>
      <link>https://arxiv.org/abs/2601.14937</link>
      <description>arXiv:2601.14937v1 Announce Type: cross 
Abstract: Classical geostatistics encodes spatial dependence by prescribing variograms or covariance kernels on Euclidean domains, whereas the SPDE--GMRF paradigm specifies Gaussian fields through an elliptic precision operator whose inverse is the corresponding Green operator. We develop an operator-based formulation of Gaussian spatial random fields on bounded domains and manifolds with internal interfaces, treating boundary and transmission conditions as explicit components of the statistical model. Starting from coercive quadratic energy functionals, variational theory yields a precise precision--covariance correspondence and shows that variograms are derived quadratic functionals of the Green operator, hence depend on boundary conditions and domain geometry. Conditioning and kriging follow from standard Gaussian update identities in both covariance and precision form, with hard constraints represented equivalently by exact interpolation constraints or by distributional source terms. Interfaces are modelled via surface penalty terms; taking variations produces flux-jump transmission conditions and induces controlled attenuation of cross-interface covariance. Finally, boundary-driven prediction and domain reduction are formulated through Dirichlet-to-Neumann operators and Schur complements, providing an operator language for upscaling, change of support, and subdomain-to-boundary mappings. Throughout, we use tools standard in spatial statistics and elliptic PDE theory to keep boundary and interface effects explicit in covariance modeling and prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14937v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan J. Segura</dc:creator>
    </item>
    <item>
      <title>Efficient and Minimax-optimal In-context Nonparametric Regression with Transformers</title>
      <link>https://arxiv.org/abs/2601.15014</link>
      <description>arXiv:2601.15014v1 Announce Type: cross 
Abstract: We study in-context learning for nonparametric regression with $\alpha$-H\"older smooth regression functions, for some $\alpha&gt;0$. We prove that, with $n$ in-context examples and $d$-dimensional regression covariates, a pretrained transformer with $\Theta(\log n)$ parameters and $\Omega\bigl(n^{2\alpha/(2\alpha+d)}\log^3 n\bigr)$ pretraining sequences can achieve the minimax-optimal rate of convergence $O\bigl(n^{-2\alpha/(2\alpha+d)}\bigr)$ in mean squared error. Our result requires substantially fewer transformer parameters and pretraining sequences than previous results in the literature. This is achieved by showing that transformers are able to approximate local polynomial estimators efficiently by implementing a kernel-weighted polynomial basis and then running gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15014v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle Ching, Ioana Popescu, Nico Smith, Tianyi Ma, William G. Underwood, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Decomposing Determinantal Varieties from Statistics via Matroid Theory</title>
      <link>https://arxiv.org/abs/2601.15128</link>
      <description>arXiv:2601.15128v1 Announce Type: cross 
Abstract: We study determinantal varieties from conditional independence models with hidden variables, focusing on their irreducible decompositions, dimensions, degrees, and Gr\"obner bases. Each variety encodes a collection of matroids, whose flats capture algebraic dependencies among variables. Using this approach, we provide a systematic description of the components, their dimensions, and defining equations, and introduce a combinatorial framework for computing the degree of the determinantal variety. Our approach highlights the central role of matroidal structures in the study of determinantal varieties and extends beyond the reach of current computational techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15128v1</guid>
      <category>math.CO</category>
      <category>math.AC</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Per Alexandersson, Yulia Alexandr, Emiliano Liwski, Fatemeh Mohammadi, Pardis Semnani</dc:creator>
    </item>
    <item>
      <title>Multi-context principal component analysis</title>
      <link>https://arxiv.org/abs/2601.15239</link>
      <description>arXiv:2601.15239v1 Announce Type: cross 
Abstract: Principal component analysis (PCA) is a tool to capture factors that explain variation in data. Across domains, data are now collected across multiple contexts (for example, individuals with different diseases, cells of different types, or words across texts). While the factors explaining variation in data are undoubtedly shared across subsets of contexts, no tools currently exist to systematically recover such factors. We develop multi-context principal component analysis (MCPCA), a theoretical and algorithmic framework that decomposes data into factors shared across subsets of contexts. Applied to gene expression, MCPCA reveals axes of variation shared across subsets of cancer types and an axis whose variability in tumor cells, but not mean, is associated with lung cancer progression. Applied to contextualized word embeddings from language models, MCPCA maps stages of a debate on human nature, revealing a discussion between science and fiction over decades. These axes are not found by combining data across contexts or by restricting to individual contexts. MCPCA is a principled generalization of PCA to address the challenge of understanding factors underlying data across contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15239v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kexin Wang, Salil Bhate, Jo\~ao M. Pereira, Joe Kileel, Matylda Figlerowicz, Anna Seigal</dc:creator>
    </item>
    <item>
      <title>Fermat Distance-to-Measure: a robust Fermat-like metric</title>
      <link>https://arxiv.org/abs/2504.02381</link>
      <description>arXiv:2504.02381v2 Announce Type: replace 
Abstract: Given a probability measure with density, Fermat distances and density-driven metrics are conformal transformations of the Euclidean metric that shrink distances in high density areas and enlarge distances in low density areas. Although they have been widely studied and have shown to be useful in various machine learning tasks, they are limited to measures with density (with respect to Lebesgue measure, or volume form on manifold). In this paper, by replacing the density with the Distance-to-Measure, we introduce a new metric, the Fermat Distance-to-Measure, defined for any probability measure in R^d. We derive strong stability properties for the Fermat Distance-to-Measure with respect to the measure and propose an estimator from random sampling of the same measure, featuring an explicit bound on its convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02381v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'er\^ome Taupin (LMO, INRIA), Fr\'ed\'eric Chazal (LMO, INRIA)</dc:creator>
    </item>
    <item>
      <title>Pivotal inference for linear predictions in stationary processes</title>
      <link>https://arxiv.org/abs/2508.21025</link>
      <description>arXiv:2508.21025v3 Announce Type: replace 
Abstract: In this paper we develop pivotal inference for the final (FPE) and relative final prediction error (RFPE) of linear forecasts in stationary processes. Our approach is based on a self-normalizing technique and avoids the estimation of the asymptotic variances of the empirical autocovariances. We provide pivotal confidence intervals for the (R)FPE, develop estimates for the minimal order of a linear prediction that is required to obtain a prespecified forecasting accuracy and also propose (pivotal) statistical tests for the hypotheses that the (R)FPE exceeds a given threshold. Additionally, we provide pivotal uncertainty quantification for the commonly used coefficient of determination $R^2$ obtained from a linear prediction based on the past $p \geq 1$ observations and develop new (pivotal) inference tools for the partial autocorrelation, which do not require the assumption of an autoregressive process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21025v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Holger Dette, Sebastian K\"uhnert</dc:creator>
    </item>
    <item>
      <title>An ordering for the strength of functional dependence</title>
      <link>https://arxiv.org/abs/2511.06498</link>
      <description>arXiv:2511.06498v2 Announce Type: replace 
Abstract: We introduce a new dependence order, termed the conditional convex order, whose minimal and maximal elements characterize independence and perfect dependence. Moreover, it characterizes conditional independence, satisfies information monotonicity, and exhibits several invariance properties. Consequently, it is an ordering for the strength of functional dependence of a random variable Y on a random vector X. As we show, various recently studied dependence measures -- including Chatterjee's rank correlation, Wasserstein correlations, and rearranged dependence measures -- are increasing in this order and inherit their fundamental properties from it. We characterize the conditional convex order by the Schur order and by the concordance order, and we verify it in settings such as additive error models, the multivariate normal distribution, and various copula-based models. Our results offer a unified perspective on the behavior of dependence measures across statistical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06498v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Ansari, Sebastian Fuchs</dc:creator>
    </item>
    <item>
      <title>Semidiscrete optimal transport with unknown costs</title>
      <link>https://arxiv.org/abs/2310.00786</link>
      <description>arXiv:2310.00786v4 Announce Type: replace-cross 
Abstract: Semidiscrete optimal transport is a challenging generalization of the classical transportation problem in linear programming. The goal is to design a joint distribution for two random variables (one continuous, one discrete) with fixed marginals, in a way that minimizes expected cost. We formulate a novel variant of this problem in which the cost functions are unknown, but can be learned through noisy observations; however, only one function can be sampled at a time. We develop a semi-myopic algorithm that couples online learning with stochastic approximation, and prove that it achieves optimal convergence rates, despite the non-smoothness of the stochastic gradient and the lack of strong concavity in the objective function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00786v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinchu Zhu, Ilya O. Ryzhov</dc:creator>
    </item>
    <item>
      <title>Dynamic angular synchronization under smoothness constraints</title>
      <link>https://arxiv.org/abs/2406.04071</link>
      <description>arXiv:2406.04071v3 Announce Type: replace-cross 
Abstract: Given an undirected measurement graph $\mathcal{H} = ([n], \mathcal{E})$, the classical angular synchronization problem consists of recovering unknown angles $\theta_1^*,\dots,\theta_n^*$ from a collection of noisy pairwise measurements of the form $(\theta_i^* - \theta_j^*) \mod 2\pi$, for all $\{i,j\} \in \mathcal{E}$. This problem arises in a variety of applications, including computer vision, time synchronization of distributed networks, and ranking from pairwise comparisons. In this paper, we consider a dynamic version of this problem where the angles, and also the measurement graphs evolve over $T$ time points. Assuming a smoothness condition on the evolution of the latent angles, we derive three algorithms for joint estimation of the angles over all time points. Moreover, for one of the algorithms, we establish non-asymptotic recovery guarantees for the mean-squared error (MSE) under different statistical models. In particular, we show that the MSE converges to zero as $T$ increases under milder conditions than in the static setting. This includes the setting where the measurement graphs are highly sparse and disconnected, and also when the measurement noise is large and can potentially increase with $T$. We complement our theoretical results with experiments on synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04071v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ernesto Araya, Mihai Cucuringu, Hemant Tyagi</dc:creator>
    </item>
    <item>
      <title>Multimodal data integration and cross-modal querying via orchestrated approximate message passing</title>
      <link>https://arxiv.org/abs/2407.19030</link>
      <description>arXiv:2407.19030v4 Announce Type: replace-cross 
Abstract: The need for multimodal data integration arises naturally when multiple complementary sets of features are measured on the same sample. Under a dependent multifactor model, we develop a fully data-driven orchestrated approximate message passing algorithm for integrating information across these feature sets to achieve statistically optimal signal recovery. In practice, these reference data sets are often queried later by new subjects that are only partially observed. Leveraging on asymptotic normality of estimates generated by our data integration method, we further develop an asymptotically valid prediction set for the latent representation of any such query subject. We demonstrate the prowess of both the data integration and the prediction set construction algorithms on both synthetic examples and real world single-cell datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19030v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagnik Nandy, Zongming Ma</dc:creator>
    </item>
    <item>
      <title>Possibilistic Instrumental Variable Regression</title>
      <link>https://arxiv.org/abs/2511.16029</link>
      <description>arXiv:2511.16029v2 Announce Type: replace-cross 
Abstract: Instrumental variable regression is a common approach for causal inference in the presence of unobserved confounding. However, identifying valid instruments is often difficult in practice. In this paper, we propose a novel method based on possibility theory that performs posterior inference on the treatment effect, conditional on a user-specified set of potential violations of the exogeneity assumption. Our method can provide informative results even when only a single, potentially invalid, instrument is available, offering a natural and principled framework for sensitivity analysis. Simulation experiments and a real-data application indicate strong performance of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16029v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gregor Steiner, Jeremie Houssineau, Mark F. J. Steel</dc:creator>
    </item>
    <item>
      <title>Convergence of Reflected Langevin Diffusion for Constrained Sampling</title>
      <link>https://arxiv.org/abs/2512.00386</link>
      <description>arXiv:2512.00386v2 Announce Type: replace-cross 
Abstract: We examine the Langevin diffusion confined to a closed, convex domain $D\subset\mathbb{R}^d$, represented as a reflected stochastic differential equation. We introduce a sequence of penalized stochastic differential equations and prove that their invariant measures converge, in Wasserstein-2 distance and with explicit polynomial rate, to the invariant measure of the reflected Langevin diffusion. We also analyze a time-discretization of the penalized process obtained via the Euler-Maruyama scheme and demonstrate the convergence to the original constrained measure. These results provide a rigorous approximation framework for reflected Langevin dynamics in both continuous and discrete time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00386v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarika Mane, Amine Boukardagha</dc:creator>
    </item>
    <item>
      <title>When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models</title>
      <link>https://arxiv.org/abs/2601.11444</link>
      <description>arXiv:2601.11444v2 Announce Type: replace-cross 
Abstract: Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11444v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rapha\"el Razafindralambo, R\'emy Sun, Fr\'ed\'eric Precioso, Damien Garreau, Pierre-Alexandre Mattei</dc:creator>
    </item>
    <item>
      <title>Optimal estimation of generalized causal effects in cluster-randomized trials with multiple outcomes</title>
      <link>https://arxiv.org/abs/2601.13428</link>
      <description>arXiv:2601.13428v2 Announce Type: replace-cross 
Abstract: Cluster-randomized trials (CRTs) are widely used to evaluate group-level interventions and increasingly collect multiple outcomes capturing complementary dimensions of benefit and risk. Investigators often seek a single global summary of treatment effect, yet existing methods largely focus on single-outcome estimands or rely on model-based procedures with unclear causal interpretation or limited robustness. We develop a unified potential outcomes framework for generalized treatment effects with multiple outcomes in CRTs, accommodating both non-prioritized and prioritized outcome settings. The proposed cluster-pair and individual-pair causal estimands are defined through flexible pairwise contrast functions and explicitly account for potentially informative cluster sizes. We establish nonparametric estimation via weighted clustered U-statistics and derive efficient influence functions to construct covariate-adjusted estimators that integrate debiased machine learning with U-statistics. The resulting estimators are consistent and asymptotically normal, attain the semiparametric efficiency bounds under mild regularity conditions, and have analytically tractable variance estimators that are proven to be consistent under cross-fitting. Simulations and an application to a CRT for chronic pain management illustrate the practical utility of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13428v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Fan Li</dc:creator>
    </item>
    <item>
      <title>Resampling-free Inference for Time Series via RKHS Embedding</title>
      <link>https://arxiv.org/abs/2601.13468</link>
      <description>arXiv:2601.13468v2 Announce Type: replace-cross 
Abstract: In this article, we study nonparametric inference problems in the context of multivariate or functional time series, including testing for goodness-of-fit, the presence of a change point in the marginal distribution, and the independence of two time series, among others. Most methodologies available in the existing literature address these problems by employing a bandwidth-dependent bootstrap or subsampling approach, which can be computationally expensive and/or sensitive to the choice of bandwidth. To address these limitations, we propose a novel class of kernel-based tests by embedding the data into a reproducing kernel Hilbert space, and construct test statistics using sample splitting, projection, and self-normalization (SN) techniques. Through a new conditioning technique, we demonstrate that our test statistics have pivotal limiting null distributions under strong mixing and mild moment assumptions. We also analyze the limiting power of our tests under local alternatives. Finally, we showcase the superior size accuracy and computational efficiency of our methods as compared to some existing ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13468v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deep Ghoshal, Xiaofeng Shao</dc:creator>
    </item>
  </channel>
</rss>
