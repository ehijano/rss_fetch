<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Sep 2024 01:58:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The content and structure of dreams are coupled to affect</title>
      <link>https://arxiv.org/abs/2409.14279</link>
      <description>arXiv:2409.14279v1 Announce Type: new 
Abstract: Dreams offer a unique window into the cognitive and affective dynamics of the sleeping and the waking mind. Recent quantitative linguistic approaches have shown promise in obtaining corpus-level measures of dream sentiment and topic occurrence. However, it is currently unclear how the affective content of individual dreams relates to their semantic content and structure. Here, we combine word embedding, topic modeling, and network analysis to investigate this relationship. By applying Discourse Atom Topic Modeling (DATM) to the DreamBank corpus of &gt;18K dream reports, we represent the latent themes arising within dreams as a sparse dictionary of topics and identify the affective associations of those topics. We show that variation in dream affect (valence and arousal) is associated with changes in topical content. By representing each dream report as a network of topics, we demonstrate that the affective content of dreams is also coupled to semantic structure. Specifically, positively valenced dreams exhibit more coherent, structured, and linear narratives, whilst negatively valenced dreams have more narrative loops and dominant topics. Additionally, topic networks of high arousal dreams are structurally dominated by few high arousal topics and incoherent topical connections, whereas low arousal dreams contain more loops. These findings suggest that affective processes are associated with both the content and structure of dreams. Our approach showcases the potential of integrating natural language processing and network analysis with psychology to elucidate the interplay of affect, cognition and narrative in dreams. This methodology has broad applications for the study of narrated experience and psychiatric symptomatology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14279v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Leckie, Anya K. Bershad, Jes Heppler, Mason McClay, Sofiia Rappe, Jacob G. Foster</dc:creator>
    </item>
    <item>
      <title>Advancing Multiscale Structural Mapping for Alzheimer's Disease using Local Gyrification Index</title>
      <link>https://arxiv.org/abs/2409.14514</link>
      <description>arXiv:2409.14514v1 Announce Type: new 
Abstract: Research question: This study aims to find whether other neurostructural measurements could be added and combined with the state-of-the-art Alzheimer's imaging marker called MSSM to improve sensitivity to neurodegeneration in Alzheimer's disease patients. Findings: By applying various neurostructural measurements such as the local gyrification index and Jacobian white to the existing Multiscale Structural Mapping of Alzheimer's Disease Neurodegeneration, better results were obtained compared to previous methods, with the addition of LGI proving to be the most effective. Meaning: The extended MSSM imaging marker may provide better ability for the detection of degeneration in Alzheimer's disease. This research shows that this method, using a single standard T1- weighted MRI, can support clinical diagnostics and help identify individuals who may need further biomarker evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14514v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jinhee Jang, Geonwoo Baek, Ikbeom Jang</dc:creator>
    </item>
    <item>
      <title>n:m Phase-Locking of Heterogeneous and Strongly Coupled Oscillators</title>
      <link>https://arxiv.org/abs/2409.14566</link>
      <description>arXiv:2409.14566v1 Announce Type: new 
Abstract: We introduce a scalar reduction method beyond weak perturbations for forced or coupled systems to determine the existence and stability of $n{:}m$ phase-locked states affected by heterogeneity. We consider various biologically relevant oscillators including the complex Ginzburg-Landau oscillator, a thalamic neuron oscillator, and a model of circadian rhythms. The scalar reduction successfully captures the emergence and disappearance of phase-locked states as a function of coupling strength and heterogeneity. We find that even small amounts of heterogeneity (often orders of magnitude smaller than the coupling strength) can significantly alter phase-locked states. The proposed method is a straightforward means to both reduce and analyze potentially high dimensional systems of oscillators that exist closer to biologically-realistic settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14566v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youngmin Park</dc:creator>
    </item>
    <item>
      <title>Drift to Remember</title>
      <link>https://arxiv.org/abs/2409.13997</link>
      <description>arXiv:2409.13997v1 Announce Type: cross 
Abstract: Lifelong learning in artificial intelligence (AI) aims to mimic the biological brain's ability to continuously learn and retain knowledge, yet it faces challenges such as catastrophic forgetting. Recent neuroscience research suggests that neural activity in biological systems undergoes representational drift, where neural responses evolve over time, even with consistent inputs and tasks. We hypothesize that representational drift can alleviate catastrophic forgetting in AI during new task acquisition. To test this, we introduce DriftNet, a network designed to constantly explore various local minima in the loss landscape while dynamically retrieving relevant tasks. This approach ensures efficient integration of new information and preserves existing knowledge. Experimental studies in image classification and natural language processing demonstrate that DriftNet outperforms existing models in lifelong learning. Importantly, DriftNet is scalable in handling a sequence of tasks such as sentiment analysis and question answering using large language models (LLMs) with billions of parameters on a single Nvidia A100 GPU. DriftNet efficiently updates LLMs using only new data, avoiding the need for full dataset retraining. Tested on GPT-2 and RoBERTa, DriftNet is a robust, cost-effective solution for lifelong learning in LLMs. This study not only advances AI systems to emulate biological learning, but also provides insights into the adaptive mechanisms of biological neural systems, deepening our understanding of lifelong learning in nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13997v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jin Du, Xinhe Zhang, Hao Shen, Xun Xian, Ganghua Wang, Jiawei Zhang, Yuhong Yang, Na Li, Jia Liu, Jie Ding</dc:creator>
    </item>
    <item>
      <title>Reconciliation of weak pairwise spike-train correlations and highly coherent local field potentials across space</title>
      <link>https://arxiv.org/abs/1805.10235</link>
      <description>arXiv:1805.10235v3 Announce Type: replace 
Abstract: Multi-electrode arrays covering several square millimeters of neural tissue provide simultaneous access to population signals such as extracellular potentials and spiking activity of one hundred or more individual neurons. The interpretation of the recorded data calls for multiscale computational models with corresponding spatial dimensions and signal predictions. Multi-layer spiking neuron network models of local cortical circuits covering about 1 mm$^2$ have been developed, integrating experimentally obtained neuron-type-specific connectivity data and reproducing features of observed in-vivo spiking statistics. Local field potentials (LFPs) can be computed from the simulated spiking activity. We here extend a local network and LFP model to an area of 4x4 mm$^2$, preserving the neuron density and introducing distance-dependent connection probabilities and conduction delays. We find that the upscaling procedure preserves the overall spiking statistics of the original model and reproduces asynchronous irregular spiking across populations and weak pairwise spike-train correlations in agreement with experimental recordings from sensory cortex. Also compatible with experimental observations, the correlation of LFP signals is strong and decays over a distance of several hundred micrometers. Enhanced spatial coherence in the low-gamma band around 50 Hz may explain the recent report of an apparent band-pass filter effect in the spatial reach of the LFP.</description>
      <guid isPermaLink="false">oai:arXiv.org:1805.10235v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johanna Senk, Espen Hagen, Sacha J. van Albada, Markus Diesmann</dc:creator>
    </item>
    <item>
      <title>Learning temporal relationships between symbols with Laplace Neural Manifolds</title>
      <link>https://arxiv.org/abs/2302.10163</link>
      <description>arXiv:2302.10163v4 Announce Type: replace 
Abstract: Firing across populations of neurons in many regions of the mammalian brain maintains a temporal memory, a neural timeline of the recent past. Behavioral results demonstrate that people can both remember the past and anticipate the future over an analogous internal timeline. This paper presents a mathematical framework for building this timeline of the future. We assume that the input to the system is a time series of symbols--sparse tokenized representations of the present--in continuous time. The goal is to record pairwise temporal relationships between symbols over a wide range of time scales. We assume that the brain has access to a temporal memory in the form of the real Laplace transform. Hebbian associations with a diversity of synaptic time scales are formed between the past timeline and the present symbol. The associative memory stores the convolution between the past and the present. Knowing the temporal relationship between the past and the present allows one to infer relationships between the present and the future. With appropriate normalization, this Hebbian associative matrix can store a Laplace successor representation and a Laplace predecessor representation from which measures of temporal contingency can be evaluated. The diversity of synaptic time constants allows for learning of non-stationary statistics as well as joint statistics between triplets of symbols. This framework synthesizes a number of recent neuroscientific findings including results from dopamine neurons in the mesolimbic forebrain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10163v4</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc W. Howard, Zahra G. Esfahani, Bao Le, Per B. Sederberg</dc:creator>
    </item>
    <item>
      <title>The emergence of subjective temporality: the self-simulational theory of temporal extension from the perspective of the free energy principle</title>
      <link>https://arxiv.org/abs/2404.12895</link>
      <description>arXiv:2404.12895v4 Announce Type: replace 
Abstract: The self-simulational theory of temporal extension describes an information-theoretically formalized mechanism by which the width of subjective temporality emerges from the architecture of self-modelling. In this paper, the perspective of the free energy principle will be assumed, to cast the emergence of subjective temporality, along with a Bayesian mechanism for hierarchical duration estimation, from first principles of the physics of self-organization. Using active inference, a deep parametric generative model of temporal inference is simulated, which realizes the described dynamics on a computational level. Two biases (i.e. variations) of time-perception naturally emerge from the simulated computational model. This concerns the intentional binding effect (i.e. the compression of the temporal interval between voluntarily initiated actions and subsequent sensory consequences) and empirically documented alterations of subjective time experience in deep states of meditative absorption (i.e. in minimal phenomenal experience). Generally, numerous systematic and domain-specific alterations of subjective temporal experience are computationally explained in a unified manner, as enabled by integration with current active inference accounts mapping onto the respective domains. This concerns - next to more general scale-invariant effects of explicit timing and central tendency effects - the temporality-modulating role of valence, impulsivity, boredom, flow-states, near death-experiences, and various psychopathologies, amongst others. The self-simulational theory of temporal extension, from the perspective of the free energy principle, explains how the subjective temporal Now emerges and varies from first principles, accounting for why sometimes, subjective time seems to fly, and sometimes, moments feel like eternities; with the computational mechanism being readily deployable synthetically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12895v4</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Erik Bellingrath</dc:creator>
    </item>
    <item>
      <title>Real-Time Machine Learning Strategies for a New Kind of Neuroscience Experiments</title>
      <link>https://arxiv.org/abs/2409.01280</link>
      <description>arXiv:2409.01280v2 Announce Type: replace 
Abstract: Function and dysfunctions of neural systems are tied to the temporal evolution of neural states. The current limitations in showing their causal role stem largely from the absence of tools capable of probing the brain's internal state in real-time. This gap restricts the scope of experiments vital for advancing both fundamental and clinical neuroscience. Recent advances in real-time machine learning technologies, particularly in analyzing neural time series as nonlinear stochastic dynamical systems, are beginning to bridge this gap. These technologies enable immediate interpretation of and interaction with neural systems, offering new insights into neural computation. However, several significant challenges remain. Issues such as slow convergence rates, high-dimensional data complexities, structured noise, non-identifiability, and a general lack of inductive biases tailored for neural dynamics are key hurdles. Overcoming these challenges is crucial for the full realization of real-time neural data analysis for the causal investigation of neural computation and advanced perturbation based brain machine interfaces. In this paper, we provide a comprehensive perspective on the current state of the field, focusing on these persistent issues and outlining potential paths forward. We emphasize the importance of large-scale integrative neuroscience initiatives and the role of meta-learning in overcoming these challenges. These approaches represent promising research directions that could redefine the landscape of neuroscience experiments and brain-machine interfaces, facilitating breakthroughs in understanding brain function, and treatment of neurological disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01280v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayesha Vermani, Matthew Dowling, Hyungju Jeon, Ian Jordan, Josue Nassar, Yves Bernaerts, Yuan Zhao, Steven Van Vaerenbergh, Il Memming Park</dc:creator>
    </item>
    <item>
      <title>Predictability maximization and the origins of word order harmony</title>
      <link>https://arxiv.org/abs/2408.16570</link>
      <description>arXiv:2408.16570v4 Announce Type: replace-cross 
Abstract: We address the linguistic problem of the sequential arrangement of a head and its dependents from an information theoretic perspective. In particular, we consider the optimal placement of a head that maximizes the predictability of the sequence. We assume that dependents are statistically independent given a head, in line with the open-choice principle and the core assumptions of dependency grammar. We demonstrate the optimality of harmonic order, i.e., placing the head last maximizes the predictability of the head whereas placing the head first maximizes the predictability of dependents. We also show that postponing the head is the optimal strategy to maximize its predictability while bringing it forward is the optimal strategy to maximize the predictability of dependents. We unravel the advantages of the strategy of maximizing the predictability of the head over maximizing the predictability of dependents. Our findings shed light on the placements of the head adopted by real languages or emerging in different kinds of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16570v4</guid>
      <category>cs.CL</category>
      <category>physics.soc-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ramon Ferrer-i-Cancho</dc:creator>
    </item>
  </channel>
</rss>
