<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Feb 2025 02:53:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An SMT Formalization of Mixed-Precision Matrix Multiplication: Modeling Three Generations of Tensor Cores</title>
      <link>https://arxiv.org/abs/2502.15999</link>
      <description>arXiv:2502.15999v1 Announce Type: new 
Abstract: Many recent computational accelerators provide non-standard (e.g., reduced precision) arithmetic operations to enhance performance for floating-point matrix multiplication. Unfortunately, the properties of these accelerators are not widely understood and lack sufficient descriptions of their behavior. This makes it difficult for tool builders beyond the original vendor to target or simulate the hardware correctly, or for algorithm designers to be confident in their code. To address these gaps, prior studies have probed the behavior of these units with manually crafted tests. Such tests are cumbersome to design, and adapting them as the accelerators evolve requires repeated manual effort.
  We present a formal model for the tensor cores of Nvidia's Volta, Turing, and Ampere GPUs. We identify specific properties -- rounding mode, precision, and accumulation order -- that drive these cores' behavior. We formalize these properties and then use the formalization to automatically generate discriminating inputs that illustrate differences among machines. Our results confirm many of the findings of previous tensor core studies, but also identify subtle disagreements. In particular, Nvidia's machines do not, as previously reported, use round-to-zero for accumulation, and their 5-term accumulator requires 3 extra carry-out bits for full accuracy. Using our formal model, we analyze two existing algorithms that use half-precision tensor cores to accelerate single-precision multiplication with error correction. Our analysis reveals that the newer algorithm, designed to be more accurate than the first, is actually less accurate for certain inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15999v1</guid>
      <category>cs.AR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Valpey, Xinyi Li, Sreepathi Pai, Ganesh Gopalakrishnan</dc:creator>
    </item>
    <item>
      <title>Teardown Analysis of Samsung S20 Exynos 990 SoC</title>
      <link>https://arxiv.org/abs/2502.16166</link>
      <description>arXiv:2502.16166v1 Announce Type: new 
Abstract: The mobile phone has evolved from a simple communication device to a complex and highly integrated system with heterogeneous devices, thanks to the rapid technological developments in the semiconductor industry. Understanding the new technology is indeed a time-consuming and challenging task. Therefore, this study performs a teardown analysis of the Samsung Exynos S20 990 System-on-Chip (SoC), a flagship mobile processor that features a three-dimensional (3D) package on-package (PoP) solution with flip chip interconnect (fcPoP). The fcPoP design integrates the SoC and the memory devices in a single package, reducing the interconnection length and improving signal integrity and power efficiency. The study reveals the complex integration of various components and the advanced features of the SoC. The study also examines the microstructure of the chip and the package using X-ray, SEM, and optical microscopy techniques. Moreover, it demonstrates how the fcPoP design enables the SoC to meet the demands of higher performance, higher bandwidth, lower power consumption, and smaller form factor, especially in 5G mobile applications. The study contributes to understanding advanced packaging methodologies and indicates potential directions for future semiconductor innovations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16166v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nabeel Ahmad Khan Jadoon, Umama Saleem, Joanna Ylanen, Daryana Ihnatiuk, Paulina Gallego Perez</dc:creator>
    </item>
    <item>
      <title>Bancroft: Genomics Acceleration Beyond On-Device Memory</title>
      <link>https://arxiv.org/abs/2502.16470</link>
      <description>arXiv:2502.16470v1 Announce Type: new 
Abstract: This paper presents Bancroft, a computational genomics acceleration platform that provides the illusion of practically infinite on-device memory capacity by compressing genomic data movement over PCIe. Bancroft introduces novel optimizations for efficient accelerator implementation to reference-based genome compression, including fixed-stride matching using cuckoo hashes and grouped header encoding, incorporated into a familiar interface supporting random accesses. We evaluate a prototype implementation of Bancroft on an affordable Alveo U50 FPGA equipped with 8 GB of HBM. Thanks to the orders of magnitude improvements in performance and resource efficiency of genomic compression, our prototype provides access to TBs of host-side genomic data at memory-class performance, measuring speeds over 30% of the on-device HBM bandwidth, an order of magnitude higher than conventional PCIe-limited architectures. Using a real-world pre-alignment filtering application, Bancroft demonstrates over 6x improvement over the conventional PCIe-attached architecture, achieving 30% of peak internal throughput of an accelerator with HBM, and 90% of the one with DDR4. Bancroft supports memory-class performance to practically infinite data capacity, using a small, fixed amount of HBM, making it an attractive solution to continued future scalability of computational genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16470v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Se-Min Lim, Seongyoung Kang, Sang-Woo Jun</dc:creator>
    </item>
    <item>
      <title>TerEffic: Highly Efficient Ternary LLM Inference on FPGA</title>
      <link>https://arxiv.org/abs/2502.16473</link>
      <description>arXiv:2502.16473v1 Announce Type: new 
Abstract: Large Language Model (LLM) deployment on edge devices is typically constrained by the need for off-chip memory access, leading to high power consumption and limited throughput. Ternary quantization for LLMs is promising in maintaining model accuracy while reducing memory footprint. However, existing accelerators have not exploited this potential for on-chip inference. We present TerEffic, an FPGA-based accelerator that carefully co-designs memory architecture and computational units to unlock highly efficient LLM inference with fully on-chip execution. Through weight compression, custom computational units, and memory hierarchy optimization, we achieve unprecedented efficiency by eliminating off-chip memory bandwidth bottlenecks. We propose two architectural variants: a fully on-chip design for smaller models and an HBM-assisted design for larger ones. When evaluated on a 370M parameter model with single-batch inference, our on-chip design achieves 12,700 tokens/sec (149 times higher than NVIDIA's Jetson Orin Nano) with a power efficiency of 467 tokens/sec/W (19 times better than Jetson Orin Nano). The HBM-assisted design provides 521 tokens/sec on a 2.7B parameter model (2 times higher than NVIDIA's A100) with 33W power consumption, achieving a power efficiency of 16 tokens/sec/W (8 times better than A100).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16473v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyang Yin, Zhenyu Bai, Pranav Venkatram, Shivam Aggarwal, Zhaoying Li, Tulika Mitra</dc:creator>
    </item>
    <item>
      <title>A Review of Memory Wall for Neuromorphic Computing</title>
      <link>https://arxiv.org/abs/2502.16823</link>
      <description>arXiv:2502.16823v1 Announce Type: new 
Abstract: This paper reviews memory technologies used in Field-Programmable Gate Arrays (FPGAs) for neuromorphic computing, a brain-inspired approach transforming artificial intelligence with improved efficiency and performance. It focuses on the essential role of memory in FPGA-based neuromorphic systems, evaluating memory types such as Static Random-Access Memory (SRAM), Dynamic Random-Access Memory (DRAM), High-Bandwidth Memory (HBM), and emerging non-volatile memories like Resistive RAM (ReRAM) and Phase-Change Memory (PCM). These technologies are analyzed based on latency, bandwidth, power consumption, density, and scalability to assess their suitability for storing and processing neural network models and synaptic weights. The review provides a comparative analysis of their strengths and limitations, supported by case studies illustrating real-world implementations and performance outcomes. This review offers insights to guide researchers and practitioners in selecting and optimizing memory technologies, enhancing the performance and energy efficiency of FPGA-based neuromorphic platforms, and advancing applications in artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16823v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dexter Le, Baran Arig, Murat Isik, I. Can Dikmen, Teoman Karadag</dc:creator>
    </item>
    <item>
      <title>APINT: A Full-Stack Framework for Acceleration of Privacy-Preserving Inference of Transformers based on Garbled Circuits</title>
      <link>https://arxiv.org/abs/2502.16877</link>
      <description>arXiv:2502.16877v1 Announce Type: new 
Abstract: As the importance of Privacy-Preserving Inference of Transformers (PiT) increases, a hybrid protocol that integrates Garbled Circuits (GC) and Homomorphic Encryption (HE) is emerging for its implementation. While this protocol is preferred for its ability to maintain accuracy, it has a severe drawback of excessive latency. To address this, existing protocols primarily focused on reducing HE latency, thus making GC the new latency bottleneck. Furthermore, previous studies only focused on individual computing layers, such as protocol or hardware accelerator, lacking a comprehensive solution at the system level. This paper presents APINT, a full-stack framework designed to reduce PiT's overall latency by addressing the latency problem of GC through both software and hardware solutions. APINT features a novel protocol that reallocates possible GC workloads to alternative methods (i.e., HE or standard matrix operation), substantially decreasing the GC workload. It also suggests GC-friendly circuit generation that reduces the number of AND gates at the most, which is the expensive operator in GC. Furthermore, APINT proposes an innovative netlist scheduling that combines coarse-grained operation mapping and fine-grained scheduling for maximal data reuse and minimal dependency. Finally, APINT's hardware accelerator, combined with its compiler speculation, effectively resolves the memory stall issue. Putting it all together, APINT achieves a remarkable end-to-end reduction in latency, outperforming the existing protocol on CPU platform by 12.2x online and 2.2x offline. Meanwhile, the APINT accelerator not only reduces its latency by 3.3x but also saves energy consumption by 4.6x while operating PiT compared to the state-of-the-art GC accelerator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16877v1</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hyunjun Cho, Jaeho Jeon, Jaehoon Heo, Joo-Young Kim</dc:creator>
    </item>
    <item>
      <title>Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM</title>
      <link>https://arxiv.org/abs/2502.16963</link>
      <description>arXiv:2502.16963v1 Announce Type: new 
Abstract: The billion-scale Large Language Models (LLMs) need deployment on expensive server-grade GPUs with large-storage HBMs and abundant computation capability. As LLM-assisted services become popular, achieving cost-effective LLM inference on budget-friendly hardware becomes the trend. Extensive researches relocate LLM parameters from expensive GPUs to host memory. However, the restricted bandwidth between the host and GPU memory limits the inference performance.
  This work introduces Hermes, a budget-friendly system that leverages the near-data processing (NDP) within commodity DRAM DIMMs to enhance the performance of a single consumer-grade GPU, achieving efficient LLM inference. The inherent activation sparsity in LLMs naturally divides weight parameters into two categories, termed ``hot" and ``cold" neurons, respectively. Hot neurons, which consist of only approximately 20\% of all weight parameters, account for 80\% of the total computational load, while cold neurons make up the other 80\% of parameters but are responsible for just 20\% of the computational load. Therefore, we propose a heterogeneous computing strategy: mapping hot neurons to a single computation-efficient GPU, while offloading cold neurons to NDP-DIMMs, which offer large memory size but limited computation capabilities. Meanwhile, the dynamic nature of activation sparsity needs a real-time partition of hot/cold neurons and adaptive remapping of cold neurons across multiple NDP-DIMM modules. Therefore, we introduce a lightweight predictor optimizing real-time neuron partition and adjustment between GPU and NDP-DIMMs. We also utilize a window-based online scheduling mechanism to maintain load balance among NDP-DIMM modules. Hermes facilitates the deployment of LLaMA2-70B on consumer-grade hardware at 13.75 tokens/s and realizes an average 75.24$\times$ speedup over the state-of-the-art offloading-based inference system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16963v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lian Liu, Shixin Zhao, Bing Li, Haimeng Ren, Zhaohui Xu, Mengdi Wang, Xiaowei Li, Yinhe Han, Ying Wang</dc:creator>
    </item>
    <item>
      <title>Be CIM or Be Memory: A Dual-mode-aware DNN Compiler for CIM Accelerators</title>
      <link>https://arxiv.org/abs/2502.17006</link>
      <description>arXiv:2502.17006v1 Announce Type: new 
Abstract: Computing-in-memory (CIM) architectures demonstrate superior performance over traditional architectures. To unleash the potential of CIM accelerators, many compilation methods have been proposed, focusing on application scheduling optimization specific to CIM. However, existing compilation methods often overlook CIM's capability to switch dynamically between compute and memory modes, which is crucial for accommodating the diverse memory and computational needs of real-world deep neural network architectures, especially the emerging large language models. To fill this gap, we introduce CMSwitch, a novel compiler to optimize resource allocation for CIM accelerators with adaptive mode-switching capabilities, thereby enhancing the performance of DNN applications. Specifically, our approach integrates the compute-memory mode switch into the CIM compilation optimization space by introducing a new hardware abstraction attribute. Then, we propose a novel compilation optimization pass that identifies the optimal network segment and the corresponding mode resource allocations using dynamic programming and mixed-integer programming. CMSwitch uses the tailored meta-operator to express the compilation result in a generalized manner. Evaluation results demonstrate that CMSwitch achieves an average speedup of 1.31$\times$ compared to existing SOTA CIM compilation works, highlighting CMSwitch's effectiveness in fully exploiting the potential of CIM processors for a wide range of real-world DNN applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17006v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shixin Zhao, Yuming Li, Bing Li, Yintao He, Mengdi Wang, Yinhe Han, Ying Wang</dc:creator>
    </item>
    <item>
      <title>Analyzing a Two-Tier Disaggregated Memory Protection Scheme Based on Memory Replication</title>
      <link>https://arxiv.org/abs/2502.17138</link>
      <description>arXiv:2502.17138v1 Announce Type: new 
Abstract: As memory technologies continue to shrink and memory error rates increase, the demand for stronger reliability becomes increasingly critical. Fine-grain memory replication has emerged as an appealing approach to improving memory fault tolerance by augmenting conventional memory protection based on error-correcting codes with an additional layer of redundancy that replicates data across independent failure domains, such as replicating memory pages across different NUMA sockets. This method can tolerate a broad spectrum of memory errors, from individual memory cell failures to more complex memory controller failures. However, applying memory replication without a holistic consideration of the interaction between error-correcting codes and replication can result in redundant duplication and unnecessary storage overhead. We propose Replication-Aware Memory-error Protection (RAMP), a model that helps explore error protection strategies to improve the storage efficiency of memory protection in memory systems that utilize memory replication for performance and availability. We use RAMP to determine a protection strategy that can lower the storage cost of individual replicas while still ensuring robust protection through the collective protection conferred by multiple replicas. Our evaluation shows that a solution derived with RAMP enhances the storage efficiency of a state-of-the-art memory protection mechanism when paired with rack-level replication for disaggregated memory. Specifically, we can reduce the storage cost of memory protection from 27% down to 17.7% with minimal performance overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17138v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haris Volos, Yiannakis Sazeides</dc:creator>
    </item>
    <item>
      <title>Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded Heterogeneous SoCs</title>
      <link>https://arxiv.org/abs/2502.17398</link>
      <description>arXiv:2502.17398v1 Announce Type: new 
Abstract: Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific hardware accelerators to improve performance and energy efficiency. In particular, programmable multi-core accelerators feature a cluster of processing elements and tightly coupled scratchpad memories to balance performance, energy efficiency, and flexibility. In embedded systems running a general-purpose OS, accelerators access data via dedicated, physically addressed memory regions. This negatively impacts memory utilization and performance by requiring a copy from the virtual host address to the physical accelerator address space. Input-Output Memory Management Units (IOMMUs) overcome this limitation by allowing devices and hosts to use a shared virtual paged address space. However, resolving IO virtual addresses can be particularly costly on high-latency memory systems as it requires up to three sequential memory accesses on IOTLB miss. In this work, we present a quantitative evaluation of shared virtual addressing in RISC-V heterogeneous embedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V SoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated the system performance by emulating the design on FPGA and implementing compute kernels from the RajaPERF benchmark suite using heterogeneous OpenMP programming. We measure the transfers and computation time on the host and accelerators for systems with different DRAM access latencies. We first show that IO virtual address translation can account for 4.2% up to 17.6% of the accelerator's runtime for gemm (General Matrix Multiplication) at low and high memory bandwidth. Then, we show that in systems containing a last-level cache, this IO address translation cost falls to 0.4% and 0.7% under the same conditions, making shared virtual addressing and zero-copy offloading suitable for such RISC-V heterogeneous SoCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17398v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cyril Koenig, Enrico Zelioli, Luca Benini</dc:creator>
    </item>
    <item>
      <title>Advanced Chain-of-Thought Reasoning for Parameter Extraction from Documents Using Large Language Models</title>
      <link>https://arxiv.org/abs/2502.16540</link>
      <description>arXiv:2502.16540v1 Announce Type: cross 
Abstract: Extracting parameters from technical documentation is crucial for ensuring design precision and simulation reliability in electronic design. However, current methods struggle to handle high-dimensional design data and meet the demands of real-time processing. In electronic design automation (EDA), engineers often manually search through extensive documents to retrieve component parameters required for constructing PySpice models, a process that is both labor-intensive and time-consuming. To address this challenge, we propose an innovative framework that leverages large language models (LLMs) to automate the extraction of parameters and the generation of PySpice models directly from datasheets. Our framework introduces three Chain-of-Thought (CoT) based techniques: (1) Targeted Document Retrieval (TDR), which enables the rapid identification of relevant technical sections; (2) Iterative Retrieval Optimization (IRO), which refines the parameter search through iterative improvements; and (3) Preference Optimization (PO), which dynamically prioritizes key document sections based on relevance. Experimental results show that applying all three methods together improves retrieval precision by 47.69% and reduces processing latency by 37.84%. Furthermore, effect size analysis using Cohen's d reveals that PO significantly reduces latency, while IRO contributes most to precision enhancement. These findings underscore the potential of our framework to streamline EDA processes, enhance design accuracy, and shorten development timelines. Additionally, our algorithm has model-agnostic generalization, meaning it can improve parameter search performance across different LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16540v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hong Cai Chen, Yi Pin Xu, Yang Zhang</dc:creator>
    </item>
    <item>
      <title>VR-Pipe: Streamlining Hardware Graphics Pipeline for Volume Rendering</title>
      <link>https://arxiv.org/abs/2502.17078</link>
      <description>arXiv:2502.17078v1 Announce Type: cross 
Abstract: Graphics rendering that builds on machine learning and radiance fields is gaining significant attention due to its outstanding quality and speed in generating photorealistic images from novel viewpoints. However, prior work has primarily focused on evaluating its performance through software-based rendering on programmable shader cores, leaving its performance when exploiting fixed-function graphics units largely unexplored.
  In this paper, we investigate the performance implications of performing radiance field rendering on the hardware graphics pipeline. In doing so, we implement the state-of-the-art radiance field method, 3D Gaussian splatting, using graphics APIs and evaluate it across synthetic and real-world scenes on today's graphics hardware. Based on our analysis, we present VR-Pipe, which seamlessly integrates two innovations into graphics hardware to streamline the hardware pipeline for volume rendering, such as radiance field methods. First, we introduce native hardware support for early termination by repurposing existing special-purpose hardware in modern GPUs. Second, we propose multi-granular tile binning with quad merging, which opportunistically blends fragments in shader cores before passing them to fixed-function blending units. Our evaluation shows that VR-Pipe greatly improves rendering performance, achieving up to a 2.78x speedup over the conventional graphics pipeline with negligible hardware overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17078v1</guid>
      <category>cs.GR</category>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junseo Lee, Jaisung Kim, Junyong Park, Jaewoong Sim</dc:creator>
    </item>
    <item>
      <title>A Hybrid Delay Model for Interconnected Multi-Input Gates</title>
      <link>https://arxiv.org/abs/2403.10540</link>
      <description>arXiv:2403.10540v3 Announce Type: replace 
Abstract: Dynamic digital timing analysis is a less accurate but fast alternative to highly accurate but slow analog simulations of digital circuits. It relies on gate delay models, which allow the determination of input-to-output delays of a gate on a per-transition basis. Accurate delay models not only consider the effect of preceding output transitions here but also delay variations induced by multi-input switching (MIS) effects in the case of multi-input gates. Starting out from a first-order hybrid delay model for CMOS two-input NOR gates, we develop a hybrid delay model for Muller C gates and show how to augment these models and their analytic delay formulas by a first-order interconnect. Moreover, we conduct a systematic evaluation of the resulting modeling accuracy: Using SPICE simulations, we quantify the MIS effects on the gate delays under various wire lengths, load capacitances, and input strengths for two different CMOS technologies, comparing these results to the predictions of appropriately parameterized versions of our new gate delay models. Overall, our experimental results reveal that they capture all MIS effects with a surprisingly good accuracy despite being first-order only.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10540v3</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arman Ferdowsi, Matthias F\"ugger, Josef Salzmann, Ulrich Schmid</dc:creator>
    </item>
    <item>
      <title>Dynamic Co-Optimization Compiler: Leveraging Multi-Agent Reinforcement Learning for Enhanced DNN Accelerator Performance</title>
      <link>https://arxiv.org/abs/2407.08192</link>
      <description>arXiv:2407.08192v3 Announce Type: replace-cross 
Abstract: This paper introduces a novel Dynamic Co-Optimization Compiler (DCOC), which employs an adaptive Multi-Agent Reinforcement Learning (MARL) framework to enhance the efficiency of mapping machine learning (ML) models, particularly Deep Neural Networks (DNNs), onto diverse hardware platforms. DCOC incorporates three specialized actor-critic agents within MARL, each dedicated to different optimization facets: one for hardware and two for software. This cooperative strategy results in an integrated hardware/software co-optimization approach, improving the precision and speed of DNN deployments. By focusing on high-confidence configurations, DCOC effectively reduces the search space, achieving remarkable performance over existing methods. Our results demonstrate that DCOC enhances throughput by up to 37.95% while reducing optimization time by up to 42.2% across various DNN models, outperforming current state-of-the-art frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08192v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3658617.3697547</arxiv:DOI>
      <dc:creator>Arya Fayyazi, Mehdi Kamal, Massoud Pedram</dc:creator>
    </item>
    <item>
      <title>Accelerating Sparse Graph Neural Networks with Tensor Core Optimization</title>
      <link>https://arxiv.org/abs/2412.12218</link>
      <description>arXiv:2412.12218v2 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) have seen extensive application in domains such as social networks, bioinformatics, and recommendation systems. However, the irregularity and sparsity of graph data challenge traditional computing methods, which are insufficient to meet the performance demands of GNNs. Recent research has explored parallel acceleration using CUDA Cores and Tensor Cores, but significant challenges persist: (1) kernel fusion leads to false high utilization, failing to treat CUDA and Tensor Cores as independent resources, and (2) heterogeneous cores have distinct computation preferences, causing inefficiencies. To address these issues, this paper proposes FTC-GNN, a novel acceleration framework that efficiently utilizes CUDA and Tensor Cores for GNN computation. FTC-GNN introduces (1) a collaborative design that enables the parallel utilization of CUDA and Tensor Cores and (2) a sparse-to-dense transformation strategy that assigns dense matrix operations to Tensor Cores while leveraging CUDA Cores for data management and sparse edge processing. This design optimizes GPU resource utilization and improves computational efficiency. Experimental results demonstrate the effectiveness of FTC-GNN using GCN and AGNN models across various datasets. For GCN, FTC-GNN achieves speedups of 4.90x, 7.10x, and 1.17x compared to DGL, PyG, and TC-GNN, respectively. For AGNN, it achieves speedups of 5.32x, 2.92x, and 1.02x, establishing its superiority in accelerating GNN computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12218v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ka Wai Wu</dc:creator>
    </item>
  </channel>
</rss>
