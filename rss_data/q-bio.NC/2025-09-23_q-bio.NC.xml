<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 10:22:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Low-Cost Shield MicroBCI to Measure EEG with STM32</title>
      <link>https://arxiv.org/abs/2509.16229</link>
      <description>arXiv:2509.16229v1 Announce Type: new 
Abstract: The article introduces an accessible pathway into neuroscience using the MicroBCI device, which leverages the STM32 Nucleo-55RG development board as the core platform. MicroBCI enables the STM32 board to function as a brain-computer interface, capable of recording EEG, EMG, and ECG signals across 8 channels. Over the past decade, the rapid growth of artificial intelligence has transformed many fields, including neurobiology. The application of machine learning methods has created opportunities for the practical use of EEG signals in diverse technological domains. This growing interest has fueled the popularity of affordable brain-computer interface systems that utilize non-invasive electrodes for EEG acquisition. The MicroBCI device demonstrates reliable noise performance and accuracy for applied research and prototyping. Furthermore, it effectively detects alpha brain waves, confirming its ability to capture key neurological signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16229v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ildar Rakhmatulin</dc:creator>
    </item>
    <item>
      <title>Emotions are Recognized Patterns of Cognitive Activities</title>
      <link>https://arxiv.org/abs/2509.16232</link>
      <description>arXiv:2509.16232v1 Announce Type: new 
Abstract: Emotions play a crucial role in human life. The research community has proposed many theories on emotions without reaching much consensus. The situation is similar for emotions in cognitive architectures and autonomous agents. I propose in this paper that emotions are recognized patterns of cognitive activities. These activities are responses of an agent to the deviations between the targets of its goals and the performances of its actions. Emotions still arise even if these activities are purely logical. I map the patterns of cognitive activities to emotions. I show the link between emotions and attention and the impacts of the parameterized functions in the cognitive architecture on the computing of emotions. My proposition bridges different theories on emotions and advances the building of consensus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16232v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Jin (Nokia Bell Labs France)</dc:creator>
    </item>
    <item>
      <title>Evolvable Graph Diffusion Optimal Transport with Pattern-Specific Alignment for Brain Connectome Modeling</title>
      <link>https://arxiv.org/abs/2509.16238</link>
      <description>arXiv:2509.16238v1 Announce Type: new 
Abstract: Network analysis of human brain connectivity indicates that individual differences in cognitive abilities arise from neurobiological mechanisms inherent in structural and functional brain networks. Existing studies routinely treat structural connectivity (SC) as optimal or fixed topological scaffolds for functional connectivity (FC), often overlooking higher-order dependencies between brain regions and limiting the modeling of complex cognitive processes. Besides, the distinct spatial organizations of SC and FC complicate direct integration, as naive alignment may distort intrinsic nonlinear patterns of brain connectivity. In this study, we propose a novel framework called Evolvable Graph Diffusion Optimal Transport with Pattern-Specific Alignment (EDT-PA), designed to identify disease-specific connectome patterns and classify brain disorders. To accurately model high-order structural dependencies, EDT-PA incorporates a spectrum of evolvable modeling blocks to dynamically capture high-order dependencies across brain regions. Additionally, a Pattern-Specific Alignment mechanism employs optimal transport to align structural and functional representations in a geometry-aware manner. By incorporating a Kolmogorov-Arnold network for flexible node aggregation, EDT-PA is capable of modeling complex nonlinear interactions among brain regions for downstream classification. Extensive evaluations on the REST-meta-MDD and ADNI datasets demonstrate that EDT-PA outperforms state-of-the-art methods, offering a more effective framework for revealing structure-function misalignments and disorder-specific subnetworks in brain disorders. The project of this work is released via this link.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16238v1</guid>
      <category>q-bio.NC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoqi Sheng, Jiawen Liu, Jiaming Liang, Yiheng Zhang, Hongmin Cai</dc:creator>
    </item>
    <item>
      <title>Quantum-like representation of neuronal networks' activity: modeling "mental entanglement"</title>
      <link>https://arxiv.org/abs/2509.16253</link>
      <description>arXiv:2509.16253v1 Announce Type: new 
Abstract: Quantum-like modeling (QLM) - quantum theory applications outside of physics - are intensively developed with applications in biology, cognition, psychology, and decision-making. For cognition, QLM should be distinguished from quantum reductionist models in the spirit of Hameroff and Penrose and well as Umezawa and Vitiello. QLM is not concerned with just quantum physical processes in the brain but also QL information processing by macroscopic neuronal structures. Although QLM of cognition and decision-making has seen some success, it suffers from a knowledge gap that exists between oscillatory neuronal network functioning in the brain and QL behavioral patterns. Recently, steps toward closing this gap have been taken using the generalized probability theory and prequantum classical statistical field theory (PCSFT) - a random field model beyond the complex Hilbert space formalism. PCSFT is used to move from the classical ``oscillatory cognition'' of the neuronal networks to QLM for decision.making. In this study, we addressed the most difficult problem within this construction: QLM for entanglement generation by classical networks, i.e., mental entanglement. We started with the observational approach to entanglement based on operator algebras describing local observables and bringing into being the tensor product structure in the space of QL states. Moreover, we applied the standard states entanglement approach: entanglement generation by spatially separated networks in the brain. Finally, we discussed possible future experiments on mental entanglement detection using the EEG/MEG technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16253v1</guid>
      <category>q-bio.NC</category>
      <category>quant-ph</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrei Khrennikov, Makiko Yamada</dc:creator>
    </item>
    <item>
      <title>Deep Learning Inductive Biases for fMRI Time Series Classification during Resting-state and Movie-watching</title>
      <link>https://arxiv.org/abs/2509.16973</link>
      <description>arXiv:2509.16973v1 Announce Type: new 
Abstract: Deep learning has advanced fMRI analysis, yet it remains unclear which architectural inductive biases are most effective at capturing functional patterns in human brain activity. This issue is particularly important in small-sample settings, as most datasets fall into this category. We compare models with three major inductive biases in deep learning including convolutional neural networks (CNNs), long short-term memory networks (LSTMs), and Transformers for the task of biological sex classification. These models are evaluated within a unified pipeline using parcellated multivariate fMRI time series from the Human Connectome Project (HCP) 7-Tesla cohort, which includes four resting-state runs and four movie-watching task runs. We assess performance on Whole-brain, subcortex, and 12 functional networks. CNNs consistently achieved the highest discrimination for sex classification in both resting-state and movie-watching, while LSTM and Transformer models underperformed. Network-resolved analyses indicated that the Whole-brain, Default Mode, Cingulo-Opercular, Dorsal Attention, and Frontoparietal networks were the most discriminative. These results were largely similar between resting-state and movie-watching. Our findings indicate that, at this dataset size, discriminative information is carried by local spatial patterns and inter-regional dependencies, favoring convolutional inductive bias. Our study provides insights for selecting deep learning architectures for fMRI time series classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16973v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Behdad Khodabandehloo, Reza Rajimehr</dc:creator>
    </item>
    <item>
      <title>Analyzing Memory Effects in Large Language Models through the lens of Cognitive Psychology</title>
      <link>https://arxiv.org/abs/2509.17138</link>
      <description>arXiv:2509.17138v1 Announce Type: new 
Abstract: Memory, a fundamental component of human cognition, exhibits adaptive yet fallible characteristics as illustrated by Schacter's memory "sins".These cognitive phenomena have been studied extensively in psychology and neuroscience, but the extent to which artificial systems, specifically Large Language Models (LLMs), emulate these cognitive phenomena remains underexplored. This study uses human memory research as a lens for understanding LLMs and systematically investigates human memory effects in state-of-the-art LLMs using paradigms drawn from psychological research. We evaluate seven key memory phenomena, comparing human behavior to LLM performance. Both people and models remember less when overloaded with information (list length effect) and remember better with repeated exposure (list strength effect). They also show similar difficulties when retrieving overlapping information, where storing too many similar facts leads to confusion (fan effect). Like humans, LLMs are susceptible to falsely "remembering" words that were never shown but are related to others (false memories), and they can apply prior learning to new, related situations (cross-domain generalization). However, LLMs differ in two key ways: they are less influenced by the order in which information is presented (positional bias) and more robust when processing random or meaningless material (nonsense effect). These results reveal both alignments and divergences in how LLMs and humans reconstruct memory. The findings help clarify how memory-like behavior in LLMs echoes core features of human cognition, while also highlighting the architectural differences that lead to distinct patterns of error and success.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17138v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaoyang Cao, Lael Schooler, Reza Zafarani</dc:creator>
    </item>
    <item>
      <title>Self-Supervised Discovery of Neural Circuits in Spatially Patterned Neural Responses with Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2509.17174</link>
      <description>arXiv:2509.17174v1 Announce Type: new 
Abstract: Inferring synaptic connectivity from neural population activity is a fundamental challenge in computational neuroscience, complicated by partial observability and mismatches between inference models and true circuit dynamics. In this study, we propose a graph-based neural inference model that simultaneously predicts neural activity and infers latent connectivity by modeling neurons as interacting nodes in a graph. The architecture features two distinct modules: one for learning structural connectivity and another for predicting future spiking activity via a graph neural network (GNN). Our model accommodates unobserved neurons through auxiliary nodes, allowing for inference in partially observed circuits. We evaluate this approach using synthetic data from ring attractor networks and real spike recordings from head direction cells in mice. Across a wide range of conditions, including varying recurrent connectivity, external inputs, and incomplete observations, our model consistently outperforms standard baselines, resolving spurious correlations more effectively and recovering accurate weight profiles. When applied to real data, the inferred connectivity aligns with theoretical predictions of continuous attractor models. These results highlight the potential of GNN-based models to infer latent neural circuitry through self-supervised structure learning, while leveraging the spike prediction task to flexibly link connectivity and dynamics across both simulated and biological neural systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17174v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kijung Yoon</dc:creator>
    </item>
    <item>
      <title>A tutorial on electrogastrography using low-cost hardware and open-source software</title>
      <link>https://arxiv.org/abs/2509.17260</link>
      <description>arXiv:2509.17260v1 Announce Type: new 
Abstract: Electrogastrography is the recording of changes in electric potential caused by the stomach's pacemaker region, typically through several cutaneous sensors placed on the abdomen. It is a worthwhile technique in medical and psychological research, but also relatively niche. Here we present a tutorial on the acquisition and analysis of the human electrogastrogram. Because dedicated equipment and software can be prohibitively expensive, we demonstrate how data can be acquired using a low-cost OpenBCI Ganglion amplifier. We also present a processing pipeline that minimises attrition, which is particularly helpful for low-cost equipment but also applicable to top-of-the-line hardware. Our approach comprises outlier rejection, frequency filtering, movement filtering, and noise reduction using independent component analysis. Where traditional approaches include a subjective step in which only one channel is manually selected for further analysis, our pipeline recomposes the electrogastrogram from all recorded channels after automatic rejection of nuisance components. The main benefits of this approach are reduced attrition, retention of data from all recorded channels, and reduced influence of researcher bias. In addition to our tutorial on the method, we offer a proof-of-principle in which our approach leads to reduced data rejection compared to established methods. We aimed to describe each step in sufficient detail to be implemented in any programming language. In addition, we made an open-source Python package freely available for ease of use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17260v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evgeniya Anisimova, Sameer N. B. Alladin, Styliani Tsamaz, Edwin S. Dalmaijer</dc:creator>
    </item>
    <item>
      <title>From Prediction to Understanding: Will AI Foundation Models Transform Brain Science?</title>
      <link>https://arxiv.org/abs/2509.17280</link>
      <description>arXiv:2509.17280v1 Announce Type: new 
Abstract: Generative pretraining (the "GPT" in ChatGPT) enables language models to learn from vast amounts of internet text without human supervision. This approach has driven breakthroughs across AI by allowing deep neural networks to learn from massive, unstructured datasets. We use the term foundation models to refer to large pretrained systems that can be adapted to a wide range of tasks within and across domains, and these models are increasingly applied beyond language to the brain sciences. These models achieve strong predictive accuracy, raising hopes that they might illuminate computational principles. But predictive success alone does not guarantee scientific understanding. Here, we outline how foundation models can be productively integrated into the brain sciences, highlighting both their promise and their limitations. The central challenge is to move from prediction to explanation: linking model computations to mechanisms underlying neural activity and cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17280v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Serre, Ellie Pavlick</dc:creator>
    </item>
    <item>
      <title>Efficient Brain Network Estimation with Sparse ICA in Non-Human Primate Neuroimaging</title>
      <link>https://arxiv.org/abs/2509.16803</link>
      <description>arXiv:2509.16803v1 Announce Type: cross 
Abstract: Independent component analysis (ICA) is widely used to separate mixed signals and recover statistically independent components. However, in non-human primate neuroimaging studies, most ICA-recovered spatial maps are often dense. To extract the most relevant brain activation patterns, post-hoc thresholding is typically applied-though this approach is often imprecise and arbitrary. To address this limitation, we employed the Sparse ICA method, which enforces both sparsity and statistical independence, allowing it to extract the most relevant activation maps without requiring additional post-processing. Simulation experiments demonstrate that Sparse ICA performs competitively against 11 classical linear ICA methods. We further applied Sparse ICA to real non-human primate neuroimaging data, identifying several independent component networks spanning different brain networks. These spatial maps revealed clearly defined activation areas, providing further evidence that Sparse ICA is effective and reliable in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16803v1</guid>
      <category>stat.AP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Li, Liang Ma, Masoud Seraji, Shujian Yu, Yun Wang, Jingyu Liu, Vince D. Calhoun</dc:creator>
    </item>
    <item>
      <title>The Principles of Human-like Conscious Machine</title>
      <link>https://arxiv.org/abs/2509.16859</link>
      <description>arXiv:2509.16859v1 Announce Type: cross 
Abstract: Determining whether another system, biological or artificial, possesses phenomenal consciousness has long been a central challenge in consciousness studies. This attribution problem has become especially pressing with the rise of large language models and other advanced AI systems, where debates about "AI consciousness" implicitly rely on some criterion for deciding whether a given system is conscious. In this paper, we propose a substrate-independent, logically rigorous, and counterfeit-resistant sufficiency criterion for phenomenal consciousness. We argue that any machine satisfying this criterion should be regarded as conscious with at least the same level of confidence with which we attribute consciousness to other humans. Building on this criterion, we develop a formal framework and specify a set of operational principles that guide the design of systems capable of meeting the sufficiency condition. We further argue that machines engineered according to this framework can, in principle, realize phenomenal consciousness. As an initial validation, we show that humans themselves can be viewed as machines that satisfy this framework and its principles. If correct, this proposal carries significant implications for philosophy, cognitive science, and artificial intelligence. It offers an explanation for why certain qualia, such as the experience of red, are in principle irreducible to physical description, while simultaneously providing a general reinterpretation of human information processing. Moreover, it suggests a path toward a new paradigm of AI beyond current statistics-based approaches, potentially guiding the construction of genuinely human-like AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16859v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fangfang Li, Xiaojie Zhang</dc:creator>
    </item>
    <item>
      <title>Discrete Heat Kernels on Simplicial Complexes and Its Application to Functional Brain Networks</title>
      <link>https://arxiv.org/abs/2509.16908</link>
      <description>arXiv:2509.16908v1 Announce Type: cross 
Abstract: Networks constitute fundamental organizational structures across biological systems, although conventional graph-theoretic analyses capture exclusively pairwise interactions, thereby omitting the intricate higher-order relationships that characterize network complexity. This work proposes a unified framework for heat kernel smoothing on simplicial complexes, extending classical signal processing methodologies from vertices and edges to cycles and higher-dimensional structures. Through Hodge Laplacian, a discrete heat kernel on a finite simplicial complex $\mathcal{K}$ is constructed to smooth signals on $k$-simplices via the boundary operator $\partial_k$. Computationally efficient sparse algorithms for constructing boundary operators are developed to implement linear diffusion processes on $k$-simplices. The methodology generalizes heat kernel smoothing to $k$-simplices, utilizing boundary structure to localize topological features while maintaining homological invariance. Simulation studies demonstrate qualitative signal enhancement across vertex and edge domains following diffusion processes. Application to parcellated human brain functional connectivity networks reveals that simplex-space smoothing attenuates spurious connections while amplifying coherent anatomical architectures, establishing practical significance for computational neuroscience applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16908v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sixtus Dakurah</dc:creator>
    </item>
    <item>
      <title>A sub-Riemannian model of neural states in the primary motor cortex</title>
      <link>https://arxiv.org/abs/2501.03247</link>
      <description>arXiv:2501.03247v2 Announce Type: replace 
Abstract: We develop a neurogeometric model for the arm area of motor cortex, which encodes complex motor primitives, ranging from simple movement features like movement direction, to short hand trajectories, termed fragments, and ultimately to more complex patterns known as neural states (Georgopoulos, Hatsopoulos, Kadmon-Harpaz et al). Based on the sub-riemannian framework introduced in 2023, we model the space of fragments as a set of short curves defined by kinematic parameters. We then introduce a geometric kernel that serves as a model for cortical connectivity and use it in a differential equation to describe cortical activity. By applying a grouping algorithm to this cortical activity model, we successfully recover the neural states observed in Kadmon-Harpaz et al, which were based on measured cortical activity. This confirms that the choice of kinematic variables and the distance metric used here are sufficient to explain the phenomena of neural state formation. The modularity of our model reflects the brain's hierarchical structure, where initial groupings in the kinematic space $\mathcal{M}$ lead to more abstract representations. This approach mimics how the brain processes stimuli at different scales, extracting both local and global properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03247v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Caterina Mazzetti, Jawad Ali, Alessandro Sarti, Giovanna Citti</dc:creator>
    </item>
    <item>
      <title>Geometric Hyperscanning of Affect under Active Inference</title>
      <link>https://arxiv.org/abs/2506.08599</link>
      <description>arXiv:2506.08599v3 Announce Type: replace 
Abstract: Second-person neuroscience holds social cognition as embodied meaning co-regulation through reciprocal interaction, modeled here as coupled active inference with affect emerging as inference over identity-relevant surprise. Each agent maintains a self-model that tracks violations in its predictive coherence while recursively modeling the other. Valence is computed from self-model prediction error, weighted by self-relevance, and modulated by prior affective states and by what we term temporal aiming, which captures affective appraisal over time. This accommodates shifts in the self-other boundary, allowing affect to emerge at individual and dyadic levels. We propose a novel method termed geometric hyperscanning, based on the Forman-Ricci curvature, to empirically operationalize these processes: it tracks topological reconfigurations in inter-brain networks, with its entro-py serving as a proxy for affective phase transitions such as rupture, co-regulation, and re-attunement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08599v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Hinrichs, Mahault Albarracin, Dimitris Bolis, Yuyue Jiang, Leonardo Christov-Moore, Leonhard Schilbach</dc:creator>
    </item>
    <item>
      <title>DCA: Graph-Guided Deep Embedding Clustering for Brain Atlases</title>
      <link>https://arxiv.org/abs/2509.01426</link>
      <description>arXiv:2509.01426v2 Announce Type: replace 
Abstract: Brain atlases are essential for reducing the dimensionality of neuroimaging data and enabling interpretable analysis. However, most existing atlases are predefined, group-level templates with limited flexibility and resolution. We present Deep Cluster Atlas (DCA), a graph-guided deep embedding clustering framework for generating individualized, voxel-wise brain parcellations. DCA combines a pretrained autoencoder with spatially regularized deep clustering to produce functionally coherent and spatially contiguous regions. Our method supports flexible control over resolution and anatomical scope, and generalizes to arbitrary brain structures. We further introduce a standardized benchmarking platform for atlas evaluation, using multiple large-scale fMRI datasets. Across multiple datasets and scales, DCA outperforms state-of-the-art atlases, improving functional homogeneity by 98.8% and silhouette coefficient by 29%, and achieves superior performance in downstream tasks such as autism diagnosis and cognitive decoding. We also observe that a fine-tuned pretrained model achieves superior results on the corresponding task. Codes and models are available at https://github.com/ncclab-sustech/DCA .</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01426v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Wang, Kaining Peng, Jingsheng Tang, Hongkai Wen, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>A mechanistic model of trust based on neural information processing</title>
      <link>https://arxiv.org/abs/2401.08064</link>
      <description>arXiv:2401.08064v2 Announce Type: replace-cross 
Abstract: Trust is central to human social interactions, manifesting in actions that make one vulnerable to another. We argue that trust will thus depend on the decision-making processes that arise in neural systems. Building on advances in the cognitive neuroscience of decision making, we propose a mechanistic model of trust arising from multiple parallel systems that perform distinct, complementary information processing. Because each system learns via different mechanisms, trust can be created (or destroyed) in multiple ways. This systems-level taxonomy of information representations provides a principled basis for differentiating forms of trust, linking them to specific learning processes, and generating testable predictions about their expression in behavior. By situating trust within a broader theory of neural decision systems, our account unifies diverse findings across psychology, neuroscience, and the social sciences, and offers a foundation for explaining how humans develop, maintain, and repair trust in a complex social world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08064v2</guid>
      <category>econ.GN</category>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Scott E. Allen, Ren\'e F. Kizilcec, A. David Redish</dc:creator>
    </item>
  </channel>
</rss>
