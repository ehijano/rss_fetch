<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Jan 2025 05:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Comparison of STR and EMLSR Performance in Wi-Fi 7 MLO</title>
      <link>https://arxiv.org/abs/2501.04149</link>
      <description>arXiv:2501.04149v1 Announce Type: new 
Abstract: This project compares the performance of simultaneous transmit and receive (STR) and enhanced multi-link single radio (EMLSR) within Multi-Link Operation (MLO) in Wi-Fi 7 networks. Using the ns-3 simulator, we evaluate both techniques under various scenarios, including changes in modulation coding scheme (MCS), bandwidth, link quality, and interference levels. Key performance metrics such as latency, throughput, and energy efficiency are analyzed to determine the trade-offs between STR and EMLSR. The results demonstrate that STR achieves higher throughput and lower latency due to dual-link utilization, making it suitable for high-load environments. In contrast, EMLSR balances energy efficiency with responsiveness, making it advantageous for power-sensitive applications. This analysis provides insights into the strengths and limitations of STR and EMLSR, guiding optimal deployment strategies for future Wi-Fi 7 networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04149v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aishwarya Choorakuzhiyil, Kevin Ho, Sara Reyes</dc:creator>
    </item>
    <item>
      <title>Drift-oriented Self-evolving Encrypted Traffic Application Classification for Actual Network Environment</title>
      <link>https://arxiv.org/abs/2501.04246</link>
      <description>arXiv:2501.04246v1 Announce Type: new 
Abstract: Encrypted traffic classification technology is a crucial decision-making information source for network management and security protection. It has the advantages of excellent response timeliness, large-scale data bearing, and cross-time-and-space analysis. The existing research on encrypted traffic classification has gradually transitioned from the closed world to the open world, and many classifier optimization and feature engineering schemes have been proposed. However, encrypted traffic classification has yet to be effectively applied to the actual network environment. The main reason is that applications on the Internet are constantly updated, including function adjustment and version change, which brings severe feature concept drift, resulting in rapid failure of the classifier. Hence, the entire model must be retrained only past very fast time, with unacceptable labeled sample constructing and model training cost. To solve this problem, we deeply study the characteristics of Internet application updates, associate them with feature concept drift, and then propose self-evolving encrypted traffic classification. We propose a feature concept drift determination method and a drift-oriented self-evolving fine-tuning method based on the Laida criterion to adapt to all applications that are likely to be updated. In the case of no exact label samples, the classifier evolves through fully fine-tuning continuously, and the time interval between two necessary retraining is greatly extended to be applied to the actual network environment. Experiments show that our approach significantly improves the classification performance of the original classifier on the following stage dataset of the following months (9\% improvement on F1-score) without any hard-to-acquire labeled sample. Under the current experimental environment, the life of the classifier is extended to more than eight months.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04246v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihan Chen, Guang Cheng, Jinhui Li, Tian Qin, Yuyang Zhou, Xing Luan</dc:creator>
    </item>
    <item>
      <title>A 5G-Edge Architecture for Computational Offloading of Computer Vision Applications</title>
      <link>https://arxiv.org/abs/2501.04267</link>
      <description>arXiv:2501.04267v1 Announce Type: new 
Abstract: Processing computer vision applications (CVA) on mobile devices is challenging due to limited battery life and computing power. While cloud-based remote processing of CVA offers abundant computational resources, it introduces latency issues that can hinder real-time applications. To overcome this problem, computational offloading to edge servers has been adopted by industry and academic research. Furthermore, 5G access can also benefit CVA with lower latency and higher bandwidth than previous cellular generations. As the number of Mobile Operators and Internet Service providers relying on 5G access is growing, it is of paramount importance to elaborate a solution for supporting real time applications with the assistance of the edge computing. Besides that, open-source based platforms for Multi-access Edge Computing (MEC) and 5G core can be deployed to rapid prototyping and testing applications. This paper aims at providing an end-to-end solution of open-source MEC and 5G Core platforms along with a commercial 5G Radio. We first conceived a 5G-edge computing environment to assist near to user processing of computer vision applications. Then a sentiment analysis application is developed and integrated to the proposed 5G-Edge architecture. Finally, we conducted a performance evaluation of the proposed solution and compare it against a remote cloud-based approach in order to highlight the benefits of our proposal. The proposed architecture achieved a 260\% throughput performance increase and reduced response time by 71.3\% compared to the remote-cloud-based offloading.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04267v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcelo V. B. da Silva, Maria Barbosa, Anderson Queiroz, Kelvin L. Dias</dc:creator>
    </item>
    <item>
      <title>Sustainability in Telecommunication Networks and Key Value Indicators: a Survey</title>
      <link>https://arxiv.org/abs/2501.04356</link>
      <description>arXiv:2501.04356v1 Announce Type: new 
Abstract: Telecommunication technologies are important enablers for both digital and ecological transitions. By offering digital alternatives to traditional modes of transportation and communication, they help reduce carbon footprints while improving access to fundamental services. Particularly in rural and remote areas, telecommunications facilitate access to education, healthcare, and employment, helping to bridge the digital divide. Additionally, telecommunications can promote sustainability by supporting renewable energy usage, gender equality, and circular economies. However, defining the role of telecommunications in sustainability remains complex due to the historical focus on performance rather than long-term societal goals. Given the significance of this theme, this paper aims to provide the reader with a deeper look at the concept of sustainability within the telecommunications sector by examining relevant initiatives and projects. It reviews the major approaches for measuring sustainability and outlines practical approaches for implementing these assessments. Furthermore, the paper explores the proposed network architectures that incorporate Key Value Indicators and discusses major technologies in this area, such as Network Digital Twins and Intent-Based Networking. Through this analysis, the paper aims to contribute to creating sustainable telecommunication networks and broader industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04356v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucia Pintor, Luigi Atzori, Antonio Iera</dc:creator>
    </item>
    <item>
      <title>Towards Beyond Communications 6G Networks Status and Challenges</title>
      <link>https://arxiv.org/abs/2501.04496</link>
      <description>arXiv:2501.04496v1 Announce Type: new 
Abstract: Wireless communication has profoundly transformed the way we experience the world. For instance, at most events, attendees commonly utilize their smartphones to document and share their experiences. This shift in user behavior largely stems from the cellular network s capacity for communication. However, as networks become increasingly sophisticated, new opportunities arise to leverage the network for services beyond mere communication, collectively termed Beyond Communication Services (BCS). These services encompass joint communications and sensing, network as a service, and distributed computing. This paper presents examples of BCS and identifies the enablers necessary to facilitate their realization in sixth generation (6G). These enablers encompass exposing data and network capabilities, optimizing protocols and procedures for BCS, optimizing compute offloading protocols and signalling, and employing application and device-driven optimization strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04496v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasileios Tsekenis, Sokratis Barmpounakis, Panagiotis Demestichas, Stefan W\"anstedt, Mohammad Asif Habibi, Hans D. Schotten, Ozgur Umut Akgul, Hamed Hellaoui, Apostolos Kousaridas, Milan Zivkovic, Panagiotis Botsinis, Sameh Eldessoki, Milan Groshev, Torgny Palenius</dc:creator>
    </item>
    <item>
      <title>Satellite-Terrestrial Routing or Inter-Satellite Routing? A Stochastic Geometry Perspective</title>
      <link>https://arxiv.org/abs/2501.04557</link>
      <description>arXiv:2501.04557v1 Announce Type: new 
Abstract: The design and comparison of satellite-terrestrial routing (STR) and inter-satellite routing (ISR) in low Earth orbit satellite constellations is a widely discussed topic. The signal propagation distance under STR is generally longer than that under ISR, resulting in greater path loss. The global deployment of gateways introduces additional costs for STR. In contrast, transmissions under ISR rely on the energy of satellites, which could be more costly. Additionally, ISLs require more complex communication protocol design, extra hardware support, and increased computational power. To maximize energy efficiency, we propose two optimal routing relay selection algorithms for ISR and STR, respectively. Furthermore, we derive the analytical expressions for the routing availability probability and energy efficiency, quantifying the performance of the algorithms. The analyses enable us to assess the performance of the proposed algorithms against existing methods through numerical results, compare the performance of STR and ISR, and provide useful insights for constellation design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04557v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ruibo Wang, Mustafa A. Kishk, Howard H. Yang, Mohamed-Slim Alouini</dc:creator>
    </item>
    <item>
      <title>Framework for Integrating Machine Learning Methods for Path-Aware Source Routing</title>
      <link>https://arxiv.org/abs/2501.04624</link>
      <description>arXiv:2501.04624v1 Announce Type: new 
Abstract: Since the advent of software-defined networking (SDN), Traffic Engineering (TE) has been highlighted as one of the key applications that can be achieved through software-controlled protocols (e.g. PCEP and MPLS). Being one of the most complex challenges in networking, TE problems involve difficult decisions such as allocating flows, either via splitting them among multiple paths or by using a reservation system, to minimize congestion. However, creating an optimized solution is cumbersome and difficult as traffic patterns vary and change with network scale, capacity, and demand. AI methods can help alleviate this by finding optimized TE solutions for the best network performance. SDN-based TE tools such as Teal, Hecate and more, use classification techniques or deep reinforcement learning to find optimal network TE solutions that are demonstrated in simulation. Routing control conducted via source routing tools, e.g., PolKA, can help dynamically divert network flows. In this paper, we propose a novel framework that leverages Hecate to practically demonstrate TE on a real network, collaborating with PolKA, a source routing tool. With real-time traffic statistics, Hecate uses this data to compute optimal paths that are then communicated to PolKA to allocate flows. Several contributions are made to show a practical implementation of how this framework is tested using an emulated ecosystem mimicking a real P4 testbed scenario. This work proves valuable for truly engineered self-driving networks helping translate theory to practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04624v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Published in The 11th Annual International Workshop on Innovating the Network for Data-Intensive Science (INDIS 2024) in conjunction with Super Computing 2024 conference</arxiv:journal_reference>
      <dc:creator>Anees Al-Najjar, Domingos Paraiso, Mariam Kiran, Cristina Dominicini, Everson Borges, Rafael Guimaraes, Magnos Martinello, Harvey Newman</dc:creator>
    </item>
    <item>
      <title>Computation and Communication Co-scheduling for Timely Multi-Task Inference at the Wireless Edge</title>
      <link>https://arxiv.org/abs/2501.04231</link>
      <description>arXiv:2501.04231v1 Announce Type: cross 
Abstract: In multi-task remote inference systems, an intelligent receiver (e.g., command center) performs multiple inference tasks (e.g., target detection) using data features received from several remote sources (e.g., edge sensors). Key challenges to facilitating timely inference in these systems arise from (i) limited computational power of the sources to produce features from their inputs, and (ii) limited communication resources of the channels to carry simultaneous feature transmissions to the receiver. We develop a novel computation and communication co-scheduling methodology which determines feature generation and transmission scheduling to minimize inference errors subject to these resource constraints. Specifically, we formulate the co-scheduling problem as a weakly-coupled Markov decision process with Age of Information (AoI)-based timeliness gauging the inference errors. To overcome its PSPACE-hard complexity, we analyze a Lagrangian relaxation of the problem, which yields gain indices assessing the improvement in inference error for each potential feature generation-transmission scheduling action. Based on this, we develop a maximum gain first (MGF) policy which we show is asymptotically optimal for the original problem as the number of inference tasks increases. Experiments demonstrate that MGF obtains significant improvements over baseline policies for varying tasks, channels, and sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04231v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Kamran Chowdhury Shisher, Adam Piaseczny, Yin Sun, Christopher G. Brinton</dc:creator>
    </item>
    <item>
      <title>Tracking UWB Devices Through Radio Frequency Fingerprinting Is Possible</title>
      <link>https://arxiv.org/abs/2501.04401</link>
      <description>arXiv:2501.04401v1 Announce Type: cross 
Abstract: Ultra-wideband (UWB) is a state-of-the-art technology designed for applications requiring centimeter-level localization. Its widespread adoption by smartphone manufacturer naturally raises security and privacy concerns. Successfully implementing Radio Frequency Fingerprinting (RFF) to UWB could enable physical layer security, but might also allow undesired tracking of the devices. The scope of this paper is to explore the feasibility of applying RFF to UWB and investigates how well this technique generalizes across different environments. We collected a realistic dataset using off-the-shelf UWB devices with controlled variation in device positioning. Moreover, we developed an improved deep learning pipeline to extract the hardware signature from the signal data. In stable conditions, the extracted RFF achieves over 99% accuracy. While the accuracy decreases in more changing environments, we still obtain up to 76% accuracy in untrained locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04401v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibaud Ardoin, Niklas Pauli, Benedikt Gro{\ss}, Mahsa Kholghi, Khan Reaz, Gerhard Wunder</dc:creator>
    </item>
    <item>
      <title>Demystification and Near-perfect Estimation of Minimum Gas Limit and Gas Used for Ethereum Smart Contracts</title>
      <link>https://arxiv.org/abs/2501.04483</link>
      <description>arXiv:2501.04483v1 Announce Type: cross 
Abstract: The Ethereum blockchain has a \emph{gas system} that associates operations with a cost in gas units. Two central concepts of this system are the \emph{gas limit} assigned by the issuer of a transaction and the \emph{gas used} by a transaction. The former is a budget that must not be exhausted before the completion of the transaction execution; otherwise, the execution fails. Therefore, it seems rather essential to determine the \emph{minimum gas limit} that ensures the execution of a transaction will not abort due to the lack of gas. Despite its practical relevance, this concept has not been properly addressed. In the literature, gas used and minimum gas limit are conflated. This paper proposes a precise notion of minimum gas limit and how it can differ from gas used by a transaction; this is also demonstrated with a quantitative study on real transactions of the Ethereum blockchain. Another significant contribution is the proposition of a fairly precise estimator for each of the two metrics. Again, the confusion between these concepts has led to the creation of estimators only for the gas used by a transaction. We demonstrate that the minimum gas limit for the state of the Ethereum blockchain (after the block) $t$ can serve as a near-perfect estimation for the execution of the transaction at block $t + \Delta$, where $\Delta \leq 11$; the same holds for estimating gas used. These precise estimators can be very valuable in helping the users predict the gas budget of transactions and developers in optimising their smart contracts; over and underestimating gas used and minimum gas limit can lead to a number of practical issues. Overall, this paper serves as an important reference for blockchain developers and users as to how the gas system really works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04483v1</guid>
      <category>cs.SE</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danilo Rafael de Lima Cabral, Pedro Antonino, Augusto Sampaio</dc:creator>
    </item>
  </channel>
</rss>
