<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Dec 2025 02:30:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Graphon-Level Bayesian Predictive Synthesis for Random Network</title>
      <link>https://arxiv.org/abs/2512.18587</link>
      <description>arXiv:2512.18587v1 Announce Type: new 
Abstract: Bayesian predictive synthesis provides a coherent Bayesian framework for combining multiple predictive distributions, or agents, into a single updated prediction, extending Bayesian model averaging to allow general pooling of full predictive densities. This paper develops a static, graphon level version of Bayesian predictive synthesis for random networks. At the graphon level we show that Bayesian predictive synthesis corresponds to the integrated squared error projection of the true graphon onto the linear span of the agent graphons. We derive nonasymptotic oracle inequalities and prove that least-squares graphon-BPS, based on a finite number of edge observations, achieves the minimax L^2 rate over this agent span. Moreover, we show that any estimator that selects a single agent graphon is uniformly inconsistent on a nontrivial subset of the convex hull of the agents, whereas graphon-level Bayesian predictive synthesis remains minimax-rate optimal-formalizing a combination beats components phenomenon. Structural properties of the underlying random graphs are controlled through explicit Lipschitz bounds that transfer graphon error into error for edge density, degree distributions, subgraph densities, clustering coefficients, and giant component phase transitions. Finally, we develop a heavy tail theory for Bayesian predictive synthesis, showing how mixtures and entropic tilts preserve regularly varying degree distributions and how exponential random graph model agents remain within their family under log linear tilting with Kullback-Leibler optimal moment calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18587v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Papamichalis, Regina Ruane</dc:creator>
    </item>
    <item>
      <title>Wavelet Latent Position Exponential Random Graphs</title>
      <link>https://arxiv.org/abs/2512.18592</link>
      <description>arXiv:2512.18592v1 Announce Type: new 
Abstract: Many network datasets exhibit connectivity with variance by resolution and large-scale organization that coexists with localized departures. When vertices have observed ordering or embedding, such as geography in spatial and village networks, or anatomical coordinates in connectomes, learning where and at what resolution connectivity departs from a baseline is crucial. Standard models typically emphasize a single representation, i.e. stochastic block models prioritize coarse partitions, latent space models prioritize global geometry, small-world generators capture local clustering with random shortcuts, and graphon formulations are fully general and do not solely supply a canonical multiresolution parameterization for interpretation and regularization. We introduce wavelet latent position exponential random graphs (WL-ERGs), an exchangeable logistic-graphon framework in which the log-odds connectivity kernel is represented in compactly supported orthonormal wavelet coordinates and mapped to edge probabilities through a logistic link. Wavelet coefficients are indexed by resolution and location, which allows multiscale structure to become sparse and directly interpretable. Although edges remain independent given latent coordinates, any finite truncation yields a conditional exponential family whose sufficient statistics are multiscale wavelet interaction counts and conditional laws admit a maximum-entropy characterization. These characteristics enable likelihood-based regularization and testing directly in coefficient space. The theory is naturally scale-resolved and includes universality for broad classes of logistic graphons, near-minimax estimation under multiscale sparsity, scale-indexed recovery and detection thresholds, and a band-limited regime in which canonical coefficient-space tilts are non-degenerate and satisfy a finite-dimensional large deviation principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18592v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Papamichalis, Regina Ruane</dc:creator>
    </item>
    <item>
      <title>A pivotal transform for the high-dimensional location-scale model</title>
      <link>https://arxiv.org/abs/2512.18705</link>
      <description>arXiv:2512.18705v1 Announce Type: new 
Abstract: We study the high-dimensional linear model with noise distribution known up to a scale parameter. With an $\ell_1$-penalty on the regression coefficients, we show that a transformation of the log-likelihood allows for a choice of the tuning parameter not depending on the scale parameter. This transformation is a generalization of the square root Lasso for quadratic loss. The tuning parameter can asymptotically be taken at the detection edge. We establish an oracle inequality, variable selection and asymptotic efficiency of the estimator of the scale parameter and the intercept. The examples include Subbotin distributions and the Gumbel distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18705v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sara van de Geer, Sylvain Sardy, Maxim\k{e} van Cutsem</dc:creator>
    </item>
    <item>
      <title>Model-Agnostic Bounds for Augmented Inverse Probability Weighted Estimators' Wald-Confidence Interval Coverage in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2512.18898</link>
      <description>arXiv:2512.18898v1 Announce Type: new 
Abstract: Nonparametric estimators, such as the augmented inverse probability weighted (AIPW) estimator, have become increasingly popular in causal inference. Numerous nonparametric estimators have been proposed, but they are all asymptotically normal with the same asymptotic variance under similar conditions, leaving little guidance for practitioners to choose an estimator. In this paper, I focus on another important perspective of their asymptotic behaviors beyond asymptotic normality, the convergence of the Wald-confidence interval (CI) coverage to the nominal coverage. Such results have been established for simpler estimators (e.g., the Berry-Esseen Theorem), but are lacking for nonparametric estimators. I consider a simple but practical setting where the AIPW estimator based on a black-box nuisance estimator, with or without cross-fitting, is used to estimate the average treatment effect in randomized controlled trials. I derive non-asymptotic Berry-Esseen-type bounds on the difference between Wald-CI coverage and the nominal coverage. I also analyze the bias of variance estimators, showing that the cross-fit variance estimator might overestimate while the non-cross-fit variance estimator might underestimate, which might explain why cross-fitting has been empirically observed to improve Wald-CI coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18898v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxiang Qiu</dc:creator>
    </item>
    <item>
      <title>Operator Tail Densities of Multivariate Copulas</title>
      <link>https://arxiv.org/abs/2512.19023</link>
      <description>arXiv:2512.19023v1 Announce Type: new 
Abstract: Operator regular variation of a multivariate distribution can be decomposed into the operator tail dependence of the underlying copula and the regular variation of the univariate marginals. In this paper, we introduce operator tail densities for copulas and show that an operator-regularly-varying density can be characterized through the operator tail density of its copula together with the marginal regular variation. As an example, we demonstrate that although a Liouville copula is not available in closed form, it nevertheless admits an explicit operator tail-dependence function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19023v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haijun Li</dc:creator>
    </item>
    <item>
      <title>Simple Cubic Variance Functions on $\R^n$, Part one</title>
      <link>https://arxiv.org/abs/2512.19303</link>
      <description>arXiv:2512.19303v1 Announce Type: new 
Abstract: The classification of natural exponential families started with the paper \cite {Morri} where Carl Morris unifies six very familiar families by the fact that their variance functions are polynomials of degree less or equal to two. Extension of this classification to $\R^n$ and to degree three is the subject of this paper.
  Keywords: Actions of the group $GL(n+1,\R)$, classification of natural exponential families, multivariate Lagrange formula. variance functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19303v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdelhanid Hassairi, G\'erard Letac</dc:creator>
    </item>
    <item>
      <title>A hybrid-Hill estimator enabled by heavy-tailed block maxima</title>
      <link>https://arxiv.org/abs/2512.19338</link>
      <description>arXiv:2512.19338v1 Announce Type: new 
Abstract: When analysing extreme values, two alternative statistical approaches have historically been held in contention: the seminal block maxima method (or annual maxima method, spurred by hydrological applications) and the peaks-over-threshold. Clamoured amongst statisticians as wasteful of potentially informative data, the block maxima method gradually fell into disfavour whilst peaks-over-threshold-based methodologies were ushered to the centre stage of extreme value statistics. This paper proposes a hybrid method which reconciles these two hitherto disconnected approaches. Appealing in its simplicity, our main result introduces a new universal limiting characterisation of extremes that eschews the customary requirement of a sufficiently large block size for the plausible block maxima-fit to an extreme value distribution. We advocate that inference should be drawn solely on larger block maxima, from which practice the mainstream peaks-over-threshold methodology coalesces. The asymptotic properties of the promised hybrid-Hill estimator herald more than its efficiency, but rather that a fully-fledged unified semi-parametric stream of statistics for extreme values is viable. A finite sample simulation study demonstrates that a reduced-bias off-shoot of the hybrid-Hill estimator fares exceptionally well against the incumbent maximum likelihood estimation that relies on a numerical fit to the entire sample of block maxima.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19338v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Neves, Chang Xu</dc:creator>
    </item>
    <item>
      <title>Sampling from multimodal distributions with warm starts: Non-asymptotic bounds for the Reweighted Annealed Leap-Point Sampler</title>
      <link>https://arxiv.org/abs/2512.17977</link>
      <description>arXiv:2512.17977v1 Announce Type: cross 
Abstract: Sampling from multimodal distributions is a central challenge in Bayesian inference and machine learning. In light of hardness results for sampling -- classical MCMC methods, even with tempering, can suffer from exponential mixing times -- a natural question is how to leverage additional information, such as a warm start point for each mode, to enable faster mixing across modes. To address this, we introduce Reweighted ALPS (Re-ALPS), a modified version of the Annealed Leap-Point Sampler (ALPS) that dispenses with the Gaussian approximation assumption. We prove the first polynomial-time bound that works in a general setting, under a natural assumption that each component contains significant mass relative to the others when tilted towards the corresponding warm start point. Similarly to ALPS, we define distributions tilted towards a mixture centered at the warm start points, and at the coldest level, use teleportation between warm start points to enable efficient mixing across modes. In contrast to ALPS, our method does not require Hessian information at the modes, but instead estimates component partition functions via Monte Carlo. This additional estimation step is crucial in allowing the algorithm to handle target distributions with more complex geometries besides approximate Gaussian. For the proof, we show convergence results for Markov processes when only part of the stationary distribution is well-mixing and estimation for partition functions for individual components of a mixture. We numerically evaluate our algorithm's mixing performance compared to ALPS on a mixture of heavy-tailed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17977v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Holden Lee, Matheau Santana-Gijzen</dc:creator>
    </item>
    <item>
      <title>Inference in partially identified moment models via regularized optimal transport</title>
      <link>https://arxiv.org/abs/2512.18084</link>
      <description>arXiv:2512.18084v1 Announce Type: cross 
Abstract: Partial identification often arises when the joint distribution of the data is known only up to its marginals. We consider the corresponding partially identified GMM model and develop a methodology for identification, estimation, and inference in this model. We characterize the sharp identified set for the parameter of interest via a support-function/optimal-transport (OT) representation. For estimation, we employ entropic regularization, which provides a smooth approximation to classical OT and can be computed efficiently by the Sinkhorn algorithm. We also propose a statistic for testing hypotheses and constructing confidence regions for the identified set. To derive the asymptotic distribution of this statistic, we establish a novel central limit theorem for the entropic OT value under general smooth costs. We then obtain valid critical values using the bootstrap for directionally differentiable functionals of Fang and Santos (2019). The resulting testing procedure controls size locally uniformly, including at parameter values on the boundary of the identified set. We illustrate its performance in a Monte Carlo simulation. Our methodology is applicable to a wide range of empirical settings, such as panels with attrition and refreshment samples, nonlinear treatment effects, nonparametric instrumental variables without large-support conditions, and Euler equations with repeated cross-sections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18084v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigory Franguridi, Laura Liu</dc:creator>
    </item>
    <item>
      <title>Copula Entropy: Theory and Applications</title>
      <link>https://arxiv.org/abs/2512.18168</link>
      <description>arXiv:2512.18168v1 Announce Type: cross 
Abstract: This is the monograph on the theory and applications of copula entropy (CE). This book first introduces the theory of CE, including its background, definition, theorems, properties, and estimation methods. The theoretical applications of CE to structure learning, association discovery, variable selection, causal discovery, system identification, time lag estimation, domain adaptation, multivariate normality test, copula hypothesis test, two-sample test, change point detection, and symmetry test are reviewed. The relationships between the theoretical applications and their connections to correlation and causality are discussed. The framework based on CE for measuring statistical independence and conditional independence is compared to the other similar ones. The advantages of CE based methodologies over the other comparable ones are evaluated with simulations. The mathematical generalizations of CE are reviewed. The real applications of CE to every branch of science and engineering are briefly introduced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18168v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Ma</dc:creator>
    </item>
    <item>
      <title>State-Space Modeling of Time-Varying Spillovers on Networks</title>
      <link>https://arxiv.org/abs/2512.18584</link>
      <description>arXiv:2512.18584v1 Announce Type: cross 
Abstract: Many modern time series arise on networks, where each component is attached to a node and interactions follow observed edges. Classical time-varying parameter VARs (TVP-VARs) treat all series symmetrically and ignore this structure, while network autoregressive models exploit a given graph but usually impose constant parameters and stationarity. We develop network state-space models in which a low-dimensional latent state controls time-varying network spillovers, own-lag persistence and nodal covariate effects. A key special case is a network time-varying parameter VAR (NTVP-VAR) that constrains each lag matrix to be a linear combination of known network operators, such as a row-normalised adjacency and the identity, and lets the associated coefficients evolve stochastically in time. The framework nests Gaussian and Poisson network autoregressions, network ARIMA models with graph differencing, and dynamic edge models driven by multivariate logistic regression. We give conditions ensuring that NTVP-VARs are well-defined in second moments despite nonstationary states, describe network versions of stability and local stationarity, and discuss shrinkage, thresholding and low-rank tensor structures for high-dimensional graphs. Conceptually, network state-space models separate where interactions may occur (the graph) from how strong they are at each time (the latent state), providing an interpretable alternative to both unstructured TVP-VARs and existing network time-series models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18584v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Papamichalis, Regina Ruane, Theofanis Papamichalis</dc:creator>
    </item>
    <item>
      <title>Convergence of the adapted empirical measure for mixing observations</title>
      <link>https://arxiv.org/abs/2512.18838</link>
      <description>arXiv:2512.18838v1 Announce Type: cross 
Abstract: The adapted Wasserstein distance $\mathcal{AW}$ is a modification of the classical Wasserstein metric, that provides robust and dynamically consistent comparisons of laws of stochastic processes, and has proved particularly useful in the analysis of stochastic control problems, model uncertainty, and mathematical finance. In applications, the law of a stochastic process $\mu$ is not directly observed, and has to be inferred from a finite number of samples. As the empirical measure is not $\mathcal{AW}$-consistent, Backhoff, Bartl, Beiglb\"ock and Wiesel introduced the adapted empirical measure $\widehat{\mu}^N$, a suitable modification, and proved its $\mathcal{AW}$-consistency when observations are i.i.d.
  In this paper we study $\mathcal{AW}$-convergence of the adapted empirical measure $\widehat{\mu}^N$ to the population distribution $\mu$, for observations satisfying a generalization of the $\eta$-mixing condition introduced by Kontorovich and Ramanan. We establish moment bounds and sub-exponential concentration inequalities for $\mathcal{AW}(\mu,\widehat{\mu}^N)$, and prove consistency of $\widehat{\mu}^N$. In addition, we extend the Bounded Differences inequality of Kontorovich and Ramanan for $\eta$-mixing observations to uncountable spaces, a result that may be of independent interest. Numerical simulations illustrating our theory are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18838v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruslan Mirmominov, Johannes Wiesel</dc:creator>
    </item>
    <item>
      <title>Testing for latent structure via the Wilcoxon--Wigner random matrix of normalized rank statistics</title>
      <link>https://arxiv.org/abs/2512.18924</link>
      <description>arXiv:2512.18924v1 Announce Type: cross 
Abstract: This paper considers the problem of testing for latent structure in large symmetric data matrices. The goal here is to develop statistically principled methodology that is flexible in its applicability, computationally efficient, and insensitive to extreme data variation, thereby overcoming limitations facing existing approaches. To do so, we introduce and systematically study certain symmetric matrices, called Wilcoxon--Wigner random matrices, whose entries are normalized rank statistics derived from an underlying independent and identically distributed sample of absolutely continuous random variables. These matrices naturally arise as the matricization of one-sample problems in statistics and conceptually lie at the interface of nonparametrics, multivariate analysis, and data reduction. Among our results, we establish that the leading eigenvalue and corresponding eigenvector of Wilcoxon--Wigner random matrices admit asymptotically Gaussian fluctuations with explicit centering and scaling terms. These asymptotic results enable rigorous parameter-free and distribution-free spectral methodology for addressing two hypothesis testing problems, namely community detection and principal submatrix detection. Numerical examples illustrate the performance of the proposed approach. Throughout, our findings are juxtaposed with existing results based on the spectral properties of independent entry symmetric random matrices in signal-plus-noise data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18924v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonquil Z. Liao, Joshua Cape</dc:creator>
    </item>
    <item>
      <title>Sharp Decoupling Inequalities for the Variances and Second Moments of Sums of Dependent Random Variables</title>
      <link>https://arxiv.org/abs/2512.19063</link>
      <description>arXiv:2512.19063v1 Announce Type: cross 
Abstract: Both complete decoupling and tangent decoupling are classical tools aiming to compare two random processes where one has a weaker dependence structure. We give a new proof for the complete decoupling inequality, which provides a lower bound for the sum of dependent square-integrable nonnegative random variables $\sum\limits^n_{i=1} d_i$ \[ \frac{1}{2} \mathbb E \left( \sum\limits^n_{i=1} z_i \right)^2 \leq \mathbb E \left( \sum\limits^n_{i=1} d_i \right)^2, \] where $z_i \stackrel{\mathcal{L}}{=} d_i$ for all $i\leq n$ and $z_i$'s are mutually independent. We will then provide the following sharp tangent decoupling inequalities \[\mathbb Var \left( \sum\limits^n_{i=1} d_i\right) \leq 2 \mathbb Var \left( \sum\limits^n_{i=1} e_i\right),\] and \[\mathbb E \left( \sum\limits^n_{i=1} d_i\right)^2 \leq 2 \mathbb E \left( \sum\limits^n_{i=1} e_i\right)^2 - \left[ \mathbb E \left( \sum\limits^n_{i=1} e_i\right) \right]^2,\] where $\{e_i\}$ is the decoupled sequences of $\{d_i\}$ and $d_i$'s are not forced to be nonnegative. Applications to construct Chebyshev-type inequality and Paley-Zygmund-type inequality, and to bound the second moments of randomly stopped sums will be provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19063v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor H. de la Pena, Heyuan Yao, Demissie Alemayehu</dc:creator>
    </item>
    <item>
      <title>Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models</title>
      <link>https://arxiv.org/abs/2512.19334</link>
      <description>arXiv:2512.19334v1 Announce Type: cross 
Abstract: We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19334v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haohua Chen, Songbin Liu, Junjie Ma</dc:creator>
    </item>
    <item>
      <title>Finite sample rates of convergence for the Bigraphical and Tensor graphical Lasso estimators</title>
      <link>https://arxiv.org/abs/2304.00441</link>
      <description>arXiv:2304.00441v4 Announce Type: replace 
Abstract: Many modern datasets exhibit dependencies among observations as well as variables. A decade ago, Kalaitzis et. al. (2013) proposed the Bigraphical Lasso, an estimator for precision matrices of matrix-normals based on the Cartesian product of graphs; they observed that the associativity of the Kronecker sum yields an approach to the modeling of datasets organized into 3 or higher-order tensors. Subsequently, Greenewald, Zhou and Hero (2019) explored this possibility to a great extent, by introducing the tensor graphical Lasso (TeraLasso) for estimating sparse $L$-way decomposable inverse covariance matrices for all $L \ge 2$, and showing the rates of convergence in the Frobenius and operator norms for estimating this class of inverse covariance matrices for sub-gaussian tensor-valued data. In this paper, we provide sharper rates of convergence for both Bigraphical and TeraLasso estimators for inverse covariance matrices. This improves upon the rates presented in GZH 2019. In particular, (a) we strengthen the bounds for the relative errors in the operator and Frobenius norm by a factor of approximately $\log p$; (b) Crucially, this improvement allows for finite sample estimation errors in both norms to be derived for the two-way Kronecker sum model. This closes the gap between the low single-sample error for the two-way model as observed in GZH 2019 and the lack of theoretical guarantee for this particular case. The two-way regime is important because it is the setting that is the most theoretically challenging, and simultaneously the most common in applications. In the current paper, we elaborate on the Kronecker Sum model, highlight the proof strategy and provide full proofs of all main theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00441v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuheng Zhou, Kristjan Greenewald</dc:creator>
    </item>
    <item>
      <title>Ordering sampling rules for sequential anomaly identification under sampling constraints</title>
      <link>https://arxiv.org/abs/2309.14528</link>
      <description>arXiv:2309.14528v2 Announce Type: replace 
Abstract: We consider the problem of sequential anomaly identification over multiple independent data streams, under the presence of a sampling constraint. The goal is to quickly identify those that exhibit anomalous statistical behavior, when it is not possible to sample every source at each time instant. Thus, in addition to a stopping rule that determines when to stop sampling, and a decision rule that indicates which sources to identify as anomalous upon stopping, one needs to specify a sampling rule that determines which sources to sample at each time instant. We focus on the family of ordering sampling rules that select the sources to be sampled at each time instant based not only on the currently estimated subset of anomalous sources as the probabilistic sampling rules \cite{Tsopela_2022}, but also on the ordering of the sources' test-statistics. We show that under an appropriate design specified explicitly, an ordering sampling rule leads to the optimal expected time for stopping among all policies that satisfy the same sampling and error constraints to a first-order asymptotic approximation as the false positive and false negative error thresholds go to zero. This is the first asymptotic optimality result for ordering sampling rules, when more than one sources can be sampled per time instant, and it is established under a general setup where the number of anomalous sources is not required to be known. A novel proof technique is introduced that encompasses all different cases of the problem concerning sources' homogeneity, and prior information on the number of anomalies. Simulations show that ordering sampling rules have better performance in finite regime compared to probabilistic sampling rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14528v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aristomenis Tsopelakos, Georgios Fellouris</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory for nonparametric testing of $k$-monotonicity in discrete distributions</title>
      <link>https://arxiv.org/abs/2407.01751</link>
      <description>arXiv:2407.01751v2 Announce Type: replace 
Abstract: In shape-constrained nonparametric inference, it is often necessary to perform preliminary tests to verify whether a probability mass function (p.m.f.) satisfies qualitative constraints such as monotonicity, convexity, or in general $k$-monotonicity. In this paper, we are interested in nonparametric testing of $k$-monotonicity of a finitely supported discrete distribution. We consider a unified testing framework based on a natural statistic which is directly derived from the very definition of $k$-monotonicity. The introduced framework allows us to design a new consistent method to select the unknown knot points that are required to consistently approximate the limit distribution of several test statistics based either on the empirical measure or the shape-constrained estimators of the p.m.f. We show that the resulting tests are asymptotically valid and consistent for any fixed alternative. Additionally, for the test based solely on the empirical measure, we study the asymptotic power under contiguous alternatives and derive a quantitative separation result that provides sufficient conditions to achieve a given power. We employ this test to design an estimator for the largest parameter $k \in \mathbb N_0$ such that the p.m.f. is $j$-monotone for all $j = 0, \ldots, k$, and show that the estimator is different from the true parameter with probability which is asymptotically smaller than the nominal level of the test. A detailed simulation study is performed to assess the finite sample performance of all the proposed tests, and applications to several real datasets are presented to illustrate the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01751v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadoua Balabdaoui, Antonio Di Noia</dc:creator>
    </item>
    <item>
      <title>Fast convergence rates for estimating the stationary density in SDEs driven by a fractional Brownian motion with semi-contractive drift</title>
      <link>https://arxiv.org/abs/2408.15904</link>
      <description>arXiv:2408.15904v2 Announce Type: replace 
Abstract: We study the estimation of the invariant density of additive fractional stochastic differential equations with Hurst parameter $H \in (0,1)$. We first focus on continuous observations and develop a kernel-based estimator achieving faster convergence rates than previously available. This result stems from a martingale decomposition combined with new bounds on the (conditional) convergence in total variation to equilibrium of fractional SDEs. For $H&lt;1/2$, we further refine the rates based on recent bounds on the marginal density. We then extend the methodology to discrete observations, showing that the same convergence rates can be attained. Moreover, we establish concentration inequalities for the estimator and introduce a data-driven bandwidth selection procedure that adapts to unknown smoothness. Numerical experiments for the fractional Ornstein-Uhlenbeck process illustrate the estimator's practical performance. Finally, our results weaken the usual convexity assumptions on the drift component, allowing us to consider settings where strong convexity only holds outside a compact set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15904v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Eulalia Nualart, Fabien Panloup, Julian Sieber</dc:creator>
    </item>
    <item>
      <title>Optimal sequencing depth for single-cell RNA-sequencing in Wasserstein space</title>
      <link>https://arxiv.org/abs/2409.14326</link>
      <description>arXiv:2409.14326v2 Announce Type: replace 
Abstract: How many samples should one collect for an empirical distribution to be as close as possible to the true population? This question is not trivial in the context of single-cell RNA-sequencing. With limited sequencing depth, profiling more cells comes at the cost of fewer reads per cell. Therefore, one must strike a balance between the number of cells sampled and the accuracy of each measured gene expression profile. In this paper, we analyze an empirical distribution of cells and obtain upper and lower bounds on the Wasserstein distance to the true population. Our analysis holds for general, non-parametric distributions of cells, and is validated by simulation experiments on a real single-cell dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14326v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakwang Kim, Sharvaj Kubal, Geoffrey Schiebinger</dc:creator>
    </item>
    <item>
      <title>Anytime Validity is Free: Inducing Sequential Tests</title>
      <link>https://arxiv.org/abs/2501.03982</link>
      <description>arXiv:2501.03982v5 Announce Type: replace 
Abstract: Anytime valid sequential tests permit us to stop testing based on the current data, without invalidating the inference. Given a maximum number of observations $N$, one may believe this must come at the cost of power when compared to a conventional test that waits until all $N$ observations have arrived. Our first contribution is to show that this is false: for any valid test based on $N$ observations, we show how to construct an anytime valid sequential test that matches it after $N$ observations. Our second contribution is that we may continue testing by using the outcome of a $[0, 1]$-valued test as a conditional significance level in subsequent testing, leading to an overall procedure that is valid at the original significance level. This shows that anytime validity and optional continuation are readily available in traditional testing, without requiring explicit use of e-values. We illustrate this by deriving the anytime valid sequentialized $z$-test and $t$-test, which at time $N$ coincide with the traditional $z$-test and $t$-test. Finally, we characterize the SPRT by invariance under test induction, and also show under an i.i.d. assumption that the SPRT is induced by the Neyman-Pearson test for a tiny significance level and huge $N$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03982v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>Unifying Different Theories of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2504.02292</link>
      <description>arXiv:2504.02292v2 Announce Type: replace 
Abstract: This paper presents a unified framework for understanding the methodology and theory behind several different methods in the conformal prediction literature, which includes standard conformal prediction (CP), weighted conformal prediction (WCP), nonexchangeable conformal prediction (NexCP), and randomly-localized conformal prediction (RLCP), among others. At the crux of our framework is the idea that conformal methods are based on revealing partial information about the data at hand, and positing a conditional distribution for the data given the partial information. Different methods arise from different choices of partial information, and of the corresponding (approximate) conditional distribution. In addition to recovering and unifying existing results, our framework leads to both new theoretical guarantees for existing methods, and new extensions of the conformal methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02292v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rina Foygel Barber, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>Estimation of Population Linear Spectral Statistics by Marchenko--Pastur Inversion</title>
      <link>https://arxiv.org/abs/2504.03390</link>
      <description>arXiv:2504.03390v4 Announce Type: replace 
Abstract: A new method of estimating population linear spectral statistics from high-dimensional data is introduced. When the dimension $d$ grows with the sample size $n$ such that $\frac{d}{n} \to c&gt;0$, the proposed method is the first with proven convergence rate of $\mathcal{O}(n^{\varepsilon - 1})$ for any $\varepsilon &gt; 0$ in a general nonparametric setting. For Gaussian data, a CLT for the estimation error with normalization factor $n$ is shown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03390v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Deitmar</dc:creator>
    </item>
    <item>
      <title>Linear Regression Using Principal Components from General Hilbert-Space-Valued Covariates</title>
      <link>https://arxiv.org/abs/2504.16780</link>
      <description>arXiv:2504.16780v3 Announce Type: replace 
Abstract: We consider linear regression with covariates that are random elements in a general Hilbert space. We first develop a principal component analysis for Hilbert-space-valued covariates based on finite-dimensional projections of the covariance operator, and establish asymptotic linearity and joint Gaussian limits for the leading eigenvalues and eigenfunctions under mild moment conditions. We then propose a principal component regression framework that combines Euclidean and Hilbert-space-valued covariates, obtain root-n consistent and asymptotically normal estimators of the regression parameters, and establish the validity of nonparametric and wild bootstrap procedures for inference. Simulation studies with two- and three-dimensional imaging predictors demonstrate accurate recovery of eigenstructures, regression coefficients, and bootstrap coverage. The methodology is further illustrated with neuroimaging data, in both a standard regression setting and a precision-medicine formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16780v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Li, Margaret Hoch, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>Density estimation via periodic scaled Korobov kernel method with exponential decay condition</title>
      <link>https://arxiv.org/abs/2506.15419</link>
      <description>arXiv:2506.15419v2 Announce Type: replace 
Abstract: We propose the periodic scaled Korobov kernel (PSKK) method for nonparametric density estimation on $\mathbb{R}^d$. By first wrapping the target density into a periodic version through modulo operation and subsequently applying kernel ridge regression in scaled Korobov spaces, we extend the kernel approach proposed by Kazashi and Nobile (SIAM J. Numer. Anal., 2023) and eliminate its requirement for inherent periodicity of the density function. This key modification enables effective estimation of densities defined on unbounded domains. We establish rigorous mean integrated squared error (MISE) bounds, proving that for densities with smoothness of order $\alpha$ and exponential decay, our PSKK method achieves an $\mathcal{O}(M^{-1/(1+1/(2\alpha)+\epsilon)})$ MISE convergence rate with an arbitrarily small $\epsilon&gt;0$. While matching the convergence rate of the previous kernel approach, our method applies to non-periodic distributions at the cost of stronger differentiability and exponential decay assumptions. Numerical experiments confirm the theoretical results and demonstrate a significant improvement over traditional kernel density estimation in large-sample regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15419v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Ye, Haoyuan Tan, Xiaoqun Wang, Zhijian He</dc:creator>
    </item>
    <item>
      <title>On Random Fields Associated with Analytic Wavelet Transform</title>
      <link>https://arxiv.org/abs/2508.10495</link>
      <description>arXiv:2508.10495v2 Announce Type: replace 
Abstract: Despite the broad application of the analytic wavelet transform (AWT), a systematic statistical characterization of its magnitude and phase as inhomogeneous random fields on the time-frequency domain when the input is a random process remains underexplored. In this work, we study the magnitude and phase of the AWT as random fields on the time-frequency domain when the observed signal is a deterministic function plus additive stationary Gaussian noise. We derive their marginal and joint distributions, establish concentration inequalities that depend on the signal-to-noise ratio (SNR), and analyze their covariance structures. Based on these results, we derive an upper bound on the probability of incorrectly identifying the time-scale ridge of the clean signal, explore the regularity of scalogram contours, and study the relationship between AWT magnitude and phase. Our findings lay the groundwork for developing rigorous AWT-based algorithms in noisy environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10495v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gi-Ren Liu, Yuan-Chung Sheu, Hau-Tieng Wu</dc:creator>
    </item>
    <item>
      <title>On the class of exponential statistical structures of type B</title>
      <link>https://arxiv.org/abs/2510.26863</link>
      <description>arXiv:2510.26863v2 Announce Type: replace 
Abstract: The article is devoted to the study of exponential statistical structures of type B, which constitute a subclass of exponential families of probability distributions. This class is characterized by a number of analytical and probabilistic properties that make it a convenient tool for solving both theoretical and applied problems in statistics. The relevance of this research lies in the need to generalize known classes of distributions and to develop a unified framework for their analysis, which is essential for applications in stochastic modeling, machine learning, financial mathematics. The paper proposes a formal definition of type B. Necessary and sufficient conditions for a statistical structure to belong to class B are established, and it is proved that such structures can be represented through a dominating measure with an explicit Laplace transform. The obtained results make it possible to describe a wide range of well-known one-dimensional and multivariate distributions, including the Binomial, Poisson, Normal, Gamma, Polynomial, Logarithmic distributions, as well as specific cases such as the Borel-Tanner and Random Walk distributions. Particular attention is given to the proof of structural theorems that determine the stability of class B under linear transformations and the addition of independent random vectors. Recursive relations for initial and central moments as well as for semi-invariants are obtained, providing an efficient analytical and computational framework for their evaluation. Furthermore, the tails of type B distributions are investigated using the properties of the Laplace transform. New exponential inequalities for estimating the probabilities of large deviations are derived. The obtained results can be applied in theoretical studies and in practical problems of stochastic modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26863v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oleksandr Volkov, Yurii Volkov</dc:creator>
    </item>
    <item>
      <title>Study of power series distributions with specified covariances</title>
      <link>https://arxiv.org/abs/2511.01081</link>
      <description>arXiv:2511.01081v2 Announce Type: replace 
Abstract: This paper presents a study of power series distributions (PSD) with prescribed covariance characteristics. Such distributions constitute a fundamental class in probability theory and mathematical statistics, as they generalize a wide range of well-known discrete distributions and enable the description of various stochastic phenomena with a predetermined variance structure. The aim of the research is to develop analytical methods for constructing power series distributions with given covariances and to establish the conditions under which a particular function can serve as the covariance of a certain PSD. The paper derives a first-order differential equation for the generating function of the distribution, which determines the relationship between its parameters and the form of the covariance function. It is shown that the choice of an analytical or polynomial covariance completely specifies the structure of the corresponding generating function. The analysis made it possible to construct new families of PSDs that generalize the classical Bernoulli, Poisson, geometric, and other distributions while preserving a given covariance structure. The proposed approach is based on the analytical relationship between the generating function and the covariance function, providing a framework for constructing stochastic models with predefined dispersion properties. The results obtained expand the theoretical framework for describing discrete distributions and open up opportunities for practical applications in statistical estimation, modeling of complex systems, financial processes, machine learning where it is crucial to control the dependence between the mean and the variation. Further research may focus on constructing continuous analogues of such distributions, studying their limiting properties, and applying them to problems of regression and Bayesian analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01081v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oleksandr Volkov, Yurii Volkov, Nataliia Voinalovych</dc:creator>
    </item>
    <item>
      <title>Mixed exponential statistical structures and their approximation operators</title>
      <link>https://arxiv.org/abs/2512.07870</link>
      <description>arXiv:2512.07870v2 Announce Type: replace 
Abstract: The paper examines the construction and analysis of a new class of mixed exponential statistical structures that combine the properties of stochastic models and linear positive operators.The relevance of the topic is driven by the growing need to develop a unified theoretical framework capable of describing both continuous and discrete random structures that possess approximation properties. The aim of the study is to introduce and analyze a generalized family of mixed exponential statistical structures and their corresponding linear positive operators, which include known operators as particular cases. We define auxiliary statistical structures B and H through differential relations between their elements, and construct the main Phillips-type structure. Recurrent relations for the central moments are obtained, their properties are established, and the convergence and approximation accuracy of the constructed operators are investigated. The proposed approach allows mixed exponential structures to be viewed as a generalization of known statistical systems, providing a unified analytical and stochastic description. The results demonstrate that mixed exponential statistical structures can be used to develop new classes of positive operators with controllable preservation and approximation properties. The proposed methodology forms a basis for further research in constructing multidimensional statistical structures, analyzing operators in weighted spaces, and studying their asymptotic characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07870v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yurii Volkov, Oleksandr Volkov, Nataliia Voinalovych</dc:creator>
    </item>
    <item>
      <title>The limit joint distributions of some statistics used in testing the quality of random number generators</title>
      <link>https://arxiv.org/abs/2512.08002</link>
      <description>arXiv:2512.08002v2 Announce Type: replace 
Abstract: The limit joint distribution of statistics that are generalizations of some statistics from the NIST STS, TestU01, and other packages is found under the following hypotheses $H_0$ and $H_1$. Hypothesis $H_0$ states that the tested sequence is a sequence of independent random vectors with a known distribution, and the simple alternative hypothesis $H_1$ converges in some sense to $H_0$ with increasing sample size. In addition, an analogue of the Berry-Esseen inequality is obtained for the statistics under consideration, and conditions for their asymptotic independence are found.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08002v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. P. Savelov</dc:creator>
    </item>
    <item>
      <title>Sharp convergence rates for Spectral methods via the feature space decomposition method</title>
      <link>https://arxiv.org/abs/2512.14473</link>
      <description>arXiv:2512.14473v2 Announce Type: replace 
Abstract: In this paper, we apply the Feature Space Decomposition (FSD) method developed in [LS24, GLS25, ALSS26] to obtain, under fairly general conditions, matching upper and lower bounds for the population excess risk of spectral methods in linear regression under the squared loss, for every covariance and every signal. This result enables us, for a given linear regression problem, to define a partial order on the set of spectral methods according to their convergence rates, thereby characterizing which spectral algorithm is superior for that specific problem. Furthermore, this allows us to generalize the saturation effect proposed in inverse problems and to provide necessary and sufficient conditions for its occurrence. Our method also shows that, under broad conditions, any spectral algorithm lacks a feature learning property, and therefore cannot overcome the barrier of the information exponent in problems such as single-index learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14473v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Lecu\'e, Zhifan Li, Zong Shang</dc:creator>
    </item>
    <item>
      <title>A Consistent ICM-based $\chi^2$ Specification Test</title>
      <link>https://arxiv.org/abs/2208.13370</link>
      <description>arXiv:2208.13370v3 Announce Type: replace-cross 
Abstract: In spite of the omnibus property of Integrated Conditional Moment (ICM) specification tests, they are not commonly used in empirical practice owing to features such as the non-pivotality of the test and the high computational cost of available bootstrap schemes, especially in large samples. This paper proposes specification and mean independence tests based on ICM metrics. The proposed test exhibits consistency, asymptotic $\chi^2$-distribution under the null hypothesis, and computational efficiency. Moreover, it demonstrates robustness to heteroskedasticity of unknown form and can be adapted to enhance power towards specific alternatives. A power comparison with classical bootstrap-based ICM tests using Bahadur slopes is also provided. Monte Carlo simulations are conducted to showcase the excellent size control and competitive power of the proposed test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.13370v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feiyu Jiang, Emmanuel Selorm Tsyawo</dc:creator>
    </item>
    <item>
      <title>Multiperiodic Processes: Ergodic Sources with a Sublinear Entropy</title>
      <link>https://arxiv.org/abs/2302.09049</link>
      <description>arXiv:2302.09049v3 Announce Type: replace-cross 
Abstract: We construct multiperiodic processes -- a simple example of stationary ergodic (but not mixing) processes over natural numbers that enjoy the vanishing entropy rate under a mild condition. Multiperiodic processes are supported on randomly shifted deterministic sequences called multiperiodic sequences, which can be efficiently generated using an algorithm called the Infinite Clock. Under a suitable parameterization, multiperiodic sequences exhibit relative frequencies of particular numbers given by Zipf's law. Exactly in the same setting, the respective multiperiodic processes satisfy an asymptotic power-law growth of block entropy, called Hilberg's law. Hilberg's law is deemed to hold for statistical language models, in particular.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09049v3</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz D\k{e}bowski</dc:creator>
    </item>
    <item>
      <title>Generalized Data Thinning Using Sufficient Statistics</title>
      <link>https://arxiv.org/abs/2303.12931</link>
      <description>arXiv:2303.12931v3 Announce Type: replace-cross 
Abstract: Our goal is to develop a general strategy to decompose a random variable $X$ into multiple independent random variables, without sacrificing any information about unknown parameters. A recent paper showed that for some well-known natural exponential families, $X$ can be "thinned" into independent random variables $X^{(1)}, \ldots, X^{(K)}$, such that $X = \sum_{k=1}^K X^{(k)}$. These independent random variables can then be used for various model validation and inference tasks, including in contexts where traditional sample splitting fails. In this paper, we generalize their procedure by relaxing this summation requirement and simply asking that some known function of the independent random variables exactly reconstruct $X$. This generalization of the procedure serves two purposes. First, it greatly expands the families of distributions for which thinning can be performed. Second, it unifies sample splitting and data thinning, which on the surface seem to be very different, as applications of the same principle. This shared principle is sufficiency. We use this insight to perform generalized thinning operations for a diverse set of families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.12931v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2353948</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association, 120(549), 511-523 (2025)</arxiv:journal_reference>
      <dc:creator>Ameer Dharamshi, Anna Neufeld, Keshav Motwani, Lucy L. Gao, Daniela Witten, Jacob Bien</dc:creator>
    </item>
    <item>
      <title>Spearman's rho for zero-inflated count data: formulation and attainable bounds</title>
      <link>https://arxiv.org/abs/2503.13148</link>
      <description>arXiv:2503.13148v2 Announce Type: replace-cross 
Abstract: We propose an alternative formulation of Spearman's rho for zero-inflated count data. The formulation yields an estimator with explicitly attainable bounds, facilitating interpretation in settings where the standard range [-1,1] is no longer informative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13148v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasper Arends, Guanjie Lyu, Mhamed Mesfioui, Elisa Perrone, Julien Trufin</dc:creator>
    </item>
    <item>
      <title>Stochastic Optimization with Optimal Importance Sampling</title>
      <link>https://arxiv.org/abs/2504.03560</link>
      <description>arXiv:2504.03560v2 Announce Type: replace-cross 
Abstract: Importance Sampling (IS) is a widely used variance reduction technique for enhancing the efficiency of Monte Carlo methods, particularly in rare-event simulation and related applications. Despite its effectiveness, the performance of IS is highly sensitive to the choice of the proposal distribution and often requires stochastic calibration. While the design and analysis of IS have been extensively studied in estimation settings, applying IS within stochastic optimization introduces a fundamental challenge: the decision variable and the importance sampling distribution are mutually dependent, creating a circular optimization structure. This interdependence complicates both convergence analysis and variance control. We consider convex stochastic optimization problems with linear constraints and propose a single-loop stochastic approximation algorithm, based on a joint variant of Nesterov's dual averaging, that jointly updates the decision variable and the importance sampling distribution, without time-scale separation or nested optimization. The method is globally convergent and achieves minimal asymptotic variance among stochastic gradient schemes, matching the performance of an oracle sampler adapted to the optimal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03560v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Bart P. G. Van Parys, Henry Lam, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>A stochastic method to estimate a zero-inflated two-part mixed model for human microbiome data</title>
      <link>https://arxiv.org/abs/2504.15411</link>
      <description>arXiv:2504.15411v2 Announce Type: replace-cross 
Abstract: Human microbiome studies based on genetic sequencing techniques produce compositional longitudinal data of the relative abundances of microbial taxa over time, allowing to understand, through mixed-effects modeling, how microbial communities evolve in response to clinical interventions, environmental changes, or disease progression. In particular, the Zero-Inflated Beta Regression (ZIBR) models jointly and over time the presence and abundance of each microbe taxon, considering the compositional nature of the data, its skewness, and the over-abundance of zeros. However, as for other complex random effects models, maximum likelihood estimation suffers from the intractability of likelihood integrals. Available estimation methods rely on log-likelihood approximation, which is prone to potential limitations such as biased estimates or unstable convergence. In this work we develop an alternative maximum likelihood estimation approach for the ZIBR model, based on the Stochastic Approximation Expectation Maximization (SAEM) algorithm. The proposed methodology allows to model unbalanced data, which is not always possible in existing approaches. We also provide estimations of the standard errors and the log-likelihood of the fitted model. The performance of the algorithm is established through simulation, and its use is demonstrated on two microbiome studies, showing its ability to detect changes in both presence and abundance of bacterial taxa over time and in response to treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15411v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Barrera, Cristian Meza, Ana Arribas-Gil</dc:creator>
    </item>
    <item>
      <title>Source-Optimal Training is Transfer-Suboptimal</title>
      <link>https://arxiv.org/abs/2511.08401</link>
      <description>arXiv:2511.08401v3 Announce Type: replace-cross 
Abstract: We prove that training a source model optimally for its own task is generically suboptimal when the objective is downstream transfer. We study the source-side optimization problem in L2-SP ridge regression and show a fundamental mismatch between the source-optimal and transfer-optimal source regularization: outside of a measure-zero set, $\tau_0^* \neq \tau_S^*$. We characterize the transfer-optimal source penalty $\tau_0^*$ as a function of task alignment and identify an alignment-dependent reversal: with imperfect alignment ($0&lt;\rho&lt;1$), transfer benefits from stronger source regularization, while in super-aligned regimes ($\rho&gt;1$), transfer benefits from weaker regularization. In isotropic settings, the decision of whether transfer helps is independent of the target sample size and noise, depending only on task alignment and source characteristics. We verify the linear predictions in a synthetic ridge regression experiment, and we present CIFAR-10 experiments as evidence that the source-optimal versus transfer-optimal mismatch can persist in nonlinear networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08401v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>C. Evans Hedges</dc:creator>
    </item>
    <item>
      <title>Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences</title>
      <link>https://arxiv.org/abs/2512.13123</link>
      <description>arXiv:2512.13123v3 Announce Type: replace-cross 
Abstract: We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\varepsilon$-optimal with probability at least $1-\alpha$, with explicit bounds on the stopping time under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13123v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis</title>
      <link>https://arxiv.org/abs/2512.13491</link>
      <description>arXiv:2512.13491v2 Announce Type: replace-cross 
Abstract: We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13491v2</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz D\k{e}bowski</dc:creator>
    </item>
    <item>
      <title>Universality of high-dimensional scaling limits of stochastic gradient descent</title>
      <link>https://arxiv.org/abs/2512.13634</link>
      <description>arXiv:2512.13634v2 Announce Type: replace-cross 
Abstract: We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this limit is the same whenever the data is drawn from mixtures of arbitrary product distributions whose first two moments match the corresponding Gaussian distribution, provided the initialization and ground truth vectors are coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13634v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reza Gheissari, Aukosh Jagannath</dc:creator>
    </item>
    <item>
      <title>Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery</title>
      <link>https://arxiv.org/abs/2512.16875</link>
      <description>arXiv:2512.16875v2 Announce Type: replace-cross 
Abstract: We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $\alpha$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\Pr_{D}[E] \ge 1-\alpha$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $\beta$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $\beta$?
  Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(\beta)^{\gamma d}$ multiplicative factor of the volume of best $\beta$-conditioned ellipsoid while covering at least $1-O(\alpha/\gamma)$ probability mass for any $\gamma &lt; \alpha$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16875v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan</dc:creator>
    </item>
  </channel>
</rss>
