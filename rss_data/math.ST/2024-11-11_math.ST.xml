<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Nov 2024 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Robust estimation for high-dimensional time series with heavy tails</title>
      <link>https://arxiv.org/abs/2411.05217</link>
      <description>arXiv:2411.05217v1 Announce Type: new 
Abstract: We study in this paper the problem of least absolute deviation (LAD) regression for high-dimensional heavy-tailed time series which have finite $\alpha$-th moment with $\alpha \in (1,2]$. To handle the heavy-tailed dependent data, we propose a Catoni type truncated minimization problem framework and obtain an $\mathcal{O}\big( \big( (d_1+d_2) (d_1\land d_2) \log^2 n / n \big)^{(\alpha - 1)/\alpha} \big)$ order excess risk, where $d_1$ and $d_2$ are the dimensionality and $n$ is the number of samples. We apply our result to study the LAD regression on high-dimensional heavy-tailed vector autoregressive (VAR) process. Simulations for the VAR($p$) model show that our new estimator with truncation are essential because the risk of the classical LAD has a tendency to blow up. We further apply our estimation to the real data and find that ours fits the data better than the classical LAD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05217v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Wang, Guodong Li, Zhijie Xiao, Lihu Xu, Wenyang Zhang</dc:creator>
    </item>
    <item>
      <title>Estimating location parameters of two exponential distributions with ordered scale parameters</title>
      <link>https://arxiv.org/abs/2411.05487</link>
      <description>arXiv:2411.05487v1 Announce Type: new 
Abstract: In the usual statistical inference problem, we estimate an unknown parameter of a statistical model using the information in the random sample. A priori information about the parameter is also known in several real-life situations. One such information is order restriction between the parameters. This prior formation improves the estimation quality. In this paper, we deal with the component-wise estimation of location parameters of two exponential distributions studied with ordered scale parameters under a bowl-shaped affine invariant loss function and generalized Pitman closeness criterion. We have shown that several benchmark estimators, such as maximum likelihood estimators (MLE), uniformly minimum variance unbiased estimators (UMVUE), and best affine equivariant estimators (BAEE), are inadmissible. We have given sufficient conditions under which the dominating estimators are derived. Under the generalized Pitman closeness criterion, a Stein-type improved estimator is proposed. As an application, we have considered special sampling schemes such as type-II censoring, progressive type-II censoring, and record values. Finally, we perform a simulation study to compare the risk performance of the improved estimators</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05487v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lakshmi Kanta Patra, Constantinos Petropoulos, Shrajal Bajpai, Naresh Garg</dc:creator>
    </item>
    <item>
      <title>The multivariate local dependence function</title>
      <link>https://arxiv.org/abs/2411.05512</link>
      <description>arXiv:2411.05512v1 Announce Type: new 
Abstract: The local dependence function is important in many applications of probability and statistics. We extend the bivariate local dependence function introduced by Bairamov and Kotz (2000) and further developed by Bairamov et al. (2003) to three-variate and multivariate local dependence function characterizing the dependency between three and more random variables in a given specific point. The definition and properties of the three-variate local dependence function are discussed. An example of a three-variate local dependence function for underlying three-variate normal distribution is presented. The graphs and tables with numerical values are provided. The multivariate extension of the local dependence function that can characterize the dependency between multiple random variables at a specific point is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05512v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ismihan Bayramoglu, Pelin Ersin</dc:creator>
    </item>
    <item>
      <title>On the limiting variance of matching estimators</title>
      <link>https://arxiv.org/abs/2411.05758</link>
      <description>arXiv:2411.05758v1 Announce Type: new 
Abstract: This paper examines the limiting variance of nearest neighbor matching estimators for average treatment effects with a fixed number of matches. We present, for the first time, a closed-form expression for this limit. Here the key is the establishment of the limiting second moment of the catchment area's volume, which resolves a question of Abadie and Imbens. At the core of our approach is a new universality theorem on the measures of high-order Voronoi cells, extending a result by Devroye, Gy\"orfi, Lugosi, and Walk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05758v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songliang Chen, Fang Han</dc:creator>
    </item>
    <item>
      <title>On the Impossibility of Equating the Youden Index with Tjur's $R^2$-like metrics in $2\times2$ Tables</title>
      <link>https://arxiv.org/abs/2411.05391</link>
      <description>arXiv:2411.05391v1 Announce Type: cross 
Abstract: In 2017, Hughes claimed an equivalence between Tjurs $R^2$ coefficient of discrimination and Youden index for assessing diagnostic test performance on $2\times 2$ contingency tables. We prove an impossibility result when averaging over binary outcomes (0s and 1s) under any continuous real-valued scoring rule. Our findings clarify the limitations of such a possible equivalence and highlights the distinct roles these metrics play in diagnostic test assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05391v1</guid>
      <category>stat.OT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linard Hoessly</dc:creator>
    </item>
    <item>
      <title>Maximum a posteriori testing in statistical inverse problems</title>
      <link>https://arxiv.org/abs/2402.00686</link>
      <description>arXiv:2402.00686v3 Announce Type: replace 
Abstract: This paper is concerned with a Bayesian approach to testing hypotheses in statistical inverse problems. Based on the posterior distribution $\Pi \left(\cdot |Y = y\right)$, we want to infer whether a feature $\langle\varphi, u^\dagger\rangle$ of the unknown quantity of interest $u^\dagger$ is positive. This can be done by the so-called maximum a posteriori test. We provide a frequentistic analysis of this test's properties such as level and power, and prove that it is a regularized test in the sense of Kretschmann et al. (2024). Furthermore we provide lower bounds for its power under classical spectral source conditions in case of Gaussian priors. Numerical simulations illustrate its superior performance both in moderately and severely ill-posed situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00686v3</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Remo Kretschmann, Frank Werner</dc:creator>
    </item>
    <item>
      <title>Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift</title>
      <link>https://arxiv.org/abs/2302.10160</link>
      <description>arXiv:2302.10160v3 Announce Type: replace-cross 
Abstract: We develop and analyze a principled approach to kernel ridge regression under covariate shift. The goal is to learn a regression function with small mean squared error over a target distribution, based on unlabeled data from there and labeled data that may have a different feature distribution. We propose to split the labeled data into two subsets, and conduct kernel ridge regression on them separately to obtain a collection of candidate models and an imputation model. We use the latter to fill the missing labels and then select the best candidate accordingly. Our non-asymptotic excess risk bounds demonstrate that our estimator adapts effectively to both the structure of the target distribution and the covariate shift. This adaptation is quantified through a notion of effective sample size that reflects the value of labeled source data for the target regression task. Our estimator achieves the minimax optimal error rate up to a polylogarithmic factor, and we find that using pseudo-labels for model selection does not significantly hinder performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10160v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Demystifying Spatial Confounding</title>
      <link>https://arxiv.org/abs/2309.16861</link>
      <description>arXiv:2309.16861v2 Announce Type: replace-cross 
Abstract: Spatial confounding is a fundamental issue in spatial regression models which arises because spatial random effects, included to approximate unmeasured spatial variation, are typically not independent of covariates in the model. This can lead to significant bias in covariate effect estimates. The problem is complex and has been the topic of extensive research with sometimes puzzling and seemingly contradictory results. Here, we develop a broad theoretical framework that brings mathematical clarity to the mechanisms of spatial confounding, providing explicit analytical expressions for the resulting bias. We see that the problem is directly linked to spatial smoothing and identify exactly how the size and occurrence of bias relate to the features of the spatial model as well as the underlying confounding scenario. Using our results, we can explain subtle and counter-intuitive behaviours. Finally, we propose a general approach for dealing with spatial confounding bias in practice, applicable for any spatial model specification. When a covariate has non-spatial information, we show that a general form of the so-called spatial+ method can be used to eliminate bias. When no such information is present, the situation is more challenging but, under the assumption of unconfounded high frequencies, we develop a procedure in which multiple capped versions of spatial+ are applied to assess the bias in this case. We illustrate our approach with an application to air temperature in Germany.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16861v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emiko Dupont, Isa Marques, Thomas Kneib</dc:creator>
    </item>
    <item>
      <title>A constructive approach to selective risk control</title>
      <link>https://arxiv.org/abs/2401.16651</link>
      <description>arXiv:2401.16651v2 Announce Type: replace-cross 
Abstract: Many modern applications require using data to select the statistical tasks and make valid inference after selection. In this article, we provide a unifying approach to control for a class of selective risks. Our method is motivated by a reformulation of the celebrated Benjamini-Hochberg (BH) procedure for multiple hypothesis testing as the fixed point iteration of the Benjamini-Yekutieli (BY) procedure for constructing post-selection confidence intervals. Building on this observation, we propose a constructive approach to control extra-selection risk (where selection is made after decision) by iterating decision strategies that control the post-selection risk (where decision is made after selection). We show that many previous methods and results are special cases of this general framework, and we further extend this approach to problems with multiple selective risks. Our development leads to two surprising results about the BH procedure: (1) in the context of one-sided location testing, the BH procedure not only controls the false discovery rate at the null but also at other locations for free; (2) in the context of permutation tests, the BH procedure with exact permutation p-values can be well approximated by a procedure which only requires a total number of permutations that is almost linear in the total number of hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16651v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zijun Gao, Wenjie Hu, Qingyuan Zhao</dc:creator>
    </item>
    <item>
      <title>Triple Component Matrix Factorization: Untangling Global, Local, and Noisy Components</title>
      <link>https://arxiv.org/abs/2404.07955</link>
      <description>arXiv:2404.07955v2 Announce Type: replace-cross 
Abstract: In this work, we study the problem of common and unique feature extraction from noisy data. When we have N observation matrices from N different and associated sources corrupted by sparse and potentially gross noise, can we recover the common and unique components from these noisy observations? This is a challenging task as the number of parameters to estimate is approximately thrice the number of observations. Despite the difficulty, we propose an intuitive alternating minimization algorithm called triple component matrix factorization (TCMF) to recover the three components exactly. TCMF is distinguished from existing works in literature thanks to two salient features. First, TCMF is a principled method to separate the three components given noisy observations provably. Second, the bulk of the computation in TCMF can be distributed. On the technical side, we formulate the problem as a constrained nonconvex nonsmooth optimization problem. Despite the intricate nature of the problem, we provide a Taylor series characterization of its solution by solving the corresponding Karush-Kuhn-Tucker conditions. Using this characterization, we can show that the alternating minimization algorithm makes significant progress at each iteration and converges into the ground truth at a linear rate. Numerical experiments in video segmentation and anomaly detection highlight the superior feature extraction abilities of TCMF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07955v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naichen Shi, Salar Fattahi, Raed Al Kontar</dc:creator>
    </item>
    <item>
      <title>Attack-Aware Noise Calibration for Differential Privacy</title>
      <link>https://arxiv.org/abs/2407.02191</link>
      <description>arXiv:2407.02191v2 Announce Type: replace-cross 
Abstract: Differential privacy (DP) is a widely used approach for mitigating privacy risks when training machine learning models on sensitive data. DP mechanisms add noise during training to limit the risk of information leakage. The scale of the added noise is critical, as it determines the trade-off between privacy and utility. The standard practice is to select the noise scale to satisfy a given privacy budget $\varepsilon$. This privacy budget is in turn interpreted in terms of operational attack risks, such as accuracy, sensitivity, and specificity of inference attacks aimed to recover information about the training data records. We show that first calibrating the noise scale to a privacy budget $\varepsilon$, and then translating {\epsilon} to attack risk leads to overly conservative risk assessments and unnecessarily low utility. Instead, we propose methods to directly calibrate the noise scale to a desired attack risk level, bypassing the step of choosing $\varepsilon$. For a given notion of attack risk, our approach significantly decreases noise scale, leading to increased utility at the same level of privacy. We empirically demonstrate that calibrating noise to attack sensitivity/specificity, rather than $\varepsilon$, when training privacy-preserving ML models substantially improves model accuracy for the same risk level. Our work provides a principled and practical way to improve the utility of privacy-preserving ML without compromising on privacy. The code is available at https://github.com/Felipe-Gomez/riskcal</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02191v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bogdan Kulynych, Juan Felipe Gomez, Georgios Kaissis, Flavio du Pin Calmon, Carmela Troncoso</dc:creator>
    </item>
    <item>
      <title>Adaptive Refinement Protocols for Distributed Distribution Estimation under $\ell^p$-Losses</title>
      <link>https://arxiv.org/abs/2410.06884</link>
      <description>arXiv:2410.06884v2 Announce Type: replace-cross 
Abstract: Consider the communication-constrained estimation of discrete distributions under $\ell^p$ losses, where each distributed terminal holds multiple independent samples and uses limited number of bits to describe the samples. We obtain the minimax optimal rates of the problem in most parameter regimes. An elbow effect of the optimal rates at $p=2$ is clearly identified. To show the optimal rates, we first design estimation protocols to achieve them. The key ingredient of these protocols is to introduce adaptive refinement mechanisms, which first generate rough estimate by partial information and then establish refined estimate in subsequent steps guided by the rough estimate. The protocols leverage successive refinement, sample compression, thresholding and random hashing methods to achieve the optimal rates in different parameter regimes. The optimality of the protocols is shown by deriving compatible minimax lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06884v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deheng Yuan, Tao Guo, Zhongyi Huang</dc:creator>
    </item>
  </channel>
</rss>
