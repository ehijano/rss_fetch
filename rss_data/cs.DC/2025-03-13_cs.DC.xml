<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Mar 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>MoE-Gen: High-Throughput MoE Inference on a Single GPU with Module-Based Batching</title>
      <link>https://arxiv.org/abs/2503.09716</link>
      <description>arXiv:2503.09716v1 Announce Type: new 
Abstract: This paper presents MoE-Gen, a high-throughput MoE inference system optimized for single-GPU execution. Existing inference systems rely on model-based or continuous batching strategies, originally designed for interactive inference, which result in excessively small batches for MoE's key modules-attention and expert modules-leading to poor throughput. To address this, we introduce module-based batching, which accumulates tokens in host memory and dynamically launches large batches on GPUs to maximize utilization. Additionally, we optimize the choice of batch sizes for each module in an MoE to fully overlap GPU computation and communication, maximizing throughput. Evaluation demonstrates that MoE-Gen achieves 8-31x higher throughput compared to state-of-the-art systems employing model-based batching (FlexGen, MoE-Lightning, DeepSpeed), and offers even greater throughput improvements over continuous batching systems (e.g., vLLM and Ollama) on popular MoE models (DeepSeek and Mixtral) across offline inference tasks. MoE-Gen's source code is publicly available at https://github.com/EfficientMoE/MoE-Gen</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09716v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tairan Xu, Leyang Xue, Zhan Lu, Adrian Jackson, Luo Mai</dc:creator>
    </item>
    <item>
      <title>Introducing MareNostrum5: A European pre-exascale energy-efficient system designed to serve a broad spectrum of scientific workloads</title>
      <link>https://arxiv.org/abs/2503.09917</link>
      <description>arXiv:2503.09917v1 Announce Type: new 
Abstract: MareNostrum5 is a pre-exascale supercomputer at the Barcelona Supercomputing Center (BSC), part of the EuroHPC Joint Undertaking. With a peak performance of 314 petaflops, MareNostrum5 features a hybrid architecture comprising Intel Sapphire Rapids CPUs, NVIDIA Hopper GPUs, and DDR5 and high-bandwidth memory (HBM), organized into four partitions optimized for diverse workloads. This document evaluates MareNostrum5 through micro-benchmarks (floating-point performance, memory bandwidth, interconnect throughput), HPC benchmarks (HPL and HPCG), and application studies using Alya, OpenFOAM, and IFS. It highlights MareNostrum5's scalability, efficiency, and energy performance, utilizing the EAR (Energy Aware Runtime) framework to assess power consumption and the effects of direct liquid cooling. Additionally, HBM and DDR5 configurations are compared to examine memory performance trade-offs. Designed to complement standard technical documentation, this study provides insights to guide both new and experienced users in optimizing their workloads and maximizing MareNostrum5's computational capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09917v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fabio Banchelli, Marta Garcia-Gasulla, Filippo Mantovani, Joan Vinyals, Josep Pocurull, David Vicente, Beatriz Eguzkitza, Flavio C. C. Galeazzo, Mario C. Acosta, Sergi Girona</dc:creator>
    </item>
    <item>
      <title>Shaved Ice: Optimal Compute Resource Commitments for Dynamic Multi-Cloud Workloads</title>
      <link>https://arxiv.org/abs/2503.10235</link>
      <description>arXiv:2503.10235v1 Announce Type: new 
Abstract: Cloud providers have introduced pricing models to incentivize long-term commitments of compute capacity. These long-term commitments allow the cloud providers to get guaranteed revenue for their investments in data centers and computing infrastructure. However, these commitments expose cloud customers to demand risk if expected future demand does not materialize. While there are existing studies of theoretical techniques for optimizing performance, latency, and cost, relatively little has been reported so far on the trade-offs between cost savings and demand risk for compute commitments for large-scale cloud services.
  We characterize cloud compute demand based on an extensive three year study of the Snowflake Data Cloud, which includes data warehousing, data lakes, data science, data engineering, and other workloads across multiple clouds. We quantify capacity demand drivers from user workloads, hardware generational improvements, and software performance improvements. Using this data, we formulate a series of practical optimizations that maximize capacity availability and minimize costs for the cloud customer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10235v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3676151.3719353</arxiv:DOI>
      <dc:creator>Murray Stokely, Neel Nadir, Jack Peele, Orestis Kostakis</dc:creator>
    </item>
    <item>
      <title>Message Size Matters: AlterBFT's Approach to Practical Synchronous BFT in Public Clouds</title>
      <link>https://arxiv.org/abs/2503.10292</link>
      <description>arXiv:2503.10292v1 Announce Type: new 
Abstract: Synchronous consensus protocols offer a significant advantage over their asynchronous and partially synchronous counterparts by providing higher fault tolerance -- an essential benefit in distributed systems, like blockchains, where participants may have incentives to act maliciously. However, despite this advantage, synchronous protocols are often met with skepticism due to concerns about their performance, as the latency of synchronous protocols is tightly linked to a conservative time bound for message delivery.
  This paper introduces AlterBFT, a new Byzantine fault-tolerant consensus protocol. The key idea behind AlterBFT lies in the new model we propose, called hybrid synchronous system model. The new model is inspired by empirical observations about network behavior in the public cloud environment and combines elements from the synchronous and partially synchronous models. Namely, it distinguishes between small messages that respect time bounds and large messages that may violate bounds but are eventually timely. Leveraging this observation, AlterBFT achieves up to 15$\times$ lower latency than state-of-the-art synchronous protocols while maintaining similar throughput and the same fault tolerance. Compared to partially synchronous protocols, AlterBFT provides higher fault tolerance, higher throughput, and comparable latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10292v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nenad Milo\v{s}evi\'c, Daniel Cason, Zarko Milo\v{s}evi\'c, Robert Soul\'e, Fernando Pedone</dc:creator>
    </item>
    <item>
      <title>Collaborative Speculative Inference for Efficient LLM Inference Serving</title>
      <link>https://arxiv.org/abs/2503.10325</link>
      <description>arXiv:2503.10325v1 Announce Type: new 
Abstract: Speculative inference is a promising paradigm employing small speculative models (SSMs) as drafters to generate draft tokens, which are subsequently verified in parallel by the target large language model (LLM). This approach enhances the efficiency of inference serving by reducing LLM inference latency and costs while preserving generation quality. However, existing speculative methods face critical challenges, including inefficient resource utilization and limited draft acceptance, which constrain their scalability and overall effectiveness. To overcome these obstacles, we present CoSine, a novel speculative inference system that decouples sequential speculative decoding from parallel verification, enabling efficient collaboration among multiple nodes. Specifically, CoSine routes inference requests to specialized drafters based on their expertise and incorporates a confidence-based token fusion mechanism to synthesize outputs from cooperating drafters, ensuring high-quality draft generation. Additionally, CoSine dynamically orchestrates the execution of speculative decoding and verification in a pipelined manner, employing batch scheduling to selectively group requests and adaptive speculation control to minimize idle periods. By optimizing parallel workflows through heterogeneous node collaboration, CoSine balances draft generation and verification throughput in real-time, thereby maximizing resource utilization. Experimental results demonstrate that CoSine achieves superior performance compared to state-of-the-art speculative approaches. Notably, with equivalent resource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5% increase in throughput compared to baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10325v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luyao Gao, Jianchun Liu, Hongli Xu, Liusheng Huang</dc:creator>
    </item>
    <item>
      <title>SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline Parallel Offloading</title>
      <link>https://arxiv.org/abs/2503.10377</link>
      <description>arXiv:2503.10377v1 Announce Type: new 
Abstract: In recent years, Large Language Models (LLMs) have exhibited remarkable capabilities, driving advancements in real-world applications. However, training LLMs on increasingly long input sequences imposes significant challenges due to high GPU memory and computational demands. Existing solutions face two key limitations: (1) memory reduction techniques, such as activation recomputation and CPU offloading, compromise training efficiency; (2) distributed parallelism strategies require excessive GPU resources, limiting the scalability of input sequence length.
  To address these gaps, we propose Adaptive Sequence Pipeline Parallel Offloading (SPPO), a novel LLM training framework that optimizes memory and computational resource efficiency for long-sequence training. SPPO introduces adaptive offloading, leveraging sequence-aware offloading, and two-level activation management to reduce GPU memory consumption without degrading the training efficiency. Additionally, SPPO develops an adaptive pipeline scheduling approach with a heuristic solver and multiplexed sequence partitioning to improve computational resource efficiency. Experimental results demonstrate that SPPO achieves up to 3.38x throughput improvement over Megatron-LM and DeepSpeed, realizing efficient training of a 7B LLM with sequence lengths of up to 4M tokens on only 128 A100 GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10377v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiaoling Chen, Shenggui Li, Wei Gao, Peng Sun, Yonggang Wen, Tianwei Zhang</dc:creator>
    </item>
    <item>
      <title>Concurrent Scheduling of High-Level Parallel Programs on Multi-GPU Systems</title>
      <link>https://arxiv.org/abs/2503.10516</link>
      <description>arXiv:2503.10516v1 Announce Type: new 
Abstract: Parallel programming models can encourage performance portability by moving the responsibility for work assignment and data distribution from the programmer to a runtime system. However, analyzing the resulting implicit memory allocations, coherence operations and their interdependencies can quickly introduce delays into the latency-sensitive execution pipeline of a distributed-memory application.
  In this paper, we show how graph-based intermediate representations help moving such scheduling work out of the critical path. In the context of SYCL programs distributed onto accelerator clusters, we introduce the instruction graph, a low-level representation that preserves full concurrency between memory management, data transfers, MPI peer-to-peer communication and kernel invocation.
  Through integration within the Celerity runtime, we demonstrate how instruction-graph scheduling enables a system architecture that performs this analysis concurrently with execution. Using a scheduler lookahead mechanism, we further detect changing access patterns to optimize memory allocation in the presence of virtualized buffers.
  We show the effectiveness of our method through strong-scaling benchmarks with multiple Celerity applications on up to 128 GPUs in a production cluster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10516v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Knorr, Philip Salzmann, Peter Thoman, Thomas Fahringer</dc:creator>
    </item>
    <item>
      <title>Efficient Precoding in XL-MIMO-AFDM System</title>
      <link>https://arxiv.org/abs/2503.10525</link>
      <description>arXiv:2503.10525v1 Announce Type: new 
Abstract: This paper explores the potential of affine frequency division multiplexing (AFDM) to mitigate the multiuser interference (MUI) problem by employing time-domain precoding in extremely-large-scale multiple-input multiple-output (XL-MIMO) systems. In XL-MIMO systems, user mobility significantly improves network capacity and transmission quality. Meanwhile, the robustness of AFDM to Doppler shift is enhanced in user mobility scenarios, which further improves the system performance. However, the multicarrier nature of AFDM has attracted much attention, and it leads to a significant increase in precoding complexity. However, the serious problem is that the multicarrier use of AFDM leads to a sharp increase in precoding complexity. Therefore, we employ efficient precoding randomized Kaczmarz (rKA) to reduce the complexity overhead. Through simulation analysis, we compare the performance of XL-MIMO-AFDM and XL-MIMO orthogonal frequency division multiplexing (XL-MIMO-OFDM) in mobile scenarios, and the results show that our proposed AFDM-based XL-MIMO precoding design can be more efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10525v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun Zhu, Yin Xu, Dazhi He, Haoyang Li, Yunfeng Guan, Wenjun Zhang, Tianyao Ma, Haozhi Yuan</dc:creator>
    </item>
    <item>
      <title>FedMSGL: A Self-Expressive Hypergraph Based Federated Multi-View Learning</title>
      <link>https://arxiv.org/abs/2503.09643</link>
      <description>arXiv:2503.09643v1 Announce Type: cross 
Abstract: Federated learning is essential for enabling collaborative model training across decentralized data sources while preserving data privacy and security. This approach mitigates the risks associated with centralized data collection and addresses concerns related to data ownership and compliance. Despite significant advancements in federated learning algorithms that address communication bottlenecks and enhance privacy protection, existing works overlook the impact of differences in data feature dimensions, resulting in global models that disproportionately depend on participants with large feature dimensions. Additionally, current single-view federated learning methods fail to account for the unique characteristics of multi-view data, leading to suboptimal performance in processing such data. To address these issues, we propose a Self-expressive Hypergraph Based Federated Multi-view Learning method (FedMSGL). The proposed method leverages self-expressive character in the local training to learn uniform dimension subspace with latent sample relation. At the central side, an adaptive fusion technique is employed to generate the global model, while constructing a hypergraph from the learned global and view-specific subspace to capture intricate interconnections across views. Experiments on multi-view datasets with different feature dimensions validated the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09643v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daoyuan Li, Zuyuan Yang, Shengli Xie</dc:creator>
    </item>
    <item>
      <title>The Shamrock code: I- Smoothed Particle Hydrodynamics on GPUs</title>
      <link>https://arxiv.org/abs/2503.09713</link>
      <description>arXiv:2503.09713v1 Announce Type: cross 
Abstract: We present Shamrock, a performance portable framework developed in C++17 with the SYCL programming standard, tailored for numerical astrophysics on Exascale architectures. The core of Shamrock is an accelerated parallel tree with negligible construction time, whose efficiency is based on binary algebra. The Smoothed Particle Hydrodynamics algorithm of the Phantom code is implemented in Shamrock. On-the-fly tree construction circumvents the necessity for extensive data communications. In tests displaying a uniform density with global timesteping with tens of billions of particles, Shamrock completes a single time step in a few seconds using over the thousand of GPUs of a super-computer. This corresponds to processing billions of particles per second, with tens of millions of particles per GPU. The parallel efficiency across the entire cluster is larger than $\sim 90\%$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09713v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timoth\'ee David--Cl\'eris, Guillaume Laibe, Yona Lapeyre</dc:creator>
    </item>
    <item>
      <title>Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo</title>
      <link>https://arxiv.org/abs/2503.09799</link>
      <description>arXiv:2503.09799v1 Announce Type: cross 
Abstract: As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09799v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary Charles, Gabriel Teston, Lucio Dery, Keith Rush, Nova Fallen, Zachary Garrett, Arthur Szlam, Arthur Douillard</dc:creator>
    </item>
    <item>
      <title>Computing the Saturation Throughput for Heterogeneous p-CSMA in a General Wireless Network</title>
      <link>https://arxiv.org/abs/2503.09869</link>
      <description>arXiv:2503.09869v1 Announce Type: cross 
Abstract: A well-known expression for the saturation throughput of heterogeneous transmitting nodes in a wireless network using p-CSMA, derived from Renewal Theory, implicitly assumes that all transmitting nodes are in range of, and therefore conflicting with, each other. This expression, as well as simple modifications of it, does not correctly capture the saturation throughput values when an arbitrary topology is specified for the conflict graph between transmitting links. For example, we show numerically that calculations based on renewal theory can underestimate throughput by 48-62% for large packet sizes when the conflict graph is represented by a star topology. This is problematic because real-world wireless networks, such as wireless IoT mesh networks, are often deployed over a large area, resulting in non-complete conflict graphs.
  To address this gap, we present a computational approach based on a novel Markov chain formulation that yields the exact saturation throughput for each node in the general network case for any given set of access probabilities, as well as a more compact expression for the special case where the packet length is twice the slot length. Using our approach, we show how the transmit probabilities could be optimized to maximize weighted utility functions of the saturation throughput values. This would allow a wireless system designer to set transmit probabilities to achieve desired throughput trade-offs in any given deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09869v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faezeh Dehghan Tarzjani, Bhaskar Krishnamachari</dc:creator>
    </item>
    <item>
      <title>Parallel Batch-Dynamic Maximal Matching with Constant Work per Update</title>
      <link>https://arxiv.org/abs/2503.09908</link>
      <description>arXiv:2503.09908v1 Announce Type: cross 
Abstract: We present a work optimal algorithm for parallel fully batch-dynamic maximal matching against an oblivious adversary. In particular it processes batches of updates (either insertions or deletions of edges) in constant expected amortized work per edge update, and in $O(\log^3 m)$ depth per batch whp, where $m$ is the maximum number of edges in the graph over time. This greatly improves on the recent result by Ghaffari and Trygub (2024) that requires $O(\log^9 m)$ amortized work per update and $O(\log^4 m )$ depth per batch, both whp. The algorithm can also be used for hyperedge maximal matching. For hypergraphs with rank $r$ (maximum cardinality of any edge) the algorithm supports batches of insertions and deletions with $O(r^3)$ expected amortized work per edge update, and $O(\log^3 m)$ depth per batch whp. This is a factor of $O(r)$ work off of the best sequential algorithm, Assadi and Solomon (2021), which uses $O(r^2)$ work per update. Ghaffari and Trygub's parallel batch-dynamic algorithm on hypergraphs requires $O(r^8 \log^9 m)$ amortized work per edge update whp. We leverage ideas from the prior algorithms but introduce substantial new ideas. Furthermore, our algorithm is relatively simple, perhaps even simpler than the sequential hyperedge algorithm. We also present the first work-efficient algorithm for maximal matching on hypergraphs. For a hypergraph with total cardinality $m'$ (i.e., sum over the cardinality of each edge), the algorithm runs in $O(m')$ work in expectation and $O(\log^2 m)$ depth whp. The algorithm also has some properties that allow us to use it as a subroutine in the dynamic algorithm to select random edges in the graph to add to the matching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09908v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy E. Blelloch, Andrew C. Brady</dc:creator>
    </item>
    <item>
      <title>Efficient Federated Fine-Tuning of Large Language Models with Layer Dropout</title>
      <link>https://arxiv.org/abs/2503.10217</link>
      <description>arXiv:2503.10217v1 Announce Type: cross 
Abstract: Fine-tuning plays a crucial role in enabling pre-trained LLMs to evolve from general language comprehension to task-specific expertise. To preserve user data privacy, federated fine-tuning is often employed and has emerged as the de facto paradigm. However, federated fine-tuning is prohibitively inefficient due to the tension between LLM complexity and the resource constraint of end devices, incurring unaffordable fine-tuning overhead. Existing literature primarily utilizes parameter-efficient fine-tuning techniques to mitigate communication costs, yet computational and memory burdens continue to pose significant challenges for developers. This work proposes DropPEFT, an innovative federated PEFT framework that employs a novel stochastic transformer layer dropout method, enabling devices to deactivate a considerable fraction of LLMs layers during training, thereby eliminating the associated computational load and memory footprint. In DropPEFT, a key challenge is the proper configuration of dropout ratios for layers, as overhead and training performance are highly sensitive to this setting. To address this challenge, we adaptively assign optimal dropout-ratio configurations to devices through an exploration-exploitation strategy, achieving efficient and effective fine-tuning. Extensive experiments show that DropPEFT can achieve a 1.3-6.3\times speedup in model convergence and a 40%-67% reduction in memory footprint compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10217v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shilong Wang, Jianchun Liu, Hongli Xu, Jiaming Yan, Xianjun Gao</dc:creator>
    </item>
    <item>
      <title>Pandemics In Silico: Scaling an Agent-Based Simulation on Realistic Social Contact Networks</title>
      <link>https://arxiv.org/abs/2401.08124</link>
      <description>arXiv:2401.08124v4 Announce Type: replace 
Abstract: Preventing the spread of infectious diseases requires implementing interventions at various levels of government and evaluating the potential impact and efficacy of those preemptive measures. Agent-based modeling can be used for detailed studies of epidemic diffusion and possible interventions. Modeling of epidemic diffusion in large social contact networks requires the use of parallel algorithms and resources. In this work, we present Loimos, a scalable parallel framework for simulating epidemic diffusion. Loimos uses a hybrid of time-stepping and discrete-event simulation to model disease spread, and is implemented on top of an asynchronous, many-task runtime. We demonstrate that Loimos is to able to achieve significant speedups while scaling to large core counts. In particular, Loimos is able to simulate 200 days of a COVID-19 outbreak on a digital twin of California in about 42 seconds, for an average of 4.6 billion traversed edges per second (TEPS), using 4096 cores on Perlmutter at NERSC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08124v4</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joy Kitson, Ian Costello, Jiangzhuo Chen, Diego Jim\'enez, Stefan Hoops, Henning Mortveit, Esteban Meneses, Jae-Seung Yeom, Madhav V. Marathe, Abhinav Bhatele</dc:creator>
    </item>
    <item>
      <title>HexiScale: Accommodating Large Language Model Training over Heterogeneous Environment</title>
      <link>https://arxiv.org/abs/2409.01143</link>
      <description>arXiv:2409.01143v2 Announce Type: replace 
Abstract: Training large language model (LLM) is a computationally intensive task, which is typically conducted in data centers with homogeneous high-performance GPUs. We explore an alternative approach by deploying training computations across heterogeneous GPUs to enable better flexibility and efficiency for heterogeneous resource utilization. To achieve this goal, we propose a novel system, HexiScale, that can flexibly support asymmetric partition of training computations in the scope of data-, pipeline-, and tensor model parallelism. We further formalize the allocation of asymmetric partitioned training computations over a set of heterogeneous GPUs as a constrained optimization problem and propose an efficient hierarchical graph partitioning algorithm. Our approach effectively allocates training computations across GPUs, fully leveraging the available computational power. We conduct empirical studies to evaluate the performance of HexiScale with state-of-the-art homogeneous and heterogeneous training systems. When training LLMs at different scales (from 7B to 30B), HexiScale achieves comparable MFU when running over heterogeneous GPUs compared to state-of-the-art training systems running over homogeneous high-performance GPUs with the same total peak FLOPS. The percentage gaps in MFU between HexiScale and comparable homogeneous settings are as low as $0.3\%$, with an average of $3.5\%$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01143v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ran Yan, Youhe Jiang, Xiaonan Nie, Fangcheng Fu, Bin Cui, Binhang Yuan</dc:creator>
    </item>
    <item>
      <title>Analyzing the Performance Portability of SYCL across CPUs, GPUs, and Hybrid Systems with SW Sequence Alignment</title>
      <link>https://arxiv.org/abs/2412.08308</link>
      <description>arXiv:2412.08308v2 Announce Type: replace 
Abstract: The high-performance computing (HPC) landscape is undergoing rapid transformation, with an increasing emphasis on energy-efficient and heterogeneous computing environments. This comprehensive study extends our previous research on SYCL's performance portability by evaluating its effectiveness across a broader spectrum of computing architectures, including CPUs, GPUs, and hybrid CPU-GPU configurations from NVIDIA, Intel, and AMD. Our analysis covers single-GPU, multi-GPU, single-CPU, and CPU-GPU hybrid setups, using two common, bioinformatic applications as a case study. The results demonstrate SYCL's versatility across different architectures, maintaining comparable performance to CUDA on NVIDIA GPUs while achieving similar architectural efficiency rates on AMD and Intel GPUs in the majority of cases tested. SYCL also demonstrated remarkable versatility and effectiveness across CPUs from various manufacturers, including the latest hybrid architectures from Intel. Although SYCL showed excellent functional portability in hybrid CPU-GPU configurations, performance varied significantly based on specific hardware combinations. Some performance limitations were identified in multi-GPU and CPU-GPU configurations, primarily attributed to workload distribution strategies rather than SYCL-specific constraints. These findings position SYCL as a promising unified programming model for heterogeneous computing environments, particularly for bioinformatic applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08308v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Manuel Costanzo, Enzo Rucci, Carlos Garc\'ia-S\'anchez, Marcelo Naiouf, Manuel Prieto-Mat\'ias</dc:creator>
    </item>
    <item>
      <title>Deep Reinforcement Learning for Dynamic Resource Allocation in Wireless Networks</title>
      <link>https://arxiv.org/abs/2502.01129</link>
      <description>arXiv:2502.01129v3 Announce Type: replace 
Abstract: This report investigates the application of deep reinforcement learning (DRL) algorithms for dynamic resource allocation in wireless communication systems. An environment that includes a base station, multiple antennas, and user equipment is created. Using the RLlib library, various DRL algorithms such as Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) are then applied. These algorithms are compared based on their ability to optimize resource allocation, focusing on the impact of different learning rates and scheduling policies. The findings demonstrate that the choice of algorithm and learning rate significantly influences system performance, with DRL providing more efficient resource allocation compared to traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01129v3</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shubham Malhotra, Fnu Yashu, Muhammad Saqib, Dipkumar Mehta, Jagdish Jangid, Sachin Dixit</dc:creator>
    </item>
    <item>
      <title>Scaling and Load-Balancing Equi-Joins</title>
      <link>https://arxiv.org/abs/2209.08475</link>
      <description>arXiv:2209.08475v3 Announce Type: replace-cross 
Abstract: The task of joining two tables is fundamental for querying databases. In this paper, we focus on the equi-join problem, where a pair of records from the two joined tables are part of the join results if equality holds between their values in the join column(s). While this is a tractable problem when the number of records in the joined tables is relatively small, it becomes very challenging as the table sizes increase, especially if hot keys (join column values with a large number of records) exist in both joined tables.
  This paper, an extended version of [metwally-SIGMOD-2022], proposes Adaptive-Multistage-Join (AM-Join) for scalable and fast equi-joins in distributed shared-nothing architectures. AM-Join utilizes (a) Tree-Join, a proposed novel algorithm that scales well when the joined tables share hot keys, and (b) Broadcast-Join, the known fastest when joining keys that are hot in only one table.
  Unlike the state-of-the-art algorithms, AM-Join (a) holistically solves the join-skew problem by achieving load balancing throughout the join execution, and (b) supports all outer-join variants without record deduplication or custom table partitioning. For the fastest AM-Join outer-join performance, we propose the Index-Broadcast-Join (IB-Join) family of algorithms for Small-Large joins, where one table fits in memory and the other can be up to orders of magnitude larger. The outer-join variants of IB-Join improves on the state-of-the-art Small-Large outer-join algorithms.
  The proposed algorithms can be adopted in any shared-nothing architecture. We implemented a MapReduce version using Spark. Our evaluation shows the proposed algorithms execute significantly faster and scale to more skewed and orders-of-magnitude bigger tables when compared to the state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.08475v3</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3722102</arxiv:DOI>
      <arxiv:journal_reference>ACM Trans. Datab. Syst. 1, 1, Article 1 (January 2025), 42 pages (2025)</arxiv:journal_reference>
      <dc:creator>Ahmed Metwally</dc:creator>
    </item>
    <item>
      <title>Robust Decentralized Learning with Local Updates and Gradient Tracking</title>
      <link>https://arxiv.org/abs/2405.00965</link>
      <description>arXiv:2405.00965v2 Announce Type: replace-cross 
Abstract: As distributed learning applications such as Federated Learning, the Internet of Things (IoT), and Edge Computing grow, it is critical to address the shortcomings of such technologies from a theoretical perspective. As an abstraction, we consider decentralized learning over a network of communicating clients or nodes and tackle two major challenges: data heterogeneity and adversarial robustness. We propose a decentralized minimax optimization method that employs two important modules: local updates and gradient tracking. Minimax optimization is the key tool to enable adversarial training for ensuring robustness. Having local updates is essential in Federated Learning (FL) applications to mitigate the communication bottleneck, and utilizing gradient tracking is essential to proving convergence in the case of data heterogeneity. We analyze the performance of the proposed algorithm, Dec-FedTrack, in the case of nonconvex-strongly concave minimax optimization, and prove that it converges a stationary point. We also conduct numerical experiments to support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00965v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sajjad Ghiasvand, Amirhossein Reisizadeh, Mahnoosh Alizadeh, Ramtin Pedarsani</dc:creator>
    </item>
    <item>
      <title>Learning-Augmented Competitive Algorithms for Spatiotemporal Online Allocation with Deadline Constraints</title>
      <link>https://arxiv.org/abs/2408.07831</link>
      <description>arXiv:2408.07831v2 Announce Type: replace-cross 
Abstract: We introduce and study spatiotemporal online allocation with deadline constraints ($\mathsf{SOAD}$), a new online problem motivated by emerging challenges in sustainability and energy. In $\mathsf{SOAD}$, an online player completes a workload by allocating and scheduling it on the points of a metric space $(X, d)$ while subject to a deadline $T$. At each time step, a service cost function is revealed that represents the cost of servicing the workload at each point, and the player must irrevocably decide the current allocation of work to points. Whenever the player moves this allocation, they incur a movement cost defined by the distance metric $d(\cdot, \ \cdot)$ that captures, e.g., an overhead cost. $\mathsf{SOAD}$ formalizes the open problem of combining general metrics and deadline constraints in the online algorithms literature, unifying problems such as metrical task systems and online search. We propose a competitive algorithm for $\mathsf{SOAD}$ along with a matching lower bound establishing its optimality. Our main algorithm, \textsc{ST-CLIP}, is a learning-augmented algorithm that takes advantage of predictions (e.g., forecasts of relevant costs) and achieves an optimal consistency-robustness trade-off. We evaluate our proposed algorithms in a simulated case study of carbon-aware spatiotemporal workload management, an application in sustainable computing that schedules a delay-tolerant batch compute job on a distributed network of data centers. In these experiments, we show that \textsc{ST-CLIP} substantially improves on heuristic baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07831v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3711701</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Meas. Anal. Comput. Syst. Volume 9, Issue 1, Article 8 (March 2025), 49 pages</arxiv:journal_reference>
      <dc:creator>Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</dc:creator>
    </item>
  </channel>
</rss>
