<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Decoding Balanced Linear Codes With Preprocessing</title>
      <link>https://arxiv.org/abs/2510.14347</link>
      <description>arXiv:2510.14347v1 Announce Type: new 
Abstract: Prange's information set algorithm is a decoding algorithm for arbitrary linear codes. It decodes corrupted codewords of any $\mathbb{F}_2$-linear code $C$ of message length $n$ up to relative error rate $O(\log n / n)$ in $\mathsf{poly}(n)$ time. We show that the error rate can be improved to $O((\log n)^2 / n)$, provided: (1) the decoder has access to a polynomial-length advice string that depends on $C$ only, and (2) $C$ is $n^{-\Omega(1)}$-balanced.
  As a consequence we improve the error tolerance in decoding random linear codes if inefficient preprocessing of the code is allowed. This reveals potential vulnerabilities in cryptographic applications of Learning Noisy Parities with low noise rate.
  Our main technical result is that the Hamming weight of $Hw$, where $H$ is a random sample of *short dual* codewords, measures the proximity of a word $w$ to the code in the regime of interest. Given such $H$ as advice, our algorithm corrects errors by locally minimizing this measure. We show that for most codes, the error rate tolerated by our decoder is asymptotically optimal among all algorithms whose decision is based on thresholding $Hw$ for an arbitrary polynomial-size advice matrix $H$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14347v1</guid>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrej Bogdanov, Rohit Chatterjee, Yunqi Li, Prashant Nalini Vasudevan</dc:creator>
    </item>
    <item>
      <title>Smoothed analysis for graph isomorphism</title>
      <link>https://arxiv.org/abs/2410.06095</link>
      <description>arXiv:2410.06095v3 Announce Type: replace-cross 
Abstract: There is no known polynomial-time algorithm for graph isomorphism testing, but elementary combinatorial "refinement" algorithms seem to be very efficient in practice. Some philosophical justification is provided by a classical theorem of Babai, Erd\H{o}s and Selkow: an extremely simple polynomial-time combinatorial algorithm (variously known as "na\"ive refinement", "na\"ive vertex classification", "colour refinement" or the "1-dimensional Weisfeiler-Leman algorithm") yields a so-called canonical labelling scheme for "almost all graphs". More precisely, for a typical outcome of a random graph $G(n,1/2)$, this simple combinatorial algorithm assigns labels to vertices in a way that easily permits isomorphism-testing against any other graph.
  We improve the Babai-Erd\H{o}s-Selkow theorem in two directions. First, we consider randomly perturbed graphs, in accordance with the smoothed analysis philosophy of Spielman and Teng: for any graph $G$, na\"ive refinement becomes effective after a tiny random perturbation to $G$ (specifically, the addition and removal of $O(n\log n)$ random edges). Actually, with a twist on na\"ive refinement, we show that $O(n)$ random additions and removals suffice. These results significantly improve on previous work of Gaudio-R\'acz-Sridhar, and are in certain senses best-possible.
  Second, we complete a long line of research on canonical labelling of random graphs: for any $p$ (possibly depending on $n$), we prove that a random graph $G(n,p)$ can typically be canonically labelled in polynomial time. This is most interesting in the extremely sparse regime where $p$ has order of magnitude $c/n$; denser regimes were previously handled by Bollob\'as, Czajka-Pandurangan, and Linial-Mosheiff. Our proof also provides a description of the automorphism group of a typical outcome of $G(n,p_n)$ (slightly correcting a prediction of Linial-Mosheiff).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06095v3</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Anastos, Matthew Kwan, Benjamin Moore</dc:creator>
    </item>
    <item>
      <title>Sensitivity Lower Bounds for Approximaiton Algorithms</title>
      <link>https://arxiv.org/abs/2411.02744</link>
      <description>arXiv:2411.02744v3 Announce Type: replace-cross 
Abstract: Sensitivity measures how much the output of an algorithm changes, in terms of Hamming distance, when part of the input is modified. While approximation algorithms with low sensitivity have been developed for many problems, no sensitivity lower bounds were previously known for approximation algorithms. In this work, we establish the first polynomial lower bound on the sensitivity of (randomized) approximation algorithms for constraint satisfaction problems (CSPs) by adapting the probabilistically checkable proof (PCP) framework to preserve sensitivity lower bounds. From this, we derive polynomial sensitivity lower bounds for approximation algorithms for a variety of problems, including maximum clique, minimum vertex cover, and maximum cut.
  Leveraging the connection between sensitivity and locality in the non-signaling model, which subsumes the LOCAL, quantum-LOCAL, and bounded dependence models, we establish locality lower bounds for several graph problems in the non-signaling model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02744v3</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noah Fleming, Yuichi Yoshida</dc:creator>
    </item>
    <item>
      <title>Cubic Incompleteness: Hilbert's Tenth Problem Over $\mathbb{N}$ Starts at $\delta=3$</title>
      <link>https://arxiv.org/abs/2510.00759</link>
      <description>arXiv:2510.00759v3 Announce Type: replace-cross 
Abstract: We prove that Hilbert's Tenth Problem over $\mathbb{N}$ remains undecidable when restricted to cubic equations (degree $\leq 3$), resolving the open case $\delta = 3$ identified by Jones (1982) and establishing sharpness against the decidability barrier at $\delta = 2$ (Lagrange's four-square theorem). For any consistent, recursively axiomatizable theory $T$ with G\"odel sentence $G_T$, we effectively construct a single polynomial $P(x_1, \ldots, x_m) \in \mathbb{Z}[\mathbf{x}]$ of degree $\leq 3$ such that $T \vdash G_T$ if and only if $\exists \mathbf{x} \in \mathbb{N}^m : P(\mathbf{x}) = 0$.
  Our reduction proceeds through four stages with explicit degree and variable accounting. First, proof-sequence encoding via Diophantine $\beta$-function and Zeckendorf representation yields $O(KN)$ quadratic constraints, where $K = O(\log(\max_i f_i))$ and $N$ is the proof length. Second, axiom--modus ponens verification is implemented via guard-gadgets wrapping each base constraint $E(\mathbf{x}) = 0$ into the system $u \cdot E(\mathbf{x}) = 0$, $u - 1 - v^2 = 0$, maintaining degree $\leq 3$ while introducing $O(KN^3)$ variables and equations. Third, system aggregation via sum-of-squares merger $P_{\text{merged}} = \sum_{i} P_i^2$ produces a single polynomial of degree $\leq 6$ with $O(KN^3)$ monomials. Fourth, recursive monomial shielding factors each monomial of degree exceeding $3$ in $O(\log d)$ rounds via auxiliary variables and degree-$\leq 3$ equations, adding $O(K^3 N^3)$ variables and restoring degree $\leq 3$. We provide bookkeeping for every guard-gadget and merging operation, plus a unified stage-by-stage variable-count table. Our construction is effective and non-uniform in the uncomputable proof length $N$, avoiding any universal cubic equation. This completes the proof that the class of cubic Diophantine equations over $\mathbb{N}$ is undecidable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00759v3</guid>
      <category>math.LO</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milan Rosko</dc:creator>
    </item>
    <item>
      <title>VC-Dimension vs Degree: An Uncertainty Principle for Boolean Functions</title>
      <link>https://arxiv.org/abs/2510.13705</link>
      <description>arXiv:2510.13705v2 Announce Type: replace-cross 
Abstract: In this paper, we uncover a new uncertainty principle that governs the complexity of Boolean functions. This principle manifests as a fundamental trade-off between two central measures of complexity: a combinatorial complexity of its supported set, captured by its Vapnik-Chervonenkis dimension ($\mathrm{VC}(f)$), and its algebraic structure, captured by its polynomial degree over various fields. We establish two primary inequalities that formalize this trade-off: $\mathrm{VC}(f)+\mathrm{deg}(f)\ge n,$ and $\mathrm{VC}(f)+\mathrm{deg}_{\mathbb{F}_2}(f)\ge n$. In particular, these results recover the classical uncertainty principle on the discrete hypercube, as well as the Sziklai--Weiner's bound in the case of $\mathbb{F}_2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13705v2</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Chang, Yijia Fang</dc:creator>
    </item>
  </channel>
</rss>
