<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Aug 2025 02:25:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Two-Timescale Dynamic Service Deployment and Task Scheduling with Spatiotemporal Collaboration in Mobile Edge Networks</title>
      <link>https://arxiv.org/abs/2508.16293</link>
      <description>arXiv:2508.16293v1 Announce Type: new 
Abstract: Collaborative edge computing addresses the resource constraints of individual edge nodes by enabling resource sharing and task co-processing across multiple nodes. To fully leverage the advantages of collaborative edge computing, joint optimization of service deployment and task scheduling is necessary. Existing optimization methods insufficiently address the collaboration across spatial and temporal dimensions, which hinders their adaptability to the spatiotemporally varying nature of user demands and system states. This paper focuses on optimizing the expected task processing delay in edge networks. We propose a two-timescale online optimization framework to jointly determine: i) service deployment decisions at each large timescale; and ii) task scheduling decisions at each small timescale. Specifically, the convex optimization technique is used to solve the task scheduling problem, while a multi-agent deep reinforcement learning technique is employed for the service deployment problem. These two methods are combined for spatiotemporal co-optimization through a two-timescale alternating optimization approach. Compared to the baseline algorithms, the proposed scheme achieves better delay performance, while also exhibiting low running time and favorable convergence behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16293v1</guid>
      <category>cs.PF</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Li, Xing Zhang, Yunji Zhao, Wenbo Wang</dc:creator>
    </item>
    <item>
      <title>GreenLLM: SLO-Aware Dynamic Frequency Scaling for Energy-Efficient LLM Serving</title>
      <link>https://arxiv.org/abs/2508.16449</link>
      <description>arXiv:2508.16449v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are becoming the backbone of modern cloud services, yet their inference costs are dominated by GPU energy. Unlike traditional GPU workloads, LLM inference has two stages with different characteristics: the prefill phase, which is latency sensitive and scales quadratically with prompt length, and the decode phase, which progresses token by token with unpredictable length. Current GPU power governors (for example, NVIDIA's default) overlook this asymmetry and treat both stages uniformly. The result is mismatched voltage and frequency settings, head-of-line blocking, and excessive energy use.
  We introduce GreenLLM, an SLO-aware serving framework that minimizes GPU energy by explicitly separating prefill and decode control. At ingress, requests are routed into length-based queues so short prompts avoid head-of-line blocking and TTFT improves. For prefill, GreenLLM collects short traces on a GPU node, fits compact latency-power models over SM frequency, and solves a queueing-aware optimization to select energy-minimal clocks per class. During decode, a lightweight dual-loop controller tracks throughput (tokens per second) and adjusts frequency with hysteretic, fine-grained steps to hold tail TBT within target bounds. Across Alibaba and Azure trace replays, GreenLLM reduces total energy by up to 34 percent versus the default DVFS baseline, with no loss of throughput and with less than 3.5 percent additional SLO violations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16449v1</guid>
      <category>cs.PF</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qunyou Liu, Darong Huang, Marina Zapater, David Atienza</dc:creator>
    </item>
    <item>
      <title>Denoising Application Performance Models with Noise-Resilient Priors</title>
      <link>https://arxiv.org/abs/2504.10996</link>
      <description>arXiv:2504.10996v3 Announce Type: replace 
Abstract: As parallel codes are scaled to larger computing systems, performance models play a crucial role in identifying potential bottlenecks. However, constructing these models analytically is often challenging. Empirical models based on performance measurements provide a practical alternative, but measurements on high-performance computing (HPC) systems are frequently affected by noise, which can lead to misleading predictions. To mitigate the impact of noise, we introduce application-specific dynamic priors into the modeling process. These priors are derived from noise-resilient measurements of computational effort, combined with domain knowledge about common algorithms used in communication routines. By incorporating these priors, we effectively constrain the model's search space, eliminating complexity classes that capture noise rather than true performance characteristics. This approach keeps the models closely aligned with theoretical expectations and substantially enhances their predictive accuracy. Moreover, it reduces experimental overhead by cutting the number of repeated measurements by half.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10996v3</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gustavo de Morais, Alexander Gei{\ss}, Alexandru Calotoiu, Gregor Corbin, Ahmad Tarraf, Torsten Hoefler, Bernd Mohr, Felix Wolf</dc:creator>
    </item>
    <item>
      <title>Automotive Middleware Performance: Comparison of FastDDS, Zenoh and vSomeIP</title>
      <link>https://arxiv.org/abs/2505.02734</link>
      <description>arXiv:2505.02734v2 Announce Type: replace 
Abstract: In this study, we evaluate the performance of current automotive communication middlewares under various operating conditions. Specifically, we examine FastDDS, a widely used open-source middleware, the newly developed Zenoh middleware, and vSomeIP, COVESAs open-source implementation of SOME/IP. Our objective is to identify the best performing middleware for specific operating conditions. To ensure accessibility, we first provide a concise overview of middleware technologies and their fundamental principles. We then introduce our testing methodology designed to systematically assess middleware performance metrics such as scaling performance, end-to-end latency, and discovery times across multiple message types, network topologies, and configurations. Finally, we compare the resulting performance data and present our results in nine findings. Our evaluation code and the resulting data will be made publicly available upon acceptance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02734v2</guid>
      <category>cs.PF</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Philipp Kl\"uner, Lucas Hegerath, Amin Dieter Hatib, Stefan Kowalewski, Bassam Alrifaee, Alexandru Kampmann</dc:creator>
    </item>
    <item>
      <title>Asymmetries of Service: Interdependence and Synchronicity</title>
      <link>https://arxiv.org/abs/2402.15533</link>
      <description>arXiv:2402.15533v3 Announce Type: replace-cross 
Abstract: On many dimensions, services can be seen to exist along spectra measuring the degree of interaction between customer and agent. For instance, every interaction features some number of contributions by each of those two sides, creating a spectrum of interdependence. Additionally, each interaction is further characterized by the relative pacing of these contributions, implying a spectrum of synchronicity. Where a service falls on such spectra can be a consequence of its design, but it can also be a function of its state. For instance, as broadly evidenced empirically, an agent with several concurrent interactions will be slowed in each individual interaction, altering the service's synchronicity. Here, we study a Hawkes cluster model of the service interaction, which we show captures the interdependence and synchronicity spectra and their resulting customer-agent (a)symmetries. We find insightful connections to behavioral operations, such as proving the occurrence of non-monotonic performance (e.g., inverted-U throughput) from concurrency-driven asynchrony. Hence, we can prescribe the agent's optimal concurrency level. Furthermore, we show how the service design dictates the efficacy of these operational improvements, proving that the concurrency-optimized throughput is itself non-monotonic as a function of the interdependence. Of possible independent interest methodologically, we establish an interpretable temporal decomposition for Hawkes clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15533v3</guid>
      <category>cs.GT</category>
      <category>cs.PF</category>
      <category>math.PR</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Daw, Galit B. Yom-Tov</dc:creator>
    </item>
    <item>
      <title>GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization</title>
      <link>https://arxiv.org/abs/2506.20807</link>
      <description>arXiv:2506.20807v2 Announce Type: replace-cross 
Abstract: Optimizing GPU kernels for high performance is a complex task, often demanding deep architectural knowledge, extensive profiling, and iterative experimentation. This challenge is amplified when targeting newer or less-documented GPU architectures where traditional development aids are scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a) strategically selecting promising prior code versions as a basis for new iterations; (b) generating hypotheses for optimization experiments, based on existing code and assimilated knowledge from general GPU literature; and (c) autonomously implementing these experiments through code modification and subsequent submission to an external evaluation system, using only observed timing data as performance feedback. We detail how this approach navigates the challenges of the AMD MI300 target architecture and leverages LLMs to compensate for limited domain-specific human expertise.
  In addition to our results, we present the architectural design, operational workflow, and qualitative insights, highlighting the potential of LLM-driven agents to democratise and accelerate GPU kernel optimization, especially in resource-constrained or rapidly updating hardware environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20807v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Martin Andrews, Sam Witteveen</dc:creator>
    </item>
  </channel>
</rss>
