<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 05:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Scalability of the second-order reliability method for stochastic differential equations with multiplicative noise</title>
      <link>https://arxiv.org/abs/2502.20114</link>
      <description>arXiv:2502.20114v1 Announce Type: new 
Abstract: We show how to efficiently compute asymptotically sharp estimates of extreme event probabilities in stochastic differential equations (SDEs) with small multiplicative Brownian noise. The underlying approximation is known as sharp large deviation theory or precise Laplace asymptotics in mathematics, the second-order reliability method (SORM) in reliability engineering, and the instanton or optimal fluctuation method with 1-loop corrections in physics. It is based on approximating the tail probability in question with the most probable realization of the stochastic process, and local perturbations around this realization. We first recall and contextualize the relevant classical theoretical result on precise Laplace asymptotics of diffusion processes [Ben Arous (1988), Stochastics, 25(3), 125-153], and then show how to compute the involved infinite-dimensional quantities - operator traces and Carleman-Fredholm determinants - numerically in a way that is scalable with respect to the time discretization and remains feasible in high spatial dimensions. Using tools from automatic differentiation, we achieve a straightforward black-box numerical computation of the SORM estimates in JAX. The method is illustrated in examples of SDEs and stochastic partial differential equations, including a two-dimensional random advection-diffusion model of a passive scalar. We thereby demonstrate that it is possible to obtain efficient and accurate SORM estimates for very high-dimensional problems, as long as the infinite-dimensional structure of the problem is correctly taken into account. Our JAX implementation of the method is made publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20114v1</guid>
      <category>stat.CO</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Schorlepp, Tobias Grafke</dc:creator>
    </item>
    <item>
      <title>rSPDE: tools for statistical modeling using fractional SPDEs</title>
      <link>https://arxiv.org/abs/2502.20385</link>
      <description>arXiv:2502.20385v1 Announce Type: new 
Abstract: The R software package rSPDE contains methods for approximating Gaussian random fields based on fractional-order stochastic partial differential equations (SPDEs). A common example of such fields are Whittle-Mat\'ern fields on bounded domains in $\mathbb{R}^d$, manifolds, or metric graphs. The package also implements various other models which are briefly introduced in this article. Besides the approximation methods, the package contains methods for simulation, prediction, and statistical inference for such models, as well as interfaces to INLA, inlabru and MetricGraph. With these interfaces, fractional-order SPDEs can be used as model components in general latent Gaussian models, for which full Bayesian inference can be performed, also for fractional models on metric graphs. This includes estimation of the smoothness parameter of the fields. This article describes the computational methods used in the package and summarizes the theoretical basis for these. The main functions of the package are introduced, and their usage is illustrated through various examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20385v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Bolin, Alexandre B. Simas</dc:creator>
    </item>
    <item>
      <title>A Principled Approach to Bayesian Transfer Learning</title>
      <link>https://arxiv.org/abs/2502.19796</link>
      <description>arXiv:2502.19796v1 Announce Type: cross 
Abstract: Updating $\textit{a priori}$ information given some observed data is the core tenet of Bayesian inference. Bayesian transfer learning extends this idea by incorporating information from a related dataset to improve the inference on the observed data which may have been collected under slightly different settings. The use of related information can be useful when the observed data is scarce, for example. Current Bayesian transfer learning methods that are based on the so-called $\textit{power prior}$ can adaptively transfer information from related data. Unfortunately, it is not always clear under which scenario Bayesian transfer learning performs best or even if it will improve Bayesian inference. Additionally, current power prior methods rely on conjugacy to evaluate the posterior of interest. We propose using leave-one-out cross validation on the target dataset as a means of evaluating Bayesian transfer learning methods. Further, we introduce a new framework, $\textit{transfer sequential Monte Carlo}$, for power prior approaches that efficiently chooses the transfer parameter while avoiding the need for conjugate priors. We assess the performance of our proposed methods in two comprehensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19796v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Bretherton, Joshua J. Bon, David J. Warne, Kerrie Mengersen, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Fast Variational Boosting for Latent Variable Models</title>
      <link>https://arxiv.org/abs/2502.19839</link>
      <description>arXiv:2502.19839v1 Announce Type: cross 
Abstract: We consider the problem of estimating complex statistical latent variable models using variational Bayes methods. These methods are used when exact posterior inference is either infeasible or computationally expensive, and they approximate the posterior density with a family of tractable distributions. The parameters of the approximating distribution are estimated using optimisation methods. This article develops a flexible Gaussian mixture variational approximation, where we impose sparsity in the precision matrix of each Gaussian component to reflect the appropriate conditional independence structure in the model. By introducing sparsity in the precision matrix and parameterising it using the Cholesky factor, each Gaussian mixture component becomes parsimonious (with a reduced number of non-zero parameters), while still capturing the dependence in the posterior distribution. Fast estimation methods based on global and local variational boosting moves combined with natural gradients and variance reduction methods are developed. The local boosting moves adjust an existing mixture component, and optimisation is only carried out on a subset of the variational parameters of a new component. The subset is chosen to target improvement of the current approximation in aspects where it is poor. The local boosting moves are fast because only a small number of variational parameters need to be optimised. The efficacy of the approach is illustrated by using simulated and real datasets to estimate generalised linear mixed models and state space models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19839v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Gunawan, David Nott, Robert Kohn</dc:creator>
    </item>
    <item>
      <title>Multiple Linked Tensor Factorization</title>
      <link>https://arxiv.org/abs/2502.20286</link>
      <description>arXiv:2502.20286v1 Announce Type: cross 
Abstract: In biomedical research and other fields, it is now common to generate high content data that are both multi-source and multi-way. Multi-source data are collected from different high-throughput technologies while multi-way data are collected over multiple dimensions, yielding multiple tensor arrays. Integrative analysis of these data sets is needed, e.g., to capture and synthesize different facets of complex biological systems. However, despite growing interest in multi-source and multi-way factorization techniques, methods that can handle data that are both multi-source and multi-way are limited. In this work, we propose a Multiple Linked Tensors Factorization (MULTIFAC) method extending the CANDECOMP/PARAFAC (CP) decomposition to simultaneously reduce the dimension of multiple multi-way arrays and approximate underlying signal. We first introduce a version of the CP factorization with L2 penalties on the latent factors, leading to rank sparsity. When extended to multiple linked tensors, the method automatically reveals latent components that are shared across data sources or individual to each data source. We also extend the decomposition algorithm to its expectation-maximization (EM) version to handle incomplete data with imputation. Extensive simulation studies are conducted to demonstrate MULTIFAC's ability to (i) approximate underlying signal, (ii) identify shared and unshared structures, and (iii) impute missing data. The approach yields an interpretable decomposition on multi-way multi-omics data for a study on early-life iron deficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20286v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyu Kang, Raghavendra B. Rao, Eric F. Lock</dc:creator>
    </item>
    <item>
      <title>New methods to compute the generalized chi-square distribution</title>
      <link>https://arxiv.org/abs/2404.05062</link>
      <description>arXiv:2404.05062v3 Announce Type: replace 
Abstract: We present four new mathematical methods, two exact and two approximate, along with open-source software, to compute the cdf, pdf and inverse cdf of the generalized chi-square distribution. Some methods are geared for speed, while others are designed to be accurate far into the tails, using which we can also measure large values of the discriminability index $d'$ between multivariate normal distributions. We compare the accuracy and speed of these and previous methods, characterize their advantages and limitations, and identify the best methods to use in different cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05062v3</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abhranil Das</dc:creator>
    </item>
    <item>
      <title>Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo</title>
      <link>https://arxiv.org/abs/2407.20722</link>
      <description>arXiv:2407.20722v2 Announce Type: replace-cross 
Abstract: Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian inference but suffer from high computational costs due to their reliance on large particle ensembles for accurate estimates. We introduce persistent sampling (PS), an extension of SMC that systematically retains and reuses particles from all prior iterations to construct a growing, weighted ensemble. By leveraging multiple importance sampling and resampling from a mixture of historical distributions, PS mitigates the need for excessively large particle counts, directly addressing key limitations of SMC such as particle impoverishment and mode collapse. Crucially, PS achieves this without additional likelihood evaluations-weights for persistent particles are computed using cached likelihood values. This framework not only yields more accurate posterior approximations but also produces marginal likelihood estimates with significantly lower variance, enhancing reliability in model comparison. Furthermore, the persistent ensemble enables efficient adaptation of transition kernels by leveraging a larger, decorrelated particle pool. Experiments on high-dimensional Gaussian mixtures, hierarchical models, and non-convex targets demonstrate that PS consistently outperforms standard SMC and related variants, including recycled and waste-free SMC, achieving substantial reductions in mean squared error for posterior expectations and evidence estimates, all at reduced computational cost. PS thus establishes itself as a robust, scalable, and efficient alternative for complex Bayesian inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20722v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minas Karamanis, Uro\v{s} Seljak</dc:creator>
    </item>
  </channel>
</rss>
