<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Feb 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adversarially-Robust Gossip Algorithms for Approximate Quantile and Mean Computations</title>
      <link>https://arxiv.org/abs/2502.15320</link>
      <description>arXiv:2502.15320v1 Announce Type: new 
Abstract: This paper presents the first gossip algorithms that are robust to adversarial corruptions. Gossip algorithms distribute information in a scalable and efficient way by having random pairs of nodes exchange small messages. Value aggregation problems are of particular interest in this setting as they occur frequently in practice and many elegant algorithms have been proposed for computing aggregates and statistics such as averages and quantiles. An important and well-studied advantage of gossip algorithms is their robustness to message delays, network churn, and unreliable message transmissions. These crucial robustness guarantees however only hold if all nodes follow the protocol and no messages are corrupted. In this paper, we remedy this by providing a framework to model both adversarial participants and message corruptions in gossip-style communications by allowing an adversary to control a small fraction of the nodes or corrupt messages arbitrarily. Despite this very powerful and general corruption model, we show that one can design robust gossip algorithms for many important aggregation problems. Our algorithms guarantee that almost all nodes converge to an approximately correct answer with optimal efficiency and essentially as fast as without corruptions. The design of adversarially-robust gossip algorithms poses completely new challenges. Despite this, our algorithms remain very simple variations of known non-robust algorithms with often only subtle changes to avoid non-compliant nodes gaining too much influence over outcomes. While our algorithms remain simple, their analysis is much more complex and often requires a completely different approach than the non-adversarial setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15320v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, Marc Kaufmann, Raghu Raman Ravi, Ulysse Schaller</dc:creator>
    </item>
    <item>
      <title>Streaming Maximal Matching with Bounded Deletions</title>
      <link>https://arxiv.org/abs/2502.15330</link>
      <description>arXiv:2502.15330v1 Announce Type: new 
Abstract: We initiate the study of the Maximal Matching problem in bounded-deletion graph streams. In this setting, a graph $G$ is revealed as an arbitrary sequence of edge insertions and deletions, where the number of insertions is unrestricted but the number of deletions is guaranteed to be at most $K$, for some given parameter $K$. The single-pass streaming space complexity of this problem is known to be $\Theta(n^2)$ when $K$ is unrestricted, where $n$ is the number of vertices of the input graph. In this work, we present new randomized and deterministic algorithms and matching lower bound results that together give a tight understanding (up to poly-log factors) of how the space complexity of Maximal Matching evolves as a function of the parameter $K$: The randomized space complexity of this problem is $\tilde{\Theta}(n \cdot \sqrt{K})$, while the deterministic space complexity is $\tilde{\Theta}(n \cdot K)$. We further show that if we relax the maximal matching requirement to an $\alpha$-approximation to Maximum Matching, for any constant $\alpha &gt; 2$, then the space complexity for both, deterministic and randomized algorithms, strikingly changes to $\tilde{\Theta}(n + K)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15330v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjeev Khanna, Christian Konrad, Jacques Dark</dc:creator>
    </item>
    <item>
      <title>Improved Sublinear-time Moment Estimation using Weighted Sampling</title>
      <link>https://arxiv.org/abs/2502.15333</link>
      <description>arXiv:2502.15333v1 Announce Type: new 
Abstract: In this work we study the {\it moment estimation} problem using weighted sampling. Given sample access to a set $A$ with $n$ weighted elements, and a parameter $t&gt;0$, we estimate the $t$-th moment of $A$ given as $S_t=\sum_{a\in A} w(a)^t$. For t=1, this is the sum estimation problem. The moment estimation problem along with a number of its variants have been extensively studied in streaming, sublinear and distributed communication models. Despite being well studied, we don't yet have a complete understanding of the sample complexity of the moment estimation problem in the sublinear model and in this work, we make progress on this front. On the algorithmic side, our upper bounds match the known upper bounds for the problem for $t&gt;1$. To the best of our knowledge, no sublinear algorithms were known for this problem for $0&lt;t&lt;1$. We design a sublinear algorithm for this problem for $t&gt;1/2$ and show that no sublinear algorithms exist for $t\leq 1/2$. We prove a $\Omega(\frac{n^{1-1/t}\ln 1/\delta}{\epsilon^2})$ lower bound for moment estimation for $t&gt;1$, and show optimal sample complexity bound $\Theta(\frac{n^{1-1/t}\ln 1/\delta}{\epsilon^2})$ for moment estimation for $t\geq 2$. Hence, we obtain a complete understanding of the sample complexity for moment estimation using proportional sampling for $t\geq 2$. We also study the moment estimation problem in the beyond worst-case analysis paradigm and identify a new {\it moment-density} parameter of the input that characterizes the sample complexity of the problem using proportional sampling and derive tight sample complexity bounds with respect to that parameter. We also study the moment estimation problem in the hybrid sampling framework in which one is given additional access to a uniform sampling oracle and show that hybrid sampling framework does not provide any additional gain over the proportional sampling oracle in the worst case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15333v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anup Bhattacharya, Pinki Pradhan</dc:creator>
    </item>
    <item>
      <title>Optimal Distributed Replacement Paths</title>
      <link>https://arxiv.org/abs/2502.15378</link>
      <description>arXiv:2502.15378v1 Announce Type: new 
Abstract: We study the replacement paths problem in the $\mathsf{CONGEST}$ model of distributed computing. Given an $s$-$t$ shortest path $P$, the goal is to compute, for every edge $e$ in $P$, the shortest-path distance from $s$ to $t$ avoiding $e$. For unweighted directed graphs, we establish the tight randomized round complexity bound for this problem as $\widetilde{\Theta}(n^{2/3} + D)$ by showing matching upper and lower bounds. Our upper bound extends to $(1+\epsilon)$-approximation for weighted directed graphs. Our lower bound applies even to the second simple shortest path problem, which asks only for the smallest replacement path length. These results improve upon the very recent work of Manoharan and Ramachandran (SIROCCO 2024), who showed a lower bound of $\widetilde{\Omega}(n^{1/2} + D)$ and an upper bound of $\widetilde{O}(n^{2/3} + \sqrt{n h_{st}} + D)$, where $h_{st}$ is the number of hops in the given $s$-$t$ shortest path $P$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15378v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi-Jun Chang, Yanyu Chen, Dipan Dey, Gopinath Mishra, Hung Thuan Nguyen, Bryce Sanchez</dc:creator>
    </item>
    <item>
      <title>Arboricity and Random Edge Queries Matter for Triangle Counting using Sublinear Queries</title>
      <link>https://arxiv.org/abs/2502.15379</link>
      <description>arXiv:2502.15379v1 Announce Type: new 
Abstract: Given a simple, unweighted, undirected graph $G=(V,E)$ with $|V|=n$ and $|E|=m$, and parameters $0 &lt; \varepsilon, \delta &lt;1$, along with \texttt{Degree}, \texttt{Neighbour}, \texttt{Edge} and \texttt{RandomEdge} query access to $G$, we provide a query based randomized algorithm to generate an estimate $\widehat{T}$ of the number of triangles $T$ in $G$, such that $\widehat{T} \in [(1-\varepsilon)T , (1+\varepsilon)T]$ with probability at least $1-\delta$. The query complexity of our algorithm is $\widetilde{O}\left({m \alpha \log(1/\delta)}/{\varepsilon^3 T}\right)$, where $\alpha$ is the arboricity of $G$. Our work can be seen as a continuation in the line of recent works [Eden et al., SIAM J Comp., 2017; Assadi et al., ITCS 2019; Eden et al. SODA 2020] that considered subgraph or triangle counting with or without the use of \texttt{RandomEdge} query. Of these works, Eden et al. [SODA 2020] considers the role of arboricity. Our work considers how \texttt{RandomEdge} query can leverage the notion of arboricity. Furthermore, continuing in the line of work of Assadi et al. [APPROX/RANDOM 2022], we also provide a lower bound of $\widetilde{\Omega}\left({m \alpha \log(1/\delta)}/{\varepsilon^2 T}\right)$ that matches the upper bound exactly on arboricity and the parameter $\delta$ and almost on $\varepsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15379v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Arijit Bishnu, Debarshi Chanda, Gopinath Mishra</dc:creator>
    </item>
    <item>
      <title>PtrHash: Minimal Perfect Hashing at RAM Throughput</title>
      <link>https://arxiv.org/abs/2502.15539</link>
      <description>arXiv:2502.15539v1 Announce Type: new 
Abstract: Given a set $S$ of $n$ keys, a minimal perfect hash function (MPHF) is a collision-free bijective map $\mathsf{H_{mphf}}$ from $S$ to $\{0, \dots, n-1\}$. This work presents a (minimal) perfect hash function that first prioritizes query throughput, while also allowing efficient construction for $10^9$ or more elements using 2.4 bits of memory per key.
  Both PTHash and PHOBIC first map all $n$ keys to $n/\lambda &lt; n$ buckets. Then, each bucket stores a pilot that controls the final hash value of the keys mapping to it. PtrHash builds on this by using 1) fixed-width (uncompressed) 8-bit pilots, 2) a construction algorithm similar to cuckoo-hashing to find suitable pilot values. Further, it 3) uses the same number of buckets and slots for each part, with 4) a single remap table to map intermediate positions $\geq n$ to $&lt;n$, 5) encoded using per-cacheline Elias-Fano coding. Lastly, 6) PtrHash support streaming queries, where we use prefetching to answer a stream of multiple queries more efficiently than one-by-one processing.
  With default parameters, PtrHash takes 2.40 bits per key. On 300 million string keys, PtrHash is as fast or faster to build than other MPHFs, and at least $1.75\times$ faster to query. When streaming multiple queries, this improves to $3.1\times$ speedup over the fastest alternative, while also being significantly faster to construct. When using $10^9$ integer keys instead, query times are as low as 12 ns/key when iterating in a for loop, or even down to 8 ns/key when using the streaming approach, within $10\%$ of the maximum memory-bound throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15539v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ragnar Groot Koerkamp</dc:creator>
    </item>
    <item>
      <title>Efficient and Provable Algorithms for Covariate Shift</title>
      <link>https://arxiv.org/abs/2502.15372</link>
      <description>arXiv:2502.15372v1 Announce Type: cross 
Abstract: Covariate shift, a widely used assumption in tackling {\it distributional shift} (when training and test distributions differ), focuses on scenarios where the distribution of the labels conditioned on the feature vector is the same, but the distribution of features in the training and test data are different. Despite the significance and extensive work on covariate shift, theoretical guarantees for algorithms in this domain remain sparse. In this paper, we distill the essence of the covariate shift problem and focus on estimating the average $\mathbb{E}_{\tilde{\mathbf{x}}\sim p_{\mathrm{test}}}\mathbf{f}(\tilde{\mathbf{x}})$, of any unknown and bounded function $\mathbf{f}$, given labeled training samples $(\mathbf{x}_i, \mathbf{f}(\mathbf{x}_i))$, and unlabeled test samples $\tilde{\mathbf{x}}_i$; this is a core subroutine for several widely studied learning problems. We give several efficient algorithms, with provable sample complexity and computational guarantees. Moreover, we provide the first rigorous analysis of algorithms in this space when $\mathbf{f}$ is unrestricted, laying the groundwork for developing a solid theoretical foundation for covariate shift problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15372v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deeksha Adil, Jaros{\l}aw B{\l}asiok</dc:creator>
    </item>
    <item>
      <title>A $2\ell k$ Kernel for $\ell$-Component Order Connectivity</title>
      <link>https://arxiv.org/abs/1610.04711</link>
      <description>arXiv:1610.04711v2 Announce Type: replace 
Abstract: In the $\ell$-Component Order Connectivity problem ($\ell \in \mathbb{N}$), we are given a graph $G$ on $n$ vertices, $m$ edges and a non-negative integer $k$ and asks whether there exists a set of vertices $S\subseteq V(G)$ such that $|S|\leq k$ and the size of the largest connected component in $G-S$ is at most $\ell$. In this paper, we give a linear programming based kernel for $\ell$-Component Order Connectivity with at most $2\ell k$ vertices that takes $n^{\mathcal{O}(\ell)}$ time for every constant $\ell$. Thereafter, we provide a separation oracle for the LP of $\ell$-COC implying that the kernel only takes $(3e)^{\ell}\cdot n^{O(1)}$ time. On the way to obtaining our kernel, we prove a generalization of the $q$-Expansion Lemma to weighted graphs. This generalization may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:1610.04711v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mithilesh Kumar, Daniel Lokshtanov</dc:creator>
    </item>
    <item>
      <title>Fair Assortment Planning</title>
      <link>https://arxiv.org/abs/2208.07341</link>
      <description>arXiv:2208.07341v5 Announce Type: replace 
Abstract: Many online platforms, ranging from online retail stores to social media platforms, employ algorithms to optimize their offered assortment of items (e.g., products and contents). These algorithms often focus exclusively on achieving the platforms' objectives, highlighting items with the highest popularity or revenue. This approach, however, can compromise the equality of opportunities for the rest of the items, in turn leading to less content diversity and increased regulatory scrutiny for the platform. Motivated by this, we introduce and study a fair assortment planning problem that enforces equality of opportunities via pairwise fairness, which requires any two items to be offered similar outcomes. We show that the problem can be formulated as a linear program (LP), called (FAIR), that optimizes over the distribution of all feasible assortments. To find a near-optimal solution to (FAIR), we propose a framework based on the Ellipsoid method, which requires a polynomial-time separation oracle to the dual of the LP. We show that finding an optimal separation oracle to the dual problem is an NP-complete problem, and hence we propose a series of approximate separation oracles, which then result in a 1/2-approx. algorithm and an FPTAS for Problem (FAIR). The approximate separation oracles are designed by (i) showing the separation oracle to the dual of the LP is equivalent to solving an infinite series of parameterized knapsack problems, and (ii) leveraging the structure of knapsack problems. Finally, we perform numerical studies on both synthetic data and real-world MovieLens data, showcasing the effectiveness of our algorithms and providing insights into the platform's price of fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07341v5</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinyi Chen, Negin Golrezaei, Fransisca Susan</dc:creator>
    </item>
    <item>
      <title>Fast John Ellipsoid Computation with Differential Privacy Optimization</title>
      <link>https://arxiv.org/abs/2408.06395</link>
      <description>arXiv:2408.06395v2 Announce Type: replace 
Abstract: Determining the John ellipsoid - the largest volume ellipsoid contained within a convex polytope - is a fundamental problem with applications in machine learning, optimization, and data analytics. Recent work has developed fast algorithms for approximating the John ellipsoid using sketching and leverage score sampling techniques. However, these algorithms do not provide privacy guarantees for sensitive input data. In this paper, we present the first differentially private algorithm for fast John ellipsoid computation. Our method integrates noise perturbation with sketching and leverages score sampling to achieve both efficiency and privacy. We prove that (1) our algorithm provides $(\epsilon,\delta)$-differential privacy and the privacy guarantee holds for neighboring datasets that are $\epsilon_0$-close, allowing flexibility in the privacy definition; (2) our algorithm still converges to a $(1+\xi)$-approximation of the optimal John ellipsoid in $\Theta(\xi^{-2}(\log(n/\delta_0) + (L\epsilon_0)^{-2}))$ iterations where $n$ is the number of data point, $L$ is the Lipschitz constant, $\delta_0$ is the failure probability, and $\epsilon_0$ is the closeness of neighboring input datasets. Our theoretical analysis demonstrates the algorithm's convergence and privacy properties, providing a robust approach for balancing utility and privacy in John ellipsoid computation. This is the first differentially private algorithm for fast John ellipsoid computation, opening avenues for future research in privacy-preserving optimization techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06395v2</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Junwei Yu</dc:creator>
    </item>
    <item>
      <title>U-index: A Universal Indexing Framework for Matching Long Patterns</title>
      <link>https://arxiv.org/abs/2502.14488</link>
      <description>arXiv:2502.14488v2 Announce Type: replace 
Abstract: Text indexing is a fundamental and well-studied problem. Classic solutions either replace the original text with a compressed representation, e.g., the FM-index and its variants, or keep it uncompressed but attach some redundancy - an index - to accelerate matching. The former solutions thus retain excellent compressed space, but areslow in practice. The latter approaches, like the suffix array, instead sacrifice space for speed.
  We show that efficient text indexing can be achieved using just a small extra space on top of the original text, provided that the query patterns are sufficiently long. More specifically, we develop a new indexing paradigm in which a sketch of a query pattern is first matched against a sketch of the text. Once candidate matches are retrieved, they are verified using the original text. This paradigm is thus universal in the sense that it allows us to use any solution to index the sketched text, like a suffix array, FM-index, or r-index.
  We explore both the theory and the practice of this universal framework. With an extensive experimental analysis, we show that, surprisingly, universal indexes can be constructed much faster than their unsketched counterparts and take a fraction of the space, as a direct consequence of (i) having a lower bound on the length of patterns and (ii) working in sketch space. Furthermore, these data structures have the potential of retaining or even improving query time, because matching against the sketched text is faster and verifying candidates can be theoretically done in constant time per occurrence (or, in practice, by short and cache-friendly scans of the text). Finally, we discuss some important applications of this novel indexing paradigm to computational biology. We hypothesize that such indexes will be particularly effective when the queries are sufficiently long, and so demonstrate applications in long-read mapping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14488v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorraine A. K. Ayad, Gabriele Fici, Ragnar Groot Koerkamp, Grigorios Loukides, Rob Patro, Giulio Ermanno Pibiri, Solon P. Pissis</dc:creator>
    </item>
    <item>
      <title>Enumerating minimal dominating sets and variants in chordal bipartite graphs</title>
      <link>https://arxiv.org/abs/2502.14611</link>
      <description>arXiv:2502.14611v2 Announce Type: replace 
Abstract: Enumerating minimal dominating sets with polynomial delay in bipartite graphs is a long-standing open problem. To date, even the subcase of chordal bipartite graphs is open, with the best known algorithm due to Golovach, Heggernes, Kant\'e, Kratsch, Saether, and Villanger running in incremental-polynomial time. We improve on this result by providing a polynomial delay and space algorithm enumerating minimal dominating sets in chordal bipartite graphs. Additionally, we show that the total and connected variants admit polynomial and incremental-polynomial delay algorithms, respectively, within the same class. This provides an alternative proof of a result by Golovach et al. for total dominating sets, and answers an open question for the connected variant. Finally, we give evidence that the techniques used in this paper cannot be generalized to bipartite graphs for (total) minimal dominating sets, unless P = NP, and show that enumerating minimal connected dominating sets in bipartite graphs is harder than enumerating minimal transversals in general hypergraphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14611v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuel Castelo, Oscar Defrain, Guilherme C. M. Gomes</dc:creator>
    </item>
  </channel>
</rss>
