<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.FL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.FL</link>
    <description>cs.FL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.FL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Jun 2025 03:03:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs</title>
      <link>https://arxiv.org/abs/2505.24110</link>
      <description>arXiv:2505.24110v1 Announce Type: cross 
Abstract: We present a formal and constructive framework establishing the equivalence between nondeterministic finite automata (NFAs) and standard feedforward ReLU neural networks. By encoding automaton states as binary vectors and transitions as sparse linear layers, we show that ReLU activations simulate nondeterministic branching, subset construction, and $\epsilon$-closures in a mathematically precise manner. Our core theoretical results prove that a three-layer ReLU network of width $\mathcal{O}(n)$ can exactly recognize any regular language accepted by an $n$-state NFA-without recurrence, memory, or approximation. Furthermore, we show that gradient descent over structure-preserving networks preserves symbolic semantics and acceptance behavior. Extensive experiments across multiple validation tasks-including parallel path tracking, symbolic subset construction, $\epsilon$-closure convergence, acceptance classification, structural training invariants, and functional equivalence-achieve perfect or near-perfect empirical alignment with ground-truth automata. This work provides the first provably complete symbolic simulation of NFAs within standard deep learning architectures, uniting automata theory with neural computation through ReLU dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24110v1</guid>
      <category>cs.LG</category>
      <category>cs.FL</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahil Rajesh Dhayalkar</dc:creator>
    </item>
    <item>
      <title>Rewriting Structured Cospans</title>
      <link>https://arxiv.org/abs/2001.09029</link>
      <description>arXiv:2001.09029v2 Announce Type: replace-cross 
Abstract: To support the study of compositional networks, we further develop the formalism of structured cospans. To enable rewriting, we introduce a category whose objects are structured cospans and characterize conditions under which this category is a topos or is adhesive. We then define a structured cospan grammar and language, using a 2-category framework. As an application, we demonstrate that for various sorts of graphs, hypergraphs, and Petri nets, any grammar induces the same language as its corresponding discrete grammar. This result allows us to extend the inductive perspective of rewriting to these structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2001.09029v2</guid>
      <category>math.CT</category>
      <category>cs.FL</category>
      <category>cs.SI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Cicala</dc:creator>
    </item>
    <item>
      <title>Neuro-Symbolic Generation of Explanations for Robot Policies with Weighted Signal Temporal Logic</title>
      <link>https://arxiv.org/abs/2504.21841</link>
      <description>arXiv:2504.21841v2 Announce Type: replace-cross 
Abstract: Neural network-based policies have demonstrated success in many robotic applications, but often lack human-explanability, which poses challenges in safety-critical deployments. To address this, we propose a neuro-symbolic explanation framework that generates a weighted signal temporal logic (wSTL) specification to describe a robot policy in a interpretable form. Existing methods typically produce explanations that are verbose and inconsistent, which hinders explainability, and loose, which do not give meaningful insights into the underlying policy. We address these issues by introducing a simplification process consisting of predicate filtering, regularization, and iterative pruning. We also introduce three novel explainability evaluation metrics -- conciseness, consistency, and strictness -- to assess explanation quality beyond conventional classification metrics. Our method is validated in three simulated robotic environments, where it outperforms baselines in generating concise, consistent, and strict wSTL explanations without sacrificing classification accuracy. This work bridges policy learning with formal methods, contributing to safer and more transparent decision-making in robotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21841v2</guid>
      <category>cs.RO</category>
      <category>cs.FL</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mikihisa Yuasa, Ramavarapu S. Sreenivas, Huy T. Tran</dc:creator>
    </item>
  </channel>
</rss>
