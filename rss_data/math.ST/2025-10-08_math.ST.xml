<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Oct 2025 04:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Inference on Gaussian mixture models with dependent labels</title>
      <link>https://arxiv.org/abs/2510.06501</link>
      <description>arXiv:2510.06501v1 Announce Type: new 
Abstract: Gaussian mixture models are widely used to model data generated from multiple latent sources. Despite its popularity, most theoretical research assumes that the labels are either independent and identically distributed, or follows a Markov chain. It remains unclear how the fundamental limits of estimation change under more complex dependence. In this paper, we address this question for the spherical two-component Gaussian mixture model. We first show that for labels with an arbitrary dependence, a naive estimator based on the misspecified likelihood is $\sqrt{n}$-consistent. Additionally, under labels that follow an Ising model, we establish the information theoretic limitations for estimation, and discover an interesting phase transition as dependence becomes stronger. When the dependence is smaller than a threshold, the optimal estimator and its limiting variance exactly matches the independent case, for a wide class of Ising models. On the other hand, under stronger dependence, estimation becomes easier and the naive estimator is no longer optimal. Hence, we propose an alternative estimator based on the variational approximation of the likelihood, and argue its optimality under a specific Ising model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06501v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seunghyun Lee, Rajarshi Mukherjee, Sumit Mukherjee</dc:creator>
    </item>
    <item>
      <title>Stable central limit theorems for discrete-time lag martingale difference arrays</title>
      <link>https://arxiv.org/abs/2510.06524</link>
      <description>arXiv:2510.06524v1 Announce Type: new 
Abstract: Recent work in dynamic causal inference introduced a class of discrete-time stochastic processes that generalize martingale difference sequences and arrays as follows: the random variates in each sequence have expectation zero given certain lagged filtrations but not given the natural filtration. We formalize this class of stochastic processes and prove a stable central limit theorem (CLT) via a Bernstein blocking scheme and an application of the classical martingale CLT. We generalize our limit theorem to vector-valued processes via the Cram\'er-Wold device and develop a simple form for the limiting variance. We demonstrate the application of these results to a problem in dynamic causal inference and present a simulation study supporting their validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06524v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Easton Huch, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>Uniform ergodicity of geodesic slice sampling</title>
      <link>https://arxiv.org/abs/2510.06748</link>
      <description>arXiv:2510.06748v1 Announce Type: new 
Abstract: Geodesic slice sampling, introduced in Durmus et al., 2024, is a slice sampling based Markov chain Monte Carlo method for approximate sampling from distributions on Riemannian manifolds. We prove that it is uniformly ergodic for distributions with compact support that have a bounded density with respect to the Riemannian measure. The constants in our convergence bound are available explicitly, and we investigate their dependence on the hyperparameters of the geodesic slice sampler, the target distribution and the underlying domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06748v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mareike Hasenpflug</dc:creator>
    </item>
    <item>
      <title>Testing the equality of estimable parameters across many populations</title>
      <link>https://arxiv.org/abs/2510.06763</link>
      <description>arXiv:2510.06763v1 Announce Type: new 
Abstract: The comparison of a parameter in $k$ populations is a classical problem in statistics. Testing for the equality of means or variances are typical examples. Most procedures designed to deal with this problem assume that $k$ is fixed and that samples with increasing sample sizes are available from each population. This paper introduces and studies a test for the comparison of an estimable parameter across $k$ populations, when $k$ is large and the sample sizes from each population are small when compared with $k$. The proposed test statistic is asymptotically distribution-free under the null hypothesis of parameter homogeneity, enabling asymptotically exact inference without parametric assumptions. Additionally, the behaviour of the proposal is studied under alternatives. Simulations are conducted to evaluate its finite-sample performance, and a linear bootstrap method is implemented to improve its behaviour for small $k$. Finally, an application to a real dataset is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06763v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marcos Romero-Madro\~nal, Mar\'ia de los Remedios Sillero-Denamiel, Mar\'ia Dolores Jim\'enez-Gamero</dc:creator>
    </item>
    <item>
      <title>Slow, Fast and Opportunistic FAMA: A Spatial Block-Correlation Analysis under Nakagami-m Fading Channels</title>
      <link>https://arxiv.org/abs/2510.07142</link>
      <description>arXiv:2510.07142v1 Announce Type: new 
Abstract: This paper studies slow, fast and opportunistic fluid antenna multiple access (FAMA) under the effect of Nakagami-m fading channels, considering the new and realistic spatial blockcorrelation model. Expressions for the outage probability (OP), based on the signal-to-interference ratio (SIR), are derived for slow FAMA. Interestingly, we provide mathematical relationships that allow the expressions of fast FAMA to be obtained from slow FAMA. Multiplexing gains for an opportunistic FAMA (OFAMA) network are presented for both slow and fast FAMA scenarios. Our analytical results are validated through Monte Carlo simulations, under various channel and system parameters. All expressions derived in this work are original.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07142v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Paulo R. de Moura, Hugerles S. Silva, Ugo S. Dias, Higo T. P. Silva</dc:creator>
    </item>
    <item>
      <title>A Bernstein polynomial approach for the estimation of cumulative distribution functions in the presence of missing data</title>
      <link>https://arxiv.org/abs/2510.07235</link>
      <description>arXiv:2510.07235v1 Announce Type: new 
Abstract: We study nonparametric estimation of univariate cumulative distribution functions (CDFs) pertaining to data missing at random. The proposed estimators smooth the inverse probability weighted (IPW) empirical CDF with the Bernstein operator, yielding monotone, $[0,1]$-valued curves that automatically adapt to bounded supports. We analyze two versions: a pseudo estimator that uses known propensities and a feasible estimator that uses propensities estimated nonparametrically from discrete auxiliary variables, the latter scenario being much more common in practice. For both, we derive pointwise bias and variance expansions, establish the optimal polynomial degree $m$ with respect to the mean integrated squared error, and prove the asymptotic normality. A key finding is that the feasible estimator has a smaller variance than the pseudo estimator by an explicit nonnegative correction term. We also develop an efficient degree selection procedure via least-squares cross-validation. Monte Carlo experiments demonstrate that, for moderate to large sample sizes, the Bernstein-smoothed feasible estimator outperforms both its unsmoothed counterpart and an integrated version of the IPW kernel density estimator proposed by Dubnicka (2009) in the same context. A real-data application to fasting plasma glucose from the 2017-2018 NHANES survey illustrates the method in a practical setting. All code needed to reproduce our analyses is readily accessible on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07235v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rihab Gharbi, Wissem Jedidi, Salah Khardani, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Efficient reductions from a Gaussian source with applications to statistical-computational tradeoffs</title>
      <link>https://arxiv.org/abs/2510.07250</link>
      <description>arXiv:2510.07250v1 Announce Type: new 
Abstract: Given a single observation from a Gaussian distribution with unknown mean $\theta$, we design computationally efficient procedures that can approximately generate an observation from a different target distribution $Q_{\theta}$ uniformly for all $\theta$ in a parameter set. We leverage our technique to establish reduction-based computational lower bounds for several canonical high-dimensional statistical models under widely-believed conjectures in average-case complexity. In particular, we cover cases in which:
  1. $Q_{\theta}$ is a general location model with non-Gaussian distribution, including both light-tailed examples (e.g., generalized normal distributions) and heavy-tailed ones (e.g., Student's $t$-distributions). As a consequence, we show that computational lower bounds proved for spiked tensor PCA with Gaussian noise are universal, in that they extend to other non-Gaussian noise distributions within our class.
  2. $Q_{\theta}$ is a normal distribution with mean $f(\theta)$ for a general, smooth, and nonlinear link function $f:\mathbb{R} \rightarrow \mathbb{R}$. Using this reduction, we construct a reduction from symmetric mixtures of linear regressions to generalized linear models with link function $f$, and establish computational lower bounds for solving the $k$-sparse generalized linear model when $f$ is an even function. This result constitutes the first reduction-based confirmation of a $k$-to-$k^2$ statistical-to-computational gap in $k$-sparse phase retrieval, resolving a conjecture posed by Cai et al. (2016). As a second application, we construct a reduction from the sparse rank-1 submatrix model to the planted submatrix model, establishing a pointwise correspondence between the phase diagrams of the two models that faithfully preserves regions of computational hardness and tractability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07250v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengqi Lou, Guy Bresler, Ashwin Pananjady</dc:creator>
    </item>
    <item>
      <title>Spectral analysis of large dimensional Chatterjee's rank correlation matrix</title>
      <link>https://arxiv.org/abs/2510.07262</link>
      <description>arXiv:2510.07262v1 Announce Type: new 
Abstract: This paper studies the spectral behavior of large dimensional Chatterjee's rank correlation matrix when observations are independent draws from a high-dimensional random vector with independent continuous components. We show that the empirical spectral distribution of its symmetrized version converges to the semicircle law, and thus providing the first example of a large correlation matrix deviating from the Marchenko-Pastur law that governs those of Pearson, Kendall, and Spearman. We further establish central limit theorems for linear spectral statistics, which in turn enable the development of Chatterjee's rank correlation-based tests of complete independence among the components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07262v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaorui Dong, Fang Han, Jianfeng Yao</dc:creator>
    </item>
    <item>
      <title>Inference in pseudo-observation-based regression using (biased) covariance estimation and naive bootstrapping</title>
      <link>https://arxiv.org/abs/2510.06815</link>
      <description>arXiv:2510.06815v1 Announce Type: cross 
Abstract: We demonstrate that the usual Huber-White estimator is not consistent for the limiting covariance of parameter estimates in pseudo-observation regression approaches. By confirming that a plug-in estimator can be used instead, we obtain asymptotically exact and consistent tests for general linear hypotheses in the parameters of the model. Additionally, we confirm that naive bootstrapping can not be used for covariance estimation in the pseudo-observation model either. However, it can be used for hypothesis testing by applying a suitable studentization. Simulations illustrate the good performance of our proposed methods in many scenarios. Finally, we obtain a general uniform law of large numbers for U- and V-statistics, as such statistics are central in the mathematical analysis of the inference procedures developed in this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06815v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Simon Mack, Morten Overgaard, Dennis Dobler</dc:creator>
    </item>
    <item>
      <title>Multivariate CLT for L\'evy processes: convergence rates without moment assumptions</title>
      <link>https://arxiv.org/abs/2510.06891</link>
      <description>arXiv:2510.06891v1 Announce Type: cross 
Abstract: We prove that the norm of a $d$-dimensional L\'evy process possesses a finite second moment if and only if the convex distance between an appropriately rescaled process at time $t$ and a standard Gaussian vector is integrable in time with respect to the scale-invariant measure $t^{-1} dt$ on $[1,\infty)$. We further prove that under the standard $\sqrt{t}$-scaling, the corresponding convex distance is integrable if and only if the norm of the L\'evy process has a finite $(2+\log)$-moment. Both equivalences also hold for the integrability with respect to $t^{-1} dt$ of the multivariate Kolmogorov distance. Our results imply: (I) polynomial Berry-Esseen bounds on the rate of convergence in the convex distance in the CLT for L\'evy processes cannot hold without finiteness of $(2+\delta)$-moments for some $\delta&gt;0$ and (II) integrability of the convex distance with respect to $t^{-1} dt$ in the domain of non-normal attraction cannot occur for any scaling function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06891v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Gonz\'alez C\'azares, David Kramer-Bang, Aleksandar Mijatovi\'c</dc:creator>
    </item>
    <item>
      <title>Beyond the Oracle Property: Adaptive LASSO in Cointegrating Regressions</title>
      <link>https://arxiv.org/abs/2510.07204</link>
      <description>arXiv:2510.07204v1 Announce Type: cross 
Abstract: This paper establishes new asymptotic results for the adaptive LASSO estimator in cointegrating regression models. We study model selection probabilities, estimator consistency, and limiting distributions under both standard and moving-parameter asymptotics. We also derive uniform convergence rates and the fastest local-to-zero rates that can still be detected by the estimator, complementing and extending the results of Lee, Shi, and Gao (2022, Journal of Econometrics, 229, 322--349). Our main findings include that under conservative tuning, the adaptive LASSO estimator is uniformly $T$-consistent and the cut-off rate for local-to-zero coefficients that can be detected by the procedure is $1/T$. Under consistent tuning, however, both rates are slower and depend on the tuning parameter. The theoretical results are complemented by a detailed simulation study showing that the finite-sample distribution of the adaptive LASSO estimator deviates substantially from what is suggested by the oracle property, whereas the limiting distributions derived under moving-parameter asymptotics provide much more accurate approximations. Finally, we show that our results also extend to models with local-to-unit-root regressors and to predictive regressions with unit-root predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07204v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karsten Reichold, Ulrike Schneider</dc:creator>
    </item>
    <item>
      <title>Sampling effects on Lasso estimation of drift functions in high-dimensional diffusion processes</title>
      <link>https://arxiv.org/abs/2408.08638</link>
      <description>arXiv:2408.08638v4 Announce Type: replace 
Abstract: In this paper, we address high-dimensional parametric estimation of the drift function in diffusion models, specifically focusing on a $d$-dimensional ergodic diffusion process observed at discrete time points. We consider both a general linear form for the drift function and the particular case of the Ornstein-Uhlenbeck (OU) process. Assuming sparsity of the parameter vector, we examine the statistical behavior of the Lasso estimator for the unknown parameter. Our primary contribution is the proof of an oracle inequality for the Lasso estimator, which holds on the intersection of three specific sets defined for our analysis. We carefully control the probability of these sets, tackling the central challenge of our study. This approach allows us to derive error bounds for the $l_1$ and $l_2$ norms, assessing the performance of the proposed Lasso estimator. Our results demonstrate that, under certain conditions, the discretization error becomes negligible, enabling us to achieve the same optimal rate of convergence as if the continuous trajectory of the process were observed. We validate our theoretical findings through numerical experiments, which show that the Lasso estimator significantly outperforms the maximum likelihood estimator (MLE) in terms of support recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08638v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Francisco Pina, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>An Empirical Analysis of the Laplace and Neural Tangent Kernels</title>
      <link>https://arxiv.org/abs/2208.03761</link>
      <description>arXiv:2208.03761v2 Announce Type: replace-cross 
Abstract: The neural tangent kernel is a kernel function defined over the parameter distribution of an infinite width neural network. Despite the impracticality of this limit, the neural tangent kernel has allowed for a more direct study of neural networks and a gaze through the veil of their black box. More recently, it has been shown theoretically that the Laplace kernel and neural tangent kernel share the same reproducing kernel Hilbert space in the space of $\mathbb{S}^{d-1}$ alluding to their equivalence. In this work, we analyze the practical equivalence of the two kernels. We first do so by matching the kernels exactly and then by matching posteriors of a Gaussian process. Moreover, we analyze the kernels in $\mathbb{R}^d$ and experiment with them in the task of regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.03761v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>California State University's ScholarWorks, Summer (2022)</arxiv:journal_reference>
      <dc:creator>Ronaldas Paulius Lencevi\v{c}ius</dc:creator>
    </item>
    <item>
      <title>Multiscale Quantile Regression with Local Error Control</title>
      <link>https://arxiv.org/abs/2403.11356</link>
      <description>arXiv:2403.11356v3 Announce Type: replace-cross 
Abstract: For robust and efficient detection of change points, we introduce a novel methodology MUSCLE (Multiscale qUantile Segmentation Controlling Local Error) that partitions serial data into multiple segments, each sharing a common quantile. It leverages multiple tests for quantile changes over different scales and locations, and variational estimation. Unlike the often adopted global error control, MUSCLE focuses on local errors defined on individual segments, significantly improving detection power in finding change points. Meanwhile, due to the built-in model complexity penalty, it enjoys the finite sample guarantee that its false discovery rate (or the expected proportion of falsely detected change points) is upper bounded by its unique tuning parameter. Further, we obtain the consistency and the localization error rates in estimating change points, under mild signal-to-noise-ratio conditions. Both match (up to log factors) the minimax optimality results in the Gaussian setup. All theories hold under the only distributional assumption of serial independence. Incorporating the wavelet tree data structure, we develop an efficient dynamic programming algorithm for computing MUSCLE. Extensive simulations as well as real data applications in electrophysiology and geophysics demonstrate its competitiveness and effectiveness. An implementation via R package muscle is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11356v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Liu, Housen Li</dc:creator>
    </item>
    <item>
      <title>Adversarial Surrogate Risk Bounds for Binary Classification</title>
      <link>https://arxiv.org/abs/2506.09348</link>
      <description>arXiv:2506.09348v2 Announce Type: replace-cross 
Abstract: A central concern in classification is the vulnerability of machine learning models to adversarial attacks. Adversarial training is one of the most popular techniques for training robust classifiers, which involves minimizing an adversarial surrogate risk. Recent work has characterized the conditions under which any sequence minimizing the adversarial surrogate risk also minimizes the adversarial classification risk in the binary setting, a property known as adversarial consistency. However, these results do not address the rate at which the adversarial classification risk approaches its optimal value along such a sequence. This paper provides surrogate risk bounds that quantify that convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09348v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalie S. Frank</dc:creator>
    </item>
    <item>
      <title>Enjoying Non-linearity in Multinomial Logistic Bandits</title>
      <link>https://arxiv.org/abs/2507.05306</link>
      <description>arXiv:2507.05306v2 Announce Type: replace-cross 
Abstract: We consider the multinomial logistic bandit problem, a variant of where a learner interacts with an environment by selecting actions to maximize expected rewards based on probabilistic feedback from multiple possible outcomes. In the binary setting, recent work has focused on understanding the impact of the non-linearity of the logistic model (Faury et al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant $\kappa_* \geq 1$, that may be exponentially large in some problem parameters and which is captured by the derivative of the sigmoid function. It encapsulates the non-linearity and improves existing regret guarantees over $T$ rounds from $\smash{O(d\sqrt{T})}$ to $\smash{O(d\sqrt{T/\kappa_*})}$, where $d$ is the dimension of the parameter space. We extend their analysis to the multinomial logistic bandit framework, making it suitable for complex applications with more than two choices, such as reinforcement learning or recommender systems. To achieve this, we extend the definition of \( \kappa_* \) to the multinomial setting and propose an efficient algorithm that leverages the problem's non-linearity. Our method yields a problem-dependent regret bound of order $ \smash{\widetilde{\mathcal{O}}( R d \sqrt{{KT}/{\kappa_*}})} $, where $R$ is the norm of the vector of rewards and $K$ is the number of outcomes. This improves upon the best existing guarantees of order $ \smash{\widetilde{\mathcal{O}}( RdK \sqrt{T} )} $. Moreover, we provide a $\smash{ \Omega(Rd\sqrt{KT/\kappa_*})}$ lower-bound, showing that our algorithm is minimax-optimal and that our definition of $\kappa_*$ is optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05306v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Boudart (SIERRA), Pierre Gaillard (Thoth), Alessandro Rudi (PSL, DI-ENS, Inria)</dc:creator>
    </item>
    <item>
      <title>Bootstrap Diagnostic Tests</title>
      <link>https://arxiv.org/abs/2509.01351</link>
      <description>arXiv:2509.01351v2 Announce Type: replace-cross 
Abstract: Violation of the assumptions underlying classical (Gaussian) limit theory often yields unreliable statistical inference. This paper shows that the bootstrap can detect such violations by delivering simple and powerful diagnostic tests that (a) induce no pre-testing bias, (b) use the same critical values across applications, and (c) are consistent against deviations from asymptotic normality. The tests compare the conditional distribution of a bootstrap statistic with the Gaussian limit implied by valid specification and assess whether the resulting discrepancy is large enough to indicate failure of the asymptotic Gaussian approximation. The method is computationally straightforward and only requires a sample of i.i.d. draws of the bootstrap statistic. We derive sufficient conditions for the randomness in the data to mix with the randomness in the bootstrap repetitions in a way such that (a), (b) and (c) above hold. We demonstrate the practical relevance and broad applicability of bootstrap diagnostics by considering several scenarios where the asymptotic Gaussian approximation may fail, including weak instruments, non-stationarity, parameters on the boundary of the parameter space, infinite variance data and singular Jacobian in applications of the delta method. An illustration drawn from the empirical macroeconomic literature concludes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01351v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Cavaliere, Luca Fanelli, Iliyan Georgiev</dc:creator>
    </item>
    <item>
      <title>Closed-form $\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\ell_p$ bias</title>
      <link>https://arxiv.org/abs/2509.21181</link>
      <description>arXiv:2509.21181v2 Announce Type: replace-cross 
Abstract: For overparameterized linear regression with isotropic Gaussian design and minimum-$\ell_p$ interpolator $p\in(1,2]$, we give a unified, high-probability characterization for the scaling of the family of parameter norms $ \\{ \lVert \widehat{w_p} \rVert_r \\}_{r \in [1,p]} $ with sample size.
  We solve this basic, but unresolved question through a simple dual-ray analysis, which reveals a competition between a signal *spike* and a *bulk* of null coordinates in $X^\top Y$, yielding closed-form predictions for (i) a data-dependent transition $n_\star$ (the "elbow"), and (ii) a universal threshold $r_\star=2(p-1)$ that separates $\lVert \widehat{w_p} \rVert_r$'s which plateau from those that continue to grow with an explicit exponent.
  This unified solution resolves the scaling of *all* $\ell_r$ norms within the family $r\in [1,p]$ under $\ell_p$-biased interpolation, and explains in one picture which norms saturate and which increase as $n$ grows.
  We then study diagonal linear networks (DLNs) trained by gradient descent. By calibrating the initialization scale $\alpha$ to an effective $p_{\mathrm{eff}}(\alpha)$ via the DLN separable potential, we show empirically that DLNs inherit the same elbow/threshold laws, providing a predictive bridge between explicit and implicit bias.
  Given that many generalization proxies depend on $\lVert \widehat {w_p} \rVert_r$, our results suggest that their predictive power will depend sensitively on which $l_r$ norm is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21181v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuofeng Zhang, Ard Louis</dc:creator>
    </item>
  </channel>
</rss>
