<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2024 05:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Taxonomy of Systemic Risks from General-Purpose AI</title>
      <link>https://arxiv.org/abs/2412.07780</link>
      <description>arXiv:2412.07780v1 Announce Type: new 
Abstract: Through a systematic review of academic literature, we propose a taxonomy of systemic risks associated with artificial intelligence (AI), in particular general-purpose AI. Following the EU AI Act's definition, we consider systemic risks as large-scale threats that can affect entire societies or economies. Starting with an initial pool of 1,781 documents, we analyzed 86 selected papers to identify 13 categories of systemic risks and 50 contributing sources. Our findings reveal a complex landscape of potential threats, ranging from environmental harm and structural discrimination to governance failures and loss of control. Key sources of systemic risk emerge from knowledge gaps, challenges in recognizing harm, and the unpredictable trajectory of AI development. The taxonomy provides a snapshot of current academic literature on systemic risks. This paper contributes to AI safety research by providing a structured groundwork for understanding and addressing the potential large-scale negative societal impacts of general-purpose AI. The taxonomy can inform policymakers in risk prioritization and regulatory development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07780v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Risto Uuk, Carlos Ignacio Gutierrez, Daniel Guppy, Lode Lauwaert, Atoosa Kasirzadeh, Lucia Velasco, Peter Slattery, Carina Prunkl</dc:creator>
    </item>
    <item>
      <title>Trustworthy artificial intelligence in the energy sector: Landscape analysis and evaluation framework</title>
      <link>https://arxiv.org/abs/2412.07782</link>
      <description>arXiv:2412.07782v1 Announce Type: new 
Abstract: The present study aims to evaluate the current fuzzy landscape of Trustworthy AI (TAI) within the European Union (EU), with a specific focus on the energy sector. The analysis encompasses legal frameworks, directives, initiatives, and standards like the AI Ethics Guidelines for Trustworthy AI (EGTAI), the Assessment List for Trustworthy AI (ALTAI), the AI act, and relevant CEN-CENELEC standardization efforts, as well as EU-funded projects such as AI4EU and SHERPA. Subsequently, we introduce a new TAI application framework, called E-TAI, tailored for energy applications, including smart grid and smart building systems. This framework draws inspiration from EGTAI but is customized for AI systems in the energy domain. It is designed for stakeholders in electrical power and energy systems (EPES), including researchers, developers, and energy experts linked to transmission system operators, distribution system operators, utilities, and aggregators. These stakeholders can utilize E-TAI to develop and evaluate AI services for the energy sector with a focus on ensuring trustworthiness throughout their development and iterative assessment processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07782v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sotiris Pelekis, Evangelos Karakolis, George Lampropoulos, Spiros Mouzakitis, Ourania Markaki, Christos Ntanos, Dimitris Askounis</dc:creator>
    </item>
    <item>
      <title>Digital Democracy in the Age of Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2412.07791</link>
      <description>arXiv:2412.07791v1 Announce Type: new 
Abstract: This chapter explores the influence of Artificial Intelligence (AI) on digital democracy, focusing on four main areas: citizenship, participation, representation, and the public sphere. It traces the evolution from electronic to virtual and network democracy, underscoring how each stage has broadened democratic engagement through technology. Focusing on digital citizenship, the chapter examines how AI can improve online engagement and promote ethical behaviour while posing privacy risks and fostering identity stereotyping. Regarding political participation, it highlights AI's dual role in mobilising civic actions and spreading misinformation. Regarding representation, AI's involvement in electoral processes can enhance voter registration, e-voting, and the efficiency of result tabulation but raises concerns regarding privacy and public trust. Also, AI's predictive capabilities shift the dynamics of political competition, posing ethical questions about manipulation and the legitimacy of democracy. Finally, the chapter examines how integrating AI and digital technologies can facilitate democratic political advocacy and personalised communication. However, this also comes with higher risks of misinformation and targeted propaganda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07791v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Claudio Novelli, Giulia Sandri</dc:creator>
    </item>
    <item>
      <title>A large language model-based approach to quantifying the effects of social determinants in liver transplant decisions</title>
      <link>https://arxiv.org/abs/2412.07924</link>
      <description>arXiv:2412.07924v1 Announce Type: new 
Abstract: Patient life circumstances, including social determinants of health (SDOH), shape both health outcomes and care access, contributing to persistent disparities across gender, race, and socioeconomic status. Liver transplantation exemplifies these challenges, requiring complex eligibility and allocation decisions where SDOH directly influence patient evaluation. We developed an artificial intelligence (AI)-driven framework to analyze how broadly defined SDOH -- encompassing both traditional social determinants and transplantation-related psychosocial factors -- influence patient care trajectories. Using large language models, we extracted 23 SDOH factors related to patient eligibility for liver transplantation from psychosocial evaluation notes. These SDOH ``snapshots'' significantly improve prediction of patient progression through transplantation evaluation stages and help explain liver transplantation decisions including the recommendation based on psychosocial evaluation and the listing of a patient for a liver transplantation. Our analysis helps identify patterns of SDOH prevalence across demographics that help explain racial disparities in liver transplantation decisions. We highlight specific unmet patient needs, which, if addressed, could improve the equity and efficacy of transplant care. While developed for liver transplantation, this systematic approach to analyzing previously unstructured information about patient circumstances and clinical decision-making could inform understanding of care decisions and disparities across various medical domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07924v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emily Robitschek, Asal Bastani, Kathryn Horwath, Savyon Sordean, Mark J. Pletcher, Jennifer C. Lai, Sergio Galletta, Elliott Ash, Jin Ge, Irene Y. Chen</dc:creator>
    </item>
    <item>
      <title>From Division to Unity: A Large-Scale Study on the Emergence of Computational Social Science, 1990-2021</title>
      <link>https://arxiv.org/abs/2412.08087</link>
      <description>arXiv:2412.08087v1 Announce Type: new 
Abstract: We present a comprehensive study on the emergence of Computational Social Science (CSS) - an interdisciplinary field leveraging computational methods to address social science questions - and its impact on adjacent social sciences. We trained a robust CSS classifier using papers from CSS-focused venues and applied it to 11 million papers spanning 1990 to 2021. Our analysis yielded three key findings. First, there were two critical inflections in the rise of CSS. The first occurred around 2005 when psychology, politics, and sociology began engaging with CSS. The second emerged in approximately 2014 when economics finally joined the trend. Sociology is currently the most engaged with CSS. Second, using the density of yearly knowledge embeddings constructed by advanced transformer models, we observed that CSS initially lacked a cohesive identity. Between 2005 and 2014, however, it began to form a distinct cluster, creating boundaries between CSS and other social sciences, particularly in politics and sociology. After 2014, these boundaries faded, and CSS increasingly blended with the social sciences. Third, shared data-driven methods homogenized CSS papers across disciplines, with politics and economics showing the most alignment, likely due to the combined influence of CSS and causal identification. Nevertheless, non-CSS papers in sociology, psychology, and politics became more divergent. Taken together, these findings highlight the dynamics of division and unity as new disciplines emerge within existing knowledge landscapes. A live demo of CSS evolution can be found at https://evolution-css.netlify.app/</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08087v1</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Honglin Bao, Jiawei Zhang, Mingxuan Cao, James A. Evans</dc:creator>
    </item>
    <item>
      <title>Time, bits, and nickel: Managing digital and analog continuity</title>
      <link>https://arxiv.org/abs/2412.07790</link>
      <description>arXiv:2412.07790v1 Announce Type: cross 
Abstract: In 1998, the Getty Center hosted the ''Time and Bits: Managing Digital Continuity'' conference, gathering the founders and thinkers of two San Francisco non-profit organizations interested in long-term thinking and archiving: the Internet Archive and the Long Now Foundation. This chapter proposes to discuss two different ways of archiving through time, in digital and analog formats, for virtual web contents and physical paper-based ones. It explores various types of archiving methods and tools and the management challenges they raise, in terms of time and space, but also innovation, maintenance, and ''continuity''. It depicts two distinct visions of the future of archiving which nonetheless converge in their mission of safeguarding, sharing, and giving access to information and knowledge for the decades and centuries to come.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07790v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.36253/979-12-215-0413-2.14</arxiv:DOI>
      <arxiv:journal_reference>Exploring the Archived Web during a Highly Transformative Age, 138 (1), Firenze University Press, 2024, Proceedings e report, 979-12-215-0412-5</arxiv:journal_reference>
      <dc:creator>Julie Momm\'eja (IDEA, CREW)</dc:creator>
    </item>
    <item>
      <title>Homophily Within and Across Groups</title>
      <link>https://arxiv.org/abs/2412.07901</link>
      <description>arXiv:2412.07901v1 Announce Type: cross 
Abstract: Traditional social network analysis often models homophily--the tendency of similar individuals to form connections--using a single parameter, overlooking finer biases within and across groups. We present an exponential family model that integrates both local and global homophily, distinguishing between strong homophily within tightly knit cliques and weak homophily spanning broader community interactions. By modeling these forms of homophily through a maximum entropy approach and deriving the network behavior under percolation, we show how higher-order assortative mixing influences network dynamics. Our framework is useful for decomposing homophily into finer levels and studying the spread of information and diseases, influence dynamics, and innovation diffusion. We demonstrate that the interaction between different levels of homophily results in complex percolation thresholds. We tested our model on various datasets with distinct homophily patterns, showcasing its applicability. These homophilic connections significantly affect the effectiveness of intervention and mitigation strategies. Hence, our findings have important implications for improving public health measures, understanding information dissemination on social media, and optimizing intervention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07901v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abbas K. Rizi, Riccardo Michielan, Clara Stegehuis, Mikko Kivel\"a</dc:creator>
    </item>
    <item>
      <title>From Lived Experience to Insight: Unpacking the Psychological Risks of Using AI Conversational Agents</title>
      <link>https://arxiv.org/abs/2412.07951</link>
      <description>arXiv:2412.07951v1 Announce Type: cross 
Abstract: Recent gain in popularity of AI conversational agents has led to their increased use for improving productivity and supporting well-being. While previous research has aimed to understand the risks associated with interactions with AI conversational agents, these studies often fall short in capturing the lived experiences. Additionally, psychological risks have often been presented as a sub-category within broader AI-related risks in past taxonomy works, leading to under-representation of the impact of psychological risks of AI use. To address these challenges, our work presents a novel risk taxonomy focusing on psychological risks of using AI gathered through lived experience of individuals. We employed a mixed-method approach, involving a comprehensive survey with 283 individuals with lived mental health experience and workshops involving lived experience experts to develop a psychological risk taxonomy. Our taxonomy features 19 AI behaviors, 21 negative psychological impacts, and 15 contexts related to individuals. Additionally, we propose a novel multi-path vignette based framework for understanding the complex interplay between AI behaviors, psychological impacts, and individual user contexts. Finally, based on the feedback obtained from the workshop sessions, we present design recommendations for developing safer and more robust AI agents. Our work offers an in-depth understanding of the psychological risks associated with AI conversational agents and provides actionable recommendations for policymakers, researchers, and developers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07951v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohit Chandra, Suchismita Naik, Denae Ford, Ebele Okoli, Munmun De Choudhury, Mahsa Ershadi, Gonzalo Ramos, Javier Hernandez, Ananya Bhattacharjee, Shahed Warreth, Jina Suh</dc:creator>
    </item>
    <item>
      <title>Machines of Meaning</title>
      <link>https://arxiv.org/abs/2412.07975</link>
      <description>arXiv:2412.07975v1 Announce Type: cross 
Abstract: One goal of Artificial Intelligence is to learn meaningful representations for natural language expressions, but what this entails is not always clear. A variety of new linguistic behaviours present themselves embodied as computers, enhanced humans, and collectives with various kinds of integration and communication. But to measure and understand the behaviours generated by such systems, we must clarify the language we use to talk about them. Computational models are often confused with the phenomena they try to model and shallow metaphors are used as justifications for (or to hype) the success of computational techniques on many tasks related to natural language; thus implying their progress toward human-level machine intelligence without ever clarifying what that means.
  This paper discusses the challenges in the specification of "machines of meaning", machines capable of acquiring meaningful semantics from natural language in order to achieve their goals. We characterize "meaning" in a computational setting, while highlighting the need for detachment from anthropocentrism in the study of the behaviour of machines of meaning. The pressing need to analyse AI risks and ethics requires a proper measurement of its capabilities which cannot be productively studied and explained while using ambiguous language. We propose a view of "meaning" to facilitate the discourse around approaches such as neural language models and help broaden the research perspectives for technology that facilitates dialogues between humans and machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07975v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Nunes, Luis Antunes</dc:creator>
    </item>
    <item>
      <title>A Systematic Literature Review on the NIS2 Directive</title>
      <link>https://arxiv.org/abs/2412.08084</link>
      <description>arXiv:2412.08084v1 Announce Type: cross 
Abstract: A directive known as NIS2 was enacted in the European Union (EU) in late 2022. It deals particularly with European critical infrastructures, enlarging their scope substantially from an older directive that only considered the energy and transport sectors as critical. The directive's focus is on cyber security of critical infrastructures, although together with other new EU laws it expands to other security domains as well. Given the importance of the directive and most of all the importance of critical infrastructures, the paper presents a systematic literature review on academic research addressing the NIS2 directive either explicitly or implicitly. According to the review, existing research has often framed and discussed the directive with the EU's other cyber security laws. In addition, existing research has often operated in numerous contextual areas, including industrial control systems, telecommunications, the energy and water sectors, and infrastructures for information sharing and situational awareness. Despite the large scope of existing research, the review reveals noteworthy research gaps and worthwhile topics to examine in further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08084v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jukka Ruohonen</dc:creator>
    </item>
    <item>
      <title>Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim Prioritization for Human Fact-checkers</title>
      <link>https://arxiv.org/abs/2412.08185</link>
      <description>arXiv:2412.08185v1 Announce Type: cross 
Abstract: Given the massive volume of potentially false claims circulating online, claim prioritization is essential in allocating limited human resources available for fact-checking. In this study, we perceive claim prioritization as an information retrieval (IR) task: just as multidimensional IR relevance, with many factors influencing which search results a user deems relevant, checkworthiness is also multi-faceted, subjective, and even personal, with many factors influencing how fact-checkers triage and select which claims to check. Our study investigates both the multidimensional nature of checkworthiness and effective tool support to assist fact-checkers in claim prioritization. Methodologically, we pursue Research through Design combined with mixed-method evaluation. We develop an AI-assisted claim prioritization prototype as a probe to explore how fact-checkers use multidimensional checkworthiness factors in claim prioritization, simultaneously probing fact-checker needs while also exploring the design space to meet those needs.
  Our study with 16 professional fact-checkers investigates: 1) how participants assessed the relative importance of different checkworthy dimensions and apply different priorities in claim selection; 2) how they created customized GPT-based search filters and the corresponding benefits and limitations; and 3) their overall user experiences with our prototype. Our work makes a conceptual contribution between multidimensional IR relevance and fact-checking checkworthiness, with findings demonstrating the value of corresponding tooling support. Specifically, we uncovered a hierarchical prioritization strategy fact-checkers implicitly use, revealing an underexplored aspect of their workflow, with actionable design recommendations for improving claim triage across multi-dimensional checkworthiness and tailoring this process with LLM integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08185v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Houjiang Liu, Jacek Gwizdka, Matthew Lease</dc:creator>
    </item>
    <item>
      <title>Machine Learning Information Retrieval and Summarisation to Support Systematic Review on Outcomes Based Contracting</title>
      <link>https://arxiv.org/abs/2412.08578</link>
      <description>arXiv:2412.08578v1 Announce Type: cross 
Abstract: As academic literature proliferates, traditional review methods are increasingly challenged by the sheer volume and diversity of available research. This article presents a study that aims to address these challenges by enhancing the efficiency and scope of systematic reviews in the social sciences through advanced machine learning (ML) and natural language processing (NLP) tools. In particular, we focus on automating stages within the systematic reviewing process that are time-intensive and repetitive for human annotators and which lend themselves to immediate scalability through tools such as information retrieval and summarisation guided by expert advice. The article concludes with a summary of lessons learnt regarding the integrated approach towards systematic reviews and future directions for improvement, including explainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08578v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Munire Bilal, Zheng Fang, Miguel Arana-Catania, Felix-Anselm van Lier, Juliana Outes Velarde, Harry Bregazzi, Eleanor Carter, Mara Airoldi, Rob Procter</dc:creator>
    </item>
    <item>
      <title>Competition and Diversity in Generative AI</title>
      <link>https://arxiv.org/abs/2412.08610</link>
      <description>arXiv:2412.08610v1 Announce Type: cross 
Abstract: Recent evidence suggests that the use of generative artificial intelligence reduces the diversity of content produced. In this work, we develop a game-theoretic model to explore the downstream consequences of content homogeneity when producers use generative AI to compete with one another. At equilibrium, players indeed produce content that is less diverse than optimal. However, stronger competition mitigates homogeneity and induces more diverse production. Perhaps more surprisingly, we show that a generative AI model that performs well in isolation (i.e., according to a benchmark) may fail to do so when faced with competition, and vice versa. We validate our results empirically by using language models to play Scattergories, a word game in which players are rewarded for producing answers that are both correct and unique. We discuss how the interplay between competition and homogeneity has implications for the development, evaluation, and use of generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08610v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manish Raghavan</dc:creator>
    </item>
    <item>
      <title>Comparing diversity, negativity, and stereotypes in Chinese-language AI technologies: a case study on Baidu, Ernie and Qwen</title>
      <link>https://arxiv.org/abs/2408.15696</link>
      <description>arXiv:2408.15696v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) and search engines have the potential to perpetuate biases and stereotypes by amplifying existing prejudices in their training data and algorithmic processes, thereby influencing public perception and decision-making. While most work has focused on Western-centric AI technologies, we study Chinese-based tools by investigating social biases embedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie and Qwen. Leveraging a dataset of 240 social groups across 13 categories describing Chinese society, we collect over 30k views encoded in the aforementioned tools by prompting them for candidate words describing such groups. We find that language models exhibit a larger variety of embedded views compared to the search engine, although Baidu and Qwen generate negative content more often than Ernie. We also find a moderate prevalence of stereotypes embedded in the language models, many of which potentially promote offensive and derogatory views. Our work highlights the importance of promoting fairness and inclusivity in AI technologies with a global perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15696v3</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geng Liu, Carlo Alberto Bono, Francesco Pierri</dc:creator>
    </item>
    <item>
      <title>The Method of Critical AI Studies, A Propaedeutic</title>
      <link>https://arxiv.org/abs/2411.18833</link>
      <description>arXiv:2411.18833v2 Announce Type: replace 
Abstract: We outline some common methodological issues in the field of critical AI studies, including a tendency to overestimate the explanatory power of individual samples (the benchmark casuistry), a dependency on theoretical frameworks derived from earlier conceptualizations of computation (the black box casuistry), and a preoccupation with a cause-and-effect model of algorithmic harm (the stack casuistry). In the face of these issues, we call for, and point towards, a future set of methodologies that might take into account existing strengths in the humanistic close analysis of cultural objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18833v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Offert, Ranjodh Singh Dhaliwal</dc:creator>
    </item>
    <item>
      <title>Topic Classification of Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment</title>
      <link>https://arxiv.org/abs/2405.12910</link>
      <description>arXiv:2405.12910v2 Announce Type: replace-cross 
Abstract: This paper addresses a critical gap in legal analytics by developing and applying a novel taxonomy for topic classification of summary judgment cases in the United Kingdom. Using a curated dataset of summary judgment cases, we use the Large Language Model Claude 3 Opus to explore functional topics and trends. We find that Claude 3 Opus correctly classified the topic with an accuracy of 87.13% and an F1 score of 0.87. The analysis reveals distinct patterns in the application of summary judgments across various legal domains. As case law in the United Kingdom is not originally labelled with keywords or a topic filtering option, the findings not only refine our understanding of the thematic underpinnings of summary judgments but also illustrate the potential of combining traditional and AI-driven approaches in legal classification. Therefore, this paper provides a new and general taxonomy for UK law. The implications of this work serve as a foundation for further research and policy discussions in the field of judicial administration and computational legal research methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12910v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Holli Sargeant, Ahmed Izzidien, Felix Steffek</dc:creator>
    </item>
    <item>
      <title>Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs</title>
      <link>https://arxiv.org/abs/2409.16490</link>
      <description>arXiv:2409.16490v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have led to the development of artificial intelligence (AI)-powered tutoring chatbots, showing promise in providing broad access to high-quality personalized education. Existing works have studied how to make LLMs follow tutoring principles, but have not studied broader uses of LLMs for supporting tutoring. Up until now, tracing student knowledge and analyzing misconceptions has been difficult and time-consuming to implement for open-ended dialogue tutoring. In this work, we investigate whether LLMs can be supportive of this task: we first use LLM prompting methods to identify the knowledge components/skills involved in each dialogue turn, i.e., a tutor utterance posing a task or a student utterance that responds to it. We also evaluate whether the student responds correctly to the tutor and verify the LLM's accuracy using human expert annotations. We then apply a range of knowledge tracing (KT) methods on the resulting labeled data to track student knowledge levels over an entire dialogue. We conduct experiments on two tutoring dialogue datasets, and show that a novel yet simple LLM-based method, LLMKT, significantly outperforms existing KT methods in predicting student response correctness in dialogues. We perform extensive qualitative analyses to highlight the challenges in dialogueKT and outline multiple avenues for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16490v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Scarlatos, Ryan S. Baker, Andrew Lan</dc:creator>
    </item>
    <item>
      <title>Shaping AI's Impact on Billions of Lives</title>
      <link>https://arxiv.org/abs/2412.02730</link>
      <description>arXiv:2412.02730v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI), like any transformative technology, has the potential to be a double-edged sword, leading either toward significant advancements or detrimental outcomes for society as a whole. As is often the case when it comes to widely-used technologies in market economies (e.g., cars and semiconductor chips), commercial interest tends to be the predominant guiding factor. The AI community is at risk of becoming polarized to either take a laissez-faire attitude toward AI development, or to call for government overregulation. Between these two poles we argue for the community of AI practitioners to consciously and proactively work for the common good. This paper offers a blueprint for a new type of innovation infrastructure including 18 concrete milestones to guide AI research in that direction. Our view is that we are still in the early days of practical AI, and focused efforts by practitioners, policymakers, and other stakeholders can still maximize the upsides of AI and minimize its downsides.
  We talked to luminaries such as recent Nobelist John Jumper on science, President Barack Obama on governance, former UN Ambassador and former National Security Advisor Susan Rice on security, philanthropist Eric Schmidt on several topics, and science fiction novelist Neal Stephenson on entertainment. This ongoing dialogue and collaborative effort has produced a comprehensive, realistic view of what the actual impact of AI could be, from a diverse assembly of thinkers with deep understanding of this technology and these domains. From these exchanges, five recurring guidelines emerged, which form the cornerstone of a framework for beginning to harness AI in service of the public good. They not only guide our efforts in discovery but also shape our approach to deploying this transformative technology responsibly and ethically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02730v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mariano-Florentino Cu\'ellar, Jeff Dean, Finale Doshi-Velez, John Hennessy, Andy Konwinski, Sanmi Koyejo, Pelonomi Moiloa, Emma Pierson, David Patterson</dc:creator>
    </item>
  </channel>
</rss>
