<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 05:02:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Survey on Food Ingredient Substitutions</title>
      <link>https://arxiv.org/abs/2501.01958</link>
      <description>arXiv:2501.01958v1 Announce Type: new 
Abstract: Diet plays a crucial role in managing chronic conditions and overall well-being. As people become more selective about their food choices, finding recipes that meet dietary needs is important. Ingredient substitution is key to adapting recipes for dietary restrictions, allergies, and availability constraints. However, identifying suitable substitutions is challenging as it requires analyzing the flavor, functionality, and health suitability of ingredients. With the advancement of AI, researchers have explored computational approaches to address ingredient substitution. This survey paper provides a comprehensive overview of the research in this area, focusing on five key aspects: (i) datasets and data sources used to support ingredient substitution research; (ii) techniques and approaches applied to solve substitution problems (iii) contextual information of ingredients considered, such as nutritional content, flavor, and pairing potential; (iv) applications for which substitution models have been developed, including dietary restrictions, constraints, and missing ingredients; (v) safety and transparency of substitution models, focusing on user trust and health concerns. The survey also highlights promising directions for future research, such as integrating neuro-symbolic techniques for deep learning and utilizing knowledge graphs for improved reasoning, aiming to guide advancements in food computation and ingredient substitution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01958v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hyunwook Kim, Revathy Venkataramanan, Amit Sheth</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Framework to Operationalize Social Stereotypes for Responsible AI Evaluations</title>
      <link>https://arxiv.org/abs/2501.02074</link>
      <description>arXiv:2501.02074v1 Announce Type: new 
Abstract: Societal stereotypes are at the center of a myriad of responsible AI interventions targeted at reducing the generation and propagation of potentially harmful outcomes. While these efforts are much needed, they tend to be fragmented and often address different parts of the issue without taking in a unified or holistic approach about social stereotypes and how they impact various parts of the machine learning pipeline. As a result, it fails to capitalize on the underlying mechanisms that are common across different types of stereotypes, and to anchor on particular aspects that are relevant in certain cases. In this paper, we draw on social psychological research, and build on NLP data and methods, to propose a unified framework to operationalize stereotypes in generative AI evaluations. Our framework identifies key components of stereotypes that are crucial in AI evaluation, including the target group, associated attribute, relationship characteristics, perceiving group, and relevant context. We also provide considerations and recommendations for its responsible use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02074v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aida Davani, Sunipa Dev, H\'ector P\'erez-Urbina, Vinodkumar Prabhakaran</dc:creator>
    </item>
    <item>
      <title>A hybrid marketplace of ideas</title>
      <link>https://arxiv.org/abs/2501.02132</link>
      <description>arXiv:2501.02132v1 Announce Type: new 
Abstract: The convergence of humans and artificial intelligence systems introduces new dynamics into the cultural and intellectual landscape. Complementing emerging cultural evolution concepts such as machine culture, AI agents represent a significant technosociological development, particularly within the anthropological study of Web3 as a community focused on decentralization through blockchain. Despite their growing presence, the cultural significance of AI agents remains largely unexplored in academic literature. We argue that, within the context of Web3, these agents challenge traditional notions of participation and influence in public discourse, creating a hybrid marketplace of ideas, a conceptual space where human and AI generated ideas coexist and compete for attention. We examine the current state of AI agents in idea generation, propagation, and engagement, positioning their role as cultural agents through the lens of memetics and encouraging further inquiry into their cultural and societal impact. Additionally, we address the implications of this paradigm for privacy, intellectual property, and governance, highlighting the societal and legal challenges of integrating AI agents into the hybrid marketplace of ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02132v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomer Jordi Chaffer, Dontrail Cotlage, Justin Goldston</dc:creator>
    </item>
    <item>
      <title>The Integration of Blockchain and Artificial Intelligence for Secure Healthcare Systems</title>
      <link>https://arxiv.org/abs/2501.02169</link>
      <description>arXiv:2501.02169v1 Announce Type: new 
Abstract: Verisign reported a 125 percent increase in data breaches within the healthcare sector in the United States during 2022, with 18.2 million patient records being impacted. Growing healthcare data volumes and diversification mean that medical information is becoming more valuable. Many Health Centers use various technologies to ease the classification, storage, and exchange of big data. This use can also make the health data of the users at risk and vulnerable. AI and blockchain are among the leading technologies at hand. With AI, data-driven operations and big data efficiency have been improved with respect to traditional techniques. Due to its potential to bring about improvements in health services and lower medical costs, this AI technology is regularly used in healthcare. Blockchain helps protect transactions on sharing information and private privacy as long as the exchange of knowledge is that of the standard. The objective of this analysis is to investigate the research and unique contributions since 2008 regarding blockchain-integrated AI and healthcare systems. The work sheds light on applied AI-based healthcare schemes with machine, ballistic, and acrylic learning and disparate blockchain structures. The use of technology in order to ensure patient data security and manage medical information effectively in healthcare settings offers a highly successful position for both healthcare providers and patients. From 2018 to 2021, the best year was 2021 to grow, enhancing everything to examine the download of the device and the counting of Google Academies, for which the joining perspective was borrowed; local research experts were asked, identified articles in recent years, and read reviews of large research grants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02169v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Umar Safdar, Simon Gabrael</dc:creator>
    </item>
    <item>
      <title>KD-MSLRT: Lightweight Sign Language Recognition Model Based on Mediapipe and 3D to 1D Knowledge Distillation</title>
      <link>https://arxiv.org/abs/2501.02321</link>
      <description>arXiv:2501.02321v1 Announce Type: new 
Abstract: Artificial intelligence has achieved notable results in sign language recognition and translation. However, relatively few efforts have been made to significantly improve the quality of life for the 72 million hearing-impaired people worldwide. Sign language translation models, relying on video inputs, involves with large parameter sizes, making it time-consuming and computationally intensive to be deployed. This directly contributes to the scarcity of human-centered technology in this field. Additionally, the lack of datasets in sign language translation hampers research progress in this area. To address these, we first propose a cross-modal multi-knowledge distillation technique from 3D to 1D and a novel end-to-end pre-training text correction framework. Compared to other pre-trained models, our framework achieves significant advancements in correcting text output errors. Our model achieves a decrease in Word Error Rate (WER) of at least 1.4% on PHOENIX14 and PHOENIX14T datasets compared to the state-of-the-art CorrNet. Additionally, the TensorFlow Lite (TFLite) quantized model size is reduced to 12.93 MB, making it the smallest, fastest, and most accurate model to date. We have also collected and released extensive Chinese sign language datasets, and developed a specialized training vocabulary. To address the lack of research on data augmentation for landmark data, we have designed comparative experiments on various augmentation methods. Moreover, we performed a simulated deployment and prediction of our model on Intel platform CPUs and assessed the feasibility of deploying the model on other platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02321v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>ulong Li, Bolin Ren, Ke Hu, Changyuan Liu, Zhengyong Jiang, Kang Dang, Jionglong Su</dc:creator>
    </item>
    <item>
      <title>Experiences and attitudes toward working remotely from home in a time of pandemic: A snapshot from a New Zealand-based online survey</title>
      <link>https://arxiv.org/abs/2501.02418</link>
      <description>arXiv:2501.02418v1 Announce Type: new 
Abstract: Due to the Covid-19 pandemic, employees from around the world were compelled to work remotely from home and, in many cases, without much preparation. A substantial body of international research has been conducted on the experiences and attitudes of remote workers as well as the implications of this phenomenon for organisations. While New Zealand research evidence is growing, most existing inquiry is qualitative. This paper provides a quantitative snapshot of remote working using survey data from participants whose jobs can be done from home (n=415). Data collection took place when the country was facing Covid-related measures.
  Based on descriptive and inferential statistics, it was found that, not only was remote working common, but that hybrid working arrangements were also more prevalent. While half of the participants wanted to work from home more frequently, age, but not gender, was significantly associated with this preference. Another relevant finding is that perceived change in the workplace culture due to flexible work arrangements was significantly associated with preference for working remotely more often. Finally, the most common perceived barriers to working from home were slow internet speed, the need to attend face-to-face meetings, and limited space at home to work. The implications of the results are discussed and some directions for future research are proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02418v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.24135/nzjer.v48i1.163</arxiv:DOI>
      <arxiv:journal_reference>New Zealand Journal of Employment Relations, 2024, volume 48, issue 1</arxiv:journal_reference>
      <dc:creator>Edgar Pacheco</dc:creator>
    </item>
    <item>
      <title>Towards New Benchmark for AI Alignment &amp; Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI</title>
      <link>https://arxiv.org/abs/2501.02531</link>
      <description>arXiv:2501.02531v1 Announce Type: new 
Abstract: With the expansion of neural networks, such as large language models, humanity is exponentially heading towards superintelligence. As various AI systems are increasingly integrated into the fabric of societies-through recommending values, devising creative solutions, and making decisions-it becomes critical to assess how these AI systems impact humans in the long run. This research aims to contribute towards establishing a benchmark for evaluating the sentiment of various Large Language Models in socially importan issues. The methodology adopted was a Likert scale survey. Seven LLMs, including GPT-4 and Bard, were analyzed and compared against sentiment data from three independent human sample populations. Temporal variations in sentiment were also evaluated over three consecutive days. The results highlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI, whereas Bard was leaning towards the neutral sentiment. The human samples, contrastingly, showed a lower average sentiment of 2.97. The temporal comparison revealed differences in sentiment evolution between LLMs in three days, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect of potential conflicts of interest and bias possibilities in LLMs' sentiment formation. Results indicate that LLMs, akin to human cognitive processes, could potentially develop unique sentiments and subtly influence societies' perceptions towards various opinions formed within the LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02531v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ljubisa Bojic, Dylan Seychell, Milan Cabarkapa</dc:creator>
    </item>
    <item>
      <title>A system for objectively measuring behavior and the environment to support large-scale studies on childhood obesity</title>
      <link>https://arxiv.org/abs/2501.02560</link>
      <description>arXiv:2501.02560v1 Announce Type: new 
Abstract: Advances in IoT technologies combined with new algorithms have enabled the collection and processing of high-rate multi-source data streams that quantify human behavior in a fine-grained level and can lead to deeper insights on individual behaviors as well as on the interplay between behaviors and the environment. In this paper, we present an integrated system that collects and extracts multiple behavioral and environmental indicators, aiming at improving public health policies for tackling obesity. Data collection takes place using passive methods based on smartphone and smartwatch applications that require minimal interaction with the user. Our goal is to present a detailed account of the design principles, the implementation processes, and the evaluation of integrated algorithms, especially given the challenges we faced, in particular (a) integrating multiple technologies, algorithms, and components under a single, unified system, and (b) large scale (big data) requirements. We also present evaluation results of the algorithms on datasets (public for most cases) such as an absolute error of 8-9 steps when counting steps, 0.86 F1-score for detecting visited locations, and an error of less than 12 mins for gross sleep time. Finally, we also briefly present studies that have been materialized using our system, thus demonstrating its potential value to public authorities and individual researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02560v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/JBHI.2025.3526794</arxiv:DOI>
      <dc:creator>Vasileios Papapanagiotou, Ioannis Sarafis, Leonidas Alagialoglou, Vasileios Gkolemis, Christos Diou, Anastasios Delopoulos</dc:creator>
    </item>
    <item>
      <title>Putnam's Critical and Explanatory Tendencies Interpreted from a Machine Learning Perspective</title>
      <link>https://arxiv.org/abs/2501.03026</link>
      <description>arXiv:2501.03026v1 Announce Type: new 
Abstract: Making sense of theory choice in normal and across extraordinary science is central to philosophy of science. The emergence of machine learning models has the potential to act as a wrench in the gears of current debates. In this paper, I will attempt to reconstruct the main movements that lead to and came out of Putnam's critical and explanatory tendency distinction, argue for the biconditional necessity of the tendencies, and conceptualize that wrench through a machine learning interpretation of my claim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03026v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheldon Z. Soudin</dc:creator>
    </item>
    <item>
      <title>Societal Adaptation to AI Human-Labor Automation</title>
      <link>https://arxiv.org/abs/2501.03092</link>
      <description>arXiv:2501.03092v1 Announce Type: new 
Abstract: AI is transforming human labor at an unprecedented pace - improving 10$\times$ per year in training effectiveness. This paper analyzes how society can adapt to AI-driven human-labor automation (HLA), using Bernardi et al.'s societal adaptation framework. Drawing on literature from general automation economics and recent AI developments, the paper develops a "threat model." The threat model is centered on mass unemployment and its socioeconomic consequences, and assumes a non-binary scenario between full AGI takeover and swift job creation. The analysis explores both "capability-modifying interventions" (CMIs) that shape how AI develops, and "adaptation interventions" (ADIs) that help society adjust. Key interventions analyzed include steering AI development toward human-complementing capabilities, implementing human-in-the-loop requirements, taxation of automation, comprehensive reorientation of education, and both material and social substitutes for work. While CMIs can slow the transition in the short-term, significant automation is inevitable. Long-term adaptation requires ADIs - from education reform to providing substitutes for both the income and psychological benefits of work. Success depends on upfront preparation through mechanisms like "if-then commitments", and crafting flexible and accurate regulation that avoids misspecification. This structured analysis of HLA interventions and their potential effects and challenges aims to guide holistic AI governance strategies for the AI economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03092v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuval Rymon</dc:creator>
    </item>
    <item>
      <title>Self-directed online information search can affect policy attitudes: a randomized encouragement design with digital behavioral data</title>
      <link>https://arxiv.org/abs/2501.03097</link>
      <description>arXiv:2501.03097v1 Announce Type: new 
Abstract: The abundance of information sources in our digital environment makes it difficult to study how such information shapes individuals' attitudes towards current policies. Our study investigates how self-directed online search in a naturalistic setting through three randomized controlled experiments with 791 German participants on three topical policy issues: basic child support, renewable energy transition, and cannabis legalization. Participants' online browsing was passively tracked, and changes in their attitudes were measured. By encouraging participants to seek online information, this study enhances ecological validity compared to traditional experiments that expose subjects to predetermined content. Significant attitude shifts were observed for child support and cannabis legalization, but not for renewable energy transition. Some findings suggest that the specificity and granularity of policy topics may affect whether and how online information shapes political views, providing insights into the nuanced impact of online information seeking on policy attitudes. By exploring participant's searches and visits, we depict the behavioral patterns that emerge on from our encouragement. Our experimental approach lays the groundwork for future research to advance understanding of the media effect within the dynamic online information landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03097v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Celina Kacperski, Roberto Ulloa, Peter Selb, Andreas Spitz, Denis Bonnay, Juhi Kulshrestha</dc:creator>
    </item>
    <item>
      <title>Early Perspectives on the Digital Europe Programme</title>
      <link>https://arxiv.org/abs/2501.03098</link>
      <description>arXiv:2501.03098v1 Announce Type: new 
Abstract: A new Digital Europe Programme (DEP), a funding instrument for development and innovation, was established in the European Union (EU) in 2021. The paper makes an empirical inquiry into the projects funded through the DEP. According to the results, the projects align well with the DEP's strategic focus on cyber security, artificial intelligence, high-performance computing, innovation hubs, small- and medium-sized enterprises, and education. Most of the projects have received an equal amount of national and EU funding. Although national origins of participating organizations do not explain the amounts of funding granted, there is a rather strong tendency for national organizations to primarily collaborate with other national organizations. Finally, information about the technological domains addressed and the economic sectors involved provides decent explanatory power for statistically explaining the funding amounts granted. With these results and the accompanying discussion, the paper contributes to the timely debate about innovation, technology development, and industrial policy in Europe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03098v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jukka Ruohonen, Paul Timmers</dc:creator>
    </item>
    <item>
      <title>Assessing the impact of external factors on the occurrence of emergencies</title>
      <link>https://arxiv.org/abs/2501.03111</link>
      <description>arXiv:2501.03111v1 Announce Type: new 
Abstract: This study investigates the impact of 19 external factors, related to weather, road traffic conditions, air quality, and time, on the occurrence of emergencies using historical data provided by the dispatch center of the Centre Hospitalier Universitaire Vaudois (CHUV). This center is responsible for managing Emergency Medical Service (EMS) resources in the majority of the French-speaking part of Switzerland. First, classical statistical methods, such as correlation, Chi-squared test, Student's $t$-test, and information value, are employed to identify dependencies between the occurrence of emergencies and the considered parameters. Additionally, SHapley Additive exPlanations (SHAP) values and permutation importance are computed using eXtreme Gradient Boosting (XGBoost) and Multilayer Perceptron (MLP) models. The results indicate that the hour of the day, along with correlated parameters, plays a crucial role in the occurrence of emergencies. Conversely, other factors do not significantly influence emergency occurrences. Subsequently, a simplified model that considers only the hour of the day is compared with our XGBoost and MLP models. These comparisons reveal no significant difference between the three models in terms of performance, supporting the use of the basic model in this context. These observations provide valuable insights for EMS resource relocation strategies, benefit predictive modeling efforts, and inform decision-making in the context of EMS. The implications extend to enhancing EMS quality, making this research essential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03111v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F\'elicien H\^eche, Philipp Schiller, Oussama Barakat, Thibaut Desmettre, Stephan Robert-Nicoud</dc:creator>
    </item>
    <item>
      <title>INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models</title>
      <link>https://arxiv.org/abs/2501.01973</link>
      <description>arXiv:2501.01973v1 Announce Type: cross 
Abstract: The rapid development of large language models (LLMs) and large vision models (LVMs) have propelled the evolution of multi-modal AI systems, which have demonstrated the remarkable potential for industrial applications by emulating human-like cognition. However, they also pose significant ethical challenges, including amplifying harmful content and reinforcing societal biases. For instance, biases in some industrial image generation models highlighted the urgent need for robust fairness assessments. Most existing evaluation frameworks focus on the comprehensiveness of various aspects of the models, but they exhibit critical limitations, including insufficient attention to content generation alignment and social bias-sensitive domains. More importantly, their reliance on pixel-detection techniques is prone to inaccuracies.
  To address these issues, this paper presents INFELM, an in-depth fairness evaluation on widely-used text-to-image models. Our key contributions are: (1) an advanced skintone classifier incorporating facial topology and refined skin pixel representation to enhance classification precision by at least 16.04%, (2) a bias-sensitive content alignment measurement for understanding societal impacts, (3) a generalizable representation bias evaluation for diverse demographic groups, and (4) extensive experiments analyzing large-scale text-to-image model outputs across six social-bias-sensitive domains. We find that existing models in the study generally do not meet the empirical fairness criteria, and representation bias is generally more pronounced than alignment errors. INFELM establishes a robust benchmark for fairness assessment, supporting the development of multi-modal AI systems that align with ethical and human-centric principles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01973v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Jin, Xing Liu, Yu Liu, Jia Qing Yap, Andrea Wong, Adriana Crespo, Qi Lin, Zhiyuan Yin, Qiang Yan, Ryan Ye</dc:creator>
    </item>
    <item>
      <title>Gender Bias in Text-to-Video Generation Models: A case study of Sora</title>
      <link>https://arxiv.org/abs/2501.01987</link>
      <description>arXiv:2501.01987v1 Announce Type: cross 
Abstract: The advent of text-to-video generation models has revolutionized content creation as it produces high-quality videos from textual prompts. However, concerns regarding inherent biases in such models have prompted scrutiny, particularly regarding gender representation. Our study investigates the presence of gender bias in OpenAI's Sora, a state-of-the-art text-to-video generation model. We uncover significant evidence of bias by analyzing the generated videos from a diverse set of gender-neutral and stereotypical prompts. The results indicate that Sora disproportionately associates specific genders with stereotypical behaviors and professions, which reflects societal prejudices embedded in its training data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01987v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Nadeem, Shahab Saquib Sohail, Erik Cambria, Bj\"orn W. Schuller, Amir Hussain</dc:creator>
    </item>
    <item>
      <title>AGGA: A Dataset of Academic Guidelines for Generative AI and Large Language Models</title>
      <link>https://arxiv.org/abs/2501.02063</link>
      <description>arXiv:2501.02063v1 Announce Type: cross 
Abstract: This study introduces AGGA, a dataset comprising 80 academic guidelines for the use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic settings, meticulously collected from official university websites. The dataset contains 188,674 words and serves as a valuable resource for natural language processing tasks commonly applied in requirements engineering, such as model synthesis, abstraction identification, and document structure assessment. Additionally, AGGA can be further annotated to function as a benchmark for various tasks, including ambiguity detection, requirements categorization, and the identification of equivalent requirements. Our methodologically rigorous approach ensured a thorough examination, with a selection of universities that represent a diverse range of global institutions, including top-ranked universities across six continents. The dataset captures perspectives from a variety of academic fields, including humanities, technology, and both public and private institutions, offering a broad spectrum of insights into the integration of GAIs and LLMs in academia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02063v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit Dhurandhar</dc:creator>
    </item>
    <item>
      <title>Validity Arguments For Constructed Response Scoring Using Generative Artificial Intelligence Applications</title>
      <link>https://arxiv.org/abs/2501.02334</link>
      <description>arXiv:2501.02334v1 Announce Type: cross 
Abstract: The rapid advancements in large language models and generative artificial intelligence (AI) capabilities are making their broad application in the high-stakes testing context more likely. Use of generative AI in the scoring of constructed responses is particularly appealing because it reduces the effort required for handcrafting features in traditional AI scoring and might even outperform those methods. The purpose of this paper is to highlight the differences in the feature-based and generative AI applications in constructed response scoring systems and propose a set of best practices for the collection of validity evidence to support the use and interpretation of constructed response scores from scoring systems using generative AI. We compare the validity evidence needed in scoring systems using human ratings, feature-based natural language processing AI scoring engines, and generative AI. The evidence needed in the generative AI context is more extensive than in the feature-based NLP scoring context because of the lack of transparency and other concerns unique to generative AI such as consistency. Constructed response score data from standardized tests demonstrate the collection of validity evidence for different types of scoring systems and highlights the numerous complexities and considerations when making a validity argument for these scores. In addition, we discuss how the evaluation of AI scores might include a consideration of how a contributory scoring approach combining multiple AI scores (from different sources) will cover more of the construct in the absence of human ratings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02334v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jodi M. Casabianca, Daniel F. McCaffrey, Matthew S. Johnson, Naim Alper, Vladimir Zubenko</dc:creator>
    </item>
    <item>
      <title>LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas and Ad-Hoc Networks</title>
      <link>https://arxiv.org/abs/2501.02469</link>
      <description>arXiv:2501.02469v1 Announce Type: cross 
Abstract: The minimal infrastructure requirements of LoRa make it suitable for deployments in remote and disaster-stricken areas. Concomitantly, the modern era is witnessing the proliferation of web applications in all aspects of human life, including IoT and other network services. Contemporary IoT and network solutions heavily rely on web applications to render services. However, despite the recent research and development pivoted around LoRa, there is still a lack of studies focusing on web application access over LoRa networks. Specifically, technical challenges like payload size limitation, low data rate, and contentions in multi-user setups limit the applicability of LoRa for web applications. Hence, we propose LoRaWeb, which enables web access over LoRa networks. The LoRaWeb hardware tethers a WiFi hotspot to which the client devices connect and access the web pages using a web browser. LoRa backbone of the network handles the web page transmission from the requester and receiver devices. LoRaWeb implements a synchronization procedure to address the aforementioned challenges for effective message exchange between requesters and responders. The system implements a caching mechanism to reduce latency and contention. Additionally, it implements a message-slicing mechanism in the application layer to overcome the hardware limitations on the message length. The actual hardware-based implementation results indicate seamless deployment, and the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and ~$6 S$ for a $10 KB$ size web page.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02469v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Atonu Ghosh, Sudip Misra</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm</title>
      <link>https://arxiv.org/abs/2501.02532</link>
      <description>arXiv:2501.02532v1 Announce Type: cross 
Abstract: In the era of rapid digital communication, vast amounts of textual data are generated daily, demanding efficient methods for latent content analysis to extract meaningful insights. Large Language Models (LLMs) offer potential for automating this process, yet comprehensive assessments comparing their performance to human annotators across multiple dimensions are lacking. This study evaluates the reliability, consistency, and quality of seven state-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and Mixtral, relative to human annotators in analyzing sentiment, political leaning, emotional intensity, and sarcasm detection. A total of 33 human annotators and eight LLM variants assessed 100 curated textual items, generating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across three time points to examine temporal consistency. Inter-rater reliability was measured using Krippendorff's alpha, and intra-class correlation coefficients assessed consistency over time. The results reveal that both humans and LLMs exhibit high reliability in sentiment analysis and political leaning assessments, with LLMs demonstrating higher internal consistency than humans. In emotional intensity, LLMs displayed higher agreement compared to humans, though humans rated emotional intensity significantly higher. Both groups struggled with sarcasm detection, evidenced by low agreement. LLMs showed excellent temporal consistency across all dimensions, indicating stable performance over time. This research concludes that LLMs, especially GPT-4, can effectively replicate human analysis in sentiment and political leaning, although human expertise remains essential for emotional intensity interpretation. The findings demonstrate the potential of LLMs for consistent and high-quality performance in certain areas of latent content analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02532v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ljubisa Bojic, Olga Zagovora, Asta Zelenkauskaite, Vuk Vukovic, Milan Cabarkapa, Selma Veseljevi\'c Jerkovic, Ana Jovan\v{c}evic</dc:creator>
    </item>
    <item>
      <title>Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models</title>
      <link>https://arxiv.org/abs/2501.02599</link>
      <description>arXiv:2501.02599v1 Announce Type: cross 
Abstract: Mathematical word problems (MWPs) involve the task of converting textual descriptions into mathematical equations. This poses a significant challenge in natural language processing, particularly for low-resource languages such as Bengali. This paper addresses this challenge by developing an innovative approach to solving Bengali MWPs using transformer-based models, including Basic Transformer, mT5, BanglaT5, and mBART50. To support this effort, the "PatiGonit" dataset was introduced, containing 10,000 Bengali math problems, and these models were fine-tuned to translate the word problems into equations accurately. The evaluation revealed that the mT5 model achieved the highest accuracy of 97.30%, demonstrating the effectiveness of transformer models in this domain. This research marks a significant step forward in Bengali natural language processing, offering valuable methodologies and resources for educational AI tools. By improving math education, it also supports the development of advanced problem-solving skills for Bengali-speaking students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02599v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jalisha Jashim Era, Bidyarthi Paul, Tahmid Sattar Aothoi, Mirazur Rahman Zim, Faisal Muhammad Shah</dc:creator>
    </item>
    <item>
      <title>Trust and Dependability in Blockchain &amp; AI Based MedIoT Applications: Research Challenges and Future Directions</title>
      <link>https://arxiv.org/abs/2501.02647</link>
      <description>arXiv:2501.02647v1 Announce Type: cross 
Abstract: This paper critically reviews the integration of Artificial Intelligence (AI) and blockchain technologies in the context of Medical Internet of Things (MedIoT) applications, where they collectively promise to revolutionize healthcare delivery. By examining current research, we underscore AI's potential in advancing diagnostics and patient care, alongside blockchain's capacity to bolster data security and patient privacy. We focus particularly on the imperative to cultivate trust and ensure reliability within these systems. Our review highlights innovative solutions for managing healthcare data and challenges such as ensuring scalability, maintaining privacy, and promoting ethical practices within the MedIoT domain. We present a vision for integrating AI-driven insights with blockchain security in healthcare, offering a comprehensive review of current research and future directions. We conclude with a set of identified research gaps and propose that addressing these is crucial for achieving the dependable, secure, and patient -centric MedIoT applications of tomorrow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02647v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ellis Solaiman, Christa Awad</dc:creator>
    </item>
    <item>
      <title>LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases</title>
      <link>https://arxiv.org/abs/2501.03112</link>
      <description>arXiv:2501.03112v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have been observed to exhibit bias in numerous ways, potentially creating or worsening outcomes for specific groups identified by protected attributes such as sex, race, sexual orientation, or age. To help address this gap, we introduce LangFair, an open-source Python package that aims to equip LLM practitioners with the tools to evaluate bias and fairness risks relevant to their specific use cases. The package offers functionality to easily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts, and subsequently calculate applicable metrics for the practitioner's use case. To guide in metric selection, LangFair offers an actionable decision framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03112v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Viren Bajaj, Zeya Ahmad</dc:creator>
    </item>
    <item>
      <title>Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity</title>
      <link>https://arxiv.org/abs/2501.03203</link>
      <description>arXiv:2501.03203v1 Announce Type: cross 
Abstract: This study seeks to enhance academic integrity by providing tools to detect AI-generated content in student work using advanced technologies. The findings promote transparency and accountability, helping educators maintain ethical standards and supporting the responsible integration of AI in education. A key contribution of this work is the generation of the CyberHumanAI dataset, which has 1000 observations, 500 of which are written by humans and the other 500 produced by ChatGPT. We evaluate various machine learning (ML) and deep learning (DL) algorithms on the CyberHumanAI dataset comparing human-written and AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT). Results demonstrate that traditional ML algorithms, specifically XGBoost and Random Forest, achieve high performance (83% and 81% accuracies respectively). Results also show that classifying shorter content seems to be more challenging than classifying longer content. Further, using Explainable Artificial Intelligence (XAI) we identify discriminative features influencing the ML model's predictions, where human-written content tends to use a practical language (e.g., use and allow). Meanwhile AI-generated text is characterized by more abstract and formal terms (e.g., realm and employ). Finally, a comparative analysis with GPTZero show that our narrowly focused, simple, and fine-tuned model can outperform generalized systems like GPTZero. The proposed model achieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when tasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a tendency to classify challenging and small-content cases as either mixed or unrecognized while our proposed model showed a more balanced performance across the three classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03203v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayat A. Najjar, Huthaifa I. Ashqar, Omar A. Darwish, Eman Hammad</dc:creator>
    </item>
    <item>
      <title>Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text</title>
      <link>https://arxiv.org/abs/2501.03212</link>
      <description>arXiv:2501.03212v1 Announce Type: cross 
Abstract: The development of Generative AI Large Language Models (LLMs) raised the alarm regarding identifying content produced through generative AI or humans. In one case, issues arise when students heavily rely on such tools in a manner that can affect the development of their writing or coding skills. Other issues of plagiarism also apply. This study aims to support efforts to detect and identify textual content generated using LLM tools. We hypothesize that LLMs-generated text is detectable by machine learning (ML), and investigate ML models that can recognize and differentiate texts generated by multiple LLMs tools. We leverage several ML and Deep Learning (DL) algorithms such as Random Forest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable Artificial Intelligence (XAI) to understand the important features in attribution. Our method is divided into 1) binary classification to differentiate between human-written and AI-text, and 2) multi classification, to differentiate between human-written text and the text generated by the five different LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity). Results show high accuracy in the multi and binary classification. Our model outperformed GPTZero with 98.5\% accuracy to 78.3\%. Notably, GPTZero was unable to recognize about 4.2\% of the observations, but our model was able to recognize the complete test dataset. XAI results showed that understanding feature importance across different classes enables detailed author/source profiles. Further, aiding in attribution and supporting plagiarism detection by highlighting unique stylistic and structural elements ensuring robust content originality verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03212v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayat Najjar, Huthaifa I. Ashqar, Omar Darwish, Eman Hammad</dc:creator>
    </item>
    <item>
      <title>Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation</title>
      <link>https://arxiv.org/abs/2501.03225</link>
      <description>arXiv:2501.03225v1 Announce Type: cross 
Abstract: The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03225v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhui Zhang, Yuchang Su, Yiming Liu, Xiaohan Wang, James Burgess, Elaine Sui, Chenyu Wang, Josiah Aklilu, Alejandro Lozano, Anjiang Wei, Ludwig Schmidt, Serena Yeung-Levy</dc:creator>
    </item>
    <item>
      <title>CERN for AI: A Theoretical Framework for Autonomous Simulation-Based Artificial Intelligence Testing and Alignment</title>
      <link>https://arxiv.org/abs/2312.09402</link>
      <description>arXiv:2312.09402v2 Announce Type: replace 
Abstract: This paper explores the potential of a multidisciplinary approach to testing and aligning artificial intelligence (AI), specifically focusing on large language models (LLMs). Due to the rapid development and wide application of LLMs, challenges such as ethical alignment, controllability, and predictability of these models emerged as global risks. This study investigates an innovative simulation-based multi-agent system within a virtual reality framework that replicates the real-world environment. The framework is populated by automated 'digital citizens,' simulating complex social structures and interactions to examine and optimize AI. Application of various theories from the fields of sociology, social psychology, computer science, physics, biology, and economics demonstrates the possibility of a more human-aligned and socially responsible AI. The purpose of such a digital environment is to provide a dynamic platform where advanced AI agents can interact and make independent decisions, thereby mimicking realistic scenarios. The actors in this digital city, operated by the LLMs, serve as the primary agents, exhibiting high degrees of autonomy. While this approach shows immense potential, there are notable challenges and limitations, most significantly the unpredictable nature of real-world social dynamics. This research endeavors to contribute to the development and refinement of AI, emphasizing the integration of social, ethical, and theoretical dimensions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09402v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1186/s40309-024-00238-0</arxiv:DOI>
      <dc:creator>Ljubisa Bojic, Matteo Cinelli, Dubravko Culibrk, Boris Delibasic</dc:creator>
    </item>
    <item>
      <title>Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models</title>
      <link>https://arxiv.org/abs/2402.01749</link>
      <description>arXiv:2402.01749v2 Announce Type: replace 
Abstract: The integration of machine learning techniques has become a cornerstone in the development of intelligent urban services, significantly contributing to the enhancement of urban efficiency, sustainability, and overall livability. Recent advancements in foundational models, such as ChatGPT, have introduced a paradigm shift within the fields of machine learning and artificial intelligence. These models, with their exceptional capacity for contextual comprehension, problem-solving, and task adaptability, present a transformative opportunity to reshape the future of smart cities and drive progress toward Urban General Intelligence (UGI). Despite increasing attention to Urban Foundation Models (UFMs), this rapidly evolving field faces critical challenges, including the lack of clear definitions, systematic reviews, and universalizable solutions. To address these issues, this paper first introduces the definition and concept of UFMs and highlights the distinctive challenges involved in their development. Furthermore, we present a data-centric taxonomy that classifies existing research on UFMs according to the various urban data modalities and types. In addition, we propose a prospective framework designed to facilitate the realization of versatile UFMs, aimed at overcoming the identified challenges and driving further progress in this field. Finally, this paper explores the wide-ranging applications of UFMs within urban contexts, illustrating their potential to significantly impact and transform urban systems. A comprehensive collection of relevant research papers and open-source resources have been collated and are continuously updated at: https://github.com/usail-hkust/Awesome-Urban-Foundation-Models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01749v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijia Zhang, Jindong Han, Zhao Xu, Hang Ni, Tengfei Lyu, Hao Liu, Hui Xiong</dc:creator>
    </item>
    <item>
      <title>Societal Adaptation to Advanced AI</title>
      <link>https://arxiv.org/abs/2405.10295</link>
      <description>arXiv:2405.10295v2 Announce Type: replace 
Abstract: Existing strategies for managing risks from advanced AI systems often focus on affecting what AI systems are developed and how they diffuse. However, this approach becomes less feasible as the number of developers of advanced AI grows, and impedes beneficial use-cases as well as harmful ones. In response, we urge a complementary approach: increasing societal adaptation to advanced AI, that is, reducing the expected negative impacts from a given level of diffusion of a given AI capability. We introduce a conceptual framework which helps identify adaptive interventions that avoid, defend against and remedy potentially harmful uses of AI systems, illustrated with examples in election manipulation, cyberterrorism, and loss of control to AI decision-makers. We discuss a three-step cycle that society can implement to adapt to AI. Increasing society's ability to implement this cycle builds its resilience to advanced AI. We conclude with concrete recommendations for governments, industry, and third-parties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10295v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jamie Bernardi, Gabriel Mukobi, Hilary Greaves, Lennart Heim, Markus Anderljung</dc:creator>
    </item>
    <item>
      <title>Undermining Mental Proof: How AI Can Make Cooperation Harder by Making Thinking Easier</title>
      <link>https://arxiv.org/abs/2407.14452</link>
      <description>arXiv:2407.14452v2 Announce Type: replace 
Abstract: Large language models and other highly capable AI systems ease the burdens of deciding what to say or do, but this very ease can undermine the effectiveness of our actions in social contexts. We explain this apparent tension by introducing the integrative theoretical concept of "mental proof," which occurs when observable actions are used to certify unobservable mental facts. From hiring to dating, mental proofs enable people to credibly communicate values, intentions, states of knowledge, and other private features of their minds to one another in low-trust environments where honesty cannot be easily enforced. Drawing on results from economics, theoretical biology, and computer science, we describe the core theoretical mechanisms that enable people to effect mental proofs. An analysis of these mechanisms clarifies when and how artificial intelligence can make low-trust cooperation harder despite making thinking easier.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14452v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zachary Wojtowicz, Simon DeDeo</dc:creator>
    </item>
    <item>
      <title>Estimating the Increase in Emissions caused by AI-augmented Search</title>
      <link>https://arxiv.org/abs/2407.16894</link>
      <description>arXiv:2407.16894v2 Announce Type: replace 
Abstract: AI-generated answers to conventional search queries dramatically increase the energy consumption. By our estimates, energy demand increase by 60-70 times. This is a based on an updated estimate of energy consumption for conventional search and recent work on the energy demand of queries to the BLOOM model, a 176B parameter model, and OpenAI's GPT-3, which is of similar complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16894v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wim Vanderbauwhede</dc:creator>
    </item>
    <item>
      <title>RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model</title>
      <link>https://arxiv.org/abs/2408.16634</link>
      <description>arXiv:2408.16634v3 Announce Type: replace 
Abstract: The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16634v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuan Shi, Jing Yan, Xiaoli Tang, Lingjuan Lyu, Boi Faltings</dc:creator>
    </item>
    <item>
      <title>Bridging Today and the Future of Humanity: AI Safety in 2024 and Beyond</title>
      <link>https://arxiv.org/abs/2410.18114</link>
      <description>arXiv:2410.18114v4 Announce Type: replace 
Abstract: The advancements in generative AI inevitably raise concerns about the associated risks and safety implications, which, in return, catalyzes significant progress in AI safety. However, as this field continues to evolve, a critical question arises: are our current efforts aligned with the long-term goal of human history and civilization? This paper presents a blueprint for an advanced human society and leverages this vision to guide contemporary AI safety efforts. It outlines a future where the Internet of Everything becomes reality, and creates a roadmap of significant technological advancements towards this envisioned future. For each stage of the advancements, this paper forecasts potential AI safety issues that humanity may face. By projecting current efforts against this blueprint, we examine the alignment between the present efforts and the long-term needs. We also identify gaps in current approaches and highlight unique challenges and missions that demand increasing attention from AI safety practitioners in the 2020s, addressing critical areas that must not be overlooked in shaping a responsible and promising future of AI. This vision paper aims to offer a broader perspective on AI safety, emphasizing that our current efforts should not only address immediate concerns but also anticipate potential risks in the expanding AI landscape, thereby promoting a more secure and sustainable future in human civilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18114v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shanshan Han</dc:creator>
    </item>
    <item>
      <title>Regulating radiology AI medical devices that evolve in their lifecycle</title>
      <link>https://arxiv.org/abs/2412.20498</link>
      <description>arXiv:2412.20498v2 Announce Type: replace 
Abstract: Over time, the distribution of medical image data drifts due to multiple factors, including shifts in patient demographics, acquisition devices, and disease manifestation. While human radiologists can extrapolate their knowledge to such changes, AI systems cannot. In fact, deep learning models are highly susceptible to even slight variations in image characteristics. Therefore, manufacturers must update their models with new data to ensure that they remain safe and effective. Until recently, conducting such model updates in the USA and European Union meant applying for re-approval. Given the time and monetary costs associated with these processes, updates were infrequent, and obsolete systems continued functioning for too long. During 2024, several developments in the regulatory frameworks of these regions have taken place that promise to streamline the process of rolling out model updates safely: The European Artificial Intelligence Act came into effect last August, and the Food and Drug Administration (FDA) released the final marketing submission recommendations for a Predetermined Change Control Plan (PCCP) in December. We give an overview of the requirements and objectives of recent regulatory efforts and summarize the building blocks needed for successfully deploying dynamic systems. At the center of these pieces of regulation - and as prerequisites for manufacturers to conduct model updates without re-approval - are the need to describe the data collection and re-training processes and to establish real-world quality monitoring mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20498v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camila Gonz\'alez, Moritz Fuchs, Daniel Pinto dos Santos, Philipp Matthies, Manuel Trenz, Maximilian Gr\"uning, Akshay Chaudhari, David B. Larson, Ahmed Othman, Moon Kim, Felix Nensa, Anirban Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Generative AI and LLMs in Industry: A text-mining Analysis and Critical Evaluation of Guidelines and Policy Statements Across Fourteen Industrial Sectors</title>
      <link>https://arxiv.org/abs/2501.00957</link>
      <description>arXiv:2501.00957v2 Announce Type: replace 
Abstract: The rise of Generative AI (GAI) and Large Language Models (LLMs) has transformed industrial landscapes, offering unprecedented opportunities for efficiency and innovation while raising critical ethical, regulatory, and operational challenges. This study conducts a text-based analysis of 160 guidelines and policy statements across fourteen industrial sectors, utilizing systematic methods and text-mining techniques to evaluate the governance of these technologies. By examining global directives, industry practices, and sector-specific policies, the paper highlights the complexities of balancing innovation with ethical accountability and equitable access. The findings provide actionable insights and recommendations for fostering responsible, transparent, and safe integration of GAI and LLMs in diverse industry contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00957v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit Dhurandhar</dc:creator>
    </item>
    <item>
      <title>IGGA: A Dataset of Industrial Guidelines and Policy Statements for Generative AIs</title>
      <link>https://arxiv.org/abs/2501.00959</link>
      <description>arXiv:2501.00959v2 Announce Type: replace 
Abstract: This paper introduces IGGA, a dataset of 160 industry guidelines and policy statements for the use of Generative AIs (GAIs) and Large Language Models (LLMs) in industry and workplace settings, collected from official company websites, and trustworthy news sources. The dataset contains 104,565 words and serves as a valuable resource for natural language processing tasks commonly applied in requirements engineering, such as model synthesis, abstraction identification, and document structure assessment. Additionally, IGGA can be further annotated to function as a benchmark for various tasks, including ambiguity detection, requirements categorization, and the identification of equivalent requirements. Our methodologically rigorous approach ensured a thorough examination, with a selection of reputable and influential companies that represent a diverse range of global institutions across six continents. The dataset captures perspectives from fourteen industry sectors, including technology, finance, and both public and private institutions, offering a broad spectrum of insights into the integration of GAIs and LLMs in industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00959v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit Dhurandhar</dc:creator>
    </item>
    <item>
      <title>Implications of Artificial Intelligence on Health Data Privacy and Confidentiality</title>
      <link>https://arxiv.org/abs/2501.01639</link>
      <description>arXiv:2501.01639v2 Announce Type: replace 
Abstract: The rapid integration of artificial intelligence (AI) in healthcare is revolutionizing medical diagnostics, personalized medicine, and operational efficiency. However, alongside these advancements, significant challenges arise concerning patient data privacy, ethical considerations, and regulatory compliance. This paper examines the dual impact of AI on healthcare, highlighting its transformative potential and the critical need for safeguarding sensitive health information. It explores the role of the Health Insurance Portability and Accountability Act (HIPAA) as a regulatory framework for ensuring data privacy and security, emphasizing the importance of robust safeguards and ethical standards in AI-driven healthcare. Through case studies, including AI applications in diabetic retinopathy, oncology, and the controversies surrounding data sharing, this study underscores the ethical and legal complexities of AI implementation. A balanced approach that fosters innovation while maintaining patient trust and privacy is imperative. The findings emphasize the importance of continuous education, transparency, and adherence to regulatory frameworks to harness AI's full potential responsibly and ethically in healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01639v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ahmad Momani</dc:creator>
    </item>
    <item>
      <title>Lived Experience Not Found: LLMs Struggle to Align with Experts on Addressing Adverse Drug Reactions from Psychiatric Medication Use</title>
      <link>https://arxiv.org/abs/2410.19155</link>
      <description>arXiv:2410.19155v2 Announce Type: replace-cross 
Abstract: Adverse Drug Reactions (ADRs) from psychiatric medications are the leading cause of hospitalizations among mental health patients. With healthcare systems and online communities facing limitations in resolving ADR-related issues, Large Language Models (LLMs) have the potential to fill this gap. Despite the increasing capabilities of LLMs, past research has not explored their capabilities in detecting ADRs related to psychiatric medications or in providing effective harm reduction strategies. To address this, we introduce the Psych-ADR benchmark and the Adverse Drug Reaction Response Assessment (ADRA) framework to systematically evaluate LLM performance in detecting ADR expressions and delivering expert-aligned mitigation strategies. Our analyses show that LLMs struggle with understanding the nuances of ADRs and differentiating between types of ADRs. While LLMs align with experts in terms of expressed emotions and tone of the text, their responses are more complex, harder to read, and only 70.86% aligned with expert strategies. Furthermore, they provide less actionable advice by a margin of 12.32% on average. Our work provides a comprehensive benchmark and evaluation framework for assessing LLMs in strategy-driven tasks within high-risk domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19155v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohit Chandra, Siddharth Sriraman, Gaurav Verma, Harneet Singh Khanuja, Jose Suarez Campayo, Zihang Li, Michael L. Birnbaum, Munmun De Choudhury</dc:creator>
    </item>
    <item>
      <title>A Comparative Study on Self-Organization in Wireless Sensor Networks</title>
      <link>https://arxiv.org/abs/2411.15690</link>
      <description>arXiv:2411.15690v2 Announce Type: replace-cross 
Abstract: With advancements in microelectromechanical systems, low-power integrated circuits, and wireless communications, wireless sensor networks (WSNs) have become increasingly significant [1][2]. These distributed networks enable efficient resource utilization and open doors to numerous applications, including personal healthcare, home automation, environmental monitoring, industrial automation, and defense surveillance. However, WSNs are susceptible to environmental factors in their deployment areas and may suffer damage. In such cases, the network must be reconfigured or repaired. To address these challenges and adapt to resource constraints, WSN mechanisms must exhibit self-organizing capabilities. For instance, in tasks like allocation, cooperative communication, and dynamic data collection, self-organization enhances the efficiency and robustness of WSNs across the application, network, and physical layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15690v2</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Simon, Salwa M. Din, Raja Jamal Chib</dc:creator>
    </item>
  </channel>
</rss>
