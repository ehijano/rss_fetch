<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Jul 2025 04:01:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Uncertainty quantification of a multi-component Hall thruster model at varying facility pressures</title>
      <link>https://arxiv.org/abs/2507.08113</link>
      <description>arXiv:2507.08113v1 Announce Type: new 
Abstract: Bayesian inference is applied to calibrate and quantify prediction uncertainty in a coupled multi-component Hall thruster model at varying facility background pressures. The model, consisting of a cathode model, discharge model, and plume model, is used to simulate two thrusters across a range of background pressures in multiple vacuum test facilities. The model outputs include thruster performance metrics, one-dimensional plasma properties, and the angular distribution of the current density in the plume. The simulated thrusters include a magnetically shielded thruster, the H9, and an unshielded thruster, the SPT-100. After calibration, the model captures several key performance trends with background pressure, including changes in thrust and upstream shifts in the ion acceleration region. Furthermore, the model exhibits predictive accuracy to within 10\% when evaluated on flow rates and pressures not included in the training data, and the model can predict some performance characteristics across test facilities to within the same range. Evaluated on the same data as prior work [Eckels et al. 2024], the model reduced predictive errors in thrust and discharge current by greater than 50%. An extrapolation to on-orbit performance is performed with an error of 9\%, capturing trends in discharge current but not thrust. Possible extensions and improvements are discussed in the context of using data for predictive Hall thruster modeling across vacuum facilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08113v1</guid>
      <category>stat.AP</category>
      <category>physics.comp-ph</category>
      <category>physics.plasm-ph</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas A. Marks, Joshua D. Eckels, Gabriel E. Mora, Alex A. Gorodetsky</dc:creator>
    </item>
    <item>
      <title>Optimal Experimental Design for Microplastics Sampling Experiments</title>
      <link>https://arxiv.org/abs/2507.08170</link>
      <description>arXiv:2507.08170v1 Announce Type: new 
Abstract: Microplastics contamination is one of the most rapidly growing research topics. However, monitoring microplastics contamination in the environment presents both logistical and statistical challenges, particularly when constrained resources limit the scale of sampling and laboratory analysis. In this paper, we propose a Bayesian framework for the optimal experimental design of microplastic sampling campaigns. Our approach integrates prior knowledge and uncertainty quantification to guide decisions on how many spatial Centrosamples to collect and how many particles to analyze for polymer composition. By modeling particle counts as a Poisson distribution and polymer types as a Multinomial distribution, we developed a conjugate Bayesian model that enables efficient posterior inference. We introduce variance-based loss functions to evaluate expected information gain for both abundance and composition, and we formulate a constrained optimization problem that incorporates realistic cost structures. Our results provide principled and interpretable recommendations for allocating limited resources across the sampling and analysis phases. Through simulated scenarios and real-world-inspired examples, we demonstrate how the proposed methodology adapts to prior assumptions and cost variations, ensuring robustness and flexibility. This work contributes to the broader field of Bayesian experimental design by offering a concrete, application-driven case study that underscores the value of formal design strategies in environmental monitoring contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08170v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco A. Aquino-L\'opez, Ana Carolina Ruiz-Fern\'andez, Joan-Albert Sanchez-Cabeza, J. Andr\'es Christen</dc:creator>
    </item>
    <item>
      <title>Modeling Wallet-Level Behavioral Shifts Post-FTX Collapse: An XAI-Driven GLM Study on Ethereum Transactions</title>
      <link>https://arxiv.org/abs/2507.08455</link>
      <description>arXiv:2507.08455v1 Announce Type: new 
Abstract: The Ethereum blockchain plays a central role in the broader cryptocurrency ecosystem, enabling a wide range of financial activity through the use of smart contracts. This paper investigates how individual Ethereum wallets responded to the collapse of FTX, one of the largest centralized cryptocurrency exchanges. Moving beyond price-based event studies, we adopt a bottom-up approach using granular wallet-level data. We construct a representative sample of Ethereum addresses and analyze their transaction behavior before and after the collapse using an explainable artificial intelligence (XAI) framework. Our proposed framework addresses data scarcity in high-resolution wallet-level daily transactions by employing a calibrated zero-inflated generalized linear fixed effects model. Our analysis quantifies distinct shifts in transaction intensity and stablecoin usage, highlighting a flight to safety within the ecosystem. These findings underscore the value of a bottom-up methodology for quantifying the user-level impact of blockchain-based shocks, offering insights beyond traditional price-level analysis through wallet-level data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08455v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Gillen, Rashmi Ranjan Bhuyan, Gourab Mukherjee, Austin Pollok</dc:creator>
    </item>
    <item>
      <title>Sensitivity measures for engineering and environmental decision support</title>
      <link>https://arxiv.org/abs/2507.08488</link>
      <description>arXiv:2507.08488v1 Announce Type: new 
Abstract: Information value, a measure for decision sensitivity, can provide essential information in engineering and environmental assessments. It quantifies the potential for improved decision-making when reducing uncertainty in specific inputs. By contrast to other sensitivity measures, it admits not only a relative ranking of input factors but also an absolute interpretation through statements like ''Eliminating the uncertainty in factor $A$ has an expected value of $5000$ Euro''. In this paper, we present a comprehensive overview of the information value by presenting the theory and methods in view of their application to engineering and environmental assessments. We show how one should differentiate between aleatory and epistemic uncertainty in the analysis. Furthermore, we introduce the evaluation of the information value in applications where the decision is described by a continuous parameter. The paper concludes with two real-life applications of the information value to highlight its power in supporting decision-making in engineering and environmental applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08488v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Straub, Wolfgang Betz, Mara Ruf, Amelie Hoffmann, Angela Landgraf, Lea Friedli, Iason Papaioannou</dc:creator>
    </item>
    <item>
      <title>Influence of river incision on landslides triggered in Nepal by the Gorkha earthquake: Results from a pixel-based susceptibility model using inlabru</title>
      <link>https://arxiv.org/abs/2507.08742</link>
      <description>arXiv:2507.08742v1 Announce Type: new 
Abstract: This study presents a comprehensive framework for modelling earthquake-induced landslides (EQILs) through a channel-based analysis of landslide centroid distributions. A key innovation is the incorporation of the normalised channel steepness index ($k_{sn}$) as a physically meaningful and novel covariate, inferring hillslope erosion and fluvial incision processes. Used within spatial point process models, $k_{sn}$ supports the generation of landslide susceptibility maps with quantified uncertainty. To address spatial data misalignment between covariates and landslide observations, we leverage the inlabru framework, which enables coherent integration through mesh-based disaggregation, thereby overcoming challenges associated with spatially misaligned data integration. Our modelling strategy explicitly prioritises prospective transferability to unseen geographical regions, provided that explanatory variable data are available. By modelling both landslide locations and sizes, we find that elevated $k_{sn}$ is strongly associated with increased landslide susceptibility but not with landslide magnitude. The best-fitting Bayesian model, validated through cross-validation, offers a scalable and interpretable solution for predicting earthquake-induced landslides in complex terrain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08742v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Man Ho Suen, Mark Naylor, Simon Mudd, Finn Lindgren</dc:creator>
    </item>
    <item>
      <title>Propensity score with factor loadings: the effect of the Paris Agreement</title>
      <link>https://arxiv.org/abs/2507.08764</link>
      <description>arXiv:2507.08764v1 Announce Type: cross 
Abstract: Factor models for longitudinal data, where policy adoption is unconfounded with respect to a low-dimensional set of latent factor loadings, have become increasingly popular for causal inference. Most existing approaches, however, rely on a causal finite-sample approach or computationally intensive methods, limiting their applicability and external validity. In this paper, we propose a novel causal inference method for panel data based on inverse propensity score weighting where the propensity score is a function of latent factor loadings within a framework of causal inference from super-population. The approach relaxes the traditional restrictive assumptions of causal panel methods, while offering advantages in terms of causal interpretability, policy relevance, and computational efficiency. Under standard assumptions, we outline a three-step estimation procedure for the ATT and derive its large-sample properties using Mestimation theory. We apply the method to assess the causal effect of the Paris Agreement, a policy aimed at fostering the transition to a low-carbon economy, on European stock returns. Our empirical results suggest a statistically significant and negative short-run effect on the stock returns of firms that issued green bonds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08764v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelo Forino, Andrea Mercatanti, Giacomo Morelli</dc:creator>
    </item>
    <item>
      <title>Accounting for Missing Data in Public Health Research Using a Synthesis of Statistical and Mathematical Models</title>
      <link>https://arxiv.org/abs/2503.02789</link>
      <description>arXiv:2503.02789v2 Announce Type: replace 
Abstract: Introduction: Missing data is a challenge to medical research. Accounting for missing data by imputing or weighting conditional on covariates relies on the variable with missingness being observed at least some of the time for all unique covariate values. This requirement is referred to as positivity, and violations can result in bias. Here, we review a novel approach to addressing positivity violations in the context of systolic blood pressure. Methods: To illustrate the proposed approach, we estimate the mean systolic blood pressure among children and adolescents aged 2-17 years old in the United States using data from 2017-2018 National Health and Nutrition Examination Survey (NHANES). As blood pressure was never measured for those aged 2-7, there exists a positivity violation by design. Using a recently proposed synthesis of statistical and mathematical models, we integrate external information with NHANES to address our motivating question. Results: With the synthesis model, the estimated mean systolic blood pressure was 100.5 (95% confidence interval: 99.9, 101.0), which is notably lower than either a complete-case analysis or extrapolation from a statistical model. The synthesis results were supported by a diagnostic comparing the performance of the mathematical model in the positive region. Conclusion: Positivity violations pose a threat to quantitative medical research, and standard approaches to addressing nonpositivity rely on restrictive untestable assumptions. Using a synthesis model, like the one detailed here, offers a viable alternative through integration of external information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02789v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Bonnie E Shook-Sa, Stephen R Cole, Eric T Lofgren, Jessie K Edwards</dc:creator>
    </item>
    <item>
      <title>Asymmetric canonical correlation analysis of Riemannian and high-dimensional data</title>
      <link>https://arxiv.org/abs/2404.11781</link>
      <description>arXiv:2404.11781v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce a novel statistical model for the integrative analysis of Riemannian-valued functional data and high-dimensional data. We apply this model to explore the dependence structure between each subject's dynamic functional connectivity -- represented by a temporally indexed collection of positive definite covariance matrices -- and high-dimensional data representing lifestyle, demographic, and psychometric measures. Specifically, we employ a reformulation of canonical correlation analysis that enables efficient control of the complexity of the functional canonical directions using tangent space sieve approximations. Additionally, we enforce an interpretable group structure on the high-dimensional canonical directions via a sparsity-promoting penalty. The proposed method shows improved empirical performance over alternative approaches and comes with theoretical guarantees. Its application to data from the Human Connectome Project reveals a dominant mode of covariation between dynamic functional connectivity and lifestyle, demographic, and psychometric measures. This mode aligns with results from static connectivity studies but reveals a unique temporal non-stationary pattern that such studies fail to capture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11781v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Buenfil, Eardi Lila</dc:creator>
    </item>
    <item>
      <title>Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators</title>
      <link>https://arxiv.org/abs/2410.12690</link>
      <description>arXiv:2410.12690v3 Announce Type: replace-cross 
Abstract: A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems. Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs. In many applications, one often has available data on related systems. For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations. A key question is how information from such ``source'' systems can be transferred for effective surrogate training on the ``target'' system of interest. We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling. The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided. Such a ``local transfer'' property is present in many scientific systems: at certain parameters, systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental. By accounting for local transfer, the LOL-GP can temper the risk of ``negative transfer'', i.e., the risk of worsening predictive performance from information transfer. We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings. We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12690v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinming Wang, Simon Mak, John Miller, Jianguo Wu</dc:creator>
    </item>
    <item>
      <title>Predicting Air Pollution in Cork, Ireland Using Machine Learning</title>
      <link>https://arxiv.org/abs/2507.04196</link>
      <description>arXiv:2507.04196v2 Announce Type: replace-cross 
Abstract: Air pollution poses a critical health threat in cities worldwide, with nitrogen dioxide levels in Cork, Ireland exceeding World Health Organization safety standards by up to $278\%$. This study leverages artificial intelligence to predict air pollution with unprecedented accuracy, analyzing nearly ten years of data from five monitoring stations combined with 30 years of weather records. We evaluated 17 machine learning algorithms, with Extra Trees emerging as the optimal solution, achieving $77\%$ prediction accuracy and significantly outperforming traditional forecasting methods. Our analysis reveals that meteorological conditions particularly temperature, wind speed, and humidity are the primary drivers of pollution levels, while traffic patterns and seasonal changes create predictable pollution cycles. Pollution exhibits dramatic seasonal variations, with winter levels nearly double those of summer, and daily rush-hour peaks reaching $120\%$ above normal levels. While Cork's air quality shows concerning violations of global health standards, our models detected an encouraging $31\%$ improvement from 2014 to 2022. This research demonstrates that intelligent forecasting systems can provide city planners and environmental officials with powerful prediction tools, enabling life-saving early warning systems and informed urban planning decisions. The technology exists today to transform urban air quality management. All research materials and code are freely available at: https://github.com/MdRashidunnabi/Air-Pollution-Analysis.git</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04196v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Rashidunnabi, Fahmida Faiza Ananna, Kailash Hambarde, Bruno Gabriel Nascimento Andrade, Dean Venables, Hugo Proenca</dc:creator>
    </item>
  </channel>
</rss>
