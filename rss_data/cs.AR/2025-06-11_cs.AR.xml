<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Jun 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>ABC-FHE : A Resource-Efficient Accelerator Enabling Bootstrappable Parameters for Client-Side Fully Homomorphic Encryption</title>
      <link>https://arxiv.org/abs/2506.08461</link>
      <description>arXiv:2506.08461v1 Announce Type: new 
Abstract: As the demand for privacy-preserving computation continues to grow, fully homomorphic encryption (FHE)-which enables continuous computation on encrypted data-has become a critical solution. However, its adoption is hindered by significant computational overhead, requiring 10000-fold more computation compared to plaintext processing. Recent advancements in FHE accelerators have successfully improved server-side performance, but client-side computations remain a bottleneck, particularly under bootstrappable parameter configurations, which involve combinations of encoding, encrypt, decoding, and decrypt for large-sized parameters. To address this challenge, we propose ABC-FHE, an area- and power-efficient FHE accelerator that supports bootstrappable parameters on the client side. ABC-FHE employs a streaming architecture to maximize performance density, minimize area usage, and reduce off-chip memory access. Key innovations include a reconfigurable Fourier engine capable of switching between NTT and FFT modes. Additionally, an on-chip pseudo-random number generator and a unified on-the-fly twiddle factor generator significantly reduce memory demands, while optimized task scheduling enhances the CKKS client-side processing, achieving reduced latency. Overall, ABC-FHE occupies a die area of 28.638 mm2 and consumes 5.654 W of power in 28 nm technology. It delivers significant performance improvements, achieving a 1112x speed-up in encoding and encryption execution time compared to a CPU, and 214x over the state-of-the-art client-side accelerator. For decoding and decryption, it achieves a 963x speed-up over the CPU and 82x over the state-of-the-art accelerator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08461v1</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungwoong Yune, Hyojeong Lee, Adiwena Putra, Hyunjun Cho, Cuong Duong Manh, Jaeho Jeon, Joo-Young Kim</dc:creator>
    </item>
    <item>
      <title>CoQMoE: Co-Designed Quantization and Computation Orchestration for Mixture-of-Experts Vision Transformer on FPGA</title>
      <link>https://arxiv.org/abs/2506.08496</link>
      <description>arXiv:2506.08496v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) exhibit superior performance in computer vision tasks but face deployment challenges on resource-constrained devices due to high computational/memory demands. While Mixture-of-Experts Vision Transformers (MoE-ViTs) mitigate this through a scalable architecture with sub-linear computational growth, their hardware implementation on FPGAs remains constrained by resource limitations. This paper proposes a novel accelerator for efficiently implementing quantized MoE models on FPGAs through two key innovations: (1) A dual-stage quantization scheme combining precision-preserving complex quantizers with hardware-friendly simplified quantizers via scale reparameterization, with only 0.28 $\%$ accuracy loss compared to full precision; (2) A resource-aware accelerator architecture featuring latency-optimized streaming attention kernels and reusable linear operators, effectively balancing performance and resource consumption. Experimental results demonstrate that our accelerator achieves nearly 155 frames per second, a 5.35$\times$ improvement in throughput, and over $80\%$ energy reduction compared to state-of-the-art (SOTA) FPGA MoE accelerators, while maintaining $&lt;1\%$ accuracy loss across vision benchmarks. Our implementation is available at https://github.com/DJ000011/CoQMoE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08496v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiale Dong, Hao Wu, Zihao Wang, Wenqi Lou, Zhendong Zheng, Lei Gong, Chao Wang, Xuehai Zhou</dc:creator>
    </item>
    <item>
      <title>POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration</title>
      <link>https://arxiv.org/abs/2506.08785</link>
      <description>arXiv:2506.08785v1 Announce Type: new 
Abstract: The increasing complexity of AI models requires flexible hardware capable of supporting diverse precision formats, particularly for energy-constrained edge platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC engine that performs efficient multiply-accumulate operations using a unified data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The architecture incorporates a layer adaptive precision strategy to align computational accuracy with workload sensitivity, optimizing both performance and energy usage. PARV-CE integrates quantization-aware execution with a reconfigurable SIMD pipeline, enabling high-throughput processing with minimal overhead through hardware-software co-design. The results demonstrate up to 2x improvement in PDP and 3x reduction in resource usage compared to SoTA designs, while retaining accuracy within 1.8% FP32 baseline. The architecture supports both on-device training and inference across a range of workloads, including DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE incorporated POLARON as a scalable and energy-efficient solution for precision-adaptive AI acceleration at edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08785v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>eess.IV</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mukul Lokhande, Santosh Kumar Vishvakarma</dc:creator>
    </item>
    <item>
      <title>STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN Accelerator with Algorithm and Hardware Co-Design</title>
      <link>https://arxiv.org/abs/2506.08842</link>
      <description>arXiv:2506.08842v1 Announce Type: new 
Abstract: Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for their event-driven characteristics and high energy efficiency. However, the temporal dependency and irregularity of spikes present significant challenges for hardware parallel processing and data reuse, leading to some existing accelerators falling short in processing latency and energy efficiency. To overcome these challenges, we introduce the STI-SNN accelerator, designed for resource-constrained applications with high energy efficiency, flexibility, and low latency. The accelerator is designed through algorithm and hardware co-design. Firstly, STI-SNN can perform inference in a single timestep. At the algorithm level, we introduce a temporal pruning approach based on the temporal efficient training (TET) loss function. This approach alleviates spike disappearance during timestep reduction, maintains inference accuracy, and expands TET's application. In hardware design, we analyze data access patterns and adopt the output stationary (OS) dataflow, eliminating the need to store membrane potentials and access memory operations. Furthermore, based on the OS dataflow, we propose a compressed and sorted representation of spikes, then cached in the line buffer to reduce the memory access cost and improve reuse efficiency. Secondly, STI-SNN supports different convolution methods. By adjusting the computation mode of processing elements (PEs) and parameterizing the computation array, STI-SNN can accommodate lightweight models based on depthwise separable convolutions (DSCs), further enhancing hardware flexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer parallel processing. For inter-layer parallelism, we ...</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08842v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kainan Wang, Chengyi Yang, Chengting Yu, Yee Sin Ang, Bo Wang, Aili Wang</dc:creator>
    </item>
    <item>
      <title>PoSyn: Secure Power Side-Channel Aware Synthesis</title>
      <link>https://arxiv.org/abs/2506.08252</link>
      <description>arXiv:2506.08252v1 Announce Type: cross 
Abstract: Power Side-Channel (PSC) attacks exploit power consumption patterns to extract sensitive information, posing risks to cryptographic operations crucial for secure systems. Traditional countermeasures, such as masking, face challenges including complex integration during synthesis, substantial area overhead, and susceptibility to optimization removal during logic synthesis. To address these issues, we introduce PoSyn, a novel logic synthesis framework designed to enhance cryptographic hardware resistance against PSC attacks. Our method centers on optimal bipartite mapping of vulnerable RTL components to standard cells from the technology library, aiming to minimize PSC leakage. By utilizing a cost function integrating critical characteristics from both the RTL design and the standard cell library, we strategically modify mapping criteria during RTL-to-netlist conversion without altering design functionality. Furthermore, we theoretically establish that PoSyn minimizes mutual information leakage, strengthening its security against PSC vulnerabilities. We evaluate PoSyn across various cryptographic hardware implementations, including AES, RSA, PRESENT, and post-quantum cryptographic algorithms such as Saber and CRYSTALS-Kyber, at technology nodes of 65nm, 45nm, and 15nm. Experimental results demonstrate a substantial reduction in success rates for Differential Power Analysis (DPA) and Correlation Power Analysis (CPA) attacks, achieving lows of 3% and 6%, respectively. TVLA analysis further confirms that synthesized netlists exhibit negligible leakage. Additionally, compared to conventional countermeasures like masking and shuffling, PoSyn significantly lowers attack success rates, achieving reductions of up to 72%, while simultaneously enhancing area efficiency by as much as 3.79 times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08252v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amisha Srivastava, Samit S. Miftah, Hyunmin Kim, Debjit Pal, Kanad Basu</dc:creator>
    </item>
    <item>
      <title>"Test, Build, Deploy" -- A CI/CD Framework for Open-Source Hardware Designs</title>
      <link>https://arxiv.org/abs/2503.19180</link>
      <description>arXiv:2503.19180v3 Announce Type: replace 
Abstract: Addressing TedX, Amber Huffman made an impassioned case that "none of us is as smart as all of us" and that open-source hardware is the future. A major contribution to software quality, open source and otherwise, on the software side, is the systems design methodology of Continuous Integration and Delivery (CI/CD), which we propose to systematically bring to hardware designs and their specifications. To do so, we automatically generate specifications using specification mining, "a machine learning approach to discovering formal specifications" which dramatically impacted the ability of software engineers to achieve quality, verification, and security. Yet applying the same techniques to hardware is non-trivial. We present a technique for generalized, continuous integration (CI) of hardware specification designs that continually deploys (CD) a hardware specification. As a proof-of-concept, we demonstrate Myrtha, a cloud-based, specification generator based on established hardware and software quality tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19180v3</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Calvin Deutschbein, Aristotle Stassinopoulos</dc:creator>
    </item>
    <item>
      <title>An FPGA Compiler for On-the-Fly Adaptive CNN Deployment and Reconfiguration</title>
      <link>https://arxiv.org/abs/2504.08534</link>
      <description>arXiv:2504.08534v2 Announce Type: replace 
Abstract: We introduce ForgeMorph, a full-stack compiler for adaptive CNN deployment on FPGAs, combining design-time optimization with runtime reconfigurability. At compile time, the NeuroForge engine performs constraint-driven design space exploration, generating RTL mappings that are Pareto-optimal with respect to user-defined latency and resource budgets. Unlike existing FPGA compilers, which rely on static scheduling and manual tuning, NeuroForge leverages analytical performance models and multi-objective genetic algorithms to efficiently search large configuration spaces and propose highly optimized hardware implementations. At runtime, the NeuroMorph module enables dynamic reconfiguration of network width and depth without requiring redeployment. This is made possible by a novel training strategy, DistillCycle, which jointly trains the full model and its subnetworks using hierarchical knowledge distillation. As a result, each execution path maintains accuracy even under aggressive resource and power constraints. We demonstrate Forge-Morph on the Zynq-7100 using custom and benchmark models including MobileNetV2, ResNet-50, SqueezeNet, and YOLOv5. The system achieves up to 50x latency reduction and 32% lower power consumption at runtime, while matching or exceeding the efficiency of state-of-the-art compilers. ForgeMorph offers a unified solution for deployment scenarios that demand flexibility, performance, and hardware efficiency</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08534v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alaa Mazouz, Duc Han Le, Van-Tam Nguyen</dc:creator>
    </item>
    <item>
      <title>Chip Placement with Diffusion Models</title>
      <link>https://arxiv.org/abs/2407.12282</link>
      <description>arXiv:2407.12282v3 Announce Type: replace-cross 
Abstract: Macro placement is a vital step in digital circuit design that defines the physical location of large collections of components, known as macros, on a 2D chip. Because key performance metrics of the chip are determined by the placement, optimizing it is crucial. Existing learning-based methods typically fall short because of their reliance on reinforcement learning (RL), which is slow and struggles to generalize, requiring online training on each new circuit. Instead, we train a diffusion model capable of placing new circuits zero-shot, using guided sampling in lieu of RL to optimize placement quality. To enable such models to train at scale, we designed a capable yet efficient architecture for the denoising model, and propose a novel algorithm to generate large synthetic datasets for pre-training. To allow zero-shot transfer to real circuits, we empirically study the design decisions of our dataset generation algorithm, and identify several key factors enabling generalization. When trained on our synthetic data, our models generate high-quality placements on unseen, realistic circuits, achieving competitive performance on placement benchmarks compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12282v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vint Lee, Minh Nguyen, Leena Elzeiny, Chun Deng, Pieter Abbeel, John Wawrzynek</dc:creator>
    </item>
    <item>
      <title>"vcd2df" -- Leveraging Data Science Insights for Hardware Security Research</title>
      <link>https://arxiv.org/abs/2505.06470</link>
      <description>arXiv:2505.06470v3 Announce Type: replace-cross 
Abstract: In this work, we hope to expand the universe of security practitioners of open-source hardware by creating a bridge from hardware design languages (HDLs) to data science languages like Python and R through novel libraries that convert VCD (value change dump) files into data frames, the expected input type of the modern data science tools. We show how insights can be derived in high-level languages from register transfer level (RTL) trace data. Additionally, we show a promising future direction in hardware security research leveraging the parallelism of Spark to study transient execution CPU vulnerabilities, and provide reproducibility researchers via GitHub and Colab.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06470v3</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Calvin Deutschbein, Jimmy Ostler, Hriday Raj</dc:creator>
    </item>
  </channel>
</rss>
