<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Jun 2024 04:01:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimizing hydrogen and e-methanol production through Power-to-X integration in biogas plants</title>
      <link>https://arxiv.org/abs/2406.00442</link>
      <description>arXiv:2406.00442v1 Announce Type: new 
Abstract: The European Union strategy for net zero emissions relies on developing hydrogen and electro fuels infrastructure. These fuels will be crucial as energy carriers and balancing agents for renewable energy variability. Large scale production requires more renewable capacity, and various Power to X (PtX) concepts are emerging in renewable rich countries. However, sourcing renewable carbon to scale carbon based electro fuels is a significant challenge. This study explores a PtX hub that sources renewable CO2 from biogas plants, integrating renewable energy, hydrogen production, and methanol synthesis on site. This concept creates an internal market for energy and materials, interfacing with the external energy system. The size and operation of the PtX hub were optimized, considering integration with local energy systems and a potential hydrogen grid. The levelized costs of hydrogen and methanol were estimated for a 2030 start, considering new legislation on renewable fuels of non biological origin (RFNBOs). Our results show the PtX hub can rely mainly on on site renewable energy, selling excess electricity to the grid. A local hydrogen grid connection improves operations, and the behind the meter market lowers energy prices, buffering against market variability. We found methanol costs could be below 650 euros per ton and hydrogen production costs below 3 euros per kg, with standalone methanol plants costing 23 per cent more. The CO2 recovery to methanol production ratio is crucial, with over 90 per cent recovery requiring significant investment in CO2 and H2 storage. Overall, our findings support planning PtX infrastructures integrated with the agricultural sector as a cost effective way to access renewable carbon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00442v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Alamia, Behzad Partoon, Eoghan Rattigan, Gorm Brunn Andresen</dc:creator>
    </item>
    <item>
      <title>Financial Deepening and Economic Growth in Select Emerging Markets with Currency Board Systems: Theory and Evidence</title>
      <link>https://arxiv.org/abs/2406.00472</link>
      <description>arXiv:2406.00472v1 Announce Type: new 
Abstract: This paper investigates some indicators of financial development in select countries with currency board systems and raises some questions about the connection between financial development and growth in currency board systems. Most of those cases are long past episodes of what we would now call emerging markets. However, the paper also looks at Hong Kong, the currency board system that is one of the world's largest and most advanced financial markets. The global financial crisis of 2008 09 created doubts about the efficiency of financial markets in advanced economies, including in Hong Kong, and unsettled the previous consensus that a large financial sector would be more stable than a smaller one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00472v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>No. 87. The Johns Hopkins Institute for Applied Economics, Global Health, and the Study of Business Enterprise, 2017</arxiv:journal_reference>
      <dc:creator>Yujuan Qiu</dc:creator>
    </item>
    <item>
      <title>Cluster-robust jackknife and bootstrap inference for binary response models</title>
      <link>https://arxiv.org/abs/2406.00650</link>
      <description>arXiv:2406.00650v1 Announce Type: new 
Abstract: We study cluster-robust inference for binary response models. Inference based on the most commonly-used cluster-robust variance matrix estimator (CRVE) can be very unreliable. We study several alternatives. Conceptually the simplest of these, but also the most computationally demanding, involves jackknifing at the cluster level. We also propose a linearized version of the cluster-jackknife variance matrix estimator as well as linearized versions of the wild cluster bootstrap. The linearizations are based on empirical scores and are computationally efficient. Throughout we use the logit model as a leading example. We also discuss a new Stata software package called logitjack which implements these procedures. Simulation results strongly favor the new methods, and two empirical examples suggest that it can be important to use them in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00650v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James G. MacKinnon, Morten {\O}rregaard Nielsen, Matthew D. Webb</dc:creator>
    </item>
    <item>
      <title>LaLonde (1986) after Nearly Four Decades: Lessons Learned</title>
      <link>https://arxiv.org/abs/2406.00827</link>
      <description>arXiv:2406.00827v1 Announce Type: new 
Abstract: In 1986, Robert LaLonde published an article that compared nonexperimental estimates to experimental benchmarks LaLonde (1986). He concluded that the nonexperimental methods at the time could not systematically replicate experimental benchmarks, casting doubt on the credibility of these methods. Following LaLonde's critical assessment, there have been significant methodological advances and practical changes, including (i) an emphasis on estimators based on unconfoundedness, (ii) a focus on the importance of overlap in covariate distributions, (iii) the introduction of propensity score-based methods leading to doubly robust estimators, (iv) a greater emphasis on validation exercises to bolster research credibility, and (v) methods for estimating and exploiting treatment effect heterogeneity. To demonstrate the practical lessons from these advances, we reexamine the LaLonde data and the Imbens-Rubin-Sacerdote lottery data. We show that modern methods, when applied in contexts with significant covariate overlap, yield robust estimates for the adjusted differences between the treatment and control groups. However, this does not mean that these estimates are valid. To assess their credibility, validation exercises (such as placebo tests) are essential, whereas goodness of fit tests alone are inadequate. Our findings highlight the importance of closely examining the assignment process, carefully inspecting overlap, and conducting validation exercises when analyzing causal effects with nonexperimental data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00827v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guido Imbens, Yiqing Xu</dc:creator>
    </item>
    <item>
      <title>A Robust Residual-Based Test for Structural Changes in Factor Models</title>
      <link>https://arxiv.org/abs/2406.00941</link>
      <description>arXiv:2406.00941v1 Announce Type: new 
Abstract: In this paper, we propose an easy-to-implement residual-based specification testing procedure for detecting structural changes in factor models, which is powerful against both smooth and abrupt structural changes with unknown break dates. The proposed test is robust against the over-specified number of factors, and serially and cross-sectionally correlated error processes. A new central limit theorem is given for the quadratic forms of panel data with dependence over both dimensions, thereby filling a gap in the literature. We establish the asymptotic properties of the proposed test statistic, and accordingly develop a simulation-based scheme to select critical value in order to improve finite sample performance. Through extensive simulations and a real-world application, we confirm our theoretical results and demonstrate that the proposed test exhibits desirable size and power in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00941v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Bin Peng, Liangjun Su, Yayi Yan</dc:creator>
    </item>
    <item>
      <title>Random Subspace Local Projections</title>
      <link>https://arxiv.org/abs/2406.01002</link>
      <description>arXiv:2406.01002v1 Announce Type: new 
Abstract: We show how random subspace methods can be adapted to estimating local projections with many controls. Random subspace methods have their roots in the machine learning literature and are implemented by averaging over regressions estimated over different combinations of subsets of these controls. We document three key results: (i) Our approach can successfully recover the impulse response functions across Monte Carlo experiments representative of different macroeconomic settings and identification schemes. (ii) Our results suggest that random subspace methods are more accurate than other dimension reduction methods if the underlying large dataset has a factor structure similar to typical macroeconomic datasets such as FRED-MD. (iii) Our approach leads to differences in the estimated impulse response functions relative to benchmark methods when applied to two widely studied empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01002v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Viet Hoang Dinh, Didier Nibbering, Benjamin Wong</dc:creator>
    </item>
    <item>
      <title>Far beyond day-ahead with econometric models for electricity price forecasting</title>
      <link>https://arxiv.org/abs/2406.00326</link>
      <description>arXiv:2406.00326v1 Announce Type: cross 
Abstract: The surge in global energy prices during the recent energy crisis, which peaked in 2022, has intensified the need for mid-term to long-term forecasting for hedging and valuation purposes. This study analyzes the statistical predictability of power prices before, during, and after the energy crisis, using econometric models with an hourly resolution. To stabilize the model estimates, we define fundamentally derived coefficient bounds. We provide an in-depth analysis of the unit root behavior of the power price series, showing that the long-term stochastic trend is explained by the prices of commodities used as fuels for power generation: gas, coal, oil, and emission allowances (EUA). However, as the forecasting horizon increases, spurious effects become extremely relevant, leading to highly significant but economically meaningless results. To mitigate these spurious effects, we propose the "current" model: estimating the current same-day relationship between power prices and their regressors and projecting this relationship into the future. This flexible and interpretable method is applied to hourly German day-ahead power prices for forecasting horizons up to one year ahead, utilizing a combination of regularized regression methods and generalized additive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00326v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Ghelasi, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Portfolio Optimization with Robust Covariance and Conditional Value-at-Risk Constraints</title>
      <link>https://arxiv.org/abs/2406.00610</link>
      <description>arXiv:2406.00610v1 Announce Type: cross 
Abstract: The measure of portfolio risk is an important input of the Markowitz framework. In this study, we explored various methods to obtain a robust covariance estimators that are less susceptible to financial data noise. We evaluated the performance of large-cap portfolio using various forms of Ledoit Shrinkage Covariance and Robust Gerber Covariance matrix during the period of 2012 to 2022. Out-of-sample performance indicates that robust covariance estimators can outperform the market capitalization-weighted benchmark portfolio, particularly during bull markets. The Gerber covariance with Mean-Absolute-Deviation (MAD) emerged as the top performer. However, robust estimators do not manage tail risk well under extreme market conditions, for example, Covid-19 period. When we aim to control for tail risk, we should add constraint on Conditional Value-at-Risk (CVaR) to make more conservative decision on risk exposure. Additionally, we incorporated unsupervised clustering algorithm K-means to the optimization algorithm (i.e. Nested Clustering Optimization, NCO). It not only helps mitigate numerical instability of the optimization algorithm, but also contributes to lower drawdown as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00610v1</guid>
      <category>q-fin.PM</category>
      <category>econ.EM</category>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiqin Zhou</dc:creator>
    </item>
    <item>
      <title>On the modelling and prediction of high-dimensional functional time series</title>
      <link>https://arxiv.org/abs/2406.00700</link>
      <description>arXiv:2406.00700v1 Announce Type: cross 
Abstract: We propose a two-step procedure to model and predict high-dimensional functional time series, where the number of function-valued time series $p$ is large in relation to the length of time series $n$. Our first step performs an eigenanalysis of a positive definite matrix, which leads to a one-to-one linear transformation for the original high-dimensional functional time series, and the transformed curve series can be segmented into several groups such that any two subseries from any two different groups are uncorrelated both contemporaneously and serially. Consequently in our second step those groups are handled separately without the information loss on the overall linear dynamic structure. The second step is devoted to establishing a finite-dimensional dynamical structure for all the transformed functional time series within each group. Furthermore the finite-dimensional structure is represented by that of a vector time series. Modelling and forecasting for the original high-dimensional functional time series are realized via those for the vector time series in all the groups. We investigate the theoretical properties of our proposed methods, and illustrate the finite-sample performance through both extensive simulation and two real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00700v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Qin Fang, Xinghao Qiao, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Granger Causality for Climatic Attribution</title>
      <link>https://arxiv.org/abs/2302.03996</link>
      <description>arXiv:2302.03996v2 Announce Type: replace 
Abstract: In this paper we test for Granger causality in high-dimensional vector autoregressive models (VARs) to disentangle and interpret the complex causal chains linking radiative forcings and global temperatures. By allowing for high dimensionality in the model, we can enrich the information set with relevant natural and anthropogenic forcing variables to obtain reliable causal relations. This provides a step forward from existing climatology literature, which has mostly treated these variables in isolation in small models. Additionally, our framework allows to disregard the order of integration of the variables by directly estimating the VAR in levels, thus avoiding accumulating biases coming from unit-root and cointegration tests. This is of particular appeal for climate time series which are well known to contain stochastic trends and long memory. We are thus able to establish causal networks linking radiative forcings to global temperatures and to connect radiative forcings among themselves, thereby allowing for tracing the path of dynamic causal effects through the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03996v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marina Friedrich, Luca Margaritella, Stephan Smeekes</dc:creator>
    </item>
    <item>
      <title>Data Scaling Effect of Deep Learning in Financial Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2309.02072</link>
      <description>arXiv:2309.02072v5 Announce Type: replace 
Abstract: For years, researchers investigated the applications of deep learning in forecasting financial time series. However, they continued to rely on the conventional econometric approach for model training that optimizes the deep learning models on individual assets. This study highlights the importance of global training, where the deep learning model is optimized across a wide spectrum of stocks. Focusing on stock volatility forecasting as an exemplar, we show that global training is not only beneficial but also necessary for deep learning-based financial time series forecasting. We further demonstrate that, given a sufficient amount of training data, a globally trained deep learning model is capable of delivering accurate zero-shot forecasts for any stocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02072v5</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>q-fin.CP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chen Liu, Minh-Ngoc Tran, Chao Wang, Richard Gerlach, Robert Kohn</dc:creator>
    </item>
    <item>
      <title>Testing for Stationary or Persistent Coefficient Randomness in Predictive Regressions</title>
      <link>https://arxiv.org/abs/2309.04926</link>
      <description>arXiv:2309.04926v5 Announce Type: replace 
Abstract: This study considers tests for coefficient randomness in predictive regressions. Our focus is on how tests for coefficient randomness are influenced by the persistence of random coefficient. We show that when the random coefficient is stationary, or I(0), Nyblom's (1989) LM test loses its optimality (in terms of power), which is established against the alternative of integrated, or I(1), random coefficient. We demonstrate this by constructing a test that is more powerful than the LM test when the random coefficient is stationary, although the test is dominated in terms of power by the LM test when the random coefficient is integrated. The power comparison is made under the sequence of local alternatives that approaches the null hypothesis at different rates depending on the persistence of the random coefficient and which test is considered. We revisit an earlier empirical research and apply the tests considered in this study to the U.S. stock returns data. The result mostly reverses the earlier finding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04926v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikihito Nishi</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of a Semiparametric Logit Model using Network Data</title>
      <link>https://arxiv.org/abs/2310.07151</link>
      <description>arXiv:2310.07151v2 Announce Type: replace 
Abstract: This paper studies the identification and estimation of a semiparametric binary network model in which the unobserved social characteristic is endogenous, that is, the unobserved individual characteristic influences both the binary outcome of interest and how links are formed within the network. The exact functional form of the latent social characteristic is not known. The proposed estimators are obtained based on matching pairs of agents whose network formation distributions are the same. The consistency and the asymptotic distribution of the estimators are proposed. The finite sample properties of the proposed estimators in a Monte-Carlo simulation are assessed. We conclude this study with an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07151v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Brice Romuald Gueyap Kounga</dc:creator>
    </item>
    <item>
      <title>Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal</title>
      <link>https://arxiv.org/abs/2310.08672</link>
      <description>arXiv:2310.08672v2 Announce Type: replace 
Abstract: In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use "nudges" to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predicted probability. Nonetheless, targeting on low baseline outcomes is common in practice, for example because the relationship between individual characteristics and treatment effects is often difficult or impossible to estimate with historical data. We propose hybrid approaches that incorporate the strengths of both predictive approaches (accurate estimation) and causal approaches (correct criterion); we show that targeting intermediate baseline outcomes is most effective in our specific application, while targeting based on low baseline outcomes is detrimental. In one year of the experiment, nudging all students improved early filing by an average of 6.4 percentage points over a baseline average of 37% filing, and we estimate that targeting half of the students using our preferred policy attains around 75% of this benefit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08672v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susan Athey, Niall Keleher, Jann Spiess</dc:creator>
    </item>
    <item>
      <title>Kernel Ridge Riesz Representers: Generalization Error and Mis-specification</title>
      <link>https://arxiv.org/abs/2102.11076</link>
      <description>arXiv:2102.11076v3 Announce Type: replace-cross 
Abstract: Kernel balancing weights provide confidence intervals for average treatment effects, based on the idea of balancing covariates for the treated group and untreated group in feature space, often with ridge regularization. Previous works on the classical kernel ridge balancing weights have certain limitations: (i) not articulating generalization error for the balancing weights, (ii) typically requiring correct specification of features, and (iii) providing inference for only average effects.
  I interpret kernel balancing weights as kernel ridge Riesz representers (KRRR) and address these limitations via a new characterization of the counterfactual effective dimension. KRRR is an exact generalization of kernel ridge regression and kernel ridge balancing weights. I prove strong properties similar to kernel ridge regression: population $L_2$ rates controlling generalization error, and a standalone closed form solution that can interpolate. The framework relaxes the stringent assumption that the underlying regression model is correctly specified by the features. It extends inference beyond average effects to heterogeneous effects, i.e. causal functions. I use KRRR to infer heterogeneous treatment effects, by age, of 401(k) eligibility on assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.11076v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Singh</dc:creator>
    </item>
    <item>
      <title>Stochastic Equilibrium the Lucas Critique and Keynesian Economics</title>
      <link>https://arxiv.org/abs/2312.16214</link>
      <description>arXiv:2312.16214v4 Announce Type: replace-cross 
Abstract: In this paper, a mathematically rigorous solution overturns existing wisdom regarding New Keynesian Dynamic Stochastic General Equilibrium. I develop a formal concept of stochastic equilibrium. I prove uniqueness and necessity, when agents are patient, with general application. Existence depends on appropriately specified eigenvalue conditions. Otherwise, no solution of any kind exists. I construct the equilibrium with Calvo pricing. I provide novel comparative statics with the non-stochastic model of mathematical significance. I uncover a bifurcation between neighbouring stochastic systems and approximations taken from the Zero Inflation Non-Stochastic Steady State (ZINSS). The correct Phillips curve agrees with the zero limit from the trend inflation framework. It contains a large lagged inflation coefficient and a small response to expected inflation. Price dispersion can be first or second order depending how shocks are scaled. The response to the output gap is always muted and is zero at standard parameters. A neutrality result is presented to explain why and align Calvo with Taylor pricing. Present and lagged demand shocks enter the Phillips curve so there is no Divine Coincidence and the system is identified from structural shocks alone. The lagged inflation slope is increasing in the inflation response, embodying substantive policy trade-offs. The Taylor principle is reversed, inactive settings are necessary, pointing towards inertial policy. The observational equivalence idea of the Lucas critique is disproven. The bifurcation results from the breakdown of the constraints implied by lagged nominal rigidity, associated with cross-equation cancellation possible only at ZINSS. There is a dual relationship between restrictions on the econometrician and constraints on repricing firms. Thus, if the model is correct, goodness of fit will jump.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16214v4</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <category>math.AT</category>
      <category>math.GN</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Staines</dc:creator>
    </item>
    <item>
      <title>Multiply-Robust Causal Change Attribution</title>
      <link>https://arxiv.org/abs/2404.08839</link>
      <description>arXiv:2404.08839v2 Announce Type: replace-cross 
Abstract: Comparing two samples of data, we observe a change in the distribution of an outcome variable. In the presence of multiple explanatory variables, how much of the change can be explained by each possible cause? We develop a new estimation strategy that, given a causal model, combines regression and re-weighting methods to quantify the contribution of each causal mechanism. Our proposed methodology is multiply robust, meaning that it still recovers the target parameter under partial misspecification. We prove that our estimator is consistent and asymptotically normal. Moreover, it can be incorporated into existing frameworks for causal attribution, such as Shapley values, which will inherit the consistency and large-sample distribution properties. Our method demonstrates excellent performance in Monte Carlo simulations, and we show its usefulness in an empirical application. Our method is implemented as part of the Python library DoWhy (arXiv:2011.04216, arXiv:2206.06821).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08839v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024</arxiv:journal_reference>
      <dc:creator>Victor Quintas-Martinez, Mohammad Taha Bahadori, Eduardo Santiago, Jeff Mu, Dominik Janzing, David Heckerman</dc:creator>
    </item>
    <item>
      <title>The ARR2 prior: flexible predictive prior definition for Bayesian auto-regressions</title>
      <link>https://arxiv.org/abs/2405.19920</link>
      <description>arXiv:2405.19920v2 Announce Type: replace-cross 
Abstract: We present the ARR2 prior, a joint prior over the auto-regressive components in Bayesian time-series models and their induced $R^2$. Compared to other priors designed for times-series models, the ARR2 prior allows for flexible and intuitive shrinkage. We derive the prior for pure auto-regressive models, and extend it to auto-regressive models with exogenous inputs, and state-space models. Through both simulations and real-world modelling exercises, we demonstrate the efficacy of the ARR2 prior in improving sparse and reliable inference, while showing greater inference quality and predictive performance than other shrinkage priors. An open-source implementation of the prior is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19920v2</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kohns, Noa Kallioinen, Yann McLatchie, Aki Vehtari</dc:creator>
    </item>
  </channel>
</rss>
