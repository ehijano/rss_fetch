<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Jul 2025 04:01:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evolutionary and Coevolutionary Multi-Agent Design Choices and Dynamics</title>
      <link>https://arxiv.org/abs/2507.05534</link>
      <description>arXiv:2507.05534v1 Announce Type: new 
Abstract: We investigate two representation alternatives for the controllers of teams of cyber agents. We combine these controller representations with different evolutionary algorithms, one of which introduces a novel LLM-supported mutation operator. Using a cyber security scenario, we evaluate agent learning when one side is trained to compete against a side that does not evolve and when two sides coevolve with each other. This allows us to quantify the relative merits and tradeoffs of representation and algorithm combinations in terms of team performance. Our versions of grammatical evolution algorithms using grammars that allow a controller to be expressed in code-like logic can achieve the best team performance. The scenario also allows us to compare the performance impact and dynamics of coevolution versus evolution under different combinations. Across the algorithms and representations, we observe that coevolution reduces the performance highs and lows of both sides while it induces fluctuations on both sides. In contrast, when only one-side is optimized, performance peaks are higher and is more sustained than when both sides are optimized with coevolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05534v1</guid>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik Hemberg, Eric Liu, Lucille Fuller, Stephen Moskal, Una-May O'Reilly</dc:creator>
    </item>
    <item>
      <title>A Universal Framework for Large-Scale Multi-Objective Optimization Based on Particle Drift and Diffusion</title>
      <link>https://arxiv.org/abs/2507.05847</link>
      <description>arXiv:2507.05847v1 Announce Type: new 
Abstract: Large-scale multi-objective optimization poses challenges to existing evolutionary algorithms in maintaining the performances of convergence and diversity because of high dimensional decision variables. Inspired by the motion of particles in physics, we propose a universal framework for large-scale multi-objective optimization based on particle drift and diffusion to solve these challenges in this paper. This framework innovatively divides the optimization process into three sub-stages: two coarse-tuning sub-stages and one fine-tuning sub-stage. Different strategies of drift-diffusion operations are performed on the guiding solutions according to the current sub-stage, ingeniously simulating the movement of particles under diverse environmental conditions. Finally, representative evolutionary algorithms are embedded into the proposed framework, and their effectiveness are evaluated through comparative experiments on various large-scale multi-objective problems with 1000 to 5000 decision variables. Moreover, comparative algorithms are conducted on neural network training problems to validate the effectiveness of the proposed framework in the practical problems. The experimental results demonstrate that the framework proposed in this paper significantly enhances the performance of convergence and diversity of MOEAs, and improves the computational efficiency of algorithms in solving large-scale multi-objective optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05847v1</guid>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia-Cheng Li, Min-Rong Chen, Guo-Qiang Zeng, Jian Weng, Man Wang, Jia-Lin Mai</dc:creator>
    </item>
    <item>
      <title>ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy</title>
      <link>https://arxiv.org/abs/2507.05279</link>
      <description>arXiv:2507.05279v1 Announce Type: cross 
Abstract: We introduce a tool designed to improve the capabilities of Large Language Models (LLMs) in assisting with code development using the ReservoirPy library, as well as in answering complex questions in the field of Reservoir Computing. By incorporating external knowledge through Retrieval-Augmented Generation (RAG) and knowledge graphs, our approach aims to reduce hallucinations and increase the factual accuracy of generated responses. The system provides an interactive experience similar to ChatGPT, tailored specifically for ReservoirPy, enabling users to write, debug, and understand Python code while accessing reliable domain-specific insights. In our evaluation, while proprietary models such as ChatGPT-4o and NotebookLM performed slightly better on general knowledge questions, our model outperformed them on coding tasks and showed a significant improvement over its base model, Codestral-22B.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05279v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Virgile Boraud (Mnemosyne), Yannis Bendi-Ouis (Mnemosyne), Paul Bernard (Mnemosyne), Xavier Hinaut (Mnemosyne)</dc:creator>
    </item>
    <item>
      <title>Dataless Neural Networks for Resource-Constrained Project Scheduling</title>
      <link>https://arxiv.org/abs/2507.05322</link>
      <description>arXiv:2507.05322v1 Announce Type: cross 
Abstract: Dataless neural networks represent a paradigm shift in applying neural architectures to combinatorial optimization problems, eliminating the need for training datasets by encoding problem instances directly into network parameters. Despite the pioneering work of Alkhouri et al. (2022) demonstrating the viability of dataless approaches for the Maximum Independent Set problem, our comprehensive literature review reveals that no published work has extended these methods to the Resource-Constrained Project Scheduling Problem (RCPSP). This paper addresses this gap by presenting the first dataless neural network approach for RCPSP, providing a complete mathematical framework that transforms discrete scheduling constraints into differentiable objectives suitable for gradient-based optimization. Our approach leverages smooth relaxations and automatic differentiation to unlock GPU parallelization for project scheduling, traditionally a domain of sequential algorithms. We detail the mathematical formulation for both precedence and renewable resource constraints, including a memory-efficient dense time-grid representation. Implementation and comprehensive experiments on PSPLIB benchmark instances (J30, J60, and J120) are currently underway, with empirical results to be reported in an updated version of this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05322v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Bara</dc:creator>
    </item>
    <item>
      <title>Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models</title>
      <link>https://arxiv.org/abs/2507.05565</link>
      <description>arXiv:2507.05565v1 Announce Type: cross 
Abstract: Assessing the trustworthiness of Large Language Models (LLMs), such as robustness, has garnered significant attention. Recently, metamorphic testing that defines Metamorphic Relations (MRs) has been widely applied to evaluate the robustness of LLM executions. However, the MR-based robustness testing still requires a scalable number of MRs, thereby necessitating the optimization of selecting MRs. Most extant LLM testing studies are limited to automatically generating test cases (i.e., MRs) to enhance failure detection. Additionally, most studies only considered a limited test space of single perturbation MRs in their evaluation of LLMs. In contrast, our paper proposes a search-based approach for optimizing the MR groups to maximize failure detection and minimize the LLM execution cost. Moreover, our approach covers the combinatorial perturbations in MRs, facilitating the expansion of test space in the robustness assessment. We have developed a search process and implemented four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel encoding to solve the MR selection problem in the LLM robustness testing. We conducted comparative experiments on the four search algorithms along with a random search, using two major LLMs with primary Text-to-Text tasks. Our statistical and empirical investigation revealed two key findings: (1) the MOEA/D algorithm performed the best in optimizing the MR space for LLM robustness testing, and (2) we identified silver bullet MRs for the LLM robustness testing, which demonstrated dominant capabilities in confusing LLMs across different Text-to-Text tasks. In LLM robustness assessment, our research sheds light on the fundamental problem for optimized testing and provides insights into search-based solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05565v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sangwon Hyun, Shaukat Ali, M. Ali Babar</dc:creator>
    </item>
    <item>
      <title>Model-free Optical Processors using In Situ Reinforcement Learning with Proximal Policy Optimization</title>
      <link>https://arxiv.org/abs/2507.05583</link>
      <description>arXiv:2507.05583v1 Announce Type: cross 
Abstract: Optical computing holds promise for high-speed, energy-efficient information processing, with diffractive optical networks emerging as a flexible platform for implementing task-specific transformations. A challenge, however, is the effective optimization and alignment of the diffractive layers, which is hindered by the difficulty of accurately modeling physical systems with their inherent hardware imperfections, noise, and misalignments. While existing in situ optimization methods offer the advantage of direct training on the physical system without explicit system modeling, they are often limited by slow convergence and unstable performance due to inefficient use of limited measurement data. Here, we introduce a model-free reinforcement learning approach utilizing Proximal Policy Optimization (PPO) for the in situ training of diffractive optical processors. PPO efficiently reuses in situ measurement data and constrains policy updates to ensure more stable and faster convergence. We experimentally validated our method across a range of in situ learning tasks, including targeted energy focusing through a random diffuser, holographic image generation, aberration correction, and optical image classification, demonstrating in each task better convergence and performance. Our strategy operates directly on the physical system and naturally accounts for unknown real-world imperfections, eliminating the need for prior system knowledge or modeling. By enabling faster and more accurate training under realistic experimental constraints, this in situ reinforcement learning approach could offer a scalable framework for various optical and physical systems governed by complex, feedback-driven dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05583v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>physics.app-ph</category>
      <category>physics.optics</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhang Li, Shiqi Chen, Tingyu Gong, Aydogan Ozcan</dc:creator>
    </item>
    <item>
      <title>Exploring Gain-Doped-Waveguide-Synapse for Neuromorphic Applications: A Pulsed Pump-Signal Approach</title>
      <link>https://arxiv.org/abs/2507.05931</link>
      <description>arXiv:2507.05931v1 Announce Type: cross 
Abstract: Neuromorphic computing promises to transform AI systems by enabling them to perceive, respond to, and adapt swiftly and accurately to dynamic data and user interactions. However, traditional silicon-based and hybrid electronic technologies for artificial neurons constrain neuromorphic processors in terms of flexibility, scalability, and energy efficiency. In this study, we pioneer the use of Doped-Gain-Layer-on-Waveguide-Synapses for bio-inspired neurons, utilizing a pulsed pump-signal mechanism to enhance neuromorphic computation. This approach addresses critical challenges in scalability and energy efficiency inherent in current technologies.
  We introduce the concept of Gain on Waveguide Dynamics for synapses, demonstrating how non-linear pulse transformations of input probe signals occur under various pump-probe configurations. Our findings reveal that primarily properties of pulse amplitude, period as well material properties such as doping densities and population dynamics influence strongly the generation of spiking responses that emulate neuronal behaviour and effectively how computational logic is. By harnessing the complex interactions of asynchronous spiking pump techniques and ion densities in excited states, our method produces event-driven responses that mirror natural neuronal functions. This gain-enhanced environment supports short-term memory capabilities alongside essential characteristics like asynchronous spike generation, threshold operation, and temporal integration, foundational to brain-inspired spiking neural network paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05931v1</guid>
      <category>physics.optics</category>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Otupiri, Ripalta Stabile</dc:creator>
    </item>
    <item>
      <title>A Differential Evolution Algorithm with Neighbor-hood Mutation for DOA Estimation</title>
      <link>https://arxiv.org/abs/2507.06020</link>
      <description>arXiv:2507.06020v1 Announce Type: cross 
Abstract: Two-dimensional (2D) Multiple Signal Classification algorithm is a powerful technique for high-resolution direction-of-arrival (DOA) estimation in array signal processing. However, the exhaustive search over the 2D an-gular domain leads to high computa-tional cost, limiting its applicability in real-time scenarios. In this work, we reformulate the peak-finding process as a multimodal optimization prob-lem, and propose a Differential Evolu-tion algorithm with Neighborhood Mutation (DE-NM) to efficiently lo-cate multiple spectral peaks without requiring dense grid sampling. Simu-lation results demonstrate that the proposed method achieves comparable estimation accuracy to the traditional grid search, while significantly reduc-ing computation time. This strategy presents a promising solution for real-time, high-resolution DOA estimation in practical applications. The imple-mentation code is available at https://github.com/zzb-nice/DOA_multimodel_optimize.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06020v1</guid>
      <category>eess.SP</category>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Zhou, Kaijie Xu, Yinghui Quan, Mengdao Xing</dc:creator>
    </item>
    <item>
      <title>Practical design and performance of physical reservoir computing using hysteresis</title>
      <link>https://arxiv.org/abs/2507.06063</link>
      <description>arXiv:2507.06063v1 Announce Type: cross 
Abstract: Physical reservoir computing is an innovative idea for using physical phenomena as computational resources. Recent research has revealed that information processing techniques can improve the performance, but for practical applications, it is equally important to study the level of performance with a simple design that is easy to construct experimentally. We focus on a reservoir composed of independent hysteretic systems as a model suitable for the practical implementation of physical reservoir computing. In this paper, we discuss the appropriate design of this reservoir, its performance, and its limitations. This research will serve as a practical guideline for constructing hysteresis-based reservoirs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06063v1</guid>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhei Yamada</dc:creator>
    </item>
    <item>
      <title>evortran: a modern Fortran package for genetic algorithms with applications from LHC data fitting to LISA signal reconstruction</title>
      <link>https://arxiv.org/abs/2507.06082</link>
      <description>arXiv:2507.06082v1 Announce Type: cross 
Abstract: evortran is a modern Fortran library designed for high-performance genetic algorithms and evolutionary optimization. evortran can be used to tackle a wide range of problems in high-energy physics and beyond, such as derivative-free parameter optimization, complex search taks, parameter scans and fitting experimental data under the presence of instrumental noise. The library is built as an fpm package with flexibility and efficiency in mind, while also offering a simple installation process, user interface and integration into existing Fortran programs. evortran offers a variety of selection, crossover, mutation and elitism strategies, with which users can tailor an evolutionary algorithm to their specific needs. evortran supports different abstraction levels: from operating directly on individuals and populations, to running full evolutionary cycles, and even enabling migration between independently evolving populations to enhance convergence and maintain diversity. In this paper, we present the functionality of the evortran library, demonstrate its capabilities with example benchmark applications, and compare its performance with existing genetic algorithm frameworks. As physics-motivated applications, we use evortran to confront extended Higgs sectors with LHC data and to reconstruct gravitational wave spectra and the underlying physical parameters from LISA mock data, demonstrating its effectiveness in realistic, data-driven scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06082v1</guid>
      <category>hep-ph</category>
      <category>cs.NE</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Biek\"otter</dc:creator>
    </item>
    <item>
      <title>SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance</title>
      <link>https://arxiv.org/abs/2507.06148</link>
      <description>arXiv:2507.06148v1 Announce Type: cross 
Abstract: In this study, SoftReMish, a new activation function designed to improve the performance of convolutional neural networks (CNNs) in image classification tasks, is proposed. Using the MNIST dataset, a standard CNN architecture consisting of two convolutional layers, max pooling, and fully connected layers was implemented. SoftReMish was evaluated against popular activation functions including ReLU, Tanh, and Mish by replacing the activation function in all trainable layers. The model performance was assessed in terms of minimum training loss and maximum validation accuracy. Results showed that SoftReMish achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%), outperforming all other functions tested. These findings demonstrate that SoftReMish offers better convergence behavior and generalization capability, making it a promising candidate for visual recognition tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06148v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mustafa Bayram G\"ucen</dc:creator>
    </item>
    <item>
      <title>On the Problem Characteristics of Multi-objective Pseudo-Boolean Functions in Runtime Analysis</title>
      <link>https://arxiv.org/abs/2503.19166</link>
      <description>arXiv:2503.19166v2 Announce Type: replace 
Abstract: Recently, there has been growing interest within the theoretical community in analytically studying multi-objective evolutionary algorithms. This runtime analysis-focused research can help formally understand algorithm behaviour, explain empirical observations, and provide theoretical insights to support algorithm development and exploration. However, the test problems commonly used in the theoretical analysis are predominantly limited to problems with heavy "artificial" characteristics (e.g., symmetric, homogeneous objectives and linear Pareto fronts), which may not be able to well represent realistic scenarios. In this paper, we first discuss commonly used multi-objective functions in the theory domain and systematically review their features, limitations and implications to practical use. Then, we present several new functions with more realistic features, such as heterogenous objectives, local optimality and nonlinearity of the Pareto front, through simply mixing and matching classical single-objective functions in the area (e.g., LeadingOnes, Jump and RoyalRoad). We hope these functions can enrich the existing test problem suites, and strengthen the connection between theoretic and practical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19166v2</guid>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zimin Liang, Miqing Li</dc:creator>
    </item>
    <item>
      <title>Enhancing Generalization of Spiking Neural Networks Through Temporal Regularization</title>
      <link>https://arxiv.org/abs/2506.19256</link>
      <description>arXiv:2506.19256v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) have received widespread attention due to their event-driven and low-power characteristics, making them particularly effective for processing event-based neuromorphic data. Recent studies have shown that directly trained SNNs suffer from severe overfitting issues due to the limited scale of neuromorphic datasets and the gradient mismatching problem, which fundamentally constrain their generalization performance. In this paper, we propose a temporal regularization training (TRT) method by introducing a time-dependent regularization mechanism to enforce stronger constraints on early timesteps. We compare the performance of TRT with other state-of-the-art methods performance on datasets including CIFAR10/100, ImageNet100, DVS-CIFAR10, and N-Caltech101. To validate the effectiveness of TRT, we conducted ablation studies and analyses including loss landscape visualization and learning curve analysis, demonstrating that TRT can effectively mitigate overfitting and flatten the training loss landscape, thereby enhancing generalizability. Furthermore, we establish a theoretical interpretation of TRT's temporal regularization mechanism based on the results of Fisher information analysis. We analyze the temporal information dynamics inside SNNs by tracking Fisher information during the TRT training process, revealing the Temporal Information Concentration (TIC) phenomenon, where Fisher information progressively concentrates in early timesteps. The time-decaying regularization mechanism implemented in TRT effectively guides the network to learn robust features in early timesteps with rich information, thereby leading to significant improvements in model generalization. Code is available at https://github.com/ZBX05/Temporal-Regularization-Training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19256v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boxuan Zhang, Zhen Xu, Kuan Tao</dc:creator>
    </item>
    <item>
      <title>A Novel Hybrid Grey Wolf Differential Evolution Algorithm</title>
      <link>https://arxiv.org/abs/2507.03022</link>
      <description>arXiv:2507.03022v2 Announce Type: replace 
Abstract: Grey wolf optimizer (GWO) is a nature-inspired stochastic meta-heuristic of the swarm intelligence field that mimics the hunting behavior of grey wolves. Differential evolution (DE) is a popular stochastic algorithm of the evolutionary computation field that is well suited for global optimization. In this part, we introduce a new algorithm based on the hybridization of GWO and two DE variants, namely the GWO-DE algorithm. We evaluate the new algorithm by applying various numerical benchmark functions. The numerical results of the comparative study are quite satisfactory in terms of performance and solution quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03022v2</guid>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>physics.app-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis D. Bougas, Pavlos Doanis, Maria S. Papadopoulou, Achilles D. Boursianis, Sotirios P. Sotiroudis, Zaharias D. Zaharis, George Koudouridis, Panagiotis Sarigiannidis, Mohammad Abdul Matint, George Karagiannidis, Sotirios K. Goudos</dc:creator>
    </item>
    <item>
      <title>Balancing Efficiency and Expressiveness: Subgraph GNNs with Walk-Based Centrality</title>
      <link>https://arxiv.org/abs/2501.03113</link>
      <description>arXiv:2501.03113v2 Announce Type: replace-cross 
Abstract: Subgraph GNNs have emerged as promising architectures that overcome the expressiveness limitations of Graph Neural Networks (GNNs) by processing bags of subgraphs. Despite their compelling empirical performance, these methods are afflicted by a high computational complexity: they process bags whose size grows linearly in the number of nodes, hindering their applicability to larger graphs. In this work, we propose an effective and easy-to-implement approach to dramatically alleviate the computational cost of Subgraph GNNs and unleash broader applications thereof. Our method, dubbed HyMN, leverages walk-based centrality measures to sample a small number of relevant subgraphs and drastically reduce the bag size. By drawing a connection to perturbation analysis, we highlight the strength of the proposed centrality-based subgraph sampling, and further prove that these walk-based centralities can be additionally used as Structural Encodings for improved discriminative power. A comprehensive set of experimental results demonstrates that HyMN provides an effective synthesis of expressiveness, efficiency, and downstream performance, unlocking the application of Subgraph GNNs to dramatically larger graphs. Not only does our method outperform more sophisticated subgraph sampling approaches, it is also competitive, and sometimes better, than other state-of-the-art approaches for a fraction of their runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03113v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Southern, Yam Eitan, Guy Bar-Shalom, Michael Bronstein, Haggai Maron, Fabrizio Frasca</dc:creator>
    </item>
    <item>
      <title>When Federated Learning Meets Quantum Computing: Survey and Research Opportunities</title>
      <link>https://arxiv.org/abs/2504.08814</link>
      <description>arXiv:2504.08814v2 Announce Type: replace-cross 
Abstract: Quantum Federated Learning (QFL) is an emerging field that harnesses advances in Quantum Computing (QC) to improve the scalability and efficiency of decentralized Federated Learning (FL) models. This paper provides a systematic and comprehensive survey of the emerging problems and solutions when FL meets QC, from research protocol to a novel taxonomy, particularly focusing on both quantum and federated limitations, such as their architectures, Noisy Intermediate Scale Quantum (NISQ) devices, and privacy preservation, so on. This work explores key developments and integration strategies, along with the impact of quantum computing on FL, keeping a sharp focus on hybrid quantum-classical approaches. The paper offers an in-depth understanding of how the strengths of QC, such as gradient hiding, state entanglement, quantum key distribution, quantum security, and quantum-enhanced differential privacy, have been integrated into FL to ensure the privacy of participants in an enhanced, fast, and secure framework. Finally, this study proposes potential future directions to address the identified research gaps and challenges, aiming to inspire faster and more secure QFL models for practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08814v2</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aakar Mathur, Ashish Gupta, Sajal K. Das</dc:creator>
    </item>
  </channel>
</rss>
