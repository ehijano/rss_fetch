<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Jul 2024 02:46:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Gordon Growth Model with Vector Autoregressive Process</title>
      <link>https://arxiv.org/abs/2406.19424</link>
      <description>arXiv:2406.19424v1 Announce Type: new 
Abstract: In this study, we introduce a Gordon's dividend discount model, based on Vector Autoregressive Process (VAR). We provide two Propositions, which are related to generic Gordon growth model and Gordon growth model, which is based on the VAR process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19424v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Battulga Gankhuu</dc:creator>
    </item>
    <item>
      <title>Minimax And Adaptive Transfer Learning for Nonparametric Classification under Distributed Differential Privacy Constraints</title>
      <link>https://arxiv.org/abs/2406.20088</link>
      <description>arXiv:2406.20088v1 Announce Type: new 
Abstract: This paper considers minimax and adaptive transfer learning for nonparametric classification under the posterior drift model with distributed differential privacy constraints. Our study is conducted within a heterogeneous framework, encompassing diverse sample sizes, varying privacy parameters, and data heterogeneity across different servers. We first establish the minimax misclassification rate, precisely characterizing the effects of privacy constraints, source samples, and target samples on classification accuracy. The results reveal interesting phase transition phenomena and highlight the intricate trade-offs between preserving privacy and achieving classification accuracy. We then develop a data-driven adaptive classifier that achieves the optimal rate within a logarithmic factor across a large collection of parameter spaces while satisfying the same set of differential privacy constraints. Simulation studies and real-world data applications further elucidate the theoretical analysis with numerical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.20088v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnab Auddy, T. Tony Cai, Abhinav Chakraborty</dc:creator>
    </item>
    <item>
      <title>Test for symmetry and confidence interval of the parameter {\mu} of skew-symmetric-Laplace-uniform distribution</title>
      <link>https://arxiv.org/abs/2406.20090</link>
      <description>arXiv:2406.20090v1 Announce Type: new 
Abstract: The skew symmetric Laplace uniform distribution SSLUD({\mu}) is introduced in Lohot, R. K. and Dixit, V. U. (2024) using the skewing mechanism of Azzalini (1985). Here we derive the most powerful (MP) test for symmetry of the SSLUD({\mu}). Since the form of the test statistic is complicated and it is difficult to obtain its exact distribution, critical values and the power of MP test are obtained using simulation. Further, we construct a confidence interval (CI) for parameter {\mu} assuming asymptotic normality and empirical distribution of the maximum likelihood estimator of {\mu}. These two methods are compared based on the average length and coverage probability of the CI. Finally, the CI of the parameter {\mu} is constructed using data on the transformed daily percentage change in the price of NIFTY 50, an Indian stock market index given in Lohot, R. K. and Dixit, V. U. (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.20090v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Raju. K. Lohot, V. U. Dixit</dc:creator>
    </item>
    <item>
      <title>Convergence, optimization and stability of singular eigenmaps</title>
      <link>https://arxiv.org/abs/2406.19510</link>
      <description>arXiv:2406.19510v1 Announce Type: cross 
Abstract: Eigenmaps are important in analysis, geometry, and machine learning, especially in nonlinear dimension reduction. Approximation of the eigenmaps of a Laplace operator depends crucially on the scaling parameter $\epsilon$. If $\epsilon$ is too small or too large, then the approximation is inaccurate or completely breaks down. However, an analytic expression for the optimal $\epsilon$ is out of reach. In our work, we use some explicitly solvable models and Monte Carlo simulations to find the approximately optimal range of $\epsilon$ that gives, on average, relatively accurate approximation of the eigenmaps. Numerically we can consider several model situations where eigen-coordinates can be computed analytically, including intervals with uniform and weighted measures, squares, tori, spheres, and the Sierpinski gasket. In broader terms, we intend to study eigen-coordinates on weighted Riemannian manifolds, possibly with boundary, and on some metric measure spaces, such as fractals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19510v1</guid>
      <category>math.PR</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernard Akwei, Bobita Atkins, Rachel Bailey, Ashka Dalal, Natalie Dinin, Jonathan Kerby-White, Tess McGuinness, Tonya Patricks, Luke Rogers, Genevieve Romanelli, Yiheng Su, Alexander Teplyaev</dc:creator>
    </item>
    <item>
      <title>Provably Efficient Posterior Sampling for Sparse Linear Regression via Measure Decomposition</title>
      <link>https://arxiv.org/abs/2406.19550</link>
      <description>arXiv:2406.19550v1 Announce Type: cross 
Abstract: We consider the problem of sampling from the posterior distribution of a $d$-dimensional coefficient vector $\boldsymbol{\theta}$, given linear observations $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\varepsilon}$. In general, such posteriors are multimodal, and therefore challenging to sample from. This observation has prompted the exploration of various heuristics that aim at approximating the posterior distribution.
  In this paper, we study a different approach based on decomposing the posterior distribution into a log-concave mixture of simple product measures. This decomposition allows us to reduce sampling from a multimodal distribution of interest to sampling from a log-concave one, which is tractable and has been investigated in detail. We prove that, under mild conditions on the prior, for random designs, such measure decomposition is generally feasible when the number of samples per parameter $n/d$ exceeds a constant threshold. We thus obtain a provably efficient (polynomial time) sampling algorithm in a regime where this was previously not known. Numerical simulations confirm that the algorithm is practical, and reveal that it has attractive statistical properties compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19550v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Montanari, Yuchen Wu</dc:creator>
    </item>
    <item>
      <title>Instance-Optimal Private Density Estimation in the Wasserstein Distance</title>
      <link>https://arxiv.org/abs/2406.19566</link>
      <description>arXiv:2406.19566v1 Announce Type: cross 
Abstract: Estimating the density of a distribution from samples is a fundamental problem in statistics. In many practical settings, the Wasserstein distance is an appropriate error metric for density estimation. For example, when estimating population densities in a geographic region, a small Wasserstein distance means that the estimate is able to capture roughly where the population mass is. In this work we study differentially private density estimation in the Wasserstein distance. We design and analyze instance-optimal algorithms for this problem that can adapt to easy instances.
  For distributions $P$ over $\mathbb{R}$, we consider a strong notion of instance-optimality: an algorithm that uniformly achieves the instance-optimal estimation rate is competitive with an algorithm that is told that the distribution is either $P$ or $Q_P$ for some distribution $Q_P$ whose probability density function (pdf) is within a factor of 2 of the pdf of $P$. For distributions over $\mathbb{R}^2$, we use a different notion of instance optimality. We say that an algorithm is instance-optimal if it is competitive with an algorithm that is given a constant-factor multiplicative approximation of the density of the distribution. We characterize the instance-optimal estimation rates in both these settings and show that they are uniformly achievable (up to polylogarithmic factors). Our approach for $\mathbb{R}^2$ extends to arbitrary metric spaces as it goes via hierarchically separated trees. As a special case our results lead to instance-optimal private learning in TV distance for discrete distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19566v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vitaly Feldman, Audra McMillan, Satchit Sivakumar, Kunal Talwar</dc:creator>
    </item>
    <item>
      <title>ScoreFusion: fusing score-based generative models via Kullback-Leibler barycenters</title>
      <link>https://arxiv.org/abs/2406.19619</link>
      <description>arXiv:2406.19619v1 Announce Type: cross 
Abstract: We study the problem of fusing pre-trained (auxiliary) generative models to enhance the training of a target generative model. We propose using KL-divergence weighted barycenters as an optimal fusion mechanism, in which the barycenter weights are optimally trained to minimize a suitable loss for the target population. While computing the optimal KL-barycenter weights can be challenging, we demonstrate that this process can be efficiently executed using diffusion score training when the auxiliary generative models are also trained based on diffusion score methods. Moreover, we show that our fusion method has a dimension-free sample complexity in total variation distance provided that the auxiliary models are well fitted for their own task and the auxiliary tasks combined capture the target well. The main takeaway of our method is that if the auxiliary models are well-trained and can borrow features from each other that are present in the target, our fusion method significantly improves the training of generative models. We provide a concise computational implementation of the fusion algorithm, and validate its efficiency in the low-data regime with numerical experiments involving mixtures models and image datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19619v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Liu (Tony),  Junze (Tony),  Ye, Jose Blanchet, Nian Si</dc:creator>
    </item>
    <item>
      <title>Deep Learning of Multivariate Extremes via a Geometric Representation</title>
      <link>https://arxiv.org/abs/2406.19936</link>
      <description>arXiv:2406.19936v1 Announce Type: cross 
Abstract: The study of geometric extremes, where extremal dependence properties are inferred from the deterministic limiting shapes of scaled sample clouds, provides an exciting approach to modelling the extremes of multivariate data. These shapes, termed limit sets, link together several popular extremal dependence modelling frameworks. Although the geometric approach is becoming an increasingly popular modelling tool, current inference techniques are limited to a low dimensional setting (d &lt; 4), and generally require rigid modelling assumptions. In this work, we propose a range of novel theoretical results to aid with the implementation of the geometric extremes framework and introduce the first approach to modelling limit sets using deep learning. By leveraging neural networks, we construct asymptotically-justified yet flexible semi-parametric models for extremal dependence of high-dimensional data. We showcase the efficacy of our deep approach by modelling the complex extremal dependencies between meteorological and oceanographic variables in the North Sea off the coast of the UK.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19936v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Callum J. R. Murphy-Barltrop, Reetam Majumder, Jordan Richards</dc:creator>
    </item>
    <item>
      <title>The Computational Curse of Big Data for Bayesian Additive Regression Trees: A Hitting Time Analysis</title>
      <link>https://arxiv.org/abs/2406.19958</link>
      <description>arXiv:2406.19958v1 Announce Type: cross 
Abstract: Bayesian Additive Regression Trees (BART) is a popular Bayesian non-parametric regression model that is commonly used in causal inference and beyond. Its strong predictive performance is supported by theoretical guarantees that its posterior distribution concentrates around the true regression function at optimal rates under various data generative settings and for appropriate prior choices. In this paper, we show that the BART sampler often converges slowly, confirming empirical observations by other researchers. Assuming discrete covariates, we show that, while the BART posterior concentrates on a set comprising all optimal tree structures (smallest bias and complexity), the Markov chain's hitting time for this set increases with $n$ (training sample size), under several common data generative settings. As $n$ increases, the approximate BART posterior thus becomes increasingly different from the exact posterior (for the same number of MCMC samples), contrasting with earlier concentration results on the exact posterior. This contrast is highlighted by our simulations showing worsening frequentist undercoverage for approximate posterior intervals and a growing ratio between the MSE of the approximate posterior and that obtainable by artificially improving convergence via averaging multiple sampler chains. Finally, based on our theoretical insights, possibilities are discussed to improve the BART sampler convergence performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19958v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Shuo Tan, Omer Ronen, Theo Saarinen, Bin Yu</dc:creator>
    </item>
    <item>
      <title>Trade-off between predictive performance and FDR control for high-dimensional Gaussian model selection</title>
      <link>https://arxiv.org/abs/2302.01831</link>
      <description>arXiv:2302.01831v4 Announce Type: replace 
Abstract: In the context of high-dimensional Gaussian linear regression for ordered variables, we study the variable selection procedure via the minimization of the penalized least-squares criterion. We focus on model selection where the penalty function depends on an unknown multiplicative constant commonly calibrated for prediction. We propose a new proper calibration of this hyperparameter to simultaneously control predictive risk and false discovery rate. We obtain non-asymptotic bounds on the False Discovery Rate with respect to the hyperparameter and we provide an algorithm to calibrate it. This algorithm is based on quantities that can typically be observed in real data applications. The algorithm is validated in an extensive simulation study and is compared with several existing variable selection procedures. Finally, we study an extension of our approach to the case in which an ordering of the variables is not available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01831v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Perrine Lacroix, Marie-Laure Martin</dc:creator>
    </item>
    <item>
      <title>Censored extreme value estimation</title>
      <link>https://arxiv.org/abs/2312.10499</link>
      <description>arXiv:2312.10499v4 Announce Type: replace 
Abstract: A novel and comprehensive methodology designed to tackle the challenges posed by extreme values in the context of random censorship is introduced. The main focus is on the analysis of integrals based on the product-limit estimator of normalized upper order statistics, called extreme Kaplan--Meier integrals. These integrals allow for the transparent derivation of various important asymptotic distributional properties, offering an alternative approach to conventional plug-in estimation methods. Notably, this methodology demonstrates robustness and wide applicability within the scope of max-domains of attraction. A noteworthy by-product is the extension of generalized Hill-type estimators of extremes to encompass all max-domains of attraction, which is of independent interest. The theoretical framework is applied to construct novel estimators for positive and real-valued extreme value indices for right-censored data. Simulation studies supporting the theory are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10499v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Igor Rodionov</dc:creator>
    </item>
    <item>
      <title>Model Identifiability for Bivariate Failure Time Data with Competing Risk: Non-parametric Cause-specific Hazards and Gamma Frailty</title>
      <link>https://arxiv.org/abs/2405.07722</link>
      <description>arXiv:2405.07722v2 Announce Type: replace 
Abstract: In survival analysis, frailty variables are often used to model the association in multivariate survival data. Identifiability is an important issue while working with such multivariate survival data with or without competing risks. In this work, we consider bivariate survival data with competing risks and investigate identifiability results with non-parametric baseline cause-specific hazards and different types of Gamma frailty. Prior to that, we prove that, when both baseline cause-specific hazards and frailty distributions are non-parametric, the model is not identifiable. We also construct a non-identifiable model when baseline cause-specific hazards are non-parametric but frailty distribution may be parametric. Thereafter, we consider four different Gamma frailty distributions, and the corresponding models are shown to be identifiable under fairly general assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07722v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Biswadeep Ghosh, Anup Dewanji, Sudipta Das</dc:creator>
    </item>
    <item>
      <title>Design-based theory for Lasso adjustment in randomized block experiments and rerandomized experiments</title>
      <link>https://arxiv.org/abs/2109.11271</link>
      <description>arXiv:2109.11271v3 Announce Type: replace-cross 
Abstract: Blocking, a special case of rerandomization, is routinely implemented in the design stage of randomized experiments to balance the baseline covariates. This study proposes a regression adjustment method based on the least absolute shrinkage and selection operator (Lasso) to efficiently estimate the average treatment effect in randomized block experiments with high-dimensional covariates. We derive the asymptotic properties of the proposed estimator and outline the conditions under which this estimator is more efficient than the unadjusted one. We provide a conservative variance estimator to facilitate valid inferences. Our framework allows one treated or control unit in some blocks and heterogeneous propensity scores across blocks, thus including paired experiments and finely stratified experiments as special cases. We further accommodate rerandomized experiments and a combination of blocking and rerandomization. Moreover, our analysis allows both the number of blocks and block sizes to tend to infinity, as well as heterogeneous treatment effects across blocks without assuming a true outcome data-generating model. Simulation studies and two real-data analyses demonstrate the advantages of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.11271v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Zhu, Hanzhong Liu, Yuehan Yang</dc:creator>
    </item>
    <item>
      <title>Optimal Rate of Kernel Regression in Large Dimensions</title>
      <link>https://arxiv.org/abs/2309.04268</link>
      <description>arXiv:2309.04268v2 Announce Type: replace-cross 
Abstract: We perform a study on kernel regression for large-dimensional data (where the sample size $n$ is polynomially depending on the dimension $d$ of the samples, i.e., $n\asymp d^{\gamma}$ for some $\gamma &gt;0$ ). We first build a general tool to characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively. When the target function falls into the RKHS associated with a (general) inner product model defined on $\mathbb{S}^{d}$, we utilize the new tool to show that the minimax rate of the excess risk of kernel regression is $n^{-1/2}$ when $n\asymp d^{\gamma}$ for $\gamma =2, 4, 6, 8, \cdots$. We then further determine the optimal rate of the excess risk of kernel regression for all the $\gamma&gt;0$ and find that the curve of optimal rate varying along $\gamma$ exhibits several new phenomena including the multiple descent behavior and the periodic plateau behavior. As an application, For the neural tangent kernel (NTK), we also provide a similar explicit description of the curve of optimal rate. As a direct corollary, we know these claims hold for wide neural networks as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04268v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weihao Lu, Haobo Zhang, Yicheng Li, Manyun Xu, Qian Lin</dc:creator>
    </item>
    <item>
      <title>Flexible Conformal Highest Predictive Conditional Density Sets</title>
      <link>https://arxiv.org/abs/2406.18052</link>
      <description>arXiv:2406.18052v2 Announce Type: replace-cross 
Abstract: We introduce our method, conformal highest conditional density sets (CHCDS), that forms conformal prediction sets using existing estimated conditional highest density predictive regions. We prove the validity of the method and that conformal adjustment is negligible under some regularity conditions. In particular, if we correctly specify the underlying conditional density estimator, the conformal adjustment will be negligible. When the underlying model is incorrect, the conformal adjustment provides guaranteed nominal unconditional coverage. We compare the proposed method via simulation and a real data analysis to other existing methods. Our numerical results show that the flexibility of being able to use any existing conditional density estimation method is a large advantage for CHCDS compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18052v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Sampson, Kung-Sik Chan</dc:creator>
    </item>
  </channel>
</rss>
