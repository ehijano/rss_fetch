<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Oct 2024 02:01:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Propensity Score Augmentation in Matching-based Estimation of Causal Effects</title>
      <link>https://arxiv.org/abs/2409.19230</link>
      <description>arXiv:2409.19230v1 Announce Type: new 
Abstract: When assessing the causal effect of a binary exposure using observational data, confounder imbalance across exposure arms must be addressed. Matching methods, including propensity score-based matching, can be used to deconfound the causal relationship of interest. They have been particularly popular in practice, at least in part due to their simplicity and interpretability. However, these methods can suffer from low statistical efficiency compared to many competing methods. In this work, we propose a novel matching-based estimator of the average treatment effect based on a suitably-augmented propensity score model. Our procedure can be shown to have greater statistical efficiency than traditional matching estimators whenever prognostic variables are available, and in some cases, can nearly reach the nonparametric efficiency bound. In addition to a theoretical study, we provide numerical results to illustrate our findings. Finally, we use our novel procedure to estimate the effect of circumcision on risk of HIV-1 infection using vaccine efficacy trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19230v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ernesto Ulloa-P\'erez, Marco Carone, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Estimating Interpretable Heterogeneous Treatment Effect with Causal Subgroup Discovery in Survival Outcomes</title>
      <link>https://arxiv.org/abs/2409.19241</link>
      <description>arXiv:2409.19241v1 Announce Type: new 
Abstract: Estimating heterogeneous treatment effect (HTE) for survival outcomes has gained increasing attention, as it captures the variation in treatment efficacy across patients or subgroups in delaying disease progression. However, most existing methods focus on post-hoc subgroup identification rather than simultaneously estimating HTE and selecting relevant subgroups. In this paper, we propose an interpretable HTE estimation framework that integrates three meta-learners that simultaneously estimate CATE for survival outcomes and identify predictive subgroups. We evaluated the performance of our method through comprehensive simulation studies across various randomized clinical trial (RCT) settings. Additionally, we demonstrated its application in a large RCT for age-related macular degeneration (AMD), a polygenic progressive eye disease, to estimate the HTE of an antioxidant and mineral supplement on time-to-AMD progression and to identify genetics-based subgroups with enhanced treatment effects. Our method offers a direct interpretation of the estimated HTE and provides evidence to support precision healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19241v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Na Bo, Ying Ding</dc:creator>
    </item>
    <item>
      <title>The co-varying ties between networks and item responses via latent variables</title>
      <link>https://arxiv.org/abs/2409.19400</link>
      <description>arXiv:2409.19400v1 Announce Type: new 
Abstract: Relationships among teachers are known to influence their teaching-related perceptions. We study whether and how teachers' advising relationships (networks) are related to their perceptions of satisfaction, students, and influence over educational policies, recorded as their responses to a questionnaire (item responses). We propose a novel joint model of network and item responses (JNIRM) with correlated latent variables to understand these co-varying ties. This methodology allows the analyst to test and interpret the dependence between a network and item responses. Using JNIRM, we discover that teachers' advising relationships contribute to their perceptions of satisfaction and students more often than their perceptions of influence over educational policies. In addition, we observe that the complementarity principle applies in certain schools, where teachers tend to seek advice from those who are different from them. JNIRM shows superior parameter estimation and model fit over separately modeling the network and item responses with latent variable models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19400v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Selena Wang, Plamena Powla, Tracy Sweet, Subhadeep Paul</dc:creator>
    </item>
    <item>
      <title>Priors for Reducing Asymptotic Bias of the Posterior Mean</title>
      <link>https://arxiv.org/abs/2409.19673</link>
      <description>arXiv:2409.19673v1 Announce Type: new 
Abstract: It is shown that the first-order term of the asymptotic bias of the posterior mean is removed by a suitable choice of a prior density. In regular statistical models including exponential families, and linear and logistic regression models, such a prior is given by the squared Jeffreys prior. We also explain the relationship between the proposed prior distribution, the moment matching prior, and the prior distribution that reduces the bias term of the posterior mode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19673v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miyata Yoichi, Yanagimoto Takemi</dc:creator>
    </item>
    <item>
      <title>Posterior Conformal Prediction</title>
      <link>https://arxiv.org/abs/2409.19712</link>
      <description>arXiv:2409.19712v1 Announce Type: new 
Abstract: Conformal prediction is a popular technique for constructing prediction intervals with distribution-free coverage guarantees. The coverage is marginal, meaning it only holds on average over the entire population but not necessarily for any specific subgroup. This article introduces a new method, posterior conformal prediction (PCP), which generates prediction intervals with both marginal and approximate conditional validity for clusters (or subgroups) naturally discovered in the data. PCP achieves these guarantees by modelling the conditional conformity score distribution as a mixture of cluster distributions. Compared to other methods with approximate conditional validity, this approach produces tighter intervals, particularly when the test data is drawn from clusters that are well represented in the validation data. PCP can also be applied to guarantee conditional coverage on user-specified subgroups, in which case it achieves robust coverage on smaller subgroups within the specified subgroups. In classification, the theory underlying PCP allows for adjusting the coverage level based on the classifier's confidence, achieving significantly smaller sets than standard conformal prediction sets. We evaluate the performance of PCP on diverse datasets from socio-economic, scientific and healthcare applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19712v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Zhang, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>Prior Sensitivity Analysis without Model Re-fit</title>
      <link>https://arxiv.org/abs/2409.19729</link>
      <description>arXiv:2409.19729v1 Announce Type: new 
Abstract: Prior sensitivity analysis is a fundamental method to check the effects of prior distributions on the posterior distribution in Bayesian inference. Exploring the posteriors under several alternative priors can be computationally intensive, particularly for complex latent variable models. To address this issue, we propose a novel method for quantifying the prior sensitivity that does not require model re-fit. Specifically, we present a method to compute the Hellinger and Kullback-Leibler distances between two posterior distributions with base and alternative priors, as well as posterior expectations under the alternative prior, using Monte Carlo integration based only on the base posterior distribution. This method significantly reduces computational costs in prior sensitivity analysis. We also extend the above approach for assessing the influence of hyperpriors in general latent variable models. We demonstrate the proposed method through examples of a simple normal distribution model, hierarchical binomial-beta model, and Gaussian process regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19729v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Compound e-values and empirical Bayes</title>
      <link>https://arxiv.org/abs/2409.19812</link>
      <description>arXiv:2409.19812v1 Announce Type: new 
Abstract: We explicitly define the notion of (exact or approximate) compound e-values which have been implicitly presented and extensively used in the recent multiple testing literature. We show that every FDR controlling procedure can be recovered by instantiating the e-BH procedure with certain compound e-values. Since compound e-values are closed under averaging, this allows for combination and derandomization of any FDR procedure. We then point out and exploit the connections to empirical Bayes. In particular, we use the fundamental theorem of compound decision theory to derive the log-optimal simple separable compound e-value for testing a set of point nulls against point alternatives: it is a ratio of mixture likelihoods. We extend universal inference to the compound setting. As one example, we construct approximate compound e-values for multiple t-tests, where the (nuisance) variances may be different across hypotheses. We provide connections to related notions in the literature stated in terms of p-values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19812v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaos Ignatiadis, Ruodu Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Subset Simulation for High-dimensional and Multi-modal Bayesian Inference</title>
      <link>https://arxiv.org/abs/2409.19910</link>
      <description>arXiv:2409.19910v1 Announce Type: new 
Abstract: Bayesian analysis plays a crucial role in estimating distribution of unknown parameters for given data and model. Due to the curse of dimensionality, it is difficult to infer high-dimensional problems, especially when multiple modes exist. This paper introduces an efficient Bayesian posterior sampling algorithm that draws inspiration from subset simulation (SuS). It is based on a new interpretation of evidence from the perspective of structural reliability estimation, regarding the likelihood function as a limit state function. The posterior samples can be obtained following the principle of importance resampling as a postprocessing procedure. The estimation variance is derived to quantify the inherent uncertainty associated with the SuS estimator of evidence. The effective sample size is introduced to measure the quality of the posterior sampling. Three benchmark examples are first considered to illustrate the performance of the proposed algorithm by comparing it with two state-of-art algorithms. It is then used for the finite element (FE) model updating, showing its applicability in practical engineering problems. The proposed SuS algorithm exhibits comparable or even better performance in evidence estimation and posterior sampling, compared to the aBUS and MULTINEST algorithms, especially when the dimension of unknown parameters is high. In the application of the proposed algorithm for FE model updating, satisfactory results are obtained when the configuration (number and location) of sensory system is proper, underscoring the importance of adequate sensor placement around critical degrees of freedom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19910v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Liao, Binbin Li, Hua-Ping Wan</dc:creator>
    </item>
    <item>
      <title>Conformal prediction for functional Ordinary kriging</title>
      <link>https://arxiv.org/abs/2409.20084</link>
      <description>arXiv:2409.20084v1 Announce Type: new 
Abstract: Functional Ordinary Kriging is the most widely used method to predict a curve at a given spatial point. However, uncertainty remains an open issue. In this article a distribution-free prediction method based on two different modulation functions and two conformity scores is proposed. Through simulations and benchmark data analyses, we demonstrate the advantages of our approach when compared to standard methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20084v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna De Magistris, Andrea Diana, Elvira Romano</dc:creator>
    </item>
    <item>
      <title>Bootstrap-based goodness-of-fit test for parametric families of conditional distributions</title>
      <link>https://arxiv.org/abs/2409.20262</link>
      <description>arXiv:2409.20262v1 Announce Type: new 
Abstract: In various scientific fields, researchers are interested in exploring the relationship between some response variable Y and a vector of covariates X. In order to make use of a specific model for the dependence structure, it first has to be checked whether the conditional density function of Y given X fits into a given parametric family. We propose a consistent bootstrap-based goodness-of-fit test for this purpose. The test statistic traces the difference between a nonparametric and a semi-parametric estimate of the marginal distribution function of Y. As its asymptotic null distribution is not distribution-free, a parametric bootstrap method is used to determine the critical value. A simulation study shows that, in some cases, the new method is more sensitive to deviations from the parametric model than other tests found in the literature. We also apply our method to real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20262v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gitte Kremling, Gerhard Dikta</dc:creator>
    </item>
    <item>
      <title>CURATE: Scaling-up Differentially Private Causal Graph Discovery</title>
      <link>https://arxiv.org/abs/2409.19060</link>
      <description>arXiv:2409.19060v1 Announce Type: cross 
Abstract: Causal Graph Discovery (CGD) is the process of estimating the underlying probabilistic graphical model that represents joint distribution of features of a dataset. CGD-algorithms are broadly classified into two categories: (i) Constraint-based algorithms (outcome depends on conditional independence (CI) tests), (ii) Score-based algorithms (outcome depends on optimized score-function). Since, sensitive features of observational data is prone to privacy-leakage, Differential Privacy (DP) has been adopted to ensure user privacy in CGD. Adding same amount of noise in this sequential-natured estimation process affects the predictive performance of the algorithms. As initial CI tests in constraint-based algorithms and later iterations of the optimization process of score-based algorithms are crucial, they need to be more accurate, less noisy. Based on this key observation, we present CURATE (CaUsal gRaph AdapTivE privacy), a DP-CGD framework with adaptive privacy budgeting. In contrast to existing DP-CGD algorithms with uniform privacy budgeting across all iterations, CURATE allows adaptive privacy budgeting by minimizing error probability (for constraint-based), maximizing iterations of the optimization problem (for score-based) while keeping the cumulative leakage bounded. To validate our framework, we present a comprehensive set of experiments on several datasets and show that CURATE achieves higher utility compared to existing DP-CGD algorithms with less privacy-leakage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19060v1</guid>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Payel Bhattacharjee, Ravi Tandon</dc:creator>
    </item>
    <item>
      <title>Solving Fredholm Integral Equations of the Second Kind via Wasserstein Gradient Flows</title>
      <link>https://arxiv.org/abs/2409.19642</link>
      <description>arXiv:2409.19642v1 Announce Type: cross 
Abstract: Motivated by a recent method for approximate solution of Fredholm equations of the first kind, we develop a corresponding method for a class of Fredholm equations of the \emph{second kind}. In particular, we consider the class of equations for which the solution is a probability measure. The approach centres around specifying a functional whose gradient flow admits a minimizer corresponding to a regularized version of the solution of the underlying equation and using a mean-field particle system to approximately simulate that flow. Theoretical support for the method is presented, along with some illustrative numerical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19642v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca R. Crucinio, Adam M. Johansen</dc:creator>
    </item>
    <item>
      <title>Automatic debiasing of neural networks via moment-constrained learning</title>
      <link>https://arxiv.org/abs/2409.19777</link>
      <description>arXiv:2409.19777v1 Announce Type: cross 
Abstract: Causal and nonparametric estimands in economics and biostatistics can often be viewed as the mean of a linear functional applied to an unknown outcome regression function. Naively learning the regression function and taking a sample mean of the target functional results in biased estimators, and a rich debiasing literature has developed where one additionally learns the so-called Riesz representer (RR) of the target estimand (targeted learning, double ML, automatic debiasing etc.). Learning the RR via its derived functional form can be challenging, e.g. due to extreme inverse probability weights or the need to learn conditional density functions. Such challenges have motivated recent advances in automatic debiasing (AD), where the RR is learned directly via minimization of a bespoke loss. We propose moment-constrained learning as a new RR learning approach that addresses some shortcomings in AD, constraining the predicted moments and improving the robustness of RR estimates to optimization hyperparamters. Though our approach is not tied to a particular class of learner, we illustrate it using neural networks, and evaluate on the problems of average treatment/derivative effect estimation using semi-synthetic data. Our numerical experiments show improved performance versus state of the art benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19777v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian L. Hines, Oliver J. Hines</dc:creator>
    </item>
    <item>
      <title>Choosing DAG Models Using Markov and Minimal Edge Count in the Absence of Ground Truth</title>
      <link>https://arxiv.org/abs/2409.20187</link>
      <description>arXiv:2409.20187v1 Announce Type: cross 
Abstract: We give a novel nonparametric pointwise consistent statistical test (the Markov Checker) of the Markov condition for directed acyclic graph (DAG) or completed partially directed acyclic graph (CPDAG) models given a dataset. We also introduce the Cross-Algorithm Frugality Search (CAFS) for rejecting DAG models that either do not pass the Markov Checker test or that are not edge minimal. Edge minimality has been used previously by Raskutti and Uhler as a nonparametric simplicity criterion, though CAFS readily generalizes to other simplicity conditions. Reference to the ground truth is not necessary for CAFS, so it is useful for finding causal structure learning algorithms and tuning parameter settings that output causal models that are approximately true from a given data set. We provide a software tool for this analysis that is suitable for even quite large or dense models, provided a suitably fast pointwise consistent test of conditional independence is available. In addition, we show in simulation that the CAFS procedure can pick approximately correct models without knowing the ground truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20187v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph D. Ramsey, Bryan Andrews, Peter Spirtes</dc:creator>
    </item>
    <item>
      <title>L-2 Regularized maximum likelihood for $\beta$-model in large and sparse networks</title>
      <link>https://arxiv.org/abs/2110.11856</link>
      <description>arXiv:2110.11856v4 Announce Type: replace 
Abstract: The $\beta$-model is a powerful tool for modeling large and sparse networks driven by degree heterogeneity, where many network models become infeasible due to computational challenge and network sparsity. However, existing estimation algorithms for $\beta$-model do not scale up. Also, theoretical understandings remain limited to dense networks. This paper brings several significant improvements over existing results to address the urgent needs of practice. We propose a new $\ell_2$-penalized MLE algorithm that can comfortably handle sparse networks of millions of nodes with much-improved memory parsimony. We establish the first rate-optimal error bounds and high-dimensional asymptotic normality results for $\beta$-models, under much weaker network sparsity assumptions than best existing results.
  Application of our method to large COVID-19 network data sets and discover meaningful results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.11856v4</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Meijia Shao, Yu Zhang, Qiuping Wang, Yuan Zhang, Jing Luo, Ting Yan</dc:creator>
    </item>
    <item>
      <title>Functional Regression Models with Functional Response: A New Approach and a Comparative Study</title>
      <link>https://arxiv.org/abs/2207.04773</link>
      <description>arXiv:2207.04773v5 Announce Type: replace 
Abstract: This paper proposes a new nonlinear approach for additive functional regression with functional response based on kernel methods along with some slight reformulation and implementation of the linear regression and the spectral additive model. The latter methods have in common that the covariates and the response are represented in a basis and so, can only be applied when the response and the covariates belong to a Hilbert space, while the proposed method only uses the distances among data and thus can be applied to those situations where any of the covariates or the response is not Hilbert, typically normed or even metric spaces with a real vector structure. A comparison of these methods with other procedures readily available in R is preformed in a simulation study and in real datasets showing the results of the advantages of the nonlinear proposals and the small loss of efficiency when the simulation scenario is truly linear. The comparison is done in the Hilbert case as it is the only scenario where all the procedures can be compared. Finally, the supplementary material provides a visualization tool for checking the linearity of the relationship between a single covariate and the response, another real data example, and a link to a GitHub repository where the code and data are available.} %and an example considering that the response is not Hilbertian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.04773v5</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Manuel Febrero Bande, Manuel Oviedo de la Fuente, Mohammad Darbalaei, Morteza Amini</dc:creator>
    </item>
    <item>
      <title>Causal and counterfactual views of missing data models</title>
      <link>https://arxiv.org/abs/2210.05558</link>
      <description>arXiv:2210.05558v2 Announce Type: replace 
Abstract: It is often said that the fundamental problem of causal inference is a missing data problem -- the comparison of responses to two hypothetical treatment assignments is made difficult because for every experimental unit only one potential response is observed. In this paper, we consider the implications of the converse view: that missing data problems are a form of causal inference. We make explicit how the missing data problem of recovering the complete data law from the observed law can be viewed as identification of a joint distribution over counterfactual variables corresponding to values had we (possibly contrary to fact) been able to observe them. Drawing analogies with causal inference, we show how identification assumptions in missing data can be encoded in terms of graphical models defined over counterfactual and observed variables. We review recent results in missing data identification from this viewpoint. In doing so, we note interesting similarities and differences between missing data and causal identification theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.05558v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Razieh Nabi, Rohit Bhattacharya, Ilya Shpitser, James Robins</dc:creator>
    </item>
    <item>
      <title>Augmenting a simulation campaign for hybrid computer model and field data experiments</title>
      <link>https://arxiv.org/abs/2301.10228</link>
      <description>arXiv:2301.10228v2 Announce Type: replace 
Abstract: The Kennedy and O'Hagan (KOH) calibration framework uses coupled Gaussian processes (GPs) to meta-model an expensive simulator (first GP), tune its ``knobs" (calibration inputs) to best match observations from a real physical/field experiment and correct for any modeling bias (second GP) when predicting under new field conditions (design inputs). There are well-established methods for placement of design inputs for data-efficient planning of a simulation campaign in isolation, i.e., without field data: space-filling, or via criterion like minimum integrated mean-squared prediction error (IMSPE). Analogues within the coupled GP KOH framework are mostly absent from the literature. Here we derive a closed form IMSPE criterion for sequentially acquiring new simulator data for KOH. We illustrate how acquisitions space-fill in design space, but concentrate in calibration space. Closed form IMSPE precipitates a closed-form gradient for efficient numerical optimization. We demonstrate that our KOH-IMSPE strategy leads to a more efficient simulation campaign on benchmark problems, and conclude with a showcase on an application to equilibrium concentrations of rare earth elements for a liquid-liquid extraction reaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.10228v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/00401706.2024.2345139</arxiv:DOI>
      <dc:creator>Scott Koermer, Justin Loda, Aaron Noble, Robert B. Gramacy</dc:creator>
    </item>
    <item>
      <title>Spectral Deconfounding for High-Dimensional Sparse Additive Models</title>
      <link>https://arxiv.org/abs/2312.02860</link>
      <description>arXiv:2312.02860v2 Announce Type: replace 
Abstract: Many high-dimensional data sets suffer from hidden confounding which affects both the predictors and the response of interest. In such situations, standard regression methods or algorithms lead to biased estimates. This paper substantially extends previous work on spectral deconfounding for high-dimensional linear models to the nonlinear setting and with this, establishes a proof of concept that spectral deconfounding is valid for general nonlinear models. Concretely, we propose an algorithm to estimate high-dimensional sparse additive models in the presence of hidden dense confounding: arguably, this is a simple yet practically useful nonlinear scope. We prove consistency and convergence rates for our method and evaluate it on synthetic data and a genetic data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02860v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyrill Scheidegger, Zijian Guo, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>A graphical framework for interpretable correlation matrix models</title>
      <link>https://arxiv.org/abs/2312.06289</link>
      <description>arXiv:2312.06289v2 Announce Type: replace 
Abstract: In this work, we present a new approach for constructing models for correlation matrices with a user-defined graphical structure. The graphical structure makes correlation matrices interpretable and avoids the quadratic increase of parameters as a function of the dimension. We suggest an automatic approach to define a prior using a natural sequence of simpler models within the Penalized Complexity framework for the unknown parameters in these models.
  We illustrate this approach with three applications: a multivariate linear regression of four biomarkers, a multivariate disease mapping, and a multivariate longitudinal joint modelling. Each application underscores our method's intuitive appeal, signifying a substantial advancement toward a more cohesive and enlightening model that facilitates a meaningful interpretation of correlation matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06289v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Freni Sterrantino, Denis Rustand, Janet van Niekerk, Elias Teixeira Krainski, H{\aa}vard Rue</dc:creator>
    </item>
    <item>
      <title>Recovery and inference of causal effects with sequential adjustment for confounding and attrition</title>
      <link>https://arxiv.org/abs/2401.16990</link>
      <description>arXiv:2401.16990v2 Announce Type: replace 
Abstract: Confounding bias and selection bias are two significant challenges to the validity of conclusions drawn from applied causal inference. The latter can stem from informative missingness, such as in cases of attrition. We introduce the Sequential Adjustment Criteria (SAC), which extend available graphical conditions for recovering causal effects using sequential regressions, allowing for the inclusion of post-exposure and forbidden variables in the admissible adjustment sets. We propose an estimator for the recovered Average Treatment Effect (ATE) based on Targeted Minimum-Loss Estimation (TMLE), which enjoys multiple robustness under certain conditions. This approach ensures consistency even in scenarios where the Double Inverse Probability Weighting (DIPW) and the na\"ive plug-in sequential regressions approaches fall short. Through a simulation study, we assess the performance of the proposed estimator against alternative methods across different graph setups and model specification scenarios. As a motivating application, we examine the effect of pharmacological treatment for Attention-Deficit/Hyperactivity Disorder (ADHD) upon the scores obtained by diagnosed Norwegian schoolchildren in national tests using observational data ($n=9,352$). Our findings align with the accumulated clinical evidence, affirming a positive but small impact of medication on academic achievement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16990v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Johan de Aguas, Johan Pensar, Tom\'as Varnet P\'erez, Guido Biele</dc:creator>
    </item>
    <item>
      <title>Understanding overfitting in random forest for probability estimation: a visualization and simulation study</title>
      <link>https://arxiv.org/abs/2402.18612</link>
      <description>arXiv:2402.18612v2 Announce Type: replace 
Abstract: Random forests have become popular for clinical risk prediction modelling. In a case study on predicting ovarian malignancy, we observed training c-statistics close to 1. Although this suggests overfitting, performance was competitive on test data. We aimed to understand the behaviour of random forests by (1) visualizing data space in three real world case studies and (2) a simulation study. For the case studies, risk estimates were visualised using heatmaps in a 2-dimensional subspace. The simulation study included 48 logistic data generating mechanisms (DGM), varying the predictor distribution, the number of predictors, the correlation between predictors, the true c-statistic and the strength of true predictors. For each DGM, 1000 training datasets of size 200 or 4000 were simulated and RF models trained with minimum node size 2 or 20 using ranger package, resulting in 192 scenarios in total. The visualizations suggested that the model learned spikes of probability around events in the training set. A cluster of events created a bigger peak, isolated events local peaks. In the simulation study, median training c-statistics were between 0.97 and 1 unless there were 4 or 16 binary predictors with minimum node size 20. Median test c-statistics were higher with higher events per variable, higher minimum node size, and binary predictors. Median training slopes were always above 1, and were not correlated with median test slopes across scenarios (correlation -0.11). Median test slopes were higher with higher true c-statistic, higher minimum node size, and higher sample size. Random forests learn local probability peaks that often yield near perfect training c-statistics without strongly affecting c-statistics on test data. When the aim is probability estimation, the simulation results go against the common recommendation to use fully grown trees in random forest models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18612v2</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1186/s41512-024-00177-1</arxiv:DOI>
      <arxiv:journal_reference>Diagn Progn Res 8, 14 (2024)</arxiv:journal_reference>
      <dc:creator>Lasai Barre\~nada, Paula Dhiman, Dirk Timmerman, Anne-Laure Boulesteix, Ben Van Calster</dc:creator>
    </item>
    <item>
      <title>Bivariate temporal dependence via mixtures of rotated copulas</title>
      <link>https://arxiv.org/abs/2403.12789</link>
      <description>arXiv:2403.12789v2 Announce Type: replace 
Abstract: Parametric bivariate copula families have been known to flexibly capture various dependence patterns, e.g., either positive or negative dependence in either the lower or upper tails of bivariate distributions. In this paper, our objective is to construct a model that is adaptable enough to capture several of these features simultaneously. We propose a mixture of 4-way rotations of a parametric copula that can achieve this goal. We illustrate the construction using the Clayton family but the concept is general and can be applied to other families. In order to include dynamic dependence regimes, the approach is extended to a time-dependent sequence of mixture copulas in which the mixture probabilities are allowed to evolve in time via a moving average and seasonal types of relationship. The properties of the proposed model and its performance are examined using simulated and real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12789v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruyi Pan, Luis E. Nieto-Barajas, Radu Craiu</dc:creator>
    </item>
    <item>
      <title>General Bayesian inference for causal effects using covariate balancing procedure</title>
      <link>https://arxiv.org/abs/2404.09414</link>
      <description>arXiv:2404.09414v2 Announce Type: replace 
Abstract: In observational studies, the propensity score plays a central role in estimating causal effects of interest. The inverse probability weighting (IPW) estimator is commonly used for this purpose. However, if the propensity score model is misspecified, the IPW estimator may produce biased estimates of causal effects. Previous studies have proposed some robust propensity score estimation procedures. However, these methods require considering parameters that dominate the uncertainty of sampling and treatment allocation. This study proposes a novel Bayesian estimating procedure that necessitates probabilistically deciding the parameter, rather than deterministically. Since the IPW estimator and propensity score estimator can be derived as solutions to certain loss functions, the general Bayesian paradigm, which does not require the considering the full likelihood, can be applied. Therefore, our proposed method only requires the same level of assumptions as ordinary causal inference contexts. The proposed Bayesian method demonstrates equal or superior results compared to some previous methods in simulation experimentss, and is also applied to real data, namely the Whitehall dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09414v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunichiro Orihara, Tomotaka Momozaki, Tomoyuki Nakagawa</dc:creator>
    </item>
    <item>
      <title>Euclidean mirrors and first-order changepoints in network time series</title>
      <link>https://arxiv.org/abs/2405.11111</link>
      <description>arXiv:2405.11111v2 Announce Type: replace 
Abstract: We describe a model for a network time series whose evolution is governed by an underlying stochastic process, known as the latent position process, in which network evolution can be represented in Euclidean space by a curve, called the Euclidean mirror. We define the notion of a first-order changepoint for a time series of networks, and construct a family of latent position process networks with underlying first-order changepoints. We prove that a spectral estimate of the associated Euclidean mirror localizes these changepoints, even when the graph distribution evolves continuously, but at a rate that changes. Simulated and real data examples on organoid networks show that this localization captures empirically significant shifts in network evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11111v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Chen, Zachary Lubberts, Avanti Athreya, Youngser Park, Carey E. Priebe</dc:creator>
    </item>
    <item>
      <title>ROMI: A Randomized Two-Stage Basket Trial Design to Optimize Doses for Multiple Indications</title>
      <link>https://arxiv.org/abs/2408.15502</link>
      <description>arXiv:2408.15502v2 Announce Type: replace 
Abstract: Optimizing doses for multiple indications is challenging. The pooled approach of finding a single optimal biological dose (OBD) for all indications ignores that dose-response or dose-toxicity curves may differ between indications, resulting in varying OBDs. Conversely, indication-specific dose optimization often requires a large sample size. To address this challenge, we propose a Randomized two-stage basket trial design that Optimizes doses in Multiple Indications (ROMI). In stage 1, for each indication, response and toxicity are evaluated for a high dose, which may be a previously obtained MTD, with a rule that stops accrual to indications where the high dose is unsafe or ineffective. Indications not terminated proceed to stage 2, where patients are randomized between the high dose and a specified lower dose. A latent-cluster Bayesian hierarchical model is employed to borrow information between indications, while considering the potential heterogeneity of OBD across indications. Indication-specific utilities are used to quantify response-toxicity trade-offs. At the end of stage 2, for each indication with at least one acceptable dose, the dose with highest posterior mean utility is selected as optimal. Two versions of ROMI are presented, one using only stage 2 data for dose optimization and the other optimizing doses using data from both stages. Simulations show that both versions have desirable operating characteristics compared to designs that either ignore indications or optimize dose independently for each indication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15502v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuqi Wang, Peter F. Thall, Kentaro Takeda, Ying Yuan</dc:creator>
    </item>
    <item>
      <title>Easy Conditioning far beyond Gaussian</title>
      <link>https://arxiv.org/abs/2409.16003</link>
      <description>arXiv:2409.16003v3 Announce Type: replace 
Abstract: Estimating and sampling from conditional densities plays a critical role in statistics and data science, with a plethora of applications. Numerous methods are available ranging from simple fitting approaches to sophisticated machine learning algorithms. However, selecting from among these often involves a trade-off between conflicting objectives of efficiency, flexibility and interpretability. Starting from well known easy conditioning results in the Gaussian case, we show, thanks to results pertaining to stability by mixing and marginal transformations, that the latter carry over far beyond the Gaussian case. This enables us to flexibly model multivariate data by accommodating broad classes of multi-modal dependence structures and marginal distributions, while enjoying fast conditioning of fitted joint distributions. In applications, we primarily focus on conditioning via Gaussian versus Gaussian mixture copula models, comparing different fitting implementations for the latter. Numerical experiments with simulated and real data demonstrate the relevance of the approach for conditional sampling, evaluated using multivariate scoring rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16003v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Faul, David Ginsbourger, Ben Spycher</dc:creator>
    </item>
    <item>
      <title>A flexible approach: variable selection procedures with multilayer FDR control via e-values</title>
      <link>https://arxiv.org/abs/2409.17039</link>
      <description>arXiv:2409.17039v2 Announce Type: replace 
Abstract: Consider a scenario where a large number of explanatory features targeting a response variable are analyzed, such that these features are partitioned into different groups according to their domain-specific structures. Furthermore, there may be several such partitions. Such multiple partitions may exist in many real-life scenarios. One such example is spatial genome-wide association studies. Researchers may not only be interested in identifying the features relevant to the response but also aim to determine the relevant groups within each partition. A group is considered relevant if it contains at least one relevant feature. To ensure the replicability of the findings at various resolutions, it is essential to provide false discovery rate (FDR) control for findings at multiple layers simultaneously. This paper presents a flexible approach that leverages various existing controlled selection procedures to generate more stable results with multilayer FDR control. The key contributions of our proposal are the development of a generalized e-filter that provides multilayer FDR control and the construction of a specific type of generalized e-values to evaluate feature importance. A primary application of our method is an improved version of Data Splitting (DS), called the eDS-filter. Furthermore, we combine the eDS-filter with the version of the group knockoff filter (gKF), resulting in a more flexible approach called the eDS+gKF filter. Simulation studies demonstrate that the proposed methods effectively control the FDR at multiple levels while maintaining or even improving power compared to other approaches. Finally, we apply the proposed method to analyze HIV mutation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17039v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyao Yu, Ruixing Ming, Min Xiao, Zhanfeng Wang</dc:creator>
    </item>
    <item>
      <title>Incorporating sparse labels into hidden Markov models using weighted likelihoods improves accuracy and interpretability in biologging studies</title>
      <link>https://arxiv.org/abs/2409.18091</link>
      <description>arXiv:2409.18091v2 Announce Type: replace 
Abstract: Ecologists often use a hidden Markov model to decode a latent process, such as a sequence of an animal's behaviours, from an observed biologging time series. Modern technological devices such as video recorders and drones now allow researchers to directly observe an animal's behaviour. Using these observations as labels of the latent process can improve a hidden Markov model's accuracy when decoding the latent process. However, many wild animals are observed infrequently. Including such rare labels often has a negligible influence on parameter estimates, which in turn does not meaningfully improve the accuracy of the decoded latent process. We introduce a weighted likelihood approach that increases the relative influence of labelled observations. We use this approach to develop two hidden Markov models to decode the foraging behaviour of killer whales (Orcinus orca) off the coast of British Columbia, Canada. Using cross-validated evaluation metrics, we show that our weighted likelihood approach produces more accurate and understandable decoded latent processes compared to existing methods. Thus, our method effectively leverages sparse labels to enhance researchers' ability to accurately decode hidden processes across various fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18091v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evan Sidrow, Nancy Heckman, Tess M. McRae, Beth L. Volpov, Andrew W. Trites, Sarah M. E. Fortune, Marie Auger-M\'eth\'e</dc:creator>
    </item>
    <item>
      <title>A Sparse Beta Regression Model for Network Analysis</title>
      <link>https://arxiv.org/abs/2010.13604</link>
      <description>arXiv:2010.13604v3 Announce Type: replace-cross 
Abstract: For statistical analysis of network data, the $\beta$-model has emerged as a useful tool, thanks to its flexibility in incorporating nodewise heterogeneity and theoretical tractability. To generalize the $\beta$-model, this paper proposes the Sparse $\beta$-Regression Model (S$\beta$RM) that unites two research themes developed recently in modelling homophily and sparsity. In particular, we employ differential heterogeneity that assigns weights only to important nodes and propose penalized likelihood with an $\ell_1$ penalty for parameter estimation. While our estimation method is closely related to the LASSO method for logistic regression, we develop new theory emphasizing the use of our model for dealing with a parameter regime that can handle sparse networks usually seen in practice. More interestingly, the resulting inference on the homophily parameter demands no debiasing normally employed in LASSO type estimation. We provide extensive simulation and data analysis to illustrate the use of the model. As a special case of our model, we extend the Erd\H{o}s-R\'{e}nyi model by including covariates and develop the associated statistical inference for sparse networks, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.13604v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Stein, Rui Feng, Chenlei Leng</dc:creator>
    </item>
    <item>
      <title>Reinforcing RCTs with Multiple Priors while Learning about External Validity</title>
      <link>https://arxiv.org/abs/2112.09170</link>
      <description>arXiv:2112.09170v5 Announce Type: replace-cross 
Abstract: This paper introduces a framework for incorporating prior information into the design of sequential experiments. These sources may include past experiments, expert opinions, or the experimenter's intuition. We model the problem using a multi-prior Bayesian approach, mapping each source to a Bayesian model and aggregating them based on posterior probabilities. Policies are evaluated on three criteria: learning the parameters of payoff distributions, the probability of choosing the wrong treatment, and average rewards. Our framework demonstrates several desirable properties, including robustness to sources lacking external validity, while maintaining strong finite sample performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.09170v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Frederico Finan, Demian Pouzo</dc:creator>
    </item>
    <item>
      <title>Empirical partially Bayes multiple testing and compound $\chi^2$ decisions</title>
      <link>https://arxiv.org/abs/2303.02887</link>
      <description>arXiv:2303.02887v2 Announce Type: replace-cross 
Abstract: A common task in high-throughput biology is to screen for associations across thousands of units of interest, e.g., genes or proteins. Often, the data for each unit are modeled as Gaussian measurements with unknown mean and variance and are summarized as per-unit sample averages and sample variances. The downstream goal is multiple testing for the means. In this domain, it is routine to "moderate" (that is, to shrink) the sample variances through parametric empirical Bayes methods before computing p-values for the means. Such an approach is asymmetric in that a prior is posited and estimated for the nuisance parameters (variances) but not the primary parameters (means). Our work initiates the formal study of this paradigm, which we term "empirical partially Bayes multiple testing." In this framework, if the prior for the variances were known, one could proceed by computing p-values conditional on the sample variances -- a strategy called partially Bayes inference by Sir David Cox. We show that these conditional p-values satisfy an Eddington/Tweedie-type formula and are approximated at nearly-parametric rates when the prior is estimated by nonparametric maximum likelihood. The estimated p-values can be used with the Benjamini-Hochberg procedure to guarantee asymptotic control of the false discovery rate. Even in the compound setting, wherein the variances are fixed, the approach retains asymptotic type-I error guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02887v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaos Ignatiadis, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Simulation-based, Finite-sample Inference for Privatized Data</title>
      <link>https://arxiv.org/abs/2303.05328</link>
      <description>arXiv:2303.05328v5 Announce Type: replace-cross 
Abstract: Privacy protection methods, such as differentially private mechanisms, introduce noise into resulting statistics which often produces complex and intractable sampling distributions. In this paper, we propose a simulation-based "repro sample" approach to produce statistically valid confidence intervals and hypothesis tests, which builds on the work of Xie and Wang (2022). We show that this methodology is applicable to a wide variety of private inference problems, appropriately accounts for biases introduced by privacy mechanisms (such as by clamping), and improves over other state-of-the-art inference methods such as the parametric bootstrap in terms of the coverage and type I error of the private inference. We also develop significant improvements and extensions for the repro sample methodology for general models (not necessarily related to privacy), including 1) modifying the procedure to ensure guaranteed coverage and type I errors, even accounting for Monte Carlo error, and 2) proposing efficient numerical algorithms to implement the confidence intervals and $p$-values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05328v5</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Zhanyu Wang</dc:creator>
    </item>
    <item>
      <title>Root-n consistent semiparametric learning with high-dimensional nuisance functions under minimal sparsity</title>
      <link>https://arxiv.org/abs/2305.04174</link>
      <description>arXiv:2305.04174v4 Announce Type: replace-cross 
Abstract: Treatment effect estimation under unconfoundedness is a fundamental task in causal inference. In response to the challenge of analyzing high-dimensional datasets collected in substantive fields such as epidemiology, genetics, economics, and social sciences, various methods for treatment effect estimation with high-dimensional nuisance parameters (the outcome regression and the propensity score) have been developed in recent years. However, it is still unclear what is the necessary and sufficient sparsity condition on the nuisance parameters such that we can estimate the treatment effect at $1 / \sqrt{n}$-rate. In this paper, we propose a new Double-Calibration strategy that corrects the estimation bias of the nuisance parameter estimates computed by regularized high-dimensional techniques and demonstrate that the corresponding Doubly-Calibrated estimator achieves $1 / \sqrt{n}$-rate as long as one of the nuisance parameters is sparse with sparsity below $\sqrt{n} / \log p$, where $p$ denotes the ambient dimension of the covariates, whereas the other nuisance parameter can be arbitrarily complex and completely misspecified. The Double-Calibration strategy can also be applied to settings other than treatment effect estimation, e.g. regression coefficient estimation in the presence of a diverging number of controls in a semiparametric partially linear model, and local average treatment effect estimation with instrumental variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04174v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Liu, Xinbo Wang, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>Accounting statement analysis at industry level. A gentle introduction to the compositional approach</title>
      <link>https://arxiv.org/abs/2305.16842</link>
      <description>arXiv:2305.16842v5 Announce Type: replace-cross 
Abstract: Compositional data are contemporarily defined as positive vectors, the ratios among whose elements are of interest to the researcher. Financial statement analysis by means of accounting ratios a.k.a. financial ratios fulfils this definition to the letter. Compositional data analysis solves the major problems in statistical analysis of standard financial ratios at industry level, such as skewness, non-normality, non-linearity, outliers, and dependence of the results on the choice of which accounting figure goes to the numerator and to the denominator of the ratio. Despite this, compositional applications to financial statement analysis are still rare. In this article, we present some transformations within compositional data analysis that are particularly useful for financial statement analysis. We show how to compute industry or sub-industry means of standard financial ratios from a compositional perspective by means of geometric means. We show how to visualise firms in an industry with a compositional principal-component-analysis biplot; how to classify them into homogeneous financial performance profiles with compositional cluster analysis; and how to introduce financial ratios as variables in a statistical model, for instance to relate financial performance and firm characteristics with compositional regression models. We show an application to the accounting statements of Spanish wineries using the decomposition of return on equity by means of DuPont analysis, and a step-by-step tutorial to the compositional freeware CoDaPack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16842v5</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Germ\`a Coenders (University of Girona), N\'uria Arimany Serrat (University of Vic - Central University of Catalonia)</dc:creator>
    </item>
    <item>
      <title>Fast spatial simulation of extreme high-resolution radar precipitation data using INLA</title>
      <link>https://arxiv.org/abs/2307.11390</link>
      <description>arXiv:2307.11390v3 Announce Type: replace-cross 
Abstract: Aiming to deliver improved precipitation simulations for hydrological impact assessment studies, we develop a methodology for modelling and simulating high-dimensional spatial precipitation extremes, focusing on both their marginal distributions and tail dependence structures. Tail dependence is crucial for assessing the consequences of extreme precipitation events, yet most stochastic weather generators do not attempt to capture this property. The spatial distribution of precipitation occurrences is modelled with four competing models, while the spatial distribution of nonzero extreme precipitation intensities are modelled with a latent Gaussian version of the spatial conditional extremes model. Nonzero precipitation marginal distributions are modelled using latent Gaussian models with gamma and generalised Pareto likelihoods. Fast inference is achieved using integrated nested Laplace approximations (INLA). We model and simulate spatial precipitation extremes in Central Norway, using 13 years of hourly radar data with a spatial resolution of $1 \times 1$~km$^2$, over an area of size $6461$~km$^2$, to describe the behaviour of extreme precipitation over a small drainage area. Inference on this high-dimensional data set is achieved within hours, and the simulations capture the main trends of the observed precipitation well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11390v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silius M. Vandeskog, Rapha\"el Huser, Oddbj{\o}rn Bruland, Sara Martino</dc:creator>
    </item>
    <item>
      <title>Under the null of valid specification, pre-tests cannot make post-test inference liberal</title>
      <link>https://arxiv.org/abs/2407.03725</link>
      <description>arXiv:2407.03725v2 Announce Type: replace-cross 
Abstract: Consider a parameter of interest, which can be consistently estimated under some conditions. Suppose also that we can at least partly test these conditions with specification tests. We consider the common practice of conducting inference on the parameter of interest conditional on not rejecting these tests. We show that if the tested conditions hold, conditional inference is valid, though possibly conservative. This holds generally, without imposing any assumption on the asymptotic dependence between the estimator of the parameter of interest and the specification test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03725v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cl\'ement de Chaisemartin, Xavier D'Haultf{\oe}uille</dc:creator>
    </item>
  </channel>
</rss>
