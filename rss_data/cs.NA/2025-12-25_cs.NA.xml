<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Dec 2025 05:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A dichotomy of finite element spaces and its application to an energy-conservative scheme for the regularized long wave equation</title>
      <link>https://arxiv.org/abs/2512.20737</link>
      <description>arXiv:2512.20737v1 Announce Type: new 
Abstract: Certain energy-conservative Galerkin discretizations for nonlinear dispersive wave equations have revealed an unusual convergence behavior: optimal convergence is attained when continuous Lagrange finite element spaces of odd polynomial degree are employed, whereas the use of even-degree polynomials leads to reduced accuracy. The present work demonstrates that this behavior is intrinsic to the structure of the finite element spaces themselves. In particular, it is shown to be closely connected to the standard $L^2$-projection of derivatives, which possesses a super-approximation property exclusively for odd polynomial degrees. We also examine the implications of this feature for an energy-conservative Galerkin approximation of the regularized long-wave equation where the energy is a cubic functional. Although the resulting scheme conserves both mass and energy, we further show that the impulse is approximated with high accuracy, and we establish {\em a priori} error bounds for the associated semi-discrete formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20737v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitrios Antonopoulos, Dimitrios Mitsotakis</dc:creator>
    </item>
    <item>
      <title>On stability of Weak Greedy Algorithm in the presence of noise</title>
      <link>https://arxiv.org/abs/2512.20750</link>
      <description>arXiv:2512.20750v1 Announce Type: new 
Abstract: This paper is devoted to the theoretical study of the efficiency, namely, stability of some greedy algorithms. In the greedy approximation theory researchers are mostly interested in the following two important properties of an algorithm -- convergence and rate of convergence. In this paper we present some results on one more important property of an algorithm -- stability. Stability means that small perturbations do not result in a large change in the outcome of the algorithm. In this paper we discuss one kind of perturbations -- noisy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20750v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V. N. Temlyakov</dc:creator>
    </item>
    <item>
      <title>Streamfunction-vorticity formulation for incompressible viscid and inviscid flows on general surfaces</title>
      <link>https://arxiv.org/abs/2512.20763</link>
      <description>arXiv:2512.20763v1 Announce Type: new 
Abstract: This paper presents a streamfunction-vorticity formulation for the Navier--Stokes and Euler equations on general surfaces. Notably, this includes non-simply connected surfaces, on which the harmonic components of the velocity field play a fundamental role in the dynamics. By relying only on scalar and finite-dimensional quantities, our formulation ensures that the resulting methods give exactly tangential and incompressible velocity fields, while also being pressure robust. Compared to traditional methods based on velocity-pressure formulations, where one can only guarantee these structural properties by increasing the computational costs, this is a key advantage. We rigorously validate our formulation by proving its equivalence to the well understood velocity-pressure formulation under reasonable regularity assumptions. Furthermore, we demonstrate the applicability of the approach with numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20763v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Br\"uers, Christoph Lehrenfeld, Max Wardetzky</dc:creator>
    </item>
    <item>
      <title>Computing nonlinear Schr\"odinger equations with Hermite functions beyond harmonic traps</title>
      <link>https://arxiv.org/abs/2512.20840</link>
      <description>arXiv:2512.20840v1 Announce Type: new 
Abstract: Hermite basis functions are a powerful tool for spatial discretisation of Schr\"odinger equations with harmonic potential. In this work we show that their stability properties extend to the simulation of Schr\"odinger equations without potential, thus leading them as a natural basis for computation of nonlinear dispersive equations on unbounded domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20840v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valeria Banica, Georg Maierhofer, Katharina Schratz</dc:creator>
    </item>
    <item>
      <title>Parameter-free inexact block Schur complement preconditioning for linear poroelasticity under a hybrid Bernardi-Raugel and weak Galerkin finite element discretization</title>
      <link>https://arxiv.org/abs/2512.20844</link>
      <description>arXiv:2512.20844v1 Announce Type: new 
Abstract: This work investigates inexact block Schur complement preconditioning for linear poroelasticity problems discretized using a hybrid approach: Bernardi-Raugel elements for solid displacement and lowest-order weak Galerkin elements for fluid pressure. When pure Dirichlet boundary conditions are applied to the displacement, the leading block of the resulting algebraic system becomes almost singular in the nearly incompressible (locking) regime, hindering efficient iterative solution. To overcome this, the system is reformulated as a three-field problem with an inherent regularization that maintains the original solution while ensuring nonsingularity. Analysis shows that both the minimal residual (MINRES) and generalized minimal residual (GMRES) methods, when preconditioned with inexact block diagonal and triangular Schur complement preconditioners, achieve convergence independent of mesh size and the locking parameter for the regularized system. Similar theoretical results are established for the situation with displacement subject to mixed boundary conditions, even without regularization. Numerical experiments in 2D and 3D confirm the benefits of regularization under pure Dirichlet conditions and the robustness of the preconditioners with respect to mesh size and the locking parameter in both boundary condition scenarios. Finally, a spinal cord simulation with discontinuous material parameters further illustrates the effectiveness and robustness of the proposed iterative solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20844v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weizhang Huang, Zhuoran Wang</dc:creator>
    </item>
    <item>
      <title>Mixed Precision General Alternating-Direction Implicit Method for Solving Large Sparse Linear Systems</title>
      <link>https://arxiv.org/abs/2512.21164</link>
      <description>arXiv:2512.21164v1 Announce Type: new 
Abstract: In this article, we introduce a three-precision formulation of the General Alternating-Direction Implicit method (GADI) designed to accelerate the solution of large-scale sparse linear systems $Ax=b$. GADI is a framework that can represent many existing Alternating-Direction Implicit (ADI) methods. These methods are a class of linear solvers based on a splitting of $A$ such that the solution of the original linear system can be decomposed into the successive computation of easy-to-solve structured subsystems. Our proposed mixed precision scheme for GADI solves these subsystems in low precision to reduce the overall execution time while computing the residual and solution update in high precision to enable the solution to converge to high accuracy. We develop a rounding error analysis of mixed precision GADI that establishes the rates of convergence of the forward and backward errors to certain limiting accuracies. Our analysis also highlights the conditions on the splitting matrices under which mixed precision GADI is guaranteed to converge for a given set of precisions. We then discuss a systematic and robust strategy for selecting the GADI regularization parameter $\alpha$, whose adjustment is critical for performance. Specifically, our proposed strategy makes use of a Gaussian Process Regression (GPR) model trained on a dataset of low-dimensional problems to initialize $\alpha$. Finally, we proceed to a performance analysis of mixed precision GADI on an NVIDIA A100 GPU to validate our approach. Using low precision (Bfloat16 or FP32) to solve the subsystems, we obtain speedups of $2.6\times$, $1.7\times$, and $3.1\times$ over a full double precision GADI implementation on large-scale 2D, 3D convection-diffusion and complex reaction-diffusion problems (up to $1.3\times 10^{8}$ unknowns), respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21164v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jifeng Ge, Bastien Vieubl\'e, Juan Zhang</dc:creator>
    </item>
    <item>
      <title>A mixed finite element method for the stochastic Boussinesq equations with multiplicative noise</title>
      <link>https://arxiv.org/abs/2512.21297</link>
      <description>arXiv:2512.21297v1 Announce Type: new 
Abstract: This work investigates a fully discrete mixed finite element method for the stochastic Boussinesq system driven by multiplicative noise. The spatial discretization is performed using a standard mixed finite element method, while the temporal discretization is based on a semi-implicit Euler-Maruyama scheme. By combining a localization technique with high-moment stability estimates, we establish error bounds for the velocity, pressure, and temperature approximations. As a direct consequence, we prove convergence in probability for the fully discrete method in both $L^2$ and $H^1$-type norms. Several numerical experiments are presented to validate the theoretical error estimates and demonstrate the effectiveness of the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21297v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liet Vo</dc:creator>
    </item>
    <item>
      <title>FORCE-$\alpha$ Numerical Fluxes within the Arbitrary High Order Semidiscrete WENO-DeC Framework: A Competitive Alternative to Upwind Fluxes</title>
      <link>https://arxiv.org/abs/2512.21306</link>
      <description>arXiv:2512.21306v1 Announce Type: new 
Abstract: This work systematically investigates the performance of FORCE--$\alpha$ numerical fluxes within an arbitrary high order semidiscrete finite volume (FV) framework for hyperbolic partial differential equations (PDEs). Such numerical fluxes have been recently introduced by Toro, Saggiorato, Tokareva, and Hidalgo (Journal of Computational Physics, 416, 2020), and constitute a family of centred fluxes obtained from a suitable modification of First--Order Centred (FORCE) numerical fluxes. In contrast with upwind fluxes, such as Rusanov, Harten--Lax--van Leer (HLL) or the exact Riemann solver (RS) numerical flux, centred ones do not consider in any way the structure of the Riemann problem at cell interfaces. Adopting centred numerical fluxes leads to a high level of flexibility of the resulting numerical schemes, for example in the context of complicated hyperbolic systems, for which RSs may be impossible to construct or computationally expensive.
  The baseline framework adopted in this investigation is a FV semidiscrete approach with Weighted Essentially Non--Oscillatory (WENO) spatial reconstruction and Deferred Correction (DeC) time discretization, and results are reported up to order 7. Previous investigations involving the same framework have established that increasing the order of accuracy tends to decrease the differences in the results obtained through different numerical fluxes. The goal of this paper is to show that the employment of FORCE--$\alpha$ numerical fluxes within such a framework is a competitive alternative to the adoption of more classical upwind fluxes. The hyperbolic system considered for this investigation is the ideal Euler equations in one and two space dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21306v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Micalizzi, Eleuterio Toro</dc:creator>
    </item>
    <item>
      <title>Variationally correct operator learning: Reduced basis neural operator with a posteriori error estimation</title>
      <link>https://arxiv.org/abs/2512.21319</link>
      <description>arXiv:2512.21319v1 Announce Type: new 
Abstract: Minimizing PDE-residual losses is a common strategy to promote physical consistency in neural operators. However, standard formulations often lack variational correctness, meaning that small residuals do not guarantee small solution errors due to the use of non-compliant norms or ad hoc penalty terms for boundary conditions. This work develops a variationally correct operator learning framework by constructing first-order system least-squares (FOSLS) objectives whose values are provably equivalent to the solution error in PDE-induced norms. We demonstrate this framework on stationary diffusion and linear elasticity, incorporating mixed Dirichlet-Neumann boundary conditions via variational lifts to preserve norm equivalence without inconsistent penalties. To ensure the function space conformity required by the FOSLS loss, we propose a Reduced Basis Neural Operator (RBNO). The RBNO predicts coefficients for a pre-computed, conforming reduced basis, thereby ensuring variational stability by design while enabling efficient training. We provide a rigorous convergence analysis that bounds the total error by the sum of finite element discretization bias, reduced basis truncation error, neural network approximation error, and statistical estimation errors arising from finite sampling and optimization. Numerical benchmarks validate these theoretical bounds and demonstrate that the proposed approach achieves superior accuracy in PDE-compliant norms compared to standard baselines, while the residual loss serves as a reliable, computable a posteriori error estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21319v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Qiu, Wolfgang Dahmen, Peng Chen</dc:creator>
    </item>
    <item>
      <title>Prediction Air Temperature in Geothermal Heat Exchangers Using Pseudorandom Numbers: The New DARL Model</title>
      <link>https://arxiv.org/abs/2512.19976</link>
      <description>arXiv:2512.19976v1 Announce Type: cross 
Abstract: The use of Earth-Air-Water Heat Exchangers (EAWHE) for sustainable air conditioning has not been widely studied. Due to their experimental nature, methods of characterizing internal thermal air distribution impose high dependence on instrumentation by sensors and entail data acquisition and computational costs. This document presents an alternative method that estimates air temperature distribution while minimizing the need for a dense network of sensors in the experimental system. The proposed model, DARL (Data of Air and Random Length), can predict the temperature of air circulating inside EAWHEs. DARL is a significant methodological advance that integrates experimental data from boundary conditions with simulations based on pseudo-random numbers (PRNs). These PRNs are generated using Fermat's prime numbers as seeds to initialize the generator. Ordinary linear regressions and robust statistical validations, including the Shapiro-Wilk test and root mean square error, have demonstrated that the model can estimate the thermal distribution of air at different lengths with a relative error of less than 6.2%. These results demonstrate the model's efficiency, predictive capacity, and potential to reduce dependence on sensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19976v1</guid>
      <category>cs.CY</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C. Ram\'irez-Dolores, J. C. Zamora-Luria, J. A. Altamirano-Acosta, L. Sarao-Cruz, P. Jim\'enez-Palma, J. Moreno-Falconi</dc:creator>
    </item>
    <item>
      <title>Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer</title>
      <link>https://arxiv.org/abs/2512.20777</link>
      <description>arXiv:2512.20777v1 Announce Type: cross 
Abstract: The matrix exponential is a fundamental operator in scientific computing and system simulation, with applications ranging from control theory and quantum mechanics to modern generative machine learning. While Pad\'e approximants combined with scaling and squaring have long served as the standard, recent Taylor-based methods, which utilize polynomial evaluation schemes that surpass the classical Paterson--Stockmeyer technique, offer superior accuracy and reduced computational complexity. This paper presents an optimized Taylor-based algorithm for the matrix exponential, specifically designed for the high-throughput requirements of generative AI flows. We provide a rigorous error analysis and develop a dynamic selection strategy for the Taylor order and scaling factor to minimize computational effort under a prescribed error tolerance. Extensive numerical experiments demonstrate that our approach provides significant acceleration and maintains high numerical stability compared to existing state-of-the-art implementations. These results establish the proposed method as a highly efficient tool for large-scale generative modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20777v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Sastre, Daniel Faronbi, Jos\'e Miguel Alonso, Peter Traver, Javier Ib\'a\~nez, Nuria Lloret</dc:creator>
    </item>
    <item>
      <title>Implicit Numerical Scheme for the Hamilton-Jacobi-Bellman Quasi-Variational Inequality in the Optimal Market-Making Problem with Alpha Signal</title>
      <link>https://arxiv.org/abs/2512.20850</link>
      <description>arXiv:2512.20850v1 Announce Type: cross 
Abstract: We address the problem of combined stochastic and impulse control for a market maker operating in a limit order book. The problem is formulated as a Hamilton-Jacobi-Bellman quasi-variational inequality (HJBQVI). We propose an implicit time-discretization scheme coupled with a policy iteration algorithm. This approach removes time-step restrictions typical of explicit methods and ensures unconditional stability. Convergence to the unique viscosity solution is established by verifying monotonicity, stability, and consistency conditions and applying the comparison principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20850v1</guid>
      <category>q-fin.MF</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexey Meteykin</dc:creator>
    </item>
    <item>
      <title>The Dynamical Anatomy of Anderson Acceleration:From Adaptive Momentum to Variable-Mass ODEs</title>
      <link>https://arxiv.org/abs/2512.21269</link>
      <description>arXiv:2512.21269v1 Announce Type: cross 
Abstract: This paper provides a rigorous derivation and analysis of accelerated optimization algorithms through the lens of High-Resolution Ordinary Differential Equations (ODEs). While classical Nesterov acceleration is well-understood via asymptotic vanishing damping, the dynamics of Anderson Acceleration (AA) remain less transparent. This work makes significant theoretical contributions to AA by bridging discrete acceleration algorithms with continuous dynamical systems, while also providing practical algorithmic innovations. Our work addresses fundamental questions about the physical nature of Anderson Acceleration that have remained unanswered since its introduction in 1965. Firstly, we prove that AA can be exactly rewritten as an adaptive momentum method and, in the high-resolution limit, converges to a second-order ODE with Variable Effective Mass. Through a Lyapunov energy analysis, we reveal the specific instability mechanism of standard AA: unchecked growth in effective mass acts as negative damping, physically injecting energy into the system and violating dissipation constraints. Conversely, high-resolution analysis identifies an implicit Hessian-driven damping term that provides stabilization in stiff regimes. Leveraging these dynamical insights, we then propose Energy-Guarded Anderson Acceleration (EG-AA), an algorithm that acts as an inertial governor to enforce thermodynamic consistency. Morevoer, our convergence analysis, formulated via the Acceleration Gain Factor, proves that EG-AA improves upon gradient descent by maximizing the geometric contraction of the linear subspace projection while actively suppressing nonlinear approximation errors. Theoretical bounds confirm that EG-AA is no worse than standard AA, and numerical experiments demonstrate strictly improved convergence stability and rates in ill-conditioned convex composite problems compared to standard Anderson mixing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21269v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kewang Chen, Yongqiu Jiang, Kees Vuik</dc:creator>
    </item>
    <item>
      <title>Concrete examples of the rate of convergence of Chernoff approximations: numerical results for the heat semigroup and open questions on them (with appendix: full list of pictures and Python code)</title>
      <link>https://arxiv.org/abs/2301.05284</link>
      <description>arXiv:2301.05284v5 Announce Type: replace 
Abstract: The article is devoted to the construction of examples that illustrate (using computer calculations) the rate of convergence of Chernoff approximations to the solution of the Cauchy problem for the heat equation. We are interested in the Chernoff theorem in general and select the heat semigroup as a model case because this semigroup (and solutions of the heat equations) are known, so it is easy to measure the distance between the exact solution and its Chernoff approximations. Two Chernoff functions (of the first and second order of Chernoff tangency to the generator of the heat semigroup, i.e. to the operator of taking the second derivative) and several initial conditions of different smoothness are considered. From the numerically plotted graphs, visually, it is determined that the approximations are close to the solution. For each of the two Chernoff functions, for several initial conditions of different smoothness and for approximation numbers up to 11 inclusive, the error (i.e. the supremum of the absolute value of the difference between the exact solution and the approximating function) corresponding to each approximation was numerically found. As it turned out, in all the cases studied, the dependence of the error on the number of the approximation has an approximately power-law form (we call this power the order of convergence). This follows from the fact that, as we discovered, the dependence of the logarithm of the error on the logarithm of the approximation number is approximately linear. Using the considered family of initial conditions, an empirical dependence of the order of convergence on the smoothness class of the initial condition is found. The orders of convergence for all the initial conditions studied are collected in a table.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.05284v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>K. A. Katalova (Dragunova), N. Nikbakht, I. D. Remizov</dc:creator>
    </item>
    <item>
      <title>Annealing-based approach to solving partial differential equations</title>
      <link>https://arxiv.org/abs/2406.17364</link>
      <description>arXiv:2406.17364v4 Announce Type: replace 
Abstract: Solving partial differential equations (PDEs) using an annealing-based approach involves solving generalized eigenvalue problems. Discretizing a PDE yields a system of linear equations (SLE). Solving an SLE can be formulated as a general eigenvalue problem, which can be transformed into an optimization problem with an objective function given by a generalized Rayleigh quotient. The proposed algorithm requires iterative computations. However, it enables efficient annealing-based computation of eigenvectors to arbitrary precision without increasing the number of variables. Investigations using simulated annealing demonstrate how the number of iterations scales with system size and annealing time. Computational performance depends on system size, annealing time, and problem characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17364v4</guid>
      <category>math.NA</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.NA</category>
      <category>quant-ph</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazue Kudo</dc:creator>
    </item>
    <item>
      <title>Physics-Informed Inference Time Scaling for Solving High-Dimensional PDE via Defect Correction</title>
      <link>https://arxiv.org/abs/2504.16172</link>
      <description>arXiv:2504.16172v3 Announce Type: replace 
Abstract: Solving high-dimensional partial differential equations (PDEs) is a critical challenge where modern data-driven solvers often lack reliability and rigorous error guarantees. We introduce Simulation-Calibrated Scientific Machine Learning (SCaSML), a framework that systematically improves pre-trained PDE solvers at inference time without any retraining. Our core idea is to use defect correction method that derive a new PDE, termed Structural-preserving Law of Defect, that precisely describes the error of a given surrogate model. Since it retains the structure of the original problem, we can solve it efficiently with traditional stochastic simulators and correct the initial machine-learned solution. We prove that SCaSML achieves a faster convergence rate, with a final error bounded by the product of the surrogate and simulation errors. On challenging PDEs up to 160 dimensions, SCaSML reduces the error of various surrogate models, including PINNs and Gaussian Processes, by 20-80%. Code of SCaSML is available at https://github.com/Francis-Fan-create/SCaSML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16172v3</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zexi Fan, Yan Sun, Shihao Yang, Yiping Lu</dc:creator>
    </item>
    <item>
      <title>The Fast Newton Transform: Interpolation in Downward Closed Polynomial Spaces</title>
      <link>https://arxiv.org/abs/2505.14909</link>
      <description>arXiv:2505.14909v3 Announce Type: replace 
Abstract: We present the Fast Newton Transform (FNT), an algorithm for performing $m$-variate Newton interpolation in downward closed polynomial spaces with time complexity $\mathcal{O}(|A|m\overline{n})$. Here, $A$ is a downward closed set of cardinality $|A|$ equal to the dimension of the associated downward closed polynomial space $\Pi_A$, where $\overline{n}$ denotes the mean of the maximum polynomial degrees across the spatial dimensions $m$. For functions being analytic in an open Bernstein poly-ellipse, geometric approximation rates apply, when interpolating with respect to $\ell^p$-sets $A_{m,n,p}$, in non-tensorial Leja ordered Chebyshev-Lobatto or Leja grids. Especially, the $\ell^2$-Euclidean case $A_{m,n,2}$ turns out to be the pivotal choice to mitigate the curse of dimensionality, leading to a ratio $|A_{m,n,2}| / |A_{m,n,\infty}|$ that decays exponentially with spatial dimension $m$, while reaching close to or the same approximation power as the tensorial $\ell^\infty$-case.
  Expanding non-periodic functions, the FNT complements the approximation capabilities of the Fast Fourier Transform (FFT), whereas the choice of $\ell^p$-sets renders the FNT time complexity to be less than the FFT time complexity in a wide range of $n$, that exponentially increases with $m$. Maintaining this advantage true for the differentials, the FNT sets a new standard in $m$-variate interpolation and approximation practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14909v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Phil-Alexander Hofmann, Michael Hecht</dc:creator>
    </item>
    <item>
      <title>Band-Limited Equivalence of Convolution Operators and its Application to Filtered Vorticity Dynamics</title>
      <link>https://arxiv.org/abs/2508.14108</link>
      <description>arXiv:2508.14108v2 Announce Type: replace 
Abstract: In this study, we established a general theorem regarding the equivalence of convolution operators restricted to a finite spectral band. We demonstrated that two kernels with identical Fourier transforms over the resolved band act identically on all band-limited functions, even if their kernels differ outside the band. This property is significant in applied mathematics and computational physics, particularly in scenarios where measurements or simulations are spectrally truncated. As an application, we examine the proportionality relation $S(\boldsymbol {r}) \approx \zeta\,\omega(\boldsymbol{r})$ in filtered vorticity dynamics and clarify why real-space diagnostics can underestimate the spectral proportionality due to unobservable degrees of freedom. Our theoretical findings were supported by numerical illustrations using synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14108v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satori Tsuzuki</dc:creator>
    </item>
    <item>
      <title>Compact Schemes for $A^+B$, $A^+AB$ and $AA^+B$</title>
      <link>https://arxiv.org/abs/2511.12855</link>
      <description>arXiv:2511.12855v2 Announce Type: replace 
Abstract: Explicit details are presented for calculation of $A^+B$, $A^+AB$ and $AA^+B$ where $A_{m\times n}$ is any nonzero matrix, $A^+$ is the Moore-Penrose pseudoinverse of $A$ and $B$ is any matrix of appropriate dimensions, where the quantities in question are found using only the storage originally allocated to the matrices $A$ and $B$ (together with some simple one dimensional indexing arrays).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12855v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Stromberg</dc:creator>
    </item>
    <item>
      <title>A massively parallel non-overlapping Schwarz preconditioner for PolyDG methods in brain electrophysiology</title>
      <link>https://arxiv.org/abs/2512.19536</link>
      <description>arXiv:2512.19536v2 Announce Type: replace 
Abstract: We investigate non-overlapping Schwarz preconditioners for the algebraic systems stemming from high-order discretizations of the coupled monodomain and Barreto-Cressman models, with applications to brain electrophysiology. The spatial discretization is based on a high-order Polytopal Discontinuous Galerkin (PolyDG) method, coupled with the Crank-Nicolson time discretization scheme with explicit extrapolation of the ion term. To improve solver efficiency, we consider additive Schwarz preconditioners within the PolyDG framework, which combines (massively parallel) local subdomain solvers with a coarse-grid correction. Numerical experiments demonstrate robustness with respect to the discretization parameters, as well as a significant reduction in iteration counts compared to the unpreconditioned solver. These features make the proposed approach well-suited for parallel large-scale simulations in brain electrophysiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19536v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caterina B. Leimer Saglio, Stefano Pagani, Paola F. Antonietti</dc:creator>
    </item>
    <item>
      <title>Explicit numerical approximations for McKean-Vlasov stochastic differential equations in finite and infinite time</title>
      <link>https://arxiv.org/abs/2401.02878</link>
      <description>arXiv:2401.02878v4 Announce Type: replace-cross 
Abstract: Inspired by the stochastic particle method, this paper establishes an easily implementable explicit numerical method for McKean-Vlasov stochastic differential equations (MV-SDEs) with superlinear growth coefficients. The paper establishes the theory on the propagation of chaos in the Lq sense. The optimal uniform-in-time strong convergence rate 1/2-order of the numerical solutions is obtained for the interacting particle system. Furthermore, it is proved that the numerical solutions capture the long-term dynamical behaviors of MV-SDEs precisely, including moment boundedness, stability, and ergodicity. Moreover, a unique numerical invariant probability measure is yielded, which converges to the underlying invariant probability measure of MV-SDEs in the L2-Wasserstein distance. Finally, several numerical experiments are carried out to support the main results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02878v4</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanping Cui, Xiaoyue Li, Yi Liu, Fengyu Wang</dc:creator>
    </item>
    <item>
      <title>HYLU: Hybrid Parallel Sparse LU Factorization</title>
      <link>https://arxiv.org/abs/2509.07690</link>
      <description>arXiv:2509.07690v5 Announce Type: replace-cross 
Abstract: This article introduces HYLU, a hybrid parallel LU factorization-based general-purpose solver designed for efficiently solving sparse linear systems (Ax=b) on multi-core shared-memory architectures. The key technical feature of HYLU is the integration of hybrid numerical kernels so that it can adapt to various sparsity patterns of coefficient matrices. Tests on 34 sparse matrices from SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL PARDISO in the numerical factorization phase by geometric means of 2.04X (for one-time solving) and 2.58X (for repeated solving). HYLU can be downloaded from https://github.com/chenxm1986/hylu.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07690v5</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoming Chen</dc:creator>
    </item>
    <item>
      <title>TOMATOES: Topology and Material Optimization for Latent Heat Thermal Energy Storage Devices</title>
      <link>https://arxiv.org/abs/2510.07057</link>
      <description>arXiv:2510.07057v2 Announce Type: replace-cross 
Abstract: Latent heat thermal energy storage (LHTES) systems are compelling candidates for energy storage, primarily owing to their high storage density. Improving their performance is crucial for developing the next-generation efficient and cost effective devices. Topology optimization (TO) has emerged as a powerful computational tool to design LHTES systems by optimally distributing a high-conductivity material (HCM) and a phase change material (PCM). However, conventional TO typically limits to optimizing the geometry for a fixed, pre-selected materials. This approach does not leverage the large and expanding databases of novel materials. Consequently, the co-design of material and geometry for LHTES remains a challenge and unexplored.
  To address this limitation, we present an automated design framework for the concurrent optimization of material choice and topology. A key challenge is the discrete nature of material selection, which is incompatible with the gradient-based methods used for TO. We overcome this by using a data-driven variational autoencoder (VAE) to project discrete material databases for both the HCM and PCM onto continuous and differentiable latent spaces. These continuous material representations are integrated into an end-to-end differentiable, transient nonlinear finite-element solver that accounts for phase change. We demonstrate this framework on a problem aimed at maximizing the discharged energy within a specified time, subject to cost constraints. The effectiveness of the proposed method is validated through several illustrative examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07057v2</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Kumar Padhy, Krishnan Suresh, Aaditya Chandrasekhar</dc:creator>
    </item>
    <item>
      <title>On the variational dual formulation of the Nash system and an adaptive convex gradient-flow approach to nonlinear PDEs</title>
      <link>https://arxiv.org/abs/2512.12878</link>
      <description>arXiv:2512.12878v2 Announce Type: replace-cross 
Abstract: We investigate the influence of base states on the consistency of the dual variational formulation for quadratic systems of PDEs, which are not necessarily conservative (typical examples include the noise-free Nash system with a quadratic Hamiltonian and multiple players). We identify a sufficient condition under which consistency holds over large time intervals. In particular, in the single-player case, there exists a sequence of base states (each exhibiting full consistency) that converges in mean to zero. We also prove existence of variational dual solutions to the noise-free Nash system for arbitrary base states. Furthermore, we propose a scheme based on Hilbertian gradient flows that, starting from an arbitrary base state, generates a sequence of new base states that is expected to converge to a solution of the original PDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12878v2</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitry Vorotnikov, Amit Acharya</dc:creator>
    </item>
  </channel>
</rss>
