<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Sep 2025 01:51:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An Error Model for Evaluating the Accuracy of Satellite-Based XCO$_2$ Products</title>
      <link>https://arxiv.org/abs/2509.16419</link>
      <description>arXiv:2509.16419v1 Announce Type: new 
Abstract: Several satellites (e.g., OCO-2 &amp; 3) and their derived products now provide spatially extensive coverage of the abundance of carbon dioxide in the atmospheric column (XCO$_2$). However, the accuracy of the XCO$_2$ reported in these products needs to be carefully assessed for any downstream scientific analysis; this involves comparison with reference datasets, such as those from the Total Carbon Column Observing Network (TCCON). Previously, systematic and random errors have been used to quantify differences between satellite-based XCO$_2$ measurements and TCCON data. The spatiotemporal density of satellite observations enables the decomposition of the error variability into these components. This study aims to unify the definitions of these error components through a hierarchical statistical model with explicit mathematical terms, which enables a formal definition of the underlying assumptions and estimation of each component. Specifically, we focus on defining model elements, like global bias and systematic and random error, as part of this framework. We use it to compare OCO-2 XCO$_2$ v11.1 data (both original scenes from the `Lite' files and 10-sec averages) and gridded Making Earth System Data Records for Use in Research Environments (MEaSUREs) products to TCCON data. The MEaSUREs products exhibit comparable systematic errors to other OCO-2 products, with larger errors over land versus ocean. We describe the methodology for creating the MEaSUREs products, including their prior and posterior error covariances, with information on spatial correlation for efficient incorporation into scientific analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16419v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vineet Yadav, Jonathan Hobbs, Hai M. Nguyen, Susan S. Kulawik, Junjie Liu, David F. Baker, Isamu Morino, Hirofumi Ohyama, Voltaire A. Velazco, Mihalis Vrekoussis, Manvendra K. Dubey</dc:creator>
    </item>
    <item>
      <title>SynthIPD: assumption-lean synthetic individual patient data generation</title>
      <link>https://arxiv.org/abs/2509.16466</link>
      <description>arXiv:2509.16466v1 Announce Type: new 
Abstract: Individual patient data (IPD) are essential for statistical inference in clinical research. However, privacy concerns, high data-sharing costs, and restrictive access often make IPD unavailable. Conventional synthetic data generation usually relies on black box models such as generative adversial networks. These methods, however, requires a large piece of IPD for model training, may be ungeneralizable and lacks interpretability. This paper introduces an assumption-lean, three-step methodology for generating synthetic IPD with survival endpoints only based on published clinical trial articles. The method mainly leverages Kaplan-Meier (KM) curves with at-risk/censoring information and subgroup-level summary statistics. It digitizes the KM curve using Scalable Vector Graphics (SVG) beyond pixel accuracy and then generates synthetic covariates based on the statistics. We illustrate the method's potential through $2$ detailed case studies and simulation studies. The method offers important implications, enabling high-fidelity IPD generation to support evidence-based medical decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16466v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zixuan Zhao, Zexin Ren, Guannan Zhai, Feifang Hu, Will Ma, En Xie, Qian Shi</dc:creator>
    </item>
    <item>
      <title>Efficient Brain Network Estimation with Sparse ICA in Non-Human Primate Neuroimaging</title>
      <link>https://arxiv.org/abs/2509.16803</link>
      <description>arXiv:2509.16803v1 Announce Type: new 
Abstract: Independent component analysis (ICA) is widely used to separate mixed signals and recover statistically independent components. However, in non-human primate neuroimaging studies, most ICA-recovered spatial maps are often dense. To extract the most relevant brain activation patterns, post-hoc thresholding is typically applied-though this approach is often imprecise and arbitrary. To address this limitation, we employed the Sparse ICA method, which enforces both sparsity and statistical independence, allowing it to extract the most relevant activation maps without requiring additional post-processing. Simulation experiments demonstrate that Sparse ICA performs competitively against 11 classical linear ICA methods. We further applied Sparse ICA to real non-human primate neuroimaging data, identifying several independent component networks spanning different brain networks. These spatial maps revealed clearly defined activation areas, providing further evidence that Sparse ICA is effective and reliable in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16803v1</guid>
      <category>stat.AP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Li, Liang Ma, Masoud Seraji, Shujian Yu, Yun Wang, Jingyu Liu, Vince D. Calhoun</dc:creator>
    </item>
    <item>
      <title>An Italian Gender Equality Index</title>
      <link>https://arxiv.org/abs/2509.17140</link>
      <description>arXiv:2509.17140v1 Announce Type: new 
Abstract: Following the works on the Gender Equality Index (GEI), we propose a composite indicator to measure the gender gap across Italian regions. Our approach differs from the original GEI in both the selection of indicators and the aggregation methodology. Specifically, the choice of indicators is inspired by the both the GEI and the WeWorld Index Italia, while the aggregation relies on an original variation of the Mazziotta-Pareto Index. Finally, we apply our results drawing 2023 open data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17140v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lorenzo Panebianco</dc:creator>
    </item>
    <item>
      <title>A Bayesian dawn in linguistics: Trends, benefits and good practices</title>
      <link>https://arxiv.org/abs/2509.17161</link>
      <description>arXiv:2509.17161v1 Announce Type: new 
Abstract: In recent years, Bayesian statistics has gained traction across a wide range of scientific disciplines. This paper explores the growing application of Bayesian methods within the field of linguistics and considers their future potential. A survey of articles from different linguistics journals indicates that Bayesian regression has transitioned from fringe to fairly mainstream over the past five years. This paper discusses the main drivers of this shift, including the increased availability of user-friendly software and the replicability crisis in adjacent disciplines, which exposed the shortcomings of the traditional statistical paradigm. It outlines the fundamental conceptual distinctions between frequentist and Bayesian approaches, emphasizing how Bayesian methods can help address the problems. Additionally, the paper highlights the methodological benefits of Bayesian regression for a diverse array of research questions and data types. It also identifies key theoretical and practical challenges associated with Bayesian analysis and offers a set of good practices and recommendations for researchers considering the adoption of Bayesian methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17161v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Natalia Levshina</dc:creator>
    </item>
    <item>
      <title>ToMATo: an efficient and robust clustering algorithm for high dimensional datasets. An illustration with spike sorting</title>
      <link>https://arxiv.org/abs/2509.17499</link>
      <description>arXiv:2509.17499v1 Announce Type: new 
Abstract: Clustering algorithms became an essential part of the neurophysiological data analysis toolbox in the last twenty five years. Many problems, from the definition of cell types/groups based on morphological, molecular and physiological data to the identification of sub-networks in fMRI data, are now routinely tackled with clustering analysis. Since the datasets to which this type of analysis is applied tend to be defined in larger and larger dimensional spaces, there is a need for efficient and robust clustering methods in high dimension. There is also a need for methods that assume as little as possible about the clusters shape and size. We report here our experience with the ToMATo (Topological Mode Analysis Tool) algorithm. It is based on a definitely deep mathematical theory (algebraic topology), but its Python based open-source implementation is easily accessible to practitioners. We applied ToMATo to a problem we know well, spike sorting. Its capability to work in the ''native'' space of the data (no dimension reduction is required) is remarkable, as well as its robustness with respect to outliers (superposed spikes).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17499v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louise Martineau (IRMA), Christophe Pouzat (IRMA), S\'egolen Geffray (IRMA)</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to aggregated chemical exposure assessment</title>
      <link>https://arxiv.org/abs/2509.17557</link>
      <description>arXiv:2509.17557v1 Announce Type: new 
Abstract: Human exposure to chemicals commonly arises from multiple sources, yet traditional assessments often treat these sources in isolation, overlooking their combined impact. We introduce a Bayesian framework for aggregated chemical exposure assessment that explicitly accounts for these intertwined pathways. By integrating diverse datasets - such as consumption surveys, demographics, chemical measurements, and market presence - our approach addresses typical data challenges, including missing values, limited sample sizes, and inconsistencies, while incorporating relevant prior knowledge. Through a simulation-based strategy that reflects the full spectrum of individual exposure scenarios, we derive robust, population-level estimates of aggregated exposure. We demonstrate the value of this method using titanium dioxide, a chemical found in foods, dietary supplements, medicines, and personal care products. By capturing the complexity of real-world exposures, this comprehensive Bayesian approach provides decision-makers with more reliable probabilistic estimates to inform public health policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17557v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie Van Den Neucker, Alexander Grigoriev, Heidi Demaegdt, Jan Mast, Karlien Cheyns, Sofie De Broe, Roberto Cerina</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonhomogeneous hidden Markov models to leverage routine in physical activity monitoring with informative wear time</title>
      <link>https://arxiv.org/abs/2509.17806</link>
      <description>arXiv:2509.17806v1 Announce Type: new 
Abstract: Missing data is among the most prominent challenges in the analysis of physical activity (PA) data collected from wearable devices, with the threat of nonignorabile missingness arising when patterns of device wear relate to underlying activity patterns. We offer a rigorous consideration of assumptions about missing data mechanisms in the context of the common modeling paradigm of state space models with a finite, meaningful, set of underlying PA states. Focusing in particular on hidden Markov models, we identify inherent limitations in the presence of missing data when covariates are required to satisfy common missing data assumptions. In response to this limitation, we propose a Bayesian non-homogeneous state space model that can accommodate covariate dependence in the transitions between latent activity states, which in this case relates to whether patients' routine behavior can inform how they transition between PA states and thus support imputation of missing PA data. We show the benefits of the proposed model for missing data imputation and inference for relevant PA summaries. Our development advances analytic capacity to confront the ubiquitous challenge of missing data when analyzing PA studies using wearables. We illustrate with the analysis of a cohort of adolescent and young adult (AYA) cancer patients who wore commercial Fitbit devices for varying durations during the course of treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17806v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beatrice Cantoni, Savannah V. Rauschendorfer, Michael E. Roth, J. Andrew Livingston, Eugenie S. Kleinerman, Corwin M. Zigler</dc:creator>
    </item>
    <item>
      <title>Predicting First Year Dropout from Pre Enrolment Motivation Statements Using Text Mining</title>
      <link>https://arxiv.org/abs/2509.16224</link>
      <description>arXiv:2509.16224v1 Announce Type: cross 
Abstract: Preventing student dropout is a major challenge in higher education and it is difficult to predict prior to enrolment which students are likely to drop out and which students are likely to succeed. High School GPA is a strong predictor of dropout, but much variance in dropout remains to be explained. This study focused on predicting university dropout by using text mining techniques with the aim of exhuming information contained in motivation statements written by students. By combining text data with classic predictors of dropout in the form of student characteristics, we attempt to enhance the available set of predictive student characteristics. Our dataset consisted of 7,060 motivation statements of students enrolling in a non-selective bachelor at a Dutch university in 2014 and 2015. Support Vector Machines were trained on 75 percent of the data and several models were estimated on the test data. We used various combinations of student characteristics and text, such as TFiDF, topic modelling, LIWC dictionary. Results showed that, although the combination of text and student characteristics did not improve the prediction of dropout, text analysis alone predicted dropout similarly well as a set of student characteristics. Suggestions for future research are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16224v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>K. F. B. Soppe, A. Bagheri, S. Nadi, I. G. Klugkist, T. Wubbels, L. D. N. V. Wijngaards-De Meij</dc:creator>
    </item>
    <item>
      <title>Learning Centre Partitions from Summaries</title>
      <link>https://arxiv.org/abs/2509.16337</link>
      <description>arXiv:2509.16337v1 Announce Type: cross 
Abstract: Multi-centre studies increasingly rely on distributed inference, where sites share only centre-level summaries. Homogeneity of parameters across centres is often violated, motivating methods that both \emph{test} for equality and \emph{learn} centre groupings before estimation. We develop multivariate Cochran-type tests that operate on summary statistics and embed them in a sequential, test-driven \emph{Clusters-of-Centres (CoC)} algorithm that merges centres (or blocks) only when equality is not rejected. We derive the asymptotic $\chi^2$-mixture distributions of the test statistics and provide plug-in estimators for implementation. To improve finite-sample integration, we introduce a multi-round bootstrap CoC that re-evaluates merges across independently resampled summary sets; under mild regularity and a separation condition, we prove a \emph{golden-partition recovery} result: as the number of rounds grows with $n$, the true partition is recovered with probability tending to one. We also give simple numerical guidelines, including a plateau-based stopping rule, to make the multi-round procedure reproducible. Simulations and a real-data analysis of U.S.\ airline on-time performance (2007) show accurate heterogeneity detection and partitions that change little with the choice of resampling scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16337v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zinsou Max Debaly, Jean-Francois Ethier, Michael H. Neumann, F\'elix Camirand Lemyre</dc:creator>
    </item>
    <item>
      <title>Parameter variability can produce heavy tails in a model for the spatial distribution of settling organisms</title>
      <link>https://arxiv.org/abs/2509.16385</link>
      <description>arXiv:2509.16385v1 Announce Type: cross 
Abstract: We show that a simple mechanistic model of spatial dispersal for settling organisms, subject to parameter variability, can generate heavy-tailed radial probability density functions. The movement of organisms in the model consists of a two-dimensional diffusion that ceases after a random time, where the parameters that characterize each of these stages have been randomized. Our findings show that these minimal assumptions can yield heavy-tailed dispersal patterns, providing a simplified framework that increases the understanding of long-distance dispersal events in movement ecology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16385v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis F. Gordillo, Priscilla E. Greenwood</dc:creator>
    </item>
    <item>
      <title>Federated Learning for Financial Forecasting</title>
      <link>https://arxiv.org/abs/2509.16393</link>
      <description>arXiv:2509.16393v1 Announce Type: cross 
Abstract: This paper studies Federated Learning (FL) for binary classification of volatile financial market trends. Using a shared Long Short-Term Memory (LSTM) classifier, we compare three scenarios: (i) a centralized model trained on the union of all data, (ii) a single-agent model trained on an individual data subset, and (iii) a privacy-preserving FL collaboration in which agents exchange only model updates, never raw data. We then extend the study with additional market features, deliberately introducing not independent and identically distributed data (non-IID) across agents, personalized FL and employing differential privacy. Our numerical experiments show that FL achieves accuracy and generalization on par with the centralized baseline, while significantly outperforming the single-agent model. The results show that collaborative, privacy-preserving learning provides collective tangible value in finance, even under realistic data heterogeneity and personalization requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16393v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuel Noseda, Alberto De Luca, Lukas Von Briel, Nathan Lacour</dc:creator>
    </item>
    <item>
      <title>Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence</title>
      <link>https://arxiv.org/abs/2509.16599</link>
      <description>arXiv:2509.16599v1 Announce Type: cross 
Abstract: Background: Evidence synthesis facilitates evidence-based medicine. Without information retrieval techniques, this task is impossible due to the vast and expanding literature. Objective: Building on prior work, this study evaluates an information retrieval-driven workflow to enhance the efficiency, transparency, and reproducibility of systematic reviews. We use endometriosis recurrence as an ideal case due to its complex and ambiguous literature. Methods: Our hybrid approach integrates PRISMA guidelines with computational techniques. We applied semi-automated deduplication to efficiently filter records before manual screening. This workflow synthesized evidence from randomised controlled trials on the efficacy of a subclass of gonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method addressed unit-of-analysis errors in multi-arm trials. Results: Our workflow efficiently reduced the screening workload. It took only 11 days to fetch and filter 812 records. Seven RCTs were eligible, providing evidence from 841 patients in 4 countries. The pooled random-effects model yielded a Risk Ratio (RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity ($I^2=0.00\%$, $\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence. Sensitivity analyses and bias assessments supported the robustness of our findings. Conclusion: This study demonstrates an information-retrieval-driven workflow for medical evidence synthesis. Our approach yields valuable clinical results while providing a framework for accelerating the systematic review process. It bridges the gap between clinical research and computer science and can be generalized to other complex systematic reviews.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16599v1</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandro Tsang</dc:creator>
    </item>
    <item>
      <title>Optimal and Efficient Sample Size Re-estimation: A Dynamic Cost Framework</title>
      <link>https://arxiv.org/abs/2509.16636</link>
      <description>arXiv:2509.16636v1 Announce Type: cross 
Abstract: Adaptive sample size re-estimation (SSR) is a well-established strategy for improving the efficiency and flexibility of clinical trials. Its central challenge is determining whether, and by how much, to increase the sample size at an interim analysis. This decision requires a rational framework for balancing the potential gain in statistical power against the risk and cost of further investment. Prevailing optimization approaches, such as the Jennison and Turnbull (JT) method, address this by maximizing power for a fixed cost per additional participant. While statistically efficient, this paradigm assumes the cost of enrolling another patient is constant, regardless of whether the interim evidence is promising or weak. This can lead to impractical recommendations and inefficient resource allocation, particularly in weak-signal scenarios.
  We reframe SSR as a decision problem under dynamic costs, where the effective cost of additional enrollment reflects the interim strength of evidence. Within this framework, we derive two novel rules: (i) a likelihood-ratio based rule, shown to be Pareto optimal in achieving smaller average sample size under the null without loss of power under the alternative; and (ii) a return-on-investment (ROI) rule that directly incorporates economic considerations by linking SSR decisions to expected net benefit. To unify existing methods, we further establish a representation theorem demonstrating that a broad class of SSR rules can be expressed through implicit dynamic cost functions, providing a common analytical foundation for their comparison. Simulation studies calibrated to Phase III trial settings confirm that dynamic-cost approaches improve resource allocation relative to fixed-cost methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16636v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Jin, Cai Wu, Qiqi Deng</dc:creator>
    </item>
    <item>
      <title>Self-Tuned Rejection Sampling within Gibbs and a Case Study in Small Area Estimation</title>
      <link>https://arxiv.org/abs/2509.17155</link>
      <description>arXiv:2509.17155v1 Announce Type: cross 
Abstract: When preparing a Gibbs sampler, some conditionals may be unfamiliar distributions without well-known variate generation routines. Rejection sampling may be used to draw from such distributions exactly; however, it can be challenging to obtain practical proposal distributions. A practical proposal is one where accepted draws are not extremely rare occurrences and which is not too computationally intensive to use repeatedly within the Gibbs sampler. Consequently, approximate methods such as Metropolis-Hastings steps tend to be used in this setting. This work revisits the vertical weighted strips (VWS) method of proposal construction from arXiv:2401.09696 for univariate conditionals within Gibbs. VWS constructs a finite mixture based on the form of the target density and provides an upper bound on the rejection probability. The rejection probability can be reduced by refining terms in the finite mixture. Na\"{i}vely constructing a new proposal for each target encountered in a Gibbs sampler can be computationally impractical. Instead, we consider proposal distributions which persist over the Gibbs sampler and tune themselves gradually to avoid very high rejection probabilities while discarding mixture terms with low contribution. We explore a motivating application in small area estimation, applied to the estimation of county-level population counts of school-aged children in poverty. Here, a Gibbs sampler for a Bayesian model of interest includes a family of unfamiliar densities to be drawn for each observation in the data. Self-tuned VWS is applied to obtain exact draws within Gibbs while keeping the computational workload of proposal maintenance under control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17155v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew M. Raim, Kyle M. Irimata, James A. Livsey</dc:creator>
    </item>
    <item>
      <title>Detecting Urban PM$_{2.5}$ Hotspots with Mobile Sensing and Gaussian Process Regression</title>
      <link>https://arxiv.org/abs/2509.17175</link>
      <description>arXiv:2509.17175v1 Announce Type: cross 
Abstract: Low-cost mobile sensors can be used to collect PM$_{2.5}$ concentration data throughout an entire city. However, identifying air pollution hotspots from the data is challenging due to the uneven spatial sampling, temporal variations in the background air quality, and the dynamism of urban air pollution sources. This study proposes a method to identify urban PM$_{2.5}$ hotspots that addresses these challenges, involving four steps: (1) equip citizen scientists with mobile PM$_{2.5}$ sensors while they travel; (2) normalise the raw data to remove the influence of background ambient pollution levels; (3) fit a Gaussian process regression model to the normalised data and (4) calculate a grid of spatially explicit 'hotspot scores' using the probabilistic framework of Gaussian processes, which conveniently summarise the relative pollution levels throughout the city. We apply our method to create the first ever map of PM$_{2.5}$ pollution in Kigali, Rwanda, at a 200m resolution. Our results suggest that the level of ambient PM$_{2.5}$ pollution in Kigali is dangerously high, and we identify the hotspots in Kigali where pollution consistently exceeds the city-wide average. We also evaluate our method using simulated mobile sensing data for Beijing, China, where we find that the hotspot scores are probabilistically well calibrated and accurately reflect the 'ground truth' spatial profile of PM$_{2.5}$ pollution. Thanks to the use of open-source software, our method can be re-applied in cities throughout the world with a handful of low-cost sensors. The method can help fill the gap in urban air quality information and empower public health officials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17175v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ni\'al Perry, Peter P. Pedersen, Charles N. Christensen, Emanuel Nussli, Sanelma Heinonen, Lorena Gordillo Dagallier, Rapha\"el Jacquat, Sebastian Horstmann, Christoph Franck</dc:creator>
    </item>
    <item>
      <title>Hodge Decomposition for Urban Traffic Flow: Limits on Dense OD Graphs and Advantages on Road Networks - Los Angeles Case</title>
      <link>https://arxiv.org/abs/2509.17203</link>
      <description>arXiv:2509.17203v1 Announce Type: cross 
Abstract: I study Hodge decomposition (HodgeRank) for urban traffic flow on two graph representations: dense origin--destination (OD) graphs and road-segment networks. Reproducing the method of Aoki et al., we observe that on dense OD graphs the curl and harmonic components are negligible and the potential closely tracks node divergence, limiting the added value of Hodge potentials. In contrast, on a real road network (UTD19, downtown Los Angeles; 15-minute resolution), potentials differ substantially from divergence and exhibit clear morning/evening reversals consistent with commute patterns. We quantify smoothness and discriminability via local/global variances derived from the graph spectrum, and propose flow-aware embeddings that combine topology, bidirectional volume, and net-flow asymmetry for clustering. Code and preprocessing steps are provided to facilitate reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17203v1</guid>
      <category>cs.SI</category>
      <category>math.AT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifei Sun</dc:creator>
    </item>
    <item>
      <title>Everything all at once: On choosing an estimand for multi-component environmental exposures</title>
      <link>https://arxiv.org/abs/2509.17960</link>
      <description>arXiv:2509.17960v1 Announce Type: cross 
Abstract: Many research questions -- particularly those in environmental health -- do not involve binary exposures. In environmental epidemiology, this includes multivariate exposure mixtures with nondiscrete components. Causal inference estimands and estimators to quantify the relationship between an exposure mixture and an outcome are relatively few. We propose an approach to quantify a relationship between a shift in the exposure mixture and the outcome -- either in the single timepoint or longitudinal setting. The shift in the exposure mixture can be defined flexibly in terms of shifting one or more components, including examining interaction between mixture components, and in terms of shifting the same or different amounts across components. The estimand we discuss has a similar interpretation as a main effect regression coefficient. First, we focus on choosing a shift in the exposure mixture supported by observed data. We demonstrate how to assess extrapolation and modify the shift to minimize reliance on extrapolation. Second, we propose estimating the relationship between the exposure mixture shift and outcome completely nonparametrically, using machine learning in model-fitting. This is in contrast to other current approaches, which employ parametric modeling for at least some relationships, which we would like to avoid because parametric modeling assumptions in complex, nonrandomized settings are tenuous at best. We are motivated by longitudinal data on pesticide exposures among participants in the CHAMACOS Maternal Cognition cohort. We examine the relationship between longitudinal exposure to agricultural pesticides and risk of hypertension. We provide step-by-step code to facilitate the easy replication and adaptation of the approaches we use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17960v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kara E. Rudolph, Shodai Inose, Nicholas Williams, Ivan Diaz, Lucia Calderon, Jacqueline M. Torres, Marianthi-Anna Kioumourtzoglou</dc:creator>
    </item>
    <item>
      <title>Automated Detection of Short-term Slow Slip Events in Southwest Japan</title>
      <link>https://arxiv.org/abs/2202.12414</link>
      <description>arXiv:2202.12414v2 Announce Type: replace 
Abstract: Inferring from the occurrence pattern of slow slip events (SSEs) the probability of triggering a damaging earthquake within the nearby velocity weakening portion of the plate interface is critical for hazard mitigation. Although robust methods exist to detect long-term SSEs consistently and efficiently, detecting short-term SSEs remains a challenge. In this study, we propose a novel statistical approach, called singular spectrum analysis isolate-detect (SSAID), for automatically estimating the start and end times of short-term SSEs in GPS data. The method recasts the problem of detecting SSEs as that of identifying change-points in a piecewise non-linear signal. This is achieved by obscuring the deviation from piecewise-linearity in the underlying SSE signals using added noise. We verify its effectiveness on a range of model-generated synthetic SSE data with different noise levels, and demonstrate its superior performance compared to two existing methods. We illustrate its capability in detecting short-term SSEs in observed GPS data from 36 stations in southwest Japan via the co-occurrence of non-volcanic tremors, hypothesis tests and fault estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.12414v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Ma, Andreas Anastasiou, Fabien Montiel</dc:creator>
    </item>
    <item>
      <title>Estimating the Heritability of Longitudinal Rate-of-Change: Genetic Insights into PSA Velocity in Prostate Cancer-Free Individuals</title>
      <link>https://arxiv.org/abs/2505.04773</link>
      <description>arXiv:2505.04773v2 Announce Type: replace 
Abstract: Serum prostate-specific antigen (PSA) is widely used for prostate cancer screening. While the genetics of PSA levels has been studied to enhance screening accuracy, the genetic basis of PSA velocity, the rate of PSA change over time, remains unclear. The Prostate, Lung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial, a large, randomized study with longitudinal PSA data (15,260 cancer-free males, averaging 5.34 samples per subject) and genome-wide genotype data, provides a unique opportunity to estimate PSA velocity heritability. We developed a mixed model to jointly estimate heritability of PSA levels at age 54 and PSA velocity. To accommodate the large dataset, we implemented two efficient computational approaches: a partitioning and meta-analysis strategy using average information restricted maximum likelihood (AI-REML), and a fast restricted Haseman-Elston (REHE) regression method. Simulations showed that both methods yield unbiased estimates of both heritability metrics, with AI-REML providing smaller variability in the estimation of velocity heritability than REHE. Applying AI-REML to PLCO data, we estimated heritability at 0.32 (s.e. = 0.07) for baseline PSA and 0.45 (s.e. = 0.18) for PSA velocity. These findings reveal a substantial genetic contribution to PSA velocity, supporting future genome-wide studies to identify variants affecting PSA dynamics and improve PSA-based screening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04773v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pei Zhang, Xiaoyu Wang, Jianxin Shi, Paul S. Albert</dc:creator>
    </item>
    <item>
      <title>Does the draw matter in an incomplete round-robin tournament? The case of the UEFA Champions League</title>
      <link>https://arxiv.org/abs/2507.15320</link>
      <description>arXiv:2507.15320v2 Announce Type: replace 
Abstract: A fundamental reform has been introduced in the 2024/25 season of club competitions organised by the Union of European Football Associations (UEFA): the well-established group stage has been replaced by an incomplete round-robin format. In this format, the 36 teams are ranked in a single league table, but play against only a subset of the competitors. While this innovative change has highlighted that the incomplete round-robin tournament is a reasonable alternative to the standard design of allocating the teams into round-robin groups, the characteristics of the new format remain unexplored. Our paper contributes to this topic by using simulations to compare the uncertainty generated by the draw in the old format with that in the new format of the UEFA Champions League. We develop a method to break down the impact of the 2024/25 reform into various components for each team. The new format is found to decrease the overall effect of the draw. However, this reduction can mainly be attributed to the inaccurate seeding system used by UEFA. If the teams are seeded based on their actual strengths, the impact of the draw is about the same in a tournament with an incomplete round-robin league or a group stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15320v2</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi, Dries Goossens, Karel Devriesere, Roel Lambers, Frits Spieksma</dc:creator>
    </item>
    <item>
      <title>Tensor-Empowered Asset Pricing with Missing Data</title>
      <link>https://arxiv.org/abs/2508.01861</link>
      <description>arXiv:2508.01861v2 Announce Type: replace 
Abstract: Missing data in financial panels presents a critical obstacle, undermining asset-pricing models and reducing the effectiveness of investment strategies. Such panels are often inherently multi-dimensional, spanning firms, time, and financial variables, which adds complexity to the imputation task. Conventional imputation methods often fail by flattening the data's multidimensional structure, struggling with heterogeneous missingness patterns, or overfitting in the face of extreme data sparsity. To address these limitations, we introduce an Adaptive, Cluster-based Temporal smoothing tensor completion framework (ACT-Tensor) tailored for severely and heterogeneously missing multi-dimensional financial data panels. ACT-Tensor incorporates two key innovations: a cluster-based completion module that captures cross-sectional heterogeneity by learning group-specific latent structures; and a temporal smoothing module that proactively removes short-lived noise while preserving slow-moving fundamental trends. Extensive experiments show that ACT-Tensor consistently outperforms state-of-the-art benchmarks in terms of imputation accuracy across a range of missing data regimes, including extreme sparsity scenarios. To assess its practical financial utility, we evaluate the imputed data with a latent factor model tailored for tensor-structured financial data. Results show that ACT-Tensor not only achieves accurate return forecasting but also significantly improves risk-adjusted returns of the constructed portfolio. These findings confirm that our method delivers highly accurate and informative imputations, offering substantial value for financial decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01861v2</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junyi Mo, Jiayu Li, Duo Zhang, Elynn Chen</dc:creator>
    </item>
    <item>
      <title>A Structure-Preserving Assessment of VBPBB for Time Series Imputation Under Periodic Trends, Noise, and Missingness Mechanisms</title>
      <link>https://arxiv.org/abs/2508.19535</link>
      <description>arXiv:2508.19535v2 Announce Type: replace 
Abstract: Incomplete time series data present significant challenges to accurate statistical analysis, particularly when the underlying data exhibit periodic structures such as seasonal or monthly trends. Traditional imputation methods often fail to preserve these temporal dynamics, leading to biased estimates and reduced analytical integrity. In this study, we introduce and evaluate a structure-preserving imputation framework that incorporates significant periodic components into the multiple imputation process via the Variable Bandpass Periodic Block Bootstrap (VBPBB). We simulate time series data containing annual and monthly periodicities and introduce varying levels of noise representing low, moderate, and high signal-to-noise scenarios to mimic real world variability. Missing data are introduced under Missing Completely at Random (MCAR) mechanisms across a range of missingness proportions (5% - 70%). VBPBB is used to extract dominant periodic components at multiple frequencies, which are then bootstrapped and included as covariates in the Amelia II multiple imputation model. The performance of this periodicity-enhanced approach is compared against standard imputation methods that do not incorporate temporal structure. Our results demonstrate that the VBPBB-enhanced imputation framework consistently outperforms conventional approaches across all tested conditions, with the greatest performance gains observed in high-noise settings and when multiple periodic components are retained. This study addresses critical limitations in existing imputation techniques by offering a flexible, periodicity-aware solution that preserves temporal structure in incomplete time series. We further explore the methodological implications of incorporating frequency-based components and discuss future directions for advancing robust imputation in temporally correlated data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19535v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asmaa Ahmad, Eric J Rose, Michael Roy, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Rethinking Beta: A Causal Take on CAPM</title>
      <link>https://arxiv.org/abs/2509.05760</link>
      <description>arXiv:2509.05760v2 Announce Type: replace-cross 
Abstract: The CAPM regression is typically interpreted as if the market return contemporaneously \emph{causes} individual returns, motivating beta-neutral portfolios and factor attribution. For realized equity returns, however, this interpretation is inconsistent: a same-period arrow $R_{m,t} \to R_{i,t}$ conflicts with the fact that $R_m$ is itself a value-weighted aggregate of its constituents, unless $R_m$ is lagged or leave-one-out -- the ``aggregator contradiction.'' We formalize CAPM as a structural causal model and analyze the admissible three-node graphs linking an external driver $Z$, the market $R_m$, and an asset $R_i$. The empirically plausible baseline is a \emph{fork}, $Z \to \{R_m, R_i\}$, not $R_m \to R_i$. In this setting, OLS beta reflects not a causal transmission, but an attenuated proxy for how well $R_m$ captures the underlying driver $Z$. Consequently, ``beta-neutral'' portfolios can remain exposed to macro or sectoral shocks, and hedging on $R_m$ can import index-specific noise. Using stylized models and large-cap U.S.\ equity data, we show that contemporaneous betas act like proxies rather than mechanisms; any genuine market-to-stock channel, if at all, appears only at a lag and with modest economic significance. The practical message is clear: CAPM should be read as associational. Risk management and attribution should shift from fixed factor menus to explicitly declared causal paths, with ``alpha'' reserved for what remains invariant once those causal paths are explicitly blocked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05760v2</guid>
      <category>econ.TH</category>
      <category>q-fin.PR</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naftali Cohen</dc:creator>
    </item>
  </channel>
</rss>
