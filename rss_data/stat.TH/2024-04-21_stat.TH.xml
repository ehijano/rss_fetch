<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Apr 2024 04:02:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A studentized permutation test in group sequential designs</title>
      <link>https://arxiv.org/abs/2404.12483</link>
      <description>arXiv:2404.12483v1 Announce Type: new 
Abstract: In group sequential designs, where several data looks are conducted for early stopping, we generally assume the vector of test statistics from the sequential analyses follows (at least approximately or asymptotially) a multivariate normal distribution. However, it is well-known that test statistics for which an asymptotic distribution is derived may suffer from poor small sample approximation. This might become even worse with an increasing number of data looks. The aim of this paper is to improve the small sample behaviour of group sequential designs while maintaining the same asymptotic properties as classical group sequential designs. This improvement is achieved through the application of a modified permutation test. In particular, this paper shows that the permutation distribution approximates the distribution of the test statistics not only under the null hypothesis but also under the alternative hypothesis, resulting in an asymptotically valid permutation test. An extensive simulation study shows that the proposed permutation test better controls the Type I error rate than its competitors in the case of small sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12483v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Long-Hao Xu, Tobias M\"utze, Frank Konietschke, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Estimating weak periodic vector autoregressive time series</title>
      <link>https://arxiv.org/abs/2404.12684</link>
      <description>arXiv:2404.12684v1 Announce Type: new 
Abstract: This article develops the asymptotic distribution of the least squares estimator of the model parameters in periodicvector autoregressive time series models (hereafter PVAR) with uncorrelated but dependent innovations. When theinnovations are dependent, this asymptotic distributions can be quite different from that of PVAR models with in-dependent and identically distributed (iid for short) innovations developed in Ursu and Duchesne (2009). Modifiedversions of the Wald tests are proposed for testing linear restrictions on the parameters. These asymptotic results are illustrated by Monte Carlo experiments. An application to a bivariate real financial data is also proposed</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12684v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11749-023-00859-w</arxiv:DOI>
      <arxiv:journal_reference>Test, 2023, 32 (3), pp.958-997</arxiv:journal_reference>
      <dc:creator>Yacouba Boubacar Ma\"inassara (UFC, LMB), Eugen Ursu (UB, BSE)</dc:creator>
    </item>
    <item>
      <title>Portmanteau test for a class of multivariate asymmetric power GARCH model</title>
      <link>https://arxiv.org/abs/2404.12685</link>
      <description>arXiv:2404.12685v1 Announce Type: new 
Abstract: We establish the asymptotic behaviour of the sum of squared residuals autocovariances and autocorrelations for the class of multi-variate power transformed asymmetric models. We then derive a portmanteau test. We establish the asymptotic distribution of the proposed statistics. These asymptotic results are illustrated by Monte Carlo experiments. An application to a bivariate real financial data is also proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12685v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/jtsa.12646</arxiv:DOI>
      <arxiv:journal_reference>Journal of Time Series Analysis, 2022, 43 (6), pp.964-1002</arxiv:journal_reference>
      <dc:creator>Yacouba Boubacar Ma\"inassara (LMB, UFC), Othman Kadmiri (LMB), Bruno Saussereau (LMB)</dc:creator>
    </item>
    <item>
      <title>Diagnostic Checking in Multivariate ARMA Models With Dependent Errors Using Normalized Residual Autocorrelations</title>
      <link>https://arxiv.org/abs/2404.12692</link>
      <description>arXiv:2404.12692v1 Announce Type: new 
Abstract: In this paper we derive the asymptotic distribution of normalized residual empirical autocovariances and autocorrelations under weak assumptions on the noise. We propose  new portmanteau statistics for vector autoregressive moving-average (VARMA) models with uncorrelated but non-independent innovations by using a self-normalization approach. We establish the asymptotic distribution of the proposed statistics. This asymptotic distribution is quite different from the usual chi-squared approximation  used under the independent and identically distributed assumption on the noise,  or the weighted sum of independent chi-squared random variables obtained under nonindependent innovations.  A set of Monte Carlo experiments and an application to the daily returns of the CAC40 is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12692v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2017.1380030</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association, 2018, 113 (524), pp.1813-1827</arxiv:journal_reference>
      <dc:creator>Yacouba Boubacar Ma\"inassara (LMB, UFC), Bruno Saussereau (LMB)</dc:creator>
    </item>
    <item>
      <title>On the probability of linear separability through intrinsic volumes</title>
      <link>https://arxiv.org/abs/2404.12889</link>
      <description>arXiv:2404.12889v1 Announce Type: new 
Abstract: A dataset with two labels is linearly separable if it can be split into its two classes with a hyperplane. This inflicts a curse on some statistical tools (such as logistic regression) but forms a blessing for others (e.g. support vector machines). Recently, the following question has regained interest: What is the probability that the data are linearly separable?
  We provide a formula for the probability of linear separability for Gaussian features and labels depending only on one marginal of the features (as in generalized linear models). In this setting, we derive an upper bound that complements the recent result by Hayakawa, Lyons, and Oberhauser [2023], and a sharp upper bound for sign-flip noise.
  To prove our results, we exploit that this probability can be expressed as a sum of the intrinsic volumes of a polyhedral cone of the form $\text{span}\{v\}\oplus[0,\infty)^n$, as shown in Cand\`es and Sur [2020]. After providing the inequality description for this cone, and an algorithm to project onto it, we calculate its intrinsic volumes. In doing so, we encounter Youden's demon problem, for which we provide a formula following Kabluchko and Zaporozhets [2020]. The key insight of this work is the following: The number of correctly labeled observations in the data affects the structure of this polyhedral cone, allowing the translation of insights from geometry into statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12889v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Kuchelmeister</dc:creator>
    </item>
    <item>
      <title>Symmetry: a General Structure in Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2404.12943</link>
      <description>arXiv:2404.12943v1 Announce Type: new 
Abstract: In this paper we present the framework of symmetry in nonparametric regression. This generalises the framework of covariate sparsity, where the regression function depends only on at most $s &lt; d$ of the covariates, which is a special case of translation symmetry with linear orbits. In general this extends to other types of functions that capture lower dimensional behavior even when these structures are non-linear. We show both that known symmetries of regression functions can be exploited to give similarly faster rates, and that unknown symmetries with Lipschitz actions can be estimated sufficiently quickly to obtain the same rates. This is done by explicit constructions of partial symmetrisation operators that are then applied to usual estimators, and with a two step M-estimator of the maximal symmetry of the regression function. We also demonstrate the finite sample performance of these estimators on synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12943v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis G. Christie, John A. D. Aston</dc:creator>
    </item>
    <item>
      <title>The phase diagram of kernel interpolation in large dimensions</title>
      <link>https://arxiv.org/abs/2404.12597</link>
      <description>arXiv:2404.12597v1 Announce Type: cross 
Abstract: The generalization ability of kernel interpolation in large dimensions (i.e., $n \asymp d^{\gamma}$ for some $\gamma&gt;0$) might be one of the most interesting problems in the recent renaissance of kernel regression, since it may help us understand the 'benign overfitting phenomenon' reported in the neural networks literature. Focusing on the inner product kernel on the sphere, we fully characterized the exact order of both the variance and bias of large-dimensional kernel interpolation under various source conditions $s\geq 0$. Consequently, we obtained the $(s,\gamma)$-phase diagram of large-dimensional kernel interpolation, i.e., we determined the regions in $(s,\gamma)$-plane where the kernel interpolation is minimax optimal, sub-optimal and inconsistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12597v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haobo Zhang, Weihao Lu, Qian Lin</dc:creator>
    </item>
    <item>
      <title>Gaussian dependence structure pairwise goodness-of-fit testing based on conditional covariance and the 20/60/20 rule</title>
      <link>https://arxiv.org/abs/2404.12696</link>
      <description>arXiv:2404.12696v1 Announce Type: cross 
Abstract: We present a novel data-oriented statistical framework that assesses the presumed Gaussian dependence structure in a pairwise setting. This refers to both multivariate normality and normal copula goodness-of-fit testing. The proposed test clusters the data according to the 20/60/20 rule and confronts conditional covariance (or correlation) estimates on the obtained subsets. The corresponding test statistic has a natural practical interpretation, desirable statistical properties, and asymptotic pivotal distribution under the multivariate normality assumption. We illustrate the usefulness of the introduced framework using extensive power simulation studies and show that our approach outperforms popular benchmark alternatives. Also, we apply the proposed methodology to commodities market data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12696v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakub Wo\'zny, Piotr Jaworski, Damian Jelitoa, Marcin Pitera, Agnieszka Wy{\l}oma\'nska</dc:creator>
    </item>
    <item>
      <title>Low solution rank of the matrix LASSO under RIP with consequences for rank-constrained algorithms</title>
      <link>https://arxiv.org/abs/2404.12828</link>
      <description>arXiv:2404.12828v1 Announce Type: cross 
Abstract: We show that solutions to the popular convex matrix LASSO problem (nuclear-norm--penalized linear least-squares) have low rank under similar assumptions as required by classical low-rank matrix sensing error bounds. Although the purpose of the nuclear norm penalty is to promote low solution rank, a proof has not yet (to our knowledge) been provided outside very specific circumstances. Furthermore, we show that this result has significant theoretical consequences for nonconvex rank-constrained optimization approaches. Specifically, we show that if (a) the ground truth matrix has low rank, (b) the (linear) measurement operator has the matrix restricted isometry property (RIP), and (c) the measurement error is small enough relative to the nuclear norm penalty, then the (unique) LASSO solution has rank (approximately) bounded by that of the ground truth. From this, we show (a) that a low-rank--projected proximal gradient descent algorithm will converge linearly to the LASSO solution from any initialization, and (b) that the nonconvex landscape of the low-rank Burer-Monteiro--factored problem formulation is benign in the sense that all second-order critical points are globally optimal and yield the LASSO solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12828v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew D. McRae</dc:creator>
    </item>
    <item>
      <title>A Guide to Feature Importance Methods for Scientific Inference</title>
      <link>https://arxiv.org/abs/2404.12862</link>
      <description>arXiv:2404.12862v1 Announce Type: cross 
Abstract: While machine learning (ML) models are increasingly used due to their high predictive power, their use in understanding the data-generating process (DGP) is limited. Understanding the DGP requires insights into feature-target associations, which many ML models cannot directly provide, due to their opaque internal mechanisms. Feature importance (FI) methods provide useful insights into the DGP under certain conditions. Since the results of different FI methods have different interpretations, selecting the correct FI method for a concrete use case is crucial and still requires expert knowledge. This paper serves as a comprehensive guide to help understand the different interpretations of FI methods. Through an extensive review of FI methods and providing new proofs regarding their interpretation, we facilitate a thorough understanding of these methods and formulate concrete recommendations for scientific inference. We conclude by discussing options for FI uncertainty estimation and point to directions for future research aiming at full statistical inference from black-box ML models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12862v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fiona Katharina Ewald, Ludwig Bothmann, Marvin N. Wright, Bernd Bischl, Giuseppe Casalicchio, Gunnar K\"onig</dc:creator>
    </item>
    <item>
      <title>Optimal single threshold stopping rules and sharp prophet inequalities</title>
      <link>https://arxiv.org/abs/2404.12949</link>
      <description>arXiv:2404.12949v1 Announce Type: cross 
Abstract: This paper considers a finite horizon optimal stopping problem for a sequence of independent and identically distributed random variables. The objective is to design stopping rules that attempt to select the random variable with the highest value in the sequence. The performance of any stopping rule may be benchmarked relative to the selection of a "prophet" that has perfect foreknowledge of the largest value. Such comparisons are typically stated in the form of "prophet inequalities." In this paper we characterize sharp prophet inequalities for single threshold stopping rules as solutions to infinite two person zero sum games on the unit square with special payoff kernels. The proposed game theoretic characterization allows one to derive sharp non-asymptotic prophet inequalities for different classes of distributions. This, in turn, gives rise to a simple and computationally tractable algorithmic paradigm for deriving optimal single threshold stopping rules. Our results also indicate that several classical observations in the literature are either incorrect or incomplete in treating this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12949v1</guid>
      <category>math.PR</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Goldenshluger, Yaakov Malinovsky, Assaf Zeevi</dc:creator>
    </item>
    <item>
      <title>Differentially private estimation in a class of directed network models</title>
      <link>https://arxiv.org/abs/2201.09648</link>
      <description>arXiv:2201.09648v3 Announce Type: replace 
Abstract: Although the theoretical properties in the $p_0$ model based on a differentially private bi-degree sequence have been derived, it is still lack of a unified theory for a general class of directed network models with the $p_{0}$ model as a special case. We use the popular Laplace data releasing method to output the bi-degree sequence of directed networks, which satisfies the private standard--differential privacy. The method of moment is used to estimate unknown parameters. We prove that the differentially private estimator is uniformly consistent and asymptotically normal under some conditions. Our results are illustrated by the Probit model. We carry out simulation studies to illustrate theoretical results and provide a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.09648v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Pan, Jianwei Hu, Peiyan Li</dc:creator>
    </item>
    <item>
      <title>Direct Bayesian Regression for Distribution-valued Covariates</title>
      <link>https://arxiv.org/abs/2303.06434</link>
      <description>arXiv:2303.06434v2 Announce Type: replace-cross 
Abstract: In this manuscript, we study the problem of scalar-on-distribution regression; that is, instances where subject-specific distributions or densities, or in practice, repeated measures from those distributions, are the covariates related to a scalar outcome via a regression model. We propose a direct regression for such distribution-valued covariates that circumvents estimating subject-specific densities and directly uses the observed repeated measures as covariates. The model is invariant to any transformation or ordering of the repeated measures. Endowing the regression function with a Gaussian Process prior, we obtain closed form or conjugate Bayesian inference. Our method subsumes the standard Bayesian non-parametric regression using Gaussian Processes as a special case. Theoretically, we show that the method can achieve an optimal estimation error bound. To our knowledge, this is the first theoretical study on Bayesian regression using distribution-valued covariates. Through simulation studies and analysis of activity count dataset, we demonstrate that our method performs better than approaches that require an intermediate density estimation step.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06434v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bohao Tang, Sandipan Pramanik, Yi Zhao, Brian Caffo, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise</title>
      <link>https://arxiv.org/abs/2310.18784</link>
      <description>arXiv:2310.18784v5 Announce Type: replace-cross 
Abstract: We study high-probability convergence guarantees of learning on streaming data in the presence of heavy-tailed noise. In the proposed scenario, the model is updated in an online fashion, as new information is observed, without storing any additional data. To combat the heavy-tailed noise, we consider a general framework of nonlinear stochastic gradient descent (SGD), providing several strong results. First, for non-convex costs and component-wise nonlinearities, we establish a convergence rate arbitrarily close to $\mathcal{O}\left(t^{-\frac{1}{4}}\right)$, whose exponent is independent of noise and problem parameters. Second, for strongly convex costs and a broader class of nonlinearities, we establish convergence of the last iterate to the optimum, with a rate $\mathcal{O}\left(t^{-\zeta} \right)$, where $\zeta \in (0,1)$ depends on problem parameters, noise and nonlinearity. As we show analytically and numerically, $\zeta$ can be used to inform the preferred choice of nonlinearity for given problem settings. Compared to state-of-the-art, who only consider clipping, require bounded noise moments of order $\eta \in (1,2]$, and establish convergence rates whose exponents go to zero as $\eta \rightarrow 1$, we provide high-probability guarantees for a much broader class of nonlinearities and symmetric density noise, with convergence rates whose exponents are bounded away from zero, even when the noise has finite first moment only. Moreover, in the case of strongly convex functions, we demonstrate analytically and numerically that clipping is not always the optimal nonlinearity, further underlining the value of our general framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18784v5</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksandar Armacki, Pranay Sharma, Gauri Joshi, Dragana Bajovic, Dusan Jakovetic, Soummya Kar</dc:creator>
    </item>
    <item>
      <title>Maximal Inequalities for Empirical Processes under General Mixing Conditions with an Application to Strong Approximations</title>
      <link>https://arxiv.org/abs/2402.11394</link>
      <description>arXiv:2402.11394v2 Announce Type: replace-cross 
Abstract: This paper provides a bound for the supremum of sample averages over a class of functions for a general class of mixing stochastic processes with arbitrary mixing rates. Regardless of the speed of mixing, the bound is comprised of a concentration rate and a novel measure of complexity. The speed of mixing, however, affects the former quantity implying a phase transition. Fast mixing leads to the standard root-n concentration rate, while slow mixing leads to a slower concentration rate, its speed depends on the mixing structure. Our findings are applied to derive strong approximation results for a general class of mixing processes with arbitrary mixing rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11394v2</guid>
      <category>math.PR</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Demian Pouzo</dc:creator>
    </item>
  </channel>
</rss>
