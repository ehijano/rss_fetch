<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A New Two-Sample Test for Covariance Matrices in High Dimensions: U-Statistics Meet Leading Eigenvalues</title>
      <link>https://arxiv.org/abs/2506.06550</link>
      <description>arXiv:2506.06550v1 Announce Type: new 
Abstract: We propose a two-sample test for covariance matrices in the high-dimensional regime, where the dimension diverges proportionally to the sample size. Our hybrid test combines a Frobenius-norm-based statistic as considered in Li and Chen (2012) with the leading eigenvalue approach proposed in Zhang et al. (2022), making it sensitive to both dense and sparse alternatives. The two statistics are combined via Fisher's method, leveraging our key theoretical result: a joint central limit theorem showing the asymptotic independence of the leading eigenvalues of the sample covariance matrix and an estimator of the Frobenius norm of the difference of the two population covariance matrices, under suitable signal conditions. The level of the test can be controlled asymptotically, and we show consistency against certain types of both sparse and dense alternatives. A comprehensive numerical study confirms the favorable performance of our method compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06550v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Lam, Nina D\"ornemann, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Rates for Regression on Manifolds and Distributions</title>
      <link>https://arxiv.org/abs/2506.07504</link>
      <description>arXiv:2506.07504v1 Announce Type: new 
Abstract: Distribution regression seeks to estimate the conditional distribution of a multivariate response given a continuous covariate. This approach offers a more complete characterization of dependence than traditional regression methods. Classical nonparametric techniques often assume that the conditional distribution has a well-defined density, an assumption that fails in many real-world settings. These include cases where data contain discrete elements or lie on complex low-dimensional structures within high-dimensional spaces. In this work, we establish minimax convergence rates for distribution regression under nonparametric assumptions, focusing on scenarios where both covariates and responses lie on low-dimensional manifolds. We derive lower bounds that capture the inherent difficulty of the problem and propose a new hybrid estimator that combines adversarial learning with simultaneous least squares to attain matching upper bounds. Our results reveal how the smoothness of the conditional distribution and the geometry of the underlying manifolds together determine the estimation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07504v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong Tang, Yun Yang</dc:creator>
    </item>
    <item>
      <title>Cascaded Multiwire-PLC/Multiple-VLC System: Characterization and Performance</title>
      <link>https://arxiv.org/abs/2506.06357</link>
      <description>arXiv:2506.06357v1 Announce Type: cross 
Abstract: This paper proposes a cascaded multiwire-power line communication (PLC)/multiple-visible light communication (VLC) system. This hybrid architecture offers low installation cost, enhanced performance, practical feasibility, and a wide range of applications. Novel analytical expressions are derived for key statistics and outage probability, bit error probability, and ergodic channel capacity metrics. Furthermore, the analytical results are validated through Monte Carlo simulations, with several performance curves presented under various channel and PLC/VLC system parameters. All expressions derived in this work are original and have not been previously published. Our proposed system proves feasible for smart environments, green communication systems, internet of things networks, industrial environments, and next-generation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06357v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugerles S. Silva, Higo T. P. Silva, Paulo V. B. Tom\'e, Felipe A. P. Figueiredo, Edson P. da Silva, Rausley A. A. de Souza</dc:creator>
    </item>
    <item>
      <title>Neural Operators for Forward and Inverse Potential-Density Mappings in Classical Density Functional Theory</title>
      <link>https://arxiv.org/abs/2506.06623</link>
      <description>arXiv:2506.06623v1 Announce Type: cross 
Abstract: Neural operators are capable of capturing nonlinear mappings between infinite-dimensional functional spaces, offering a data-driven approach to modeling complex functional relationships in classical density functional theory (cDFT). In this work, we evaluate the performance of several neural operator architectures in learning the functional relationships between the one-body density profile $\rho(x)$, the one-body direct correlation function $c_1(x)$, and the external potential $V_{ext}(x)$ of inhomogeneous one-dimensional (1D) hard-rod fluids, using training data generated from analytical solutions of the underlying statistical-mechanical model. We compared their performance in terms of the Mean Squared Error (MSE) loss in establishing the functional relationships as well as in predicting the excess free energy across two test sets: (1) a group test set generated via random cross-validation (CV) to assess interpolation capability, and (2) a newly constructed dataset for leave-one-group CV to evaluate extrapolation performance. Our results show that FNO achieves the most accurate predictions of the excess free energy, with the squared ReLU activation function outperforming other activation choices. Among the DeepONet variants, the Residual Multiscale Convolutional Neural Network (RMSCNN) combined with a trainable Gaussian derivative kernel (GK-RMSCNN-DeepONet) demonstrates the best performance. Additionally, we applied the trained models to solve for the density profiles at various external potentials and compared the results with those obtained from the direct mapping $V_{ext} \mapsto \rho$ with neural operators, as well as with Gaussian Process Regression (GPR) combined with Active Learning by Error Control (ALEC), which has shown strong performance in previous studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06623v1</guid>
      <category>physics.chem-ph</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Runtong Pan, Xinyi Fang, Kamyar Azizzadenesheli, Miguel Liu-Schiaffini, Mengyang Gu, Jianzhong Wu</dc:creator>
    </item>
    <item>
      <title>Statistical Limits for Finite-Rank Tensor Estimation</title>
      <link>https://arxiv.org/abs/2506.06749</link>
      <description>arXiv:2506.06749v1 Announce Type: cross 
Abstract: This paper provides a unified framework for analyzing tensor estimation problems that allow for nonlinear observations, heteroskedastic noise, and covariate information. We study a general class of high-dimensional models where each observation depends on the interactions among a finite number of unknown parameters. Our main results provide asymptotically exact formulas for the mutual information (equivalently, the free energy) as well as the minimum mean-squared error in the Bayes-optimal setting. We then apply this framework to derive sharp characterizations of statistical thresholds for two novel scenarios: (1) tensor estimation in heteroskedastic noise that is independent but not identically distributed, and (2) higher-order assignment problems, where the goal is to recover an unknown permutation from tensor-valued observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06749v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Rossetti, Galen Reeves</dc:creator>
    </item>
    <item>
      <title>Uncovering the topology of an infinite-server queueing network from population data</title>
      <link>https://arxiv.org/abs/2506.07057</link>
      <description>arXiv:2506.07057v1 Announce Type: cross 
Abstract: This paper studies statistical inference in a network of infinite-server queues, with the aim of estimating the underlying parameters (routing matrix, arrival rates, parameters pertaining to the service times) using observations of the network population vector at Poisson time points. We propose a method-of-moments estimator and establish its consistency. The method relies on deriving the covariance structure of different nodes at different sampling epochs. Numerical experiments demonstrate that the method yields accurate estimates, even in settings with a large number of parameters. Two model variants are considered: one that assumes a known parametric form for the service-time distributions, and a model-free version that does not require such assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07057v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hritika Gupta, Michel Mandjes, Liron Ravner, Jiesen Wang</dc:creator>
    </item>
    <item>
      <title>Strongly Consistent Community Detection in Popularity Adjusted Block Models</title>
      <link>https://arxiv.org/abs/2506.07224</link>
      <description>arXiv:2506.07224v1 Announce Type: cross 
Abstract: The Popularity Adjusted Block Model (PABM) provides a flexible framework for community detection in network data by allowing heterogeneous node popularity across communities. However, this flexibility increases model complexity and raises key unresolved challenges, particularly in effectively adapting spectral clustering techniques and efficiently achieving strong consistency in label recovery. To address these challenges, we first propose the Thresholded Cosine Spectral Clustering (TCSC) algorithm and establish its weak consistency under the PABM. We then introduce the one-step Refined TCSC algorithm and prove that it achieves strong consistency under the PABM, correctly recovering all community labels with high probability. We further show that the two-step Refined TCSC accelerates clustering error convergence, especially with small sample sizes. Additionally, we propose a data-driven approach for selecting the number of communities, which outperforms existing methods under the PABM. The effectiveness and robustness of our methods are validated through extensive simulations and real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07224v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quan Yuan, Binghui Liu, Danning Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>One-dimensional quantile-stratified sampling and its application in statistical simulations</title>
      <link>https://arxiv.org/abs/2506.07437</link>
      <description>arXiv:2506.07437v1 Announce Type: cross 
Abstract: In this paper we examine quantile-stratified samples from a known univariate probability distribution, with stratification occurring over a partition of the quantile regions in the distribution. We examine some general properties of this sampling method and we contrast it with standard IID sampling to highlight its similarities and differences. We examine the applications of this sampling method to various statistical simulations including importance sampling. We conduct simulation analysis to compare the performance of standard importance sampling against the quantile-stratified importance sampling to see how they each perform on a range of functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07437v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben O'Neill</dc:creator>
    </item>
    <item>
      <title>Individual Treatment Effect: Prediction Intervals and Sharp Bounds</title>
      <link>https://arxiv.org/abs/2506.07469</link>
      <description>arXiv:2506.07469v1 Announce Type: cross 
Abstract: Individual treatment effect (ITE) is often regarded as the ideal target of inference in causal analyses and has been the focus of several recent studies. In this paper, we describe the intrinsic limits regarding what can be learned concerning ITEs given data from large randomized experiments. We consider when a valid prediction interval for the ITE is informative and when it can be bounded away from zero. The joint distribution over potential outcomes is only partially identified from a randomized trial. Consequently, to be valid, an ITE prediction interval must be valid for all joint distribution consistent with the observed data and hence will in general be wider than that resulting from knowledge of this joint distribution. We characterize prediction intervals in the binary treatment and outcome setting, and extend these insights to models with continuous and ordinal outcomes. We derive sharp bounds on the probability mass function (pmf) of the individual treatment effect (ITE). Finally, we contrast prediction intervals for the ITE and confidence intervals for the average treatment effect (ATE). This also leads to the consideration of Fisher versus Neyman null hypotheses. While confidence intervals for the ATE shrink with increasing sample size due to its status as a population parameter, prediction intervals for the ITE generally do not vanish, leading to scenarios where one may reject the Neyman null yet still find evidence consistent with the Fisher null, highlighting the challenges of individualized decision-making under partial identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07469v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhehao Zhang, Thomas S. Richardson</dc:creator>
    </item>
    <item>
      <title>Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds</title>
      <link>https://arxiv.org/abs/2506.07614</link>
      <description>arXiv:2506.07614v1 Announce Type: cross 
Abstract: We study the problem of sampling from strongly log-concave distributions over $\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the randomized midpoint method) for overdamped/underdamped Langevin dynamics. We prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic speedup in dependence on the target accuracy ($\epsilon$) over the Euler-Maruyama discretization, surpassing existing bounds for randomized midpoint methods. Notably, in the case of underdamped Langevin dynamics, we demonstrate the complexity of $W_2$ convergence is much smaller than the complexity lower bounds for convergence in $L^2$ strong error established in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07614v1</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishikesh Srinivasan, Dheeraj Nagaraj</dc:creator>
    </item>
    <item>
      <title>Stability of Mean-Field Variational Inference</title>
      <link>https://arxiv.org/abs/2506.07856</link>
      <description>arXiv:2506.07856v1 Announce Type: cross 
Abstract: Mean-field variational inference (MFVI) is a widely used method for approximating high-dimensional probability distributions by product measures. This paper studies the stability properties of the mean-field approximation when the target distribution varies within the class of strongly log-concave measures. We establish dimension-free Lipschitz continuity of the MFVI optimizer with respect to the target distribution, measured in the 2-Wasserstein distance, with Lipschitz constant inversely proportional to the log-concavity parameter. Under additional regularity conditions, we further show that the MFVI optimizer depends differentiably on the target potential and characterize the derivative by a partial differential equation. Methodologically, we follow a novel approach to MFVI via linearized optimal transport: the non-convex MFVI problem is lifted to a convex optimization over transport maps with a fixed base measure, enabling the use of calculus of variations and functional analysis. We discuss several applications of our results to robust Bayesian inference and empirical Bayes, including a quantitative Bernstein--von Mises theorem for MFVI, as well as to distributed stochastic control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07856v1</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shunan Sheng, Bohan Wu, Alberto Gonz\'alez-Sanz, Marcel Nutz</dc:creator>
    </item>
    <item>
      <title>A structural nested rate model for estimating the effects of time-varying exposure on recurrent event outcomes in the presence of death</title>
      <link>https://arxiv.org/abs/2506.07910</link>
      <description>arXiv:2506.07910v1 Announce Type: cross 
Abstract: Assessing the causal effect of time-varying exposures on recurrent event processes is challenging in the presence of a terminating event. Our objective is to estimate both the short-term and delayed marginal causal effects of exposures on recurrent events while addressing the bias of a potentially correlated terminal event. Existing estimators based on marginal structural models and proportional rate models are unsuitable for estimating delayed marginal causal effects for many reasons, and furthermore, they do not account for competing risks associated with a terminating event. To address these limitations, we propose a class of semiparametric structural nested recurrent event models and two estimators of short-term and delayed marginal causal effects of exposures. We establish the asymptotic linearity of these two estimators under regularity conditions through the novel use of modern empirical process and semiparametric efficiency theory. We examine the performance of these estimators via simulation and provide an R package sncure to apply our methods in real data scenarios. Finally, we present the utility of our methods in the context of a large epidemiological study of 299,661 Medicare beneficiaries, where we estimate the effects of fine particulate matter air pollution on recurrent hospitalizations for cardiovascular disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07910v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Mork, Robert L. Strawderman, Michelle Audirac, Francesca Dominici, Ashkan Ertefaie</dc:creator>
    </item>
    <item>
      <title>The Fundamental Limits of Structure-Agnostic Functional Estimation</title>
      <link>https://arxiv.org/abs/2305.04116</link>
      <description>arXiv:2305.04116v2 Announce Type: replace 
Abstract: Many recent developments in causal inference, and functional estimation problems more generally, have been motivated by the fact that classical one-step (first-order) debiasing methods, or their more recent sample-split double machine-learning avatars, can outperform plugin estimators under surprisingly weak conditions. These first-order corrections improve on plugin estimators in a black-box fashion, and consequently are often used in conjunction with powerful off-the-shelf estimation methods. These first-order methods are however provably suboptimal in a minimax sense for functional estimation when the nuisance functions live in Holder-type function spaces. This suboptimality of first-order debiasing has motivated the development of "higher-order" debiasing methods. The resulting estimators are, in some cases, provably optimal over Holder-type spaces, but both the estimators which are minimax-optimal and their analyses are crucially tied to properties of the underlying function space.
  In this paper we investigate the fundamental limits of structure-agnostic functional estimation, where relatively weak conditions are placed on the underlying nuisance functions. We show that there is a strong sense in which existing first-order methods are optimal. We achieve this goal by providing a formalization of the problem of functional estimation with black-box nuisance function estimates, and deriving minimax lower bounds for this problem. Our results highlight some clear tradeoffs in functional estimation -- if we wish to remain agnostic to the underlying nuisance function spaces, impose only high-level rate conditions, and maintain compatibility with black-box nuisance estimators then first-order methods are optimal. When we have an understanding of the structure of the underlying nuisance functions then carefully constructed higher-order estimators can outperform first-order estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04116v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sivaraman Balakrishnan, Edward H. Kennedy, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Nonlinear Filtering and Spatial Asymptotic Consistency for SPDEs Observed via Spatio-Temporal Point Processes</title>
      <link>https://arxiv.org/abs/2408.15920</link>
      <description>arXiv:2408.15920v2 Announce Type: replace 
Abstract: In this paper, we develop the mathematical framework for filtering problems arising from biophysical applications where data is collected from confocal laser scanning microscopy recordings of the space-time evolution of intracellular wave dynamics of biophysical quantities. In these applications, signals are described by stochastic partial differential equations (SPDEs) and observations can be modelled as functionals of marked point processes whose intensities depend on the underlying signal. We derive both the unnormalized and normalized filtering equations for these systems, demonstrate the asymptotic consistency and approximations of finite dimensional observation schemes respectively partial observations. Our theoretical results are validated through extensive simulations using synthetic and real data. These findings contribute to a deeper understanding of filtering with point process observations and provide a robust framework for future research in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15920v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jan Szalankiewicz, Cristina Martinez-Torres, Wilhelm Stannat</dc:creator>
    </item>
    <item>
      <title>Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent</title>
      <link>https://arxiv.org/abs/2409.08469</link>
      <description>arXiv:2409.08469v3 Announce Type: replace 
Abstract: We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy ($\mathsf{KSD}$) and Wasserstein-2 metrics. Our key insight is that the time derivative of the relative entropy between the joint density of $N$ particle locations and the $N$-fold product target measure, starting from a regular initial distribution, splits into a dominant `negative part' proportional to $N$ times the expected $\mathsf{KSD}^2$ and a smaller `positive part'. This observation leads to $\mathsf{KSD}$ rates of order $1/\sqrt{N}$, in both continuous and discrete time, providing a near optimal (in the sense of matching the corresponding i.i.d. rates) double exponential improvement over the recent result by Shi and Mackey (2024). Under mild assumptions on the kernel and potential, these bounds also grow polynomially in the dimension $d$. By adding a bilinear component to the kernel, the above approach is used to further obtain Wasserstein-2 convergence in continuous time. For the case of `bilinear + Mat\'ern' kernels, we derive Wasserstein-2 rates that exhibit a curse-of-dimensionality similar to the i.i.d. setting. We also obtain marginal convergence and long-time propagation of chaos results for the time-averaged particle laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08469v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayan Banerjee, Krishnakumar Balasubramanian, Promit Ghosal</dc:creator>
    </item>
    <item>
      <title>On the number of modes of Gaussian kernel density estimators</title>
      <link>https://arxiv.org/abs/2412.09080</link>
      <description>arXiv:2412.09080v2 Announce Type: replace 
Abstract: We consider the Gaussian kernel density estimator with bandwidth $\beta^{-\frac12}$ of $n$ iid Gaussian samples. Using the Kac-Rice formula and an Edgeworth expansion, we prove that the expected number of modes on the real line scales as $\Theta(\sqrt{\beta\log\beta})$ as $\beta,n\to\infty$ provided $n^c\lesssim \beta\lesssim n^{2-c}$ for some constant $c&gt;0$. An impetus behind this investigation is to determine the number of clusters to which Transformers are drawn in a metastable state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09080v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Borjan Geshkovski, Philippe Rigollet, Yihang Sun</dc:creator>
    </item>
    <item>
      <title>Characterization of Efficient Influence Function for Off-Policy Evaluation Under Optimal Policies</title>
      <link>https://arxiv.org/abs/2505.13809</link>
      <description>arXiv:2505.13809v3 Announce Type: replace 
Abstract: Off-policy evaluation (OPE) provides a powerful framework for estimating the value of a counterfactual policy using observational data, without the need for additional experimentation. Despite recent progress in robust and efficient OPE across various settings, rigorous efficiency analysis of OPE under an estimated optimal policy remains limited. In this paper, we establish a concise characterization of the efficient influence function (EIF) for the value function under optimal policy within canonical Markov decision process models. Specifically, we provide the sufficient conditions for the existence of the EIF and characterize its expression. We also give the conditions under which the EIF does not exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13809v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyu Wei</dc:creator>
    </item>
    <item>
      <title>On Rank Correlation Coefficients</title>
      <link>https://arxiv.org/abs/2506.06056</link>
      <description>arXiv:2506.06056v2 Announce Type: replace 
Abstract: In the present paper, we propose a new rank correlation coefficient $r_n$, which is a sample analogue of the theoretical correlation coefficient $r$, which, in turn, was proposed in the recent work of Stepanov (2025b). We discuss the properties of $r_n$ and compare $r_n$ with known rank Spearman $\rho_{S,n}$, Kendall $\tau_n$ and sample Pearson $\rho_n$ correlation coefficients. Simulation experiments show that when the relationship between $X$ and $Y$ is not close to linear, $r_n$ performs better than other correlation coefficients. We also find analytically the values of $Var(\tau_n)$ and $Var(r_n)$. This allows to estimate theoretically the asymptotic performance of $\tau_n$ and $r_n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06056v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexei Stepanov</dc:creator>
    </item>
    <item>
      <title>The Inverse Problem for Single Trajectories of Rough Differential Equations</title>
      <link>https://arxiv.org/abs/2201.10300</link>
      <description>arXiv:2201.10300v2 Announce Type: replace-cross 
Abstract: Motivated by the need to develop a general framework for performing statistical inference for discretely observed random rough differential equations, our aim is to construct a geometric $p$-rough path ${\bf X}$ whose response $Y$, when driving a rough differential equation, matches the observed trajectory $y$. We call this the \textit{continuous inverse problem} and start by rigorously defining its solution. We then develop a framework where the solution can be constructed as a limit of solutions to appropriately designed \textit{discrete inverse problems}, so that convergence holds in $p$-variation. Our approach is based on calibrating the bounded variation paths whose limit defines the rough path `lift' of path $X$ to rough path ${\bf X}$ to the observed trajectory $y$. Moreover, we develop a general numerical algorithm for constructing the solution to the discrete inverse problem. The core idea of the algorithm is to use the signature representation of the path, iterating between the response and the control, each time correcting according to the required properties.
  We apply our framework to the case where the geometric $p$-rough path ${\bf X}$ is defined as the limit of piecewise linear paths in the $p$-variation topology. We express the discrete inverse problem for a fixed observation rate as a solution to a system of equations driven by piecewise linear paths and prove convergence to the solution of the continuous inverse problem for observation time $\delta\to 0$. Finally, we show that, in this context, the numerical algorithm for solving the discrete inverse problem simplifies to an iterative simultaneous update of the local gradients and we prove that it converges in $p$-variation uniformly with respect to $\delta$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.10300v2</guid>
      <category>math.CA</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Morrish, Anastasia Papavasiliou, Theodore Papamarkou, Yang Zhao</dc:creator>
    </item>
    <item>
      <title>Cox reduction and confidence sets of models: a theoretical elucidation</title>
      <link>https://arxiv.org/abs/2302.12627</link>
      <description>arXiv:2302.12627v3 Announce Type: replace-cross 
Abstract: For sparse high-dimensional regression problems, Cox and Battey [1, 9] emphasised the need for confidence sets of models: an enumeration of those small sets of variables that fit the data equivalently well in a suitable statistical sense. This is to be contrasted with the single model returned by penalised regression procedures, effective for prediction but potentially misleading for subject-matter understanding. The proposed construction of such sets relied on preliminary reduction of the full set of variables, and while various possibilities could be considered for this, [9] proposed a succession of regression fits based on incomplete block designs. The purpose of the present paper is to provide insight on both aspects of that work. For an unspecified reduction strategy, we begin by characterising models that are likely to be retained in the model confidence set, emphasising geometric aspects. We then evaluate possible reduction schemes based on penalised regression or marginal screening, before theoretically elucidating the reduction of [9]. We identify features of the covariate matrix that may reduce its efficacy, and indicate improvements to the original proposal. An advantage of the approach is its ability to reveal its own stability or fragility for the data at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12627v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-STS934</arxiv:DOI>
      <arxiv:journal_reference>Statistical Science 40(2): 313-328 (2025)</arxiv:journal_reference>
      <dc:creator>R. M. Lewis, H. S. Battey</dc:creator>
    </item>
    <item>
      <title>Debiased regression adjustment in completely randomized experiments with moderately high-dimensional covariates</title>
      <link>https://arxiv.org/abs/2309.02073</link>
      <description>arXiv:2309.02073v3 Announce Type: replace-cross 
Abstract: Completely randomized experiment is the gold standard for causal inference. When the covariate information for each experimental candidate is available, one typical way is to include them in covariate adjustments for more accurate treatment effect estimation. In this paper, we investigate this problem under the randomization-based framework, i.e., that the covariates and potential outcomes of all experimental candidates are assumed as deterministic quantities and the randomness comes solely from the treatment assignment mechanism. Under this framework, to achieve asymptotically valid inference, existing estimators usually require either (i) that the dimension of covariates $p$ is much smaller than the sample size $n$; or (ii) certain sparsity constraints on the linear representations of potential outcomes constructed via possibly high-dimensional covariates. In this paper, we consider the moderately high-dimensional regime where $p$ is allowed to be in the same order of magnitude as $n$. We develop a novel debiased estimator with a corresponding inference procedure and establish its asymptotic normality under mild assumptions. Our estimator is model-free and does not require any sparsity constraint on potential outcome's linear representations. We also discuss its asymptotic efficiency improvements over the unadjusted treatment effect estimator under different dimensionality constraints. Numerical analysis confirms that compared to other regression adjustment based treatment effect estimators, our debiased estimator performs well in moderately high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02073v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Lu, Fan Yang, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2402.14264</link>
      <description>arXiv:2402.14264v4 Announce Type: replace-cross 
Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, the statistical optimality of these methods has still remained an open area of investigation, especially in regimes where these methods do not achieve parametric rates. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), as well as weighted variants of the former, which arise in policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14264v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Moment varieties of the inverse Gaussian and gamma distributions are nondefective</title>
      <link>https://arxiv.org/abs/2409.18421</link>
      <description>arXiv:2409.18421v3 Announce Type: replace-cross 
Abstract: We show that the parameters of a $k$-mixture of inverse Gaussian or gamma distributions are algebraically identifiable from the first $3k-1$ moments, and rationally identifiable from the first $3k+2$ moments. Our proofs are based on Terracini's classification of defective surfaces, careful analysis of the intersection theory of moment varieties, and a recent result on sufficient conditions for rational identifiability of secant varieties by Massarenti--Mella.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18421v3</guid>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jsc.2025.102460</arxiv:DOI>
      <dc:creator>Oskar Henriksson, Kristian Ranestad, Lisa Seccia, Teresa Yu</dc:creator>
    </item>
    <item>
      <title>Automatic Doubly Robust Forests</title>
      <link>https://arxiv.org/abs/2412.07184</link>
      <description>arXiv:2412.07184v2 Announce Type: replace-cross 
Abstract: This paper proposes the automatic Doubly Robust Random Forest (DRRF) algorithm for estimating the conditional expectation of a moment functional in the presence of high-dimensional nuisance functions. DRRF extends the automatic debiasing framework based on the Riesz representer to the conditional setting and enables nonparametric, forest-based estimation (Athey et al., 2019; Oprescu et al., 2019). In contrast to existing methods, DRRF does not require prior knowledge of the form of the debiasing term or impose restrictive parametric or semi-parametric assumptions on the target quantity. Additionally, it is computationally efficient in making predictions at multiple query points. We establish consistency and asymptotic normality results for the DRRF estimator under general assumptions, allowing for the construction of valid confidence intervals. Through extensive simulations in heterogeneous treatment effect (HTE) estimation, we demonstrate the superior performance of DRRF over benchmark approaches in terms of estimation accuracy, robustness, and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07184v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaomeng Chen, Junting Duan, Victor Chernozhukov, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Randomness, exchangeability, and conformal prediction</title>
      <link>https://arxiv.org/abs/2501.11689</link>
      <description>arXiv:2501.11689v3 Announce Type: replace-cross 
Abstract: This paper argues for a wider use of the functional theory of randomness, a modification of the algorithmic theory of randomness getting rid of unspecified additive constants. Both theories are useful for understanding relationships between the assumptions of IID data and data exchangeability. While the assumption of IID data is standard in machine learning, conformal prediction relies on data exchangeability. Nouretdinov, V'yugin, and Gammerman showed, using the language of the algorithmic theory of randomness, that conformal prediction is a universal method under the assumption of IID data. In this paper (written for the Alex Gammerman Festschrift) I will selectively review connections between exchangeability and the property of being IID, early history of conformal prediction, my encounters and collaboration with Alex and other interesting people, and a translation of Nouretdinov et al.'s results into the language of the functional theory of randomness, which moves it closer to practice. Namely, the translation says that every confidence predictor that is valid for IID data can be transformed to a conformal predictor without losing much in predictive efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11689v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Vovk</dc:creator>
    </item>
    <item>
      <title>Tensor Product Neural Networks for Functional ANOVA Model</title>
      <link>https://arxiv.org/abs/2502.15215</link>
      <description>arXiv:2502.15215v4 Announce Type: replace-cross 
Abstract: Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions (commonly referred to as components), is one of the most popular tools for interpretable AI, and recently, various neural networks have been developed for estimating each component in the functional ANOVA model. However, such neural networks are highly unstable when estimating each component since the components themselves are not uniquely defined. That is, there are multiple functional ANOVA decompositions for a given function. In this paper, we propose a novel neural network which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably and accurately. We call our proposed neural network ANOVA Tensor Product Neural Network (ANOVA-TPNN) since it is motivated by the tensor product basis expansion. Theoretically, we prove that ANOVA-TPNN can approximate any smooth function well. Empirically, we show that ANOVA-TPNN provide much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural networks do.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15215v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seokhun Park, Insung Kong, Yongchan Choi, Chanmoo Park, Yongdai Kim</dc:creator>
    </item>
    <item>
      <title>Universality of conformal prediction under the assumption of randomness</title>
      <link>https://arxiv.org/abs/2502.19254</link>
      <description>arXiv:2502.19254v2 Announce Type: replace-cross 
Abstract: Conformal predictors provide set or functional predictions that are valid under the assumption of randomness, i.e., under the assumption of independent and identically distributed data. The question asked in this paper is whether there are predictors that are valid in the same sense under the assumption of randomness and that are more efficient than conformal predictors. The answer is that the class of conformal predictors is universal in that only limited gains in predictive efficiency are possible. The previous work in this area has relied on the algorithmic theory of randomness and so involved unspecified constants, whereas this paper's results are much more practical. They are also shown to be optimal in some respects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19254v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Vovk</dc:creator>
    </item>
    <item>
      <title>Equilibrium Distribution for t-Distributed Stochastic Neighbor Embedding with Generalized Kernels</title>
      <link>https://arxiv.org/abs/2505.24311</link>
      <description>arXiv:2505.24311v2 Announce Type: replace-cross 
Abstract: T-distributed stochastic neighbor embedding (t-SNE) is a well-known algorithm for visualizing high-dimensional data by finding low-dimensional representations. In this paper, we study the convergence of t-SNE with generalized kernels and extend the results of Auffinger and Fletcher in 2023. Our work starts by giving a concrete formulation of generalized input and output kernels. Then we prove that under certain conditions, the t-SNE algorithm converges to an equilibrium distribution for a wide range of input and output kernels as the number of data points diverges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24311v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yi Gu</dc:creator>
    </item>
    <item>
      <title>Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds</title>
      <link>https://arxiv.org/abs/2506.03100</link>
      <description>arXiv:2506.03100v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has seen many empirical successes in recent years by aiding the LLM with external knowledge. However, its theoretical aspect has remained mostly unexplored. In this paper, we propose the first finite-sample generalization bound for RAG in in-context linear regression and derive an exact bias-variance tradeoff. Our framework views the retrieved texts as query-dependent noisy in-context examples and recovers the classical in-context learning (ICL) and standard RAG as the limit cases. Our analysis suggests that an intrinsic ceiling on generalization error exists on RAG as opposed to the ICL. Furthermore, our framework is able to model retrieval both from the training data and from external corpora by introducing uniform and non-uniform RAG noise. In line with our theory, we show the sample efficiency of ICL and RAG empirically with experiments on common QA benchmarks, such as Natural Questions and TriviaQA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03100v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yang Guo, Yutian Tao, Yifei Ming, Robert D. Nowak, Yingyu Liang</dc:creator>
    </item>
  </channel>
</rss>
