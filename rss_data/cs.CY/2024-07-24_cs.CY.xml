<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Comparative Analysis Vision of Worldwide AI Courses</title>
      <link>https://arxiv.org/abs/2407.16881</link>
      <description>arXiv:2407.16881v1 Announce Type: new 
Abstract: This research investigates the curriculum structures of undergraduate Artificial Intelligence (AI) education across universities worldwide. By examining the curricula of leading universities, the research seeks to contribute to a deeper understanding of AI education on a global scale, facilitating the alignment of educational practices with the evolving needs of the AI landscape. This research delves into the diverse course structures of leading universities, exploring contemporary trends and priorities to reveal the nuanced approaches in AI education. It also investigates the core AI topics and learning contents frequently taught, comparing them with the CS2023 curriculum guidance to identify convergence and divergence. Additionally, it examines how universities across different countries approach AI education, analyzing educational objectives, priorities, potential careers, and methodologies to understand the global landscape and implications of AI pedagogy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16881v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianing Xia (Deakin University, Australia), Man Li (Macquarie University, Australia), Jianxin Li (Deakin University, Australia)</dc:creator>
    </item>
    <item>
      <title>Cluster Model for parsimonious selection of variables and enhancing Students Employability Prediction</title>
      <link>https://arxiv.org/abs/2407.16884</link>
      <description>arXiv:2407.16884v1 Announce Type: new 
Abstract: Educational Data Mining (EDM) is a promising field, where data mining is widely used for predicting students performance. One of the most prevalent and recent challenge that higher education faces today is making students skillfully employable. Institutions possess large volume of data; still they are unable to reveal knowledge and guide their students. Data in education is generally very large, multidimensional and unbalanced in nature. Process of extracting knowledge from such data has its own set of problems and is a very complicated task. In this paper, Engineering and MCA (Masters in Computer Applications) students data is collected from various universities and institutes pan India. The dataset is large, unbalanced and multidimensional in nature. A cluster based model is presented in this paper, which, when applied at preprocessing stage helps in parsimonious selection of variables and improves the performance of predictive algorithms. Hence, facilitate in better prediction of Students Employability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16884v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pooja Thakar, Anil Mehta,  Manisha</dc:creator>
    </item>
    <item>
      <title>GPT-4's One-Dimensional Mapping of Morality: How the Accuracy of Country-Estimates Depends on Moral Domain</title>
      <link>https://arxiv.org/abs/2407.16886</link>
      <description>arXiv:2407.16886v1 Announce Type: new 
Abstract: Prior research demonstrates that Open AI's GPT models can predict variations in moral opinions between countries but that the accuracy tends to be substantially higher among high-income countries compared to low-income ones. This study aims to replicate previous findings and advance the research by examining how accuracy varies with different types of moral questions. Using responses from the World Value Survey and the European Value Study, covering 18 moral issues across 63 countries, we calculated country-level mean scores for each moral issue and compared them with GPT-4's predictions. Confirming previous findings, our results show that GPT-4 has greater predictive success in high-income than in low-income countries. However, our factor analysis reveals that GPT-4 bases its predictions primarily on a single dimension, presumably reflecting countries' degree of conservatism/liberalism. Conversely, the real-world moral landscape appears to be two-dimensional, differentiating between personal-sexual and violent-dishonest issues. When moral issues are categorized based on their moral domain, GPT-4's predictions are found to be remarkably accurate in the personal-sexual domain, across both high-income (r = .77) and low-income (r = .58) countries. Yet the predictive accuracy significantly drops in the violent-dishonest domain for both high-income (r = .30) and low-income (r = -.16) countries, indicating that GPT-4's one-dimensional world-view does not fully capture the complexity of the moral landscape. In sum, this study underscores the importance of not only considering country-specific characteristics to understand GPT-4's moral understanding, but also the characteristics of the moral issues at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16886v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pontus Strimling, Joel Krueger, Simon Karlsson</dc:creator>
    </item>
    <item>
      <title>Comprehensive AI Assessment Framework: Enhancing Educational Evaluation with Ethical AI Integration</title>
      <link>https://arxiv.org/abs/2407.16887</link>
      <description>arXiv:2407.16887v1 Announce Type: new 
Abstract: The integration of generative artificial intelligence (GenAI) tools into education has been a game-changer for teaching and assessment practices, bringing new opportunities, but also novel challenges which need to be dealt with. This paper presents the Comprehensive AI Assessment Framework (CAIAF), an evolved version of the AI Assessment Scale (AIAS) by Perkins, Furze, Roe, and MacVaugh, targeted toward the ethical integration of AI into educational assessments. This is where the CAIAF differs, as it incorporates stringent ethical guidelines, with clear distinctions based on educational levels, and advanced AI capabilities of real-time interactions and personalized assistance. The framework developed herein has a very intuitive use, mainly through the use of a color gradient that enhances the user-friendliness of the framework. Methodologically, the framework has been developed through the huge support of a thorough literature review and practical insight into the topic, becoming a dynamic tool to be used in different educational settings. The framework will ensure better learning outcomes, uphold academic integrity, and promote responsible use of AI, hence the need for this framework in modern educational practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16887v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sel\c{c}uk K{\i}l{\i}n\c{c}</dc:creator>
    </item>
    <item>
      <title>A Nested Model for AI Design and Validation</title>
      <link>https://arxiv.org/abs/2407.16888</link>
      <description>arXiv:2407.16888v1 Announce Type: new 
Abstract: The burgeoning field of artificial intelligence (AI) has yet to fully permeate real-world applications, largely due to issues of trust, transparency, and concerns about fairness and discrimination. Despite the increasing need for new and revised regulations to address the ethical and legal risks of using AI, there is a mismatch between regulatory science and AI, hindering the creation of a consistent framework. This highlights the need for guidance and regulation, especially as new AI legislation emerges. To bridge this gap, we propose a five-layer nested model for AI design and validation. This model is designed to address the challenges faced by AI practitioners and streamline the design and validation of AI applications and workflows, thereby improving fairness, trust, and AI adoption. This model not only parallels regulations and addresses the daily challenges faced by AI practitioners, but also provides prescriptive guidance for determining appropriate evaluation approaches by identifying threats to validity unique to each layer. We also provide three recommendations motivated by this model: authors should distinguish between layers when claiming contributions to clarify the specific areas in which the contribution is made and to avoid confusion, authors should explicitly state upstream assumptions to ensure that the context and limitations of their AI system are clearly understood, AI venues should promote thorough testing and validation of AI systems and their compliance with regulatory requirements</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16888v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshat Dubey, Zewen Yang, Georges Hattab</dc:creator>
    </item>
    <item>
      <title>Towards better visualizations of urban sound environments: insights from interviews</title>
      <link>https://arxiv.org/abs/2407.16889</link>
      <description>arXiv:2407.16889v1 Announce Type: new 
Abstract: Urban noise maps and noise visualizations traditionally provide macroscopic representations of noise levels across cities. However, those representations fail at accurately gauging the sound perception associated with these sound environments, as perception highly depends on the sound sources involved. This paper aims at analyzing the need for the representations of sound sources, by identifying the urban stakeholders for whom such representations are assumed to be of importance. Through spoken interviews with various urban stakeholders, we have gained insight into current practices, the strengths and weaknesses of existing tools and the relevance of incorporating sound sources into existing urban sound environment representations. Three distinct use of sound source representations emerged in this study: 1) noise-related complaints for industrials and specialized citizens, 2) soundscape quality assessment for citizens, and 3) guidance for urban planners. Findings also reveal diverse perspectives for the use of visualizations, which should use indicators adapted to the target audience, and enable data accessibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16889v1</guid>
      <category>cs.CY</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>INTERNOISE 2024, Aug 2024, Nantes (France), France</arxiv:journal_reference>
      <dc:creator>Modan Tailleur (LS2N), Pierre Aumond (UMRAE), Vincent Tourre (AAU), Mathieu Lagrange (LS2N)</dc:creator>
    </item>
    <item>
      <title>Why Machines Can't Be Moral: Turing's Halting Problem and the Moral Limits of Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2407.16890</link>
      <description>arXiv:2407.16890v1 Announce Type: new 
Abstract: In this essay, I argue that explicit ethical machines, whose moral principles are inferred through a bottom-up approach, are unable to replicate human-like moral reasoning and cannot be considered moral agents. By utilizing Alan Turing's theory of computation, I demonstrate that moral reasoning is computationally intractable by these machines due to the halting problem. I address the frontiers of machine ethics by formalizing moral problems into 'algorithmic moral questions' and by exploring moral psychology's dual-process model. While the nature of Turing Machines theoretically allows artificial agents to engage in recursive moral reasoning, critical limitations are introduced by the halting problem, which states that it is impossible to predict with certainty whether a computational process will halt. A thought experiment involving a military drone illustrates this issue, showing that an artificial agent might fail to decide between actions due to the halting problem, which limits the agent's ability to make decisions in all instances, undermining its moral agency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16890v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Massimo Passamonti</dc:creator>
    </item>
    <item>
      <title>Cultural Value Differences of LLMs: Prompt, Language, and Model Size</title>
      <link>https://arxiv.org/abs/2407.16891</link>
      <description>arXiv:2407.16891v1 Announce Type: new 
Abstract: Our study aims to identify behavior patterns in cultural values exhibited by large language models (LLMs). The studied variants include question ordering, prompting language, and model size. Our experiments reveal that each tested LLM can efficiently behave with different cultural values. More interestingly: (i) LLMs exhibit relatively consistent cultural values when presented with prompts in a single language. (ii) The prompting language e.g., Chinese or English, can influence the expression of cultural values. The same question can elicit divergent cultural values when the same LLM is queried in a different language. (iii) Differences in sizes of the same model (e.g., Llama2-7B vs 13B vs 70B) have a more significant impact on their demonstrated cultural values than model differences (e.g., Llama2 vs Mixtral). Our experiments reveal that query language and model size of LLM are the main factors resulting in cultural value differences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16891v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qishuai Zhong, Yike Yun, Aixin Sun</dc:creator>
    </item>
    <item>
      <title>Exploring Fusion Techniques in Multimodal AI-Based Recruitment: Insights from FairCVdb</title>
      <link>https://arxiv.org/abs/2407.16892</link>
      <description>arXiv:2407.16892v1 Announce Type: new 
Abstract: Despite the large body of work on fairness-aware learning for individual modalities like tabular data, images, and text, less work has been done on multimodal data, which fuses various modalities for a comprehensive analysis. In this work, we investigate the fairness and bias implications of multimodal fusion techniques in the context of multimodal AI-based recruitment systems using the FairCVdb dataset. Our results show that early-fusion closely matches the ground truth for both demographics, achieving the lowest MAEs by integrating each modality's unique characteristics. In contrast, late-fusion leads to highly generalized mean scores and higher MAEs. Our findings emphasise the significant potential of early-fusion for accurate and fair applications, even in the presence of demographic biases, compared to late-fusion. Future research could explore alternative fusion strategies and incorporate modality-related fairness constraints to improve fairness. For code and additional insights, visit: https://github.com/Swati17293/Multimodal-AI-Based-Recruitment-FairCVdb</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16892v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swati Swati, Arjun Roy, Eirini Ntoutsi</dc:creator>
    </item>
    <item>
      <title>The Price of Prompting: Profiling Energy Use in Large Language Models Inference</title>
      <link>https://arxiv.org/abs/2407.16893</link>
      <description>arXiv:2407.16893v1 Announce Type: new 
Abstract: In the rapidly evolving realm of artificial intelligence, deploying large language models (LLMs) poses increasingly pressing computational and environmental challenges. This paper introduces MELODI - Monitoring Energy Levels and Optimization for Data-driven Inference - a multifaceted framework crafted to monitor and analyze the energy consumed during LLM inference processes. MELODI enables detailed observations of power consumption dynamics and facilitates the creation of a comprehensive dataset reflective of energy efficiency across varied deployment scenarios. The dataset, generated using MELODI, encompasses a broad spectrum of LLM deployment frameworks, multiple language models, and extensive prompt datasets, enabling a comparative analysis of energy use. Using the dataset, we investigate how prompt attributes, including length and complexity, correlate with energy expenditure. Our findings indicate substantial disparities in energy efficiency, suggesting ample scope for optimization and adoption of sustainable measures in LLM deployment. Our contribution lies not only in the MELODI framework but also in the novel dataset, a resource that can be expanded by other researchers. Thus, MELODI is a foundational tool and dataset for advancing research into energy-conscious LLM deployment, steering the field toward a more sustainable future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16893v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Erik Johannes Husom, Arda Goknil, Lwin Khin Shar, Sagar Sen</dc:creator>
    </item>
    <item>
      <title>Estimating the Increase in Emissions caused by AI-augmented Search</title>
      <link>https://arxiv.org/abs/2407.16894</link>
      <description>arXiv:2407.16894v1 Announce Type: new 
Abstract: AI-generated answers to conventional search queries dramatically increase the energy consumption. By our estimates, energy demand increase by 60-70 times. This is a based on an updated estimate of energy consumption for conventional search and recent work on the energy demand of queries to the BLOOM model, a 176B parameter model, and OpenAI's ChatGPT, which is of similar complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16894v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wim Vanderbauwhede</dc:creator>
    </item>
    <item>
      <title>(Unfair) Norms in Fairness Research: A Meta-Analysis</title>
      <link>https://arxiv.org/abs/2407.16895</link>
      <description>arXiv:2407.16895v1 Announce Type: new 
Abstract: Algorithmic fairness has emerged as a critical concern in artificial intelligence (AI) research. However, the development of fair AI systems is not an objective process. Fairness is an inherently subjective concept, shaped by the values, experiences, and identities of those involved in research and development. To better understand the norms and values embedded in current fairness research, we conduct a meta-analysis of algorithmic fairness papers from two leading conferences on AI fairness and ethics, AIES and FAccT, covering a final sample of 139 papers over the period from 2018 to 2022. Our investigation reveals two concerning trends: first, a US-centric perspective dominates throughout fairness research; and second, fairness studies exhibit a widespread reliance on binary codifications of human identity (e.g., "Black/White", "male/female"). These findings highlight how current research often overlooks the complexities of identity and lived experiences, ultimately failing to represent diverse global contexts when defining algorithmic bias and fairness. We discuss the limitations of these research design choices and offer recommendations for fostering more inclusive and representative approaches to fairness in AI systems, urging a paradigm shift that embraces nuanced, global understandings of human identity and values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16895v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jennifer Chien, A. Stevie Bergman, Kevin R. McKee, Nenad Tomasev, Vinodkumar Prabhakaran, Rida Qadri, Nahema Marchal, William Isaac</dc:creator>
    </item>
    <item>
      <title>Free to play: UN Trade and Development's experience with developing its own open-source Retrieval Augmented Generation Large Language Model application</title>
      <link>https://arxiv.org/abs/2407.16896</link>
      <description>arXiv:2407.16896v1 Announce Type: new 
Abstract: Generative artificial intelligence (AI), and in particular Large Language Models (LLMs), have exploded in popularity and attention since the release to the public of ChatGPT's Generative Pre-trained Transformer (GPT)-3.5 model in November of 2022. Due to the power of these general purpose models and their ability to communicate in natural language, they can be useful in a range of domains, including the work of official statistics and international organizations. However, with such a novel and seemingly complex technology, it can feel as if generative AI is something that happens to an organization, something that can be talked about but not understood, that can be commented on but not contributed to. Additionally, the costs of adoption and operation of proprietary solutions can be both uncertain and high, a barrier for often cost-constrained international organizations. In the face of these challenges, United Nations Trade and Development (UNCTAD), through its Global Crisis Response Group (GCRG), has explored and developed its own open-source Retrieval Augmented Generation (RAG) LLM application. RAG makes LLMs aware of and more useful for the organization's domain and work. Developing in-house solutions comes with pros and cons, with pros including cost, flexibility, and fostering institutional knowledge. Cons include time and skill investments and gaps and application polish and power. The three libraries developed to produce the app, nlp_pipeline for document processing and statistical analysis, local_rag_llm for running a local RAG LLM, and streamlit_rag for the user interface, are publicly available on PyPI and GitHub with Dockerfiles. A fourth library, local_llm_finetune, is also available for fine-tuning existing LLMs which can then be used in the application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16896v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Hopp</dc:creator>
    </item>
    <item>
      <title>Introducing Individuality into Students' High School Timetables</title>
      <link>https://arxiv.org/abs/2407.16898</link>
      <description>arXiv:2407.16898v1 Announce Type: new 
Abstract: In a perfect world, each high school student could pursue their interests through a personalized timetable that supports their strengths, weaknesses, and curiosities. While recent research has shown that school systems are evolving to support those developments by strengthening modularity in their curricula, there is often a hurdle that prevents the complete success of such a system: the scheduling process is too complex. While there are many tools that assist with scheduling timetables in an effective way, they usually arrange students into groups and classes with similar interests instead of handling each student individually. In this paper, we propose an extension of the popular XHSTT framework that adds two new constraints to model the individual student choices as well as the requirements for group formation that arise from them. Those two constraints were identified through extensive interviews with school administrators and other school timetabling experts from six European countries. We propose a corresponding ILP formulation and show first optimization results for real-world instances from schools in Germany.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16898v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andreas Krystallidis, Rub\'en Ruiz-Torrubiano</dc:creator>
    </item>
    <item>
      <title>The Potential and Perils of Generative Artificial Intelligence for Quality Improvement and Patient Safety</title>
      <link>https://arxiv.org/abs/2407.16902</link>
      <description>arXiv:2407.16902v1 Announce Type: new 
Abstract: Generative artificial intelligence (GenAI) has the potential to improve healthcare through automation that enhances the quality and safety of patient care. Powered by foundation models that have been pretrained and can generate complex content, GenAI represents a paradigm shift away from the more traditional focus on task-specific classifiers that have dominated the AI landscape thus far. We posit that the imminent application of GenAI in healthcare will be through well-defined, low risk, high value, and narrow applications that automate healthcare workflows at the point of care using smaller foundation models. These models will be finetuned for different capabilities and application specific scenarios and will have the ability to provide medical explanations, reference evidence within a retrieval augmented framework and utilizing external tools. We contrast this with a general, all-purpose AI model for end-to-end clinical decision making that improves clinician performance, including safety-critical diagnostic tasks, which will require greater research prior to implementation. We consider areas where 'human in the loop' Generative AI can improve healthcare quality and safety by automating mundane tasks. Using the principles of implementation science will be critical for integrating 'end to end' GenAI systems that will be accepted by healthcare teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16902v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laleh Jalilian, Daniel McDuff, Achuta Kadambi</dc:creator>
    </item>
    <item>
      <title>US-China perspectives on extreme AI risks and global governance</title>
      <link>https://arxiv.org/abs/2407.16903</link>
      <description>arXiv:2407.16903v1 Announce Type: new 
Abstract: The United States and China will play an important role in navigating safety and security challenges relating to advanced artificial intelligence. We sought to better understand how experts in each country describe safety and security threats from advanced artificial intelligence, extreme risks from AI, and the potential for international cooperation. Specifically, we compiled publicly-available statements from major technical and policy leaders in both the United States and China. We focused our analysis on advanced forms of artificial intelligence, such as artificial general intelligence (AGI), that may have the most significant impacts on national and global security. Experts in both countries expressed concern about risks from AGI, risks from intelligence explosions, and risks from AI systems that escape human control. Both countries have also launched early efforts designed to promote international cooperation around safety standards and risk management practices. Notably, our findings only reflect information from publicly available sources. Nonetheless, our findings can inform policymakers and researchers about the state of AI discourse in the US and China. We hope such work can contribute to policy discussions around advanced AI, its global security threats, and potential international dialogues or agreements to mitigate such threats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16903v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash Wasil, Tim Durgin</dc:creator>
    </item>
    <item>
      <title>Assessing the role of clinical summarization and patient chart review within communications, medical management, and diagnostics</title>
      <link>https://arxiv.org/abs/2407.16905</link>
      <description>arXiv:2407.16905v1 Announce Type: new 
Abstract: Effective summarization of unstructured patient data in electronic health records (EHRs) is crucial for accurate diagnosis and efficient patient care, yet clinicians often struggle with information overload and time constraints. This review dives into recent literature and case studies on both the significant impacts and outstanding issues of patient chart review on communications, diagnostics, and management. It also discusses recent efforts to integrate artificial intelligence (AI) into clinical summarization tasks, and its transformative impact on the clinician's potential, including but not limited to reductions of administrative burden and improved patient-centered care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16905v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chanseo Lee, Kimon-Aristotelis Vogt, Sonu Kumar</dc:creator>
    </item>
    <item>
      <title>European Network For Gender Balance in Informatics (EUGAIN): Activities and Results</title>
      <link>https://arxiv.org/abs/2407.16906</link>
      <description>arXiv:2407.16906v1 Announce Type: new 
Abstract: This chapter provides a summary of the activities and results of the European Network For Gender Balance in Informatics (EUGAIN, EU COST Action CA19122). The main aim and objective of the network is to improve gender balance in informatics at all levels, from undergraduate and graduate studies to participation and leadership both in academia and industry, through the creation of a European network of colleagues working at the forefront of the efforts for gender balance in informatics in their countries and research communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16906v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Letizia Jaccheri, Barbora Buhnova, Birgit Penzenstadler, Karima Boudaoud, Valentina Lenarduzzi</dc:creator>
    </item>
    <item>
      <title>Research on Education Big Data for Students Academic Performance Analysis based on Machine Learning</title>
      <link>https://arxiv.org/abs/2407.16907</link>
      <description>arXiv:2407.16907v1 Announce Type: new 
Abstract: The application of the Internet in the field of education is becoming more and more popular, and a large amount of educational data is generated in the process. How to effectively use these data has always been a key issue in the field of educational data mining. In this work, a machine learning model based on Long Short-Term Memory Network (LSTM) was used to conduct an in-depth analysis of educational big data to evaluate student performance. The LSTM model efficiently processes time series data, allowing us to capture time-dependent and long-term trends in students' learning activities. This approach is particularly useful for analyzing student progress, engagement, and other behavioral patterns to support personalized education. In an experimental analysis, we verified the effectiveness of the deep learning method in predicting student performance by comparing the performance of different models. Strict cross-validation techniques are used to ensure the accuracy and generalization of experimental results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16907v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chun Wang, Jiexiao Chen, Ziyang Xie, Jianke Zou</dc:creator>
    </item>
    <item>
      <title>Using Helium Balloon Flying Drones for Introductory CS Education</title>
      <link>https://arxiv.org/abs/2407.16909</link>
      <description>arXiv:2407.16909v1 Announce Type: new 
Abstract: In the rapidly evolving field of computer science education, novel approaches to teaching fundamental concepts are crucial for engaging a diverse student body. Given the growing demand for a computing-skilled workforce, it is essential to adapt educational methods to capture the interest of a broader audience than what current computing education typically targets. Engaging educational experiences have been shown to have a positive impact on learning outcomes and examination performance, especially within computing education. Moreover, physical computing devices have been shown to correlate with increased student motivation when students are studying computer science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16909v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanley Cao, Christopher Gregg</dc:creator>
    </item>
    <item>
      <title>Handling Device Heterogeneity for Deep Learning-based Localization</title>
      <link>https://arxiv.org/abs/2407.16923</link>
      <description>arXiv:2407.16923v1 Announce Type: new 
Abstract: Deep learning-based fingerprinting is one of the current promising technologies for outdoor localization in cellular networks. However, deploying such localization systems for heterogeneous phones affects their accuracy as the cellular received signal strength (RSS) readings vary for different types of phones. In this paper, we introduce a number of techniques for addressing the phones heterogeneity problem in the deep-learning based localization systems. The basic idea is either to approximate a function that maps the cellular RSS measurements between different devices or to transfer the knowledge across them.
  Evaluation of the proposed techniques using different Android phones on four independent testbeds shows that our techniques can improve the localization accuracy by more than 220% for the four testbeds as compared to the state-of-the-art systems. This highlights the promise of the proposed device heterogeneity handling techniques for enabling a wide deployment of deep learning-based localization systems over different devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16923v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed Shokry, Moustafa Youssef</dc:creator>
    </item>
    <item>
      <title>DeepCell: A Ubiquitous Accurate Provider-side Cellular-based Localization</title>
      <link>https://arxiv.org/abs/2407.16927</link>
      <description>arXiv:2407.16927v1 Announce Type: new 
Abstract: Although outdoor localization is already available to the general public and businesses through the wide spread use of the GPS, it is not supported by low-end phones, requires a direct line of sight to satellites and can drain phone battery quickly. The current fingerprinting solutions can provide high-accuracy localization but are based on the client side. This limits their ubiquitous deployment and accuracy. In this paper, we introduce DeepCell: a provider-side fingerprinting localization system that can provide high accuracy localization for any cell phone. To build its fingerprint, DeepCell leverages the unlabeled cellular measurements recorded by the cellular provider while opportunistically synchronizing with selected client devices to get location labels. The fingerprint is then used to train a deep neural network model that is harnessed for localization. To achieve this goal, DeepCell need to address a number of challenges including using unlabeled data from the provider side, handling noise and sparsity, scaling the data to large areas, and finally providing enough data that is required for training deep models without overhead. Evaluation of DeepCell in a typical realistic environment shows that it can achieve a consistent median accuracy of 29m. This accuracy outperforms the state-of-the-art client-based cellular-based systems by more than 75.4%. In addition, the same accuracy is extended to low-end phones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16927v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed Shokry, Moustafa Youssef</dc:creator>
    </item>
    <item>
      <title>Pensieve Discuss: Scalable Small-Group CS Tutoring System with AI</title>
      <link>https://arxiv.org/abs/2407.17007</link>
      <description>arXiv:2407.17007v1 Announce Type: new 
Abstract: Small-group tutoring in Computer Science (CS) is effective, but presents the challenge of providing a dedicated tutor for each group and encouraging collaboration among group members at scale. We present Pensieve Discuss, a software platform that integrates synchronous editing for scaffolded programming problems with online human and AI tutors, designed to improve student collaboration and experience during group tutoring sessions. Our semester-long deployment to 800 students in a CS1 course demonstrated consistently high collaboration rates, positive feedback about the AI tutor's helpfulness and correctness, increased satisfaction with the group tutoring experience, and a substantial increase in question volume. The use of our system was preferred over an interface lacking AI tutors and synchronous editing capabilities. Our experiences suggest that small-group tutoring sessions are an important avenue for future research in educational AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17007v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yoonseok Yang, Jack Liu, J. D. Zamfirescu-Pereira, John DeNero</dc:creator>
    </item>
    <item>
      <title>The EU-US Data Privacy Framework: Is the Dragon Eating its Own Tail?</title>
      <link>https://arxiv.org/abs/2407.17021</link>
      <description>arXiv:2407.17021v1 Announce Type: new 
Abstract: The European Commission adequacy decision on the EU US Data Privacy Framework, adopted on July 10th, 2023, marks a crucial moment in transatlantic data protection. Following an Executive Order issued by President Biden in October 2022, this decision confirms that the United States meets European Union standards for personal data protection. The decision extends to all transfers from the European Economic Area to US entities participating in the framework, promoting privacy rights while facilitating data exchange. Key aspects include oversight of US public authorities access to transferred data, the introduction of a dual tier redress mechanism, and granting new rights to EU individuals, encompassing data access and rectification. However, the framework presents both promise and challenges in health data transfers. While streamlining exchange and aligning legal standards, it grapples with the complexities of divergent privacy laws. The recent bill for the introduction of a US federal privacy law emphasizes the urgent need for ongoing reform. Lingering concerns persist regarding the framework resilience, especially amid potential legal battles before the Court of Justice of the EU. The history of transatlantic data transfers between the EU and the US is riddled with vulnerabilities, reminiscent of the Ouroboros, an ancient symbol of a serpent or dragon eating its own tail, hinting at the looming possibility of the framework facing invalidation once again. This article delves into the main requirements of the framework and offers insights on how healthcare organizations can navigate it effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17021v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcelo Corrales Compagnucci</dc:creator>
    </item>
    <item>
      <title>Mapping the individual, social, and biospheric impacts of Foundation Models</title>
      <link>https://arxiv.org/abs/2407.17129</link>
      <description>arXiv:2407.17129v1 Announce Type: new 
Abstract: Responding to the rapid roll-out and large-scale commercialization of foundation models, large language models, and generative AI, an emerging body of work is shedding light on the myriad impacts these technologies are having across society. Such research is expansive, ranging from the production of discriminatory, fake and toxic outputs, and privacy and copyright violations, to the unjust extraction of labor and natural resources. The same has not been the case in some of the most prominent AI governance initiatives in the global north like the UK's AI Safety Summit and the G7's Hiroshima process, which have influenced much of the international dialogue around AI governance. Despite the wealth of cautionary tales and evidence of algorithmic harm, there has been an ongoing over-emphasis within the AI governance discourse on technical matters of safety and global catastrophic or existential risks. This narrowed focus has tended to draw attention away from very pressing social and ethical challenges posed by the current brute-force industrialization of AI applications. To address such a visibility gap between real-world consequences and speculative risks, this paper offers a critical framework to account for the social, political, and environmental dimensions of foundation models and generative AI. We identify 14 categories of risks and harms and map them according to their individual, social, and biospheric impacts. We argue that this novel typology offers an integrative perspective to address the most urgent negative impacts of foundation models and their downstream applications. We conclude with recommendations on how this typology could be used to inform technical and normative interventions to advance responsible AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17129v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3630106.3658939</arxiv:DOI>
      <arxiv:journal_reference>In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24). Association for Computing Machinery, New York, NY, USA, 776-796</arxiv:journal_reference>
      <dc:creator>Andr\'es Dom\'inguez Hern\'andez, Shyam Krishna, Antonella Maia Perini, Michael Katell, SJ Bennett, Ann Borda, Youmna Hashem, Semeli Hadjiloizou, Sabeehah Mahomed, Smera Jayadeva, Mhairi Aitken, David Leslie</dc:creator>
    </item>
    <item>
      <title>AI Emergency Preparedness: Examining the federal government's ability to detect and respond to AI-related national security threats</title>
      <link>https://arxiv.org/abs/2407.17347</link>
      <description>arXiv:2407.17347v1 Announce Type: new 
Abstract: We examine how the federal government can enhance its AI emergency preparedness: the ability to detect and prepare for time-sensitive national security threats relating to AI. Emergency preparedness can improve the government's ability to monitor and predict AI progress, identify national security threats, and prepare effective response plans for plausible threats and worst-case scenarios. Our approach draws from fields in which experts prepare for threats despite uncertainty about their exact nature or timing (e.g., counterterrorism, cybersecurity, pandemic preparedness). We focus on three plausible risk scenarios: (1) loss of control (threats from a powerful AI system that becomes capable of escaping human control), (2) cybersecurity threats from malicious actors (threats from a foreign actor that steals the model weights of a powerful AI system), and (3) biological weapons proliferation (threats from users identifying a way to circumvent the safeguards of a publicly-released model in order to develop biological weapons.) We evaluate the federal government's ability to detect, prevent, and respond to these threats. Then, we highlight potential gaps and offer recommendations to improve emergency preparedness. We conclude by describing how future work on AI emergency preparedness can be applied to improve policymakers' understanding of risk scenarios, identify gaps in detection capabilities, and form preparedness plans to improve the effectiveness of federal responses to AI-related national security threats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17347v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash Wasil, Everett Smith, Corin Katzke, Justin Bullock</dc:creator>
    </item>
    <item>
      <title>Media Manipulations in the Coverage of Events of the Ukrainian Revolution of Dignity: Historical, Linguistic, and Psychological Approaches</title>
      <link>https://arxiv.org/abs/2407.17425</link>
      <description>arXiv:2407.17425v1 Announce Type: new 
Abstract: This article examines the use of manipulation in the coverage of events of the Ukrainian Revolution of Dignity in the mass media, namely in the content of the online newspaper Ukrainian Truth (Ukrainska pravda), online newspaper High Castle (Vysokyi Zamok), and online newspaper ZIK during the public protest, namely during the Ukrainian Revolution of Dignity. Contents of these online newspapers the historical, linguistic, and psychological approaches are used. Also media manipulations in the coverage of events of the Ukrainian Revolution of Dignity are studied. Internet resources that cover news are analyzed. Current and most popular Internet resources are identified. The content of online newspapers is analyzed and statistically processed. Internet content of newspapers by the level of significance of data (very significant data, significant data and insignificant data) is classified. The algorithm of detection of the media manipulations in the highlighting the course of the Ukrainian revolutions based on historical, linguistic, and psychological approaches is designed. Methods of counteracting information attacks in online newspapers are developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17425v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan Khoma, Solomia Fedushko, Zoryana Kunch</dc:creator>
    </item>
    <item>
      <title>Uncertainty-preserving deep knowledge tracing with state-space models</title>
      <link>https://arxiv.org/abs/2407.17427</link>
      <description>arXiv:2407.17427v1 Announce Type: new 
Abstract: A central goal of both knowledge tracing and traditional assessment is to quantify student knowledge and skills at a given point in time. Deep knowledge tracing flexibly considers a student's response history but does not quantify epistemic uncertainty, while IRT and CDM compute measurement error but only consider responses to individual tests in isolation from a student's past responses. Elo and BKT could bridge this divide, but the simplicity of the underlying models limits information sharing across skills and imposes strong inductive biases. To overcome these limitations, we introduce Dynamic LENS, a modeling paradigm that combines the flexible uncertainty-preserving properties of variational autoencoders with the principled information integration of Bayesian state-space models. Dynamic LENS allows information from student responses to be collected across time, while treating responses from the same test as exchangeable observations generated by a shared latent state. It represents student knowledge as Gaussian distributions in high-dimensional space and combines estimates both within tests and across time using Bayesian updating. We show that Dynamic LENS has similar predictive performance to competing models, while preserving the epistemic uncertainty - the deep learning analogue to measurement error - that DKT models lack. This approach provides a conceptual bridge across an important divide between models designed for formative practice and summative assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17427v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>S. Thomas Christie, Carson Cook, Anna N. Rafferty</dc:creator>
    </item>
    <item>
      <title>How Do Students Interact with an LLM-powered Virtual Teaching Assistant in Different Educational Settings?</title>
      <link>https://arxiv.org/abs/2407.17429</link>
      <description>arXiv:2407.17429v1 Announce Type: new 
Abstract: Jill Watson, a virtual teaching assistant powered by LLMs, answers student questions and engages them in extended conversations on courseware provided by the instructors. In this paper, we analyze student interactions with Jill across multiple courses and colleges, focusing on the types and complexity of student questions based on Bloom's Revised Taxonomy and tool usage patterns. We find that, by supporting a wide range of cognitive demands, Jill encourages students to engage in sophisticated, higher-order cognitive questions. However, the frequency of usage varies significantly across deployments, and the types of questions asked depend on course-specific contexts. These findings pave the way for future work on AI-driven educational tools tailored to individual learning styles and course structure, potentially enhancing both the teaching and learning experience in classrooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17429v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratyusha Maiti, Ashok K. Goel</dc:creator>
    </item>
    <item>
      <title>AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies</title>
      <link>https://arxiv.org/abs/2407.17436</link>
      <description>arXiv:2407.17436v1 Announce Type: new 
Abstract: Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-Bench 2024, uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-Bench 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17436v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zeng, Yu Yang, Andy Zhou, Jeffrey Ziwei Tan, Yuheng Tu, Yifan Mai, Kevin Klyman, Minzhou Pan, Ruoxi Jia, Dawn Song, Percy Liang, Bo Li</dc:creator>
    </item>
    <item>
      <title>Multimodal Machine Learning in Mental Health: A Survey of Data, Algorithms, and Challenges</title>
      <link>https://arxiv.org/abs/2407.16804</link>
      <description>arXiv:2407.16804v1 Announce Type: cross 
Abstract: The application of machine learning (ML) in detecting, diagnosing, and treating mental health disorders is garnering increasing attention. Traditionally, research has focused on single modalities, such as text from clinical notes, audio from speech samples, or video of interaction patterns. Recently, multimodal ML, which combines information from multiple modalities, has demonstrated significant promise in offering novel insights into human behavior patterns and recognizing mental health symptoms and risk factors. Despite its potential, multimodal ML in mental health remains an emerging field, facing several complex challenges before practical applications can be effectively developed. This survey provides a comprehensive overview of the data availability and current state-of-the-art multimodal ML applications for mental health. It discusses key challenges that must be addressed to advance the field. The insights from this survey aim to deepen the understanding of the potential and limitations of multimodal ML in mental health, guiding future research and development in this evolving domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16804v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zahraa Al Sahili, Ioannis Patras, Matthew Purver</dc:creator>
    </item>
    <item>
      <title>TAMIGO: Empowering Teaching Assistants using LLM-assisted viva and code assessment in an Advanced Computing Class</title>
      <link>https://arxiv.org/abs/2407.16805</link>
      <description>arXiv:2407.16805v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have significantly transformed the educational landscape, offering new tools for students, instructors, and teaching assistants. This paper investigates the application of LLMs in assisting teaching assistants (TAs) with viva and code assessments in an advanced computing class on distributed systems in an Indian University. We develop TAMIGO, an LLM-based system for TAs to evaluate programming assignments.
  For viva assessment, the TAs generated questions using TAMIGO and circulated these questions to the students for answering. The TAs then used TAMIGO to generate feedback on student answers. For code assessment, the TAs selected specific code blocks from student code submissions and fed it to TAMIGO to generate feedback for these code blocks. The TAMIGO-generated feedback for student answers and code blocks was used by the TAs for further evaluation.
  We evaluate the quality of LLM-generated viva questions, model answers, feedback on viva answers, and feedback on student code submissions. Our results indicate that LLMs are highly effective at generating viva questions when provided with sufficient context and background information. However, the results for LLM-generated feedback on viva answers were mixed; instances of hallucination occasionally reduced the accuracy of feedback. Despite this, the feedback was consistent, constructive, comprehensive, balanced, and did not overwhelm the TAs. Similarly, for code submissions, the LLM-generated feedback was constructive, comprehensive and balanced, though there was room for improvement in aligning the feedback with the instructor-provided rubric for code evaluation. Our findings contribute to understanding the benefits and limitations of integrating LLMs into educational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16805v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anishka IIITD, Diksha Sethi, Nipun Gupta, Shikhar Sharma, Srishti Jain, Ujjwal Singhal, Dhruv Kumar</dc:creator>
    </item>
    <item>
      <title>A Standardized Machine-readable Dataset Documentation Format for Responsible AI</title>
      <link>https://arxiv.org/abs/2407.16883</link>
      <description>arXiv:2407.16883v1 Announce Type: cross 
Abstract: Data is critical to advancing AI technologies, yet its quality and documentation remain significant challenges, leading to adverse downstream effects (e.g., potential biases) in AI applications. This paper addresses these issues by introducing Croissant-RAI, a machine-readable metadata format designed to enhance the discoverability, interoperability, and trustworthiness of AI datasets. Croissant-RAI extends the Croissant metadata format and builds upon existing responsible AI (RAI) documentation frameworks, offering a standardized set of attributes and practices to facilitate community-wide adoption. Leveraging established web-publishing practices, such as Schema.org, Croissant-RAI enables dataset users to easily find and utilize RAI metadata regardless of the platform on which the datasets are published. Furthermore, it is seamlessly integrated into major data search engines, repositories, and machine learning frameworks, streamlining the reading and writing of responsible AI metadata within practitioners' existing workflows. Croissant-RAI was developed through a community-led effort. It has been designed to be adaptable to evolving documentation requirements and is supported by a Python library and a visual editor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16883v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nitisha Jain, Mubashara Akhtar, Joan Giner-Miguelez, Rajat Shinde, Joaquin Vanschoren, Steffen Vogler, Sujata Goswami, Yuhan Rao, Tim Santos, Luis Oala, Michalis Karamousadakis, Manil Maskey, Pierre Marcenac, Costanza Conforti, Michael Kuchnik, Lora Aroyo, Omar Benjelloun, Elena Simperl</dc:creator>
    </item>
    <item>
      <title>Regulating AI Adaptation: An Analysis of AI Medical Device Updates</title>
      <link>https://arxiv.org/abs/2407.16900</link>
      <description>arXiv:2407.16900v1 Announce Type: cross 
Abstract: While the pace of development of AI has rapidly progressed in recent years, the implementation of safe and effective regulatory frameworks has lagged behind. In particular, the adaptive nature of AI models presents unique challenges to regulators as updating a model can improve its performance but also introduce safety risks. In the US, the Food and Drug Administration (FDA) has been a forerunner in regulating and approving hundreds of AI medical devices. To better understand how AI is updated and its regulatory considerations, we systematically analyze the frequency and nature of updates in FDA-approved AI medical devices. We find that less than 2% of all devices report having been updated by being re-trained on new data. Meanwhile, nearly a quarter of devices report updates in the form of new functionality and marketing claims. As an illustrative case study, we analyze pneumothorax detection models and find that while model performance can degrade by as much as 0.18 AUC when evaluated on new sites, re-training on site-specific data can mitigate this performance drop, recovering up to 0.23 AUC. However, we also observed significant degradation on the original site after re-training using data from new sites, providing insight from one example that challenges the current one-model-fits-all approach to regulatory approvals. Our analysis provides an in-depth look at the current state of FDA-approved AI device updates and insights for future regulatory policies toward model updating and adaptive AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16900v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>CHIL 2024</arxiv:journal_reference>
      <dc:creator>Kevin Wu, Eric Wu, Kit Rodolfa, Daniel E. Ho, James Zou</dc:creator>
    </item>
    <item>
      <title>How Video Passthrough Headsets Influence Perception of Self and Others</title>
      <link>https://arxiv.org/abs/2407.16904</link>
      <description>arXiv:2407.16904v1 Announce Type: cross 
Abstract: With the increasing adoption of mixed reality headsets with video passthrough functionality, concerns over perceptual and social effects have surfaced. Building on prior qualitative findings, this study quantitatively investigates the impact of video passthrough on users. Forty participants completed a body transfer task twice, once while wearing a headset in video passthrough and once without a headset. Results indicate that using video passthrough induces simulator sickness, creates social absence, (another person in the physical room feels less present), alters self-reported body schema, and distorts distance perception. On the other hand, compared to past research which showed perceptual aftereffects from video passthrough, the current study found none. We discuss the broader implications for the widespread adoption of mixed reality headsets and their impact on theories surrounding presence and body transfer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16904v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monique Santoso, Jeremy N. Bailenson</dc:creator>
    </item>
    <item>
      <title>Synthetic Data, Similarity-based Privacy Metrics, and Regulatory (Non-)Compliance</title>
      <link>https://arxiv.org/abs/2407.16929</link>
      <description>arXiv:2407.16929v1 Announce Type: cross 
Abstract: In this paper, we argue that similarity-based privacy metrics cannot ensure regulatory compliance of synthetic data. Our analysis and counter-examples show that they do not protect against singling out and linkability and, among other fundamental issues, completely ignore the motivated intruder test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16929v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georgi Ganev</dc:creator>
    </item>
    <item>
      <title>Bridging Trust into the Blockchain: A Systematic Review on On-Chain Identity</title>
      <link>https://arxiv.org/abs/2407.17276</link>
      <description>arXiv:2407.17276v1 Announce Type: cross 
Abstract: The ongoing regulation of blockchain-based services and applications requires the identification of users who are issuing transactions on the blockchain. This systematic review explores the current status, identifies research gaps, and outlines future research directions for establishing trusted and privacy-compliant identities on the blockchain (on-chain identity). A systematic search term was applied across various scientific databases, collecting 2232 potentially relevant research papers. These papers were narrowed down in two methodologically executed steps to 98 and finally to 13 relevant sources. The relevant articles were then systematically analyzed based on a set of screening questions. The results of the selected studies have provided insightful findings on the mechanisms of on-chain identities. On-chain identities are established using zero-knowledge proofs, public key infrastructure/certificates, and web of trust approaches. The technologies and architectures used by the authors are also highlighted. Trust has emerged as a key research gap, manifesting in two ways: firstly, a gap in how to trust the digital identity representation of a physical human; secondly, a gap in how to trust identity providers that issue identity confirmations on-chain. Potential future research avenues are suggested to help fill the current gaps in establishing trust and on-chain identities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17276v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Awid Vaziry, Kaustabh Barman, Patrick Herbke</dc:creator>
    </item>
    <item>
      <title>Integrating Sustainability Concerns into Agile Software Development Process</title>
      <link>https://arxiv.org/abs/2407.17426</link>
      <description>arXiv:2407.17426v1 Announce Type: cross 
Abstract: Software has the potential to be a key driver in fostering sustainability. Despite this potential, it is not clear if and how the software industry integrates consideration of sustainability into its common software development processes. This research starts by investigating the current state of sustainability consideration within the software engineering industry through a survey. The results highlight a lack of progress in practically integrating sustainability considerations into software development activities. To address this gap, a case study with an industry partner is conducted to demonstrate how sustainability concerns and effects can be integrated into agile software development. The findings of this case study demonstrate practical approaches to integrating sustainability into software development practices. Reflecting on the findings from the survey and the case study, we note some insights on scaling up the adoption of sustainability consideration into the daily practice of agile software development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17426v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shola Oyedeji, Ruzanna Chitchyan, Mikhail Ola Adisa, Hatef Shamshiri</dc:creator>
    </item>
    <item>
      <title>BlueTempNet: A Temporal Multi-network Dataset of Social Interactions in Bluesky Social</title>
      <link>https://arxiv.org/abs/2407.17451</link>
      <description>arXiv:2407.17451v1 Announce Type: cross 
Abstract: Decentralized social media platforms like Bluesky Social (Bluesky) have made it possible to publicly disclose some user behaviors with millisecond-level precision. Embracing Bluesky's principles of open-source and open-data, we present the first collection of the temporal dynamics of user-driven social interactions. BlueTempNet integrates multiple types of networks into a single multi-network, including user-to-user interactions (following and blocking users) and user-to-community interactions (creating and joining communities). Communities are user-formed groups in custom Feeds, where users subscribe to posts aligned with their interests. Following Bluesky's public data policy, we collect existing Bluesky Feeds, including the users who liked and generated these Feeds, and provide tools to gather users' social interactions within a date range. This data-collection strategy captures past user behaviors and supports the future data collection of user behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17451v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ujun Jeong, Bohan Jiang, Zhen Tan, H. Russell Bernard, Huan Liu</dc:creator>
    </item>
    <item>
      <title>Hidden or Inferred: Fair Learning-To-Rank with Unknown Demographics</title>
      <link>https://arxiv.org/abs/2407.17459</link>
      <description>arXiv:2407.17459v1 Announce Type: cross 
Abstract: As learning-to-rank models are increasingly deployed for decision-making in areas with profound life implications, the FairML community has been developing fair learning-to-rank (LTR) models. These models rely on the availability of sensitive demographic features such as race or sex. However, in practice, regulatory obstacles and privacy concerns protect this data from collection and use. As a result, practitioners may either need to promote fairness despite the absence of these features or turn to demographic inference tools to attempt to infer them. Given that these tools are fallible, this paper aims to further understand how errors in demographic inference impact the fairness performance of popular fair LTR strategies. In which cases would it be better to keep such demographic attributes hidden from models versus infer them? We examine a spectrum of fair LTR strategies ranging from fair LTR with and without demographic features hidden versus inferred to fairness-unaware LTR followed by fair re-ranking. We conduct a controlled empirical investigation modeling different levels of inference errors by systematically perturbing the inferred sensitive attribute. We also perform three case studies with real-world datasets and popular open-source inference methods. Our findings reveal that as inference noise grows, LTR-based methods that incorporate fairness considerations into the learning process may increase bias. In contrast, fair re-ranking strategies are more robust to inference errors. All source code, data, and experimental artifacts of our experimental study are available here: https://github.com/sewen007/hoiltr.git</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17459v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Oluseun Olulana, Kathleen Cachel, Fabricio Murai, Elke Rundensteiner</dc:creator>
    </item>
    <item>
      <title>Privacy Perceptions and Behaviors of Google Personal Account Holders in Saudi Arabia</title>
      <link>https://arxiv.org/abs/2308.10148</link>
      <description>arXiv:2308.10148v4 Announce Type: replace 
Abstract: While privacy perceptions and behaviors have been investigated in Western societies, little is known about these issues in non-Western societies. To bridge this gap, we interviewed 30 Google personal account holders in Saudi Arabia about their privacy perceptions and behaviors regarding the activity data that Google saves about them. Our study focuses on Google's Activity Controls, which enable users to control whether, and how, Google saves their Web \&amp; App Activity, Location History, and YouTube History. Our results show that although most participants have some level of awareness about Google's data practices and the Activity Controls, many have only vague awareness, and the majority have not used the available controls. When participants viewed their saved activity data, many were surprised by what had been saved. While many participants find Google's use of their data to improve the services provided to them acceptable, the majority find the use of their data for ad purposes unacceptable. We observe that our Saudi participants exhibit similar trends and patterns in privacy awareness, attitudes, preferences, concerns, and behaviors to what has been found in studies in the US. Our results emphasize the need for: 1) improved techniques to inform users about privacy settings during account sign-up, to remind users about their settings, and to raise awareness about privacy settings; 2) improved privacy setting interfaces to reduce the costs that deter many users from changing the settings; and 3) further research to explore privacy concerns in non-Western cultures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10148v4</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eman Alashwali, Lorrie Faith Cranor</dc:creator>
    </item>
    <item>
      <title>How do machines learn? Evaluating the AIcon2abs method</title>
      <link>https://arxiv.org/abs/2401.07386</link>
      <description>arXiv:2401.07386v3 Announce Type: replace 
Abstract: This paper evaluates AI from concrete to Abstract (AIcon2abs), a recently proposed method that enables awareness among the general public on machine learning. Such is possible due to the use of WiSARD, an easily understandable machine learning mechanism, thus requiring little effort and no technical background from the target users. WiSARD is adherent to digital computing; training consists of writing to RAM-type memories, and classification consists of reading from these memories. The model enables easy visualization and understanding of training and classification tasks' internal realization through ludic activities. Furthermore, the WiSARD model does not require an Internet connection for training and classification, and it can learn from a few or one example. WiSARD can also create "mental images" of what it has learned so far, evidencing key features pertaining to a given class. The AIcon2abs method's effectiveness was assessed through the evaluation of a remote course with a workload of approximately 6 hours. It was completed by thirty-four Brazilian subjects: 5 children between 8 and 11 years old; 5 adolescents between 12 and 17 years old; and 24 adults between 21 and 72 years old. The collected data was analyzed from two perspectives: (i) from the perspective of a pre-experiment (of a mixed methods nature) and (ii) from a phenomenological perspective (of a qualitative nature). AIcon2abs was well-rated by almost 100% of the research subjects, and the data collected revealed quite satisfactory results concerning the intended outcomes. This research has been approved by the CEP/HUCFF/FM/UFRJ Human Research Ethics Committee.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07386v3</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rubens Lacerda Queiroz, Cabral Lima, Fabio Ferrentini Sampaio, Priscila Machado Vieira Lima</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap: Unravelling Local Government Data Sharing Barriers in Estonia and Beyond</title>
      <link>https://arxiv.org/abs/2406.08461</link>
      <description>arXiv:2406.08461v2 Announce Type: replace 
Abstract: Open Government Data (OGD) plays a crucial role in transforming smart cities into sustainable and intelligent entities by providing data for analytics, real-time monitoring, and informed decision-making. This data is increasingly used in urban digital twins, enhancing city management through stakeholder collaboration. However, local administrative data remains underutilized even in digitally advanced countries like Estonia. This study explores barriers preventing Estonian municipalities from sharing OGD, using a qualitative approach through interviews with Estonian municipalities and drawing on the OGD-adapted Innovation Resistance Theory model (IRT). Interviews with local government officials highlight ongoing is-sues in data provision and quality. By addressing overlooked weaknesses in the Estonian open data ecosystem and providing actionable recommendations, this research contributes to a more resilient and sustainable open data ecosystem. Additionally, by validating the OGD-adapted Innovation Resistance Theory model and proposing a revised version tailored for local government contexts, the study advances theoretical frameworks for understanding data sharing resistance. Ultimately, this study serves as a call to action for policymakers and practitioners to prioritize local OGD initiatives, ensuring the full utilization of OGD in smart city development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08461v2</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katrin Rajam\"ae Soosaar, Anastasija Nikiforova</dc:creator>
    </item>
    <item>
      <title>Do Generative AI Models Output Harm while Representing Non-Western Cultures: Evidence from A Community-Centered Approach</title>
      <link>https://arxiv.org/abs/2407.14779</link>
      <description>arXiv:2407.14779v2 Announce Type: replace 
Abstract: Our research investigates the impact of Generative Artificial Intelligence (GAI) models, specifically text-to-image generators (T2Is), on the representation of non-Western cultures, with a focus on Indian contexts. Despite the transformative potential of T2Is in content creation, concerns have arisen regarding biases that may lead to misrepresentations and marginalizations. Through a community-centered approach and grounded theory analysis of 5 focus groups from diverse Indian subcultures, we explore how T2I outputs to English prompts depict Indian culture and its subcultures, uncovering novel representational harms such as exoticism and cultural misappropriation. These findings highlight the urgent need for inclusive and culturally sensitive T2I systems. We propose design guidelines informed by a sociotechnical perspective, aiming to address these issues and contribute to the development of more equitable and representative GAI technologies globally. Our work also underscores the necessity of adopting a community-centered approach to comprehend the sociotechnical dynamics of these models, complementing existing work in this space while identifying and addressing the potential negative repercussions and harms that may arise when these models are deployed on a global scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14779v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sourojit Ghosh, Pranav Narayanan Venkit, Sanjana Gautam, Shomir Wilson, Aylin Caliskan</dc:creator>
    </item>
    <item>
      <title>Visual Stereotypes of Autism Spectrum in DALL-E, Stable Diffusion, SDXL, and Midjourney</title>
      <link>https://arxiv.org/abs/2407.16292</link>
      <description>arXiv:2407.16292v2 Announce Type: replace 
Abstract: Avoiding systemic discrimination requires investigating AI models' potential to propagate stereotypes resulting from the inherent biases of training datasets. Our study investigated how text-to-image models unintentionally perpetuate non-rational beliefs regarding autism. The research protocol involved generating images based on 53 prompts aimed at visualizing concrete objects and abstract concepts related to autism across four models: DALL-E, Stable Diffusion, SDXL, and Midjourney (N=249). Expert assessment of results was performed via a framework of 10 deductive codes representing common stereotypes contested by the community regarding their presence and spatial intensity, quantified on ordinal scales and subject to statistical analysis of inter-rater reliability and size effects. The models frequently utilised controversial themes and symbols which were unevenly distributed, however, with striking homogeneity in terms of skin colour, gender, and age, with autistic individuals portrayed as engaged in solitary activities, interacting with objects rather than people, and displaying stereotypical emotional expressions such as pale, anger, or sad. Secondly we observed representational insensitivity regarding autism images despite directional prompting aimed at falsifying the above results. Additionally, DALL-E explicitly denied perpetuating stereotypes. We interpret this as ANNs mirroring the human cognitive architecture regarding the discrepancy between background and reflective knowledge, as justified by our previous research on autism-related stereotypes in humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16292v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Wodzi\'nski, Marcin Rz\k{a}deczka, Anastazja Szu{\l}a, Marta Sok\'o{\l}, Marcin Moskalewicz</dc:creator>
    </item>
    <item>
      <title>A Finger on the Pulse of Cardiovascular Health: Estimating Blood Pressure with Smartphone Photoplethysmography-Based Pulse Waveform Analysis</title>
      <link>https://arxiv.org/abs/2401.11117</link>
      <description>arXiv:2401.11117v3 Announce Type: replace-cross 
Abstract: Utilizing mobile phone cameras for continuous blood pressure (BP) monitoring presents a cost-effective and accessible approach, yet it is challenged by limitations in accuracy and interpretability. This study introduces four innovative strategies to enhance smartphone-based photoplethysmography for BP estimation (SPW-BP), addressing the interpretability-accuracy dilemma. First, we employ often-neglected data-quality improvement techniques, such as height normalization, corrupt data removal, and boundary signal reconstruction. Second, we conduct a comprehensive analysis of thirty waveform indicators across three categories to identify the most predictive features. Third, we use SHapley Additive exPlanations (SHAP) analysis to ensure the transparency and explainability of machine learning outcomes. Fourth, we utilize Bland-Altman analysis alongside AAMI and BHS standards for comparative evaluation. Data from 127 participants demonstrated a significant correlation between smartphone-captured waveform features and those from standard BP monitoring devices. Employing multiple linear regression within a cross-validation framework, waveform variables predicted systolic blood pressure (SBP) with a mean absolute error (MAE) of 3.08-16.64 mmHg and diastolic blood pressure (DBP) with an MAE of 2.86-13.16 mmHg. Further application of Random Forest models significantly improved the prediction MAE for SBP to 2.61-15.21 mmHg and for DBP to 2.14-11.22 mmHg, indicating enhanced predictive accuracy. Correlation and SHAP analysis identified key features for improving BP estimation. However, Bland-Altman analysis revealed systematic biases, and MAE analysis showed that the results did not meet AAMI and BHS accuracy standards. Our findings highlight the potential of SPW-BP, yet suggest that smartphone PPG technology is not yet a viable alternative to traditional medical devices for BP measurement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11117v3</guid>
      <category>eess.SP</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan Liu, Fangyuan Liu, Qi Zhong, Shiguang Ni</dc:creator>
    </item>
    <item>
      <title>MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms</title>
      <link>https://arxiv.org/abs/2402.14154</link>
      <description>arXiv:2402.14154v2 Announce Type: replace-cross 
Abstract: Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to these challenges, yet they struggle to accurately interpret human emotions and complex content such as misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need for advancements in models' social understanding capabilities. Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks. However, MLLMs demonstrate performance improvements post fine-tuning, suggesting potential pathways for improvement. Our code and data are available at https://github.com/claws-lab/MMSoc.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14154v2</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, Srijan Kumar</dc:creator>
    </item>
    <item>
      <title>Tailoring Vaccine Messaging with Common-Ground Opinions</title>
      <link>https://arxiv.org/abs/2405.10861</link>
      <description>arXiv:2405.10861v2 Announce Type: replace-cross 
Abstract: One way to personalize chatbot interactions is by establishing common ground with the intended reader. A domain where establishing mutual understanding could be particularly impactful is vaccine concerns and misinformation. Vaccine interventions are forms of messaging which aim to answer concerns expressed about vaccination. Tailoring responses in this domain is difficult, since opinions often have seemingly little ideological overlap. We define the task of tailoring vaccine interventions to a Common-Ground Opinion (CGO). Tailoring responses to a CGO involves meaningfully improving the answer by relating it to an opinion or belief the reader holds. In this paper we introduce TAILOR-CGO, a dataset for evaluating how well responses are tailored to provided CGOs. We benchmark several major LLMs on this task; finding GPT-4-Turbo performs significantly better than others. We also build automatic evaluation metrics, including an efficient and accurate BERT model that outperforms finetuned LLMs, investigate how to successfully tailor vaccine messaging to CGOs, and provide actionable recommendations from this investigation.
  Code and model weights: https://github.com/rickardstureborg/tailor-cgo Dataset: https://huggingface.co/datasets/DukeNLP/tailor-cgo</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10861v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rickard Stureborg, Sanxing Chen, Ruoyu Xie, Aayushi Patel, Christopher Li, Chloe Qinyu Zhu, Tingnan Hu, Jun Yang, Bhuwan Dhingra</dc:creator>
    </item>
    <item>
      <title>What Do Privacy Advertisements Communicate to Consumers?</title>
      <link>https://arxiv.org/abs/2405.13857</link>
      <description>arXiv:2405.13857v3 Announce Type: replace-cross 
Abstract: When companies release marketing materials aimed at promoting their privacy practices or highlighting specific privacy features, what do they actually communicate to consumers? In this paper, we explore the impact of privacy marketing on: (1) consumers' attitudes toward the organizations providing the campaigns, (2) overall privacy awareness, and (3) the actionability of suggested privacy advice. To this end, we investigated the impact of four privacy advertising videos and one privacy game published by five different technology companies. We conducted 24 semi-structured interviews with participants randomly assigned to view one or two of the videos or play the game. Our findings suggest that awareness of privacy features can contribute to positive perceptions of a company or its products. The ads we tested were more successful in communicating the advertised privacy features than the game we tested. We observed that advertising a single privacy feature using a single metaphor in a short ad increased awareness of the advertised feature. The game failed to communicate privacy features or motivate study participants to use the features. Our results also suggest that privacy campaigns can be useful for raising awareness about privacy features and improving brand image, but may not be the most effective way to teach viewers how to use privacy features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13857v3</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoxin Shen, Eman Alashwali, Lorrie Faith Cranor</dc:creator>
    </item>
    <item>
      <title>Using Case Studies to Teach Responsible AI to Industry Practitioners</title>
      <link>https://arxiv.org/abs/2407.14686</link>
      <description>arXiv:2407.14686v2 Announce Type: replace-cross 
Abstract: Responsible AI (RAI) is the science and the practice of making the design, development, and use of AI socially sustainable: of reaping the benefits of innovation while controlling the risks. Naturally, industry practitioners play a decisive role in our collective ability to achieve the goals of RAI. Unfortunately, we do not yet have consolidated educational materials and effective methodologies for teaching RAI to practitioners. In this paper, we propose a novel stakeholder-first educational approach that uses interactive case studies to achieve organizational and practitioner -level engagement and advance learning of RAI. We discuss a partnership with Meta, an international technology company, to co-develop and deliver RAI workshops to a diverse audience within the company. Our assessment results indicate that participants found the workshops engaging and reported a positive shift in understanding and motivation to apply RAI to their work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14686v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Julia Stoyanovich, Rodrigo Kreis de Paula, Armanda Lewis, Chloe Zheng</dc:creator>
    </item>
  </channel>
</rss>
