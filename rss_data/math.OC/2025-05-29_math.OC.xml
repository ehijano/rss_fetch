<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 May 2025 01:43:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Support identification for parameter variations in a PDE system via regularized methods</title>
      <link>https://arxiv.org/abs/2505.21663</link>
      <description>arXiv:2505.21663v1 Announce Type: new 
Abstract: We study the inverse problem of recovering the spatial support of parameter variations in a system of partial differential equations (PDEs) from boundary measurements. A reconstruction method is developed based on the monotonicity properties of the Neumann-to-Dirichlet operator, which provides a theoretical foundation for stable support identification. To improve reconstruction accuracy, particularly when parameters have disjoint supports, we propose a combined regularization approach integrating monotonicity principles with Truncated Singular Value Decomposition (TSVD) regularization. This hybrid strategy enhances robustness against noise and ensures sharper support localization. Numerical experiments demonstrate the effectiveness of the proposed method, confirming its applicability in practical scenarios with varying parameter configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21663v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Houcine Meftahi, Chayma Nssibi</dc:creator>
    </item>
    <item>
      <title>Optimal dynamic thermal plant control: A study and benchmark</title>
      <link>https://arxiv.org/abs/2505.21679</link>
      <description>arXiv:2505.21679v1 Announce Type: new 
Abstract: District heating networks play a vital role in thermal energy supply in many countries. Thus, it comes to no surprise that these has been a central role in improving energy efficiency for private and public energy suppliers alike around the globe. Many studies have previously investigated the potential of energy saving by low temperature operation of the DHN and the integration of renewable energies. Many other studies consider this problem in terms of mixed integer lin-ear programming. Here, we instead investigate the utilization of well-established continuous optimization methods to improve DHN operation efficiency. We demonstrate that optimal control is able to model low temperature operation of a DHN for savings of around 8%, but can even further improve its operation when considering dynamic energy pricing, reducing the cost of operation by roughly 12%. We demonstrate the applicability of this method in a realistic, openly available network in Switzerland (OpenDHN), with a total runtime of less than 5 minutes on a standard desktop com-puter per experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21679v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Grandits, Stefano Coss, Gundolf Haase</dc:creator>
    </item>
    <item>
      <title>What Data Enables Optimal Decisions? An Exact Characterization for Linear Optimization</title>
      <link>https://arxiv.org/abs/2505.21692</link>
      <description>arXiv:2505.21692v1 Announce Type: new 
Abstract: We study the fundamental question of how informative a dataset is for solving a given decision-making task. In our setting, the dataset provides partial information about unknown parameters that influence task outcomes. Focusing on linear programs, we characterize when a dataset is sufficient to recover an optimal decision, given an uncertainty set on the cost vector. Our main contribution is a sharp geometric characterization that identifies the directions of the cost vector that matter for optimality, relative to the task constraints and uncertainty set. We further develop a practical algorithm that, for a given task, constructs a minimal or least-costly sufficient dataset. Our results reveal that small, well-chosen datasets can often fully determine optimal decisions -- offering a principled foundation for task-aware data selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21692v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Bennouna, Amine Bennouna, Saurabh Amin, Asuman Ozdaglar</dc:creator>
    </item>
    <item>
      <title>Preconditioning transformations of adjoint systems for evolution equations</title>
      <link>https://arxiv.org/abs/2505.21705</link>
      <description>arXiv:2505.21705v1 Announce Type: new 
Abstract: Achieving robust control and optimization in high-fidelity physics simulations is extremely challenging, especially for evolutionary systems whose solutions span vast scales across space, time, and physical variables. In conjunction with gradient-based methods, adjoint systems are widely used in the optimization of systems subject to differential equation constraints. In optimization, gradient-based methods are often transformed using suitable preconditioners to accelerate the convergence of the optimization algorithm. Inspired by preconditioned gradient descent methods, we introduce a framework for the preconditioning of adjoint systems associated to evolution equations, which allows one to reshape the dynamics of the adjoint system. We develop two classes of adjoint preconditioning transformations: those that transform both the state dynamics and the adjoint equation and those that transform only the adjoint equation while leaving the state dynamics invariant. Both classes of transformations have the flexibility to include generally nonlinear state-dependent transformations. Using techniques from symplectic geometry and Hamiltonian mechanics, we further show that these preconditioned adjoint systems preserve the property that the adjoint system backpropagates the derivative of an objective function. We then apply this framework to the setting of coupled evolution equations, where we develop a notion of scale preconditioning of the adjoint equations when the state dynamics exhibit large scale-separation. We demonstrate the proposed scale preconditioning on an inverse problem for the radiation diffusion equations. Naive gradient descent is unstable for any practical gradient descent step size, whereas our proposed scale-preconditioned adjoint descent converges in 10-15 gradient-based optimization iterations, with highly accurate reproduction of the wavefront at the final time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21705v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian K. Tran, Ben S. Southworth, Hannah F. Blumhoefer, Samuel Olivier</dc:creator>
    </item>
    <item>
      <title>Assessing EV Charging Impacts on Power Distribution Systems: A Unified Co-Simulation Framework</title>
      <link>https://arxiv.org/abs/2505.21773</link>
      <description>arXiv:2505.21773v1 Announce Type: new 
Abstract: The growing adoption of electric vehicles (EVs) is expected to significantly increase demand on electric power distribution systems, many of which are already nearing capacity. To address this, the paper presents a comprehensive framework for analyzing the impact of large-scale EV integration on distribution networks. Using the open-source simulator OpenDSS, the framework builds detailed, scalable models of electric distribution systems, incorporating high-fidelity synthetic data from the SMART-DS project. The study models three feeders from an urban substation in San Francisco down to the household level. A key contribution is the framework's ability to identify critical system components likely to require upgrades due to increased EV loads. It also incorporates advanced geospatial visualization through QGIS, which aids in understanding how charging demands affect specific grid areas, helping stakeholders target infrastructure reinforcements. To ensure realistic load modeling, the framework uses EV load profiles based on U.S. Department of Energy projections, factoring in vehicle types, charging behaviors, usage patterns, and adoption rates. By leveraging large-scale synthetic data, the model remains relevant for real-world utility planning. It supports diverse simulation scenarios, from light to heavy EV charging loads and distributed vs. centralized charging patterns, offering a practical planning tool for utilities and policymakers. Additionally, its modular design enables easy adaptation to different geographic regions, feeder setups, and adoption scenarios, making it suitable for future studies on evolving grid conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21773v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Iranpour, Mohammad Rasoul Narimani, Xudong Jia</dc:creator>
    </item>
    <item>
      <title>Optimal Pricing Strategies for Heterogeneous Customers in Dual-Channel Closed-Loop Supply Chains: A Modeling Approach</title>
      <link>https://arxiv.org/abs/2505.21787</link>
      <description>arXiv:2505.21787v1 Announce Type: new 
Abstract: Dual-channel closed-loop supply chains (DCCLSCs) play a vital role in attaining both sustainability and profitability. This paper introduces a game-theoretic model to analyze optimal pricing strategies for primary and replacement customers within three distinct recycling frameworks: manufacturer-led, retailer-led, and collaborative recycling. The model identifies equilibrium pricing and subsidy decisions for each scenario, considering the primary customer's preference for the direct channel and the specific roles in recycling. The findings indicate that manufacturers tend to set lower prices in direct channels compared to retailers, aiming to stimulate demand and promote trade-ins. Manufacturer-led recycling initiatives result in stable pricing, whereas retailer-led recycling necessitates higher subsidies. Collaborative recycling strategies yield lower prices and an increase in trade-ins. Primary customers' preference for the direct channel significantly impacts pricing strategies, with a stronger preference leading to lower direct-channel prices and higher manufacturer subsidies. This paper contributes to the field by incorporating primary customer channel preferences and diverse recycling frameworks into DCCLSC pricing models. These insights assist manufacturers and retailers in adjusting pricing strategies and trade-in incentives according to primary customer preferences and associated costs, thereby enhancing profitability and recycling efficiency within DCCLSCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21787v1</guid>
      <category>math.OC</category>
      <category>econ.TH</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Xiao, Hisashi Kurata, Ting Wang</dc:creator>
    </item>
    <item>
      <title>PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective</title>
      <link>https://arxiv.org/abs/2505.21799</link>
      <description>arXiv:2505.21799v1 Announce Type: new 
Abstract: The ever-growing scale of deep learning models and datasets underscores the critical importance of efficient optimization methods. While preconditioned gradient methods such as Adam and AdamW are the de facto optimizers for training neural networks and large language models, structure-aware preconditioned optimizers like Shampoo and Muon, which utilize the matrix structure of gradients, have demonstrated promising evidence of faster convergence. In this paper, we introduce a unifying framework for analyzing "matrix-aware" preconditioned methods, which not only sheds light on the effectiveness of Muon and related optimizers but also leads to a class of new structure-aware preconditioned methods. A key contribution of this framework is its precise distinction between preconditioning strategies that treat neural network weights as vectors (addressing curvature anisotropy) versus those that consider their matrix structure (addressing gradient anisotropy). This perspective provides new insights into several empirical phenomena in language model pre-training, including Adam's training instabilities, Muon's accelerated convergence, and the necessity of learning rate warmup for Adam. Building upon this framework, we introduce PolarGrad, a new class of preconditioned optimization methods based on the polar decomposition of matrix-valued gradients. As a special instance, PolarGrad includes Muon with updates scaled by the nuclear norm of the gradients. We provide numerical implementations of these methods, leveraging efficient numerical polar decomposition algorithms for enhanced convergence. Our extensive evaluations across diverse matrix optimization problems and language model pre-training tasks demonstrate that PolarGrad outperforms both Adam and Muon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21799v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Tsz-Kit Lau, Qi Long, Weijie Su</dc:creator>
    </item>
    <item>
      <title>A Hybrid Subgradient Method for Nonsmooth Nonconvex Bilevel Optimization</title>
      <link>https://arxiv.org/abs/2505.22040</link>
      <description>arXiv:2505.22040v1 Announce Type: new 
Abstract: In this paper, we focus on the nonconvex-nonconvex bilevel optimization problem (BLO), where both upper-level and lower-level objectives are nonconvex, with the upper-level problem potentially being nonsmooth. We develop a two-timescale momentum-accelerated subgradient method (TMG) that employs two-timescale stepsizes, and establish its local convergence when initialized within a sufficiently small neighborhood of the feasible region. To develop a globally convergent algorithm for (BLO), we introduce a feasibility restoration scheme (FRG) that drives iterates toward the feasible region. Both (TMG) and (FRG) only require the first-order derivatives of the upper-level and lower-level objective functions, ensuring efficient computations in practice. We then develop a novel hybrid method that alternates between (TMG) and (FRG) and adaptively estimates its hyperparameters. Under mild conditions, we establish the global convergence properties of our proposed algorithm. Preliminary numerical experiments demonstrate the high efficiency and promising potential of our proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22040v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nachuan Xiao, Xiaoyin Hu, Xin Liu, Kim-Chuan Toh</dc:creator>
    </item>
    <item>
      <title>Data-Driven Adjustable Robust Optimization</title>
      <link>https://arxiv.org/abs/2505.22075</link>
      <description>arXiv:2505.22075v1 Announce Type: new 
Abstract: In this paper, we develop a two-stage data-driven approach to address the adjustable robust optimization problem, where the uncertainty set is adjustable to manage infeasibility caused by significant or poorly quantified uncertainties. In the first stage, we synthesize an uncertainty set to ensure the feasibility of the problem as much as possible using the collected uncertainty samples. In the second stage, we find the optimal solution while ensuring that the constraints are satisfied under the new uncertainty set. This approach enlarges the feasible state set, at the expense of the risk of possible constraint violation. We analyze two scenarios: one where the uncertainty is non-stochastic, and another where the uncertainty is stochastic but with unknown probability distribution, leading to a distributionally robust optimization problem. In the first case, we scale the uncertainty set and find the best subset that fits the uncertainty samples. In the second case, we employ the Wasserstein metric to quantify uncertainty based on training data, and for polytope uncertainty sets, we further provide a finite program reformulation of the problem. The effectiveness of the proposed methods is demonstrated through an optimal power flow problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22075v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoxing Ren, Alessio Moreschini, Zhongda Chu, Yulong Gao, Thomas Parisini</dc:creator>
    </item>
    <item>
      <title>PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning</title>
      <link>https://arxiv.org/abs/2505.22085</link>
      <description>arXiv:2505.22085v1 Announce Type: new 
Abstract: Averaging techniques such as Ruppert--Polyak averaging and exponential movering averaging (EMA) are powerful approaches to accelerate optimization procedures of stochastic gradient descent (SGD) optimization methods such as the popular ADAM optimizer. However, depending on the specific optimization problem under consideration, the type and the parameters for the averaging need to be adjusted to achieve the smallest optimization error. In this work we propose an averaging approach, which we refer to as parallel averaged ADAM (PADAM), in which we compute parallely different averaged variants of ADAM and during the training process dynamically select the variant with the smallest optimization error. A central feature of this approach is that this procedure requires no more gradient evaluations than the usual ADAM optimizer as each of the averaged trajectories relies on the same underlying ADAM trajectory and thus on the same underlying gradients. We test the proposed PADAM optimizer in 13 stochastic optimization and deep neural network (DNN) learning problems and compare its performance with known optimizers from the literature such as standard SGD, momentum SGD, Adam with and without EMA, and ADAMW. In particular, we apply the compared optimizers to physics-informed neural network, deep Galerkin, deep backward stochastic differential equation and deep Kolmogorov approximations for boundary value partial differential equation problems from scientific machine learning, as well as to DNN approximations for optimal control and optimal stopping problems. In nearly all of the considered examples PADAM achieves, sometimes among others and sometimes exclusively, essentially the smallest optimization error. This work thus strongly suggest to consider PADAM for scientific machine learning problems and also motivates further research for adaptive averaging procedures within the training of DNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22085v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnulf Jentzen, Julian Kranz, Adrian Riekert</dc:creator>
    </item>
    <item>
      <title>A Nurse Staffing and Scheduling Problem with Bounded Flexibility and Demand Uncertainty</title>
      <link>https://arxiv.org/abs/2505.22124</link>
      <description>arXiv:2505.22124v1 Announce Type: new 
Abstract: Nurse staffing and scheduling are persistent challenges in healthcare due to demand fluctuations and individual nurse preferences. This study introduces the concept of bounded flexibility, balancing nurse satisfaction with strict rostering rules, particularly a real-world time regularity policy from a major hospital in Singapore. We model the problem as a multi-stage stochastic program to address evolving demand, optimizing both aggregate staffing and detailed scheduling decisions. A reformulation into a two-stage structure using block-separable recourse reduces computational burden without loss of accuracy. To solve the problem efficiently, we develop a Generative AI-guided algorithm. Numerical experiments with real hospital data show substantial cost savings and improved nurse flexibility with minimal compromise to schedule regularity. Numerical experiments based on real-world nurse profiles, nurse preferences, and patient demand data are conducted to evaluate the performance of the proposed methods. Our results demonstrate that the stochastic model achieves significant cost savings compared to the deterministic model. Notably, a slight reduction in the regularity level can remarkably enhance nurse flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22124v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Si Zhang, Paul Mingzheng Tang, Hoong Chuin Lau</dc:creator>
    </item>
    <item>
      <title>Some iterative algorithms on Riemannian manifolds and Banach spaces with good global convergence guarantee</title>
      <link>https://arxiv.org/abs/2505.22180</link>
      <description>arXiv:2505.22180v1 Announce Type: new 
Abstract: In this paper, we introduce some new iterative optimisation algorithms on Riemannian manifolds and Hilbert spaces which have good global convergence guarantees to local minima. More precisely, these algorithms have the following properties: If $\{x_n\}$ is a sequence constructed by one such algorithm then:
  - Finding critical points: Any cluster point of $\{x_n\}$ is a critical point of the cost function $f$.
  - Convergence guarantee: Under suitable assumptions, the sequence $\{x_n\}$ either converges to a point $x^*$, or diverges to $\infty$.
  - Avoidance of saddle points: If $x_0$ is randomly chosen, then the sequence $\{x_n\}$ cannot converge to a saddle point.
  Our results apply for quite general situations: the cost function $f$ is assumed to be only $C^2$ or $C^3$, and either $f$ has at most countably many critical points (which is a generic situation) or satisfies certain Lojasiewicz gradient inequalities. To illustrate the results, we provide a nice application with optimisation over the unit sphere in a Euclidean space.
  As for tools needed for the results, in the Riemannian manifold case we introduce a notion of "strong local retraction" and (to deal with Newton's method type) a notion of "real analytic-like strong local retraction". In the case of Banach spaces, we introduce a slight generalisation of the notion of "shyness", and design a new variant of Backtracking New Q-Newton's method which is more suitable to the infinite dimensional setting (and in the Euclidean setting is simpler than the current versions).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22180v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tuyen Trung Truong</dc:creator>
    </item>
    <item>
      <title>An Exact System Optimum Assignment Model for Transit Demand Management</title>
      <link>https://arxiv.org/abs/2505.22241</link>
      <description>arXiv:2505.22241v1 Announce Type: new 
Abstract: Mass transit systems are experiencing increasing congestion in many cities. The schedule-based transit assignment problem (STAP) involves a joint choice model for departure times and routes, defining a space-time path in which passengers decide when to depart and which route to take. User equilibrium (UE) models for the STAP indicates the current congestion cost, while a system optimum (SO) models can provide insights for congestion relief directions. However, current STAP methods rely on approximate SO (Approx. SO) models, which underestimate the potential for congestion reduction in the system. The few studies in STAP that compute exact SO solutions ignore realistic constraints such as hard capacity, multi-line networks, or spatial-temporal competing demand flows. The paper proposes an exact SO method for the STAP that overcomes these limitations. We apply our approach to a case study involving part of the Hong Kong Mass Transit Railway network, which includes 5 lines, 12 interacting origin-destination pairs and 52,717 passengers. Computing an Approx. SO solution for this system indicates a modest potential for congestion reduction measures, with a cost reduction of 17.39% from the UE solution. Our exact SO solution is 36.35% lower than the UE solution, which is more than double the potential for congestion reduction. We then show how the exact SO solution can be used to identify opportunities for congestion reduction: (i) which origin-destination pairs have the most potential to reduce congestion; (ii) how many passengers can be reasonably shifted; (iii) future system potential with increasing demand and expanding network capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22241v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xia Zhou, Mark Wallace, Daniel D. Harabor, Zhenliang Ma</dc:creator>
    </item>
    <item>
      <title>Optimizing Server Locations for Stochastic Emergency Service Systems</title>
      <link>https://arxiv.org/abs/2505.22249</link>
      <description>arXiv:2505.22249v1 Announce Type: new 
Abstract: This paper presents a new model for solving the optimal server location problem in a stochastic system that accounts for unit availability, heterogeneity, and interdependencies. We show that this problem is NP-hard and derive both lower and upper bounds for the optimal solution by leveraging a special case of the classic $p$-Median problem. To overcome the computational challenges, we propose two Bayesian optimization approaches: (i) a parametric method that employs a sparse Bayesian linear model with a horseshoe prior (SparBL), and (ii) a non-parametric method based on a Gaussian process surrogate model with $p$-Median as mean prior (GP-$p$M). We prove that both algorithms achieve sublinear regret rates and converge to the optimal solution, with the parametric approach demonstrating particular effectiveness in high-dimensional settings. Numerical experiments and a case study using real-world data from St. Paul, Minnesota emergency response system show that our approaches consistently and efficiently identify optimal solutions, significantly outperforming the $p$-Median solution and other baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22249v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Hua, Arthur J. Swersey, Wenqian Xing, Yi Zhang</dc:creator>
    </item>
    <item>
      <title>Quantitative regularity properties for the optimal design problem</title>
      <link>https://arxiv.org/abs/2505.22365</link>
      <description>arXiv:2505.22365v1 Announce Type: new 
Abstract: In this paper we slightly improve the regularity theory for the so called optimal design problem. We first establish the uniform rectifiability of the boundary of the optimal set, for a larger class of minimizers, in any dimension. As an application, we improve the bound obtained by Larsen in dimension~2 about the mutual distance between two connected components. Finally we also prove that the full regularity in dimension 2 holds true provided that the ratio between the two constants in front of the Dirichlet energy is not larger than 4, which partially answers to a question raised by Larsen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22365v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lorenzo Lamberti, Antoine Lemenant</dc:creator>
    </item>
    <item>
      <title>Learning to Pursue AC Optimal Power Flow Solutions with Feasibility Guarantees</title>
      <link>https://arxiv.org/abs/2505.22399</link>
      <description>arXiv:2505.22399v1 Announce Type: new 
Abstract: This paper focuses on an AC optimal power flow (OPF) problem for distribution feeders equipped with controllable distributed energy resources (DERs). We consider a solution method that is based on a continuous approximation of the projected gradient flow - referred to as the safe gradient flow - that incorporates voltage and current information obtained either through real-time measurements or power flow computations. These two setups enable both online and offline implementations. The safe gradient flow involves the solution of convex quadratic programs (QPs). To enhance computational efficiency, we propose a novel framework that employs a neural network approximation of the optimal solution map of the QP. The resulting method has two key features: (a) it ensures that the DERs' setpoints are practically feasible, even for an online implementation or when an offline algorithm has an early termination; (b) it ensures convergence to a neighborhood of a strict local optimizer of the AC OPF. The proposed method is tested on a 93-node distribution system with realistic loads and renewable generation. The test shows that our method successfully regulates voltages within limits during periods with high renewable generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22399v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damola Ajeyemi, Yiting Chen, Antonin Colot, Jorge Cortes, Emiliano Dall'Anese</dc:creator>
    </item>
    <item>
      <title>Continuity and approximability of competitive spectral radii</title>
      <link>https://arxiv.org/abs/2505.22468</link>
      <description>arXiv:2505.22468v1 Announce Type: new 
Abstract: The competitive spectral radius extends the notion of joint spectral radius to the two-player case: two players alternatively select matrices in prescribed compact sets, resulting in an infinite matrix product; one player wishes to maximize the growth rate of this product, whereas the other player wishes to minimize it. We show that when the matrices represent linear operators preserving a cone and satisfying a "strict positivity" assumption, the competitive spectral radius depends continuously - and even in a Lipschitz-continuous way - on the matrix sets. Moreover, we show that the competive spectral radius can be approximated up to any accuracy. This relies on the solution of a discretized infinite dimensional non-linear eigenproblem. We illustrate the approach with an example of age-structured population dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22468v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marianne Akian, St\'ephane Gaubert, Lo\"ic Marchesini, Ian Morris</dc:creator>
    </item>
    <item>
      <title>Image denoising as a conditional expectation</title>
      <link>https://arxiv.org/abs/2505.21546</link>
      <description>arXiv:2505.21546v1 Announce Type: cross 
Abstract: All techniques for denoising involve a notion of a true (noise-free) image, and a hypothesis space. The hypothesis space may reconstruct the image directly as a grayscale valued function, or indirectly by its Fourier or wavelet spectrum. Most common techniques estimate the true image as a projection to some subspace. We propose an interpretation of a noisy image as a collection of samples drawn from a certain probability space. Within this interpretation, projection based approaches are not guaranteed to be unbiased and convergent. We present a data-driven denoising method in which the true image is recovered as a conditional expectation. Although the probability space is unknown apriori, integrals on this space can be estimated by kernel integral operators. The true image is reformulated as the least squares solution to a linear equation in a reproducing kernel Hilbert space (RKHS), and involving various kernel integral operators as linear transforms. Assuming the true image to be a continuous function on a compact planar domain, the technique is shown to be convergent as the number of pixels goes to infinity. We also show that for a picture with finite number of pixels, the convergence result can be used to choose the various parameters for an optimum denoising result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21546v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sajal Chakroborty, Suddhasattwa Das</dc:creator>
    </item>
    <item>
      <title>Learning Where to Learn: Training Distribution Selection for Provable OOD Performance</title>
      <link>https://arxiv.org/abs/2505.21626</link>
      <description>arXiv:2505.21626v1 Announce Type: cross 
Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in machine learning. Models trained on one data distribution often experience substantial performance degradation when evaluated on shifted or unseen domains. To address this challenge, the present paper studies the design of training data distributions that maximize average-case OOD performance. First, a theoretical analysis establishes a family of generalization bounds that quantify how the choice of training distribution influences OOD error across a predefined family of target distributions. These insights motivate the introduction of two complementary algorithmic strategies: (i) directly formulating OOD risk minimization as a bilevel optimization problem over the space of probability measures and (ii) minimizing a theoretical upper bound on OOD error. Last, the paper evaluates the two approaches across a range of function approximation and operator learning examples. The proposed methods significantly improve OOD accuracy over standard empirical risk minimization with a fixed distribution. These results highlight the potential of distribution-aware training as a principled and practical framework for robust OOD generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21626v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Guerra, Nicholas H. Nelsen, Yunan Yang</dc:creator>
    </item>
    <item>
      <title>Apprenticeship learning with prior beliefs using inverse optimization</title>
      <link>https://arxiv.org/abs/2505.21639</link>
      <description>arXiv:2505.21639v1 Announce Type: cross 
Abstract: The relationship between inverse reinforcement learning (IRL) and inverse optimization (IO) for Markov decision processes (MDPs) has been relatively underexplored in the literature, despite addressing the same problem. In this work, we revisit the relationship between the IO framework for MDPs, IRL, and apprenticeship learning (AL). We incorporate prior beliefs on the structure of the cost function into the IRL and AL problems, and demonstrate that the convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a relaxation of our framework. Notably, the AL formalism is a special case in our framework when the regularization term is absent. Focusing on the suboptimal expert setting, we formulate the AL problem as a regularized min-max problem. The regularizer plays a key role in addressing the ill-posedness of IRL by guiding the search for plausible cost functions. To solve the resulting regularized-convex-concave-min-max problem, we use stochastic mirror descent (SMD) and establish convergence bounds for the proposed method. Numerical experiments highlight the critical role of regularization in learning cost vectors and apprentice policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21639v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauricio Junca, Esteban Leiva</dc:creator>
    </item>
    <item>
      <title>AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent</title>
      <link>https://arxiv.org/abs/2505.21651</link>
      <description>arXiv:2505.21651v1 Announce Type: cross 
Abstract: The learning rate is an important tuning parameter for stochastic gradient descent (SGD) and can greatly influence its performance. However, appropriate selection of a learning rate schedule across all iterations typically requires a non-trivial amount of user tuning effort. To address this, we introduce AutoSGD: an SGD method that automatically determines whether to increase or decrease the learning rate at a given iteration and then takes appropriate action. We introduce theory supporting the convergence of AutoSGD, along with its deterministic counterpart for standard gradient descent. Empirical results suggest strong performance of the method on a variety of traditional optimization problems and machine learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21651v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikola Surjanovic, Alexandre Bouchard-C\^ot\'e, Trevor Campbell</dc:creator>
    </item>
    <item>
      <title>Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing</title>
      <link>https://arxiv.org/abs/2505.21671</link>
      <description>arXiv:2505.21671v1 Announce Type: cross 
Abstract: We study a sequential decision-making problem on a $n$-node graph $G$ where each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from a joint distribution $P$ that is Markov with respect to $G$. At each step, selecting a node reveals its label and yields a label-dependent reward. The goal is to adaptively choose nodes to maximize expected accumulated discounted rewards. We impose a frontier exploration constraint, where actions are limited to neighbors of previously selected nodes, reflecting practical constraints in settings such as contact tracing and robotic exploration. We design a Gittins index-based policy that applies to general graphs and is provably optimal when $G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$ time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and $O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world graphs show that our method consistently outperforms natural baselines, including in non-tree, budget-limited, and undiscounted settings. For example, in HIV testing simulations on real-world sexual interaction networks, our policy detects nearly all positive cases with only half the population tested, substantially outperforming other baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21671v1</guid>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davin Choo, Yuqi Pan, Tonghan Wang, Milind Tambe, Alastair van Heerden, Cheryl Johnson</dc:creator>
    </item>
    <item>
      <title>Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference</title>
      <link>https://arxiv.org/abs/2505.21721</link>
      <description>arXiv:2505.21721v1 Announce Type: cross 
Abstract: We prove that, given a mean-field location-scale variational family, black-box variational inference (BBVI) with the reparametrization gradient converges at an almost dimension-independent rate. Specifically, for strongly log-concave and log-smooth targets, the number of iterations for BBVI with a sub-Gaussian family to achieve an objective $\epsilon$-close to the global optimum is $\mathrm{O}(\log d)$, which improves over the $\mathrm{O}(d)$ dependence of full-rank location-scale families. For heavy-tailed families, we provide a weaker $\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the number of finite moments. Additionally, if the Hessian of the target log-density is constant, the complexity is free of any explicit dimension dependence. We also prove that our bound on the gradient variance, which is key to our result, cannot be improved using only spectral bounds on the Hessian of the target log-density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21721v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyurae Kim, Yi-An Ma, Trevor Campbell, Jacob R. Gardner</dc:creator>
    </item>
    <item>
      <title>DualSchool: How Reliable are LLMs for Optimization Education?</title>
      <link>https://arxiv.org/abs/2505.21775</link>
      <description>arXiv:2505.21775v1 Announce Type: cross 
Abstract: Consider the following task taught in introductory optimization courses which addresses challenges articulated by the community at the intersection of (generative) AI and OR: generate the dual of a linear program. LLMs, being trained at web-scale, have the conversion process and many instances of Primal to Dual Conversion (P2DC) at their disposal. Students may thus reasonably expect that LLMs would perform well on the P2DC task. To assess this expectation, this paper introduces DualSchool, a comprehensive framework for generating and verifying P2DC instances. The verification procedure of DualSchool uses the Canonical Graph Edit Distance, going well beyond existing evaluation methods for optimization models, which exhibit many false positives and negatives when applied to P2DC. Experiments performed by DualSchool reveal interesting findings. Although LLMs can recite the conversion procedure accurately, state-of-the-art open LLMs fail to consistently produce correct duals. This finding holds even for the smallest two-variable instances and for derivative tasks, such as correctness, verification, and error classification. The paper also discusses the implications for educators, students, and the development of large reasoning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21775v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Klamkin, Arnaud Deza, Sikai Cheng, Haoruo Zhao, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain Systems</title>
      <link>https://arxiv.org/abs/2505.21838</link>
      <description>arXiv:2505.21838v1 Announce Type: cross 
Abstract: This paper investigates the robust output regulation problem of second-order nonlinear uncertain systems with an unknown exosystem. Instead of the adaptive control approach, this paper resorts to a robust control methodology to solve the problem and thus avoid the bursting phenomenon. In particular, this paper constructs generic internal models for the steady-state state and input variables of the system. By introducing a coordinate transformation, this paper converts the robust output regulation problem into a nonadaptive stabilization problem of an augmented system composed of the second-order nonlinear uncertain system and the generic internal models. Then, we design the stabilization control law and construct a strict Lyapunov function that guarantees the robustness with respect to unmodeled disturbances. The analysis shows that the output zeroing manifold of the augmented system can be made attractive by the proposed nonadaptive control law, which solves the robust output regulation problem. Finally, we demonstrate the effectiveness of the proposed nonadaptive internal model approach by its application to the control of the Duffing system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21838v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>nlin.CD</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maobin Lu, Martin Guay, Telema Harry, Shimin Wang, Jordan Cooper</dc:creator>
    </item>
    <item>
      <title>Online distributed optimization for spatio-temporally constrained real-time peer-to-peer energy trading</title>
      <link>https://arxiv.org/abs/2505.21884</link>
      <description>arXiv:2505.21884v1 Announce Type: cross 
Abstract: The proliferation of distributed renewable energy triggers the peer-to-peer (P2P) energy market formations. To make profits, prosumers equipped with photovoltaic (PV) panels and even the energy storage system (ESS) can actively participate in the real-time P2P energy market and trade energy. However, in real situations, system states such as energy demands and renewable energy power generation are highly uncertain, making it difficult for prosumers to make optimal real-time decisions. Moreover, severe problems with the physical network can arise from the real-time P2P energy trading, such as bus voltage violations and line overload. To handle these problems, this work first formulates the real-time P2P energy trading problem as a spatio-temporally constrained stochastic optimization problem by considering ESS and the spatial physical network constraints. To deal with the uncertainties online, a modified Lyapunov optimization method is innovatively proposed to approximately reformulate the stochastic optimization problem into an online one by relaxing the time-coupling constraints. Compared with the state-of-the-art online methods, the proposed one renders more flexibility and better performance for the real-time P2P energy market operation. Additionally, to protect the prosumers' privacy, an online distributed algorithm based on the consensus alternating direction method of multipliers (ADMM) is developed to solve the reformulated online problem by decoupling the spatial constraints. The theoretical near-optimal performance guarantee of the proposed online distributed algorithm is derived, and its performance can be further improved by minimizing the performance gap. Simulation results demonstrate that the proposed online distributed algorithm can guarantee the fast, stable, and safe long-term operation of the real-time P2P energy market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21884v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.apenergy.2022.120216</arxiv:DOI>
      <arxiv:journal_reference>Applied Energy 331 (2023): 120216</arxiv:journal_reference>
      <dc:creator>Junhong Liu, Qinfei Long, Rong-Peng Liu, Wenjie Liu, Yunhe Hou</dc:creator>
    </item>
    <item>
      <title>Higher-Order Group Synchronization</title>
      <link>https://arxiv.org/abs/2505.21932</link>
      <description>arXiv:2505.21932v1 Announce Type: cross 
Abstract: Group synchronization is the problem of determining reliable global estimates from noisy local measurements on networks. The typical task for group synchronization is to assign elements of a group to the nodes of a graph in a way that respects group elements given on the edges which encode information about local pairwise relationships between the nodes. In this paper, we introduce a novel higher-order group synchronization problem which operates on a hypergraph and seeks to synchronize higher-order local measurements on the hyperedges to obtain global estimates on the nodes. Higher-order group synchronization is motivated by applications to computer vision and image processing, among other computational problems. First, we define the problem of higher-order group synchronization and discuss its mathematical foundations. Specifically, we give necessary and sufficient synchronizability conditions which establish the importance of cycle consistency in higher-order group synchronization. Then, we propose the first computational framework for general higher-order group synchronization; it acts globally and directly on higher-order measurements using a message passing algorithm. We discuss theoretical guarantees for our framework, including convergence analyses under outliers and noise. Finally, we show potential advantages of our method through numerical experiments. In particular, we show that in certain cases our higher-order method applied to rotational and angular synchronization outperforms standard pairwise synchronization methods and is more robust to outliers. We also show that our method has comparable performance on simulated cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM reconstruction package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21932v1</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adriana L. Duncan, Joe Kileel</dc:creator>
    </item>
    <item>
      <title>Physical Reduced Stochastic Equations for Continuously Monitored Non-Markovian Quantum Systems with a Markovian Embedding</title>
      <link>https://arxiv.org/abs/2505.22070</link>
      <description>arXiv:2505.22070v1 Announce Type: cross 
Abstract: An effective approach to modeling non-Markovian quantum systems is to embed a principal (quantum) system of interest into a larger quantum system. A widely employed embedding is one that uses another quantum system, referred to as the auxiliary system, which is coupled to the principal system, and both the principal and auxiliary can be coupled to quantum white noise processes. The principal and auxiliary together form a quantum Markov system and the quantum white noises act as a bath (environment) for this system.
  Recently it was shown that the conditional evolution of the principal system in this embedding under continuous monitoring by a travelling quantum probe can be expressed as a system of coupled stochastic differential equations (SDEs) that involve only operators of the principal system. The reduced conditional state of the principal only (conditioned on the measurement outcomes) are determined by the "diagonal" blocks of this coupled systems of SDEs. It is shown here that the "off-diagonal" blocks can be exactly eliminated up to their initial conditions, leaving a reduced closed system of SDEs for the diagonal blocks only. Under additional conditions the off-diagonal initial conditions can be made to vanish. This new closed system of equations, which includes an integration term involving a two-time stochastic kernel, represents the non-Markovian stochastic dynamics of the principal system under continuous-measurement. The system of equations determine the reduced conditional state of the principal only and may be viewed as a stochastic Nakajima-Zwanzig type of equation for continuously monitored non-Markovian quantum systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22070v1</guid>
      <category>quant-ph</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hendra I. Nurdin</dc:creator>
    </item>
    <item>
      <title>(Near)-Optimal Algorithms for Sparse Separable Convex Integer Programs</title>
      <link>https://arxiv.org/abs/2505.22212</link>
      <description>arXiv:2505.22212v1 Announce Type: cross 
Abstract: We study the general integer programming (IP) problem of optimizing a separable convex function over the integer points of a polytope: $\min \{f(\mathbf{x}) \mid A\mathbf{x} = \mathbf{b}, \, \mathbf{l} \leq \mathbf{x} \leq \mathbf{u}, \, \mathbf{x} \in \mathbb{Z}^n\}$. The number of variables $n$ is a variable part of the input, and we consider the regime where the constraint matrix $A$ has small coefficients $\|A\|_\infty$ and small primal or dual treedepth $\mathrm{td}_P(A)$ or $\mathrm{td}_D(A)$, respectively. Equivalently, we consider block-structured matrices, in particular $n$-fold, tree-fold, $2$-stage and multi-stage matrices.
  We ask about the possibility of near-linear time algorithms in the general case of (non-linear) separable convex functions. The techniques of previous works for the linear case are inherently limited to it; in fact, no strongly-polynomial algorithm may exist due to a simple unconditional information-theoretic lower bound of $n \log \|\mathbf{u}-\mathbf{l}\|_\infty$, where $\mathbf{l}, \mathbf{u}$ are the vectors of lower and upper bounds. Our first result is that with parameters $\mathrm{td}_P(A)$ and $\|A\|_\infty$, this lower bound can be matched (up to dependency on the parameters). Second, with parameters $\mathrm{td}_D(A)$ and $\|A\|_\infty$, the situation is more involved, and we design an algorithm with time complexity $g(\mathrm{td}_D(A), \|A\|_\infty) n \log n \log \|\mathbf{u}-\mathbf{l}\|_\infty$ where $g$ is some computable function. We conjecture that a stronger lower bound is possible in this regime, and our algorithm is in fact optimal.
  Our algorithms combine ideas from scaling, proximity, and sensitivity of integer programs, together with a new dynamic data structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22212v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christoph Hunkenschr\"oder, Martin Kouteck\'y, Asaf Levin, Tung Anh Vu</dc:creator>
    </item>
    <item>
      <title>On data usage and predictive behavior of data-driven predictive control with 1-norm regularization</title>
      <link>https://arxiv.org/abs/2505.22307</link>
      <description>arXiv:2505.22307v1 Announce Type: cross 
Abstract: We investigate the data usage and predictive behavior of data-driven predictive control (DPC) with 1-norm regularization. Our analysis enables the offline removal of unused data and facilitates a comparison between the identified symmetric structure and data usage against prior knowledge of the true system. This comparison helps assess the suitability of the DPC scheme for effective control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22307v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Kl\"adtke, Moritz Schulze Darup</dc:creator>
    </item>
    <item>
      <title>Accelerating Optimization via Differentiable Stopping Time</title>
      <link>https://arxiv.org/abs/2505.22509</link>
      <description>arXiv:2505.22509v1 Announce Type: cross 
Abstract: Optimization is an important module of modern machine learning applications. Tremendous efforts have been made to accelerate optimization algorithms. A common formulation is achieving a lower loss at a given time. This enables a differentiable framework with respect to the algorithm hyperparameters. In contrast, its dual, minimizing the time to reach a target loss, is believed to be non-differentiable, as the time is not differentiable. As a result, it usually serves as a conceptual framework or is optimized using zeroth-order methods. To address this limitation, we propose a differentiable stopping time and theoretically justify it based on differential equations. An efficient algorithm is designed to backpropagate through it. As a result, the proposed differentiable stopping time enables a new differentiable formulation for accelerating algorithms. We further discuss its applications, such as online hyperparameter tuning and learning to optimize. Our proposed methods show superior performance in comprehensive experiments across various problems, which confirms their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22509v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhonglin Xie, Yiman Fong, Haoran Yuan, Zaiwen Wen</dc:creator>
    </item>
    <item>
      <title>One Rank at a Time: Cascading Error Dynamics in Sequential Learning</title>
      <link>https://arxiv.org/abs/2505.22602</link>
      <description>arXiv:2505.22602v1 Announce Type: cross 
Abstract: Sequential learning -- where complex tasks are broken down into simpler, hierarchical components -- has emerged as a paradigm in AI. This paper views sequential learning through the lens of low-rank linear regression, focusing specifically on how errors propagate when learning rank-1 subspaces sequentially. We present an analysis framework that decomposes the learning process into a series of rank-1 estimation problems, where each subsequent estimation depends on the accuracy of previous steps. Our contribution is a characterization of the error propagation in this sequential process, establishing bounds on how errors -- e.g., due to limited computational budgets and finite precision -- affect the overall model accuracy. We prove that these errors compound in predictable ways, with implications for both algorithmic design and stability guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22602v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahtab Alizadeh Vandchali (Jasper),  Fangshuo (Jasper),  Liao, Anastasios Kyrillidis</dc:creator>
    </item>
    <item>
      <title>Iterative approximations of periodic trajectories for nonlinear systems with discontinuous inputs</title>
      <link>https://arxiv.org/abs/2401.00310</link>
      <description>arXiv:2401.00310v4 Announce Type: replace 
Abstract: Nonlinear control-affine systems described by ordinary differential equations with bounded measurable input functions are considered. The solvability of general boundary value problems for these systems is formulated in the sense of Carath\'eodory solutions. It is shown that, under the dominant linearization assumption, the considered class of boundary value problems admits a unique solution for any admissible control. These solutions can be obtained as the limit of the proposed simple iterative scheme and, in the case of periodic boundary conditions, via the developed Newton-type schemes. Under additional technical assumptions, sufficient contraction conditions of the corresponding generating operators are derived analytically. The proposed iterative approach is applied to compute periodic solutions of a realistic chemical reaction model with discontinuous control inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00310v4</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Zuyev, Peter Benner</dc:creator>
    </item>
    <item>
      <title>On Averaging and Extrapolation for Gradient Descent</title>
      <link>https://arxiv.org/abs/2402.12493</link>
      <description>arXiv:2402.12493v4 Announce Type: replace 
Abstract: This work considers the effect of averaging, and more generally extrapolation, of the iterates of gradient descent in smooth convex optimization. After running the method, rather than reporting the final iterate, one can report either a convex combination of the iterates (averaging) or a generic combination of the iterates (extrapolation). For several common stepsize sequences, including recently developed accelerated periodically long stepsize schemes, we show averaging cannot improve gradient descent's worst-case performance and is, in fact, strictly worse than simply returning the last iterate. In contrast, we prove a conceptually simple and computationally cheap extrapolation scheme strictly improves the worst-case convergence rate: when initialized at the origin, reporting $(1+1/\sqrt{16N\log(N)})x_N$ rather than $x_N$ improves the best possible worst-case performance by the same amount as conducting $O(\sqrt{N/\log(N)})$ more gradient steps. Our analysis and characterizations of the best-possible convergence guarantees are computer-aided, using performance estimation problems. Numerically, we find similar (small) benefits from such simple extrapolation for a range of gradient methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12493v4</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan Luner, Benjamin Grimmer</dc:creator>
    </item>
    <item>
      <title>Stochastic Bregman Subgradient Methods for Nonsmooth Nonconvex Optimization Problems</title>
      <link>https://arxiv.org/abs/2404.17386</link>
      <description>arXiv:2404.17386v2 Announce Type: replace 
Abstract: This paper focuses on the problem of minimizing a locally Lipschitz continuous function. Motivated by the effectiveness of Bregman gradient methods in training nonsmooth deep neural networks and the recent progress in stochastic subgradient methods for nonsmooth nonconvex optimization problems \cite{bolte2021conservative,bolte2022subgradient,xiao2023adam}, we investigate the long-term behavior of stochastic Bregman subgradient methods in such context, especially when the objective function lacks Clarke regularity. We begin by exploring a general framework for Bregman-type methods, establishing their convergence by a differential inclusion approach. For practical applications, we develop a stochastic Bregman subgradient method that allows the subproblems to be solved inexactly. Furthermore, we demonstrate how a single timescale momentum can be integrated into the Bregman subgradient method with slight modifications to the momentum update. Additionally, we introduce a Bregman proximal subgradient method for solving composite optimization problems possibly with constraints, whose convergence can be guaranteed based on the general framework. Numerical experiments on training nonsmooth neural networks are conducted to validate the effectiveness of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17386v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuangyu Ding, Kim-Chuan Toh</dc:creator>
    </item>
    <item>
      <title>Decision-Focused Forecasting: A Differentiable Multistage Optimisation Architecture</title>
      <link>https://arxiv.org/abs/2405.14719</link>
      <description>arXiv:2405.14719v2 Announce Type: replace 
Abstract: Most decision-focused learning work has focused on single stage problems whereas many real-world decision problems are more appropriately modelled using multistage optimisation. In multistage problems contextual information is revealed over time, decisions have to be taken sequentially, and decisions now have an intertemporal effect on future decisions. Decision-focused forecasting is a recurrent differentiable optimisation architecture that expresses a fully differentiable multistage optimisation approach. This architecture enables us to account for the intertemporal decision effects of forecasts. We show what gradient adjustments are made to account for the state-path caused by forecasting. We apply the model to multistage problems in energy storage arbitrage and portfolio optimisation and report that our model outperforms existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14719v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Egon Per\v{s}ak, Miguel F. Anjos</dc:creator>
    </item>
    <item>
      <title>A proximal splitting algorithm for generalized DC programming with applications in signal recovery</title>
      <link>https://arxiv.org/abs/2409.01535</link>
      <description>arXiv:2409.01535v4 Announce Type: replace 
Abstract: The difference-of-convex (DC) program is an important model in nonconvex optimization due to its structure, which encompasses a wide range of practical applications. In this paper, we aim to tackle a generalized class of DC programs, where the objective function is formed by summing a possibly nonsmooth nonconvex function and a differentiable nonconvex function with Lipschitz continuous gradient, and then subtracting a nonsmooth continuous convex function. We develop a proximal splitting algorithm that utilizes proximal evaluation for the concave part and Douglas--Rachford splitting for the remaining components. The algorithm guarantees subsequential convergence to a {\color{black}critical} point of the problem model. Under the widely used Kurdyka--{\L}ojasiewicz property, we establish global convergence of the full sequence of iterates and derive convergence rates for both the iterates and the objective function values, without assuming the concave part is differentiable. The performance of the proposed algorithm is tested on signal recovery problems with a nonconvex regularization term and exhibits competitive results compared to notable algorithms in the literature on both synthetic data and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01535v4</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tan Nhat Pham, Minh N. Dao, Nima Amjady, Rakibuzzaman Shah</dc:creator>
    </item>
    <item>
      <title>Simultaneously Solving FBSDEs and their Associated Semilinear Elliptic PDEs with Small Neural Operators</title>
      <link>https://arxiv.org/abs/2410.14788</link>
      <description>arXiv:2410.14788v2 Announce Type: replace 
Abstract: Forward-backwards stochastic differential equations (FBSDEs) play an important role in optimal control, game theory, economics, mathematical finance, and in reinforcement learning. Unfortunately, the available FBSDE solvers operate on \textit{individual} FBSDEs, meaning that they cannot provide a computationally feasible strategy for solving large families of FBSDEs, as these solvers must be re-run several times. \textit{Neural operators} (NOs) offer an alternative approach for \textit{simultaneously solving} large families of decoupled FBSDEs by directly approximating the solution operator mapping \textit{inputs:} terminal conditions and dynamics of the backwards process to \textit{outputs:} solutions to the associated FBSDE. Though universal approximation theorems (UATs) guarantee the existence of such NOs, these NOs are unrealistically large. Upon making only a few simple theoretically-guided tweaks to the standard convolutional NO build, we confirm that ``small'' NOs can uniformly approximate the solution operator to structured families of FBSDEs with random terminal time, uniformly on suitable compact sets determined by Sobolev norms using a logarithmic depth, a constant width, and a polynomial rank in the reciprocal approximation error.
  This result is rooted in our second result, and main contribution to the NOs for PDE literature, showing that our convolutional NOs of similar depth and width but grow only \textit{quadratically} (at a dimension-free rate) when uniformly approximating the solution operator of the associated class of semilinear Elliptic PDEs to these families of FBSDEs. A key insight into how NOs work we uncover is that the convolutional layers of our NO can approximately implement the fixed point iteration used to prove the existence of a unique solution to these semilinear Elliptic PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14788v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>q-fin.CP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takashi Furuya, Anastasis Kratsios</dc:creator>
    </item>
    <item>
      <title>A Stochastic Approximation Approach for Efficient Decentralized Optimization on Random Networks</title>
      <link>https://arxiv.org/abs/2410.18774</link>
      <description>arXiv:2410.18774v2 Announce Type: replace 
Abstract: A challenging problem in decentralized optimization is to develop algorithms with fast convergence on random and time varying topologies under unreliable and bandwidth-constrained communication network. This paper studies a stochastic approximation approach with a Fully Stochastic Primal Dual Algorithm (FSPDA) framework. Our framework relies on a novel observation that randomness in time varying topology can be incorporated in a stochastic augmented Lagrangian formulation, whose expected value admits saddle points that coincide with stationary solutions of the decentralized optimization problem. With the FSPDA framework, we develop two new algorithms supporting efficient sparsified communication on random time varying topologies -- FSPDA-SA allows agents to execute multiple local gradient steps depending on the time varying topology to accelerate convergence, and FSPDA-STORM further incorporates a variance reduction step to improve sample complexity. For problems with smooth (possibly non-convex) objective function, within $T$ iterations, we show that FSPDA-SA (resp. FSPDA-STORM) finds an $\mathcal{O}( 1/\sqrt{T} )$-stationary (resp. $\mathcal{O}( 1/T^{2/3} )$) solution. Numerical experiments show the benefits of the FSPDA algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18774v2</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chung-Yiu Yau, Haoming Liu, Hoi-To Wai</dc:creator>
    </item>
    <item>
      <title>Bang-Bang Optimal Control of Vaccination in Metapopulation Epidemics with Linear Cost Structures</title>
      <link>https://arxiv.org/abs/2503.15154</link>
      <description>arXiv:2503.15154v3 Announce Type: replace 
Abstract: This paper investigates optimal vaccination strategies in a metapopulation epidemic model. We consider a linear cost to better capture operational considerations, such as the total number of vaccines or hospitalizations, in contrast to the standard quadratic cost assumption on the control. The model incorporates state and mixed control-state constraints, and we derive necessary optimality conditions based on Pontryagin's Maximum Principle. We use Pontryagin's result to rule out the possibility of the occurrence of singular arcs and to provide a full characterization of the optimal control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15154v3</guid>
      <category>math.OC</category>
      <category>q-bio.PE</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Machado Moschen, Maria Soledad Aronna</dc:creator>
    </item>
    <item>
      <title>On classical solutions in the stabilization problem for nonholonomic control systems with time-varying feedback laws</title>
      <link>https://arxiv.org/abs/2503.18006</link>
      <description>arXiv:2503.18006v2 Announce Type: replace 
Abstract: We consider the stabilization problem for driftless control-affine systems under the bracket-generating condition. In our previous works, a class of time-varying feedback laws has been constructed to stabilize the equilibrium of a nonholonomic system under rather general controllability assumptions. This stabilization scheme is based on the sampling concept, which is not equivalent to the classical definition of solutions for the corresponding nonautonomous closed-loop system. In the present paper, we refine the previous results by presenting sufficient conditions for the convergence of classical solutions of the closed-loop system to the equilibrium. Our theoretical findings are applied to a multidimensional driftless control-affine system and illustrated through numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18006v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Zuyev, Victoria Grushkovskaya</dc:creator>
    </item>
    <item>
      <title>Extending direct data-driven predictive control towards systems with finite control sets</title>
      <link>https://arxiv.org/abs/2404.02727</link>
      <description>arXiv:2404.02727v2 Announce Type: replace-cross 
Abstract: Although classical model predictive control with finite control sets (FCS-MPC) is quite a popular control method, particularly in the realm of power electronics systems, its direct data-driven predictive control (FCS-DPC) counterpart has received relatively limited attention. In this paper, we introduce a novel reformulation of a commonly used DPC scheme that allows for the application of a modified sphere decoding algorithm, known for its efficiency and prominence in FCS-MPC applications. We test the reformulation on a popular electrical drive example and compare the computation times of sphere decoding FCS-DPC with an enumeration-based and a MIQP method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02727v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.23919/ECC64448.2024.10590896</arxiv:DOI>
      <dc:creator>Manuel Kl\"adtke, Moritz Schulze Darup, Daniel E. Quevedo</dc:creator>
    </item>
    <item>
      <title>Communication- and Computation-Efficient Distributed Submodular Optimization in Robot Mesh Networks</title>
      <link>https://arxiv.org/abs/2407.10382</link>
      <description>arXiv:2407.10382v3 Announce Type: replace-cross 
Abstract: We provide a communication- and computation-efficient method for distributed submodular optimization in robot mesh networks. Submodularity is a property of diminishing returns that arises in active information gathering such as mapping, surveillance, and target tracking. Our method, Resource-Aware distributed Greedy (RAG), introduces a new distributed optimization paradigm that enables scalable and near-optimal action coordination. To this end, RAG requires each robot to make decisions based only on information received from and about their neighbors. In contrast, the current paradigms allow the relay of information about all robots across the network. As a result, RAG's decision-time scales linearly with the network size, while state-of-the-art near-optimal submodular optimization algorithms scale cubically. We also characterize how the designed mesh-network topology affects RAG's approximation performance. Our analysis implies that sparser networks favor scalability without proportionally compromising approximation performance: while RAG's decision time scales linearly with network size, the gain in approximation performance scales sublinearly. We demonstrate RAG's performance in simulated scenarios of area detection with up to 45 robots, simulating realistic robot-to-robot (r2r) communication speeds such as the 0.25 Mbps speed of the Digi XBee 3 Zigbee 3.0. In the simulations, RAG enables real-time planning, up to three orders of magnitude faster than competitive near-optimal algorithms, while also achieving superior mean coverage performance. To enable the simulations, we extend the high-fidelity and photo-realistic simulator AirSim by integrating a scalable collaborative autonomy pipeline to tens of robots and simulating r2r communication delays. Our code is available at https://github.com/UM-iRaL/Resource-Aware-Coordination-AirSim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10382v3</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zirui Xu, Sandilya Sai Garimella, Vasileios Tzoumas</dc:creator>
    </item>
    <item>
      <title>Sensitivity of causal distributionally robust optimization</title>
      <link>https://arxiv.org/abs/2408.17109</link>
      <description>arXiv:2408.17109v2 Announce Type: replace-cross 
Abstract: We study the causal distributionally robust optimization (DRO) in both discrete- and continuous- time settings. The framework captures model uncertainty, with potential models penalized in function of their adapted Wasserstein distance to a given reference model. Strength of the penalty is controlled using a real-valued parameter which, in the special case of an indicator penalty, is simply the radius of the uncertainty ball. Our main results derive the first-order sensitivity of the value of causal DRO with respect to the penalization parameter, i.e., we compute the sensitivity to model uncertainty. Moreover, we investigate the case where a martingale constraint is imposed on the underlying model, as is the case for pricing measures in mathematical finance. We introduce different scaling regimes, which allow us to obtain the continuous-time sensitivities as nontrivial limits of their discrete-time counterparts. We illustrate our results with examples. The sensitivities are naturally expressed using optional projections of Malliavin derivatives. To establish our results we obtain several novel results which are of independent interest. In particular, we introduce pathwise Malliavin derivatives and show these extend the classical notion. We also establish a novel stochastic Fubini theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17109v2</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Jiang, Jan Obloj</dc:creator>
    </item>
    <item>
      <title>Robust Output Tracking for an Uncertain and Nonlinear 3D PDE-ODE System: Preventing Induced Seismicity in Underground Reservoirs</title>
      <link>https://arxiv.org/abs/2412.06327</link>
      <description>arXiv:2412.06327v2 Announce Type: replace-cross 
Abstract: This paper presents a robust control strategy for output tracking of a nonlinear 3D PDE-ODE system, where the ODE has logistic-like dynamics. The output feedback control was developed by bounding the solution and its time derivative for both the infinite-dimensional system and the nonlinear ODE. These bounds were then leveraged to ensure the boundedness of the control coefficient and the perturbations in the error dynamics. The mathematical framework proves the controller's ability to manage two output types within the system, overcoming model uncertainties and heterogeneities, using minimal system information, and a continuous control signal. A case study addressing induced seismicity mitigation while ensuring energy production in the Groningen gas reservoir highlights the control's effectiveness. The strategy guarantees precise tracking of target seismicity rates and pressures across reservoir regions, even under parameter uncertainties. Numerical simulations validate the approach in two scenarios: gas extraction while not exceeding the intrinsic seismicity of the region and the addition of CO2 injections, achieving net-zero environmental impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06327v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Diego Guti\'errez-Oribio, Ioannis Stefanou</dc:creator>
    </item>
    <item>
      <title>Outlier-Robust Linear System Identification Under Heavy-tailed Noise</title>
      <link>https://arxiv.org/abs/2501.00421</link>
      <description>arXiv:2501.00421v2 Announce Type: replace-cross 
Abstract: We consider the problem of estimating the state transition matrix of a linear time-invariant (LTI) system, given access to multiple independent trajectories sampled from the system. Several recent papers have conducted a non-asymptotic analysis of this problem, relying crucially on the assumption that the process noise is either Gaussian or sub-Gaussian, i.e., "light-tailed". In sharp contrast, we work under a significantly weaker noise model, assuming nothing more than the existence of the fourth moment of the noise distribution. For this setting, we provide the first set of results demonstrating that one can obtain sample-complexity bounds for linear system identification that are nearly of the same order as under sub-Gaussian noise. To achieve such results, we develop a novel robust system identification algorithm that relies on constructing multiple weakly-concentrated estimators, and then boosting their performance using suitable tools from high-dimensional robust statistics. Interestingly, our analysis reveals how the kurtosis of the noise distribution, a measure of heavy-tailedness, affects the number of trajectories needed to achieve desired estimation error bounds. Finally, we show that our algorithm and analysis technique can be easily extended to account for scenarios where an adversary can arbitrarily corrupt a small fraction of the collected trajectory data. Our work takes the first steps towards building a robust statistical learning theory for control under non-ideal assumptions on the data-generating process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00421v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vinay Kanakeri, Aritra Mitra</dc:creator>
    </item>
    <item>
      <title>Differentiable Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2501.08522</link>
      <description>arXiv:2501.08522v2 Announce Type: replace-cross 
Abstract: Singular value decomposition is widely used in modal analysis, such as proper orthogonal decomposition and resolvent analysis, to extract key features from complex problems. SVD derivatives need to be computed efficiently to enable the large scale design optimization. However, for a general complex matrix, no method can accurately compute this derivative to machine precision and remain scalable with respect to the number of design variables without requiring the all of the singular variables. We propose two algorithms to efficiently compute this derivative based on the adjoint method and reverse automatic differentiation and RAD-based singular value derivative formula. Differentiation results for each method proposed were compared with FD results for one square and one tall rectangular matrix example and matched with the FD results to about 5 to 7 digits. Finally, we demonstrate the scalability of the proposed method by calculating the derivatives of singular values with respect to the snapshot matrix derived from the POD of a large dataset for a laminar-turbulent transitional flow over a flat plate, sourced from the John Hopkins turbulence database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08522v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.CV</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rohit Kanchi, Sicheng He</dc:creator>
    </item>
    <item>
      <title>Solving Inverse Problems with Deep Linear Neural Networks: Global Convergence Guarantees for Gradient Descent with Weight Decay</title>
      <link>https://arxiv.org/abs/2502.15522</link>
      <description>arXiv:2502.15522v2 Announce Type: replace-cross 
Abstract: Machine learning methods are commonly used to solve inverse problems, wherein an unknown signal must be estimated from few measurements generated via a known acquisition procedure. In particular, neural networks perform well empirically but have limited theoretical guarantees. In this work, we study an underdetermined linear inverse problem that admits several possible solution mappings. A standard remedy (e.g., in compressed sensing) establishing uniqueness of the solution mapping is to assume knowledge of latent low-dimensional structure in the source signal. We ask the following question: do deep neural networks adapt to this low-dimensional structure when trained by gradient descent with weight decay regularization? We prove that mildly overparameterized deep linear networks trained in this manner converge to an approximate solution that accurately solves the inverse problem while implicitly encoding latent subspace structure. To our knowledge, this is the first result to rigorously show that deep linear networks trained with weight decay automatically adapt to latent subspace structure in the data under practical stepsize and weight initialization schemes. Our work highlights that regularization and overparameterization improve generalization, while overparameterization also accelerates convergence during training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15522v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Laus, Suzanna Parkinson, Vasileios Charisopoulos, Felix Krahmer, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>Deficient Excitation in Parameter Learning</title>
      <link>https://arxiv.org/abs/2503.02235</link>
      <description>arXiv:2503.02235v2 Announce Type: replace-cross 
Abstract: This paper investigates parameter learning problems under deficient excitation (DE). The DE condition is a rank-deficient, and therefore, a more general evolution of the well-known persistent excitation condition. Under the DE condition, a proposed online algorithm is able to calculate the identifiable and non-identifiable subspaces, and finally give an optimal parameter estimate in the sense of least squares. In particular, the learning error within the identifiable subspace exponentially converges to zero in the noise-free case, even without persistent excitation. The DE condition also provides a new perspective for solving distributed parameter learning problems, where the challenge is posed by local regressors that are often insufficiently excited. To improve knowledge of the unknown parameters, a cooperative learning protocol is proposed for a group of estimators that collect measured information under complementary DE conditions. This protocol allows each local estimator to operate locally in its identifiable subspace, and reach a consensus with neighbours in its non-identifiable subspace. As a result, the task of estimating unknown parameters can be achieved in a distributed way using cooperative local estimators. Application examples in system identification are given to demonstrate the effectiveness of the theoretical results developed in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02235v2</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ganghui Cao, Shimin Wang, Martin Guay, Jinzhi Wang, Zhisheng Duan, Marios M. Polycarpou</dc:creator>
    </item>
    <item>
      <title>Optimal Output Feedback Learning Control for Discrete-Time Linear Quadratic Regulation</title>
      <link>https://arxiv.org/abs/2503.06226</link>
      <description>arXiv:2503.06226v3 Announce Type: replace-cross 
Abstract: This paper studies the linear quadratic regulation (LQR) problem of unknown discrete-time systems via dynamic output feedback learning control. In contrast to the state feedback, the optimality of the dynamic output feedback control for solving the LQR problem requires an implicit condition on the convergence of the state observer. Moreover, due to unknown system matrices and the existence of observer error, it is difficult to analyze the convergence and stability of most existing output feedback learning-based control methods. To tackle these issues, we propose a generalized dynamic output feedback learning control approach with guaranteed convergence, stability, and optimality performance for solving the LQR problem of unknown discrete-time linear systems. In particular, a dynamic output feedback controller is designed to be equivalent to a state feedback controller. This equivalence relationship is an inherent property without requiring convergence of the estimated state by the state observer, which plays a key role in establishing the off-policy learning control approaches. By value iteration and policy iteration schemes, the adaptive dynamic programming based learning control approaches are developed to estimate the optimal feedback control gain. In addition, a model-free stability criterion is provided by finding a nonsingular parameterization matrix, which contributes to establishing a switched iteration scheme. Furthermore, the convergence, stability, and optimality analyses of the proposed output feedback learning control approaches are given. Finally, the theoretical results are validated by two numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06226v3</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kedi Xie, Martin Guay, Shimin Wang, Fang Deng, Maobin Lu</dc:creator>
    </item>
    <item>
      <title>An LiGME Regularizer of Designated Isolated Minimizers -- An Application to Discrete-Valued Signal Estimation</title>
      <link>https://arxiv.org/abs/2503.10126</link>
      <description>arXiv:2503.10126v2 Announce Type: replace-cross 
Abstract: For a regularized least squares estimation of discrete-valued signals, we propose a Linearly involved Generalized Moreau Enhanced (LiGME) regularizer, as a nonconvex regularizer, of designated isolated minimizers. The proposed regularizer is designed as a Generalized Moreau Enhancement (GME) of the so-called sum-of-absolute-values (SOAV) convex regularizer. Every candidate vector in the discrete-valued set is aimed to be assigned to an isolated local minimizer of the proposed regularizer while the overall convexity of the regularized least squares model is maintained. Moreover, a global minimizer of the proposed model can be approximated iteratively by using a variant of the constrained LiGME (cLiGME) algorithm. To enhance the accuracy of the proposed estimation, we also propose a pair of simple modifications, called respectively an iterative reweighting and a generalized superiorization. Numerical experiments demonstrate the effectiveness of the proposed model and algorithms in a scenario of multiple-input multiple-output (MIMO) signal detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10126v2</guid>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satoshi Shoji, Wataru Yata, Keita Kume, Isao Yamada</dc:creator>
    </item>
    <item>
      <title>Towards Resilient and Sustainable Global Industrial Systems: An Evolutionary-Based Approach</title>
      <link>https://arxiv.org/abs/2503.11688</link>
      <description>arXiv:2503.11688v2 Announce Type: replace-cross 
Abstract: This paper presents a new complex optimization problem in the field of automatic design of advanced industrial systems and proposes a hybrid optimization approach to solve the problem. The problem is multi-objective as it aims at finding solutions that minimize CO2 emissions, transportation time, and costs. The optimization approach combines an evolutionary algorithm and classical mathematical programming to design resilient and sustainable global manufacturing networks. Further, it makes use of the OWL ontology for data consistency and constraint management. The experimental validation demonstrates the effectiveness of the approach in both single and double sourcing scenarios. The proposed methodology, in general, can be applied to any industry case with complex manufacturing and supply chain challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11688v2</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V\'aclav Jirkovsk\'y, Ji\v{r}\'i Kubal\'ik, Petr Kadera, Arnd Schirrmann, Andreas Mitschke, Andreas Zindel</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Nonequilibrium Stochastic Processes</title>
      <link>https://arxiv.org/abs/2505.13609</link>
      <description>arXiv:2505.13609v2 Announce Type: replace-cross 
Abstract: We show that bootstrap methods based on the positivity of probability measures provide a systematic framework for studying both synchronous and asynchronous nonequilibrium stochastic processes on infinite lattices. First, we formulate linear programming problems that use positivity and invariance property of invariant measures to derive rigorous bounds on their expectation values. Second, for time evolution in asynchronous processes, we exploit the master equation along with positivity and initial conditions to construct linear and semidefinite programming problems that yield bounds on expectation values at both short and late times. We illustrate both approaches using two canonical examples: the contact process in 1+1 and 2+1 dimensions, and the Domany-Kinzel model in both synchronous and asynchronous forms in 1+1 dimensions. Our bounds on invariant measures yield rigorous lower bounds on critical rates, while those on time evolutions provide two-sided bounds on the half-life of the infection density and the temporal correlation length in the subcritical phase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13609v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>hep-th</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minjae Cho</dc:creator>
    </item>
    <item>
      <title>On Triangular Forms for x-Flat Control-Affine Systems With Two Inputs</title>
      <link>https://arxiv.org/abs/2505.15562</link>
      <description>arXiv:2505.15562v2 Announce Type: replace-cross 
Abstract: This paper examines a broadly applicable triangular normal form for two-input x-flat control-affine systems. First, we show that this triangular form encompasses a wide range of established normal forms. Next, we prove that any x-flat system can be transformed into this triangular structure after a finite number of prolongations of each input. Finally, we introduce a refined algorithm for identifying candidates for x-flat outputs. Through illustrative examples, we demonstrate the usefulness of our results. In particular, we show that the refined algorithm exceeds the capabilities of existing methods for computing flat outputs based on triangular forms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15562v2</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georg Hartl, Conrad Gst\"ottner, Markus Sch\"oberl</dc:creator>
    </item>
    <item>
      <title>Second domain variation for a product of domain functionals</title>
      <link>https://arxiv.org/abs/2505.20307</link>
      <description>arXiv:2505.20307v2 Announce Type: replace-cross 
Abstract: The second domain variation of the $p$-capacity and the $q$ - torsional rigidity for compact sets in $R^d, d\geq3$ with $1&lt;p&lt;d$ is computed. Conditions on $p$ and $q&gt;1$ are given such that the ball is a local minimzer or maximizer of the product.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20307v2</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alfred Wagner</dc:creator>
    </item>
  </channel>
</rss>
