<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The QIC-Index: A Novel, Data-Centric Metric for Quantifying the Impact of Research Data Sharing</title>
      <link>https://arxiv.org/abs/2510.03307</link>
      <description>arXiv:2510.03307v1 Announce Type: new 
Abstract: We introduce the QIC-Index, a novel metric to address the failure of publication-centric metrics to value research data sharing. The QIC-Index quantifies the impact of individual data objects by calculating a score based on their Quality (Q), Impact (I), and Collaboration (C). By rewarding the sharing of high-quality, impactful, and collaborative data, our framework aligns individual incentives with the goals of open science and aims to foster a more transparent and efficient research culture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03307v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin G. Frasch</dc:creator>
    </item>
    <item>
      <title>Markov kernels in Mathlib's probability library</title>
      <link>https://arxiv.org/abs/2510.04070</link>
      <description>arXiv:2510.04070v1 Announce Type: new 
Abstract: The probability folder of Mathlib, Lean's mathematical library, makes a heavy use of Markov kernels. We present their definition and properties and describe the formalization of the disintegration theorem for Markov kernels. That theorem is used to define conditional probability distributions of random variables as well as posterior distributions. We then explain how Markov kernels are used in a more unusual way to get a common definition of independence and conditional independence and, following the same principles, to define sub-Gaussian random variables. Finally, we also discuss the role of kernels in our formalization of entropy and Kullback-Leibler divergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04070v1</guid>
      <category>cs.DL</category>
      <category>math.PR</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R\'emy Degenne</dc:creator>
    </item>
    <item>
      <title>LLM-Based Information Extraction to Support Scientific Literature Research and Publication Workflows</title>
      <link>https://arxiv.org/abs/2510.04749</link>
      <description>arXiv:2510.04749v1 Announce Type: new 
Abstract: The increasing volume of scholarly publications requires advanced tools for efficient knowledge discovery and management. This paper introduces ongoing work on a system using Large Language Models (LLMs) for the semantic extraction of key concepts from scientific documents. Our research, conducted within the German National Research Data Infrastructure for and with Computer Science (NFDIxCS) project, seeks to support FAIR (Findable, Accessible, Interoperable, and Reusable) principles in scientific publishing. We outline our explorative work, which uses in-context learning with various LLMs to extract concepts from papers, initially focusing on the Business Process Management (BPM) domain. A key advantage of this approach is its potential for rapid domain adaptation, often requiring few or even zero examples to define extraction targets for new scientific fields. We conducted technical evaluations to compare the performance of commercial and open-source LLMs and created an online demo application to collect feedback from an initial user-study. Additionally, we gathered insights from the computer science research community through user stories collected during a dedicated workshop, actively guiding the ongoing development of our future services. These services aim to support structured literature reviews, concept-based information retrieval, and integration of extracted knowledge into existing knowledge graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04749v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-06136-2_9</arxiv:DOI>
      <arxiv:journal_reference>New Trends in Theory and Practice of Digital Libraries. TPDL 2025. Communications in Computer and Information Science, vol 2694. pp 90-99</arxiv:journal_reference>
      <dc:creator>Samy Ateia, Udo Kruschwitz, Melanie Scholz, Agnes Koschmider, Moayad Almohaishi</dc:creator>
    </item>
    <item>
      <title>Generalization and the Rise of System-level Creativity in Science</title>
      <link>https://arxiv.org/abs/2510.03240</link>
      <description>arXiv:2510.03240v1 Announce Type: cross 
Abstract: Innovation ecosystems require careful policy stewardship to drive sustained advance in human health, welfare, security and prosperity. We develop new measures that reliably decompose the influence of innovations in terms of the degree to which each represents a field-level foundation, an extension of foundational work, or a generalization that synthesizes and modularizes contributions from distant fields to catalyze combinatorial innovation. Using 23 million scientific works, we demonstrate that while foundational and extensional work within fields has declined in recent years-a trend garnering much recent attention-generalizations across fields have increased and accelerated with the rise of the web, social media, and artificial intelligence, shifting the locus of innovation from within fields to across the system as a whole. We explore implications for science policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03240v1</guid>
      <category>cs.SI</category>
      <category>cs.DL</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongbo Fang, James Evans</dc:creator>
    </item>
    <item>
      <title>Flexible metadata harvesting for ecology using large language models</title>
      <link>https://arxiv.org/abs/2508.20115</link>
      <description>arXiv:2508.20115v2 Announce Type: replace 
Abstract: Large, open datasets can accelerate ecological research, particularly by enabling researchers to develop new insights by reusing datasets from multiple sources. However, to find the most suitable datasets to combine and integrate, researchers must navigate diverse ecological and environmental data provider platforms with varying metadata availability and standards. To overcome this obstacle, we have developed a large language model (LLM)-based metadata harvester that flexibly extracts metadata from any dataset's landing page, and converts these to a user-defined, unified format using existing metadata standards. We validate that our tool is able to extract both structured and unstructured metadata with equal accuracy, aided by our LLM post-processing protocol. Furthermore, we utilise LLMs to identify links between datasets, both by calculating embedding similarity and by unifying the formats of extracted metadata to enable rule-based processing. Our tool, which flexibly links the metadata of different datasets, can therefore be used for ontology creation or graph-based queries, for example, to find relevant ecological and environmental datasets in a virtual research environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20115v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-06136-2_32</arxiv:DOI>
      <dc:creator>Zehao Lu, Thijs L van der Plas, Parinaz Rashidi, W Daniel Kissling, Ioannis N Athanasiadis</dc:creator>
    </item>
    <item>
      <title>Identity resolution of software metadata using Large Language Models</title>
      <link>https://arxiv.org/abs/2505.23500</link>
      <description>arXiv:2505.23500v2 Announce Type: replace-cross 
Abstract: Software is an essential component of research. However, little attention has been paid to it compared with that paid to research data. Recently, there has been an increase in efforts to acknowledge and highlight the importance of software in research activities. Structured metadata from platforms like bio.tools, Bioconductor, and Galaxy ToolShed offers valuable insights into research software in the Life Sciences. Although originally intended to support discovery and integration, this metadata can be repurposed for large-scale analysis of software practices. However, its quality and completeness vary across platforms, reflecting diverse documentation practices. To gain a comprehensive view of software development and sustainability, consolidating this metadata is necessary, but requires robust mechanisms to address its heterogeneity and scale.
  This article presents an evaluation of instruction-tuned large language models for the task of software metadata identity resolution, a critical step in assembling a cohesive collection of research software. Such a collection is the reference component for the Software Observatory at OpenEBench, a platform that aggregates metadata to monitor the FAIRness of research software in the Life Sciences. We benchmarked multiple models against a human-annotated gold standard, examined their behavior on ambiguous cases, and introduced an agreement-based proxy for high-confidence automated decisions. The proxy achieved high precision and statistical robustness, while also highlighting the limitations of current models and the broader challenges of automating semantic judgment in FAIR-aligned software metadata across registries and repositories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23500v2</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eva Mart\'in del Pico, Josep Llu\'is Gelp\'i, Salvador Capella-Guti\'errez</dc:creator>
    </item>
  </channel>
</rss>
