<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Feb 2025 02:38:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Cyclic Relaxed Douglas-Rachford Splitting for Inconsistent Nonconvex Feasibility</title>
      <link>https://arxiv.org/abs/2502.12285</link>
      <description>arXiv:2502.12285v1 Announce Type: new 
Abstract: We study the cyclic relaxed Douglas-Rachford algorithm for possibly nonconvex, and inconsistent feasibility problems. This algorithm can be viewed as a convex relaxation between the cyclic Douglas-Rachford algorithm first introduced by Borwein and Tam [2014] and the classical cyclic projections algorithm. We characterize the fixed points of the cyclic relaxed Douglas-Rachford algorithm and show the relation of the {\em shadows} of these fixed points to the fixed points of the cyclic projections algorithm. Finally, we provide conditions that guarantee local quantitative convergence estimates in the nonconvex, inconsistent setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12285v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thi Lan Dinh, G. S. Matthijs Jansen, D. Russell Luke</dc:creator>
    </item>
    <item>
      <title>Symmetric Rank-One Quasi-Newton Methods for Deep Learning Using Cubic Regularization</title>
      <link>https://arxiv.org/abs/2502.12298</link>
      <description>arXiv:2502.12298v1 Announce Type: new 
Abstract: Stochastic gradient descent and other first-order variants, such as Adam and AdaGrad, are commonly used in the field of deep learning due to their computational efficiency and low-storage memory requirements. However, these methods do not exploit curvature information. Consequently, iterates can converge to saddle points or poor local minima. On the other hand, Quasi-Newton methods compute Hessian approximations which exploit this information with a comparable computational budget. Quasi-Newton methods re-use previously computed iterates and gradients to compute a low-rank structured update. The most widely used quasi-Newton update is the L-BFGS, which guarantees a positive semi-definite Hessian approximation, making it suitable in a line search setting. However, the loss functions in DNNs are non-convex, where the Hessian is potentially non-positive definite. In this paper, we propose using a limited-memory symmetric rank-one quasi-Newton approach which allows for indefinite Hessian approximations, enabling directions of negative curvature to be exploited. Furthermore, we use a modified adaptive regularized cubics approach, which generates a sequence of cubic subproblems that have closed-form solutions with suitable regularization choices. We investigate the performance of our proposed method on autoencoders and feed-forward neural network models and compare our approach to state-of-the-art first-order adaptive stochastic methods as well as other quasi-Newton methods.x</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12298v1</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Ranganath, Mukesh Singhal, Roummel Marcia</dc:creator>
    </item>
    <item>
      <title>Robust Steady-State-Aware Model Predictive Control for Systems with Limited Computational Resources and External Disturbances</title>
      <link>https://arxiv.org/abs/2502.12348</link>
      <description>arXiv:2502.12348v1 Announce Type: new 
Abstract: Model Predictive Control (MPC) is a powerful control strategy; however, its reliance on online optimization poses significant challenges for implementation on systems with limited computational resources. One possible approach to address this issue is to shorten the prediction horizon and adjust the conventional MPC formulation to enlarge the region of attraction. However, these methods typically introduce additional computational load. Recently, steady-state-aware MPC has been introduced to ensure output tracking and convergence to a given desired steady-state configuration while maintaining constraint satisfaction at all times without adding extra computational load. Despite its promising performance, steady-state-aware MPC does not account for external disturbances, which can significantly limit its applicability to real-world systems. This paper aims to advance the method further by enhancing its robustness against external disturbances. To achieve this, we adopt the tube-based design framework, which decouples nominal trajectory optimization from robust control synthesis, thereby requiring no additional online computational resources. Theoretical guarantees of the proposed methodology are shown analytically, and its effectiveness is assessed through simulations and experimental studies on a Parrot Bebop 2 drone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12348v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hassan Jafari Ozoumchelooei, Mehdi Hosseinzadeh</dc:creator>
    </item>
    <item>
      <title>Homogenization and Mean-Field Approximation for Multi-Player Games</title>
      <link>https://arxiv.org/abs/2502.12389</link>
      <description>arXiv:2502.12389v1 Announce Type: new 
Abstract: We investigate how the framework of mean-field games may be used to investigate strategic interactions in large heterogeneous populations. We consider strategic interactions in a population of players which may be partitioned into near-homogeneous sub-populations subject to peer group effects and interactions across groups. We prove a quantitative homogenization result for multi-player games in this setting: we show that $\epsilon$-Nash equilibria of a general multi-player game with heterogeneity may be computed in terms of the Nash equilibria of an auxiliary multi-population mean-field game. We provide explicit and non-asymptotic bounds for the distance from optimality in terms of the number of players and the deviations from homogeneity in sub-populations. The best mean-field approximation corresponds to an optimal partition into sub-populations, which may be formulated as the solution of a mixed-integer program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12389v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rama Cont, Anran Hu</dc:creator>
    </item>
    <item>
      <title>Point source localisation with unbalanced optimal transport</title>
      <link>https://arxiv.org/abs/2502.12417</link>
      <description>arXiv:2502.12417v1 Announce Type: new 
Abstract: Replacing the quadratic proximal penalty familiar from Hilbert spaces by an unbalanced optimal transport distance, we develop forward-backward type optimisation methods in spaces of Radon measures. We avoid the actual computation of the optimal transport distances through the use of transport three-plans and the rough concept of transport subdifferentials. The resulting algorithm has a step similar to the sliding heuristics previously introduced for conditional gradient methods, however, now non-heuristically derived from the geometry of the space. We demonstrate the improved numerical performance of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12417v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tuomo Valkonen</dc:creator>
    </item>
    <item>
      <title>A Stochastic Linear-Quadratic Leader-Follower Differential Game with Elephant Memory</title>
      <link>https://arxiv.org/abs/2502.12437</link>
      <description>arXiv:2502.12437v1 Announce Type: new 
Abstract: This paper is concerned with a stochastic linear-quadratic leader-follower differential game with elephant memory. The model is general in that the state equation for both the leader and the follower includes the elephant memory of the state and the control, which are part of the diffusion term. Under certain assumptions, the state feedback representation of the open-loop Stackelberg strategy is derived by introducing two Riccati equations and a special matrix-valued equation. Finally, theoretical results are illustrated by means of an example concerning a dynamic advertising problem with elephant memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12437v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Xinpo Li, Jingtao Shi</dc:creator>
    </item>
    <item>
      <title>Hybrid Data-enabled Predictive Control: Incorporating model knowledge into the DeePC</title>
      <link>https://arxiv.org/abs/2502.12467</link>
      <description>arXiv:2502.12467v1 Announce Type: new 
Abstract: Predictive control can either be data-based (e.g. data-enabled predictive control, or DeePC) or model-based (model predictive control). In this paper we aim to bridge the gap between the two by investigating the case where only a partial model is available, i.e. incorporating model knowledge into DeePC. This has potential advantages over a purely data-based approach in terms of noise and computational expense. We formulate an approach to take advantage of partial model knowledge which we call hybrid data-enabled predictive control (HDeePC) and prove feasible set equivalence and equivalent closed-loop behavior in the noiseless, LTI case. Finally, two examples illustrate the potential of HDeePC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12467v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremy D. Watson</dc:creator>
    </item>
    <item>
      <title>Approximate solutions in multiobjective interval-valued optimization problems: Existence theorems and optimality conditions</title>
      <link>https://arxiv.org/abs/2502.12506</link>
      <description>arXiv:2502.12506v1 Announce Type: new 
Abstract: This paper is devoted to the study of approximate solutions for a multiobjective interval-valued optimization problem based on an interval order. We establish new existence theorems of approximate solutions for such a problem under some mild conditions. Moreover, we give KKT optimality conditions for approximate solutions for such a problem whose associated functions are nonsmooth and nonconvex. We also propose the approximate KKT optimality condition of an approximate solution for such a problem. Finally, we apply some obtained results to a noncooperative game involving the multiobjective interval-valued function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12506v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuang-liang Zhang, Yun-cheng Liu, Nan-jing Huang</dc:creator>
    </item>
    <item>
      <title>An Irreversible Investment Problem with Incomplete Information about Profitability</title>
      <link>https://arxiv.org/abs/2502.12661</link>
      <description>arXiv:2502.12661v1 Announce Type: new 
Abstract: We analyze an irreversible investment decision for a project which yields a flow of future operating profits given by a geometric Brownian motion with unknown drift. In contrast to similar optimal stopping problems with incomplete information, the agent's payoff now depends directly on the unknown drift and not only indirectly through the underlying dynamics. Hence, many standard arguments are not applicable. Nonetheless, we show that it is optimal to invest in the project if the current profit level exceeds a threshold depending on the current belief for the true state of the unknown drift. These thresholds are described by a boundary function, for which we establish structural properties like monotonicity and continuity. To prove these, we identify a central class of stopping times with useful features. Moreover, we characterize the boundary function as the unique solution of a nonlinear integral equation. Building on this characterization we compute the boundary function numerically and investigate the value of information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12661v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Gierens, Berenice Anne Neumann</dc:creator>
    </item>
    <item>
      <title>Integrated demand-side management and timetabling for an urban transit system: A Benders decomposition approach</title>
      <link>https://arxiv.org/abs/2502.12952</link>
      <description>arXiv:2502.12952v1 Announce Type: new 
Abstract: The intelligent upgrading of metropolitan rail transit systems has made it feasible to implement demand-side management policies that integrate multiple operational strategies in practical operations. However, the tight interdependence between supply and demand necessitates a coordinated approach combining demand-side management policies and supply-side resource allocations to enhance the urban rail transit ecosystem. In this study, we propose a mathematical and computational framework that optimizes train timetables, passenger flow control strategies, and trip-shifting plans through the pricing policy. Our framework incorporates an emerging trip-booking approach that transforms waiting at the stations into waiting at home, thereby mitigating station overcrowding. Additionally, it ensures service fairness by maintaining an equitable likelihood of delays across different stations. We formulate the problem as an integer linear programming model, aiming to minimize passengers' waiting time and government subsidies required to offset revenue losses from fare discounts used to encourage trip shifting. To improve computational efficiency, we develop a Benders decomposition-based algorithm within the branch-and-cut method, which decomposes the model into train timetabling with partial passenger assignment and passenger flow control subproblems. We propose valid inequalities based on our model's properties to strengthen the linear relaxation bounds at each node. Computational results from proof-of-concept and real-world case studies on the Beijing metro show that our solution method outperforms commercial solvers in terms of computational efficiency. We can obtain high-quality solutions, including optimal ones, at the root node with reduced branching requirements thanks to our novel decomposition framework and valid inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12952v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lixing Yang, Yahan Lu, Jiateng Yin, Sh. Sharif Azadeh</dc:creator>
    </item>
    <item>
      <title>A measure-valued HJB perspective on Bayesian optimal adaptive control</title>
      <link>https://arxiv.org/abs/2502.12957</link>
      <description>arXiv:2502.12957v1 Announce Type: new 
Abstract: We consider a Bayesian adaptive optimal stochastic control problem where a hidden static signal has a non-separable influence on the drift of a noisy observation. Being allowed to control the specific form of this dependence, we aim at optimising a cost functional depending on the posterior distribution of the hidden signal. Expressing the dynamics of this posterior distribution in the observation filtration, we embed our problem into a genuinely infinite-dimensional stochastic control problem featuring so-called measure-valued martingales. We address this problem by use of viscosity theory and approximation arguments. Specifically, we show equivalence to a corresponding weak formulation, characterise the optimal value of the problem in terms of the unique continuous viscosity solution of an associated HJB equation, and construct a piecewise constant and arbitrarily-close-to-optimal control to our main problem of study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12957v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander M. G. Cox, Sigrid K\"allblad, Chaorui Wang</dc:creator>
    </item>
    <item>
      <title>Integrated Scheduling Model for Arrivals and Departures in Metroplex Terminal Area</title>
      <link>https://arxiv.org/abs/2502.12196</link>
      <description>arXiv:2502.12196v1 Announce Type: cross 
Abstract: In light of the rapid expansion of civil aviation, addressing the delays and congestion phenomena in the vicinity of metroplex caused by the imbalance between air traffic flow and capacity is crucial. This paper first proposes a bi-level optimization model for the collaborative flight sequencing of arrival and departure flights in the metroplex with multiple airports, considering both the runway systems and TMA (Terminal Control Area) entry/exit fixes. Besides, the model is adaptive to various traffic scenarios. The genetic algorithm is employed to solve the proposed model. The Shanghai TMA, located in China, is used as a case study, and it includes two airports, Shanghai Hongqiao International Airport and Shanghai Pudong International Airport. The results demonstrate that the model can reduce arrival delay by 51.52%, departure delay by 18.05%, and the runway occupation time of departure flights by 23.83%. Furthermore, the model utilized in this study significantly enhances flight scheduling efficiency, providing a more efficient solution than the traditional FCFS (First Come, First Served) approach. Additionally, the algorithm employed offers further improvements over the NSGA II algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12196v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tonghe li, Jixin Liu, Hao Jiang, Weili Zeng, Lei Yang</dc:creator>
    </item>
    <item>
      <title>A Novel Unified Parametric Assumption for Nonconvex Optimization</title>
      <link>https://arxiv.org/abs/2502.12329</link>
      <description>arXiv:2502.12329v1 Announce Type: cross 
Abstract: Nonconvex optimization is central to modern machine learning, but the general framework of nonconvex optimization yields weak convergence guarantees that are too pessimistic compared to practice. On the other hand, while convexity enables efficient optimization, it is of limited applicability to many practical problems. To bridge this gap and better understand the practical success of optimization algorithms in nonconvex settings, we introduce a novel unified parametric assumption. Our assumption is general enough to encompass a broad class of nonconvex functions while also being specific enough to enable the derivation of a unified convergence theorem for gradient-based methods. Notably, by tuning the parameters of our assumption, we demonstrate its versatility in recovering several existing function classes as special cases and in identifying functions amenable to efficient optimization. We derive our convergence theorem for both deterministic and stochastic optimization, and conduct experiments to verify that our assumption can hold practically over optimization trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12329v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artem Riabinin, Ahmed Khaled, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>Navigating Demand Uncertainty in Container Shipping: Deep Reinforcement Learning for Enabling Adaptive and Feasible Master Stowage Planning</title>
      <link>https://arxiv.org/abs/2502.12756</link>
      <description>arXiv:2502.12756v2 Announce Type: cross 
Abstract: Reinforcement learning (RL) has shown promise in solving various combinatorial optimization problems. However, conventional RL faces challenges when dealing with real-world constraints, especially when action space feasibility is explicit and dependent on the corresponding state or trajectory. In this work, we focus on using RL in container shipping, often considered the cornerstone of global trade, by dealing with the critical challenge of master stowage planning. The main objective is to maximize cargo revenue and minimize operational costs while navigating demand uncertainty and various complex operational constraints, namely vessel capacity and stability, which must be dynamically updated along the vessel's voyage. To address this problem, we implement a deep reinforcement learning framework with feasibility projection to solve the master stowage planning problem (MPP) under demand uncertainty. The experimental results show that our architecture efficiently finds adaptive, feasible solutions for this multi-stage stochastic optimization problem, outperforming traditional mixed-integer programming and RL with feasibility regularization. Our AI-driven decision-support policy enables adaptive and feasible planning under uncertainty, optimizing operational efficiency and capacity utilization while contributing to sustainable and resilient global supply chains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12756v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaike van Twiller, Yossiri Adulyasak, Erick Delage, Djordje Grbic, Rune M{\o}ller Jensen</dc:creator>
    </item>
    <item>
      <title>Optimizing Social Network Interventions via Hypergradient-Based Recommender System Design</title>
      <link>https://arxiv.org/abs/2502.12973</link>
      <description>arXiv:2502.12973v1 Announce Type: cross 
Abstract: Although social networks have expanded the range of ideas and information accessible to users, they are also criticized for amplifying the polarization of user opinions. Given the inherent complexity of these phenomena, existing approaches to counteract these effects typically rely on handcrafted algorithms and heuristics. We propose an elegant solution: we act on the network weights that model user interactions on social networks (e.g., frequency of communication), to optimize a performance metric (e.g., polarization reduction), while users' opinions follow the classical Friedkin-Johnsen model. Our formulation gives rise to a challenging large-scale optimization problem with non-convex constraints, for which we develop a gradient-based algorithm. Our scheme is simple, scalable, and versatile, as it can readily integrate different, potentially non-convex, objectives. We demonstrate its merit by: (i) rapidly solving complex social network intervention problems with 3 million variables based on the Reddit and DBLP datasets; (ii) significantly outperforming competing approaches in terms of both computation time and disagreement reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12973v1</guid>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marino K\"uhne, Panagiotis D. Grontas, Giulia De Pasquale, Giuseppe Belgioioso, Florian D\"orfler, John Lygeros</dc:creator>
    </item>
    <item>
      <title>Fragility-aware Classification for Understanding Risk and Improving Generalization</title>
      <link>https://arxiv.org/abs/2502.13024</link>
      <description>arXiv:2502.13024v1 Announce Type: cross 
Abstract: Classification models play a critical role in data-driven decision-making applications such as medical diagnosis, user profiling, recommendation systems, and default detection. Traditional performance metrics, such as accuracy, focus on overall error rates but fail to account for the confidence of incorrect predictions, thereby overlooking the risk of confident misjudgments. This risk is particularly significant in cost-sensitive and safety-critical domains like medical diagnosis and autonomous driving, where overconfident false predictions may cause severe consequences. To address this issue, we introduce the Fragility Index (FI), a novel metric that evaluates classification performance from a risk-averse perspective by explicitly capturing the tail risk of confident misjudgments. To enhance generalizability, we define FI within the robust satisficing (RS) framework, incorporating data uncertainty. We further develop a model training approach that optimizes FI while maintaining tractability for common loss functions. Specifically, we derive exact reformulations for cross-entropy loss, hinge-type loss, and Lipschitz loss, and extend the approach to deep learning models. Through synthetic experiments and real-world medical diagnosis tasks, we demonstrate that FI effectively identifies misjudgment risk and FI-based training improves model robustness and generalizability. Finally, we extend our framework to deep neural network training, further validating its effectiveness in enhancing deep learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13024v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Yang, Zheng Cui, Daniel Zhuoyu Long, Jin Qi, Ruohan Zhan</dc:creator>
    </item>
    <item>
      <title>Constrained Online Convex Optimization with Polyak Feasibility Steps</title>
      <link>https://arxiv.org/abs/2502.13112</link>
      <description>arXiv:2502.13112v1 Announce Type: cross 
Abstract: In this work, we study online convex optimization with a fixed constraint function $g : \mathbb{R}^d \rightarrow \mathbb{R}$. Prior work on this problem has shown $O(\sqrt{T})$ regret and cumulative constraint satisfaction $\sum_{t=1}^{T} g(x_t) \leq 0$, while only accessing the constraint value and subgradient at the played actions $g(x_t), \partial g(x_t)$. Using the same constraint information, we show a stronger guarantee of anytime constraint satisfaction $g(x_t) \leq 0 \ \forall t \in [T]$, and matching $O(\sqrt{T})$ regret guarantees. These contributions are thanks to our approach of using Polyak feasibility steps to ensure constraint satisfaction, without sacrificing regret. Specifically, after each step of online gradient descent, our algorithm applies a subgradient descent step on the constraint function where the step-size is chosen according to the celebrated Polyak step-size. We further validate this approach with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13112v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Spencer Hutchinson, Mahnoosh Alizadeh</dc:creator>
    </item>
    <item>
      <title>Statistical Inference of Constrained Stochastic Optimization via Sketched Sequential Quadratic Programming</title>
      <link>https://arxiv.org/abs/2205.13687</link>
      <description>arXiv:2205.13687v5 Announce Type: replace 
Abstract: We consider online statistical inference of constrained stochastic nonlinear optimization problems. We apply the Stochastic Sequential Quadratic Programming (StoSQP) method to solve these problems, which can be regarded as applying second-order Newton's method to the Karush-Kuhn-Tucker (KKT) conditions. In each iteration, the StoSQP method computes the Newton direction by solving a quadratic program, and then selects a proper adaptive stepsize $\bar{\alpha}_t$ to update the primal-dual iterate. To reduce dominant computational cost of the method, we inexactly solve the quadratic program in each iteration by employing an iterative sketching solver. Notably, the approximation error of the sketching solver need not vanish as iterations proceed, meaning that the per-iteration computational cost does not blow up. For the above StoSQP method, we show that under mild assumptions, the rescaled primal-dual sequence $1/\sqrt{\bar{\alpha}_t}\cdot (x_t - x^\star, \lambda_t - \lambda^\star)$ converges to a mean-zero Gaussian distribution with a nontrivial covariance matrix depending on the underlying sketching distribution. To perform inference in practice, we also analyze a plug-in covariance matrix estimator. We illustrate the asymptotic normality result of the method both on benchmark nonlinear problems in CUTEst test set and on linearly/nonlinearly constrained regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.13687v5</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sen Na, Michael W. Mahoney</dc:creator>
    </item>
    <item>
      <title>Optimization on product manifolds under a preconditioned metric</title>
      <link>https://arxiv.org/abs/2306.08873</link>
      <description>arXiv:2306.08873v3 Announce Type: replace 
Abstract: Since optimization on Riemannian manifolds relies on the chosen metric, it is appealing to know that how the performance of a Riemannian optimization method varies with different metrics and how to exquisitely construct a metric such that a method can be accelerated. To this end, we propose a general framework for optimization problems on product manifolds endowed with a preconditioned metric, and we develop Riemannian methods under this metric. Generally, the metric is constructed by an operator that aims to approximate the diagonal blocks of the Riemannian Hessian of the cost function. We propose three specific approaches to design the operator: exact block diagonal preconditioning, left and right preconditioning, and Gauss--Newton type preconditioning. Specifically, we tailor new preconditioned metrics and adapt the proposed Riemannian methods to the canonical correlation analysis and the truncated singular value decomposition problems, which provably accelerate the Riemannian methods. Additionally, we adopt the Gauss--Newton type preconditioning to solve the tensor ring completion problem. Numerical results among these applications verify that a delicate metric does accelerate the Riemannian optimization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08873v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Gao, Renfeng Peng, Ya-xiang Yuan</dc:creator>
    </item>
    <item>
      <title>Notification Timing for On-Demand Personnel Scheduling</title>
      <link>https://arxiv.org/abs/2312.06139</link>
      <description>arXiv:2312.06139v2 Announce Type: replace 
Abstract: Modern business models have enabled service systems to leverage a large pool of casual employees with flexible hours, paid based on piece rates, to fulfill on-demand work. These systems have been successfully implemented in sectors such as ride-sharing, delivery services, and microtasks. However, because casual employees engage infrequently and may lack experience, maintaining service quality remains a key challenge. We introduce a novel scheduling system designed to provide experienced casual employees to service companies, optimizing their operations through a dynamic, data-driven approach. Similar to traditional on-call systems, it contacts casual personnel in order of seniority to inform them about available work. However, our system offers greater flexibility, allowing employees to take time to decide and freely select from available shifts. Senior employees can also replace (bump) junior employees from the schedule if no other preferred shift is available, subject to certain conditions. While permitted, these replacements create disruptions and dissatisfaction among employees. The management aims to efficiently assign all shifts while minimizing bumps. However, uncertainty arises regarding when an employee will select a shift. The key challenge is determining the optimal timing to notify employees to reduce disruptions. We first establish that this problem is $\mathcal{NP}$-complete even with perfect information. To address this, we propose a two-stage stochastic formulation for the dynamic problem and develop a heuristic algorithm that approximates the optimal policy using a threshold-based structure. These policies are fine-tuned using offline solutions with pre-known uncertainty, allowing for optimization. Testing on real-world data demonstrates that our approach outperforms the current strategy used by our industry partner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06139v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prakash Gawas, Antoine Legrain, Louis-Martin Rousseau</dc:creator>
    </item>
    <item>
      <title>An Optimal Periodic Dividend and Risk Control Problem for an Insurance Company</title>
      <link>https://arxiv.org/abs/2312.17131</link>
      <description>arXiv:2312.17131v2 Announce Type: replace 
Abstract: We study the problem of optimal risk policies and dividend strategies for an insurance company operating under the constraint that the timing of shareholder payouts is governed by the arrival times of a Poisson process. Concurrently, risk control is continuously managed through proportional reinsurance. Our analysis confirms the optimality of a periodic-classical barrier strategy for maximizing the expected net present value until the first instance of bankruptcy across all admissible periodic-classical strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17131v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Kelbert, Harold A. Moreno-Franco</dc:creator>
    </item>
    <item>
      <title>Integer Points in Arbitrary Convex Cones: The Case of the PSD and SOC Cones</title>
      <link>https://arxiv.org/abs/2403.09927</link>
      <description>arXiv:2403.09927v3 Announce Type: replace 
Abstract: We investigate the semigroup of integer points inside a convex cone. We extend classical results in integer linear programming to integer conic programming. We show that the semigroup associated with nonpolyhedral cones can sometimes have a notion of finite generating set. We show this is true for the cone of positive semidefinite matrices (PSD) and the second-order cone (SOC). Both cones have a finite generating set of integer points, similar in spirit to Hilbert bases, under the action of a finitely generated group. We also extend notions of total dual integrality, Gomory-Chv\'{a}tal closure, and Carath\'{e}odory rank to integer points in arbitrary cones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09927v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10107-024-02188-8</arxiv:DOI>
      <dc:creator>Jes\'us A. De Loera, Brittney Marsters, Luze Xu, Shixuan Zhang</dc:creator>
    </item>
    <item>
      <title>Efficient Nonlinear MPC by Leveraging LPV Embedding and Sequential Quadratic Programming</title>
      <link>https://arxiv.org/abs/2403.19195</link>
      <description>arXiv:2403.19195v2 Announce Type: replace 
Abstract: In this paper, we present efficient solutions for the nonlinear program (NLP) associated with nonlinear model predictive control (NMPC) by leveraging the linear parameter-varying (LPV) embedding of nonlinear models and sequential quadratic programming (SQP). The corresponding quadratic program (QP) subproblem is systematically constructed and efficiently updated using the scheduling parameter from the LPV embedding, enabling fast convergence while adapting to the behavior of the controlled system. Furthermore, the approach provides insight into the problem, its connection to SQP, and a clearer understanding of the differences between solving NMPC as an NLP and using the LPV-MPC approach, compared to similar methods in the literature. The efficiency of the proposed approach is demonstrated against state-of-the-art methods, including NLP algorithms, in control benchmarks and practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19195v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitrios S. Karachalios, Hossam S. Abbas</dc:creator>
    </item>
    <item>
      <title>String stability and guaranteed safety via funnel cruise control for vehicle platoons</title>
      <link>https://arxiv.org/abs/2405.07262</link>
      <description>arXiv:2405.07262v2 Announce Type: replace 
Abstract: We study decentralized control strategies for platoons of autonomous vehicles with heterogeneous and nonlinear dynamics. Based on ideas from funnel control, we present a novel decentralized control algorithm which is able to guarantee a safety distance between any two vehicles, a good traffic flow and it achieves string stability of the controlled platoon. We illustrate the performance of the controller by simulations of two extreme scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07262v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Berger, Bart Besselink</dc:creator>
    </item>
    <item>
      <title>MIP-DD: A Delta Debugger for Mixed Integer Programming Solvers</title>
      <link>https://arxiv.org/abs/2405.19770</link>
      <description>arXiv:2405.19770v4 Announce Type: replace 
Abstract: The recent performance improvements in mixed-integer programming (MIP) have been accompanied by a significantly increased complexity of the codes of MIP solvers, which poses challenges in fixing implementation errors. In this paper, we introduce MIP-DD, a solver-independent tool, which to the best of our knowledge is the first open-source delta debugger for MIP. Delta debugging is a hypothesis-trial-result approach to isolate the cause of a solver failure. MIP-DD simplifies MIP instances while maintaining the undesired behavior. Preliminary versions already supported and motivated fixes for many bugs in the SCIP releases 8.0.1 to 8.1.1. In these versions, MIP-DD successfully contributed to 24 out of all 51 documented MIP-related bugfixes even for some long-known issues. In selected case studies we highlight that instances triggering fundamental bugs in SCIP can typically be reduced to a few variables and constraints in less than an hour. This makes it significantly easier to manually trace and check the solution process on the resulting simplified instances. A promising future application of MIP-DD is the analysis of performance bottlenecks, which could very well benefit from simple adversarial instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19770v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Hoen, Dominik Kamp, Ambros Gleixner</dc:creator>
    </item>
    <item>
      <title>Analyzing the numerical correctness of branch-and-bound decisions for mixed-integer programming</title>
      <link>https://arxiv.org/abs/2412.14710</link>
      <description>arXiv:2412.14710v2 Announce Type: replace 
Abstract: Most state-of-the-art branch-and-bound solvers for mixed-integer linear programming rely on limited-precision floating-point arithmetic and use numerical tolerances when reasoning about feasibility and optimality during their search. While the practical success of floating-point MIP solvers bears witness to their overall numerical robustness, it is well-known that numerically challenging input can lead them to produce incorrect results. Even when their final answer is correct, one critical question remains: Were the individual decisions taken during branch-and-bound justified, i.e., can they be verified in exact arithmetic? In this paper, we attempt a first such a posteriori analysis of a pure LP-based branch-and-bound solver by checking all intermediate decisions critical to the correctness of the result: accepting solutions as integer feasible, declaring the LP relaxation infeasible, and pruning subtrees as subopti mal. Our computational study in the academic MIP solver SCIP confirms the expectation that in the overwhelming majority of cases, all decisions are correct. When errors do occur on numerically challenging instances, they typically affect only a small, typically single-digit, amount of leaf nodes that would require further processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14710v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Hoen, Ambros Gleixner</dc:creator>
    </item>
    <item>
      <title>A Framework for Stochastic Fairness in Dominant Resource Allocation with Cloud Computing Applications</title>
      <link>https://arxiv.org/abs/2501.18051</link>
      <description>arXiv:2501.18051v2 Announce Type: replace 
Abstract: Allocation of limited resources under uncertain requirements often necessitates fairness considerations, with applications in computer systems, health systems, and humanitarian logistics. This paper introduces a distributionally robust (DR) stochastic fairness framework for multi-resource allocation, leveraging rough estimates of the mean and variance of resource requirement distributions. The framework employs a sampled approximation DR (SA-DR) model to develop the concept of stochastic fairness, satisfying key properties such as stochastic Pareto efficiency, stochastic sharing incentive, and stochastic envy-freeness under suitable conditions. We show the convergence of the SA-DR model to the DR model and propose a finitely convergent algorithm to solve the SA-DR model. We empirically evaluate the performance of our moment-based SA-DR model -- which uses only rough estimates of the mean and variance of the resource requirement distribution -- against alternative resource allocation models under varying levels of information availability. We demonstrate that our moment-based partial-information SA-DR model can achieve performance closer to the full-information model than the worst-case information model. Convergence of the sampled approximation model and comparisons across models are illustrated using data from cloud computing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18051v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Lei, Akhil Singla, Sanjay Mehrotra</dc:creator>
    </item>
    <item>
      <title>High-precision linear minimization is no slower than projection</title>
      <link>https://arxiv.org/abs/2501.18454</link>
      <description>arXiv:2501.18454v2 Announce Type: replace 
Abstract: This note demonstrates that, for all compact convex sets, high-precision linear minimization can be performed via a single evaluation of the projection and a scalar-vector multiplication. In consequence, if $\varepsilon$-approximate linear minimization takes at least $L(\varepsilon)$ vector-arithmetic operations and projection requires $P$ operations, then $\mathcal{O}(P)\geq \mathcal{O}(L(\varepsilon))$ is guaranteed. This concept is expounded with examples, an explicit error bound, and an exact linear minimization result for polyhedral sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18454v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zev Woodstock</dc:creator>
    </item>
    <item>
      <title>Error Bound Analysis for the Regularized Loss of Deep Linear Neural Networks</title>
      <link>https://arxiv.org/abs/2502.11152</link>
      <description>arXiv:2502.11152v2 Announce Type: replace 
Abstract: The optimization foundations of deep linear networks have received significant attention lately. However, due to the non-convexity and hierarchical structure, analyzing the regularized loss of deep linear networks remains a challenging task. In this work, we study the local geometric landscape of the regularized squared loss of deep linear networks, providing a deeper understanding of its optimization properties. Specifically, we characterize the critical point set and establish an error-bound property for all critical points under mild conditions. Notably, we identify the sufficient and necessary conditions under which the error bound holds. To support our theoretical findings, we conduct numerical experiments demonstrating that gradient descent exhibits linear convergence when optimizing the regularized loss of deep linear networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11152v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Po Chen, Rujun Jiang, Peng Wang</dc:creator>
    </item>
    <item>
      <title>Over-parameterised Shallow Neural Networks with Asymmetrical Node Scaling: Global Convergence Guarantees and Feature Learning</title>
      <link>https://arxiv.org/abs/2302.01002</link>
      <description>arXiv:2302.01002v2 Announce Type: replace-cross 
Abstract: We consider gradient-based optimisation of wide, shallow neural networks, where the output of each hidden node is scaled by a positive parameter. The scaling parameters are non-identical, differing from the classical Neural Tangent Kernel (NTK) parameterisation. We prove that for large such neural networks, with high probability, gradient flow and gradient descent converge to a global minimum and can learn features in some sense, unlike in the NTK parameterisation. We perform experiments illustrating our theoretical results and discuss the benefits of such scaling in terms of prunability and transfer learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01002v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Transactions on Machine Learning Research, 2025</arxiv:journal_reference>
      <dc:creator>Francois Caron, Fadhel Ayed, Paul Jung, Hoil Lee, Juho Lee, Hongseok Yang</dc:creator>
    </item>
    <item>
      <title>GBO:AMulti-Granularity Optimization Algorithm via Granular-ball for Continuous Problems</title>
      <link>https://arxiv.org/abs/2303.12807</link>
      <description>arXiv:2303.12807v2 Announce Type: replace-cross 
Abstract: Optimization problems aim to find the optimal solution, which is becoming increasingly complex and difficult to solve. Traditional evolutionary optimization methods always overlook the granular characteristics of solution space. In the real scenario of numerous optimizations, the solution space is typically partitioned into sub-regions characterized by varying degree distributions. These sub-regions present different granularity characteristics at search potential and difficulty. Considering the granular characteristics of the solution space, the number of coarse-grained regions is smaller than the number of points, so the calculation is more efficient. On the other hand, coarse-grained characteristics are not easily affected by fine-grained sample points, so the calculation is more robust. To this end, this paper proposes a new multi-granularity evolutionary optimization method, namely the Granular-ball Optimization (GBO) algorithm, which characterizes and searches the solution space from coarse to fine. Specifically, using granular-balls instead of traditional points for optimization increases the diversity and robustness of the random search process. At the same time, the search range in different iteration processes is limited by the radius of granular-balls, covering the solution space from large to small. The mechanism of granular-ball splitting is applied to continuously split and evolve the large granular-balls into smaller ones for refining the solution space. Extensive experiments on commonly used benchmarks have shown that GBO outperforms popular and advanced evolutionary algorithms. The code can be found in the supporting materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.12807v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyin Xia, Xinyu Lin, Guan Wang, De-Gang Chen, Sen Zhao, Guoyin Wang, Jing Liang</dc:creator>
    </item>
    <item>
      <title>A Type II Hamiltonian Variational Principle and Adjoint Systems for Lie Groups</title>
      <link>https://arxiv.org/abs/2311.03527</link>
      <description>arXiv:2311.03527v2 Announce Type: replace-cross 
Abstract: We present a novel Type II variational principle on the cotangent bundle of a Lie group which enforces Type II boundary conditions, i.e., fixed initial position and final momentum. In general, such Type II variational principles are only globally defined on vector spaces or locally defined on general manifolds; however, by left translation, we are able to define this variational principle globally on cotangent bundles of Lie groups. Type II boundary conditions are particularly important for adjoint sensitivity analysis, which is our motivating application. As such, we additionally discuss adjoint systems on Lie groups, their properties, and how they can be used to solve optimization problems subject to dynamics on Lie groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03527v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10883-025-09730-7</arxiv:DOI>
      <arxiv:journal_reference>J Dyn Control Syst 31, 8 (2025)</arxiv:journal_reference>
      <dc:creator>Brian K. Tran, Melvin Leok</dc:creator>
    </item>
    <item>
      <title>De Finetti's Control for Refracted Skew Brownian Motion</title>
      <link>https://arxiv.org/abs/2402.11471</link>
      <description>arXiv:2402.11471v2 Announce Type: replace-cross 
Abstract: In this paper we propose a refracted skew Brownian motion as a risk model with endogenous regime switching, which generalizes the refracted diffusion risk process introduced by Gerber and Shiu. We consider an optimal dividend problem for the refracted skew Brownian risk model and identify sufficient conditions, respectively, for barrier strategy, band strategy and their variants to be optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11471v2</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongqin Gao, Xiaowen Zhou</dc:creator>
    </item>
    <item>
      <title>Untangling Lariats: Subgradient Following of Variationally Penalized Objectives</title>
      <link>https://arxiv.org/abs/2405.04710</link>
      <description>arXiv:2405.04710v2 Announce Type: replace-cross 
Abstract: We describe an apparatus for subgradient-following of the optimum of convex problems with variational penalties. In this setting, we receive a sequence $y_i,\ldots,y_n$ and seek a smooth sequence $x_1,\ldots,x_n$. The smooth sequence needs to attain the minimum Bregman divergence to an input sequence with additive variational penalties in the general form of $\sum_i{}g_i(x_{i+1}-x_i)$. We derive known algorithms such as the fused lasso and isotonic regression as special cases of our approach. Our approach also facilitates new variational penalties such as non-smooth barrier functions.
  We then introduce and analyze new multivariate problems in which $\mathbf{x}_i,\mathbf{y}_i\in\mathbb{R}^d$ with variational penalties that depend on $\|\mathbf{x}_{i+1}-\mathbf{x}_i\|$. The norms we consider are $\ell_2$ and $\ell_\infty$ which promote group sparsity. We also derive a novel lattice-based procedure for subgradient following of variational penalties characterized through the output of arbitrary convolutional filters. This paradigm yields efficient solvers for high-order filtering problems of temporal sequences in which sparse discrete derivatives such as acceleration and jerk are desirable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04710v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai-Chia Mo, Shai Shalev-Shwartz, Nis{\ae}l Sh\'artov</dc:creator>
    </item>
    <item>
      <title>Quantitative stability in optimal transport for general power costs</title>
      <link>https://arxiv.org/abs/2407.19337</link>
      <description>arXiv:2407.19337v2 Announce Type: replace-cross 
Abstract: We establish novel quantitative stability results for optimal transport problems with respect to perturbations in the target measure. We provide explicit bounds on the stability of optimal transport potentials and maps, which are relevant for both theoretical and practical applications. Our results apply to a wide range of costs, including all Wasserstein distances with power cost exponent strictly larger than $1$ and leverage mostly assumptions on the source measure, such as log-concavity and bounded support. Our work provides a significant step forward in the understanding of stability of optimal transport problems, as previous results where mostly limited to the case of the quadratic cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19337v2</guid>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Octave Mischler, Dario Trevisan</dc:creator>
    </item>
    <item>
      <title>Reducing Matroid Optimization to Basis Search</title>
      <link>https://arxiv.org/abs/2408.04118</link>
      <description>arXiv:2408.04118v4 Announce Type: replace-cross 
Abstract: Much energy has been devoted to developing a matroid's computational properties, yet parallel algorithm design for matroid optimization seems less understood. Specifically, the current state of the art is a folklore reduction from optimization to the search based on methods originating in [KUW88]. However, while this reduction adds only constant overhead in terms of \emph{adaptive complexity}, it imposes a high cost in \emph{query complexity}. In response, we present a new reduction from optimization to search within the class of \emph{binary matroids} which, when $n$ and $r$ take the size of the ground set and matroid rank respectively, implies a novel optimization algorithm terminating in $\mathcal{O}(\sqrt{n}\cdot\log r)$ parallel rounds using only $\mathcal{O}(rn\cdot\log r)$ independence queries. This is a significant improvement in query complexity when the matroid is sparse, meaning $r \ll n$, while trading off only a logarithmic factor of the rank in the adaptive complexity. At a technical level, our method begins by observing that a basis is optimal if and only if it is the set of points of minimum weight in any cocircuit. Importantly, this certificate reveals that simultaneous tests for \emph{local optimality} in cocircuits is a general paradigm for parallel matroid optimization. By combining this idea with connections between bases and cocircuits we obtain our reduction, whose efficiency follows by analyzing the lattice of flats. A primary goal of our study is initiating a finer understanding of parallel matroid optimization. And so, since many of our techniques begin with observations about general matroids and their flats, we hope that our efforts aid the future design of parallel matroid algorithms and applications of lattice theory thereof.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04118v4</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Streit, Vijay K. Garg</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization for Non-Convex Two-Stage Stochastic Optimization Problems</title>
      <link>https://arxiv.org/abs/2408.17387</link>
      <description>arXiv:2408.17387v2 Announce Type: replace-cross 
Abstract: Bayesian optimization is a sample-efficient method for solving expensive, black-box optimization problems. Stochastic programming concerns optimization under uncertainty where, typically, average performance is the quantity of interest. In the first stage of a two-stage problem, here-and-now decisions must be made in the face of uncertainty, while in the second stage, wait-and-see decisions are made after the uncertainty has been resolved. Many methods in stochastic programming assume that the objective is cheap to evaluate and linear or convex. We apply Bayesian optimization to solve non-convex, two-stage stochastic programs which are black-box and expensive to evaluate as, for example, is often the case with simulation objectives. We formulate a knowledge-gradient-based acquisition function to jointly optimize the first- and second-stage variables, establish a guarantee of asymptotic consistency, and provide a computationally efficient approximation. We demonstrate comparable empirical results to an alternative we formulate with fewer approximations, which alternates its focus between the two variable types, and superior empirical results over the state of the art and the standard, na\"ive, two-step benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17387v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack M. Buckingham, Ivo Couckuyt, Juergen Branke</dc:creator>
    </item>
    <item>
      <title>Derivative-Free Optimization via Finite Difference Approximation: An Experimental Study</title>
      <link>https://arxiv.org/abs/2411.00112</link>
      <description>arXiv:2411.00112v2 Announce Type: replace-cross 
Abstract: Derivative-free optimization (DFO) is vital in solving complex optimization problems where only noisy function evaluations are available through an oracle. Within this domain, DFO via finite difference (FD) approximation has emerged as a powerful method. Two classical approaches are the Kiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation (SPSA) algorithms, which estimate gradients using just two samples in each iteration to conserve samples. However, this approach yields imprecise gradient estimators, necessitating diminishing step sizes to ensure convergence, often resulting in slow optimization progress. In contrast, FD estimators constructed from batch samples approximate gradients more accurately. While gradient descent algorithms using batch-based FD estimators achieve more precise results in each iteration, they require more samples and permit fewer iterations. This raises a fundamental question: which approach is more effective -- KW-style methods or DFO with batch-based FD estimators? This paper conducts a comprehensive experimental comparison among these approaches, examining the fundamental trade-off between gradient estimation accuracy and iteration steps. Through extensive experiments in both low-dimensional and high-dimensional settings, we demonstrate a surprising finding: when an efficient batch-based FD estimator is applied, its corresponding gradient descent algorithm generally shows better performance compared to classical KW and SPSA algorithms in our tested scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00112v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wang Du-Yi, Liang Guo, Liu Guangwu, Zhang Kun</dc:creator>
    </item>
    <item>
      <title>Relative Optimal Transport</title>
      <link>https://arxiv.org/abs/2411.05678</link>
      <description>arXiv:2411.05678v2 Announce Type: replace-cross 
Abstract: We develop a theory of optimal transport relative to a distinguished subset, which acts as a reservoir of mass, allowing us to compare measures of different total variation. This relative transportation problem has an optimal solution and we obtain relative versions of the Kantorovich-Rubinstein norm, Wasserstein distance, Kantorovich-Rubinstein duality and Monge-Kantorovich duality. We also prove relative versions of the Riesz-Markov-Kakutani theorem, which connect the spaces of measures arising from the relative optimal transport problem to spaces of Lipschitz functions. For a boundedly compact Polish space, we show that our relative 1-finite real-valued Radon measures with relative Kantorovich-Rubinstein norm coincide with the sequentially order continuous dual of relative Lipschitz functions with the operator norm. As part of our work we develop a theory of Riesz cones that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05678v2</guid>
      <category>math.MG</category>
      <category>math.AT</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Bubenik, Alex Elchesen</dc:creator>
    </item>
    <item>
      <title>Structured Sampling for Robust Euclidean Distance Geometry</title>
      <link>https://arxiv.org/abs/2412.10664</link>
      <description>arXiv:2412.10664v2 Announce Type: replace-cross 
Abstract: This paper addresses the problem of estimating the positions of points from distance measurements corrupted by sparse outliers. Specifically, we consider a setting with two types of nodes: anchor nodes, for which exact distances to each other are known, and target nodes, for which complete but corrupted distance measurements to the anchors are available. To tackle this problem, we propose a novel algorithm powered by Nystr\"om method and robust principal component analysis. Our method is computationally efficient as it processes only a localized subset of the distance matrix and does not require distance measurements between target nodes. Empirical evaluations on synthetic datasets, designed to mimic sensor localization, and on molecular experiments, demonstrate that our algorithm achieves accurate recovery with a modest number of anchors, even in the presence of high levels of sparse outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10664v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandra Kundu, Abiy Tasissa, HanQin Cai</dc:creator>
    </item>
    <item>
      <title>Gradient Equilibrium in Online Learning: Theory and Applications</title>
      <link>https://arxiv.org/abs/2501.08330</link>
      <description>arXiv:2501.08330v3 Announce Type: replace-cross 
Abstract: We present a new perspective on online learning that we refer to as gradient equilibrium: a sequence of iterates achieves gradient equilibrium if the average of gradients of losses along the sequence converges to zero. In general, this condition is not implied by, nor implies, sublinear regret. It turns out that gradient equilibrium is achievable by standard online learning methods such as gradient descent and mirror descent with constant step sizes (rather than decaying step sizes, as is usually required for no regret). Further, as we show through examples, gradient equilibrium translates into an interpretable and meaningful property in online prediction problems spanning regression, classification, quantile estimation, and others. Notably, we show that the gradient equilibrium framework can be used to develop a debiasing scheme for black-box predictions under arbitrary distribution shift, based on simple post hoc online descent updates. We also show that post hoc gradient updates can be used to calibrate predicted quantiles under distribution shift, and that the framework leads to unbiased Elo scores for pairwise preference prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08330v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Michael I. Jordan, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>Towards identifying possible fault-tolerant advantage of quantum linear system algorithms in terms of space, time and energy</title>
      <link>https://arxiv.org/abs/2502.11239</link>
      <description>arXiv:2502.11239v2 Announce Type: replace-cross 
Abstract: Quantum computing, a prominent non-Von Neumann paradigm beyond Moore's law, can offer superpolynomial speedups for certain problems. Yet its advantages in efficiency for tasks like machine learning remain under investigation, and quantum noise complicates resource estimations and classical comparisons. We provide a detailed estimation of space, time, and energy resources for fault-tolerant superconducting devices running the Harrow-Hassidim-Lloyd (HHL) algorithm, a quantum linear system solver relevant to linear algebra and machine learning. Excluding memory and data transfer, possible quantum advantages over the classical conjugate gradient method could emerge at $N \approx 2^{33} \sim 2^{48}$ or even lower, requiring ${O}(10^5)$ physical qubits, ${O}(10^{12}\sim10^{13})$ Joules, and ${O}(10^6)$ seconds under surface code fault-tolerance with three types of magic state distillation (15-1, 116-12, 225-1). Key parameters include condition number, sparsity, and precision $\kappa, s\approx{O}(10\sim100)$, $\epsilon\sim0.01$, and physical error $10^{-5}$. Our resource estimator adjusts $N, \kappa, s, \epsilon$, providing a map of quantum-classical boundaries and revealing where a practical quantum advantage may arise. Our work quantitatively determine how advanced a fault-tolerant quantum computer should be to achieve possible, significant benefits on problems related to real-world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11239v2</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Tu, Mark Dubynskyi, Mohammadhossein Mohammadisiahroudi, Ekaterina Riashchentceva, Jinglei Cheng, Dmitry Ryashchentsev, Tam\'as Terlaky, Junyu Liu</dc:creator>
    </item>
  </channel>
</rss>
