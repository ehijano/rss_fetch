<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Reverse Designing Ferroelectric Capacitors with Machine Learning-based Compact Modeling</title>
      <link>https://arxiv.org/abs/2508.20216</link>
      <description>arXiv:2508.20216v1 Announce Type: new 
Abstract: Machine learning-based compact models provide a rapid and efficient approach for estimating device behavior across multiple input parameter variations. In this study, we introduce two reverse-design algorithms that utilize these compact models to identify device parameters corresponding to desired electrical characteristics. The algorithms effectively determine parameter sets, such as layer thicknesses, required to achieve specific device performance criteria. Significantly, the proposed methods are uniquely enabled by machine learning-based compact modeling; alternative computationally intensive approaches, such as phase-field modeling, would impose impractical time constraints for iterative design processes. Our comparative analysis demonstrates a substantial reduction in computation time when employing machine learning-based compact models compared to traditional phase-field methods, underscoring a clear and substantial efficiency advantage. Additionally, the accuracy and computational efficiency of both reverse-design algorithms are evaluated and compared, highlighting the practical advantages of machine learning-based compact modeling approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20216v1</guid>
      <category>cs.ET</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Ferrer, Jack Hutchins, Revanth Koduru, Sumeet Kumar Gupta, Admedullah Aziz</dc:creator>
    </item>
    <item>
      <title>Blind Source Separation-Enabled Joint Communication and Sensing in IBFD MIMO Systems</title>
      <link>https://arxiv.org/abs/2508.20409</link>
      <description>arXiv:2508.20409v1 Announce Type: new 
Abstract: This paper addresses the challenge of joint communication and sensing (JCAS) in next-generation wireless networks, with an emphasis on in-band full-duplex (IBFD) multiple-input multiple-output (MIMO) systems. Traditionally, self-interference (SI) in IBFD systems is a major obstacle to recovering the signal of interest (SOI). Under the JCAS paradigm, however, this high-power SI signal presents an opportunity for efficient sensing. Since each transceiver node has access to the original SI signal, its environmental reflections can be exploited to estimate channel conditions and detect changes, without requiring dedicated radar waveforms. We propose a blind source separation (BSS)-based framework to simultaneously perform self-interference cancellation (SIC) and extract sensing information in IBFD MIMO settings. The approach applies the Fast Independent Component Analysis (FastICA) algorithm to separate the SI and SOI signals while enabling simultaneous signal recovery and channel estimation. Simulation results confirm the framework's effectiveness, showing improved sensing and communication performance as signal frame size increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20409v1</guid>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyao Li, Conrad Prisby, Thomas Yang</dc:creator>
    </item>
    <item>
      <title>The Unwritten Contract of Cloud-based Elastic Solid-State Drives</title>
      <link>https://arxiv.org/abs/2508.17372</link>
      <description>arXiv:2508.17372v1 Announce Type: cross 
Abstract: Elastic block storage (EBS) with the storage-compute disaggregated architecture stands as a pivotal piece in today's cloud. EBS furnishes users with storage capabilities through the elastic solid-state drive (ESSD). Nevertheless, despite the widespread integration into cloud services, the absence of a thorough ESSD performance characterization raises critical doubt: when more and more services are shifted onto the cloud, can ESSD satisfactorily substitute the storage responsibilities of the local SSD and offer comparable performance?
  In this paper, we for the first time target this question by characterizing two ESSDs from Amazon AWS and Alibaba Cloud. We present an unwritten contract of cloud-based ESSDs, encapsulating four observations and five implications for cloud storage users. Specifically, the observations are counter-intuitive and contrary to the conventional perceptions of what one would expect from the local SSD. The implications we hope could guide users in revisiting the designs of their deployed cloud software, i.e., harnessing the distinct characteristics of ESSDs for better system performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17372v1</guid>
      <category>cs.PF</category>
      <category>cs.ET</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingjia Wang, Ming-Chang Yang</dc:creator>
    </item>
    <item>
      <title>Spatio-Temporal Pruning for Compressed Spiking Large Language Models</title>
      <link>https://arxiv.org/abs/2508.20122</link>
      <description>arXiv:2508.20122v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) present significant challenges for deployment in energy-constrained environments due to their large model sizes and high inference latency. Spiking Neural Networks (SNNs), inspired by the sparse event-driven neural processing and energy-efficient information transmission in the brain, offer a promising alternative for achieving low-power computing. Integrating the event-driven efficiency of spiking neurons with the advanced capabilities of LLMs represents a promising direction for power-efficient LLMs. This work specifically delves into the design of compressed spiking LLMs. Here, we revisit spatial and temporal pruning from the perspective of SNNs and propose a novel spatio-temporal pruning framework for Spiking LLMs to optimize computational efficiency while preserving high performance. Our spatial pruning technique reduces the number of active neurons and attention heads, effectively lowering the computational complexity of the model. Meanwhile, temporal pruning minimizes inference latency by dynamically adjusting the number of timesteps required for different layers. By combining these approaches with other compression techniques, we present the first work in the domain of Spiking LLMs to jointly explore spatial pruning, temporal pruning, extreme quantization and knowledge distillation strategies. Extensive experimental evaluation of our proposed framework for SpikingBERT on the large-scale GLUE benchmark demonstrates the efficacy of our approach in terms of computational operations and inference latency. Our approach offers a compelling solution for real-time, low-power natural language processing applications, making Spiking LLMs more practical for deployment on edge devices and in power-constrained settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20122v1</guid>
      <category>cs.NE</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Jiang, Malyaban Bal, Brian Matejek, Susmit Jha, Adam Cobb, Abhronil Sengupta</dc:creator>
    </item>
    <item>
      <title>QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming</title>
      <link>https://arxiv.org/abs/2508.20134</link>
      <description>arXiv:2508.20134v1 Announce Type: cross 
Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early quantum advantages on classically intractable problems, spanning physics simulations to Gaussian boson sampling. Yet, realizing these benefits remains challenging for non-experts, primarily due to the complexities of programming in Open Quantum Assembly Language (OpenQASM). Although Large Language Model (LLM)-based agents have shown promise in automating classical programming workflows, their quantum counterparts have largely been restricted to specialized tasks such as quantum chemistry or error correction. In this paper, we present QAgent, an LLM-powered multi-agent system that fully automates OpenQASM programming. By integrating task planning, in-context few-shot learning, retrieval-augmented generation (RAG) for long-term context, predefined generation tools, and chain-of-thought (CoT) reasoning, the agents systematically improve both compilation and functional correctness. Our evaluations demonstrate substantial improvements: across multiple LLMs of varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\% compared to previous static LLM-based approaches. We envision this multi-agent system as a key enabler for democratizing quantum programming, bridging expertise gaps, and accelerating the practical adoption of quantum computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20134v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>quant-ph</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenxiao Fu, Fan Chen, Lei Jiang</dc:creator>
    </item>
    <item>
      <title>Human-Centered Design for Connected Automation: Predicting Pedestrian Crossing Intentions</title>
      <link>https://arxiv.org/abs/2508.20464</link>
      <description>arXiv:2508.20464v1 Announce Type: cross 
Abstract: Road traffic remains a leading cause of death worldwide, with pedestrians and other vulnerable road users accounting for over half of the 1.19 million annual fatalities, much of it due to human error. Level-5 automated driving systems (ADSs), capable of full self-driving without human oversight, have the potential to reduce these incidents. However, their effectiveness depends not only on automation performance but also on their ability to communicate intent and coordinate safely with pedestrians in the absence of traditional driver cues. Understanding how pedestrians interpret and respond to ADS behavior is therefore critical to the development of connected vehicle systems. This study extends the Theory of Planned Behavior (TPB) by incorporating four external factors (i.e. safety, trust, compatibility, and understanding) to model pedestrian decision-making in road-crossing scenarios involving level-5 ADSs. Using data from an online survey (n = 212), results show that perceived behavioral control, attitude, and social information significantly predict pedestrians' crossing intentions. External factors, particularly perceived safety and understanding, strongly influence these constructs. Findings provide actionable insights for designing external human-machine interfaces (eHMIs) and cooperative V2X communication strategies that support safe, transparent interactions between automated vehicles and pedestrians. This work contributes to the development of inclusive, human-centered connected mobility systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20464v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanaz Motamedi, Viktoria Marcus, Griffin Pitts</dc:creator>
    </item>
    <item>
      <title>Encoding Tactile Stimuli for Organoid Intelligence in Braille Recognition</title>
      <link>https://arxiv.org/abs/2508.20850</link>
      <description>arXiv:2508.20850v1 Announce Type: cross 
Abstract: This study proposes a generalizable encoding strategy that maps tactile sensor data to electrical stimulation patterns, enabling neural organoids to perform an open-loop artificial tactile Braille classification task. Human forebrain organoids cultured on a low-density microelectrode array (MEA) are systematically stimulated to characterize the relationship between electrical stimulation parameters (number of pulse, phase amplitude, phase duration, and trigger delay) and organoid responses, measured as spike activity and spatial displacement of the center of activity. Implemented on event-based tactile inputs recorded from the Evetac sensor, our system achieved an average Braille letter classification accuracy of 61 percent with a single organoid, which increased significantly to 83 percent when responses from a three-organoid ensemble were combined. Additionally, the multi-organoid configuration demonstrated enhanced robustness against various types of artificially introduced noise. This research demonstrates the potential of organoids as low-power, adaptive bio-hybrid computational elements and provides a foundational encoding framework for future scalable bio-hybrid computing architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20850v1</guid>
      <category>cs.NE</category>
      <category>cs.ET</category>
      <category>cs.RO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Liu (School of Engineering Mathematics and Technology, University of Bristol, United Kingdom), Hemma Philamore (School of Engineering Mathematics and Technology, University of Bristol, United Kingdom), Benjamin Ward-Cherrier (School of Engineering Mathematics and Technology, University of Bristol, United Kingdom)</dc:creator>
    </item>
    <item>
      <title>Lattice Random Walk Discretisations of Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2508.20883</link>
      <description>arXiv:2508.20883v1 Announce Type: cross 
Abstract: We introduce a lattice random walk discretisation scheme for stochastic differential equations (SDEs) that samples binary or ternary increments at each step, suppressing complex drift and diffusion computations to simple 1 or 2 bit random values. This approach is a significant departure from traditional floating point discretisations and offers several advantages; including compatibility with stochastic computing architectures that avoid floating-point arithmetic in place of directly manipulating the underlying probability distribution of a bitstream, elimination of Gaussian sampling requirements, robustness to quantisation errors, and handling of non-Lipschitz drifts. We prove weak convergence and demonstrate the advantages through experiments on various SDEs, including state-of-the-art diffusion models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20883v1</guid>
      <category>math.NA</category>
      <category>cs.ET</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Duffield, Maxwell Aifer, Denis Melanson, Zach Belateche, Patrick J. Coles</dc:creator>
    </item>
    <item>
      <title>A Systematic Review and Layered Framework for Privacy-by-Design in Self-Sovereign Identity Systems</title>
      <link>https://arxiv.org/abs/2502.02520</link>
      <description>arXiv:2502.02520v2 Announce Type: replace 
Abstract: The use of Self-Sovereign Identity (SSI) systems for digital identity management is gaining traction and interest. Countries such as Bhutan have already implemented an SSI infrastructure to manage the identity of their citizens. The EU, thanks to the revised eIDAS regulation, is opening the door for SSI vendors to develop SSI systems for the planned EU digital identity wallet. These developments, which fall within the sovereign domain, raise questions about individual privacy. The design of SSI systems is complex, often characterized by a large number of components and architectural choices because the current SSI communities differ on how to create identifiers, how to build and present credentials, and even how to design a user wallet. SSI stacks developed by different organizations provide different privacy features for different privacy needs. This paper performs a systematic mapping and review of SSI components and technologies into a novel four-layer privacy framework to address the design complexity of SSI systems. Based on this review, we provide an accompanying Design Assistance Dashboard (DAD). The DAD shows the interdependencies between SSI components in different layers, and maps these components to different privacy requirements and considerations, even providing a simple privacy class for each component.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02520v2</guid>
      <category>cs.ET</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Montassar Naghmouchi, Maryline Laurent</dc:creator>
    </item>
    <item>
      <title>SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study</title>
      <link>https://arxiv.org/abs/2508.18250</link>
      <description>arXiv:2508.18250v2 Announce Type: replace 
Abstract: This work explores the cross-node scaling potential of SOT-MRAM for last-level caches (LLCs) under heterogeneous system scaling paradigm. We perform extensive Design-Technology Co-Optimization (DTCO) exercises to evaluate the bitcell footprint for different cell configurations at a representative 7 nm technology and to assess their implications on read and write power-performance. We crucially identify the MTJ routing struggle in conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary bitcell area scaling challenge and propose to use BEOL read selectors (BEOL RSs) that enable (10 -- 40) % bitcell area reduction and eventually match sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet the required SOT switching current, provided the magnetic free layer properties be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This is particularly to attribute to their (i) more available Si fins for write transistor and (ii) lower bitline resistance at reduced cell width. We nevertheless underscore the read tradeoff associated with BEOL RSs, with the low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost relative to 2T1R. This article thus highlights the realistic prospects and hurdles of BEOL RSs towards holistic power-performance-area scaling of SOT-MRAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18250v2</guid>
      <category>cs.ET</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Xiang, Fernando Garc\'ia-Redondo, Arvind Sharma, Van Dai Nguyen, Andrea Fantini, Philippe Matagne, Siddharth Rao, Subhali Subhechha, Lynn Verschueren, Mohammed Aftab Baig, Marie Garcia Bardon, Geert Hellings</dc:creator>
    </item>
    <item>
      <title>Cutting is All You Need: Execution of Large-Scale Quantum Neural Networks on Limited-Qubit Devices</title>
      <link>https://arxiv.org/abs/2412.04844</link>
      <description>arXiv:2412.04844v2 Announce Type: replace-cross 
Abstract: The rapid advancement in Quantum Computing, particularly through Noisy-Intermediate Scale Quantum (NISQ) devices, has spurred significant interest in Quantum Machine Learning (QML) applications. Despite their potential, fully-quantum algorithms remain impractical due to the limitations of current NISQ devices. Hybrid quantum-classical neural networks (HQNNs) have emerged as a viable alternative, leveraging both quantum and classical computations to enhance machine learning capabilities. However, the constrained resources of NISQ devices, particularly the limited number of qubits, pose significant challenges for executing large-scale quantum circuits.
  This work addresses these current challenges by proposing a novel and practical methodology for quantum circuit cutting of HQNNs, allowing large quantum circuits to be executed on limited-qubit NISQ devices. Our approach not only preserves the accuracy of the original circuits but also supports the training of quantum parameters across all subcircuits, which is crucial for the learning process in HQNNs. We propose a cutting methodology for HQNNs that employs a greedy algorithm for identifying efficient cutting points, and the implementation of trainable subcircuits, all designed to maximize the utility of NISQ devices in HQNNs. The findings suggest that quantum circuit cutting is a promising technique for advancing QML on current quantum hardware, since the cut circuit achieves comparable accuracy and much lower qubit requirements than the original circuit. The code is available at https://github.com/eBrain4Everyone/QNN-Cutting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04844v2</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Marchisio, Emman Sychiuco, Muhammad Kashif, Muhammad Shafique</dc:creator>
    </item>
    <item>
      <title>HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling</title>
      <link>https://arxiv.org/abs/2508.20016</link>
      <description>arXiv:2508.20016v2 Announce Type: replace-cross 
Abstract: Schedulers are critical for optimal resource utilization in high-performance computing. Traditional methods to evaluate schedulers are limited to post-deployment analysis, or simulators, which do not model associated infrastructure. In this work, we present the first-of-its-kind integration of scheduling and digital twins in HPC. This enables what-if studies to understand the impact of parameter configurations and scheduling decisions on the physical assets, even before deployment, or regarching changes not easily realizable in production. We (1) provide the first digital twin framework extended with scheduling capabilities, (2) integrate various top-tier HPC systems given their publicly available datasets, (3) implement extensions to integrate external scheduling simulators. Finally, we show how to (4) implement and evaluate incentive structures, as-well-as (5) evaluate machine learning based scheduling, in such novel digital-twin based meta-framework to prototype scheduling. Our work enables what-if scenarios of HPC systems to evaluate sustainability, and the impact on the simulated system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20016v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Maiterth, Wesley H. Brewer, Jaya S. Kuruvella, Arunavo Dey, Tanzima Z. Islam, Kevin Menear, Dmitry Duplyakin, Rashadul Kabir, Tapasya Patki, Terry Jones, Feiyi Wang</dc:creator>
    </item>
  </channel>
</rss>
