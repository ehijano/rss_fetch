<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 May 2025 01:44:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Computing in Integrated Terrestrial and Non-Terrestrial Networks: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2505.03016</link>
      <description>arXiv:2505.03016v1 Announce Type: new 
Abstract: The rapid growth of Internet-of-things (IoT) devices, smart vehicles, and other connected objects is driving demand for ubiquitous connectivity and intensive computing capacity. 5G and upcoming 6G networks are crucial to meeting these demands and the fast-evolving services and applications. However, traditional terrestrial networks face limitations in coverage and capacity. Integrated Terrestrial and Non-Terrestrial Networks (ITNTN) are emerging to address these challenges. In essence, ITNTN combines ground-based infrastructure with aerial, space, and water surface networks to provide seamless connectivity and computing resources anytime, anywhere. Given the stringent quality-of-service (QoS) of future services, edge computing will be an inseparable component of ITNTN. Consequently, we dive in this survey into current efforts of integrating cloud/fog/edge computing into ITNTN layers to facilitate stringent QoS services and address the data processing needs of modern applications. Since there have been only limited and partial efforts in integrating computing functionalities within ITNTN, we aim to extend the discussion to the full integration of computing and identifying the challenges and future research directions to achieve it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03016v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hoe Ziet Wong, Insaf Rzig, Safwan Alfattani, Wael Jaafar</dc:creator>
    </item>
    <item>
      <title>A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case</title>
      <link>https://arxiv.org/abs/2505.03196</link>
      <description>arXiv:2505.03196v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate strong potential across a variety of tasks in communications and networking due to their advanced reasoning capabilities. However, because different LLMs have different model structures and are trained using distinct corpora and methods, they may offer varying optimization strategies for the same network issues. Moreover, the limitations of an individual LLM's training data, aggravated by the potential maliciousness of its hosting device, can result in responses with low confidence or even bias. To address these challenges, we propose a blockchain-enabled collaborative framework that connects multiple LLMs into a Trustworthy Multi-LLM Network (MultiLLMN). This architecture enables the cooperative evaluation and selection of the most reliable and high-quality responses to complex network optimization problems. Specifically, we begin by reviewing related work and highlighting the limitations of existing LLMs in collaboration and trust, emphasizing the need for trustworthiness in LLM-based systems. We then introduce the workflow and design of the proposed Trustworthy MultiLLMN framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G communication systems and the difficulty of addressing such threats through traditional modeling techniques, we present FBS defense as a case study to empirically validate the effectiveness of our approach. Finally, we outline promising future research directions in this emerging area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03196v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Haoxiang Luo, Gang Sun, Yinqiu Liu, Dusit Niyato, Hongfang Yu, Mohammed Atiquzzaman, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>Efficient Wi-Fi Sensing for IoT Forensics with Lossy Compression of CSI Data</title>
      <link>https://arxiv.org/abs/2505.03375</link>
      <description>arXiv:2505.03375v1 Announce Type: new 
Abstract: Wi-Fi sensing is an emerging technology that uses channel state information (CSI) from ambient Wi-Fi signals to monitor human activity without the need for dedicated sensors. Wi-Fi sensing does not only represent a pivotal technology in intelligent Internet of Things (IoT) systems, but it can also provide valuable insights in forensic investigations. However, the high dimensionality of CSI data presents major challenges for storage, transmission, and processing in resource-constrained IoT environments. In this paper, we investigate the impact of lossy compression on the accuracy of Wi-Fi sensing, evaluating both traditional techniques and a deep learning-based approach. Our results reveal that simple, interpretable techniques based on principal component analysis can significantly reduce the CSI data volume while preserving classification performance, making them highly suitable for lightweight IoT forensic scenarios. On the other hand, deep learning models exhibit higher potential in complex applications like activity recognition (achieving compression ratios up to 16000:1 with minimal impact on sensing performance) but require careful tuning and greater computational resources. By considering two different sensing applications, this work demonstrates the feasibility of integrating lossy compression schemes into Wi-Fi sensing pipelines to make intelligent IoT systems more efficient and improve the storage requirements in forensic applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03375v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo Cerutti, Fabio Palmese, Marco Cominelli, Alessandro E. C. Redondi</dc:creator>
    </item>
    <item>
      <title>Advancing Remote and Continuous Cardiovascular Patient Monitoring through a Novel and Resource-efficient IoT-Driven Framework</title>
      <link>https://arxiv.org/abs/2505.03409</link>
      <description>arXiv:2505.03409v1 Announce Type: new 
Abstract: Cardiovascular diseases are a leading cause of fatalities worldwide, often occurring suddenly with limited time for intervention. Current healthcare monitoring systems for cardiac patients rely heavily on hospitalization, which can be impractical for continuous monitoring. This paper presents a novel IoT-based solution for remote, real-time tracking of critical cardiac metrics, addressing the pressing need for accessible and continuous healthcare, particularly for the aging population in Pakistan. The proposed IoT kit measures essential parameters such as body temperature, heart rate (HR), blood pressure (BP), oxygen saturation (SPO2), and electrocardiography (ECG).
  A key innovation of the system is its integration with a cloud-based application, enabling constant remote monitoring and incorporating an alarm mechanism to alert medical professionals for timely intervention, reducing the risk of catastrophic incidents. The system was tested in a clinical environment with 20 participants, demonstrating results closely aligned with those obtained using standard medical devices. The findings validate the system's potential for reliable remote monitoring, offering a significant step forward in proactive cardiac healthcare management. This novel approach combines IoT technology with cloud-based applications to provide a cost-effective and efficient solution for reducing unexpected fatalities among cardiac patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03409v1</guid>
      <category>cs.NI</category>
      <category>cs.IR</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanam Nayab, Sohail Raza Chohan, Aqsa Jameel, Syed Rehan Shah, Syed Ahsan Masud Zaidi, Aditya Nath Jha, Kamran Siddique</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Reinforcement Learning Scheduling to Support Low Latency in Teleoperated Driving</title>
      <link>https://arxiv.org/abs/2505.03558</link>
      <description>arXiv:2505.03558v1 Announce Type: new 
Abstract: The teleoperated driving (TD) scenario comes with stringent Quality of Service (QoS) communication constraints, especially in terms of end-to-end (E2E) latency and reliability. In this context, Predictive Quality of Service (PQoS), possibly combined with Reinforcement Learning (RL) techniques, is a powerful tool to estimate QoS degradation and react accordingly. For example, an intelligent agent can be trained to select the optimal compression configuration for automotive data, and reduce the file size whenever QoS conditions deteriorate. However, compression may inevitably compromise data quality, with negative implications for the TD application. An alternative strategy involves operating at the Radio Access Network (RAN) level to optimize radio parameters based on current network conditions, while preserving data quality. In this paper, we propose Multi-Agent Reinforcement Learning (MARL) scheduling algorithms, based on Proximal Policy Optimization (PPO), to dynamically and intelligently allocate radio resources to minimize E2E latency in a TD scenario. We evaluate two training paradigms, i.e., decentralized learning with local observations (IPPO) vs. centralized aggregation (MAPPO), in conjunction with two resource allocation strategies, i.e., proportional allocation (PA) and greedy allocation (GA). We prove via ns-3 simulations that MAPPO, combined with GA, achieves the best results in terms of latency, especially as the number of vehicles increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03558v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giacomo Avanzi, Marco Giordani, Michele Zorzi</dc:creator>
    </item>
    <item>
      <title>Energy, Scalability, Data and Security in Massive IoT: Current Landscape and Future Directions</title>
      <link>https://arxiv.org/abs/2505.03036</link>
      <description>arXiv:2505.03036v1 Announce Type: cross 
Abstract: The Massive Internet of Things (MIoT) envisions an interconnected ecosystem of billions of devices, fundamentally transforming diverse sectors such as healthcare, smart cities, transportation, agriculture, and energy management. However, the vast scale of MIoT introduces significant challenges, including network scalability, efficient data management, energy conservation, and robust security mechanisms. This paper presents a thorough review of existing and emerging MIoT technologies designed to address these challenges, including Low-Power Wide-Area Networks (LPWAN), 5G/6G capabilities, edge and fog computing architectures, and hybrid access methodologies. We further investigate advanced strategies such as AI-driven resource allocation, federated learning for privacy-preserving analytics, and decentralized security frameworks using blockchain. Additionally, we analyze sustainable practices, emphasizing energy harvesting and integrating green technologies to reduce environmental impact. Through extensive comparative analysis, this study identifies critical innovations and architectural adaptations required to support efficient, resilient, and scalable MIoT deployments. Key insights include the role of network slicing and intelligent resource management for scalability, adaptive protocols for real-time data handling, and lightweight AI models suited to the constraints of MIoT devices. This research ultimately contributes to a deeper understanding of how MIoT systems can evolve to meet the growing demand for seamless, reliable connectivity while prioritizing sustainability, security, and performance across diverse applications. Our findings serve as a roadmap for future advancements, underscoring the potential of MIoT to support a globally interconnected, intelligent infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03036v1</guid>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Imane Cheikh, S\'ebastien Roy, Essaid Sabir, Rachid Aouami</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Tutorial and Survey of O-RAN: Exploring Slicing-aware Architecture, Deployment Options, Use Cases, and Challenges</title>
      <link>https://arxiv.org/abs/2405.03555</link>
      <description>arXiv:2405.03555v5 Announce Type: replace 
Abstract: Open-radio access network (O-RAN) seeks to establish the principles of openness, programmability, automation, intelligence, and hardware-software disaggregation with interoperable and standard-compliant interfaces. It advocates for multi-vendorism and multi-stakeholderism within a cloudified and virtualized wireless infrastructure, aimed at enhancing the deployment, operation, and management of RAN architecture. These enhancements promise increased flexibility, performance optimization, service innovation, energy efficiency, and cost effectiveness across fifth-generation (5G), sixth-generation (6G), and beyond networks. A silent feature of O-RAN architecture is its support for network slicing, which entails interaction with other domains of the cellular network, notably the transport network (TN) and the core network (CN), to realize end-to-end (E2E) network slicing. The study of this feature requires exploring the stances and contributions of diverse standards development organizations (SDOs). In this context, we note that despite the ongoing industrial deployments and standardization efforts, the research and standardization communities have yet to comprehensively address network slicing in O-RAN. To address this gap, this paper provides a comprehensive exploration of network slicing in O-RAN through an in-depth review of specification documents from O-RAN Alliance and research papers from leading industry and academic institutions. The paper commences with an overview of the relevant standardization and open source contributions, subsequently delving into the latest O-RAN architecture with an emphasis on its slicing aspects. Furthermore, the paper explores O-RAN deployment scenarios, examining options for the deployment and orchestration of RAN and TN slice subnets. It also discusses the slicing of the underlying infrastructure and provides an overview of various use cases related...</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03555v5</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Khurshid Alam, Mohammad Asif Habibi, Matthias Tammen, Dennis Krummacker, Walid Saad, Marco Di Renzo, Tommaso Melodia, Xavier Costa-P\'erez, M\'erouane Debbah, Ashutosh Dutta, Hans D. Schotten</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Exploratory Learning-Aided Community Detection Under Topological Uncertainty</title>
      <link>https://arxiv.org/abs/2304.04497</link>
      <description>arXiv:2304.04497v4 Announce Type: replace-cross 
Abstract: In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often uncertain, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a unified framework for detecting overlapping communities via exploratory learning aided by easy-to-collect node metadata when networks are topologically unknown (or only partially known). Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via community-affiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through extensive experiments on three real-world datasets including two large networks, we demonstrate: (a) the superiority of META-CODE over benchmark community detection methods, achieving remarkable gains up to 65.55% on the Facebook dataset over the best competitor among our selected competitive methods in terms of normalized mutual information (NMI), (b) the impact of each module in META-CODE, (c) the effectiveness of node queries in META-CODE based on empirical evaluations and theoretical findings, and (d) the convergence of the inferred network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.04497v4</guid>
      <category>cs.SI</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.NI</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Hou, Cong Tran, Ming Li, Won-Yong Shin</dc:creator>
    </item>
    <item>
      <title>Editable-DeepSC: Reliable Cross-Modal Semantic Communications for Facial Editing</title>
      <link>https://arxiv.org/abs/2411.15702</link>
      <description>arXiv:2411.15702v2 Announce Type: replace-cross 
Abstract: Real-time computer vision (CV) plays a crucial role in various real-world applications, whose performance is highly dependent on communication networks. Nonetheless, the data-oriented characteristics of conventional communications often do not align with the special needs of real-time CV tasks. To alleviate this issue, the recently emerged semantic communications only transmit task-related semantic information and exhibit a promising landscape to address this problem. However, the communication challenges associated with Semantic Facial Editing, one of the most important real-time CV applications on social media, still remain largely unexplored. In this paper, we fill this gap by proposing Editable-DeepSC, a novel cross-modal semantic communication approach for facial editing. Firstly, we theoretically discuss different transmission schemes that separately handle communications and editings, and emphasize the necessity of Joint Editing-Channel Coding (JECC) via iterative attributes matching, which integrates editings into the communication chain to preserve more semantic mutual information. To compactly represent the high-dimensional data, we leverage inversion methods via pre-trained StyleGAN priors for semantic coding. To tackle the dynamic channel noise conditions, we propose SNR-aware channel coding via model fine-tuning. Extensive experiments indicate that Editable-DeepSC can achieve superior editings while significantly saving the transmission bandwidth, even under high-resolution and out-of-distribution (OOD) settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15702v2</guid>
      <category>cs.IT</category>
      <category>cs.CV</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Chen, Wenbo Yu, Qinshan Zhang, Tianqu Zhuang, Yong Jiang, Shu-Tao Xia</dc:creator>
    </item>
    <item>
      <title>SemSpaceFL: A Collaborative Hierarchical Federated Learning Framework for Semantic Communication in 6G LEO Satellites</title>
      <link>https://arxiv.org/abs/2505.00966</link>
      <description>arXiv:2505.00966v2 Announce Type: replace-cross 
Abstract: The advent of the sixth-generation (6G) wireless networks, enhanced by artificial intelligence, promises ubiquitous connectivity through Low Earth Orbit (LEO) satellites. These satellites are capable of collecting vast amounts of geographically diverse and real-time data, which can be immensely valuable for training intelligent models. However, limited inter-satellite communication and data privacy constraints hinder data collection on a single server for training. Therefore, we propose SemSpaceFL, a novel hierarchical federated learning (HFL) framework for LEO satellite networks, with integrated semantic communication capabilities. Our framework introduces a two-tier aggregation architecture where satellite models are first aggregated at regional gateways before final consolidation at a cloud server, which explicitly accounts for satellite mobility patterns and energy constraints. The key innovation lies in our novel aggregation approach, which dynamically adjusts the contribution of each satellite based on its trajectory and association with different gateways, which ensures stable model convergence despite the highly dynamic nature of LEO constellations. To further enhance communication efficiency, we incorporate semantic encoding-decoding techniques trained through the proposed HFL framework, which enables intelligent data compression while maintaining signal integrity. Our experimental results demonstrate that the proposed aggregation strategy achieves superior performance and faster convergence compared to existing benchmarks, while effectively managing the challenges of satellite mobility and energy limitations in dynamic LEO networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00966v2</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Loc X. Nguyen, Sheikh Salman Hassan, Yu Min Park, Yan Kyaw Tun, Zhu Han, Choong Seon Hong</dc:creator>
    </item>
  </channel>
</rss>
