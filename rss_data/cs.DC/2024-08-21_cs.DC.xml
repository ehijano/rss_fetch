<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Aug 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Softening the Impact of Collisions in Contention Resolution</title>
      <link>https://arxiv.org/abs/2408.11275</link>
      <description>arXiv:2408.11275v1 Announce Type: new 
Abstract: Contention resolution addresses the problem of coordinating access to a shared communication channel. Time is discretized into synchronized slots, and a packet can be sent in any slot. If no packet is sent, then the slot is empty; if a single packet is sent, then it is successful; and when multiple packets are sent at the same time, a collision occurs, resulting in the failure of the corresponding transmissions. In each slot, every packet receives ternary channel feedback indicating whether the current slot is empty, successful, or a collision.
  Much of the prior work on contention resolution has focused on optimizing the makespan, which is the number of slots required for all packets to succeed. However, in many modern systems, collisions are also costly in terms of the time they incur, which existing contention-resolution algorithms do not address.
  In this paper, we design and analyze a randomized algorithm, Collision Aversion Backoff (CAB), that optimizes both the makespan and the collision cost. We consider the static case where an unknown $n\geq 2$ packets are initially present in the system, and each collision has a known cost $\mathcal{C}$, where $1 \leq \mathcal{C} \leq n^{\kappa}$ for a known constant $\kappa\geq 0$. With error probability polynomially small in $n$, CAB guarantees that all packets succeed with makespan and a total expected collision cost of $\tilde{O}(n\sqrt{\mathcal{C}})$. We give a lower bound for the class of fair algorithms: where, in each slot, every packet executing the fair algorithm sends with the same probability (and the probability may change from slot to slot). Our lower bound is asymptotically tight up to a $\texttt{poly}(\log n)$-factor for sufficiently large $\mathcal{C}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11275v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Umesh Biswas, Trisha Chakraborty, Maxwell Young</dc:creator>
    </item>
    <item>
      <title>Telepathic Datacenters: Fast RPCs using Shared CXL Memory</title>
      <link>https://arxiv.org/abs/2408.11325</link>
      <description>arXiv:2408.11325v1 Announce Type: new 
Abstract: Datacenter applications often rely on remote procedure calls (RPCs) for fast, efficient, and secure communication. However, RPCs are slow, inefficient, and hard to use as they require expensive serialization and compression to communicate over a packetized serial network link. Compute Express Link 3.0 (CXL) offers an alternative solution, allowing applications to share data using a cache-coherent, shared-memory interface across clusters of machines.
  RPCool is a new framework that exploits CXL's shared memory capabilities. RPCool avoids serialization by passing pointers to data structures in shared memory. While avoiding serialization is useful, directly sharing pointer-rich data eliminates the isolation that copying data over traditional networks provides, leaving the receiver vulnerable to invalid pointers and concurrent updates to shared data by the sender. RPCool restores this safety with careful and efficient management of memory permissions. Another significant challenge with CXL shared memory capabilities is that they are unlikely to scale to an entire datacenter. RPCool addresses this by falling back to RDMA-based communication.
  Overall, RPCool reduces the round-trip latency by 1.93$\times$ and 7.2$\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms, respectively. Moreover, RPCool performs either comparably or better than other RPC mechanisms across a range of workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11325v1</guid>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suyash Mahar, Ehsan Hajyjasini, Seungjin Lee, Zifeng Zhang, Mingyao Shen, Steven Swanson</dc:creator>
    </item>
    <item>
      <title>Stream-K++: Adaptive GPU GEMM Kernel Scheduling and Selection using Bloom Filters</title>
      <link>https://arxiv.org/abs/2408.11417</link>
      <description>arXiv:2408.11417v1 Announce Type: new 
Abstract: General matrix multiplication (GEMM) operations are crucial in various computational fields. As GPU architectures evolve, optimizing GEMM performance becomes increasingly important. This paper introduces Stream-K++, an enhancement to the promising Stream-K GEMM scheduling algorithm. We expand Stream-K's scheduling policies from three to seven and implement an efficient solution selection mechanism using Bloom filters. Our approach rapidly eliminates up to 95.8% of unsuitable configurations while maintaining a 100% true-negative rate. Implemented using the AMD Composable Kernel library and evaluated on AMD Instinct MI250X GPUs, Stream-K++ demonstrates significant performance gains (up to 43%) in select scenarios. It remains competitive (within 20% of optimal) for 60-97.6% of problem sizes. Our flexible framework, implemented in the Opensieve C++ library, allows for easy adaptation to new problem sizes, scheduling policies, or additional tuning parameters, paving the way for future optimizations in GPU-based GEMM operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11417v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Harisankar Sadasivan, Muhammad Osama, Maksim Podkorytov, Carlus Huang, Jun Liu</dc:creator>
    </item>
    <item>
      <title>High Performance Unstructured SpMM Computation Using Tensor Cores</title>
      <link>https://arxiv.org/abs/2408.11551</link>
      <description>arXiv:2408.11551v1 Announce Type: new 
Abstract: High-performance sparse matrix-matrix (SpMM) multiplication is paramount for science and industry, as the ever-increasing sizes of data prohibit using dense data structures. Yet, existing hardware, such as Tensor Cores (TC), is ill-suited for SpMM, as it imposes strict constraints on data structures that cannot be met by unstructured sparsity found in many applications. To address this, we introduce (S)parse (Ma)trix Matrix (T)ensor Core-accelerated (SMaT): a novel SpMM library that utilizes TCs for unstructured sparse matrices. Our block-sparse library leverages the low-level CUDA MMA (matrix-matrix-accumulate) API, maximizing the performance offered by modern GPUs. Algorithmic optimizations such as sparse matrix permutation further improve performance by minimizing the number of non-zero blocks. The evaluation on NVIDIA A100 shows that SMaT outperforms SotA libraries (DASP, cuSPARSE, and Magicube) by up to 125x (on average 2.6x). SMaT can be used to accelerate many workloads in scientific computing, large-model training, inference, and others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11551v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrik Okanovic, Grzegorz Kwasniewski, Paolo Sylos Labini, Maciej Besta, Flavio Vella, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>Understanding Data Movement in Tightly Coupled Heterogeneous Systems: A Case Study with the Grace Hopper Superchip</title>
      <link>https://arxiv.org/abs/2408.11556</link>
      <description>arXiv:2408.11556v1 Announce Type: new 
Abstract: Heterogeneous supercomputers have become the standard in HPC. GPUs in particular have dominated the accelerator landscape, offering unprecedented performance in parallel workloads and unlocking new possibilities in fields like AI and climate modeling. With many workloads becoming memory-bound, improving the communication latency and bandwidth within the system has become a main driver in the development of new architectures. The Grace Hopper Superchip (GH200) is a significant step in the direction of tightly coupled heterogeneous systems, in which all CPUs and GPUs share a unified address space and support transparent fine grained access to all main memory on the system. We characterize both intra- and inter-node memory operations on the Quad GH200 nodes of the new Swiss National Supercomputing Centre Alps supercomputer, and show the importance of careful memory placement on example workloads, highlighting tradeoffs and opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11556v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Luigi Fusco, Mikhail Khalilov, Marcin Chrapek, Giridhar Chukkapalli, Thomas Schulthess, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>Cost-Effective Big Data Orchestration Using Dagster: A Multi-Platform Approach</title>
      <link>https://arxiv.org/abs/2408.11635</link>
      <description>arXiv:2408.11635v1 Announce Type: new 
Abstract: The rapid advancement of big data technologies has underscored the need for robust and efficient data processing solutions. Traditional Spark-based Platform-as-a-Service (PaaS) solutions, such as Databricks and Amazon Web Services Elastic MapReduce, provide powerful analytics capabilities but often result in high operational costs and vendor lock-in issues. These platforms, while user-friendly, can lead to significant inefficiencies due to their cost structures and lack of transparent pricing. This paper introduces a cost-effective and flexible orchestration framework using Dagster. Our solution aims to reduce dependency on any single PaaS provider by integrating various Spark execution environments. We demonstrate how Dagster's orchestration capabilities can enhance data processing efficiency, enforce best coding practices, and significantly reduce operational costs. In our implementation, we achieved a 12% performance improvement over EMR and a 40% cost reduction compared to DBR, translating to over 300 euros saved per pipeline run. Our goal is to provide a flexible, developer-controlled computing environment that maintains or improves performance and scalability while mitigating the risks associated with vendor lock-in. The proposed framework supports rapid prototyping and testing, which is essential for continuous development and operational efficiency, contributing to a more sustainable model of large data processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11635v1</guid>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hernan Picatto, Georg Heiler, Peter Klimek</dc:creator>
    </item>
    <item>
      <title>Towards the Unmanned Aerial Vehicle Traffic Management Systems (UTMs): Security Risks and Challenges</title>
      <link>https://arxiv.org/abs/2408.11125</link>
      <description>arXiv:2408.11125v1 Announce Type: cross 
Abstract: Every aspect of our life depends on the ability to communicate effectively. Organizations that manage to establish communication routines, protocols and means thrive. An Aerial Traffic Management System operates similarly as an organization but certainly in a more strict manner. Third party agencies ensure several aspects of their functionality, the utmost to be consider safety. Many people take safety as granted but it is a pretty difficult part our daily functions. Thus, apart from digesting new things and habits of the new era, simultaneously we have to ensure safety in every part of it. It is true that the more data we produce, the more information we create and the more specialization we must introduce in order to be effective in a reasonable time basis. A Unmanned Aircraft System Traffic Management (UTM) is a system that consists of miscellaneous modules where each of them needs its consideration regarding safety. In other words, a UTM is the state-of-the-art system that demand a high quality of services and specialization, if we need to consider them reliable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11125v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantinos Spalas</dc:creator>
    </item>
    <item>
      <title>Topological Characterization of Consensus in Distributed Systems</title>
      <link>https://arxiv.org/abs/1905.09590</link>
      <description>arXiv:1905.09590v3 Announce Type: replace 
Abstract: We provide a complete characterization of both uniform and non-uniform deterministic consensus solvability in distributed systems with benign process and communication faults using point-set topology. More specifically, we non-trivially extend the approach introduced by Alpern and Schneider in 1985, by introducing novel fault-aware topologies on the space of infinite executions: the process-view topology, induced by a distance function that relies on the local view of a given process in an execution, and the minimum topology, which is induced by a distance function that focuses on the local view of the process that is the last to distinguish two executions. Consensus is solvable in a given model if and only if the sets of admissible executions leading to different decision values is disconnected in these topologies. By applying our approach to a wide range of different applications, we provide a topological explanation of a number of existing algorithms and impossibility results and develop several new ones, including a general equivalence of the strong and weak validity conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:1905.09590v3</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Nowak, Ulrich Schmid, Kyrill Winkler</dc:creator>
    </item>
    <item>
      <title>Colorless Tasks and Extension-Based Proofs</title>
      <link>https://arxiv.org/abs/2303.14769</link>
      <description>arXiv:2303.14769v2 Announce Type: replace 
Abstract: The concept of extension-based proofs models the idea of a valency argument, which is widely used in distributed computing. Extension-based proofs are limited in power: it has been shown that there is no extension-based proof of the impossibility of a wait-free protocol for $(n,k)$-set agreement among $n &gt; k \geq 2$ processes. There are only a few tasks that have been proven to have no extension-based proof of the impossibility, since the techniques in these works are closely related to the specific task.
  We give a necessary and sufficient condition for colorless tasks to have no extension-based proofs of the impossibility of wait-free protocols in the NIIS model. We introduce a general adversarial strategy decoupled from any concrete task specification. In this strategy, some properties of the chromatic subdivision that is widely used in distributed computing are proved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.14769v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yusong Shi, Weidong Liu</dc:creator>
    </item>
    <item>
      <title>Carbon Connect: An Ecosystem for Sustainable Computing</title>
      <link>https://arxiv.org/abs/2405.13858</link>
      <description>arXiv:2405.13858v2 Announce Type: replace 
Abstract: Computing is at a moment of profound opportunity. Emerging applications -- such as capable artificial intelligence, immersive virtual realities, and pervasive sensor systems -- drive unprecedented demand for computer. Despite recent advances toward net zero carbon emissions, the computing industry's gross energy usage continues to rise at an alarming rate, outpacing the growth of new energy installations and renewable energy deployments. A shift towards sustainability is needed to spark a transformation in how computer systems are manufactured, allocated, and consumed. Carbon Connect envisions coordinated research thrusts that produce design and management strategies for sustainable, next-generation computer systems. These strategies must flatten and then reverse growth trajectories for computing power and carbon for society's most rapidly growing applications such as artificial intelligence and virtual spaces. We will require accurate models for carbon accounting in computing technology. For embodied carbon, we must re-think conventional design strategies -- over-provisioned monolithic servers, frequent hardware refresh cycles, custom silicon -- and adopt life-cycle design strategies that more effectively reduce, reuse and recycle hardware at scale. For operational carbon, we must not only embrace renewable energy but also design systems to use that energy more efficiently. Finally, new hardware design and management strategies must be cognizant of economic policy and regulatory landscape, aligning private initiatives with societal goals. Many of these broader goals will require computer scientists to develop deep, enduring collaborations with researchers in economics, law, and industrial ecology to spark change in broader practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13858v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin C. Lee, David Brooks, Arthur van Benthem, Udit Gupta, Gage Hills, Vincent Liu, Benjamin Pierce, Christopher Stewart, Emma Strubell, Gu-Yeon Wei, Adam Wierman, Yuan Yao, Minlan Yu</dc:creator>
    </item>
    <item>
      <title>Borel Vizing's Theorem for Graphs of Subexponential Growth</title>
      <link>https://arxiv.org/abs/2307.00095</link>
      <description>arXiv:2307.00095v2 Announce Type: replace-cross 
Abstract: We show that every Borel graph $G$ of subexponential growth has a Borel proper edge-coloring with $\Delta(G) + 1$ colors. We deduce this from a stronger result, namely that an $n$-vertex (finite) graph $G$ of subexponential growth can be properly edge-colored using $\Delta(G) + 1$ colors by an $O(\log^\ast n)$-round deterministic distributed algorithm in the $\mathsf{LOCAL}$ model, where the implied constants in the $O(\cdot)$ notation are determined by a bound on the growth rate of $G$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00095v2</guid>
      <category>math.CO</category>
      <category>cs.DC</category>
      <category>math.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Bernshteyn, Abhishek Dhawan</dc:creator>
    </item>
    <item>
      <title>SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless Functions</title>
      <link>https://arxiv.org/abs/2403.17574</link>
      <description>arXiv:2403.17574v2 Announce Type: replace-cross 
Abstract: As an emerging cloud computing deployment paradigm, serverless computing is gaining traction due to its efficiency and ability to harness on-demand cloud resources. However, a significant hurdle remains in the form of the cold start problem, causing latency when launching new function instances from scratch. Existing solutions tend to use over-simplistic strategies for function pre-loading/unloading without full invocation pattern exploitation, rendering unsatisfactory optimization of the trade-off between cold start latency and resource waste. To bridge this gap, we propose SPES, the first differentiated scheduler for runtime cold start mitigation by optimizing serverless function provision. Our insight is that the common architecture of serverless systems prompts the concentration of certain invocation patterns, leading to predictable invocation behaviors. This allows us to categorize functions and pre-load/unload proper function instances with finer-grained strategies based on accurate invocation prediction. Experiments demonstrate the success of SPES in optimizing serverless function provision on both sides: reducing the 75th-percentile cold start rates by 49.77% and the wasted memory time by 56.43%, compared to the state-of-the-art. By mitigating the cold start issue, SPES is a promising advancement in facilitating cloud services deployed on serverless architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17574v2</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Cheryl Lee, Zhouruixing Zhu, Tianyi Yang, Yintong Huo, Yuxin Su, Pinjia He, Michael R. Lyu</dc:creator>
    </item>
    <item>
      <title>Federated Learning for Collaborative Inference Systems: The Case of Early Exit Networks</title>
      <link>https://arxiv.org/abs/2405.04249</link>
      <description>arXiv:2405.04249v2 Announce Type: replace-cross 
Abstract: As Internet of Things (IoT) technology advances, end devices like sensors and smartphones are progressively equipped with AI models tailored to their local memory and computational constraints. Local inference reduces communication costs and latency; however, these smaller models typically underperform compared to more sophisticated models deployed on edge servers or in the cloud. Cooperative Inference Systems (CISs) address this performance trade-off by enabling smaller devices to offload part of their inference tasks to more capable devices. These systems often deploy hierarchical models that share numerous parameters, exemplified by Deep Neural Networks (DNNs) that utilize strategies like early exits or ordered dropout. In such instances, Federated Learning (FL) may be employed to jointly train the models within a CIS. Yet, traditional training methods have overlooked the operational dynamics of CISs during inference, particularly the potential high heterogeneity in serving rates across clients. To address this gap, we propose a novel FL approach designed explicitly for use in CISs that accounts for these variations in serving rates. Our framework not only offers rigorous theoretical guarantees, but also surpasses state-of-the-art (SOTA) training algorithms for CISs, especially in scenarios where inference request rates or data availability are uneven among clients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04249v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caelin Kaplan, Angelo Rodio, Tareq Si Salem, Chuan Xu, Giovanni Neglia</dc:creator>
    </item>
  </channel>
</rss>
