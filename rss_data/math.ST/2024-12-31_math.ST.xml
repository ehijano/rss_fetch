<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 05:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Covariance test and universal bootstrap by operator norm</title>
      <link>https://arxiv.org/abs/2412.20019</link>
      <description>arXiv:2412.20019v1 Announce Type: new 
Abstract: Testing covariance matrices is crucial in high-dimensional statistics. Traditional methods based on the Frobenius and supremum norms are widely used but often treat the covariance matrix as a vector and neglect its inherent matrix structure. This paper introduces a new testing framework based on the operator norm, designed to capture the spectral properties of the covariance matrix more accurately. The commonly used empirical bootstrap and multiplier bootstrap methods are shown to fail for operator norm-based statistics. To derive the critical values of this type of statistics, we propose a universal bootstrap procedure, utilizing the concept of universality from random matrix theory. Our method demonstrates consistency across both high- and ultra-high-dimensional regimes, accommodating scenarios where the dimension-to-sample-size ratio $p/n$ converges to a nonzero constant or diverges to infinity. As a byproduct, we provide the first proof of the Tracy-Widom law for the largest eigenvalue of sample covariances with non-Gaussian entries as $p/n \to \infty$. We also show such universality does not hold for the Frobenius norm and supremum norm statistics. Extensive simulations and a real-world data study support our findings, highlighting the favorable finite sample performance of the proposed operator norm-based statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20019v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guoyu Zhang, Dandan Jiang, Fang Yao</dc:creator>
    </item>
    <item>
      <title>Estimation of conditional inequality measures</title>
      <link>https://arxiv.org/abs/2412.20228</link>
      <description>arXiv:2412.20228v1 Announce Type: new 
Abstract: Classical inequality measures such as the Gini index are often used to describe the sparsity of the distribution of a certain feature in a population. It is sometimes also used to compare the inequalities between some subpopulations, conditioned on certain values of the covariates. The concept of measuring inequality in subpopulation was described in the literature and it is strongly related to the decomposition of the Gini index. In this paper, the idea of conditional inequality measures is extended to the case where covariates are continuous. Curves of conditional inequality measures are introduced, especially, the curves of the conditional quantile versions of the Zenga and $D$ indices are considered. Various methods of their estimation based on quantile regression are presented. An approach using isotonic regression is used to prevent quantile crossing in quantile regression. The accuracy of the estimators considered is compared in simulation studies. Furthermore, an analysis of the growth in salary inequalities with respect to employee age is included to demonstrate the potential of conditional inequality measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20228v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicja Jokiel-Rokita, Sylwester Pi\k{a}tek, Rafa{\l} Topolnicki</dc:creator>
    </item>
    <item>
      <title>A Particle Algorithm for Mean-Field Variational Inference</title>
      <link>https://arxiv.org/abs/2412.20385</link>
      <description>arXiv:2412.20385v1 Announce Type: new 
Abstract: Variational inference is a fast and scalable alternative to Markov chain Monte Carlo and has been widely applied to posterior inference tasks in statistics and machine learning. A traditional approach for implementing mean-field variational inference (MFVI) is coordinate ascent variational inference (CAVI), which relies crucially on parametric assumptions on complete conditionals. In this paper, we introduce a novel particle-based algorithm for mean-field variational inference, which we term PArticle VI (PAVI). Notably, our algorithm does not rely on parametric assumptions on complete conditionals, and it applies to the nonparametric setting. We provide non-asymptotic finite-particle convergence guarantee for our algorithm. To our knowledge, this is the first end-to-end guarantee for particle-based MFVI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20385v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiang Du, Kaizheng Wang, Edith Zhang, Chenyang Zhong</dc:creator>
    </item>
    <item>
      <title>On the supremum of the sets of copulas with given curvilinear section</title>
      <link>https://arxiv.org/abs/2412.20629</link>
      <description>arXiv:2412.20629v1 Announce Type: new 
Abstract: By employing the total variation of certain functions, we give an explicit formula for the supremum of the set of all copulas with a given curvilinear section. This supremum being a copula is characterized and the relationship between this supremum and that of the set of all quasi-copulas with the same curvilinear section is considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20629v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Ouyang, Yonghui Sun, Hua-Peng Zhang</dc:creator>
    </item>
    <item>
      <title>Adaptive Bayes estimator for stochastic differential equations with jumps under small noise asymptotics</title>
      <link>https://arxiv.org/abs/2412.20640</link>
      <description>arXiv:2412.20640v1 Announce Type: new 
Abstract: In this paper, we consider parameter estimation for stochastic differential equations driven by Wiener processes and compound Poisson processes. We assume unknown parameters corresponding to coefficients of the drift term, diffusion term, and jump term, as well as the Poisson intensity and the probability density function of the underlying jump. We propose estimators based on adaptive Bayesian estimation from discrete observations. We demonstrate the consistency and asymptotic normality of the estimators within the framework of small noise asymptotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20640v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuntaro Suzuki, Takaaki Wakamatsu, Yasutaka Shimizu</dc:creator>
    </item>
    <item>
      <title>Sparse PCA: Phase Transitions in the Critical Sparsity Regime</title>
      <link>https://arxiv.org/abs/2412.21038</link>
      <description>arXiv:2412.21038v1 Announce Type: new 
Abstract: This work studies estimation of sparse principal components in high dimensions. Specifically, we consider a class of estimators based on kernel PCA, generalizing the covariance thresholding algorithm proposed by Krauthgamer et al. (2015). Focusing on Johnstone's spiked covariance model, we investigate the "critical" sparsity regime, where the sparsity level $m$, sample size $n$, and dimension $p$ each diverge and $m/\sqrt{n} \rightarrow \beta$, $p/n \rightarrow \gamma$.
  Within this framework, we develop a fine-grained understanding of signal detection and recovery. Our results establish a detectability phase transition, analogous to the Baik--Ben Arous--P\'ech\'e (BBP) transition: above a certain threshold -- depending on the kernel function, $\gamma$, and $\beta$ -- kernel PCA is informative. Conversely, below the threshold, kernel principal components are asymptotically orthogonal to the signal. Notably, above this detection threshold, we find that consistent support recovery is possible with high probability. Sparsity plays a key role in our analysis, and results in more nuanced phenomena than in related studies of kernel PCA with delocalized (dense) components. Finally, we identify optimal kernel functions for detection -- and consequently, support recovery -- and numerical calculations suggest that soft thresholding is nearly optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21038v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael J. Feldman, Theodor Misiakiewicz, Elad Romanov</dc:creator>
    </item>
    <item>
      <title>Optimal e-value testing for properly constrained hypotheses</title>
      <link>https://arxiv.org/abs/2412.21125</link>
      <description>arXiv:2412.21125v1 Announce Type: new 
Abstract: Hypothesis testing via e-variables can be framed as a sequential betting game, where a player each round picks an e-variable. A good player's strategy results in an effective statistical test that rejects the null hypothesis as soon as sufficient evidence arises. Building on recent advances, we address the question of restricting the pool of e-variables to simplify strategy design without compromising effectiveness. We extend the results of Clerico(2024), by characterising optimal sets of e-variables for a broad class of non-parametric hypothesis tests, defined by finitely many regular constraints. As an application, we discuss optimality in algorithmic mean estimation, including the case of heavy-tailed random variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21125v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugenio Clerico</dc:creator>
    </item>
    <item>
      <title>Low coordinate degree algorithms II: Categorical signals and generalized stochastic block models</title>
      <link>https://arxiv.org/abs/2412.21155</link>
      <description>arXiv:2412.21155v1 Announce Type: new 
Abstract: We study when low coordinate degree functions (LCDF) -- linear combinations of functions depending on small subsets of entries of a vector -- can test for the presence of categorical structure, including community structure and generalizations thereof, in high-dimensional data. This complements the first paper of this series, which studied the power of LCDF in testing for continuous structure like real-valued signals perturbed by additive noise. We apply the tools developed there to a general form of stochastic block model (SBM), where a population is assigned random labels and every $p$-tuple of the population generates an observation according to an arbitrary probability measure associated to the $p$ labels of its members. We show that the performance of LCDF admits a unified analysis for this class of models. As applications, we prove tight lower bounds against LCDF (and therefore also against low degree polynomials) for nearly arbitrary graph and regular hypergraph SBMs, always matching suitable generalizations of the Kesten-Stigum threshold. We also prove tight lower bounds for group synchronization and abelian group sumset problems under the "truth-or-Haar" noise model, and use our technical results to give an improved analysis of Gaussian multi-frequency group synchronization. In most of these models, for some parameter settings our lower bounds give new evidence for conjectural statistical-to-computational gaps. Finally, interpreting some of our findings, we propose a precise analogy between categorical and continuous signals: a general SBM as above behaves, in terms of the tradeoff between subexponential runtime cost of testing algorithms and the signal strength needed for a testing algorithm to succeed, like a spiked $p_*$-tensor model of a certain order $p_*$ that may be computed from the parameters of the SBM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21155v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitriy Kunisky</dc:creator>
    </item>
    <item>
      <title>Debiased Nonparametric Regression for Statistical Inference and Distributionally Robustness</title>
      <link>https://arxiv.org/abs/2412.20173</link>
      <description>arXiv:2412.20173v1 Announce Type: cross 
Abstract: This study proposes a debiasing method for smooth nonparametric estimators. While machine learning techniques such as random forests and neural networks have demonstrated strong predictive performance, their theoretical properties remain relatively underexplored. Specifically, many modern algorithms lack assurances of pointwise asymptotic normality and uniform convergence, which are critical for statistical inference and robustness under covariate shift and have been well-established for classical methods like Nadaraya-Watson regression. To address this, we introduce a model-free debiasing method that guarantees these properties for smooth estimators derived from any nonparametric regression approach. By adding a correction term that estimates the conditional expected residual of the original estimator, or equivalently, its estimation error, we obtain a debiased estimator with proven pointwise asymptotic normality, uniform convergence, and Gaussian process approximation. These properties enable statistical inference and enhance robustness to covariate shift, making the method broadly applicable to a wide range of nonparametric regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20173v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Well-posedness and approximation of reflected McKean-Vlasov SDEs with applications</title>
      <link>https://arxiv.org/abs/2412.20247</link>
      <description>arXiv:2412.20247v1 Announce Type: cross 
Abstract: In this paper, we establish well-posedness of reflected McKean-Vlasov SDEs and their particle approximations in smooth non-convex domains. We prove convergence of the interacting particle system to the corresponding mean-field limit with the optimal rate of convergence. We motivate this study with applications to sampling and optimization in constrained domains by considering reflected mean-field Langevin SDEs and two reflected consensus-based optimization (CBO) models, respectively. We utilize reflection coupling to study long-time behaviour of reflected mean-field SDEs and also investigate convergence of the reflected CBO models to the global minimum of a constrained optimization problem. We numerically test reflected CBO models on benchmark constrained optimization problems and an inverse problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20247v1</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. D. Hinds, A. Sharma, M. V. Tretyakov</dc:creator>
    </item>
    <item>
      <title>On the Missing Factor in Some Concentration Inequalities for Martingales</title>
      <link>https://arxiv.org/abs/2412.20542</link>
      <description>arXiv:2412.20542v1 Announce Type: cross 
Abstract: In this note, we improve some concentration inequalities for martingales with bounded increments. These results recover the missing factor in Freedman-style inequalities and are near optimal. We also provide minor refinements of concentration inequalities for functions of independent random variables. These proofs use techniques from the works of Bentkus and Pinelis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20542v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Uncertainty of high-dimensional genetic data prediction with polygenic risk scores</title>
      <link>https://arxiv.org/abs/2412.20611</link>
      <description>arXiv:2412.20611v1 Announce Type: cross 
Abstract: In many predictive tasks, there are a large number of true predictors with weak signals, leading to substantial uncertainties in prediction outcomes. The polygenic risk score (PRS) is an example of such a scenario, where many genetic variants are used as predictors for complex traits, each contributing only a small amount of information. Although PRS has been a standard tool in genetic predictions, its uncertainty remains largely unexplored. In this paper, we aim to establish the asymptotic normality of PRS in high-dimensional predictions without sparsity constraints. We investigate the popular marginal and ridge-type estimators in PRS applications, developing central limit theorems for both individual-level predicted values (e.g., genetically predicted human height) and cohort-level prediction accuracy measures (e.g., overall predictive $R$-squared in the testing dataset). Our results demonstrate that ignoring the prediction-induced uncertainty can lead to substantial underestimation of the true variance of PRS-based estimators, which in turn may cause overconfidence in the accuracy of confidence intervals and hypothesis testing. These findings provide key insights omitted by existing first-order asymptotic studies of high-dimensional sparsity-free predictions, which often focus solely on the point limits of predictive risks. We develop novel and flexible second-order random matrix theory results to assess the asymptotic normality of functionals with a general covariance matrix, without assuming Gaussian distributions for the data. We evaluate our theoretical results through extensive numerical analyses using real data from the UK Biobank. Our analysis underscores the importance of incorporating uncertainty assessments at both the individual and cohort levels when applying and interpreting PRS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20611v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoxuan Fu, Jiaoyang Huang, Zirui Fan, Bingxin Zhao</dc:creator>
    </item>
    <item>
      <title>From sparse to dense functional time series: phase transitions of detecting structural breaks and beyond</title>
      <link>https://arxiv.org/abs/2412.20858</link>
      <description>arXiv:2412.20858v1 Announce Type: cross 
Abstract: We develop a novel methodology for detecting abrupt break points in mean functions of functional time series, adaptable to arbitrary sampling schemes. By employing B-spline smoothing, we introduce $\mathcal L_{\infty}$ and $\mathcal L_2$ test statistics statistics based on a smoothed cumulative summation (CUMSUM) process, and derive the corresponding asymptotic distributions under the null and local alternative hypothesis, as well as the phase transition boundary from sparse to dense. We further establish the convergence rate of the proposed break point estimators and conduct statistical inference on the jump magnitude based on the estimated break point, also applicable across sparsely, semi-densely, and densely, observed random functions. Extensive numerical experiments validate the effectiveness of the proposed procedures. To illustrate the practical relevance, we apply the developed methods to analyze electricity price data and temperature data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20858v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leheng Cai, Qirui Hu</dc:creator>
    </item>
    <item>
      <title>Average partial effect estimation using double machine learning</title>
      <link>https://arxiv.org/abs/2308.09207</link>
      <description>arXiv:2308.09207v2 Announce Type: replace 
Abstract: Single-parameter summaries of variable effects are desirable for ease of interpretation, but linear models, which would deliver these, may fit poorly to the data. A modern approach is to estimate the average partial effect -- the average slope of the regression function with respect to the predictor of interest -- using a doubly robust semiparametric procedure. Most existing work has focused on specific forms of nuisance function estimators. We extend the scope to arbitrary plug-in nuisance function estimation, allowing for the use of modern machine learning methods which in particular may deliver non-differentiable regression function estimates. Our procedure involves resmoothing a user-chosen first-stage regression estimator to produce a differentiable version, and modelling the conditional distribution of the predictors through a location-scale model. We show that our proposals lead to a semiparametric efficient estimator under relatively weak assumptions. Our theory makes use of a new result on the sub-Gaussianity of Lipschitz score functions that may be of independent interest. We demonstrate the attractive numerical performance of our approach in a variety of settings including ones with misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09207v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harvey Klyne, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning</title>
      <link>https://arxiv.org/abs/2405.04715</link>
      <description>arXiv:2405.04715v3 Announce Type: replace 
Abstract: Pursuing causality from data is a fundamental problem in scientific discovery, treatment intervention, and transfer learning. This paper introduces a novel algorithmic method for addressing nonparametric invariance and causality learning in regression models across multiple environments, where the joint distribution of response variables and covariates varies, but the conditional expectations of outcome given an unknown set of quasi-causal variables are invariant. The challenge of finding such an unknown set of quasi-causal or invariant variables is compounded by the presence of endogenous variables that have heterogeneous effects across different environments. The proposed Focused Adversarial Invariant Regularization (FAIR) framework utilizes an innovative minimax optimization approach that drives regression models toward prediction-invariant solutions through adversarial testing. Leveraging the representation power of neural networks, FAIR neural networks (FAIR-NN) are introduced for causality pursuit. It is shown that FAIR-NN can find the invariant variables and quasi-causal variables under a minimal identification condition and that the resulting procedure is adaptive to low-dimensional composition structures in a non-asymptotic analysis. Under a structural causal model, variables identified by FAIR-NN represent pragmatic causality and provably align with exact causal mechanisms under conditions of sufficient heterogeneity. Computationally, FAIR-NN employs a novel Gumbel approximation with decreased temperature and a stochastic gradient descent ascent algorithm. The procedures are demonstrated using simulated and real-data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04715v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Gu, Cong Fang, Peter B\"uhlmann, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Freedom in constructing quasi-copulas vs. copulas</title>
      <link>https://arxiv.org/abs/2407.15393</link>
      <description>arXiv:2407.15393v3 Announce Type: replace 
Abstract: The main goal of this paper is to study the extent of freedom one has in constructing quasi-copulas vs. copulas. Specifically, it exhibits three construction methods for quasi-copulas based on recent developments: a representation of multivariate quasi-copulas by means of infima and suprema of copulas, an extension of a classical result on shuffles of min to the setting of quasi-copulas, and a construction method for quasi-copulas obeying a given signed mass pattern on a patch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15393v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matja\v{z} Omladi\v{c}, Nik Stopar</dc:creator>
    </item>
    <item>
      <title>Local linear smoothing for regression surfaces on the simplex using Dirichlet kernels</title>
      <link>https://arxiv.org/abs/2408.07209</link>
      <description>arXiv:2408.07209v2 Announce Type: replace 
Abstract: This paper introduces a local linear smoother for regression surfaces on the simplex. The estimator solves a least-squares regression problem weighted by a locally adaptive Dirichlet kernel, ensuring good boundary properties. Asymptotic results for the bias, variance, mean squared error, and mean integrated squared error are derived, generalizing the univariate results of Chen [Ann. Inst. Statist. Math., 54(2) (2002), pp. 312-323]. A simulation study shows that the proposed local linear estimator with Dirichlet kernel outperforms its only direct competitor in the literature, the Nadaraya-Watson estimator with Dirichlet kernel due to Bouzebda, Nezzal and Elhattab [AIMS Math., 9(9) (2024), pp. 26195-26282].</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07209v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Central limit theorems for vector-valued composite functionals with smoothing and applications</title>
      <link>https://arxiv.org/abs/2412.19367</link>
      <description>arXiv:2412.19367v2 Announce Type: replace 
Abstract: This paper focuses on vector-valued composite functionals, which may be nonlinear in probability. Our primary goal is to establish central limit theorems for these functionals when mixed estimators are employed. Our study is relevant to the evaluation and comparison of risk in decision-making contexts and extends to functionals that arise in machine learning methods. A generalized family of composite risk functionals is presented, which encompasses most of the known coherent risk measures including systemic measures of risk. The paper makes two main contributions. First, we analyze vector-valued functionals, providing a framework for evaluating high-dimensional risks. This framework facilitates the comparison of multiple risk measures, as well as the estimation and asymptotic analysis of systemic risk and its optimal value in decision-making problems. Second, we derive novel central limit theorems for optimized composite functionals when mixed types of estimators: empirical and smoothed estimators are used. We provide verifiable sufficient conditions for the central limit formulae and show their applicability to several popular measures of risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19367v2</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huhui Chen, Darinka Dentcheva, Yang Lin, Gregory J. Stock</dc:creator>
    </item>
    <item>
      <title>Functional Limit Theorems for Hawkes Processes</title>
      <link>https://arxiv.org/abs/2401.11495</link>
      <description>arXiv:2401.11495v3 Announce Type: replace-cross 
Abstract: We prove that the long-run behavior of Hawkes processes is fully determined by the average number and the dispersion of child events. For subcritical processes we provide FLLNs and FCLTs under minimal conditions on the kernel of the process with the precise form of the limit theorems depending strongly on the dispersion of child events. For a critical Hawkes process with weakly dispersed child events, functional central limit theorems do not hold. Instead, we prove that the rescaled intensity processes and rescaled Hawkes processes behave like CIR-processes without mean-reversion, respectively integrated CIR-processes. We provide the rate of convergence by establishing an upper bound on the Wasserstein distance between the distributions of rescaled Hawkes process and the corresponding limit process. By contrast, critical Hawkes process with heavily dispersed child events share many properties of subcritical ones. In particular, functional limit theorems hold. However, unlike subcritical processes critical ones with heavily dispersed child events display long-range dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11495v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00440-024-01348-3</arxiv:DOI>
      <dc:creator>Ulrich Horst, Wei Xu</dc:creator>
    </item>
    <item>
      <title>Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces</title>
      <link>https://arxiv.org/abs/2402.04691</link>
      <description>arXiv:2402.04691v3 Announce Type: replace-cross 
Abstract: This study investigates leveraging stochastic gradient descent (SGD) to learn operators between general Hilbert spaces. We propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity. Under these conditions, we establish upper bounds for convergence rates of the SGD algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the SGD algorithm. It is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning. We show that the SGD estimator will converge to the best linear approximation of the nonlinear target operator. Moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel Hilbert spaces yields new convergence results, thereby refining the conclusions of existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04691v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Shi, Jia-Qi Yang</dc:creator>
    </item>
  </channel>
</rss>
