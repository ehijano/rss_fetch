<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Sep 2024 01:58:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Data-driven Viscosity Solver for Fluid Simulation</title>
      <link>https://arxiv.org/abs/2409.14653</link>
      <description>arXiv:2409.14653v1 Announce Type: new 
Abstract: We propose a data-driven viscosity solver based on U-shaped convolutional neural network to predict velocity changes due to viscosity. Our solver takes velocity derivatives, fluid volume, and solid indicator quantities as input. The traditional marker-and-cell (MAC) grid stores velocities at the edges of the grid, causing the dimensions of the velocity field vary from axis to axis. In our work, we suggest a symmetric MAC grid that maintains consistent dimensions across axes without interpolation or symmetry breaking. The proposed grid effectively transfers spatial fluid quantities such as partial derivatives of velocity, enabling networks to generate accurate predictions. Additionally, we introduce a physics-based loss inspired by the variational formulation of viscosity to enhance the network's generalization for a wide range of viscosity coefficients. We demonstrate various fluid simulation results, including 2D and 3D fluid-rigid body scenes and a scene exhibiting the buckling effect. Our code is available at \url{https://github.com/SSTDV-Project/python-fluid-simulation.}</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14653v1</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5220/0012397300003660</arxiv:DOI>
      <dc:creator>Wonjung Park, Hyunsoo Kim, Jinah Park</dc:creator>
    </item>
    <item>
      <title>Hybrid Drawing Solutions in AR Bitmap-to-Vector Techniques on 3D Surfaces</title>
      <link>https://arxiv.org/abs/2409.15171</link>
      <description>arXiv:2409.15171v1 Announce Type: new 
Abstract: Recent advancements in augmented reality and virtual reality have significantly enhanced workflows for drawing 3D objects. Despite these technological strides, existing AR tools often lack the necessary precision and struggle to maintain quality when scaled, posing challenges for larger-scale drawing tasks. This paper introduces a novel AR tool that uniquely integrates bitmap drawing and vectorization techniques. This integration allows engineers to perform rapid, real-time drawings directly on 3D models, with the capability to vectorize the data for scalable accuracy and editable points, ensuring no loss in fidelity when modifying or resizing the drawings. We conducted user studies involving professional engineers, designers, and contractors to evaluate the tool's integration into existing workflows, its usability, and its impact on project outcomes. The results demonstrate that our enhancements significantly improve the efficiency of drawing processes. Specifically, the ability to perform quick, editable, and scalable drawings directly on 3D models not only enhances productivity but also ensures adaptability across various project sizes and complexities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15171v1</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pengcheng Ding, Yedian Cheng, Mirjana Prpa</dc:creator>
    </item>
    <item>
      <title>Intrinsic Single-Image HDR Reconstruction</title>
      <link>https://arxiv.org/abs/2409.13803</link>
      <description>arXiv:2409.13803v1 Announce Type: cross 
Abstract: The low dynamic range (LDR) of common cameras fails to capture the rich contrast in natural scenes, resulting in loss of color and details in saturated pixels. Reconstructing the high dynamic range (HDR) of luminance present in the scene from single LDR photographs is an important task with many applications in computational photography and realistic display of images. The HDR reconstruction task aims to infer the lost details using the context present in the scene, requiring neural networks to understand high-level geometric and illumination cues. This makes it challenging for data-driven algorithms to generate accurate and high-resolution results. In this work, we introduce a physically-inspired remodeling of the HDR reconstruction problem in the intrinsic domain. The intrinsic model allows us to train separate networks to extend the dynamic range in the shading domain and to recover lost color details in the albedo domain. We show that dividing the problem into two simpler sub-tasks improves performance in a wide variety of photographs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13803v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sebastian Dille, Chris Careaga, Ya\u{g}{\i}z Aksoy</dc:creator>
    </item>
    <item>
      <title>PanoCoach: Enhancing Tactical Coaching and Communication in Soccer with Mixed-Reality Telepresence</title>
      <link>https://arxiv.org/abs/2409.13859</link>
      <description>arXiv:2409.13859v1 Announce Type: cross 
Abstract: Soccer, as a dynamic team sport, requires seamless coordination and integration of tactical strategies across all players. Adapting to new tactical systems is a critical but often challenging aspect of soccer at all professional levels. Even the best players can struggle with this process, primarily due to the complexities of conveying and internalizing intricate tactical patterns. Traditional communication methods like whiteboards, on-field instructions, and video analysis often present significant difficulties in perceiving spatial relationships, anticipating team movements, and facilitating live conversation during training sessions. These challenges can lead to inconsistent interpretations of the coach's tactics among players, regardless of their skill level. To bridge the gap between tactical communication and physical execution, we propose a mixed-reality telepresence solution designed to support multi-view tactical explanations during practice. Our concept involves a multi-screen setup combining a tablet for coaches to annotate and demonstrate concepts in both 2D and 3D views, alongside VR to immerse athletes in a first-person perspective, allowing them to experience a sense of presence during coaching. Demo video uploaded at https://youtu.be/O7o4Wzd-7rw</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13859v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Kang, Hanspeter Pfister, Tica Lin</dc:creator>
    </item>
    <item>
      <title>Content-aware Tile Generation using Exterior Boundary Inpainting</title>
      <link>https://arxiv.org/abs/2409.14184</link>
      <description>arXiv:2409.14184v1 Announce Type: cross 
Abstract: We present a novel and flexible learning-based method for generating tileable image sets. Our method goes beyond simple self-tiling, supporting sets of mutually tileable images that exhibit a high degree of diversity. To promote diversity we decouple structure from content by foregoing explicit copying of patches from an exemplar image. Instead we leverage the prior knowledge of natural images and textures embedded in large-scale pretrained diffusion models to guide tile generation constrained by exterior boundary conditions and a text prompt to specify the content. By carefully designing and selecting the exterior boundary conditions, we can reformulate the tile generation process as an inpainting problem, allowing us to directly employ existing diffusion-based inpainting models without the need to retrain a model on a custom training set. We demonstrate the flexibility and efficacy of our content-aware tile generation method on different tiling schemes, such as Wang tiles, from only a text prompt. Furthermore, we introduce a novel Dual Wang tiling scheme that provides greater texture continuity and diversity than existing Wang tile variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14184v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3687981</arxiv:DOI>
      <dc:creator>Sam Sartor, Pieter Peers</dc:creator>
    </item>
    <item>
      <title>SynBench: A Synthetic Benchmark for Non-rigid 3D Point Cloud Registration</title>
      <link>https://arxiv.org/abs/2409.14474</link>
      <description>arXiv:2409.14474v1 Announce Type: cross 
Abstract: Non-rigid point cloud registration is a crucial task in computer vision. Evaluating a non-rigid point cloud registration method requires a dataset with challenges such as large deformation levels, noise, outliers, and incompleteness. Despite the existence of several datasets for deformable point cloud registration, the absence of a comprehensive benchmark with all challenges makes it difficult to achieve fair evaluations among different methods. This paper introduces SynBench, a new non-rigid point cloud registration dataset created using SimTool, a toolset for soft body simulation in Flex and Unreal Engine. SynBench provides the ground truth of corresponding points between two point sets and encompasses key registration challenges, including varying levels of deformation, noise, outliers, and incompleteness. To the best of the authors' knowledge, compared to existing datasets, SynBench possesses three particular characteristics: (1) it is the first benchmark that provides various challenges for non-rigid point cloud registration, (2) SynBench encompasses challenges of varying difficulty levels, and (3) it includes ground truth corresponding points both before and after deformation. The authors believe that SynBench enables future non-rigid point cloud registration methods to present a fair comparison of their achievements. SynBench is publicly available at: https://doi.org/10.11588/data/R9IKCF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14474v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Monji-Azad, Marvin Kinz, Claudia Scherl, David M\"annle, J\"urgen Hesser, Nikolas L\"ow</dc:creator>
    </item>
    <item>
      <title>Dynamic Realms: 4D Content Analysis, Recovery and Generation with Geometric, Topological and Physical Priors</title>
      <link>https://arxiv.org/abs/2409.14692</link>
      <description>arXiv:2409.14692v1 Announce Type: cross 
Abstract: My research focuses on the analysis, recovery, and generation of 4D content, where 4D includes three spatial dimensions (x, y, z) and a temporal dimension t, such as shape and motion. This focus goes beyond static objects to include dynamic changes over time, providing a comprehensive understanding of both spatial and temporal variations. These techniques are critical in applications like AR/VR, embodied AI, and robotics. My research aims to make 4D content generation more efficient, accessible, and higher in quality by incorporating geometric, topological, and physical priors. I also aim to develop effective methods for 4D content recovery and analysis using these priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14692v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhiyang Dou</dc:creator>
    </item>
    <item>
      <title>Human Hair Reconstruction with Strand-Aligned 3D Gaussians</title>
      <link>https://arxiv.org/abs/2409.14778</link>
      <description>arXiv:2409.14778v1 Announce Type: cross 
Abstract: We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data. In contrast to recent approaches that leverage unstructured Gaussians to model human avatars, our method reconstructs the hair using 3D polylines, or strands. This fundamental difference allows the use of the resulting hairstyles out-of-the-box in modern computer graphics engines for editing, rendering, and simulation. Our 3D lifting method relies on unstructured Gaussians to generate multi-view ground truth data to supervise the fitting of hair strands. The hairstyle itself is represented in the form of the so-called strand-aligned 3D Gaussians. This representation allows us to combine strand-based hair priors, which are essential for realistic modeling of the inner structure of hairstyles, with the differentiable rendering capabilities of 3D Gaussian Splatting. Our method, named Gaussian Haircut, is evaluated on synthetic and real scenes and demonstrates state-of-the-art performance in the task of strand-based hair reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14778v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Egor Zakharov, Vanessa Sklyarova, Michael Black, Giljoo Nam, Justus Thies, Otmar Hilliges</dc:creator>
    </item>
    <item>
      <title>Efficient Nearest Neighbor Search Using Dynamic Programming</title>
      <link>https://arxiv.org/abs/2409.15023</link>
      <description>arXiv:2409.15023v2 Announce Type: cross 
Abstract: When dealing with point clouds distributed on manifold surfaces in 3D space, or when the query point is far from the data, the efficiency of traditional nearest neighbor search algorithms (e.g., KD Tree and R Tree) may degrade. In extreme cases, the complexity of the query can approach O(n). In this paper, we propose a novel dynamic programming technique that precomputes a Directed Acyclic Graph (DAG) to enable more efficient nearest neighbor queries for 2D manifold data. By leveraging this structure, only a small number of distance comparisons between point pairs are required to accurately identify the nearest neighbor. Extensive experimental results demonstrate that our method achieves query speeds that are 1x-10x faster than traditional methods. Moreover, our algorithm exhibits significant potential. It achieves query efficiency comparable to KD-trees on uniformly distributed point clouds. Additionally, our algorithm supports nearest neighbor queries among the first k points. Coupled with our algorithm, a farthest point sampling algorithm with lower complexity can also be implemented. Furthermore, our method has the potential to support nearest neighbor queries with different types of primitives and distance metrics. We believe that the method proposed in this paper represents the most concise and straightforward exact nearest neighbor search algorithm currently available, and it will contribute significantly to advancements in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15023v2</guid>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Wang, Jiantao Song, Shiqing Xin, Shuangmin Chen, Changhe Tu, Wenping Wang, Jiaye Wang</dc:creator>
    </item>
    <item>
      <title>Compositional Neural Textures</title>
      <link>https://arxiv.org/abs/2404.12509</link>
      <description>arXiv:2404.12509v2 Announce Type: replace 
Abstract: Texture plays a vital role in enhancing visual richness in both real photographs and computer-generated imagery. However, the process of editing textures often involves laborious and repetitive manual adjustments of textons, which are the recurring local patterns that characterize textures. This work introduces a fully unsupervised approach for representing textures using a compositional neural model that captures individual textons. We represent each texton as a 2D Gaussian function whose spatial support approximates its shape, and an associated feature that encodes its detailed appearance. By modeling a texture as a discrete composition of Gaussian textons, the representation offers both expressiveness and ease of editing. Textures can be edited by modifying the compositional Gaussians within the latent space, and new textures can be efficiently synthesized by feeding the modified Gaussians through a generator network in a feed-forward manner. This approach enables a wide range of applications, including transferring appearance from an image texture to another image, diversifying textures,texture interpolation, revealing/modifying texture variations, edit propagation, texture animation, and direct texton manipulation. The proposed approach contributes to advancing texture analysis, modeling, and editing techniques, and opens up new possibilities for creating visually appealing images with controllable textures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12509v2</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3680528.3687561</arxiv:DOI>
      <dc:creator>Peihan Tu, Li-Yi Wei, Matthias Zwicker</dc:creator>
    </item>
    <item>
      <title>Unified Gaussian Primitives for Scene Representation and Rendering</title>
      <link>https://arxiv.org/abs/2406.09733</link>
      <description>arXiv:2406.09733v2 Announce Type: replace 
Abstract: Searching for a unified scene representation remains a research challenge in computer graphics. Traditional mesh-based representations are unsuitable for dense, fuzzy elements, and introduce additional complexity for filtering and differentiable rendering. Conversely, voxel-based representations struggle to model hard surfaces and suffer from intensive memory requirement. We propose a general-purpose rendering primitive based on 3D Gaussian distribution for unified scene representation, featuring versatile appearance ranging from glossy surfaces to fuzzy elements, as well as physically based scattering to enable accurate global illumination. We formulate the rendering theory for the primitive based on non-exponential transport and derive efficient rendering operations to be compatible with Monte Carlo path tracing. The new representation can be converted from different sources, including meshes and 3D Gaussian splatting, and further refined via transmittance optimization thanks to its differentiability. We demonstrate the versatility of our representation in various rendering applications such as global illumination and appearance editing, while supporting arbitrary lighting conditions by nature. Additionally, we compare our representation to existing volumetric representations, highlighting its efficiency to reproduce details.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09733v2</guid>
      <category>cs.GR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yang Zhou, Songyin Wu, Ling-Qi Yan</dc:creator>
    </item>
    <item>
      <title>Jump Restore Light Transport</title>
      <link>https://arxiv.org/abs/2409.07148</link>
      <description>arXiv:2409.07148v4 Announce Type: replace 
Abstract: Markov chain Monte Carlo (MCMC) algorithms come to rescue when sampling from a complex, high-dimensional distribution by a conventional method is intractable. Even though MCMC is a powerful tool, it is also hard to control and tune in practice. Simultaneously achieving both local exploration of the state space and global discovery of the target distribution is a challenging task. In this work, we present a MCMC formulation that subsumes all existing MCMC samplers employed in rendering. We then present a novel framework for adjusting an arbitrary Markov chain, making it exhibit invariance with respect to a specified target distribution. To showcase the potential of the proposed framework, we focus on a first simple application in light transport simulation. As a by-product, we introduce continuous-time MCMC sampling to the computer graphics community. We show how any existing MCMC-based light transport algorithm can be embedded into our framework. We empirically and theoretically prove that this embedding is superior to running the standalone algorithm. In fact, our approach will convert any existing algorithm into a highly parallelizable variant with shorter running time, smaller error and less variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07148v4</guid>
      <category>cs.GR</category>
      <category>math.PR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sascha Holl, Hans-Peter Seidel, Gurprit Singh</dc:creator>
    </item>
    <item>
      <title>Effective Multi-Dimensional 3D Scatterplots as 2D Figures</title>
      <link>https://arxiv.org/abs/2406.06146</link>
      <description>arXiv:2406.06146v2 Announce Type: replace-cross 
Abstract: Computationally and data intensive workloads including design space exploration or large studies often lead to multi-dimensional results, which are often not trivial to digest with conventional plotting software. 3D scatterplots can be a powerful technique to visualise and explore such datasets, especially with the help of colour mapping and other approaches to represent more than the 3 dimensions of the Cartesian coordinate system. However, modern software commonly lacks this multi-dimensional functionality or is ineffective. One limitation is the frequent use of isometric axes, which is equivalent to removing one entire dimension. In manuscripts, additional visual cues such as movement are also not present to mitigate for the loss of depth perception and spatial information, hence their relatively limited use as static figures.
  In this work, we present a novel open-source JavaFX-based plotting framework that focuses on easy exploration of multi-dimensional datasets, and provides unique features or feature combinations to improve knowledge transfer from single stand-alone plots. An empirical study was conducted within an academic institution to quantify the effectiveness of feature or feature combinations on 3D scatterplots in terms of reading time and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06146v2</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippos Papaphilippou, Lucy Hederman</dc:creator>
    </item>
    <item>
      <title>V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians</title>
      <link>https://arxiv.org/abs/2409.13648</link>
      <description>arXiv:2409.13648v2 Announce Type: replace-cross 
Abstract: Experiencing high-fidelity volumetric video as seamlessly as 2D videos is a long-held dream. However, current dynamic 3DGS methods, despite their high rendering quality, face challenges in streaming on mobile devices due to computational and bandwidth constraints. In this paper, we introduce V^3 (Viewing Volumetric Videos), a novel approach that enables high-quality mobile rendering through the streaming of dynamic Gaussians. Our key innovation is to view dynamic 3DGS as 2D videos, facilitating the use of hardware video codecs. Additionally, we propose a two-stage training strategy to reduce storage requirements with rapid training speed. The first stage employs hash encoding and shallow MLP to learn motion, then reduces the number of Gaussians through pruning to meet the streaming requirements, while the second stage fine tunes other Gaussian attributes using residual entropy loss and temporal loss to improve temporal continuity. This strategy, which disentangles motion and appearance, maintains high rendering quality with compact storage requirements. Meanwhile, we designed a multi-platform player to decode and render 2D Gaussian videos. Extensive experiments demonstrate the effectiveness of V^3, outperforming other methods by enabling high-quality rendering and streaming on common devices, which is unseen before. As the first to stream dynamic Gaussians on mobile devices, our companion player offers users an unprecedented volumetric video experience, including smooth scrolling and instant sharing. Our project page with source code is available at https://authoritywang.github.io/v3/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13648v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Penghao Wang, Zhirui Zhang, Liao Wang, Kaixin Yao, Siyuan Xie, Jingyi Yu, Minye Wu, Lan Xu</dc:creator>
    </item>
  </channel>
</rss>
