<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Aug 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Comparison of Sustainable Development Goals Labeling Systems based on Topic Coverage</title>
      <link>https://arxiv.org/abs/2408.13455</link>
      <description>arXiv:2408.13455v1 Announce Type: new 
Abstract: With the growing importance of sustainable development goals (SDGs), various labeling systems have emerged for effective monitoring and evaluation. This study assesses six labeling systems across 1.85 million documents at both paper level and topic level. Our findings indicate that the SDGO and SDSN systems are more aggressive, while systems such as Auckland, Aurora, SIRIS, and Elsevier exhibit significant topic consistency, with similarity scores exceeding 0.75 for most SDGs. However, similarities at the paper level generally fall short, particularly for specific SDGs like SDG 10. We highlight the crucial role of contextual information in keyword-based labeling systems, noting that overlooking context can introduce bias in the retrieval of papers (e.g., variations in "migration" between biomedical and geographical contexts). These results reveal substantial discrepancies among SDG labeling systems, emphasizing the need for improved methodologies to enhance the accuracy and relevance of SDG evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13455v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Li Li, Yu Zhao, Zhesi Shen</dc:creator>
    </item>
    <item>
      <title>Transdisciplinary research: How much is academia heeding the call to work more closely with societal stakeholders such as industry, government, and nonprofits?</title>
      <link>https://arxiv.org/abs/2408.14024</link>
      <description>arXiv:2408.14024v1 Announce Type: new 
Abstract: Transdisciplinary research, the co-creation of scientific knowledge by multiple stakeholders, is considered essential for addressing major societal problems. Research policy makers and academic leaders frequently call for closer collaboration between academia and societal stakeholders to address the grand challenges of our time. This bibliometric study evaluates progress in collaboration between academia and three societal stakeholders: industry, government, and nonprofit organisations. It analyses the level of co-publishing between academia and these societal stakeholders over the period 2013-2022. We found that research collaboration between academia and all stakeholder types studied grew in absolute terms. However, academia-industry collaboration declined 16% relative to overall academic output while academia-government and academia-nonprofit collaboration grew at roughly the same pace as academic output. Country and field of research breakdowns revealed wide variance. In light of previous work, we consider potential explanations for the gap between policymakers' aspirations and the real global trends. This study is a useful demonstration of large scale, quantitative bibliometric techniques for research policymakers to track the impact of decisions related to funding, intellectual property law, and nonprofit support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14024v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip James Purnell</dc:creator>
    </item>
    <item>
      <title>Analysis of the ICML 2023 Ranking Data: Can Authors' Opinions of Their Own Papers Assist Peer Review in Machine Learning?</title>
      <link>https://arxiv.org/abs/2408.13430</link>
      <description>arXiv:2408.13430v1 Announce Type: cross 
Abstract: We conducted an experiment during the review process of the 2023 International Conference on Machine Learning (ICML) that requested authors with multiple submissions to rank their own papers based on perceived quality. We received 1,342 rankings, each from a distinct author, pertaining to 2,592 submissions. In this paper, we present an empirical analysis of how author-provided rankings could be leveraged to improve peer review processes at machine learning conferences. We focus on the Isotonic Mechanism, which calibrates raw review scores using author-provided rankings. Our analysis demonstrates that the ranking-calibrated scores outperform raw scores in estimating the ground truth ``expected review scores'' in both squared and absolute error metrics. Moreover, we propose several cautious, low-risk approaches to using the Isotonic Mechanism and author-provided rankings in peer review processes, including assisting senior area chairs' oversight of area chairs' recommendations, supporting the selection of paper awards, and guiding the recruitment of emergency reviewers. We conclude the paper by addressing the study's limitations and proposing future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13430v1</guid>
      <category>stat.AP</category>
      <category>cs.DL</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Buxin Su, Jiayao Zhang, Natalie Collina, Yuling Yan, Didong Li, Kyunghyun Cho, Jianqing Fan, Aaron Roth, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias</title>
      <link>https://arxiv.org/abs/2405.15739</link>
      <description>arXiv:2405.15739v3 Announce Type: replace 
Abstract: Citation practices are crucial in shaping the structure of scientific knowledge, yet they are often influenced by contemporary norms and biases. The emergence of Large Language Models (LLMs) introduces a new dynamic to these practices. Interestingly, the characteristics and potential biases of references recommended by LLMs that entirely rely on their parametric knowledge, and not on search or retrieval-augmented generation, remain unexplored. Here, we analyze these characteristics in an experiment using a dataset from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4's knowledge cut-off date. In our experiment, LLMs are tasked with suggesting scholarly references for the anonymized in-text citations within these papers. Our findings reveal a remarkable similarity between human and LLM citation patterns, but with a more pronounced high citation bias, which persists even after controlling for publication year, title length, number of authors, and venue. The results hold for both GPT-4, and the more capable models GPT-4o and Claude 3.5 where the papers are part of the training data. Additionally, we observe a large consistency between the characteristics of LLM's existing and non-existent generated references, indicating the model's internalization of citation patterns. By analyzing citation graphs, we show that the references recommended are embedded in the relevant citation context, suggesting an even deeper conceptual internalization of the citation networks. While LLMs can aid in citation generation, they may also amplify existing biases, such as the Matthew effect, and introduce new ones, potentially skewing scientific knowledge dissemination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15739v3</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andres Algaba, Carmen Mazijn, Vincent Holst, Floriano Tori, Sylvia Wenmackers, Vincent Ginis</dc:creator>
    </item>
  </channel>
</rss>
