<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 May 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Legible Label Layout for Data Visualization, Algorithm and Integration into Vega-Lite</title>
      <link>https://arxiv.org/abs/2405.10953</link>
      <description>arXiv:2405.10953v1 Announce Type: new 
Abstract: Legible labels should not overlap with other labels and other marks in a chart. When a chart contains a large number of data points, manually positioning these labels for each data point in the chart is a tedious task. A labeling algorithm is necessary to automatically layout the labels for a chart with a large number of data points. The state-of-the-art labeling algorithm detects overlaps using a set of points to approximate each mark's shape. This approach is inefficient for large marks or many marks as it requires too many points to detect overlaps. In response, we present a bitmap-based label placement algorithm, which leverages an occupancy bitmap to accelerate overlap detection. To create an occupancy bitmap, we rasterize marks onto a bitmap based on the area they occupy in the chart. With the bitmap, we can efficiently place labels without overlapping existing marks, regardless of the number and geometric complexity of the marks. This bitmap-based algorithm offers significant performance improvements over the state-of-the-art approach while placing a similar number of labels. We also integrate this algorithm into Vega-Lite as one of its encoding channels, label encoding. Label encoding allows users to encode fields in each data point with a text label to annotate the mark that represents the data point in a chart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10953v1</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chanwut Kittivorawong</dc:creator>
    </item>
    <item>
      <title>A Comparative Study of Garment Draping Techniques</title>
      <link>https://arxiv.org/abs/2405.11056</link>
      <description>arXiv:2405.11056v1 Announce Type: new 
Abstract: We present a comparison review that evaluates popular techniques for garment draping for 3D fashion design, virtual try-ons, and animations. A comparative study is performed between various methods for garment draping of clothing over the human body. These include numerous models, such as physics and machine learning based techniques, collision handling, and more. Performance evaluations and trade-offs are discussed to ensure informed decision-making when choosing the most appropriate approach. These methods aim to accurately represent deformations and fine wrinkles of digital garments, considering the factors of data requirements, and efficiency, to produce realistic results. The research can be insightful to researchers, designers, and developers in visualizing dynamic multi-layered 3D clothing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11056v1</guid>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prerana Achar, Mayank Patel, Anushka Mulik, Neha Katre, Stevina Dias, Chirag Raman</dc:creator>
    </item>
    <item>
      <title>Generative AI for 2D Character Animation</title>
      <link>https://arxiv.org/abs/2405.11098</link>
      <description>arXiv:2405.11098v1 Announce Type: new 
Abstract: In this pilot project, we teamed up with artists to develop new workflows for 2D animation while producing a short educational cartoon. We identified several workflows to streamline the animation process, bringing the artists' vision to the screen more effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11098v1</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaime Guajardo, Ozgun Bursalioglu, Dan B Goldman</dc:creator>
    </item>
    <item>
      <title>PBI: Position-Based Dynamics Handles Updated Lagrangian Inelasticity</title>
      <link>https://arxiv.org/abs/2405.11694</link>
      <description>arXiv:2405.11694v1 Announce Type: new 
Abstract: Position-based Dynamics (PBD) and its extension, eXtended Position-based Dynamics (XPBD), have been predominantly applied to compliant constrained dynamics, with their potential in finite strain inelasticity remaining underexplored. XPBD stands in contrast to other meshless methods, such as the Material Point Method (MPM). MPM is based on discretizing the weak form of governing partial differential equations within a continuum domain, coupled with a hybrid Lagrangian-Eulerian method for tracking deformation gradients. In contrast, XPBD generally entails applying specific constraints, whether hard or compliant, to collections of point masses. This paper revisits this perception, investigating the potential of XPBD in handling inelastic materials that are described with continuum mechanics based yield surfaces and elastoplastic flow rules. Our inspiration is that a robust estimation of the velocity gradient is key to effectively tracking deformation gradients in any meshless context. By further incorporating implicit inelastic constitutive relationships, we introduce an updated Lagrangian augmentation to XPBD. This enhancement enables the simulation of elastoplastic, viscoplastic, and granular substances following their standard constitutive laws. We demonstrate the effectiveness of our method through high-resolution and real-time simulations of diverse materials such as snow, sand, and plasticine, and its integration with standard XPBD simulations of cloth and water.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11694v1</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Yu, Xuan Li, Lei Lan, Yin Yang, Chenfanfu Jiang</dc:creator>
    </item>
    <item>
      <title>Optimizing Surgical Plans for Parenchyma-Sparing Liver Resections through Contour-Guided Resection and Surface Approximation</title>
      <link>https://arxiv.org/abs/2405.10960</link>
      <description>arXiv:2405.10960v1 Announce Type: cross 
Abstract: Objective: This study introduces a novel method for defining virtual resections in liver cancer surgery, aimed at enhancing the adaptability of parenchyma-sparing resection (PSR) plans. By comparing these with traditional anatomical resection (AR) plans, we explore the potential for optimization in surgical planning. Methods: Leveraging contours and spline surface approximations directly from the liver's surface, our method aligns closely with actual surgical procedures, offering a more realistic representation of curved resection paths. This technique, tested against 14 cases from the OSLO-COMET study, incorporates surface deformation for versatile plan modeling, comparing volumetric outcomes of PSR and AR. Results: The study highlights significant benefits of PSR over AR, including reduced resected volume ($32.71 \pm 13.80$ ml for PSR vs. $249.53 \pm 135.23$ ml for AR, $p &lt;0.0001$) and higher remnant liver volume ($1922.77 \pm 442.86$ ml for PSR vs. $1716.87 \pm 403.00$ ml for AR, $p &lt;0.0001$). PSR also showed a considerably higher remnant percentage ($98.16 \pm 0.81%$) compared to AR ($87.40 \pm 6.49%$, $p &lt;0.0001$). Conclusion: The proposed approach is able to define virtual resections accommodating a wide variety of resections (i.e., PSR and AR). Careful surgical planning using virtual resections can optimize the resection strategy. Significance: This study presents a novel computer-aided planning system for liver surgery, demonstrating its efficacy and flexibility for definition of virtual resections. Virtual surgery planning can be used for optimization of resection strategies leading to increased preservation of healthy tissue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10960v1</guid>
      <category>physics.med-ph</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gabriella d'Albenzio, Ruoyan Meng, Davit Aghayan, Egidijus Pelanis, Rebecca Hisey, Sarkis Drejian, \r{A}smund Avdem Fretland, Ole Jakob Elle, Bj{\o}rn Edwin, Rafael Palomar</dc:creator>
    </item>
    <item>
      <title>Flexible Motion In-betweening with Diffusion Models</title>
      <link>https://arxiv.org/abs/2405.11126</link>
      <description>arXiv:2405.11126v1 Announce Type: cross 
Abstract: Motion in-betweening, a fundamental task in character animation, consists of generating motion sequences that plausibly interpolate user-provided keyframe constraints. It has long been recognized as a labor-intensive and challenging process. We investigate the potential of diffusion models in generating diverse human motions guided by keyframes. Unlike previous inbetweening methods, we propose a simple unified model capable of generating precise and diverse motions that conform to a flexible range of user-specified spatial constraints, as well as text conditioning. To this end, we propose Conditional Motion Diffusion In-betweening (CondMDI) which allows for arbitrary dense-or-sparse keyframe placement and partial keyframe constraints while generating high-quality motions that are diverse and coherent with the given keyframes. We evaluate the performance of CondMDI on the text-conditioned HumanML3D dataset and demonstrate the versatility and efficacy of diffusion models for keyframe in-betweening. We further explore the use of guidance and imputation-based approaches for inference-time keyframing and compare CondMDI against these methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11126v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Setareh Cohan, Guy Tevet, Daniele Reda, Xue Bin Peng, Michiel van de Panne</dc:creator>
    </item>
    <item>
      <title>Dynamic modeling of a sliding ring on an elastic rod with incremental potential formulation</title>
      <link>https://arxiv.org/abs/2208.01238</link>
      <description>arXiv:2208.01238v4 Announce Type: replace 
Abstract: Mechanical interactions between rigid rings and flexible cables find broad application in both daily life (hanging clothes) and engineering systems (closing a tether-net). A reduced-order method for the dynamic analysis of sliding rings on a deformable one-dimensional (1D) rod-like object is proposed. In contrast to the conventional approach of discretizing joint rings into multiple nodes and edges for contact detection and numerical simulation, a single point is used to reduce the order of the model. To ensure that the sliding ring and flexible rod do not deviate from their desired positions, a new barrier function is formulated using the incremental potential theory. Subsequently, the interaction between tangent frictional forces is obtained through a delayed dissipative approach. The proposed barrier functional and the associated frictional functional are C2 continuous, hence the nonlinear elastodynamic system can be solved variationally by an implicit time-stepping scheme. The numerical framework is initially applied to simple examples where the analytical solutions are available for validation. Then, multiple complex practical engineering examples are considered to showcase the effectiveness of the proposed method. The simplified ring-to-rod interaction model has the capacity to enhance the realism of visual effects in image animations, while simultaneously facilitating the optimization of designs for space debris removal systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.01238v4</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Weicheng Huang, Peifei Xu, Zhaowei Liu</dc:creator>
    </item>
    <item>
      <title>Polycube Layouts via Iterative Dual Loops</title>
      <link>https://arxiv.org/abs/2402.00652</link>
      <description>arXiv:2402.00652v2 Announce Type: replace 
Abstract: Polycube layouts for 3D models effectively support a wide variety of applications such as hexahedral mesh construction, seamless texture mapping, spline fitting, and multi-block grid generation. However, the automated construction of valid polycube layouts suffers from robustness issues: the state-of-the-art deformation-based methods are not guaranteed to find a valid solution. In this paper we present a novel approach which is guaranteed to return a valid polycube layout for 3D models of genus 0. Our algorithm is based on a dual representation of polycubes; we construct polycube layouts by iteratively adding or removing dual loops. The iterative nature of our algorithm facilitates a seamless trade-off between quality and complexity of the solution. Our method is efficient and can be implemented using comparatively simple algorithmic building blocks. We experimentally compare the results of our algorithm against state-of-the-art methods. Our fully automated method always produces provably valid polycube layouts whose quality - assessed via the quality of derived hexahedral meshes - is on par with state-of-the-art deformation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00652v2</guid>
      <category>cs.GR</category>
      <category>cs.CG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxim Snoep, Bettina Speckmann, Kevin Verbeek</dc:creator>
    </item>
    <item>
      <title>View-Consistent 3D Editing with Gaussian Splatting</title>
      <link>https://arxiv.org/abs/2403.11868</link>
      <description>arXiv:2403.11868v4 Announce Type: replace 
Abstract: The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing, offering efficient, high-fidelity rendering and enabling precise local manipulations. Currently, diffusion-based 2D editing models are harnessed to modify multi-view rendered images, which then guide the editing of 3DGS models. However, this approach faces a critical issue of multi-view inconsistency, where the guidance images exhibit significant discrepancies across views, leading to mode collapse and visual artifacts of 3DGS. To this end, we introduce View-consistent Editing (VcEdit), a novel framework that seamlessly incorporates 3DGS into image editing processes, ensuring multi-view consistency in edited guidance images and effectively mitigating mode collapse issues. VcEdit employs two innovative consistency modules: the Cross-attention Consistency Module and the Editing Consistency Module, both designed to reduce inconsistencies in edited images. By incorporating these consistency modules into an iterative pattern, VcEdit proficiently resolves the issue of multi-view inconsistency, facilitating high-quality 3DGS editing across a diverse range of scenes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11868v4</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, Hanwang Zhang</dc:creator>
    </item>
    <item>
      <title>Efficient Data-driven Scene Simulation using Robotic Surgery Videos via Physics-embedded 3D Gaussians</title>
      <link>https://arxiv.org/abs/2405.00956</link>
      <description>arXiv:2405.00956v2 Announce Type: replace-cross 
Abstract: Surgical scene simulation plays a crucial role in surgical education and simulator-based robot learning. Traditional approaches for creating these environments with surgical scene involve a labor-intensive process where designers hand-craft tissues models with textures and geometries for soft body simulations. This manual approach is not only time-consuming but also limited in the scalability and realism. In contrast, data-driven simulation offers a compelling alternative. It has the potential to automatically reconstruct 3D surgical scenes from real-world surgical video data, followed by the application of soft body physics. This area, however, is relatively uncharted. In our research, we introduce 3D Gaussian as a learnable representation for surgical scene, which is learned from stereo endoscopic video. To prevent over-fitting and ensure the geometrical correctness of these scenes, we incorporate depth supervision and anisotropy regularization into the Gaussian learning process. Furthermore, we apply the Material Point Method, which is integrated with physical properties, to the 3D Gaussians to achieve realistic scene deformations. Our method was evaluated on our collected in-house and public surgical videos datasets. Results show that it can reconstruct and simulate surgical scenes from endoscopic videos efficiently-taking only a few minutes to reconstruct the surgical scene-and produce both visually and physically plausible deformations at a speed approaching real-time. The results demonstrate great potential of our proposed method to enhance the efficiency and variety of simulations available for surgical education and robot learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00956v2</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenya Yang, Kai Chen, Yonghao Long, Qi Dou</dc:creator>
    </item>
    <item>
      <title>Text-to-Vector Generation with Neural Path Representation</title>
      <link>https://arxiv.org/abs/2405.10317</link>
      <description>arXiv:2405.10317v2 Announce Type: replace-cross 
Abstract: Vector graphics are widely used in digital art and highly favored by designers due to their scalability and layer-wise properties. However, the process of creating and editing vector graphics requires creativity and design expertise, making it a time-consuming task. Recent advancements in text-to-vector (T2V) generation have aimed to make this process more accessible. However, existing T2V methods directly optimize control points of vector graphics paths, often resulting in intersecting or jagged paths due to the lack of geometry constraints. To overcome these limitations, we propose a novel neural path representation by designing a dual-branch Variational Autoencoder (VAE) that learns the path latent space from both sequence and image modalities. By optimizing the combination of neural paths, we can incorporate geometric constraints while preserving expressivity in generated SVGs. Furthermore, we introduce a two-stage path optimization method to improve the visual and topological quality of generated SVGs. In the first stage, a pre-trained text-to-image diffusion model guides the initial generation of complex vector graphics through the Variational Score Distillation (VSD) process. In the second stage, we refine the graphics using a layer-wise image vectorization strategy to achieve clearer elements and structure. We demonstrate the effectiveness of our method through extensive experiments and showcase various applications. The project page is https://intchous.github.io/T2V-NPR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10317v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiying Zhang, Nanxuan Zhao, Jing Liao</dc:creator>
    </item>
  </channel>
</rss>
