<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Feb 2026 05:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>BitLogic: Training Framework for Gradient-Based FPGA-Native Neural Networks</title>
      <link>https://arxiv.org/abs/2602.07400</link>
      <description>arXiv:2602.07400v1 Announce Type: cross 
Abstract: The energy and latency costs of deep neural network inference are increasingly driven by deployment rather than training, motivating hardware-specialized alternatives to arithmetic-heavy models. Field-Programmable Gate Arrays (FPGAs) provide an attractive substrate for such specialization, yet existing FPGA-based neural approaches are fragmented and difficult to compare. We present BitLogic, a fully gradient-based, end-to-end trainable framework for FPGA-native neural networks built around Lookup Table (LUT) computation. BitLogic replaces multiply-accumulate operations with differentiable LUT nodes that map directly to FPGA primitives, enabling native binary computation, sparse connectivity, and efficient hardware realization. The framework offers a modular functional API supporting diverse architectures, along with learned encoders, hardware-aware heads, and multiple boundary-consistent LUT relaxations. An automated Register Transfer Level (RTL) export pipeline translates trained PyTorch models into synthesizable HDL, ensuring equivalence between software and hardware inference. Experiments across standard vision benchmarks and heterogeneous hardware platforms demonstrate competitive accuracy and substantial gains in FPGA efficiency, including 72.3% test accuracy on CIFAR-10 achieved with fewer than 0.3M logic gates, while attaining sub-20 ns single-sample inference using only LUT resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07400v1</guid>
      <category>cs.LG</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Simon B\"uhrer, Andreas Plesner, Aczel Till, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>A quantum-inspired multi-level tensor-train monolithic space-time method for nonlinear PDEs</title>
      <link>https://arxiv.org/abs/2602.07945</link>
      <description>arXiv:2602.07945v1 Announce Type: cross 
Abstract: We propose a multilevel tensor-train (TT) framework for solving nonlinear partial differential equations (PDEs) in a global space-time formulation. While space-time TT solvers have demonstrated significant potential for compressed high-dimensional simulations, the literature contains few systematic comparisons with classical time-stepping methods, limited error convergence analyses, and little quantitative assessment of the impact of TT rounding on numerical accuracy. Likewise, existing studies fail to demonstrate performance across a diverse set of PDEs and parameter ranges. In practice, monolithic Newton iterations may stagnate or fail to converge in strongly nonlinear, stiff, or advection-dominated regimes, where poor initial guesses and severely ill-conditioned space-time Jacobians hinder robust convergence. We overcome this limitation by introducing a coarse-to-fine multilevel strategy fully embedded within the TT format. Each level refines both spatial and temporal resolutions while transferring the TT solution through low-rank prolongation operators, providing robust initializations for successive Newton solves. Residuals, Jacobians, and transfer operators are represented directly in TT and solved with the adaptive-rank DMRG algorithm. Numerical experiments for a selection of nonlinear PDEs including Fisher-KPP, viscous Burgers, sine-Gordon, and KdV cover diffusive, convective, and dispersive dynamics, demonstrating that the multilevel TT approach consistently converges where single-level space-time Newton iterations fail. In dynamic, advection-dominated (nonlinear) scenarios, multilevel TT surpasses single-level TT, achieving high accuracy with significantly reduced computational cost, specifically when high-fidelity numerical simulation is required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07945v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>physics.comp-ph</category>
      <category>quant-ph</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>N. R. Rapaka, R. Peddinti, E. Tiunov, N. J. Faraj, A. N. Alkhooori, L. Aolita, Y. Addad, M. K. Riahi</dc:creator>
    </item>
    <item>
      <title>A Machine Learning accelerated geophysical fluid solver</title>
      <link>https://arxiv.org/abs/2602.08670</link>
      <description>arXiv:2602.08670v1 Announce Type: cross 
Abstract: Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08670v1</guid>
      <category>cs.CV</category>
      <category>cs.CE</category>
      <category>cs.PF</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yang Bai</dc:creator>
    </item>
    <item>
      <title>On Resolving Non-Preemptivity in Multitask Scheduling: An Optimal Algorithm in Deterministic and Stochastic Worlds</title>
      <link>https://arxiv.org/abs/2411.06348</link>
      <description>arXiv:2411.06348v2 Announce Type: replace-cross 
Abstract: The efficient scheduling of multi-task jobs across multiprocessor systems has become increasingly critical with the rapid expansion of computational systems. This challenge, known as Multiprocessor Multitask Scheduling (MPMS), is essential for optimizing the performance and scalability of applications in fields such as cloud computing and deep learning. In this paper, we study the MPMS problem under both deterministic and stochastic models, where each job is composed of multiple tasks and can only be completed when all its tasks are finished. We introduce $\mathsf{NP}$-$\mathsf{SRPT}$, a non-preemptive variant of the SRPT algorithm, designed to accommodate scenarios with non-preemptive tasks. Our algorithm achieves a competitive ratio of $\ln \alpha + \beta + 1$ for minimizing response time, where $\alpha$ represents the ratio of the largest to the smallest job workload, and $\beta$ captures the ratio of the largest non-preemptive task workload to the smallest job workload. We further establish that this competitive ratio is order-optimal when the number of processors is fixed. For the stochastic $\mathsf{M}$/$\mathsf{G}$/$\mathsf{N}$ system, we prove that $\mathsf{NP}$-$\mathsf{SRPT}$ achieves asymptotically optimal mean response time as the traffic intensity approaches $1$, assuming task size distribution with finite support. Moreover, the asymptotic optimality extends to infinite task size distributions under mild probabilistic assumptions, including the standard $\mathsf{M}$/$\mathsf{M}$/$\mathsf{N}$ model. Finally, we extend the analysis to the setting of unknown job sizes, proving that non-preemptive adaptations of the $\mathsf{M\text{-}Gittins}$ and $\mathsf{M\text{-}SERPT}$ policies achieve asymptotic optimality and near-optimality, respectively, for a broad class of job size distributions. Experimental results validate the effectiveness of $\mathsf{NP}$-$\mathsf{SRPT}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06348v2</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <category>math.PR</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Li</dc:creator>
    </item>
    <item>
      <title>Efficient Graph Knowledge Distillation from GNNs to Kolmogorov--Arnold Networks via Self-Attention Dynamic Sampling</title>
      <link>https://arxiv.org/abs/2509.00560</link>
      <description>arXiv:2509.00560v2 Announce Type: replace-cross 
Abstract: Recent success of graph neural networks (GNNs) in modeling complex graph-structured data has fueled interest in deploying them on resource-constrained edge devices. However, their substantial computational and memory demands present ongoing challenges. Knowledge distillation (KD) from GNNs to MLPs offers a lightweight alternative, but MLPs remain limited by fixed activations and the absence of neighborhood aggregation, constraining distilled performance. To tackle these intertwined limitations, we propose SA-DSD, a novel self-attention-guided dynamic sampling distillation framework. To the best of our knowledge, this is the first work to employ an enhanced Kolmogorov-Arnold Network (KAN) as the student model. We improve Fourier KAN (FR-KAN+) with learnable frequency bases, phase shifts, and optimized algorithms, substantially improving nonlinear fitting capability over MLPs while preserving low computational complexity. To explicitly compensate for the absence of neighborhood aggregation that is inherent to both MLPs and KAN-based students, SA-DSD leverages a self-attention mechanism to dynamically identify influential nodes, construct adaptive sampling probability matrices, and enforce teacher-student prediction consistency. Extensive experiments on six real world datasets demonstrate that, under inductive and most of transductive settings, SA-DSD surpasses three GNN teachers by 3.05%-3.62% and improves FR-KAN+ by 15.61%. Moreover, it achieves a 16.69x parameter reduction and a 55.75% decrease in average runtime per epoch compared to key benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00560v2</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Can Cui, Zilong Fu, Penghe Huang, Yuanyuan Li, Wu Deng, Dongyan Li</dc:creator>
    </item>
  </channel>
</rss>
