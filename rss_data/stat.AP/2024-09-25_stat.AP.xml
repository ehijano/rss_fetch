<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Sep 2024 01:51:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Stable Survival Extrapolation via Transfer Learning</title>
      <link>https://arxiv.org/abs/2409.16044</link>
      <description>arXiv:2409.16044v1 Announce Type: new 
Abstract: The mean survival is the key ingredient of the decision process in several applications, including health economic evaluations. It is defined as the area under the complete survival curve thus necessitating extrapolation of the observed data. In this article we employ a Bayesian mortality model and transfer its projections in order to construct the baseline population that acts as an anchor of the survival model. This can be seen as an implicit bias-variance trade-off in unseen data. We then propose extrapolation methods based on flexible parametric polyhazard models which can naturally accommodate diverse shapes, including non-proportional hazards and crossing survival curves while typically maintaining a natural interpretation. We estimate the mean survival and related estimands in three cases, namely breast cancer, cardiac arrhythmia and advanced melanoma. Specifically, we evaluate the survival disadvantage of triple negative breast cancer cases, the efficacy of combining immunotherapy with mRNA cancer therapeutic for advanced melanoma treatment and the suitability of implantable cardioverter defibrilators for cardiac arrhythmia. The latter is conducted in a competing risks context illustrating how working on the cause-specific hazard alone minimizes potential instability. The results suggest that the proposed approach offers a flexible, interpretable and robust approach when survival extrapolation is required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16044v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasios Apsemidis, Nikolaos Demiris</dc:creator>
    </item>
    <item>
      <title>Bayesian competing risks survival modeling for assessing the cause of death of patients with heart failure</title>
      <link>https://arxiv.org/abs/2409.16080</link>
      <description>arXiv:2409.16080v1 Announce Type: new 
Abstract: Competing risk models are survival models with several events of interest acting in competition and whose occurrence is only observed for the event that occurs first in time. This paper presents a Bayesian approach to these models in which the issue of model selection is treated in a special way by proposing generalizations of some of the Bayesian procedures used in univariate survival analysis. This research is motivated by a study on the survival of patients with hearth failure undergoing cardiac resynchronization therapy, a procedure which involves the implant of a device to stabilize the heartbeat. Two different causes of causes of death have been considered: cardiovascular and non-cardiovascular, and a set of baseline covariates are examined in order to better understand their relationship with both causes of death. Model selection procedures and model checking analyses have been implemented and assessed. The posterior distribution of some relevant outputs such as transition probabilities have been computed and discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16080v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jes\'us Guti\'errez-Botella, Carmen Armero, Thomas Kneib, Mar\'ia P. Pata, Javier Garc\'ia-Seara</dc:creator>
    </item>
    <item>
      <title>Mining Causality: AI-Assisted Search for Instrumental Variables</title>
      <link>https://arxiv.org/abs/2409.14202</link>
      <description>arXiv:2409.14202v1 Announce Type: cross 
Abstract: The instrumental variables (IVs) method is a leading empirical strategy for causal inference. Finding IVs is a heuristic and creative process, and justifying its validity (especially exclusion restrictions) is largely rhetorical. We propose using large language models (LLMs) to search for new IVs through narratives and counterfactual reasoning, similar to how a human researcher would. The stark difference, however, is that LLMs can accelerate this process exponentially and explore an extremely large search space. We demonstrate how to construct prompts to search for potentially valid IVs. We argue that multi-step prompting is useful and role-playing prompts are suitable for mimicking the endogenous decisions of economic agents. We apply our method to three well-known examples in economics: returns to schooling, production functions, and peer effects. We then extend our strategy to finding (i) control variables in regression and difference-in-differences and (ii) running variables in regression discontinuity designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14202v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukjin Han</dc:creator>
    </item>
    <item>
      <title>Reservoir Static Property Estimation Using Nearest-Neighbor Neural Network</title>
      <link>https://arxiv.org/abs/2409.15295</link>
      <description>arXiv:2409.15295v1 Announce Type: cross 
Abstract: This note presents an approach for estimating the spatial distribution of static properties in reservoir modeling using a nearest-neighbor neural network. The method leverages the strengths of neural networks in approximating complex, non-linear functions, particularly for tasks involving spatial interpolation. It incorporates a nearest-neighbor algorithm to capture local spatial relationships between data points and introduces randomization to quantify the uncertainty inherent in the interpolation process. This approach addresses the limitations of traditional geostatistical methods, such as Inverse Distance Weighting (IDW) and Kriging, which often fail to model the complex non-linear dependencies in reservoir data. By integrating spatial proximity and uncertainty quantification, the proposed method can improve the accuracy of static property predictions like porosity and permeability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15295v1</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuhe Wang</dc:creator>
    </item>
    <item>
      <title>Damage detection in an uncertain nonlinear beam based on stochastic Volterra series</title>
      <link>https://arxiv.org/abs/2409.15349</link>
      <description>arXiv:2409.15349v1 Announce Type: cross 
Abstract: The damage detection problem in mechanical systems, using vibration measurements, is commonly called Structural Health Monitoring (SHM). Many tools are able to detect damages by changes in the vibration pattern, mainly, when damages induce nonlinear behavior. However, a more difficult problem is to detect structural variation associated with damage, when the mechanical system has nonlinear behavior even in the reference condition. In these cases, more sophisticated methods are required to detect if the changes in the response are based on some structural variation or changes in the vibration regime, because both can generate nonlinearities. Among the many ways to solve this problem, the use of the Volterra series has several favorable points, because they are a generalization of the linear convolution, allowing the separation of linear and nonlinear contributions by input filtering through the Volterra kernels. On the other hand, the presence of uncertainties in mechanical systems, due to noise, geometric imperfections, manufacturing irregularities, environmental conditions, and others, can also change the responses, becoming more difficult the damage detection procedure. An approach based on a stochastic version of Volterra series is proposed to be used in the detection of a breathing crack in a beam vibrating in a nonlinear regime of motion, even in reference condition (without crack). The system uncertainties are simulated by the variation imposed in the linear stiffness and damping coefficient. The results show, that the nonlinear analysis done, considering the high order Volterra kernels, allows the approach to detect the crack with a small propagation and probability confidence, even in the presence of uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15349v1</guid>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ymssp.2018.07.028</arxiv:DOI>
      <arxiv:journal_reference>Mechanical Systems and Signal Processing, Vol. 125, pp. 288-310, 2019</arxiv:journal_reference>
      <dc:creator>Luis Gustavo Giacon Villani, Samuel da Silva, Americo Cunha Jr</dc:creator>
    </item>
    <item>
      <title>An R package for nonparametric inference on dynamic populations with infinitely many types</title>
      <link>https://arxiv.org/abs/2409.15539</link>
      <description>arXiv:2409.15539v1 Announce Type: cross 
Abstract: Fleming-Viot diffusions are widely used stochastic models for population dynamics which extend the celebrated Wright-Fisher diffusions. They describe the temporal evolution of the relative frequencies of the allelic types in an ideally infinite panmictic population, whose individuals undergo random genetic drift and at birth can mutate to a new allelic type drawn from a possibly infinite potential pool, independently of their parent. Recently, Bayesian nonparametric inference has been considered for this model when a finite sample of individuals is drawn from the population at several discrete time points. Previous works have fully described the relevant estimators for this problem, but current software is available only for the Wright-Fisher finite-dimensional case. Here we provide software for the general case, overcoming some non trivial computational challenges posed by this setting. The R package FVDDPpkg efficiently approximates the filtering and smoothing distribution for Fleming-Viot diffusions, given finite samples of individuals collected at different times. A suitable Monte Carlo approximation is also introduced in order to reduce the computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15539v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Ascolani, Stefano Damato, Matteo Ruggiero</dc:creator>
    </item>
    <item>
      <title>Anonymity and Identity Online</title>
      <link>https://arxiv.org/abs/2409.15948</link>
      <description>arXiv:2409.15948v1 Announce Type: cross 
Abstract: Economics Job Market Rumors (EJMR) is an online forum and clearinghouse for information on the academic job market for economists. It also includes content that is abusive, defamatory, racist, misogynistic, or otherwise "toxic." Almost all of this content is created anonymously by contributors who receive a four-character username when posting on EJMR. Using only publicly available data we show that the statistical properties of the scheme by which these usernames were generated allows the IP addresses from which most posts were made to be determined with high probability. We recover 47,630 distinct IP addresses of EJMR posters and attribute them to 66.1% of the roughly 7 million posts made over the past 12 years. We geolocate posts and describe aggregated cross-sectional variation -- particularly regarding toxic, misogynistic, and hate speech -- across sub-forums, geographies, institutions, and IP addresses. Our analysis suggests that content on EJMR comes from all echelons of the economics profession, including, but not limited to, its elite institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15948v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Ederer, Paul Goldsmith-Pinkham, Kyle Jensen</dc:creator>
    </item>
    <item>
      <title>Robust Inference for Non-Linear Regression Models with Applications in Enzyme Kinetics</title>
      <link>https://arxiv.org/abs/2409.15995</link>
      <description>arXiv:2409.15995v1 Announce Type: cross 
Abstract: Despite linear regression being the most popular statistical modelling technique, in real-life we often need to deal with situations where the true relationship between the response and the covariates is nonlinear in parameters. In such cases one needs to adopt appropriate non-linear regression (NLR) analysis, having wider applications in biochemical and medical studies among many others. In this paper we propose a new improved robust estimation and testing methodologies for general NLR models based on the minimum density power divergence approach and apply our proposal to analyze the widely popular Michaelis-Menten (MM) model in enzyme kinetics. We establish the asymptotic properties of our proposed estimator and tests, along with their theoretical robustness characteristics through influence function analysis. For the particular MM model, we have further empirically justified the robustness and the efficiency of our proposed estimator and the testing procedure through extensive simulation studies and several interesting real data examples of enzyme-catalyzed (biochemical) reactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15995v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suryasis Jana, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Robust inference for intermittently-monitored step-stress tests under Weibull lifetime distributions</title>
      <link>https://arxiv.org/abs/2208.02674</link>
      <description>arXiv:2208.02674v2 Announce Type: replace 
Abstract: Many modern products exhibit high reliability under normal operating conditions. Conducting life tests under these conditions may result in very few observed failures, insufficient for accurate inferences. Instead, accelerated life tests (ALTs) must be performed. One of the most popular ALT designs is the step-stress test, which shortens the product's lifetime by progressively increasing the stress level at which units are subjected to at some pre-specified times. Classical estimation methods based on the maximum likelihood estimator (MLE) enjoy suitable asymptotic properties but they lack robustness. That is, data contaminationcan significantly impact the statistical analysis. In this paper, we develop robust inferential methods for highly reliable devices based on the density power divergence (DPD) for estimating and testing under the step-stress model with intermittent monitoring and Weibull lifetime distributions. We theoretically and empirically examine asymptotic and robustness properties of the minimum DPD estimators and associated Wald-type test statistics. Moreover, we develop robust estimators and confidence intervals for some important lifetime characteristics. The effect of temperature in solar lights, medium power silicon bipolar transistors and LED lights using real data arising from an step-stress ALT is analyzed applying the robust methods proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02674v2</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Narayanaswamy Balakrishnan, Mar\'ia Jaenada, Leandro Pardo</dc:creator>
    </item>
    <item>
      <title>Bayesian Federated Inference for regression models based on non-shared multicenter data sets from heterogeneous populations</title>
      <link>https://arxiv.org/abs/2402.02898</link>
      <description>arXiv:2402.02898v2 Announce Type: replace 
Abstract: To estimate accurately the parameters of a regression model, the sample size must be large enough relative to the number of possible predictors for the model. In practice, sufficient data is often lacking, which can lead to overfitting of the model and, as a consequence, unreliable predictions of the outcome of new patients. Pooling data from different data sets collected in different (medical) centers would alleviate this problem, but is often not feasible due to privacy regulation or logistic problems. An alternative route would be to analyze the local data in the centers separately and combine the statistical inference results with the Bayesian Federated Inference (BFI) methodology. The aim of this approach is to compute from the inference results in separate centers what would have been found if the statistical analysis was performed on the combined data. We explain the methodology under homogeneity and heterogeneity across the populations in the separate centers, and give real life examples for better understanding. Excellent performance of the proposed methodology is shown. An R-package to do all the calculations has been developed and is illustrated in this paper. The mathematical details are given in the Appendix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02898v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marianne A Jonker, Hassan Pazira, Anthony CC Coolen</dc:creator>
    </item>
    <item>
      <title>Wastewater-based Epidemiology for COVID-19 Surveillance and Beyond: A Survey</title>
      <link>https://arxiv.org/abs/2403.15291</link>
      <description>arXiv:2403.15291v2 Announce Type: replace 
Abstract: The pandemic of COVID-19 has imposed tremendous pressure on public health systems and social economic ecosystems over the past years. To alleviate its social impact, it is important to proactively track the prevalence of COVID-19 within communities. The traditional way to estimate the disease prevalence is to estimate from reported clinical test data or surveys. However, the coverage of clinical tests is often limited and the tests can be labor-intensive, requires reliable and timely results, and consistent diagnostic and reporting criteria. Recent studies revealed that patients who are diagnosed with COVID-19 often undergo fecal shedding of SARS-CoV-2 virus into wastewater, which makes wastewater-based epidemiology for COVID-19 surveillance a promising approach to complement traditional clinical testing. In this paper, we survey the existing literature regarding wastewater-based epidemiology for COVID-19 surveillance and summarize the current advances in the area. Specifically, we have covered the key aspects of wastewater sampling, sample testing, and presented a comprehensive and organized summary of wastewater data analytical methods. Finally, we provide the open challenges on current wastewater-based COVID-19 surveillance studies, aiming to encourage new ideas to advance the development of effective wastewater-based surveillance systems for general infectious diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15291v2</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>q-bio.PE</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Chen, Yunfan Wang, Gursharn Kaur, Aniruddha Adiga, Baltazar Espinoza, Srinivasan Venkatramanan, Andrew Warren, Bryan Lewis, Justin Crow, Rekha Singh, Alexandra Lorentz, Denise Toney, Madhav Marathe</dc:creator>
    </item>
    <item>
      <title>Parameterizations for Large-Scale Variational System Identification Using Unconstrained Optimization</title>
      <link>https://arxiv.org/abs/2404.10137</link>
      <description>arXiv:2404.10137v2 Announce Type: replace 
Abstract: This paper details how to parameterize the posterior distribution of state-space systems to generate improved optimization problems for system identification using variational inference. Three different parameterizations of the assumed state-path posterior distribution are proposed based on this representation: time-varying, steady-state, and convolution smoother; each resulting in a different parameter estimator. In contrast to existing methods for variational system identification, the proposed estimators can be implemented with unconstrained optimization methods. Furthermore, when applied to mini-batches in conjunction with stochastic optimization, the convolution-smoother formulation enables identification of large linear and nonlinear state-space systems from very large datasets. For linear systems, the method achieves the same performance as the inherently sequential prediction-error methods using an embarrassingly parallel algorithm that benefits from large speedups when computed in modern graphical processing units (GPUs). The ability of the proposed estimators to identify large models, work with large datasets split into mini-batches, and work in parallel on GPUs make them well-suited for identifying deep models for applications in systems and control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10137v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Dimas Abreu Archanjo Dutra</dc:creator>
    </item>
    <item>
      <title>Seismic fragility curves fitting revisited: ordinal regression models and their generalization</title>
      <link>https://arxiv.org/abs/2405.15513</link>
      <description>arXiv:2405.15513v2 Announce Type: replace 
Abstract: This study revisits the modeling of seismic fragility curves by applying ordinal regression models, offering an alternative to the commonly used log-normal distribution function. It compares various ordinal regression approaches, including Cumulative, Sequential, and Adjacent Category models, along with extensions that account for category-specific effects and variance heterogeneity. The methodologies are applied to bridge damage data from the 2008 Wenchuan earthquake, using both frequentist and Bayesian inference methods, with model diagnostics conducted using surrogate residuals. The analysis examines eleven models, from basic forms to those incorporating heteroscedastic extensions and category-specific effects. Based on leave-one-out cross-validation, the Sequential model with category-specific effects performs well compared to traditional Cumulative probit models. The results indicate differences in damage probability predictions between the models, suggesting the potential for more flexible fragility curve modeling techniques to improve seismic risk assessments. This study highlights the importance of continued evaluation of existing methods to enhance the predictive accuracy and applicability of seismic fragility models in performance-based earthquake engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15513v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Libo Chen</dc:creator>
    </item>
    <item>
      <title>Quantifying uncertainty in area and regression coefficient estimation from remote sensing maps</title>
      <link>https://arxiv.org/abs/2407.13659</link>
      <description>arXiv:2407.13659v3 Announce Type: replace 
Abstract: Remote sensing map products are used to obtain estimates of environmental quantities, such as deforested area or the effect of conservation zones on deforestation. However, the quality of map products varies, and - because maps are outputs of complex machine learning algorithms that take in a variety of remotely sensed variables as inputs - errors are difficult to characterize. Thus, population-level estimates from such maps may be biased. In this paper, we compare several uncertainty quantification methods - stratified estimator, post-stratified estimator, and prediction-powered inference - that combine a small amount of randomly sampled ground truth data with large-scale remote sensing map products to generate unbiased estimates. Applying these methods across four remote sensing use cases in area and regression coefficient estimation, we find that they result in estimates that are more reliable than using the map product as if it were 100% accurate and have lower uncertainty than using only the ground truth and ignoring the map product. Prediction-powered inference uses ground truth data to correct for bias in the map product estimate and (unlike stratification) does not require us to choose a map product before sampling. This is the first work to (1) apply prediction-powered inference to remote sensing estimation tasks, and (2) perform uncertainty quantification on remote sensing regression coefficients without assumptions on the structure of map product errors. To improve the utility of machine learning-generated remote sensing maps for downstream applications, we recommend that map producers provide a randomly sampled holdout ground truth dataset to be used for calibration in uncertainty quantification alongside their maps. Data and code are available at https://github.com/Earth-Intelligence-Lab/uncertainty-quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13659v3</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>eess.SP</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kerri Lu, Stephen Bates, Sherrie Wang</dc:creator>
    </item>
    <item>
      <title>Elastic Bayesian Model Calibration</title>
      <link>https://arxiv.org/abs/2305.08834</link>
      <description>arXiv:2305.08834v2 Announce Type: replace-cross 
Abstract: Functional data are ubiquitous in scientific modeling. For instance, quantities of interest are modeled as functions of time, space, energy, density, etc. Uncertainty quantification methods for computer models with functional response have resulted in tools for emulation, sensitivity analysis, and calibration that are widely used. However, many of these tools do not perform well when the computer model's parameters control both the amplitude variation of the functional output and its alignment (or phase variation). This paper introduces a framework for Bayesian model calibration when the model responses are misaligned functional data. The approach generates two types of data out of the misaligned functional responses: (1) aligned functions so that the amplitude variation is isolated and (2) warping functions that isolate the phase variation. These two types of data are created for the computer simulation data (both of which may be emulated) and the experimental data. The calibration approach uses both types so that it seeks to match both the amplitude and phase of the experimental data. The framework is careful to respect constraints that arise especially when modeling phase variation, and is framed in a way that it can be done with readily available calibration software. We demonstrate the techniques on two simulated data examples and on two dynamic material science problems: a strength model calibration using flyer plate experiments and an equation of state model calibration using experiments performed on the Sandia National Laboratories' Z-machine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08834v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Devin Francom, J. Derek Tucker, Gabriel Huerta, Kurtis Shuler, Daniel Ries</dc:creator>
    </item>
    <item>
      <title>Does AI help humans make better decisions? A methodological framework for experimental evaluation</title>
      <link>https://arxiv.org/abs/2403.12108</link>
      <description>arXiv:2403.12108v2 Announce Type: replace-cross 
Abstract: The use of Artificial Intelligence (AI), or more generally data-driven algorithms, has become ubiquitous in today's society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions compared to a human-alone or AI-alone system. We introduce a new methodological framework to experimentally answer this question without additional assumptions. We measure a decision maker's ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded experimental design, in which the provision of AI-generated recommendations is randomized across cases with humans making final decisions. Under this experimental design, we show how to compare the performance of three alternative decision-making systems -- human-alone, human-with-AI, and AI-alone. We also show when to provide a human-decision maker with AI recommendations and when they should follow such recommendations. We apply the proposed methodology to the data from our own randomized controlled trial of a pretrial risk assessment instrument. We find that the risk assessment recommendations do not improve the classification accuracy of a judge's decision to impose cash bail. Our analysis also shows that the risk assessment-alone decisions generally perform worse than human decisions with or without algorithmic assistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12108v2</guid>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Ben-Michael, D. James Greiner, Melody Huang, Kosuke Imai, Zhichao Jiang, Sooahn Shin</dc:creator>
    </item>
  </channel>
</rss>
