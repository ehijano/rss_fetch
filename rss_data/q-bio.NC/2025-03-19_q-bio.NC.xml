<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Mar 2025 04:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>How Metacognitive Architectures Remember Their Own Thoughts: A Systematic Review</title>
      <link>https://arxiv.org/abs/2503.13467</link>
      <description>arXiv:2503.13467v1 Announce Type: new 
Abstract: Inspired by human cognition, metacognition has gained significant attention for its potential to enhance autonomy, adaptability, and robust learning in artificial agents. Yet research on Computational Metacognitive Architectures (CMAs) remains fragmented: diverse theories, terminologies, and design choices have led to disjointed developments and limited comparability across systems. Existing overviews and surveys often remain at a broad, conceptual level, making it difficult to synthesize deeper insights into the underlying algorithms and representations, and their respective success. We address this gap by performing an explorative systematic review of how CMAs model, store, remember and process their metacognitive experiences, one of Flavell's (1979) three foundational components of metacognition. Following this organizing principle, we identify 35 CMAs that feature episodic introspective data ranging from symbolic event traces to sub-symbolic arousal metrics. We consider different aspects - ranging from the underlying psychological theories to the content and structure of collected data, to the algorithms used and evaluation results - and derive a unifying perspective that allows us to compare in depth how different Computational Metacognitive Architectures (CMAs) leverage metacognitive experiences for tasks such as error diagnosis, self-repair, and goal-driven learning. Our findings highlight both the promise of metacognitive experiences - in boosting adaptability, explainability, and overall system performance - and the persistent lack of shared standards or evaluation benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13467v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robin Nolte, Mihai Pomarlan, Ayden Janssen, Daniel Be{\ss}ler, Kamyar Javanmardi, Sascha Jongebloed, Robert Porzel, John Bateman, Michael Beetz, Rainer Malaka</dc:creator>
    </item>
    <item>
      <title>Subthreshold moment analysis of neuronal populations driven by synchronous synaptic inputs</title>
      <link>https://arxiv.org/abs/2503.13702</link>
      <description>arXiv:2503.13702v1 Announce Type: new 
Abstract: Even when driven by the same stimulus, neuronal responses are well-known to exhibit a striking level of spiking variability. In-vivo electrophysiological recordings also reveal a surprisingly large degree of variability at the subthreshold level. In prior work, we considered biophysically relevant neuronal models to account for the observed magnitude of membrane voltage fluctuations. We found that accounting for these fluctuations requires weak but nonzero synchrony in the spiking activity, in amount that are consistent with experimentally measured spiking correlations. Here we investigate whether such synchrony can explain additional statistical features of the measured neural activity, including neuronal voltage covariability and voltage skewness. Addressing this question involves conducting a generalized moment analysis of conductance-based neurons in response to input drives modeled as correlated jump processes. Technically, we perform such an analysis using fixed-point techniques from queuing theory that are applicable in the stationary regime of activity. We found that weak but nonzero synchrony can consistently explain the experimentally reported voltage covariance and skewness. This confirms the role of synchrony as a primary driver of cortical variability and supports that physiological neural activity emerges as a population-level phenomenon, especially in the spontaneous regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13702v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Logan A. Becker, Francois Baccelli, Thibaud Taillefumier</dc:creator>
    </item>
    <item>
      <title>Neural Constraints on Cognitive Experience and Mental Health</title>
      <link>https://arxiv.org/abs/2503.13981</link>
      <description>arXiv:2503.13981v1 Announce Type: new 
Abstract: Understanding how neural dynamics shape cognitive experiences remains a central challenge in neuroscience and psychiatry. Here, we present a novel framework leveraging state-to-output controllability from dynamical systems theory to model the interplay between cognitive perturbations, neural activity, and subjective experience. We demonstrate that large-scale fMRI signals are constrained to low-dimensional manifolds, where affective and cognitive states are naturally organized. Furthermore, we provide a theoretically robust method to estimate the controllability Gramian from steady-state neural responses, offering a direct measure of the energy required to steer cognitive outcomes. In five healthy participants viewing 2,185 emotionally evocative short videos, our analyses reveal a strong alignment between neural activations and affective ratings, with an average correlation of $r \approx 0.7$. In a clinical cohort of 255 patients with major depressive disorder, biweekly Hamilton Rating Scale trajectories over 11 weeks significantly mapped onto these manifolds, explaining approximately 20% more variance than chance ($p &lt; 10^{-10}$, numerically better than chance in 93% reaching statistical significance in one-third of subjects). Our work bridges dynamical systems theory and clinical neuroscience, providing a principled approach to optimize mental health treatments by targeting the most efficient neural pathways for cognitive change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13981v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bita Shariatpanahi, Erfan Nozari, Soroush Daftarian, Fahimeh Arab, Mina Kheirkhah, Felix P. Bernhard, Shiva Khodadadi, Erik J. Giltay, Kaat Hebbrecht, Stefan G. Hofmann, Tim Hahn, Hamidreza Jamalabadi</dc:creator>
    </item>
    <item>
      <title>Revealing higher-order neural representations with generative artificial intelligence</title>
      <link>https://arxiv.org/abs/2503.14333</link>
      <description>arXiv:2503.14333v1 Announce Type: cross 
Abstract: Studies often aim to reveal how neural representations encode aspects of an observer's environment, such as its contents or structure. These are ``first-order" representations (FORs), because they're ``about" the external world. A less-common target is ``higher-order" representations (HORs), which are ``about" FORs -- their contents, stability, or uncertainty. HORs of uncertainty appear critically involved in adaptive behaviors including learning under uncertainty, influencing learning rates and internal model updating based on environmental feedback. However, HORs about uncertainty are unlikely to be direct ``read-outs" of FOR characteristics, instead reflecting estimation processes which may be lossy, bias-prone, or distortive and which may also incorporate estimates of distributions of uncertainty the observer is likely to experience. While some research has targeted neural representations of ``instantaneously" estimated uncertainty, how the brain represents \textit{distributions} of expected uncertainty remains largely unexplored. Here, we propose a novel reinforcement learning (RL) based generative artificial intelligence (genAI) approach to explore neural representations of uncertainty distributions. We use existing functional magnetic resonance imaging data, where humans learned to `de-noise' their brain states to achieve target neural patterns, to train denoising diffusion genAI models with RL algorithms to learn noise distributions similar to how humans might learn to do the same. We then explore these models' learned noise-distribution HORs compared to control models trained with traditional backpropagation. Results reveal model-dependent differences in noise distribution representations -- with the RL-based model offering much higher explanatory power for human behavior -- offering an exciting path towards using genAI to explore neural noise-distribution HORs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14333v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hojjat Azimi Asrari, Megan A. K. Peters</dc:creator>
    </item>
    <item>
      <title>Deep Jansen-Rit Parameter Inference for Model-Driven Analysis of Brain Activity</title>
      <link>https://arxiv.org/abs/2406.05002</link>
      <description>arXiv:2406.05002v2 Announce Type: replace 
Abstract: Accurately modeling effective connectivity (EC) is critical for understanding how the brain processes and integrates sensory information. Yet, it remains a formidable challenge due to complex neural dynamics and noisy measurements such as those obtained from the electroencephalogram (EEG). Model-driven EC infers local (within a brain region) and global (between brain regions) EC parameters by fitting a generative model of neural activity onto experimental data. This approach offers a promising route for various applications, including investigating neurodevelopmental disorders. However, current approaches fail to scale to whole-brain analyses and are highly noise-sensitive. In this work, we employ three deep-learning architectures--a transformer, a long short-term memory (LSTM) network, and a convolutional neural network and bidirectional LSTM (CNN-BiLSTM) network--for inverse modeling and compare their performance with simulation-based inference in estimating the Jansen-Rit neural mass model (JR-NMM) parameters from simulated EEG data under various noise conditions. We demonstrate a reliable estimation of key local parameters, such as synaptic gains and time constants. However, other parameters like local JR-NMM connectivity cannot be evaluated reliably from evoked-related potentials (ERP). We also conduct a sensitivity analysis to characterize the influence of JR-NMM parameters on ERP and evaluate their learnability. Our results show the feasibility of deep-learning approaches to estimate the subset of learnable JR-NMM parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05002v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deepa Tilwani, Christian O'Reilly</dc:creator>
    </item>
    <item>
      <title>CATD: Unified Representation Learning for EEG-to-fMRI Cross-Modal Generation</title>
      <link>https://arxiv.org/abs/2408.00777</link>
      <description>arXiv:2408.00777v2 Announce Type: replace-cross 
Abstract: Multi-modal neuroimaging analysis is crucial for a comprehensive understanding of brain function and pathology, as it allows for the integration of different imaging techniques, thus overcoming the limitations of individual modalities. However, the high costs and limited availability of certain modalities pose significant challenges. To address these issues, this paper proposed the Condition-Aligned Temporal Diffusion (CATD) framework for end-to-end cross-modal synthesis of neuroimaging, enabling the generation of functional magnetic resonance imaging (fMRI)-detected Blood Oxygen Level Dependent (BOLD) signals from more accessible Electroencephalography (EEG) signals. By constructing Conditionally Aligned Block (CAB), heterogeneous neuroimages are aligned into a potential space, achieving a unified representation that provides the foundation for cross-modal transformation in neuroimaging. The combination with the constructed Dynamic Time-Frequency Segmentation (DTFS) module also enables the use of EEG signals to improve the temporal resolution of BOLD signals, thus augmenting the capture of the dynamic details of the brain. Experimental validation demonstrated the effectiveness of the framework in improving the accuracy of neural activity prediction, identifying abnormal brain regions, and enhancing the temporal resolution of BOLD signals. The proposed framework establishes a new paradigm for cross-modal synthesis of neuroimaging by unifying heterogeneous neuroimaging data into a potential representation space, showing promise in medical applications such as improving Parkinson's disease prediction and identifying abnormal brain regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00777v2</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TMI.2025.3550206</arxiv:DOI>
      <dc:creator>Weiheng Yao, Zhihan Lyu, Mufti Mahmud, Ning Zhong, Baiying Lei, Shuqiang Wang</dc:creator>
    </item>
  </channel>
</rss>
