<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Sep 2024 04:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multi-Tier Preservation of Discrete Morse Smale Complexes in Error-Bounded Lossy Compression</title>
      <link>https://arxiv.org/abs/2409.17346</link>
      <description>arXiv:2409.17346v1 Announce Type: new 
Abstract: We propose a multi-tier paradigm to preserve various components of Morse-Smale complexes in lossy compressed scalar fields, including extrema, saddles, separatrices, and persistence diagrams. Existing error-bounded lossy compressors rarely consider preserving topological structures such as discrete Morse-Smale complexes, leading to significant inaccuracies in data interpretation and potentially resulting in incorrect scientific conclusions. This paper mainly focuses on preserving the Morse-Smale complexes in 2D or 3D discrete scalar fields by precisely preserving critical simplices and the separatrices that connect them. Our approach generates a series of edits during compression time, which are applied to the decompressed data to accurately reconstruct the complexes while maintaining the error within prescribed bounds. We design a workflow that iteratively fixes critical simplices and separatrices in alternating steps until convergence within finite iterations. Our approach addresses diverse application needs by offering users flexible options to balance compression efficiency and feature preservation. To enable effective integration with lossy compressors, we use GPU parallelism to enhance the performance of each workflow component. We conduct experiments on various datasets to demonstrate the effectiveness of our method in accurately preserving Morse-Smale complexes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17346v1</guid>
      <category>cs.GR</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxiao Li, Xin Liang, Bei Wang, Hanqi Guo</dc:creator>
    </item>
    <item>
      <title>SShaDe: a framework for scalable shape deformation via local representations</title>
      <link>https://arxiv.org/abs/2409.17961</link>
      <description>arXiv:2409.17961v1 Announce Type: new 
Abstract: With the increase of computational power for the available hardware, the demand for high-resolution data in computer graphics applications increases. Consequently, classical geometry processing techniques based on linear algebra solutions are starting to become obsolete. In this setting, we propose a novel approach for tackling mesh deformation tasks on high-resolution meshes. By reducing the input size with a fast remeshing technique and preserving a consistent representation of the original mesh with local reference frames, we provide a solution that is both scalable and robust. We extensively test our technique and compare it against state-of-the-art methods, proving that our approach can handle meshes with hundreds of thousands of vertices in tens of seconds while still achieving results comparable with the other solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17961v1</guid>
      <category>cs.GR</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filippo Maggioli, Daniele Baieri, Zorah L\"ahner, Simone Melzi</dc:creator>
    </item>
    <item>
      <title>AIM 2024 Challenge on Efficient Video Super-Resolution for AV1 Compressed Content</title>
      <link>https://arxiv.org/abs/2409.17256</link>
      <description>arXiv:2409.17256v1 Announce Type: cross 
Abstract: Video super-resolution (VSR) is a critical task for enhancing low-bitrate and low-resolution videos, particularly in streaming applications. While numerous solutions have been developed, they often suffer from high computational demands, resulting in low frame rates (FPS) and poor power efficiency, especially on mobile platforms. In this work, we compile different methods to address these challenges, the solutions are end-to-end real-time video super-resolution frameworks optimized for both high performance and low runtime. We also introduce a new test set of high-quality 4K videos to further validate the approaches. The proposed solutions tackle video up-scaling for two applications: 540p to 4K (x4) as a general case, and 360p to 1080p (x3) more tailored towards mobile devices. In both tracks, the solutions have a reduced number of parameters and operations (MACs), allow high FPS, and improve VMAF and PSNR over interpolation baselines. This report gauges some of the most efficient video super-resolution methods to date.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17256v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marcos V Conde, Zhijun Lei, Wen Li, Christos Bampis, Ioannis Katsavounidis, Radu Timofte</dc:creator>
    </item>
    <item>
      <title>Visualization of Age Distributions as Elements of Medical Data-Stories</title>
      <link>https://arxiv.org/abs/2409.17854</link>
      <description>arXiv:2409.17854v1 Announce Type: cross 
Abstract: In various fields, including medicine, age distributions are crucial. Despite widespread media coverage of health topics, there remains a need to enhance health communication. Narrative medical visualization is promising for improving information comprehension and retention. This study explores the most effective ways to present age distributions of diseases through narrative visualizations. We conducted a thorough analysis of existing visualizations, held workshops with a broad audience, and reviewed relevant literature. From this, we identified design choices focusing on comprehension, aesthetics, engagement, and memorability. We specifically tested three pictogram variants: pictograms as bars, stacked pictograms, and annotations. After evaluating 18 visualizations with 72 participants and three expert reviews, we determined that annotations were most effective for comprehension and aesthetics. However, traditional bar charts were preferred for engagement, and other variants were more memorable. The study provides a set of design recommendations based on these insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17854v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sophia Dowlatabadi, Bernhard Preim, Monique Meuschke</dc:creator>
    </item>
    <item>
      <title>Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions</title>
      <link>https://arxiv.org/abs/2409.17988</link>
      <description>arXiv:2409.17988v1 Announce Type: cross 
Abstract: The stark contrast in the design philosophy of an event camera makes it particularly ideal for operating under high-speed, high dynamic range and low-light conditions, where standard cameras underperform. Nonetheless, event cameras still suffer from some amount of motion blur, especially under these challenging conditions, in contrary to what most think. This is attributed to the limited bandwidth of the event sensor pixel, which is mostly proportional to the light intensity. Thus, to ensure that event cameras can truly excel in such conditions where it has an edge over standard cameras, it is crucial to account for event motion blur in downstream applications, especially reconstruction. However, none of the recent works on reconstructing Neural Radiance Fields (NeRFs) from events, nor event simulators, have considered the full effects of event motion blur. To this end, we propose, Deblur e-NeRF, a novel method to directly and effectively reconstruct blur-minimal NeRFs from motion-blurred events generated under high-speed motion or low-light conditions. The core component of this work is a physically-accurate pixel bandwidth model proposed to account for event motion blur under arbitrary speed and lighting conditions. We also introduce a novel threshold-normalized total variation loss to improve the regularization of large textureless patches. Experiments on real and novel realistically simulated sequences verify our effectiveness. Our code, event simulator and synthetic event dataset will be open-sourced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17988v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weng Fei Low, Gim Hee Lee</dc:creator>
    </item>
    <item>
      <title>Simplicial Approximation of Deforming 3D Spaces for Visualizing Fusion Plasma Simulation Data</title>
      <link>https://arxiv.org/abs/2309.02677</link>
      <description>arXiv:2309.02677v2 Announce Type: replace 
Abstract: We introduce a fast and invertible approximation for data simulated as 2D planar meshes with connectivities along the poloidal dimension in deforming 3D toroidal (donut-like) spaces generated by fusion simulations. In fusion simulations, scientific variables (e.g., density and temperature) are interpolated following a complex magnetic-field-line-following scheme in the toroidal space represented by a cylindrical coordinate system. This deformation in 3D space poses challenges for visualization tasks such as volume rendering and isosurfacing. To address these challenges, we propose a novel paradigm for visualizing and analyzing such data based on a newly developed algorithm for constructing a 3D simplicial mesh within the deforming 3D space. Our algorithm introduces no new nodes and operates with reduced time complexity, generating a mesh that connects the 2D meshes using tetrahedra while adhering to the constraints on node connectivities imposed by the magnetic field-line scheme. In the algorithm, we first divide the space into smaller partitions to reduce complexity based on the input geometries and constraints on connectivities. Then we independently search for a feasible tetrahedralization of each partition taking nonconvexity into consideration. We demonstrate use cases of our method for visualizing XGC simulation data on ITER and W7-X.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02677v2</guid>
      <category>cs.GR</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Congrong Ren, Hanqi Guo</dc:creator>
    </item>
    <item>
      <title>SR-CurvANN: Advancing 3D Surface Reconstruction through Curvature-Aware Neural Networks</title>
      <link>https://arxiv.org/abs/2407.17896</link>
      <description>arXiv:2407.17896v2 Announce Type: replace 
Abstract: Incomplete or missing data in three-dimensional (3D) models can lead to erroneous or flawed renderings, limiting their usefulness in applications such as visualization, geometric computation, and 3D printing. Conventional surface-repair techniques often fail to infer complex geometric details in missing areas. Neural networks successfully address hole-filling tasks in 2D images using inpainting techniques. The combination of surface reconstruction algorithms, guided by the model's curvature properties and the creativity of neural networks in the inpainting processes should provide realistic results in the hole completion task. In this paper, we propose a novel method entitled SR-CurvANN (Surface Reconstruction Based on Curvature-Aware Neural Networks) that incorporates neural network-based 2D inpainting to effectively reconstruct 3D surfaces. We train the neural networks with images that represent planar representations of the curvature at vertices of hundreds of 3D models. Once the missing areas have been inferred, a coarse-to-fine surface deformation process ensures that the surface fits the reconstructed curvature image. Our proposal makes it possible to learn and generalize patterns from a wide variety of training 3D models, generating comprehensive inpainted curvature images and surfaces. Experiments conducted on 959 models with several holes have demonstrated that SR-CurvANN excels in the shape completion process, filling holes with a remarkable level of realism and precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17896v2</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marina Hern\'andez-Bautista, Francisco J. Melero</dc:creator>
    </item>
    <item>
      <title>EAGLES: Efficient Accelerated 3D Gaussians with Lightweight EncodingS</title>
      <link>https://arxiv.org/abs/2312.04564</link>
      <description>arXiv:2312.04564v3 Announce Type: replace-cross 
Abstract: Recently, 3D Gaussian splatting (3D-GS) has gained popularity in novel-view scene synthesis. It addresses the challenges of lengthy training times and slow rendering speeds associated with Neural Radiance Fields (NeRFs). Through rapid, differentiable rasterization of 3D Gaussians, 3D-GS achieves real-time rendering and accelerated training. They, however, demand substantial memory resources for both training and storage, as they require millions of Gaussians in their point cloud representation for each scene. We present a technique utilizing quantized embeddings to significantly reduce per-point memory storage requirements and a coarse-to-fine training strategy for a faster and more stable optimization of the Gaussian point clouds. Our approach develops a pruning stage which results in scene representations with fewer Gaussians, leading to faster training times and rendering speeds for real-time rendering of high resolution scenes. We reduce storage memory by more than an order of magnitude all while preserving the reconstruction quality. We validate the effectiveness of our approach on a variety of datasets and scenes preserving the visual quality while consuming 10-20x lesser memory and faster training/inference speed. Project page and code is available https://efficientgaussian.github.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04564v3</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sharath Girish, Kamal Gupta, Abhinav Shrivastava</dc:creator>
    </item>
    <item>
      <title>Complexity as Design Material</title>
      <link>https://arxiv.org/abs/2409.07465</link>
      <description>arXiv:2409.07465v2 Announce Type: replace-cross 
Abstract: Complexity is often seen as a inherent negative in information design, with the job of the designer being to reduce or eliminate complexity, and with principles like Tufte's "data-ink ratio" or "chartjunk" to operationalize minimalism and simplicity in visualizations. However, in this position paper, we call for a more expansive view of complexity as a design material, like color or texture or shape: an element of information design that can be used in many ways, many of which are beneficial to the goals of using data to understand the world around us. We describe complexity as a phenomenon that occurs not just in visual design but in every aspect of the sensemaking process, from data collection to interpretation. For each of these stages, we present examples of ways that these various forms of complexity can be used (or abused) in visualization design. We ultimately call on the visualization community to build a more nuanced view of complexity, to look for places to usefully integrate complexity in multiple stages of the design process, and, even when the goal is to reduce complexity, to look for the non-visual forms of complexity that may have otherwise been overlooked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07465v2</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Windhager, Alfie Abduhl-Rahman, Mark-Jan Bludau, Nicole Hengesbach, Houda Lamqaddam, Isabel Meirelles, Bettina Speckmann, Michael Correll</dc:creator>
    </item>
  </channel>
</rss>
