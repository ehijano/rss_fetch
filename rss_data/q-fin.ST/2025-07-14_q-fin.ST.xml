<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.ST</link>
    <description>q-fin.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Jul 2025 04:02:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Framework for Predictive Directional Trading Based on Volatility and Causal Inference</title>
      <link>https://arxiv.org/abs/2507.09347</link>
      <description>arXiv:2507.09347v1 Announce Type: new 
Abstract: Purpose: This study introduces a novel framework for identifying and exploiting predictive lead-lag relationships in financial markets. We propose an integrated approach that combines advanced statistical methodologies with machine learning models to enhance the identification and exploitation of predictive relationships between equities. Methods: We employed a Gaussian Mixture Model (GMM) to cluster nine prominent stocks based on their mid-range historical volatility profiles over a three-year period. From the resulting clusters, we constructed a multi-stage causal inference pipeline, incorporating the Granger Causality Test (GCT), a customised Peter-Clark Momentary Conditional Independence (PCMCI) test, and Effective Transfer Entropy (ETE) to identify robust, predictive linkages. Subsequently, Dynamic Time Warping (DTW) and a K-Nearest Neighbours (KNN) classifier were utilised to determine the optimal time lag for trade execution. The resulting strategy was rigorously backtested. Results: The proposed volatility-based trading strategy, tested from 8 June 2023 to 12 August 2023, demonstrated substantial efficacy. The portfolio yielded a total return of 15.38%, significantly outperforming the 10.39% return of a comparative Buy-and-Hold strategy. Key performance metrics, including a Sharpe Ratio up to 2.17 and a win rate up to 100% for certain pairs, confirmed the strategy's viability. Conclusion: This research contributes a systematic and robust methodology for identifying profitable trading opportunities derived from volatility-based causal relationships. The findings have significant implications for both academic research in financial modelling and the practical application of algorithmic trading, offering a structured approach to developing resilient, data-driven strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09347v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ivan Letteri</dc:creator>
    </item>
    <item>
      <title>Mapping Crisis-Driven Market Dynamics: A Transfer Entropy and Kramers-Moyal Approach to Financial Networks</title>
      <link>https://arxiv.org/abs/2507.09554</link>
      <description>arXiv:2507.09554v1 Announce Type: new 
Abstract: Financial markets are dynamic, interconnected systems where local shocks can trigger widespread instability, challenging portfolio managers and policymakers. Traditional correlation analysis often miss the directionality and temporal dynamics of information flow. To address this, we present a unified framework integrating Transfer Entropy (TE) and the N-dimensional Kramers-Moyal (KM) expansion to map static and time-resolved coupling among four major indices: Nasdaq Composite (^IXIC), WTI crude oil (WTI), gold (GC=F), and the US Dollar Index (DX-Y.NYB). TE captures directional information flow. KM models non-linear stochastic dynamics, revealing interactions often overlooked by linear methods. Using daily data from August 11, 2014, to September 8, 2024, we compute returns, confirm non-stationary using a conduct sliding-window TE and KM analyses. We find that during the COVID-19 pandemic (March-June 2020) and the Russia-Ukraine crisis (Feb-Apr 2022), average TE increases by 35% and 28%, respectively, indicating heightened directional flow. Drift coefficients highlight gold-dollar interactions as a persistent safe-haven channel, while oil-equity linkages show regime shifts, weakening under stress and rebounding quickly. Our results expose the shortcomings of linear measures and underscore the value of combining information-theoretic and stochastic drift methods. This approach offers actionable insights for adaptive hedging and informs macro-prudential policy by revealing the evolving architecture of systemic risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09554v1</guid>
      <category>q-fin.ST</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pouriya Khalilian, Amirhossein N. Golestani, Mohammad Eslamifar, Mostafa T. Firouzjaee, Javad T. Firouzjaee</dc:creator>
    </item>
    <item>
      <title>Representation learning with a transformer by contrastive learning for money laundering detection</title>
      <link>https://arxiv.org/abs/2507.08835</link>
      <description>arXiv:2507.08835v1 Announce Type: cross 
Abstract: The present work tackles the money laundering detection problem. A new procedure is introduced which exploits structured time series of both qualitative and quantitative data by means of a transformer neural network. The first step of this procedure aims at learning representations of time series through contrastive learning (without any labels). The second step leverages these representations to generate a money laundering scoring of all observations. A two-thresholds approach is then introduced, which ensures a controlled false-positive rate by means of the Benjamini-Hochberg (BH) procedure. Experiments confirm that the transformer is able to produce general representations that succeed in exploiting money laundering patterns with minimal supervision from domain experts. It also illustrates the higher ability of the new procedure for detecting nonfraudsters as well as fraudsters, while keeping the false positive rate under control. This greatly contrasts with rule-based procedures or the ones based on LSTM architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08835v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harold Gu\'eneau (SAMM), Alain Celisse (LPP, MODAL), Pascal Delange</dc:creator>
    </item>
    <item>
      <title>An Accurate Discretized Approach to Parameter Estimation in the CKLS Model via the CIR Framework</title>
      <link>https://arxiv.org/abs/2507.10041</link>
      <description>arXiv:2507.10041v1 Announce Type: cross 
Abstract: This paper provides insight into the estimation and asymptotic behavior of parameters in interest rate models, focusing primarily on the Cox-Ingersoll-Ross (CIR) process and its extension -- the more general Chan-Karolyi-Longstaff-Sanders (CKLS) framework ($\alpha\in[0.5,1]$). The CIR process is widely used in modeling interest rates which possess the mean reverting feature. An Extension of CIR model, CKLS model serves as a foundational case for analyzing more complex dynamics. We employ Euler-Maruyama discretization to transform the continuous-time stochastic differential equations (SDEs) of these models into a discretized form that facilitates efficient simulation and estimation of parameters using linear regression techniques. We established the strong consistency and asymptotic normality of the estimators for the drift and volatility parameters, providing a theoretical underpinning for the parameter estimation process. Additionally, we explore the boundary behavior of these models, particularly in the context of unattainability at zero and infinity, by examining the scale and speed density functions associated with generalized SDEs involving polynomial drift and diffusion terms. Furthermore, we derive sufficient conditions for the existence of a stationary distribution within the CKLS framework and the corresponding stationary density function; and discuss its dependence on model parameters for $\alpha\in[0.5,1]$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10041v1</guid>
      <category>stat.AP</category>
      <category>q-fin.ST</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourojyoti Barick</dc:creator>
    </item>
    <item>
      <title>Modern approaches to building interpretable models of the property market using machine learning on the base of mass cadastral valuation</title>
      <link>https://arxiv.org/abs/2506.15723</link>
      <description>arXiv:2506.15723v2 Announce Type: replace 
Abstract: In this article, we review modern approaches to building interpretable models of property markets using machine learning on the base of mass valuation of property in the Primorye region, Russia. The researcher, lacking expertise in this topic, encounters numerous difficulties in the effort to build a good model. The main source of this is the huge difference between noisy real market data and ideal data which is very common in all types of tutorials on machine learning. This paper covers all stages of modeling: the collection of initial data, identification of outliers, the search and analysis of patterns in the data, the formation and final choice of price factors, the building of the model, and the evaluation of its efficiency. For each stage, we highlight potential issues and describe sound methods for overcoming emerging difficulties on actual examples. We show that the combination of classical linear regression with interpolation methods of geostatistics allows to build an effective model for land parcels. For flats, when many objects are attributed to one spatial point the application of geostatistical methods is difficult. Therefore we suggest linear regression with automatic generation and selection of additional rules on the base of decision trees, so called the RuleFit method. Thus we show, that despite such a strong restriction as the requirement of interpretability which is important in practical aspects, for example, legal matters, it is still possible to build effective models of real property markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15723v2</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Irina G. Tanashkina, Alexey S. Tanashkin, Alexander S. Maksimchuik, Anna Yu. Poshivailo</dc:creator>
    </item>
  </channel>
</rss>
