<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.IT</link>
    <description>cs.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 02:33:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Holographic Metasurface-Based Beamforming for Multi-Altitude LEO Satellite Networks</title>
      <link>https://arxiv.org/abs/2501.04164</link>
      <description>arXiv:2501.04164v1 Announce Type: new 
Abstract: Low Earth Orbit (LEO) satellite networks are capable of improving the global Internet service coverage. In this context, we propose a hybrid beamforming design for holographic metasurface based terrestrial users in multi-altitude LEO satellite networks. Firstly, the holographic beamformer is optimized by maximizing the downlink channel gain from the serving satellite to the terrestrial user. Then, the digital beamformer is designed by conceiving a minimum mean square error (MMSE) based detection algorithm for mitigating the interference arriving from other satellites. To dispense with excessive overhead of full channel state information (CSI) acquisition of all satellites, we propose a low-complexity MMSE beamforming algorithm that only relies on the distribution of the LEO satellite constellation harnessing stochastic geometry, which can achieve comparable throughput to that of the algorithm based on the full CSI in the case of a dense LEO satellite deployment. Furthermore, it outperforms the maximum ratio combining (MRC) algorithm, thanks to its inter-satellite interference mitigation capacity. The simulation results show that our proposed holographic metasurface based hybrid beamforming architecture is capable of outperforming the state-of-the-art antenna array architecture in terms of its throughput, given the same physical size of the transceivers. Moreover, we demonstrate that the beamforming performance attained can be substantially improved by taking into account the mutual coupling effect, imposed by the dense placement of the holographic metasurface elements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04164v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingchao Li, Mohammed El-Hajjar, Kaijun Cao, Chao Xu, Harald Haas, Lajos Hanzo</dc:creator>
    </item>
    <item>
      <title>Computation and Communication Co-scheduling for Timely Multi-Task Inference at the Wireless Edge</title>
      <link>https://arxiv.org/abs/2501.04231</link>
      <description>arXiv:2501.04231v1 Announce Type: new 
Abstract: In multi-task remote inference systems, an intelligent receiver (e.g., command center) performs multiple inference tasks (e.g., target detection) using data features received from several remote sources (e.g., edge sensors). Key challenges to facilitating timely inference in these systems arise from (i) limited computational power of the sources to produce features from their inputs, and (ii) limited communication resources of the channels to carry simultaneous feature transmissions to the receiver. We develop a novel computation and communication co-scheduling methodology which determines feature generation and transmission scheduling to minimize inference errors subject to these resource constraints. Specifically, we formulate the co-scheduling problem as a weakly-coupled Markov decision process with Age of Information (AoI)-based timeliness gauging the inference errors. To overcome its PSPACE-hard complexity, we analyze a Lagrangian relaxation of the problem, which yields gain indices assessing the improvement in inference error for each potential feature generation-transmission scheduling action. Based on this, we develop a maximum gain first (MGF) policy which we show is asymptotically optimal for the original problem as the number of inference tasks increases. Experiments demonstrate that MGF obtains significant improvements over baseline policies for varying tasks, channels, and sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04231v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Kamran Chowdhury Shisher, Adam Piaseczny, Yin Sun, Christopher G. Brinton</dc:creator>
    </item>
    <item>
      <title>A note on the differential spectrum of a class of locally APN functions</title>
      <link>https://arxiv.org/abs/2501.04233</link>
      <description>arXiv:2501.04233v1 Announce Type: new 
Abstract: Let $\gf_{p^n}$ denote the finite field containing $p^n$ elements, where $n$ is a positive integer and $p$ is a prime. The function $f_u(x)=x^{\frac{p^n+3}{2}}+ux^2$ over $\gf_{p^n}[x]$ with $u\in\gf_{p^n}\setminus\{0,\pm1\}$ was recently studied by Budaghyan and Pal in \cite{Budaghyan2024ArithmetizationorientedAP}, whose differential uniformity is at most $5$ when $p^n\equiv3~(mod~4)$. In this paper, we study the differential uniformity and the differential spectrum of $f_u$ for $u=\pm1$. We first give some properties of the differential spectrum of any cryptographic function. Moreover, by solving some systems of equations over finite fields, we express the differential spectrum of $f_{\pm1}$ in terms of the quadratic character sums.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04233v1</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haode Yan, Ketong Ren</dc:creator>
    </item>
    <item>
      <title>Separate Source Channel Coding Is Still What You Need: An LLM-based Rethinking</title>
      <link>https://arxiv.org/abs/2501.04285</link>
      <description>arXiv:2501.04285v1 Announce Type: new 
Abstract: Along with the proliferating research interest in Semantic Communication (SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to the widely assumed existence in efficiently delivering information semantics. %has emerged as a pivotal area of research, aiming to enhance the efficiency and reliability of information transmission through deep learning-based methods. Nevertheless, this paper challenges the conventional JSCC paradigm, and advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy the underlying more degree of freedom for optimization. We demonstrate that SSCC, after leveraging the strengths of Large Language Model (LLM) for source coding and Error Correction Code Transformer (ECCT) complemented for channel decoding, offers superior performance over JSCC. Our proposed framework also effectively highlights the compatibility challenges between SemCom approaches and digital communication systems, particularly concerning the resource costs associated with the transmission of high precision floating point numbers. Through comprehensive evaluations, we establish that empowered by LLM-based compression and ECCT-enhanced error correction, SSCC remains a viable and effective solution for modern communication systems. In other words, separate source and channel coding is still what we need!</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04285v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianqi Ren, Rongpeng Li, Ming-min Zhao, Xianfu Chen, Guangyi Liu, Yang Yang, Zhifeng Zhao, Honggang Zhang</dc:creator>
    </item>
    <item>
      <title>Finite Dimensional Lattice Codes with Self Error-Detection and Retry Decoding</title>
      <link>https://arxiv.org/abs/2501.04307</link>
      <description>arXiv:2501.04307v1 Announce Type: new 
Abstract: Lattice codes with optimal decoding coefficient are capacity-achieving when dimension $N \rightarrow \infty$. In communications systems, finite dimensional lattice codes are considered, where the optimal decoding coefficients may still fail decoding even when $R&lt; C$. This paper presents a new retry decoding scheme for finite dimensional lattice-based transmissions. When decoding errors are detected, the receiver is allowed to adjust the value of decoding coefficients and retry decoding, instead of requesting a re-transmission immediately which causes high latency. This scheme is considered for both point-to-point single user transmission and compute-forward (CF) relaying with power unconstrained relays, by which a lower word error rate (WER) is achieved than conventional one-shot decoding with optimal coefficients. A lattice/lattice code construction, called CRC-embedded lattice/lattice code, is presented to provide physical layer error detection to enable retry decoding. For CF relaying, a shaping lattice design is given so that the decoder is able to detect errors from CF linear combinations without requiring individual users' messages. The numerical results show gains of up to 1.31 dB and 1.08 dB at error probability $10^{-5}$ for a 2-user CF relay using 128- and 256-dimensional lattice codes with optimized CRC length and 2 decoding trials in total.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04307v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiajie Xue, Brian M. Kurkoski</dc:creator>
    </item>
    <item>
      <title>Lower Bound on the Error Rate of Genie-Aided Lattice Decoding</title>
      <link>https://arxiv.org/abs/2501.04328</link>
      <description>arXiv:2501.04328v1 Announce Type: new 
Abstract: A genie-aided decoder for finite dimensional lattice codes is considered. The decoder may exhaustively search through all possible scaling factors $\alpha \in \mathbb{R}$. We show that this decoder can achieve lower word error rate (WER) than the one-shot decoder using $\alpha_{MMSE}$ as a scaling factor. A lower bound on the WER for the decoder is found by considering the covering sphere of the lattice Voronoi region. The proposed decoder and the bound are valid for both power-constrained lattice codes and lattices. If the genie is applied at the decoder, E8 lattice code has 0.5 dB gain and BW16 lattice code has 0.4 dB gain at WER of $10^{-4}$ compared with the one-shot decoder using $\alpha_{MMSE}$. A method for estimating the WER of the decoder is provided by considering the effective sphere of the lattice Voronoi region, which shows an accurate estimate for E8 and BW16 lattice codes. In the case of per-dimension power $P \rightarrow \infty$, an asymptotic expression of the bound is given in a closed form. A practical implementation of a simplified decoder is given by considering CRC-embedded $n=128$ polar code lattice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04328v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ISIT50566.2022.9834527</arxiv:DOI>
      <arxiv:journal_reference>2022 IEEE International Symposium on Information Theory (ISIT), pp. 3232-3237</arxiv:journal_reference>
      <dc:creator>Jiajie Xue, Brian M. Kurkoski</dc:creator>
    </item>
    <item>
      <title>Evidence-based multimodal fusion on structured EHRs and free-text notes for ICU outcome prediction</title>
      <link>https://arxiv.org/abs/2501.04389</link>
      <description>arXiv:2501.04389v1 Announce Type: new 
Abstract: Objective: Accurate Intensive Care Unit (ICU) outcome prediction is critical for improving patient treatment quality and ICU resource allocation. Existing research mainly focuses on structured data and lacks effective frameworks to integrate clinical notes from heterogeneous electronic health records (EHRs). This study aims to explore a multimodal framework based on evidence theory that can effectively combine heterogeneous structured EHRs and free-text notes for accurate and reliable ICU outcome prediction. Materials and Methods: We proposed an evidence-based multimodal fusion framework to predict ICU outcomes, including mortality and prolonged length of stay (PLOS), by utilizing both structured EHR data and free-text notes from the MIMIC-III database. We compare the performance against baseline models that use only structured EHRs, free-text notes, or existing multimodal approaches. Results: The results demonstrate that the evidence-based multimodal fusion model achieved both accurate and reliable prediction. Specifically, it outperformed the best baseline by 1.05%/1.02% in BACC, 9.74%/6.04% in F1 score, 1.28%/0.9% in AUROC, and 6.21%/2.68% in AUPRC for predicting mortality and PLOS, respectively. Additionally, it improved the reliability of the predictions with a 26.8%/15.1% reduction in the Brier score and a 25.0%/13.3% reduction in negative log-likelihood. Conclusion: This study demonstrates that the evidence-based multimodal fusion framework can serve as a strong baseline for predictions using structured EHRs and free-text notes. It effectively reduces false positives, which can help improve the allocation of medical resources in the ICU. This framework can be further applied to analyze multimodal EHRs for other clinical tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04389v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yucheng Ruan, Daniel J. Tan, See Kiong Ng, Ling Huang, Mengling Feng</dc:creator>
    </item>
    <item>
      <title>Approximation Rates in Fr\'echet Metrics: Barron Spaces, Paley-Wiener Spaces, and Fourier Multipliers</title>
      <link>https://arxiv.org/abs/2501.04023</link>
      <description>arXiv:2501.04023v1 Announce Type: cross 
Abstract: Operator learning is a recent development in the simulation of Partial Differential Equations (PDEs) by means of neural networks. The idea behind this approach is to learn the behavior of an operator, such that the resulting neural network is an (approximate) mapping in infinite-dimensional spaces that is capable of (approximately) simulating the solution operator governed by the PDE. In our work, we study some general approximation capabilities for linear differential operators by approximating the corresponding symbol in the Fourier domain. Analogous to the structure of the class of H\"ormander-Symbols, we consider the approximation with respect to a topology that is induced by a sequence of semi-norms. In that sense, we measure the approximation error in terms of a Fr\'echet metric, and our main result identifies sufficient conditions for achieving a predefined approximation error. Secondly, we then focus on a natural extension of our main theorem, in which we manage to reduce the assumptions on the sequence of semi-norms. Based on some existing approximation result for the exponential spectral Barron space, we then present a concrete example of symbols that can be approximated well, and we also show the analogy of this approximation to the design of digital filters in Signal Processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04023v1</guid>
      <category>math.NA</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Abdeljawad, Thomas Dittrich</dc:creator>
    </item>
    <item>
      <title>A Novel Non-Stationary Channel Emulator for 6G MIMO Wireless Channels</title>
      <link>https://arxiv.org/abs/2501.04240</link>
      <description>arXiv:2501.04240v1 Announce Type: cross 
Abstract: The performance evaluation of sixth generation (6G) communication systems is anticipated to be a controlled and repeatable process in the lab, which brings up the demand for wireless channel emulators. However, channel emulation for 6G space-time-frequency (STF) non-stationary channels is missing currently. In this paper, a non-stationary multiple-input multiple-output (MIMO) geometry-based stochastic model (GBSM) that accurately characterizes the channel STF properties is introduced firstly. Then, a subspace-based method is proposed for reconstructing the channel fading obtained from the GBSM and a channel emulator architecture with frequency domain processing is presented for 6G MIMO systems. Moreover, the spatial time-varying channel transfer functions (CTFs) of the channel simulation and the channel emulation are compared and analyzed. The Doppler power spectral density (PSD) and delay PSD are further derived and compared between the channel model simulation and subspace-based emulation. The results demonstrate that the proposed channel emulator is capable of reproducing the non-stationary channel characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04240v1</guid>
      <category>eess.SY</category>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Zong, Lijian Xin, Jie Huang, Cheng-Xiang Wang</dc:creator>
    </item>
    <item>
      <title>Beam Domain Channel Estimation for Spatial Non-Stationary Massive MIMO Systems</title>
      <link>https://arxiv.org/abs/2501.04242</link>
      <description>arXiv:2501.04242v1 Announce Type: cross 
Abstract: In massive multiple-input multiple-output (MIMO) systems, the channel estimation scheme is subject to the spatial non-stationarity and inevitably power leakage in the beam domain. In this paper, a beam domain channel estimation scheme is investigated for spatial non-stationary (SNS) massive MIMO systems considering power leakage. %a novel beam domain channel estimation scheme is proposed for spatial non-stationary (SNS) massive MIMO systems. Specifically, a realistic massive MIMO beam domain channel model (BDCM) is introduced to capture the spatial non-stationarity considering power leakage by introducing the illustration of visibility region (VR). Then, a beam domain structure-based sparsity adaptive matching pursuit (BDS-SAMP) scheme is proposed based on the cross-block sparse structure and power ratio threshold of beam domain channel. Finally, the simulation results validate the accuracy of proposed BDS-SAMP scheme with low pilot overhead and reasonable complexity by comparing with conventional schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04242v1</guid>
      <category>eess.SY</category>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Hou, Hengtai Chang, Cheng-Xiang Wang, Jie Huang, Songjiang Yang</dc:creator>
    </item>
    <item>
      <title>Tracking UWB Devices Through Radio Frequency Fingerprinting Is Possible</title>
      <link>https://arxiv.org/abs/2501.04401</link>
      <description>arXiv:2501.04401v1 Announce Type: cross 
Abstract: Ultra-wideband (UWB) is a state-of-the-art technology designed for applications requiring centimeter-level localization. Its widespread adoption by smartphone manufacturer naturally raises security and privacy concerns. Successfully implementing Radio Frequency Fingerprinting (RFF) to UWB could enable physical layer security, but might also allow undesired tracking of the devices. The scope of this paper is to explore the feasibility of applying RFF to UWB and investigates how well this technique generalizes across different environments. We collected a realistic dataset using off-the-shelf UWB devices with controlled variation in device positioning. Moreover, we developed an improved deep learning pipeline to extract the hardware signature from the signal data. In stable conditions, the extracted RFF achieves over 99% accuracy. While the accuracy decreases in more changing environments, we still obtain up to 76% accuracy in untrained locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04401v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibaud Ardoin, Niklas Pauli, Benedikt Gro{\ss}, Mahsa Kholghi, Khan Reaz, Gerhard Wunder</dc:creator>
    </item>
    <item>
      <title>Re-ranking the Context for Multimodal Retrieval Augmented Generation</title>
      <link>https://arxiv.org/abs/2501.04695</link>
      <description>arXiv:2501.04695v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge to generate a response within a context with improved accuracy and reduced hallucinations. However, multi-modal RAG systems face unique challenges: (i) the retrieval process may select irrelevant entries to user query (e.g., images, documents), and (ii) vision-language models or multi-modal language models like GPT-4o may hallucinate when processing these entries to generate RAG output. In this paper, we aim to address the first challenge, i.e, improving the selection of relevant context from the knowledge-base in retrieval phase of the multi-modal RAG. Specifically, we leverage the relevancy score (RS) measure designed in our previous work for evaluating the RAG performance to select more relevant entries in retrieval process. The retrieval based on embeddings, say CLIP-based embedding, and cosine similarity usually perform poorly particularly for multi-modal data. We show that by using a more advanced relevancy measure, one can enhance the retrieval process by selecting more relevant pieces from the knowledge-base and eliminate the irrelevant pieces from the context by adaptively selecting up-to-$k$ entries instead of fixed number of entries. Our evaluation using COCO dataset demonstrates significant enhancement in selecting relevant context and accuracy of the generated response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04695v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.IR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>Coded Distributed Computing with Pre-set Assignments of Data and Output Functions</title>
      <link>https://arxiv.org/abs/2201.06300</link>
      <description>arXiv:2201.06300v3 Announce Type: replace 
Abstract: Coded distributed computing can reduce the communication load for distributed computing systems by introducing redundant computation and creating multicasting opportunities. However, the existing schemes require delicate data placement and output function assignment, which is not feasible when distributed nodes fetch data without the orchestration of a master node. In this paper, we consider the general systems where the data placement and output function assignment are arbitrary but pre-set. We propose two coded computing schemes, One-shot Coded Transmission (OSCT) and Few-shot Coded Transmission (FSCT), to reduce the communication load. Both schemes first group the nodes into clusters and divide the transmission of each cluster into multiple rounds, and then design coded transmission in each round to maximize the multicast gain. The key difference between OSCT and FSCT is that the former uses a one-shot transmission where each encoded message can be decoded independently by the intended nodes, while the latter allows each node to jointly decode multiple received symbols to achieve potentially larger multicast gains. Furthermore, based on the lower bound proposed by Yu et al., we derive sufficient conditions for the optimality of OSCT and FSCT, respectively. This not only recovers the existing optimality results but also includes some cases where our schemes are optimal while others are not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.06300v3</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhan Wang, Youlong Wu</dc:creator>
    </item>
    <item>
      <title>Linear complementary pairs of skew constacyclic codes</title>
      <link>https://arxiv.org/abs/2312.07183</link>
      <description>arXiv:2312.07183v3 Announce Type: replace 
Abstract: Linear complementary pairs (LCPs) of codes have been studied since they were introduced in the context of discussing mitigation measures against possible hardware attacks to integrated circuits. In this situation, the security parameters for LCPs of codes are defined as the (Hamming) distance and the dual distance of the codes in the pair. We study the properties of LCPs of skew constacyclic codes, since their algebraic structure provides tools for studying their duals and their distances. As a result, we give a characterization for those pairs, as well as multiple results that lead to constructing pairs with designed security parameters. We extend skew BCH codes to a constacyclic context and show that an LCP of codes can be immediately constructed from a skew BCH constacyclic code. Additionally, we describe a Hamming weight-preserving automorphism group in the set of skew constacyclic codes, which can be used for constructing LCPs of codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07183v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.RA</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>F. J. Lobillo, Jos\'e Manuel Mu\~noz</dc:creator>
    </item>
    <item>
      <title>Correlated Privacy Mechanisms for Differentially Private Distributed Mean Estimation</title>
      <link>https://arxiv.org/abs/2407.03289</link>
      <description>arXiv:2407.03289v2 Announce Type: replace 
Abstract: Differentially private distributed mean estimation (DP-DME) is a fundamental building block in privacy-preserving federated learning, where a central server estimates the mean of $d$-dimensional vectors held by $n$ users while ensuring $(\epsilon,\delta)$-DP. Local differential privacy (LDP) and distributed DP with secure aggregation (SA) are the most common notions of DP used in DP-DME settings with an untrusted server. LDP provides strong resilience to dropouts, colluding users, and adversarial attacks, but suffers from poor utility. In contrast, SA-based DP-DME achieves an $O(n)$ utility gain over LDP in DME, but requires increased communication and computation overheads and complex multi-round protocols to handle dropouts and attacks. In this work, we present a generalized framework for DP-DME, that captures LDP and SA-based mechanisms as extreme cases. Our framework provides a foundation for developing and analyzing a variety of DP-DME protocols that leverage correlated privacy mechanisms across users. To this end, we propose CorDP-DME, a novel DP-DME mechanism based on the correlated Gaussian mechanism, that spans the gap between DME with LDP and distributed DP. We prove that CorDP-DME offers a favorable balance between utility and resilience to dropout and collusion. We provide an information-theoretic analysis of CorDP-DME, and derive theoretical guarantees for utility under any given privacy parameters and dropout/colluding user thresholds. Our results demonstrate that (anti) correlated Gaussian DP mechanisms can significantly improve utility in mean estimation tasks compared to LDP -- even in adversarial settings -- while maintaining better resilience to dropouts and attacks compared to distributed DP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03289v2</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sajani Vithana, Viveck R. Cadambe, Flavio P. Calmon, Haewon Jeong</dc:creator>
    </item>
    <item>
      <title>On the Performance of Short Binary BCH Codes for Ultra-Low Latency Wireless Communications</title>
      <link>https://arxiv.org/abs/2412.18725</link>
      <description>arXiv:2412.18725v2 Announce Type: replace 
Abstract: In recent years, polar codes have been considered for communication systems that require high re-liability and ultra-low latency, such as sixth generation (6G) wireless communications. This paper presents simulation results showing that short binary extended BCH (eBCH) codes with low-complexity decoding outperform polar codes for lengths 64 and 128. In the simulations, polar mapping under additive white Gaussian noise (AWGN) is assumed and ordered-statistics decoding (OSD) of eBCH codes is compared with CRC-aided successive-cancellation list decoding (SCLD-CRC) of polar codes of the same lengths and rates. The results indicate that short-length binary eBCH codes achieve lower average bit error rate values (higher reliability) and thus should be considered as strong candidates in communication systems requiring extremely low latency, i.e., short code lengths of up to 128 bits. The eBCH simulation results are obtained with OSD and re-processing order equal to one so that complexity is comparable to SCLD-CRC. Specifically, error performances are quantified of length-64 and selected length-128 eBCH codes with order-1 OSD and polar codes with SCLD-CRC for the same rates and lengths. These results serve to verify that short binary eBCH codes do indeed outperform short polar codes with comparable decoding complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18725v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Morelos-Zaragoza (San Jose State University), Jeffrey Ma (San Jose State University)</dc:creator>
    </item>
    <item>
      <title>Tightening continuity bounds for entropies and bounds on quantum capacities</title>
      <link>https://arxiv.org/abs/2310.17329</link>
      <description>arXiv:2310.17329v3 Announce Type: replace-cross 
Abstract: Uniform continuity bounds on entropies are generally expressed in terms of a single distance measure between a pair of probability distributions or quantum states, typically, the total variation distance or trace distance. However, if an additional distance measure between the probability distributions or states is known, then the continuity bounds can be significantly strengthened. Here, we prove a tight uniform continuity bound for the Shannon entropy in terms of both the local- and total variation distances, sharpening an inequality proven in [I. Sason, IEEE Trans. Inf. Th., 59, 7118 (2013)]. We also obtain a uniform continuity bound for the von Neumann entropy in terms of both the operator norm- and trace distances. The bound is tight when the quotient of the trace distance by the operator norm distance is an integer. We then apply our results to compute upper bounds on the quantum- and private classical capacities of channels. We begin by refining the concept of approximate degradable channels, namely, $\varepsilon$-degradable channels, which are, by definition, $\varepsilon$-close in diamond norm to their complementary channel when composed with a degrading channel. To this end, we introduce the notion of $(\varepsilon,\nu)$-degradable channels; these are $\varepsilon$-degradable channels that are, in addition, $\nu$-close in completely bounded spectral norm to their complementary channel, when composed with the same degrading channel. This allows us to derive improved upper bounds to the quantum- and private classical capacities of such channels. Moreover, these bounds can be further improved by considering certain unstabilized versions of the above norms. We show that upper bounds on the latter can be efficiently expressed as semidefinite programs. We illustrate our results by obtaining a new upper bound on the quantum capacity of the qubit depolarizing channel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17329v3</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JSAIT.2024.3469929</arxiv:DOI>
      <arxiv:journal_reference>IEEE Journal on Selected Areas in Information Theory, vol. 5, p. 645-658 (2024)</arxiv:journal_reference>
      <dc:creator>Michael G. Jabbour, Nilanjana Datta</dc:creator>
    </item>
    <item>
      <title>Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search</title>
      <link>https://arxiv.org/abs/2403.09570</link>
      <description>arXiv:2403.09570v4 Announce Type: replace-cross 
Abstract: In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate. Furthermore, higher-fidelity evaluations of the optimization objectives often entail a larger cost. Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information about the optimal value or the optimal solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the current task with the goal of collecting information transferable to future tasks. The proposed method transfers across tasks distributions over parameters of a Gaussian process surrogate model by implementing particle-based variational Bayesian updates. Theoretical insights based on the analysis of the expected regret substantiate the benefits of acquiring transferable knowledge across tasks. Furthermore, experimental results across synthetic and real-world examples reveal that the proposed acquisition strategy that caters to future tasks can significantly improve the optimization efficiency as soon as a sufficient number of tasks is processed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09570v4</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunchuan Zhang, Sangwoo Park, Osvaldo Simeone</dc:creator>
    </item>
  </channel>
</rss>
