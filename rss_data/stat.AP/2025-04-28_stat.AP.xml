<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Apr 2025 03:05:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Auto-Regressive Standard Precipitation Index: A Bayesian Approach for Drought Characterization</title>
      <link>https://arxiv.org/abs/2504.18197</link>
      <description>arXiv:2504.18197v1 Announce Type: new 
Abstract: This study proposes Auto-Regressive Standardized Precipitation Index (ARSPI) as a novel alternative to the traditional Standardized Precipitation Index (SPI) for measuring drought by relaxing the assumption of independent and identical rainfall distribution over time. ARSPI utilizes an auto-regressive framework to tackle the auto-correlated characteristics of precipitation, providing a more precise depiction of drought dynamics. The proposed model integrates a spike-and-slab log-normal distribution for zero rainfall seasons. The Bayesian Monte Carlo Markov Chain (MCMC) approach simplifies the SPI computation using the non-parametric predictive density estimation of total rainfall across various time windows from simulated samples. The MCMC simulations further ensure robust estimation of severity, duration, peak and return period with greater precision. This study also provides a comparison between the performances of ARSPI and SPI using the precipitation data from the Colorado River Basin (1893-1991). ARSPI emerges to be more efficient than the benchmark SPI in terms of model fit. ARSPI shows enhanced sensitivity to climatic extremes, making it a valuable tool for hydrological research and water resource management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18197v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soham Ghosh, Sujay Mukhoti, Pritee Sharma</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap: Introducing Joint Models for Longitudinal and Time-to-event Data in the Social Sciences</title>
      <link>https://arxiv.org/abs/2504.18288</link>
      <description>arXiv:2504.18288v1 Announce Type: new 
Abstract: In time-to-event analyses in social sciences, there often exist endogenous time-varying variables, where the event status is correlated with the trajectory of the covariate itself. Ignoring this endogeneity will result in biased estimates. In the field of biostatistics this issue is tackled by estimating a joint model for longitudinal and time-to-event data as it handles endogenous covariates properly. This method is underused in the social sciences even though it is very useful to model longitudinal and time-to-event processes appropriately. Therefore, this paper provides a gentle introduction to the method of joint models and highlights its advantages for social science research questions. We demonstrate its usage on an example on marital satisfaction and marriage dissolution and compare the results with classical approaches such as a time-to-event model with a time-varying covariate. In addition to demonstrating the method, our results contribute to the understanding of the relationship between marriage satisfaction, marriage dissolution and other covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18288v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sophie Potts, Anja Rappl, Karin Kurz, Elisabeth Bergherr</dc:creator>
    </item>
    <item>
      <title>Bernstein Polynomial Processes for Continuous Time Change Detection</title>
      <link>https://arxiv.org/abs/2504.17876</link>
      <description>arXiv:2504.17876v1 Announce Type: cross 
Abstract: There is a lack of methodological results for continuous time change detection due to the challenges of noninformative prior specification and efficient posterior inference in this setting. Most methodologies to date assume data are collected according to uniformly spaced time intervals. This assumption incurs bias in the continuous time setting where, a priori, two consecutive observations measured closely in time are less likely to change than two consecutive observations that are far apart in time. Models proposed in this setting have required MCMC sampling which is not ideal. To address these issues, we derive the heterogeneous continuous time Markov chain that models change point transition probabilities noninformatively. By construction, change points under this model can be inferred efficiently using the forward backward algorithm and do not require MCMC sampling. We then develop a novel loss function for the continuous time setting, derive its Bayes estimator, and demonstrate its performance on synthetic data. A case study using time series of remotely sensed observations is then carried out on three change detection applications. To reduce falsely detected changes in this setting, we develop a semiparametric mean function that captures interannual variability due to weather in addition to trend and seasonal components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17876v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Cunha, Mark Friedl, Luis Carvalho</dc:creator>
    </item>
    <item>
      <title>Adapting Probabilistic Risk Assessment for AI</title>
      <link>https://arxiv.org/abs/2504.18536</link>
      <description>arXiv:2504.18536v1 Announce Type: cross 
Abstract: Modern general-purpose artificial intelligence (AI) systems present an urgent risk management challenge, as their rapidly evolving capabilities and potential for catastrophic harm outpace our ability to reliably assess their risks. Current methods often rely on selective testing and undocumented assumptions about risk priorities, frequently failing to make a serious attempt at assessing the set of pathways through which Al systems pose direct or indirect risks to society and the biosphere. This paper introduces the probabilistic risk assessment (PRA) for AI framework, adapting established PRA techniques from high-reliability industries (e.g., nuclear power, aerospace) for the new challenges of advanced AI. The framework guides assessors in identifying potential risks, estimating likelihood and severity, and explicitly documenting evidence, underlying assumptions, and analyses at appropriate granularities. The framework's implementation tool synthesizes the results into a risk report card with aggregated risk estimates from all assessed risks. This systematic approach integrates three advances: (1) Aspect-oriented hazard analysis provides systematic hazard coverage guided by a first-principles taxonomy of AI system aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk pathway modeling analyzes causal chains from system aspects to societal impacts using bidirectional analysis and incorporating prospective techniques; and (3) Uncertainty management employs scenario decomposition, reference scales, and explicit tracing protocols to structure credible projections with novelty or limited data. Additionally, the framework harmonizes diverse assessment methods by integrating evidence into comparable, quantified absolute risk estimates for critical decisions. We have implemented this as a workbook tool for AI developers, evaluators, and regulators, available on the project website.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18536v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Katariina Wisakanto, Joe Rogero, Avyay M. Casheekar, Richard Mallah</dc:creator>
    </item>
    <item>
      <title>Democracy and Growth in the 21st Century</title>
      <link>https://arxiv.org/abs/2104.07617</link>
      <description>arXiv:2104.07617v4 Announce Type: replace-cross 
Abstract: We find that, in the 21st century, democracy has persistent negative impacts on growth in GDP and night-time light intensity. This finding emerges from five different instrumental variable strategies that account for potential invalidity in some of the instruments. Our analysis also suggests a key mechanism: In this century, many electoral democracies shift toward populism and protectionism. These political changes weaken trade and investment, collectively dampening economic growth. Democracies also experienced lower growth in subjective life satisfaction among citizens. However, democracy causes slower growth in CO2 emissions and energy use, suggesting a trade-off between economic growth and socio-environmental performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.07617v4</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yusuke Narita</dc:creator>
    </item>
    <item>
      <title>Flexible Covariate Adjustments in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2107.07942</link>
      <description>arXiv:2107.07942v5 Announce Type: replace-cross 
Abstract: Empirical regression discontinuity (RD) studies often include covariates in their specifications to increase the precision of their estimates. In this paper, we propose a novel class of estimators that use such covariate information more efficiently than existing methods and can accommodate many covariates. Our estimators are simple to implement and involve running a standard RD analysis after subtracting a function of the covariates from the original outcome variable. We characterize the function of the covariates that minimizes the asymptotic variance of these estimators. We also show that the conventional RD framework gives rise to a special robustness property which implies that the optimal adjustment function can be estimated flexibly via modern machine learning techniques without affecting the first-order properties of the final RD estimator. We demonstrate our methods' scope for efficiency improvements by reanalyzing data from a large number of recently published empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.07942v5</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Noack, Tomasz Olma, Christoph Rothe</dc:creator>
    </item>
    <item>
      <title>Post-hoc regularisation of unfolded cross-section measurements</title>
      <link>https://arxiv.org/abs/2207.02125</link>
      <description>arXiv:2207.02125v5 Announce Type: replace-cross 
Abstract: Neutrino cross-section measurements are often presented as unfolded binned distributions in "true" variables. The ill-posedness of the unfolding problem can lead to results with strong anti-correlations and fluctuations between bins, which make comparisons to theoretical models in plots difficult. To alleviate this problem, one can introduce regularisation terms in the unfolding procedure. These suppress the anti-correlations in the result, at the cost of introducing some bias towards the expected shape of the data. This paper discusses a method using simple linear algebra, which makes it is possible to regularise any result that is presented as a central value and a covariance matrix. This "post-hoc" regularisation is generally much faster than repeating the unfolding method with different regularisation terms. The method also yields a regularisation matrix which connects the regularised to the unregularised result, and can be used to retain the full statistical power of the unregularised result when publishing a nicer looking regularised result. In addition to the regularisation method, this paper also presents some thoughts on the presentation of correlated data in general. When using the proposed method, the bias of the regularisation can be understood as a data visualisation problem rather than a statistical one. The strength of the regularisation can be chosen by minimising the difference between the implicitly uncorrelated distribution shown in the plots and the actual distribution described by the unregularised central value and covariance. Aside from minimising the difference between the shown and the actual result, additional information can be provided by showing the local log-likelihood gradient of the models shown in the plots. This adds more information about where the model is "pulled" by the data than just comparing the bin values to the data's central values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.02125v5</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/1748-0221/17/10/P10021</arxiv:DOI>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
    <item>
      <title>Predicting Coastal Water Levels in the Context of Climate Change Using Kolmogorov-Zurbenko Time Series Analysis Methods</title>
      <link>https://arxiv.org/abs/2412.09419</link>
      <description>arXiv:2412.09419v2 Announce Type: replace-cross 
Abstract: Given recent increases in ocean water levels brought on by climate change, this investigation decomposed changes in coastal water levels into its fundamental components to predict maximum water levels for a given coastal location. The study focused on Virginia Key, Florida, in the United States, located near the coast of Miami. Hourly mean lower low water (MLLW) levels were obtained from the National Data Buoy Center from January 28, 1994, through December 31, 2023. In the temporal dimension, Kolmogorov-Zurbenko filters were used to extract long-term trends, annual and daily tides, and higher frequency harmonics, while in the spectral dimension, Kolmogorov-Zurbenko periodograms with DiRienzo-Zurbenko algorithm smoothing were used to confirm known tidal frequencies and periods. A linear model predicted that the long-term trend in water level will rise 2.02 feet from January 1994 to December 2050, while a quadratic model predicted a rise of 5.91 during the same period. In addition, the combined crests of annual tides, daily tides, and higher frequency harmonics increase water levels up to 2.16 feet, yielding a combined total of 4.18 feet as a lower bound and a combined total of 8.09 feet as an upper bound. These findings provide a foundation for more accurate prediction of coastal flooding during severe weather events and provide an impetus for policy choices with respect to residential communities, businesses, and wildlife habitats. Further, using Kolmogorov-Zurbenko analytic methods to study coastal sites throughout the world could draw a more comprehensive picture of the impact climate change is having on coastal waters globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09419v2</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barry Loneck, Igor Zurbenko, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Integrative Learning of Quantum Dot Intensity Fluctuations under Excitation via Tailored Dynamic Mixture Modeling</title>
      <link>https://arxiv.org/abs/2501.01292</link>
      <description>arXiv:2501.01292v2 Announce Type: replace-cross 
Abstract: Semiconductor nano-crystals, known as quantum dots (QDs), have attracted significant attention for their unique fluorescence properties. Under continuous excitation, QDs emit photons with intricate intensity fluctuation: the intensity of photon emission fluctuates during the excitation, and such a fluctuation pattern can vary across different QDs even under the same experimental conditions. What adding to the complication is that the processed intensity series are non-Gaussian and truncated due to necessary thresholding and normalization. Conventional normality-based single-dot analysis fall short of addressing these complexities. In collaboration with chemists, we develop an integrative learning approach to simultaneously analyzing intensity series from multiple QDs. Motivated by the unique data structure and the hypothesized behaviors of the QDs, our approach leverages the celebrated hidden Markov model as its structural backbone to characterize individual dot intensity fluctuations, while assuming that, in each state the normalized intensity follows a 0/1 inflated Beta distribution, the state/emission distributions are shared across the QDs, and the state transition dynamics can vary among a few QD clusters. This framework allows for a precise, collective characterization of intensity fluctuation patterns and have the potential to transform current practice in chemistry. Applying our method to experimental data from 128 QDs, we reveal three shared intensity states and capture several distinct intensity transition patterns, underscoring the effectiveness of our approach in providing deeper insights into QD behaviors and their design and application potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01292v2</guid>
      <category>physics.app-ph</category>
      <category>cond-mat.mes-hall</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Yang, Hawi Nyiera, Yonglei Sun, Jing Zhao, Kun Chen</dc:creator>
    </item>
  </channel>
</rss>
