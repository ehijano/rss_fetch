<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 01:42:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sequential Design for the Efficient Estimation of Offshore Structure Failure Probability</title>
      <link>https://arxiv.org/abs/2509.18319</link>
      <description>arXiv:2509.18319v1 Announce Type: new 
Abstract: Estimation of the failure probability of offshore structures exposed to extreme ocean environments is critical to their safe design and operation. The conditional density of the environment (CDE) quantifies regions of the space of long term environment responsible for extreme structural response. Moreover, the probability of structural failure is obtained by simply integrating the CDE over the environment space. In this work, two methodologies for estimation of the CDE and failure probability are considered. The first (IS-PT) combines parallel tempering MCMC (for CDE estimation) with important sampling (for eventual estimation of failure probability). The second (AGE) combines adaptive Gaussian emulation with Bayesian quadrature. We evaluate IS-PT and two variants of the AGE procedure in application to a simple synthetic structure with multimodal CDE, and a monopile structure exhibiting non-linear resonant response. IS-PT provides reliable results for both applications for lesser compute cost than naive integration. The AGE procedures require balancing exploration and exploitation of the environment space, using a typically-unknown weight parameter, lambda. When lambda is known, perhaps from prior engineering knowledge, AGE provides a further reduction in computational cost over IS-PT. However, when unknown, IS-PT is more reliable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18319v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Speers, Jonathan Angus Tawn, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Hierarchical Semi-Markov Models with Duration-Aware Dynamics for Activity Sequences</title>
      <link>https://arxiv.org/abs/2509.18414</link>
      <description>arXiv:2509.18414v1 Announce Type: new 
Abstract: Residential electricity demand at granular scales is driven by what people do and for how long. Accurately forecasting this demand for applications like microgrid management and demand response therefore requires generative models that can produce realistic daily activity sequences, capturing both the timing and duration of human behavior. This paper develops a generative model of human activity sequences using nationally representative time-use diaries at a 10-minute resolution. We use this model to quantify which demographic factors are most critical for improving predictive performance.
  We propose a hierarchical semi-Markov framework that addresses two key modeling challenges. First, a time-inhomogeneous Markov \emph{router} learns the patterns of ``which activity comes next." Second, a semi-Markov \emph{hazard} component explicitly models activity durations, capturing ``how long" activities realistically last. To ensure statistical stability when data are sparse, the model pools information across related demographic groups and time blocks. The entire framework is trained and evaluated using survey design weights to ensure our findings are representative of the U.S. population.
  On a held-out test set, we demonstrate that explicitly modeling durations with the hazard component provides a substantial and statistically significant improvement over purely Markovian models. Furthermore, our analysis reveals a clear hierarchy of demographic factors: Sex, Day-Type, and Household Size provide the largest predictive gains, while Region and Season, though important for energy calculations, contribute little to predicting the activity sequence itself. The result is an interpretable and robust generator of synthetic activity traces, providing a high-fidelity foundation for downstream energy systems modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18414v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohit Dube, Natarajan Gautam, Amarnath Banerjee, Harsha Nagarajan</dc:creator>
    </item>
    <item>
      <title>Evaluating Bias Reduction Methods in Binary Emax Model for Reliable Dose-Response Estimation</title>
      <link>https://arxiv.org/abs/2509.18459</link>
      <description>arXiv:2509.18459v1 Announce Type: new 
Abstract: The Binary Emax model is widely employed in dose-response analysis during Phase II clinical studies to identify the optimal dose for subsequence confirmatory trials. The parameter estimation and inference heavily rely on the asymptotic properties of Maximum Likelihood (ML) estimators; however, this approach may be questionable under small or moderate sample sizes and is not robust to violation of model assumptions. To provide a reliable solution, this paper examines three bias-reduction methods: the Cox-Snell bias correction, Firth-score modification, and a maximum penalized likelihood estimator (MPLE) using Jeffreys prior. Through comprehensive simulation studies, we evaluate the performance of these methods in reducing bias and controlling variance, especially when model assumptions are violated. The results demonstrate that both Firth and MPLE methods provide robust estimates, with MPLE outperforming in terms of stability and lower variance. We further illustrate the practical application of these methods using data from the TURANDOT study, a Phase II clinical trial. Our findings suggest that MPLE with Jeffreys prior offers an effective and reliable alternative to the Firth method, particularly for dose-response relationships that deviate from monotonicity, making it valuable for robust parameter estimation in dose-ranging studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18459v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiangshan Zhang, Vivek Pradhan, Yuxi Zhao</dc:creator>
    </item>
    <item>
      <title>The information flow among Green Bonds exchange traded funds</title>
      <link>https://arxiv.org/abs/2509.19285</link>
      <description>arXiv:2509.19285v1 Announce Type: new 
Abstract: This article investigates the information flow between 13 Green Bond ETFs (Exchange Traded Funds) from three global markets: the USA, Canada,and Europe, between 2021 and 2022. We used the transfer entropy and effective transfer entropy methods to model and investigate the Green Bond price information flow between these global markets. The American market demonstrated market dominance among the other two markets (Canadian and European). The FLMB Green Bond of the American ETF presented the greatest flow of information transfer among the ETFs analyzed, being considered the dominant ETF among the three Green Bond ETF markets investigated. The HGGB ETF has emerged as a major information transmitter in Europe and in the Canadian market, but it has had a strong influence from the American ETF FLMB. In the European market, the FLRG and GRON.MI bonds played a major role in the flow of information sent to other ETFs in Europe. The KLMH.F in Europe is highlighted as the largest receiver of information. Thus, through this article, it was possible to understand the direction of the flow of information between the Green Bond ETF markets and their dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19285v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenderson Gomes Barbosa, Kerolly Kedma Felix do Nascimento, Fabio Sandro dos Santos, Tiago A. E. Ferreira</dc:creator>
    </item>
    <item>
      <title>Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters</title>
      <link>https://arxiv.org/abs/2509.18124</link>
      <description>arXiv:2509.18124v1 Announce Type: cross 
Abstract: This study explores the application of supervised machine learning algorithms to predict coffee ratings based on a combination of influential textual and numerical attributes extracted from user reviews. Through careful data preprocessing including text cleaning, feature extraction using TF-IDF, and selection with SelectKBest, the study identifies key factors contributing to coffee quality assessments. Six models (Decision Tree, KNearest Neighbors, Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained and evaluated using optimized hyperparameters. Model performance was assessed primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as Multi-layer Perceptron, consistently outperform simpler classifiers (Decision Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1 scores, G-mean and AUC. The findings highlight the essence of rigorous feature selection and hyperparameter tuning in building robust predictive systems for sensory product evaluation, offering a data driven approach to complement traditional coffee cupping by expertise of trained professionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18124v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edmund Agyemang, Lawrence Agbota, Vincent Agbenyeavu, Peggy Akabuah, Bismark Bimpong, Christopher Attafuah</dc:creator>
    </item>
    <item>
      <title>KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots</title>
      <link>https://arxiv.org/abs/2509.18141</link>
      <description>arXiv:2509.18141v1 Announce Type: cross 
Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots provides valuable insights for evidence synthesis in clinical research. However, existing approaches often rely on manual digitization, which is error-prone and lacks scalability. To address these limitations, we develop KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD directly from KM plots with high accuracy, robustness, and reproducibility. KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD without manual input or intervention. Its hybrid reasoning architecture automates the conversion of unstructured information into structured data flows and validates data extraction from complex KM plots. To improve accessibility, KM-GPT is equipped with a user-friendly web interface and an integrated AI assistant, enabling researchers to reconstruct IPD without requiring programming expertise. KM-GPT was rigorously evaluated on synthetic and real-world datasets, consistently demonstrating superior accuracy. To illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and biomarker-based subgroup analyses. By automating traditionally manual processes and providing a scalable, web-based solution, KM-GPT transforms clinical research by leveraging reconstructed IPD to enable more informed downstream analyses, supporting evidence-based decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18141v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Zhao, Haoyue Sun, Yantian Ding, Yanxun Xu</dc:creator>
    </item>
    <item>
      <title>Surrogate Modelling of Proton Dose with Monte Carlo Dropout Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2509.18155</link>
      <description>arXiv:2509.18155v1 Announce Type: cross 
Abstract: Accurate proton dose calculation using Monte Carlo (MC) is computationally demanding in workflows like robust optimisation, adaptive replanning, and probabilistic inference, which require repeated evaluations. To address this, we develop a neural surrogate that integrates Monte Carlo dropout to provide fast, differentiable dose predictions along with voxelwise predictive uncertainty. The method is validated through a series of experiments, starting with a one-dimensional analytic benchmark that establishes accuracy, convergence, and variance decomposition. Two-dimensional bone-water phantoms, generated using TOPAS Geant4, demonstrate the method's behavior under domain heterogeneity and beam uncertainty, while a three-dimensional water phantom confirms scalability for volumetric dose prediction. Across these settings, we separate epistemic (model) from parametric (input) contributions, showing that epistemic variance increases under distribution shift, while parametric variance dominates at material boundaries. The approach achieves significant speedups over MC while retaining uncertainty information, making it suitable for integration into robust planning, adaptive workflows, and uncertainty-aware optimisation in proton therapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18155v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Pim, Tristan Pryer</dc:creator>
    </item>
    <item>
      <title>On Multi-entity, Multivariate Quickest Change Point Detection</title>
      <link>https://arxiv.org/abs/2509.18310</link>
      <description>arXiv:2509.18310v1 Announce Type: cross 
Abstract: We propose a framework for online Change Point Detection (CPD) from multi-entity, multivariate time series data, motivated by applications in crowd monitoring where traditional sensing methods (e.g., video surveillance) may be infeasible. Our approach addresses the challenge of detecting system-wide behavioral shifts in complex, dynamic environments where the number and behavior of individual entities may be uncertain or evolve. We introduce the concept of Individual Deviation from Normality (IDfN), computed via a reconstruction-error-based autoencoder trained on normal behavior. We aggregate these individual deviations using mean, variance, and Kernel Density Estimates (KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or abrupt changes, we apply statistical deviation metrics and the Cumulative Sum (CUSUM) technique to these scores. Our unsupervised approach eliminates the need for labeled data or feature extraction, enabling real-time operation on streaming input. Evaluations on both synthetic datasets and crowd simulations, explicitly designed for anomaly detection in group behaviors, demonstrate that our method accurately detects significant system-level changes, offering a scalable and privacy-preserving solution for monitoring complex multi-agent systems. In addition to this methodological contribution, we introduce new, challenging multi-entity multivariate time series datasets generated from crowd simulations in Unity and coupled nonlinear oscillators. To the best of our knowledge, there is currently no publicly available dataset of this type designed explicitly to evaluate CPD in complex collective and interactive systems, highlighting an essential gap that our work addresses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18310v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bahar Kor, Bipin Gaikwad, Abani Patra, Eric L. Miller</dc:creator>
    </item>
    <item>
      <title>Latent class multivariate probit and latent trait models for evaluating test accuracy without a gold standard: A simulation study</title>
      <link>https://arxiv.org/abs/2509.18489</link>
      <description>arXiv:2509.18489v1 Announce Type: cross 
Abstract: In the context of an imperfect gold standard, latent class modelling can be used to estimate accuracy of multiple medical tests. However, the conditional independence (CI) assumption is rarely thought to be clinically valid. Two models accommodating conditional dependence are the latent class multivariate probit (LC-MVP) and latent trait models. Despite LC-MVP's greater flexibility - modelling full correlation matrices versus the latent trait's restricted structure - the latent trait has been more widely used. No simulation studies have directly compared these two models.
  We conducted a comprehensive simulation study comparing both models across five data generating mechanisms: CI, low-heterogeneity (latent trait-generated), and high-heterogeneity (LC-MVP-generated) correlation structures. We evaluated multiple priors, including novel constrained correlation priors using Pinkney's method that preserves prior interpretability. Models were fit using our BayesMVP R package, which achieves GPU-like speed-ups on these inherently serial models.
  The LC-MVP model demonstrated superior overall performance. Whilst the latent trait model performed acceptably on its own generated data, it failed for high-heterogeneity structures, sometimes performing worse than the CI model. The CI model did badly for most dependent structures. We also found ceiling effects: high sensitivities reduced the importance of correlation recovery, explaining paradoxes where models achieved good performance despite poor correlation recovery.
  Our results strongly favour LC-MVP for practical applications. The latent trait model's severe consequences under realistic correlation structures make it a more risky choice. However, LC-MVP with custom correlation constraints and priors provides a safer, more flexible framework for test accuracy evaluation without a perfect gold standard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18489v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enzo Cerullo, Sean Pinkney, Alex J. Sutton, Tim Lucas, Nicola J. Cooper, Hayley E. Jones</dc:creator>
    </item>
    <item>
      <title>Quantifying the Effect of a Parallax Correcting Algorithm for Passive Microwave Satellite Precipitation Retrievals across the Continental United States</title>
      <link>https://arxiv.org/abs/2509.18695</link>
      <description>arXiv:2509.18695v1 Announce Type: cross 
Abstract: Satellite precipitation retrieval algorithms whose measurement instruments are tilted to the zenith line are subject to a spatial mismatch between the theoretical ground coordinates and the coordinate pair corresponding to the cloud layers sending spectral signals to the satellite. This is the case of the precipitation retrievals of the GPM Passive Microwave Imagery (GMI) on board the core satellite of the Global Precipitation Mission (GPM) that uses the Goddard Profiling Algorithm (GPROF). Currently, no geometrical correction is applied to GMI retrievals of surface precipitation, creating a horizontal displacement (or parallax mismatching) between the reported surface and the corrected coordinates corresponding to the cloud structures intersecting the field of view.
  GPROF precipitation retrievals over the Continental United States are analyzed using the ground-validated Multi-Resolution Multi-Sensor (GV-MRMS) system data and the European Centre for Medium-Range Weather Forecasts Reanalysis version 5 (ERA5) temperature profiles. Results applying this parallax correction scheme show improvements in the overall retrieval accuracy of GPROF, mainly during the summer months, for every precipitation type, when the freezing level (FL) is relatively high. The development of this new parallax-correction algorithm for passive microwave radiometers will significantly improve the accuracy of remote sensing data by minimizing spatial distortions in atmospheric measurements, leading to more precise weather forecasting, climate monitoring, and environmental assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18695v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andres F. Monsalve, Hernan A. Moreno, Eric Goldenstern, Christian Kummerow</dc:creator>
    </item>
    <item>
      <title>Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market</title>
      <link>https://arxiv.org/abs/2509.18820</link>
      <description>arXiv:2509.18820v1 Announce Type: cross 
Abstract: Based on the cryptocurrency market dynamics, this study presents a general methodology for analyzing evolving correlation structures in complex systems using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By extending traditional metrics, this approach captures correlations at varying fluctuation amplitudes and time scales. The method employs $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance (Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in $q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially centralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's declining dominance and increased diversification among assets. A key finding is that medium-scale fluctuations exhibit stronger correlations than large-scale ones, with $q$MSTs based on the latter being more decentralized. Properly exploiting such facts may offer the possibility of a more flexible optimal portfolio construction. Distance metrics highlight that major disruptions amplify correlation differences, leading to fully decentralized structures during crashes. These results demonstrate $q$MSTs' effectiveness in uncovering fluctuation-dependent correlations, with potential applications beyond finance, including biology, social and other complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18820v1</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/v7cl-h7xr</arxiv:DOI>
      <dc:creator>Marcin W\k{a}torek, Marija Bezbradica, Martin Crane, Jaros{\l}aw Kwapie\'n, Stanis{\l}aw Dro\.zd\.z</dc:creator>
    </item>
    <item>
      <title>A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement</title>
      <link>https://arxiv.org/abs/2509.19088</link>
      <description>arXiv:2509.19088v1 Announce Type: cross 
Abstract: Do "digital twins" capture individual responses in surveys and experiments? We run 19 pre-registered studies on a national U.S. panel and their LLM-powered digital twins (constructed based on previously-collected extensive individual-level data) and compare twin and human answers across 164 outcomes. The correlation between twin and human answers is modest (approximately 0.2 on average) and twin responses are less variable than human responses. While constructing digital twins based on rich individual-level data improves our ability to capture heterogeneity across participants and predict relative differences between them, it does not substantially improve our ability to predict the exact answers given by specific participants or enhance predictions of population means. Twin performance varies by domain and is higher among more educated, higher-income, and ideologically moderate participants. These results suggest current digital twins can capture some degree of relative differences but are unreliable for individual-level predictions and sample mean and variance estimation, underscoring the need for careful validation before use. Our data and code are publicly available for researchers and practitioners interested in optimizing digital twin pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19088v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiany Peng, George Gui, Daniel J. Merlau, Grace Jiarui Fan, Malek Ben Sliman, Melanie Brucks, Eric J. Johnson, Vicki Morwitz, Abdullah Althenayyan, Silvia Bellezza, Dante Donati, Hortense Fong, Elizabeth Friedman, Ariana Guevara, Mohamed Hussein, Kinshuk Jerath, Bruce Kogut, Kristen Lane, Hannah Li, Patryk Perkowski, Oded Netzer, Olivier Toubia</dc:creator>
    </item>
    <item>
      <title>The DeepJoint algorithm: An innovative approach for studying the longitudinal evolution of quantitative mammographic density and its association with screen-detected breast cancer risk</title>
      <link>https://arxiv.org/abs/2403.13488</link>
      <description>arXiv:2403.13488v3 Announce Type: replace 
Abstract: Mammographic density is a dynamic risk factor for breast cancer and affects the sensitivity of mammography-based screening. While automated machine and deep learning-based methods provide more consistent and precise measurements compared to subjective BI-RADS assessments, they often fail to account for the longitudinal evolution of density. Many of these methods assess mammographic density in a cross-sectional manner, overlooking correlations in repeated measures, irregular visit intervals, missing data, and informative dropouts. Joint models, however, are well-suited for capturing the longitudinal relationship between biomarkers and survival outcomes. We present the DeepJoint algorithm, an open-source solution that integrates deep learning for quantitative mammographic density estimation with joint modeling to assess the longitudinal relationship between mammographic density and breast cancer risk. Our method efficiently analyzes processed mammograms from various manufacturers, estimating both dense area and percent density--established risk factors for breast cancer. We utilize a joint model to explore their association with breast cancer risk and provide individualized risk predictions. Bayesian inference and the Monte Carlo consensus algorithm make the approach reliable for large screening datasets. Our method allows for accurate analysis of processed mammograms from multiple manufacturers, offering a comprehensive view of breast cancer risk based on individual longitudinal density profiles. The complete pipeline is publicly available, promoting broader application and comparison with other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13488v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manel Rakez, Julien Guillaumin, Aurelien Chick, Gaelle Coureau, Foucauld Chamming's, Pierre Fillard, Brice Amadeo, Virginie Rondeau</dc:creator>
    </item>
    <item>
      <title>The ICML 2023 Ranking Experiment: Examining Author Self-Assessment in ML/AI Peer Review</title>
      <link>https://arxiv.org/abs/2408.13430</link>
      <description>arXiv:2408.13430v3 Announce Type: replace 
Abstract: We conducted an experiment during the review process of the 2023 International Conference on Machine Learning (ICML), asking authors with multiple submissions to rank their papers based on perceived quality. In total, we received 1,342 rankings, each from a different author, covering 2,592 submissions. In this paper, we present an empirical analysis of how author-provided rankings could be leveraged to improve peer review processes at machine learning conferences. We focus on the Isotonic Mechanism, which calibrates raw review scores using the author-provided rankings. Our analysis shows that these ranking-calibrated scores outperform the raw review scores in estimating the ground truth ``expected review scores'' in terms of both squared and absolute error metrics. Furthermore, we propose several cautious, low-risk applications of the Isotonic Mechanism and author-provided rankings in peer review, including supporting senior area chairs in overseeing area chairs' recommendations, assisting in the selection of paper awards, and guiding the recruitment of emergency reviewers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13430v3</guid>
      <category>stat.AP</category>
      <category>cs.DL</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Buxin Su, Jiayao Zhang, Natalie Collina, Yuling Yan, Didong Li, Kyunghyun Cho, Jianqing Fan, Aaron Roth, Weijie Su</dc:creator>
    </item>
    <item>
      <title>Fairness is in the details: Face Dataset Auditing</title>
      <link>https://arxiv.org/abs/2504.08396</link>
      <description>arXiv:2504.08396v3 Announce Type: replace 
Abstract: Auditing involves verifying the proper implementation of a given policy. As such, auditing is essential for ensuring compliance with the principles of fairness, equity, and transparency mandated by the European Union's AI Act. Moreover, biases present during the training phase of a learning system can persist in the modeling process and result in discrimination against certain subgroups of individuals when the model is deployed in production. Assessing bias in image datasets is a particularly complex task, as it first requires a feature extraction step, then to consider the extraction's quality in the statistical tests. This paper proposes a robust methodology for auditing image datasets based on so-called "sensitive" features, such as gender, age, and ethnicity. The proposed methodology consists of both a feature extraction phase and a statistical analysis phase. The first phase introduces a novel convolutional neural network (CNN) architecture specifically designed for extracting sensitive features with a limited number of manual annotations. The second phase compares the distributions of sensitive features across subgroups using a novel statistical test that accounts for the imprecision of the feature extraction model. Our pipeline constitutes a comprehensive and fully automated methodology for dataset auditing. We illustrate our approach using two manually annotated datasets. The code and datasets are available at github.com/ValentinLafargue/FairnessDetails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08396v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentin Lafargue, Emmanuelle Claeys, Jean-Michel Loubes</dc:creator>
    </item>
    <item>
      <title>Attenuation Bias with Latent Predictors</title>
      <link>https://arxiv.org/abs/2507.22218</link>
      <description>arXiv:2507.22218v2 Announce Type: replace 
Abstract: Many core concepts in political science are latent and therefore can only be measured with error. Measurement error in a predictor attenuates slope coefficient estimates in regression, biasing them toward zero. We show that widely used strategies for correcting attenuation bias -- including instrumental variables and the method of composition -- are themselves biased, sometimes even more than simple regression ignoring the measurement error altogether. We derive appropriate bias correction methods using split-sample measurement strategies. Our approach is modular and can be easily deployed with additive score, factor, or machine learning models, requiring no joint estimation while yielding consistent slopes under standard assumptions. Simulations and applications -- political knowledge, democracy indices, and text-based sentiment -- show stronger relationships after correction, sometimes by 50 percent. Open-source software implements the procedure. Results underscore that latent predictors demand tailored error correction; otherwise, conventional practice can exacerbate bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22218v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Connor T. Jerzak, Stephen A. Jessee</dc:creator>
    </item>
    <item>
      <title>On the minimum strength of (unobserved) covariates to overturn an insignificant result</title>
      <link>https://arxiv.org/abs/2408.13901</link>
      <description>arXiv:2408.13901v2 Announce Type: replace-cross 
Abstract: We study conditions under which the addition of variables to a regression equation can turn a previously statistically insignificant result into a significant one. Specifically, we characterize the minimum strength of association required for these variables--both with the dependent and independent variables, or with the dependent variable alone--to elevate the observed t-statistic above a specified significance threshold. Interestingly, we show that it is considerably difficult to overturn a statistically insignificant result solely by reducing the standard error. Instead, included variables must also alter the point estimate to achieve such reversals in practice. Our results can be used to conduct sensitivity analyses against unobserved variables and to bound the maximum t-value one can obtain given different subsets of observed covariates, and may also offer algebraic explanations for patterns of reversals seen in empirical research, such as those documented by Lenz and Sahn (2021).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13901v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danielle Tsao, Ronan Perry, Carlos Cinelli</dc:creator>
    </item>
    <item>
      <title>"6 choose 4": A framework to understand and facilitate discussion of strategies for overall survival safety monitoring</title>
      <link>https://arxiv.org/abs/2410.04020</link>
      <description>arXiv:2410.04020v2 Announce Type: replace-cross 
Abstract: Advances in anticancer therapies have significantly contributed to declining death rates in certain disease and clinical settings. However, they have also made it difficult to power a clinical trial in these settings with overall survival (OS) as the primary efficacy endpoint. Therefore, two approaches have been recently proposed for the pre-specified analysis of OS as a safety endpoint (Fleming et al., 2024; Rodriguez et al., 2024). In this paper, we provide a simple, unifying framework that includes the aforementioned approaches (and a couple others) as special cases. By highlighting each approach's focus, priority, tolerance for risk, and strengths or challenges for practical implementation, this framework can help to facilitate discussions between stakeholders on "fit-for-purpose OS data collection and assessment of harm" (American Association for Cancer Research, 2024). We apply this framework to a real clinical trial in large B-cell lymphoma to illustrate its application and value. Several recommendations and open questions are also raised.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04020v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Mark Yan, Jue Wang</dc:creator>
    </item>
    <item>
      <title>EarthquakeNPP: A Benchmark for Earthquake Forecasting with Neural Point Processes</title>
      <link>https://arxiv.org/abs/2410.08226</link>
      <description>arXiv:2410.08226v2 Announce Type: replace-cross 
Abstract: For decades, classical point process models, such as the epidemic-type aftershock sequence (ETAS) model, have been widely used for forecasting the event times and locations of earthquakes. Recent advances have led to Neural Point Processes (NPPs), which promise greater flexibility and improvements over such classical models. However, the currently-used benchmark for NPPs does not represent an up-to-date challenge in the seismological community, since it contains data leakage and omits the largest earthquake sequence from the region. Additionally, initial earthquake forecasting benchmarks fail to compare NPPs with state-of-the-art forecasting models commonly used in seismology. To address these gaps, we introduce EarthquakeNPP: a collection of benchmark datasets to facilitate testing of NPPs on earthquake data, accompanied by an implementation of the state-of-the-art forecasting model: ETAS. The datasets cover a range of small to large target regions within California, dating from 1971 to 2021, and include different methodologies for dataset generation. Benchmarking experiments, using both log-likelihood and generative evaluation metrics widely recognised in seismology, show that none of the five NPPs tested outperform ETAS. These findings suggest that current NPP implementations are not yet suitable for practical earthquake forecasting. Nonetheless, EarthquakeNPP provides a platform to foster future collaboration between the seismology and machine learning communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08226v2</guid>
      <category>physics.geo-ph</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Stockman, Daniel Lawson, Maximilian Werner</dc:creator>
    </item>
    <item>
      <title>Language Models as Causal Effect Generators</title>
      <link>https://arxiv.org/abs/2411.08019</link>
      <description>arXiv:2411.08019v2 Announce Type: replace-cross 
Abstract: In this work, we present sequence-driven structural causal models (SD-SCMs), a framework for specifying causal models with user-defined structure and language-model-defined mechanisms. We characterize how an SD-SCM enables sampling from observational, interventional, and counterfactual distributions according to the desired causal structure. We then leverage this procedure to propose a new type of benchmark for causal inference methods, generating individual-level counterfactual data to test treatment effect estimation. We create an example benchmark consisting of thousands of datasets, and test a suite of popular estimation methods for average, conditional average, and individual treatment effect estimation. We find under this benchmark that (1) causal methods outperform non-causal methods and that (2) even state-of-the-art methods struggle with individualized effect estimation, suggesting this benchmark captures some inherent difficulties in causal estimation. Apart from generating data, this same technique can underpin the auditing of language models for (un)desirable causal effects, such as misinformation or discrimination. We believe SD-SCMs can serve as a useful tool in any application that would benefit from sequential data with controllable causal structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08019v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucius E. J. Bynum, Kyunghyun Cho</dc:creator>
    </item>
    <item>
      <title>Saturation-Aware Snapshot Compressive Imaging: Theory and Algorithm</title>
      <link>https://arxiv.org/abs/2501.11869</link>
      <description>arXiv:2501.11869v2 Announce Type: replace-cross 
Abstract: Snapshot Compressive Imaging (SCI) uses coded masks to compress a 3D data cube into a single 2D snapshot. In practice, multiplexing can push intensities beyond the sensor's dynamic range, producing saturation that violates the linear SCI model and degrades reconstruction. This paper provides the first theoretical characterization of SCI recovery under saturation. We model clipping as an element-wise nonlinearity and derive a finite-sample recovery bound for compression-based SCI that links reconstruction error to mask density and the extent of saturation. The analysis yields a clear design rule: optimal Bernoulli masks use densities below one-half, decreasing further as saturation strengthens. Guided by this principle, we optimize mask patterns and introduce a novel reconstruction framework, Saturation-Aware PnP Net (SAPnet), which explicitly enforces consistency with saturated measurements. Experiments on standard video-SCI benchmarks confirm our theory and demonstrate that SAPnet significantly outperforms existing PnP-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11869v2</guid>
      <category>eess.IV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengyu Zhao, Shirin Jalali</dc:creator>
    </item>
    <item>
      <title>Bayesian Multivariate Density-Density Regression</title>
      <link>https://arxiv.org/abs/2504.12617</link>
      <description>arXiv:2504.12617v2 Announce Type: replace-cross 
Abstract: We introduce a novel and scalable Bayesian framework for multivariate-density-density regression (DDR), designed to model relationships between multivariate distributions. Our approach addresses the critical issue of distributions residing in spaces of differing dimensions. We utilize a generalized Bayes framework, circumventing the need for a fully specified likelihood by employing the sliced Wasserstein distance to measure the discrepancy between fitted and observed distributions. This choice not only handles high-dimensional data and varying sample sizes efficiently but also facilitates a Metropolis-adjusted Langevin algorithm (MALA) for posterior inference. Furthermore, we establish the posterior consistency of our generalized Bayesian approach, ensuring that the posterior distribution concentrates around the true parameters as the sample size increases. Through simulations and application to a population-scale single-cell dataset, we show that Bayesian DDR provides robust fits, superior predictive performance compared to traditional methods, and valuable insights into complex biological interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12617v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Yang Ni, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>Probabilistic patient risk profiling with pair-copula constructions</title>
      <link>https://arxiv.org/abs/2506.13731</link>
      <description>arXiv:2506.13731v2 Announce Type: replace-cross 
Abstract: We propose vine copula-based classifiers for probabilistic risk prediction in perioperative settings. We obtain full joint probability models for mixed continuous-ordinal variables by fitting a separate vine copula to each outcome class, capturing nonlinear and tail-asymmetric dependence. In a cohort of 767 elective bowel surgeries (81 serious vs. 686 non-serious complications), posterior probabilities from the fitted vine classification models are used to allocate patients into low-, moderate-, and high-risk groups. Compared to weighted logistic regression and random forests with stratified sampling, the vine copula-based classifiers achieve up to 10% lower class-specific Brier scores and negative log-likelihoods on the out-of-sample. The vine copula-based classifier identifies a large cohort of true low-risk patients potentially eligible for early discharge. Scenario analyses based on the fitted vine copula models provide interpretable risk profiles, including nonlinear relationships between body mass index, surgery duration, and blood loss, which might remain undetected under linear models. These results demonstrate that vine copula-based classifiers offer a reliable and interpretable framework for individualized, probability-based patient risk profiling. As such, they represent a new, promising tool for data-driven decision-making in perioperative care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13731v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Ozge \c{S}ahin</dc:creator>
    </item>
    <item>
      <title>Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes</title>
      <link>https://arxiv.org/abs/2508.19356</link>
      <description>arXiv:2508.19356v3 Announce Type: replace-cross 
Abstract: Graphs are central to the chemical sciences, providing a natural language to describe molecules, proteins, reactions, and industrial processes. They capture interactions and structures that underpin materials, biology, and medicine. This primer, Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes, introduces graphs as mathematical objects in chemistry and shows how learning algorithms (particularly graph neural networks) can operate on them. We outline the foundations of graph design, key prediction tasks, representative examples across chemical sciences, and the role of machine learning in graph-based modeling. Together, these concepts prepare readers to apply graph methods to the next generation of chemical discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19356v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1021/acsinfocus.7e9017</arxiv:DOI>
      <dc:creator>Jos\'e Manuel Barraza-Chavez, Rana A. Barghout, Ricardo Almada-Monter, Adrian Jinich, Radhakrishnan Mahadevan, Benjamin Sanchez-Lengeling</dc:creator>
    </item>
  </channel>
</rss>
