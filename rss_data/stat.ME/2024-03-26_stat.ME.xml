<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Mar 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 27 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sparse inference in Poisson Log-Normal model by approximating the L0-norm</title>
      <link>https://arxiv.org/abs/2403.17087</link>
      <description>arXiv:2403.17087v1 Announce Type: new 
Abstract: Variable selection methods are required in practical statistical modeling, to identify and include only the most relevant predictors, and then improving model interpretability. Such variable selection methods are typically employed in regression models, for instance in this article for the Poisson Log Normal model (PLN, Chiquet et al., 2021). This model aim to explain multivariate count data using dependent variables, and its utility was demonstrating in scientific fields such as ecology and agronomy. In the case of the PLN model, most recent papers focus on sparse networks inference through combination of the likelihood with a L1 -penalty on the precision matrix. In this paper, we propose to rely on a recent penalization method (SIC, O'Neill and Burke, 2023), which consists in smoothly approximating the L0-penalty, and that avoids the calibration of a tuning parameter with a cross-validation procedure. Moreover, this work focuses on the coefficient matrix of the PLN model and establishes an inference procedure ensuring effective variable selection performance, so that the resulting fitted model explaining multivariate count data using only relevant explanatory variables. Our proposal involves implementing a procedure that integrates the SIC penalization algorithm (epsilon-telescoping) and the PLN model fitting algorithm (a variational EM algorithm). To support our proposal, we provide theoretical results and insights about the penalization method, and we perform simulation studies to assess the method, which is also applied on real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17087v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Togo Jean Yves Kioye, Paul-Marie Grollemund, Jocelyn Chauvet, Pierre Druilhet, Erwan Saint-Loubert-Bie, Christophe Chassard</dc:creator>
    </item>
    <item>
      <title>Covariate-adjusted Group Sequential Comparisons of Survival Probabilities</title>
      <link>https://arxiv.org/abs/2403.17117</link>
      <description>arXiv:2403.17117v1 Announce Type: new 
Abstract: In confirmatory clinical trials, survival outcomes are frequently studied and interim analyses for efficacy and/or futility are often desirable. Methods such as the log rank test and Cox regression model are commonly used to compare treatments in this setting. They rely on a proportional hazards (PH) assumption and are subject to type I error rate inflation and loss of power when PH are violated. Such violations may be expected a priori, particularly when the mechanisms of treatments differ such as immunotherapy vs. chemotherapy for treating cancer. We develop group sequential tests for comparing survival curves with covariate adjustment that allow for interim analyses in the presence of non-PH and offer easily interpreted, clinically meaningful summary measures of the treatment effect. The joint distribution of repeatedly computed test statistics converges to the canonical joint distribution with a Markov structure. The asymptotic distribution of the test statistics allows marginal comparisons of survival probabilities at multiple fixed time points and facilitates both critical value specification to maintain type I error control and sample size/power determination. Simulations demonstrate that the achieved type I error rate and power of the proposed tests meet targeted levels and are robust to the PH assumption and covariate influence. The proposed tests are illustrated using a clinical trial dataset from the Blood and Marrow Transplant Clinical Trials Network 1101 trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17117v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Zhang, Brent Logan, Michael Martens</dc:creator>
    </item>
    <item>
      <title>High-dimensional Factor Analysis for Network-linked Data</title>
      <link>https://arxiv.org/abs/2403.17121</link>
      <description>arXiv:2403.17121v1 Announce Type: new 
Abstract: Factor analysis is a widely used statistical tool in many scientific disciplines, such as psychology, economics, and sociology. As observations linked by networks become increasingly common, incorporating network structures into factor analysis remains an open problem. In this paper, we focus on high-dimensional factor analysis involving network-connected observations, and propose a generalized factor model with latent factors that account for both the network structure and the dependence structure among high-dimensional variables. These latent factors can be shared by the high-dimensional variables and the network, or exclusively applied to either of them. We develop a computationally efficient estimation procedure and establish asymptotic inferential theories. Notably, we show that by borrowing information from the network, the proposed estimator of the factor loading matrix achieves optimal asymptotic variance under much milder identifiability constraints than existing literature. Furthermore, we develop a hypothesis testing procedure to tackle the challenge of discerning the shared and individual latent factors' structure. The finite sample performance of the proposed method is demonstrated through simulation studies and a real-world dataset involving a statistician co-authorship network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17121v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinming Li, Gongjun Xu, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Mean-Variance Spanning Tests</title>
      <link>https://arxiv.org/abs/2403.17127</link>
      <description>arXiv:2403.17127v1 Announce Type: new 
Abstract: We introduce a new framework for the mean-variance spanning (MVS) hypothesis testing. The procedure can be applied to any test-asset dimension and only requires stationary asset returns and the number of benchmark assets to be smaller than the number of time periods. It involves individually testing moment conditions using a robust Student-t statistic based on the batch-mean method and combining the p-values using the Cauchy combination test. Simulations demonstrate the superior performance of the test compared to state-of-the-art approaches. For the empirical application, we look at the problem of domestic versus international diversification in equities. We find that the advantages of diversification are influenced by economic conditions and exhibit cross-country variation. We also highlight that the rejection of the MVS hypothesis originates from the potential to reduce variance within the domestic global minimum-variance portfolio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17127v1</guid>
      <category>stat.ME</category>
      <category>q-fin.GN</category>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.2139/ssrn.4703023</arxiv:DOI>
      <dc:creator>David Ardia, S\'ebastien Laurent, Rosnel Sessinou</dc:creator>
    </item>
    <item>
      <title>A Personalized Predictive Model that Jointly Optimizes Discrimination and Calibration</title>
      <link>https://arxiv.org/abs/2403.17132</link>
      <description>arXiv:2403.17132v1 Announce Type: new 
Abstract: Precision medicine is accelerating rapidly in the field of health research. This includes fitting predictive models for individual patients based on patient similarity in an attempt to improve model performance. We propose an algorithm which fits a personalized predictive model (PPM) using an optimal size of a similar subpopulation that jointly optimizes model discrimination and calibration, as it is criticized that calibration is not assessed nearly as often as discrimination despite poorly calibrated models being potentially misleading. We define a mixture loss function that considers model discrimination and calibration, and allows for flexibility in emphasizing one performance measure over another. We empirically show that the relationship between the size of subpopulation and calibration is quadratic, which motivates the development of our jointly optimized model. We also investigate the effect of within-population patient weighting on performance and conclude that the size of subpopulation has a larger effect on the predictive performance of the PPM compared to the choice of weight function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17132v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiana Krikella, Joel A. Dubin</dc:creator>
    </item>
    <item>
      <title>Statistical Inference on Hierarchical Simultaneous Autoregressive Models with Missing Data</title>
      <link>https://arxiv.org/abs/2403.17257</link>
      <description>arXiv:2403.17257v1 Announce Type: new 
Abstract: Efficient estimation methods for simultaneous autoregressive (SAR) models with missing data in the response variable have been well-developed in the literature. It is common practice to introduce a measurement error into SAR models. The measurement error serves to distinguish the noise component from the spatial process. However, the previous literature has not considered adding a measurement error to the SAR models with missing data. The maximum likelihood estimation for such models with large datasets is challenging and computationally expensive. This paper proposes two efficient likelihood-based estimation methods: the marginal maximum likelihood (ML) and expectation-maximisation (EM) algorithms for estimating SAR models with both measurement errors and missing data in the response variable. The spatial error model (SEM) and the spatial autoregressive model (SAM), two popular SAR model types, are considered. The missing data mechanism is assumed to follow missing at random (MAR). While naive calculation approaches lead to computational complexities of $O(n^3)$, where n is the total number of observations, our computational approaches for both the marginal ML and EM algorithms are designed to reduce the computational complexity. The performance of the proposed methods is investigated empirically using simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17257v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjana Wijayawardhana, Thomas Suesse, David Gunawan</dc:creator>
    </item>
    <item>
      <title>Statistical analysis and method to propagate the impact of measurement uncertainty on dynamic mode decomposition</title>
      <link>https://arxiv.org/abs/2403.17318</link>
      <description>arXiv:2403.17318v1 Announce Type: new 
Abstract: We apply random matrix theory to study the impact of measurement uncertainty on dynamic mode decomposition. Specifically, when the measurements follow a normal probability density function, we show how the moments of that density propagate through the dynamic mode decomposition. While we focus on the first and second moments, the analytical expressions we derive are general and can be extended to higher-order moments. Further, the proposed numerical method to propagate uncertainty is agnostic of specific dynamic mode decomposition formulations. Of particular relevance, the estimated second moments provide confidence bounds that may be used as a metric of trustworthiness, that is, how much one can rely on a finite-dimensional linear operator to represent an underlying dynamical system. We perform numerical experiments on two canonical systems and verify the estimated confidence levels by comparing the moments to those obtained from Monte Carlo simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17318v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>P. Algikar, P. Sharma, M. Netto, L. Mili</dc:creator>
    </item>
    <item>
      <title>A Bayesian shrinkage estimator for transfer learning</title>
      <link>https://arxiv.org/abs/2403.17321</link>
      <description>arXiv:2403.17321v1 Announce Type: new 
Abstract: Transfer learning (TL) has emerged as a powerful tool to supplement data collected for a target task with data collected for a related source task. The Bayesian framework is natural for TL because information from the source data can be incorporated in the prior distribution for the target data analysis. In this paper, we propose and study Bayesian TL methods for the normal-means problem and multiple linear regression. We propose two classes of prior distributions. The first class assumes the difference in the parameters for the source and target tasks is sparse, i.e., many parameters are shared across tasks. The second assumes that none of the parameters are shared across tasks, but the differences are bounded in $\ell_2$-norm. For the sparse case, we propose a Bayes shrinkage estimator with theoretical guarantees under mild assumptions. The proposed methodology is tested on synthetic data and outperforms state-of-the-art TL methods. We then use this method to fine-tune the last layer of a neural network model to predict the molecular gap property in a material science application. We report improved performance compared to classical fine tuning and methods using only the target data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17321v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed A. Abba, Jonathan P. Williams, Brian J. Reich</dc:creator>
    </item>
    <item>
      <title>A Type of Nonlinear Fr\'echet Regressions</title>
      <link>https://arxiv.org/abs/2403.17481</link>
      <description>arXiv:2403.17481v1 Announce Type: new 
Abstract: The existing Fr\'echet regression is actually defined within a linear framework, since the weight function in the Fr\'echet objective function is linearly defined, and the resulting Fr\'echet regression function is identified to be a linear model when the random object belongs to a Hilbert space. Even for nonparametric and semiparametric Fr\'echet regressions, which are usually nonlinear, the existing methods handle them by local linear (or local polynomial) technique, and the resulting Fr\'echet regressions are (locally) linear as well. We in this paper introduce a type of nonlinear Fr\'echet regressions. Such a framework can be utilized to fit the essentially nonlinear models in a general metric space and uniquely identify the nonlinear structure in a Hilbert space. Particularly, its generalized linear form can return to the standard linear Fr\'echet regression through a special choice of the weight function. Moreover, the generalized linear form possesses methodological and computational simplicity because the Euclidean variable and the metric space element are completely separable. The favorable theoretical properties (e.g. the estimation consistency and presentation theorem) of the nonlinear Fr\'echet regressions are established systemically. The comprehensive simulation studies and a human mortality data analysis demonstrate that the new strategy is significantly better than the competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17481v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lu Lin, Ze Chen</dc:creator>
    </item>
    <item>
      <title>Adaptive Bayesian Structure Learning of DAGs With Non-conjugate Prior</title>
      <link>https://arxiv.org/abs/2403.17489</link>
      <description>arXiv:2403.17489v1 Announce Type: new 
Abstract: Directed Acyclic Graphs (DAGs) are solid structures used to describe and infer the dependencies among variables in multivariate scenarios. Having a thorough comprehension of the accurate DAG-generating model is crucial for causal discovery and estimation. Our work suggests utilizing a non-conjugate prior for Gaussian DAG structure learning to enhance the posterior probability. We employ the idea of using the Bessel function to address the computational burden, providing faster MCMC computation compared to the use of conjugate priors. In addition, our proposal exhibits a greater rate of adaptation when compared to the conjugate prior, specifically for the inclusion of nodes in the DAG-generating model. Simulation studies demonstrate the superior accuracy of DAG learning, and we obtain the same maximum a posteriori and median probability model estimate for the AML data, using the non-conjugate prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17489v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. Nazari, M. Arashi, A. Sadeghkhani</dc:creator>
    </item>
    <item>
      <title>Measuring Dependence between Events</title>
      <link>https://arxiv.org/abs/2403.17580</link>
      <description>arXiv:2403.17580v1 Announce Type: new 
Abstract: Measuring dependence between two events, or equivalently between two binary random variables, amounts to expressing the dependence structure inherent in a $2\times 2$ contingency table in a real number between $-1$ and $1$. Countless such dependence measures exist, but there is little theoretical guidance on how they compare and on their advantages and shortcomings. Thus, practitioners might be overwhelmed by the problem of choosing a suitable measure. We provide a set of natural desirable properties that a proper dependence measure should fulfill. We show that Yule's Q and the little-known Cole coefficient are proper, while the most widely-used measures, the phi coefficient and all contingency coefficients, are improper. They have a severe attainability problem, that is, even under perfect dependence they can be very far away from $-1$ and $1$, and often differ substantially from the proper measures in that they understate strength of dependence. The structural reason is that these are measures for equality of events rather than of dependence. We derive the (in some instances non-standard) limiting distributions of the measures and illustrate how asymptotically valid confidence intervals can be constructed. In a case study on drug consumption we demonstrate how misleading conclusions may arise from the use of improper dependence measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17580v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc-Oliver Pohle, Timo Dimitriadis, Jan-Lukas Wermuth</dc:creator>
    </item>
    <item>
      <title>A location Invariant Statistic-Based Consistent Estimation Method for Three-Parameter Generalized Exponential Distribution</title>
      <link>https://arxiv.org/abs/2403.17609</link>
      <description>arXiv:2403.17609v1 Announce Type: new 
Abstract: In numerous instances, the generalized exponential distribution can be used as an alternative to the gamma distribution or the Weibull distribution when analyzing lifetime or skewed data. This article offers a consistent method for estimating the parameters of a three-parameter generalized exponential distribution that sidesteps the issue of an unbounded likelihood function. The method is hinged on a maximum likelihood estimation of shape and scale parameters that uses a location-invariant statistic. Important estimator properties, such as uniqueness and consistency, are demonstrated. In addition, quantile estimates for the lifetime distribution are provided. We present a Monte Carlo simulation study along with comparisons to a number of well-known estimation techniques in terms of bias and root mean square error. For illustrative purposes, a real-world lifetime data set is analyzed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17609v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kiran Prajapat, Sharmishtha Mitra, Debasis Kundu</dc:creator>
    </item>
    <item>
      <title>A family of Chatterjee's correlation coefficients and their properties</title>
      <link>https://arxiv.org/abs/2403.17670</link>
      <description>arXiv:2403.17670v1 Announce Type: new 
Abstract: Quantifying the strength of functional dependence between random scalars $X$ and $Y$ is an important statistical problem. While many existing correlation coefficients excel in identifying linear or monotone functional dependence, they fall short in capturing general non-monotone functional relationships. In response, we propose a family of correlation coefficients $\xi^{(h,F)}_n$, characterized by a continuous bivariate function $h$ and a cdf function $F$. By offering a range of selections for $h$ and $F$, $\xi^{(h,F)}_n$ encompasses a diverse class of novel correlation coefficients, while also incorporates the Chatterjee's correlation coefficient (Chatterjee, 2021) as a special case. We prove that $\xi^{(h,F)}_n$ converges almost surely to a deterministic limit $\xi^{(h,F)}$ as sample size $n$ approaches infinity. In addition, under appropriate conditions imposed on $h$ and $F$, the limit $\xi^{(h,F)}$ satisfies the three appealing properties: (P1). it belongs to the range of $[0,1]$; (P2). it equals 1 if and only if $Y$ is a measurable function of $X$; and (P3). it equals 0 if and only if $Y$ is independent of $X$. As amplified by our numerical experiments, our proposals provide practitioners with a variety of options to choose the most suitable correlation coefficient tailored to their specific practical needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17670v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhong Gao, Qizhai Li</dc:creator>
    </item>
    <item>
      <title>On the properties of distance covariance for categorical data: Robustness, sure screening, and approximate null distributions</title>
      <link>https://arxiv.org/abs/2403.17882</link>
      <description>arXiv:2403.17882v1 Announce Type: new 
Abstract: Pearson's Chi-squared test, though widely used for detecting association between categorical variables, exhibits low statistical power in large sparse contingency tables. To address this limitation, two novel permutation tests have been recently developed: the distance covariance permutation test and the U-statistic permutation test. Both leverage the distance covariance functional but employ different estimators. In this work, we explore key statistical properties of the distance covariance for categorical variables. Firstly, we show that unlike Chi-squared, the distance covariance functional is B-robust for any number of categories (fixed or diverging). Second, we establish the strong consistency of distance covariance screening under mild conditions, and simulations confirm its advantage over Chi-squared screening, especially for large sparse tables. Finally, we derive an approximate null distribution for a bias-corrected distance correlation estimate, demonstrating its effectiveness through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17882v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyang Zhang</dc:creator>
    </item>
    <item>
      <title>Are Made and Missed Different? An analysis of Field Goal Attempts of Professional Basketball Players via Depth Based Testing Procedure</title>
      <link>https://arxiv.org/abs/2403.17221</link>
      <description>arXiv:2403.17221v1 Announce Type: cross 
Abstract: In this paper, we develop a novel depth-based testing procedure on spatial point processes to examine the difference in made and missed field goal attempts for NBA players. Specifically, our testing procedure can statistically detect the differences between made and missed field goal attempts for NBA players. We first obtain the depths of two processes under the polar coordinate system. A two-dimensional Kolmogorov-Smirnov test is then performed to test the difference between the depths of the two processes. Throughout extensive simulation studies, we show our testing procedure with good frequentist properties under both null hypothesis and alternative hypothesis. A comparison against the competing methods shows that our proposed procedure has better testing reliability and testing power. Application to the shot chart data of 191 NBA players in the 2017-2018 regular season offers interesting insights about these players' made and missed shot patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17221v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kai Qi, Guanyu Hu, Wei Wu</dc:creator>
    </item>
    <item>
      <title>Exploring the Boundaries of Ambient Awareness in Twitter</title>
      <link>https://arxiv.org/abs/2403.17776</link>
      <description>arXiv:2403.17776v1 Announce Type: cross 
Abstract: Ambient awareness refers to the ability of social media users to obtain knowledge about who knows what (i.e., users' expertise) in their network, by simply being exposed to other users' content (e.g, tweets on Twitter). Previous work, based on user surveys, reveals that individuals self-report ambient awareness only for parts of their networks. However, it is unclear whether it is their limited cognitive capacity or the limited exposure to diagnostic tweets (i.e., online content) that prevents people from developing ambient awareness for their complete network. In this work, we focus on in-wall ambient awareness (IWAA) in Twitter and conduct a two-step data-driven analysis, that allows us to explore to which extent IWAA is likely, or even possible. First, we rely on reactions (e.g., likes), as strong evidence of users being aware of experts in Twitter. Unfortunately, such strong evidence can be only measured for active users, which represent the minority in the network. Thus to study the boundaries of IWAA to a larger extent, in the second part of our analysis, we instead focus on the passive exposure to content generated by other users -- which we refer to as in-wall visibility. This analysis shows that (in line with \citet{levordashka2016ambient}) only for a subset of users IWAA is plausible, while for the majority it is unlikely, if even possible, to develop IWAA. We hope that our methodology paves the way for the emergence of data-driven approaches for the study of ambient awareness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17776v1</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Sanchez-Martin, Sonja Utz, Isabel Valera</dc:creator>
    </item>
    <item>
      <title>Bayesian multivariate logistic regression for superiority and inferiority decision-making under observable treatment heterogeneity</title>
      <link>https://arxiv.org/abs/2206.04133</link>
      <description>arXiv:2206.04133v4 Announce Type: replace 
Abstract: The effects of treatments may differ between persons with different characteristics. Addressing such treatment heterogeneity is crucial to investigate whether patients with specific characteristics are likely to benefit from a new treatment. The current paper presents a novel Bayesian method for superiority decision-making in the context of randomized controlled trials with multivariate binary responses and heterogeneous treatment effects. The framework is based on three elements: a) Bayesian multivariate logistic regression analysis with a P\'olya-Gamma expansion; b) a transformation procedure to transfer obtained regression coefficients to a more intuitive multivariate probability scale (i.e., success probabilities and the differences between them); and c) a compatible decision procedure for treatment comparison with prespecified decision error rates. Procedures for a priori sample size estimation under a non-informative prior distribution are included. A numerical evaluation demonstrated that decisions based on a priori sample size estimation resulted in anticipated error rates among the trial population as well as subpopulations. Further, average and conditional treatment effect parameters could be estimated unbiasedly when the sample was large enough. Illustration with the International Stroke Trial dataset revealed a trend towards heterogeneous effects among stroke patients: Something that would have remained undetected when analyses were limited to average treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.04133v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xynthia Kavelaars, Joris Mulder, Maurits Kaptein</dc:creator>
    </item>
    <item>
      <title>A Practical Introduction to Regression Discontinuity Designs: Extensions</title>
      <link>https://arxiv.org/abs/2301.08958</link>
      <description>arXiv:2301.08958v2 Announce Type: replace 
Abstract: This monograph, together with its accompanying first part Cattaneo, Idrobo and Titiunik (2020), collects and expands the instructional materials we prepared for more than $50$ short courses and workshops on Regression Discontinuity (RD) methodology that we taught between 2014 and 2023. In this second monograph, we discuss several topics in RD methodology that build on and extend the analysis of RD designs introduced in Cattaneo, Idrobo and Titiunik (2020). Our first goal is to present an alternative RD conceptual framework based on local randomization ideas. This methodological approach can be useful in RD designs with discretely-valued scores, and can also be used more broadly as a complement to the continuity-based approach in other settings. Then, employing both continuity-based and local randomization approaches, we extend the canonical Sharp RD design in multiple directions: fuzzy RD designs, RD designs with discrete scores, and multi-dimensional RD designs. The goal of our two-part monograph is purposely practical and hence we focus on the empirical analysis of RD designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08958v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1017/9781009441896</arxiv:DOI>
      <dc:creator>Matias D. Cattaneo, Nicolas Idrobo, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>A distribution-free mixed-integer optimization approach to hierarchical modelling of clustered and longitudinal data</title>
      <link>https://arxiv.org/abs/2302.03157</link>
      <description>arXiv:2302.03157v2 Announce Type: replace 
Abstract: Recent advancements in Mixed Integer Optimization (MIO) algorithms, paired with hardware enhancements, have led to significant speedups in resolving MIO problems. These strategies have been utilized for optimal subset selection, specifically for choosing $k$ features out of $p$ in linear regression given $n$ observations. In this paper, we broaden this method to facilitate cluster-aware regression, where selection aims to choose $\lambda$ out of $K$ clusters in a linear mixed effects (LMM) model with $n_k$ observations for each cluster. Through comprehensive testing on a multitude of synthetic and real datasets, we exhibit that our method efficiently solves problems within minutes. Through numerical experiments, we also show that the MIO approach outperforms both Gaussian- and Laplace-distributed LMMs in terms of generating sparse solutions with high predictive power. Traditional LMMs typically assume that clustering effects are independent of individual features. However, we introduce an innovative algorithm that evaluates cluster effects for new data points, thereby increasing the robustness and precision of this model. The inferential and predictive efficacy of this approach is further illustrated through its application in student scoring and protein expression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03157v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Madhav Sankaranarayanan, Intekhab Hossain, Tom Chen</dc:creator>
    </item>
    <item>
      <title>Estimation of Long-Range Dependent Models with Missing Data: to Impute or not to Impute?</title>
      <link>https://arxiv.org/abs/2303.04754</link>
      <description>arXiv:2303.04754v2 Announce Type: replace 
Abstract: Among the most important models for long-range dependent time series is the class of ARFIMA$(p,d,q)$ (Autoregressive Fractionally Integrated Moving Average) models. Estimating the long-range dependence parameter $d$ in ARFIMA models is a well-studied problem, but the literature regarding the estimation of $d$ in the presence of missing data is very sparse. There are two basic approaches to dealing with the problem: missing data can be imputed using some plausible method, and then the estimation can proceed as if no data were missing, or we can use a specially tailored methodology to estimate $d$ in the presence of missing data. In this work, we review some of the methods available for both approaches and compare them through a Monte Carlo simulation study. We present a comparison among 35 different setups to estimate $d$, under tenths of different scenarios, considering percentages of missing data ranging from as few as 10\% up to 70\% and several levels of dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04754v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guilherme Pumi, Gladys Choque Ulloa, Taiane Schaedler Prass</dc:creator>
    </item>
    <item>
      <title>Recursive Neyman Algorithm for Optimum Sample Allocation under Box Constraints on Sample Sizes in Strata</title>
      <link>https://arxiv.org/abs/2304.07034</link>
      <description>arXiv:2304.07034v4 Announce Type: replace 
Abstract: The optimum sample allocation in stratified sampling is one of the basic issues of survey methodology. It is a procedure of dividing the overall sample size into strata sample sizes in such a way that for given sampling designs in strata the variance of the stratified $\pi$ estimator of the population total (or mean) for a given study variable assumes its minimum. In this work, we consider the optimum allocation of a sample, under lower and upper bounds imposed jointly on sample sizes in strata. We are concerned with the variance function of some generic form that, in particular, covers the case of the simple random sampling without replacement in strata. The goal of this paper is twofold. First, we establish (using the Karush-Kuhn-Tucker conditions) a generic form of the optimal solution, the so-called optimality conditions. Second, based on the established optimality conditions, we derive an efficient recursive algorithm, named RNABOX, which solves the allocation problem under study. The RNABOX can be viewed as a generalization of the classical recursive Neyman allocation algorithm, a popular tool for optimum allocation when only upper bounds are imposed on sample strata-sizes. We implement RNABOX in R as a part of our package stratallo which is available from the Comprehensive R Archive Network (CRAN) repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07034v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacek Weso{\l}owski, Robert Wieczorkowski, Wojciech W\'ojciak</dc:creator>
    </item>
    <item>
      <title>Identification and multiply robust estimation in causal mediation analysis across principal strata</title>
      <link>https://arxiv.org/abs/2304.10025</link>
      <description>arXiv:2304.10025v3 Announce Type: replace 
Abstract: We consider assessing causal mediation in the presence of a post-treatment event (examples include noncompliance, a clinical event, or a terminal event). We identify natural mediation effects for the entire study population and for each principal stratum characterized by the joint potential values of the post-treatment event. We derive efficient influence functions for each mediation estimand, which motivate a set of multiply robust estimators for inference. The multiply robust estimators are consistent under four types of misspecifications and are efficient when all nuisance models are correctly specified. We illustrate our methods via simulations and two real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10025v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Cheng, Fan Li</dc:creator>
    </item>
    <item>
      <title>A General Form of Covariate Adjustment in Randomized Clinical Trials</title>
      <link>https://arxiv.org/abs/2306.10213</link>
      <description>arXiv:2306.10213v2 Announce Type: replace 
Abstract: In randomized clinical trials, adjusting for baseline covariates can improve credibility and efficiency for demonstrating and quantifying treatment effects. This article studies the augmented inverse propensity weighted (AIPW) estimator, which is a general form of covariate adjustment that uses linear, generalized linear, and non-parametric or machine learning models for the conditional mean of the response given covariates. Under covariate-adaptive randomization, we establish general theorems that show a complete picture of the asymptotic normality, {efficiency gain, and applicability of AIPW estimators}. In particular, we provide for the first time a rigorous theoretical justification of using machine learning methods with cross-fitting for dependent data under covariate-adaptive randomization. Based on the general theorems, we offer insights on the conditions for guaranteed efficiency gain and universal applicability {under different randomization schemes}, which also motivate a joint calibration strategy using some constructed covariates after applying AIPW. Our methods are implemented in the R package RobinCar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10213v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marlena S. Bannick, Jun Shao, Jingyi Liu, Yu Du, Yanyao Yi, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal Besov Priors for Bayesian Inverse Problems</title>
      <link>https://arxiv.org/abs/2306.16378</link>
      <description>arXiv:2306.16378v2 Announce Type: replace 
Abstract: Fast development in science and technology has driven the need for proper statistical tools to capture special data features such as abrupt changes or sharp contrast. Many inverse problems in data science require spatiotemporal solutions derived from a sequence of time-dependent objects with these spatial features, e.g., dynamic reconstruction of computerized tomography (CT) images with edges. Conventional methods based on Gaussian processes (GP) often fall short in providing satisfactory solutions since they tend to offer over-smooth priors. Recently, the Besov process (BP), defined by wavelet expansions with random coefficients, has emerged as a more suitable prior for Bayesian inverse problems of this nature. While BP excels in handling spatial inhomogeneity, it does not automatically incorporate temporal correlation inherited in the dynamically changing objects. In this paper, we generalize BP to a novel spatiotemporal Besov process (STBP) by replacing the random coefficients in the series expansion with stochastic time functions as Q-exponential process (Q-EP) which governs the temporal correlation structure. We thoroughly investigate the mathematical and statistical properties of STBP. A white-noise representation of STBP is also proposed to facilitate the inference. Simulations, two limited-angle CT reconstruction examples and a highly non-linear inverse problem involving Navier-Stokes equation are used to demonstrate the advantage of the proposed STBP in preserving spatial features while accounting for temporal changes compared with the classic STGP and a time-uncorrelated approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16378v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiwei Lan, Mirjeta Pasha, Shuyi Li, Weining Shen</dc:creator>
    </item>
    <item>
      <title>The observed Fisher information attached to the EM algorithm, illustrated on Shepp and Vardi estimation procedure for positron emission tomography</title>
      <link>https://arxiv.org/abs/2310.17248</link>
      <description>arXiv:2310.17248v2 Announce Type: replace 
Abstract: The Shepp &amp; Vardi (1982) implementation of the EM algorithm for PET scan tumor estimation provides a point estimate of the tumor. The current study presents a closed-form formula of the observed Fisher information for Shepp &amp; Vardi PET scan tumor estimation. Keywords: PET scan, EM algorithm, Fisher information matrix, standard errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17248v2</guid>
      <category>stat.ME</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Isaac Meilijson</dc:creator>
    </item>
    <item>
      <title>Parametric Bootstrap on Networks with Non-Exchangeable Nodes</title>
      <link>https://arxiv.org/abs/2402.01866</link>
      <description>arXiv:2402.01866v2 Announce Type: replace 
Abstract: This paper studies the parametric bootstrap method for networks to quantify the uncertainty of statistics of interest. While existing network resampling methods primarily focus on count statistics under node-exchangeable (graphon) models, we consider more general network statistics (including local statistics) under the Chung-Lu model without node-exchangeability. We show that the natural network parametric bootstrap that first estimates the network generating model and then draws bootstrap samples from the estimated model generally suffers from bootstrap bias. As a general recipe for addressing this problem, we show that a two-level bootstrap procedure provably reduces the bias. This essentially extends the classical idea of iterative bootstrap to the network setting with a growing number of parameters. Moreover, the second-level bootstrap provides a way to construct higher-accuracy confidence intervals for many network statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01866v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhixuan Shao, Can M. Le</dc:creator>
    </item>
    <item>
      <title>Exploring Spatial Generalized Functional Linear Models: A Comparative Simulation Study and Analysis of COVID-19</title>
      <link>https://arxiv.org/abs/2403.03389</link>
      <description>arXiv:2403.03389v2 Announce Type: replace 
Abstract: Implementation of spatial generalized linear models with a functional covariate can be accomplished through the use of a truncated basis expansion of the covariate process. In practice, one must select a truncation level for use. We compare five criteria for the selection of an appropriate truncation level, including AIC and BIC based on a log composite likelihood, a fraction of variance explained criterion, a fitted mean squared error, and a prediction error with one standard error rule. Based on the use of extensive simulation studies, we propose that BIC constitutes a reasonable default criterion for the selection of the truncation level for use in a spatial functional generalized linear model. In addition, we demonstrate that the spatial model with a functional covariate outperforms other models when the data contain spatial structure and response variables are in fact influenced by a functional covariate process. We apply the spatial functional generalized linear model to a problem in which the objective is to relate COVID-19 vaccination rates in counties of states in the Midwestern United States to the number of new cases from previous weeks in those same geographic regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03389v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sooran Kim, Mark S. Kaiser, Xiongtao Dai</dc:creator>
    </item>
    <item>
      <title>Toward a Theory of Causation for Interpreting Neural Code Models</title>
      <link>https://arxiv.org/abs/2302.03788</link>
      <description>arXiv:2302.03788v4 Announce Type: replace-cross 
Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of $do_{code}$, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of $do_{code}$ as a useful method to detect and facilitate the elimination of confounding bias in NCMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03788v4</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk</dc:creator>
    </item>
    <item>
      <title>Randomly sampling bipartite networks with fixed degree sequences</title>
      <link>https://arxiv.org/abs/2305.04937</link>
      <description>arXiv:2305.04937v3 Announce Type: replace-cross 
Abstract: Statistical analysis of bipartite networks frequently requires randomly sampling from the set of all bipartite networks with the same degree sequence as an observed network. Trade algorithms offer an efficient way to generate samples of bipartite networks by incrementally `trading' the positions of some of their edges. However, it is difficult to know how many such trades are required to ensure that the sample is random. I propose a stopping rule that focuses on the distance between sampled networks and the observed network, and stops performing trades when this distribution stabilizes. Analyses demonstrate that, for over 300 different degree sequences, using this stopping rule ensures a random sample with a high probability, and that it is practical for use in empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04937v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary P. Neal</dc:creator>
    </item>
    <item>
      <title>PPI++: Efficient Prediction-Powered Inference</title>
      <link>https://arxiv.org/abs/2311.01453</link>
      <description>arXiv:2311.01453v2 Announce Type: replace-cross 
Abstract: We present PPI++: a computationally lightweight methodology for estimation and inference based on a small labeled dataset and a typically much larger dataset of machine-learning predictions. The methods automatically adapt to the quality of available predictions, yielding easy-to-compute confidence sets -- for parameters of any dimensionality -- that always improve on classical intervals using only the labeled data. PPI++ builds on prediction-powered inference (PPI), which targets the same problem setting, improving its computational and statistical efficiency. Real and synthetic experiments demonstrate the benefits of the proposed adaptations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01453v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, John C. Duchi, Tijana Zrnic</dc:creator>
    </item>
    <item>
      <title>Riemannian Laplace Approximation with the Fisher Metric</title>
      <link>https://arxiv.org/abs/2311.02766</link>
      <description>arXiv:2311.02766v3 Announce Type: replace-cross 
Abstract: Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the method, and demonstrating practical improvements in a range of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02766v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto Klami</dc:creator>
    </item>
  </channel>
</rss>
