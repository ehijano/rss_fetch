<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Apr 2025 01:58:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Projection Inference for set-identified SVARs</title>
      <link>https://arxiv.org/abs/2504.14106</link>
      <description>arXiv:2504.14106v1 Announce Type: new 
Abstract: We study the properties of projection inference for set-identified Structural Vector Autoregressions. A nominal $1-\alpha$ projection region collects the structural parameters that are compatible with a $1-\alpha$ Wald ellipsoid for the model's reduced-form parameters (autoregressive coefficients and the covariance matrix of residuals).
  We show that projection inference can be applied to a general class of stationary models, is computationally feasible, and -- as the sample size grows large -- it produces regions for the structural parameters and their identified set with both frequentist coverage and \emph{robust} Bayesian credibility of at least $1-\alpha$.
  A drawback of the projection approach is that both coverage and robust credibility may be strictly above their nominal level. Following the work of \cite{Kaido_Molinari_Stoye:2014}, we `calibrate' the radius of the Wald ellipsoid to guarantee that -- for a given posterior on the reduced-form parameters -- the robust Bayesian credibility of the projection method is exactly $1-\alpha$. If the bounds of the identified set are differentiable, our calibrated projection also covers the identified set with probability $1-\alpha$. %eliminating the excess of robust Bayesian credibility also eliminates excessive frequentist coverage.
  We illustrate the main results of the paper using the demand/supply-model for the U.S. labor market in Baumeister_Hamilton(2015)</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14106v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bulat Gafarov, Matthias Meier, Jos\'e Luis Montiel Olea</dc:creator>
    </item>
    <item>
      <title>Finite Population Identification and Design-Based Sensitivity Analysis</title>
      <link>https://arxiv.org/abs/2504.14127</link>
      <description>arXiv:2504.14127v1 Announce Type: new 
Abstract: We develop an approach to sensitivity analysis that uses design distributions to calibrate sensitivity parameters in a finite population model. We use this approach to (1) give a new formal analysis of the role of randomization, (2) provide a new motivation for examining covariate balance, and (3) show how to construct design-based confidence intervals for the average treatment effect, which allow for heterogeneous treatment effects but do not rely on asymptotics. This approach to confidence interval construction relies on partial identification analysis rather than hypothesis test inversion. Moreover, these intervals also have a non-frequentist, identification-based interpretation. We illustrate our approach in three empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14127v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendan Kline, Matthew A. Masten</dc:creator>
    </item>
    <item>
      <title>Global identification of dynamic panel models with interactive effects</title>
      <link>https://arxiv.org/abs/2504.14354</link>
      <description>arXiv:2504.14354v1 Announce Type: new 
Abstract: This paper examines the problem of global identification in dynamic panel models with interactive effects, a fundamental issue in econometric theory. We focus on the setting where the number of cross-sectional units (N) is large, but the time dimension (T) remains fixed. While local identification based on the Jacobian matrix is well understood and relatively straightforward to establish, achieving global identification remains a significant challenge. Under a set of mild and easily satisfied conditions, we demonstrate that the parameters of the model are globally identified, ensuring that no two distinct parameter values generate the same probability distribution of the observed data. Our findings contribute to the broader literature on identification in panel data models and have important implications for empirical research that relies on interactive effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14354v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jushan Bai, Pablo Mones</dc:creator>
    </item>
    <item>
      <title>Gaussian Transforms Modeling and the Estimation of Distributional Regression Functions</title>
      <link>https://arxiv.org/abs/2011.06416</link>
      <description>arXiv:2011.06416v2 Announce Type: replace 
Abstract: We propose flexible Gaussian representations for conditional cumulative distribution functions and give a concave likelihood criterion for their estimation. Optimal representations satisfy the monotonicity property of conditional cumulative distribution functions, including in finite samples and under general misspecification. We use these representations to provide a unified framework for the flexible Maximum Likelihood estimation of conditional density, cumulative distribution, and quantile functions at parametric rate. Our formulation yields substantial simplifications and finite sample improvements over related methods. An empirical application to the gender wage gap in the United States illustrates our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.06416v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard Spady, Sami Stouli</dc:creator>
    </item>
    <item>
      <title>Policy Learning with New Treatments</title>
      <link>https://arxiv.org/abs/2210.04703</link>
      <description>arXiv:2210.04703v3 Announce Type: replace 
Abstract: I study the problem of a decision maker choosing a policy which allocates treatment to a heterogeneous population on the basis of experimental data that includes only a subset of possible treatment values. The effects of new treatments are partially identified by shape restrictions on treatment response. Policies are compared according to the minimax regret criterion, and I show that the empirical analog of the population decision problem has a tractable linear- and integer-programming formulation. I prove the maximum regret of the estimated policy converges to the lowest possible maximum regret at a rate which is the maximum of N^-1/2 and the rate at which conditional average treatment effects are estimated in the experimental data. In an application to designing targeted subsidies for electrical grid connections in rural Kenya, I find that nearly the entire population should be given a treatment not implemented in the experiment, reducing maximum regret by over 60% compared to the policy that restricts to the treatments implemented in the experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.04703v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Higbee</dc:creator>
    </item>
    <item>
      <title>Nonparametric mixed logit model with market-level parameters estimated from market share data</title>
      <link>https://arxiv.org/abs/2309.13159</link>
      <description>arXiv:2309.13159v3 Announce Type: replace 
Abstract: We propose a nonparametric mixed logit model that is estimated using market-level choice share data. The model treats each market as an agent and represents taste heterogeneity through market-specific parameters by solving a multiagent inverse utility maximization problem, addressing the limitations of existing market-level choice models with parametric estimation. A simulation study is conducted to evaluate the performance of our model in terms of estimation time, estimation accuracy, and out-of-sample predictive accuracy. In a real data application, we estimate the travel mode choice of 53.55 million trips made by 19.53 million residents in New York State. These trips are aggregated based on population segments and census block group-level origin-destination (OD) pairs, resulting in 120,740 markets. We benchmark our model against multinomial logit (MNL), nested logit (NL), inverse product differentiation logit (IPDL), and the BLP models. The results show that the proposed model improves the out-of-sample accuracy from 65.30% to 81.78%, with a computation time less than one-tenth of that taken to estimate the BLP model. The price elasticities and diversion ratios retrieved from our model and benchmark models exhibit similar substitution patterns. Moreover, the market-level parameters estimated by our model provide additional insights and facilitate their seamless integration into supply-side optimization models for transportation design. By measuring the compensating variation for the driving mode, we found that a $9 congestion toll would impact roughly 60 % of the total travelers. As an application of supply-demand integration, we showed that a 50% discount of transit fare could bring a maximum ridership increase of 9402 trips per day under a budget of $50,000 per day.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13159v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.trb.2025.103220</arxiv:DOI>
      <arxiv:journal_reference>Transportation Research Part B 196 (2025) 103220</arxiv:journal_reference>
      <dc:creator>Xiyuan Ren, Joseph Y. J. Chow, Prateek Bansal</dc:creator>
    </item>
    <item>
      <title>Fused LASSO as Non-Crossing Quantile Regression</title>
      <link>https://arxiv.org/abs/2403.14036</link>
      <description>arXiv:2403.14036v3 Announce Type: replace 
Abstract: Growth-at-Risk is vital for empirical macroeconomics but is often suspect to quantile crossing due to data limitations. While existing literature addresses this through post-processing of the fitted quantiles, these methods do not correct the estimated coefficients. We advocate for imposing non-crossing constraints during estimation and demonstrate their equivalence to fused LASSO with quantile-specific shrinkage parameters. By re-examining Growth-at-Risk through an interquantile shrinkage lens, we achieve improved left-tail forecasts and better identification of variables that drive quantile variation. We show that these improvements have ramifications for policy tools such as Expected Shortfall and Quantile Local Projections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14036v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tibor Szendrei, Arnab Bhattacharjee, Mark E. Schaffer</dc:creator>
    </item>
    <item>
      <title>Testing Conditional Stochastic Dominance at Target Points</title>
      <link>https://arxiv.org/abs/2503.14747</link>
      <description>arXiv:2503.14747v2 Announce Type: replace 
Abstract: This paper introduces a novel test for conditional stochastic dominance (CSD) at specific values of the conditioning covariates, referred to as target points. The test is relevant for analyzing income inequality, evaluating treatment effects, and studying discrimination. We propose a Kolmogorov--Smirnov-type test statistic that utilizes induced order statistics from independent samples. Notably, the test features a data-independent critical value, eliminating the need for resampling techniques such as the bootstrap. Our approach avoids kernel smoothing and parametric assumptions, instead relying on a tuning parameter to select relevant observations. We establish the asymptotic properties of our test, showing that the induced order statistics converge to independent draws from the true conditional distributions and that the test is asymptotically of level $\alpha$ under weak regularity conditions. While our results apply to both continuous and discrete data, in the discrete case, the critical value only provides a valid upper bound. To address this, we propose a refined critical value that significantly enhances power, requiring only knowledge of the support size of the distributions. Additionally, we analyze the test's behavior in the limit experiment, demonstrating that it reduces to a problem analogous to testing unconditional stochastic dominance in finite samples. This framework allows us to prove the validity of permutation-based tests for stochastic dominance when the random variables are continuous. Monte Carlo simulations confirm the strong finite-sample performance of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14747v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico A. Bugni, Ivan A. Canay, Deborah Kim</dc:creator>
    </item>
    <item>
      <title>Inference for Synthetic Controls via Refined Placebo Tests</title>
      <link>https://arxiv.org/abs/2401.07152</link>
      <description>arXiv:2401.07152v3 Announce Type: replace-cross 
Abstract: The synthetic control method is often applied to problems with one treated unit and a small number of control units. A common inferential task in this setting is to test null hypotheses regarding the average treatment effect on the treated. Inference procedures that are justified asymptotically are often unsatisfactory due to (1) small sample sizes that render large-sample approximation fragile and (2) simplification of the estimation procedure that is implemented in practice. An alternative is permutation inference, which is related to a common diagnostic called the placebo test. It has provable Type-I error guarantees in finite samples without simplification of the method, when the treatment is uniformly assigned. Despite this robustness, the placebo test suffers from low resolution since the null distribution is constructed from only $N$ reference estimates, where $N$ is the sample size. This creates a barrier for statistical inference at a common level like $\alpha = 0.05$, especially when $N$ is small. We propose a novel leave-two-out procedure that bypasses this issue, while still maintaining the same finite-sample Type-I error guarantee under uniform assignment for a wide range of $N$. Unlike the placebo test whose Type-I error always equals the theoretical upper bound, our procedure often achieves a lower unconditional Type-I error than theory suggests; this enables useful inference in the challenging regime when $\alpha &lt; 1/N$. Empirically, our procedure achieves a higher power when the effect size is reasonably large and a comparable power otherwise. We generalize our procedure to non-uniform assignments and show how to conduct sensitivity analysis. From a methodological perspective, our procedure can be viewed as a new type of randomization inference different from permutation or rank-based inference, which is particularly effective in small samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07152v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lihua Lei, Timothy Sudijono</dc:creator>
    </item>
  </channel>
</rss>
