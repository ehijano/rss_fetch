<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Sep 2024 04:00:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Nonparametric Density Estimation for Data Scattered on Irregular Spatial Domains: A Likelihood-Based Approach Using Bivariate Penalized Spline Smoothing</title>
      <link>https://arxiv.org/abs/2408.16963</link>
      <description>arXiv:2408.16963v1 Announce Type: new 
Abstract: Accurately estimating data density is crucial for making informed decisions and modeling in various fields. This paper presents a novel nonparametric density estimation procedure that utilizes bivariate penalized spline smoothing over triangulation for data scattered over irregular spatial domains. The approach is likelihood-based with a regularization term that addresses the roughness of the logarithm of density based on a second-order differential operator. The proposed method offers greater efficiency and flexibility in estimating density over complex domains and has been theoretically supported by establishing the asymptotic convergence rate under mild natural conditions. Through extensive simulation studies and a real-world application that analyzes motor vehicle theft data from Portland City, Oregon, we demonstrate the advantages of the proposed method over existing techniques detailed in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16963v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kunal Das, Shan Yu, Guannan Wang, Li Wang</dc:creator>
    </item>
    <item>
      <title>Non-parametric Monitoring of Spatial Dependence</title>
      <link>https://arxiv.org/abs/2408.17022</link>
      <description>arXiv:2408.17022v1 Announce Type: new 
Abstract: In process monitoring applications, measurements are often taken regularly or randomly from different spatial locations in two or three dimensions. Here, we consider streams of regular, rectangular data sets and use spatial ordinal patterns (SOPs) as a non-parametric approach to detect spatial dependencies. A key feature of our proposed SOP charts is that they are distribution-free and do not require prior Phase-I analysis. We conduct an extensive simulation study, demonstrating the superiority and effectiveness of the proposed charts compared to traditional parametric approaches. We apply the SOP-based control charts to detect heavy rainfall in Germany, war-related fires in (eastern) Ukraine, and manufacturing defects in textile production. The wide range of applications and insights illustrate the broad utility of our non-parametric approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17022v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Ad\"ammer, Philipp Wittenberg, Christian H. Wei{\ss}, Murat Caner Testik</dc:creator>
    </item>
    <item>
      <title>Model-based clustering for covariance matrices via penalized Wishart mixture models</title>
      <link>https://arxiv.org/abs/2408.17040</link>
      <description>arXiv:2408.17040v1 Announce Type: new 
Abstract: Covariance matrices provide a valuable source of information about complex interactions and dependencies within the data. However, from a clustering perspective, this information has often been underutilized and overlooked. Indeed, commonly adopted distance-based approaches tend to rely primarily on mean levels to characterize and differentiate between groups. Recently, there have been promising efforts to cluster covariance matrices directly, thereby distinguishing groups solely based on the relationships between variables. From a model-based perspective, a probabilistic formalization has been provided by considering a mixture model with component densities following a Wishart distribution. Notwithstanding, this approach faces challenges when dealing with a large number of variables, as the number of parameters to be estimated increases quadratically. To address this issue, we propose a sparse Wishart mixture model, which assumes that the component scale matrices possess a cluster-dependent degree of sparsity. Model estimation is performed by maximizing a penalized log-likelihood, enforcing a covariance graphical lasso penalty on the component scale matrices. This penalty not only reduces the number of non-zero parameters, mitigating the challenges of high-dimensional settings, but also enhances the interpretability of results by emphasizing the most relevant relationships among variables. The proposed methodology is tested on both simulated and real data, demonstrating its ability to unravel the complexities of neuroimaging data and effectively cluster subjects based on the relational patterns among distinct brain regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17040v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Cappozzo, Alessandro Casa</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian Clustering for Integrative Analysis of Multi-View Data</title>
      <link>https://arxiv.org/abs/2408.17153</link>
      <description>arXiv:2408.17153v1 Announce Type: new 
Abstract: In the era of Big Data, scalable and accurate clustering algorithms for high-dimensional data are essential. We present new Bayesian Distance Clustering (BDC) models and inference algorithms with improved scalability while maintaining the predictive accuracy of modern Bayesian non-parametric models. Unlike traditional methods, BDC models the distance between observations rather than the observations directly, offering a compromise between the scalability of distance-based methods and the enhanced predictive power and probabilistic interpretation of model-based methods. However, existing BDC models still rely on performing inference on the partition model to group observations into clusters. The support of this partition model grows exponentially with the dataset's size, complicating posterior space exploration and leading to many costly likelihood evaluations. Inspired by K-medoids, we propose using tessellations in discrete space to simplify inference by focusing the learning task on finding the best tessellation centers, or "medoids." Additionally, we extend our models to effectively handle multi-view data, such as data comprised of clusters that evolve across time, enhancing their applicability to complex datasets. The real data application in numismatics demonstrates the efficacy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17153v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rafael Cabral, Maria de Iorio, Andrew Harris</dc:creator>
    </item>
    <item>
      <title>A note on promotion time cure models with a new biological consideration</title>
      <link>https://arxiv.org/abs/2408.17188</link>
      <description>arXiv:2408.17188v1 Announce Type: new 
Abstract: We introduce a generalized promotion time cure model motivated by a new biological consideration. The new approach is flexible to model heterogeneous survival data, in particular for addressing intra-sample heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17188v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.SC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Zhao, Fatih K{\i}z{\i}laslan</dc:creator>
    </item>
    <item>
      <title>Estimation and inference of average treatment effects under heterogeneous additive treatment effect model</title>
      <link>https://arxiv.org/abs/2408.17205</link>
      <description>arXiv:2408.17205v1 Announce Type: new 
Abstract: Randomized experiments are the gold standard for estimating treatment effects, yet network interference challenges the validity of traditional estimators by violating the stable unit treatment value assumption and introducing bias. While cluster randomized experiments mitigate this bias, they encounter limitations in handling network complexity and fail to distinguish between direct and indirect effects. To address these challenges, we develop a design-based asymptotic theory for the existing Horvitz--Thompson estimators of the direct, indirect, and global average treatment effects under Bernoulli trials. We assume the heterogeneous additive treatment effect model with a hidden network that drives interference. Observing that these estimators are inconsistent in dense networks, we introduce novel eigenvector-based regression adjustment estimators to ensure consistency. We establish the asymptotic normality of the proposed estimators and provide conservative variance estimators under the design-based inference framework, offering robust conclusions independent of the underlying stochastic processes of the network and model parameters. Our method's adaptability is demonstrated across various interference structures, including partial interference and local interference in a two-sided marketplace. Numerical studies further illustrate the efficacy of the proposed estimators, offering practical insights into handling network interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17205v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Lu, Hongzi Li, Hanzhong Liu</dc:creator>
    </item>
    <item>
      <title>Likelihood estimation for stochastic differential equations with mixed effects</title>
      <link>https://arxiv.org/abs/2408.17257</link>
      <description>arXiv:2408.17257v1 Announce Type: new 
Abstract: Stochastic differential equations provide a powerful and versatile tool for modelling dynamic phenomena affected by random noise. In case of repeated observations of time series for several experimental units, it is often the case that some of the parameters vary between the individual experimental units, which has motivated a considerable interest in stochastic differential equations with mixed effects, where a subset of the parameters are random. These models enables simultaneous representation of randomness in the dynamics and variability between experimental units. When the data are observations at discrete time points, the likelihood function is only rarely explicitly available, so for likelihood based inference numerical methods are needed. We present Gibbs samplers and stochastic EM-algorithms based on the simple methods for simulation of diffusion bridges in Bladt and S{\o}rensen (2014). These methods are easy to implement and have no tuning parameters. They are, moreover, computationally efficient at low sampling frequencies because the computing time increases linearly with the time between observations. The algorithms are shown to simplify considerably for exponential families of diffusion processes. In a simulation study, the estimation methods are shown to work well for Ornstein-Uhlenbeck processes and t-diffusions with mixed effects. Finally, the Gibbs sampler is applied to neuronal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17257v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Baltazar-Larios, Mogens Bladt, Michael S{\o}rensen</dc:creator>
    </item>
    <item>
      <title>Incorporating Memory into Continuous-Time Spatial Capture-Recapture Models</title>
      <link>https://arxiv.org/abs/2408.17278</link>
      <description>arXiv:2408.17278v1 Announce Type: new 
Abstract: Obtaining reliable and precise estimates of wildlife species abundance and distribution is essential for the conservation and management of animal populations and natural reserves. Remote sensors such as camera traps are increasingly employed to gather data on uniquely identifiable individuals. Spatial capture-recapture (SCR) models provide estimates of population and spatial density from such data. These models introduce spatial correlation between observations of the same individual through a latent activity center. However SCR models assume that observations are independent over time and space, conditional on their given activity center, so that observed sightings at a given time and location do not influence the probability of being seen at future times and/or locations. With detectors like camera traps, this is ecologically unrealistic given the smooth movement of animals over space through time. We propose a new continuous-time modeling framework that incorporates both an individual's (latent) activity center and (known) previous location and time of detection. We demonstrate that standard SCR models can produce substantially biased density estimates when there is correlation in the times and locations of detections, and that our new model performs substantially better than standard SCR models on data simulated through a movement model as well as in a real camera trap study of American martens where an improvement in model fit is observed when incorporating the observed locations and times of previous observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17278v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clara Panchaud, Ruth King, David Borchers, Hannah Worthington, Ian Durbach, Paul Van Dam-Bates</dc:creator>
    </item>
    <item>
      <title>On Nonparanormal Likelihoods</title>
      <link>https://arxiv.org/abs/2408.17346</link>
      <description>arXiv:2408.17346v1 Announce Type: new 
Abstract: Nonparanormal models describe the joint distribution of multivariate responses via latent Gaussian, and thus parametric, copulae while allowing flexible nonparametric marginals. Some aspects of such distributions, for example conditional independence, are formulated parametrically. Other features, such as marginal distributions, can be formulated non- or semiparametrically. Such models are attractive when multivariate normality is questionable.
  Most estimation procedures perform two steps, first estimating the nonparametric part. The copula parameters come second, treating the marginal estimates as known. This is sufficient for some applications. For other applications, e.g. when a semiparametric margin features parameters of interest or when standard errors are important, a simultaneous estimation of all parameters might be more advantageous.
  We present suitable parameterisations of nonparanormal models, possibly including semiparametric effects, and define four novel nonparanormal log-likelihood functions. In general, the corresponding one-step optimization problems are shown to be non-convex. In some cases, however, biconvex problems emerge. Several convex approximations are discussed.
  From a low-level computational point of view, the core contribution is the score function for multivariate normal log-probabilities computed via Genz' procedure. We present transformation discriminant analysis when some biomarkers are subject to limit-of-detection problems as an application and illustrate possible empirical gains in semiparametric efficient polychoric correlation analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17346v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>Comparing Propensity Score-Based Methods in Estimating the Treatment Effects: A Simulation Study</title>
      <link>https://arxiv.org/abs/2408.17385</link>
      <description>arXiv:2408.17385v1 Announce Type: new 
Abstract: In observational studies, the recorded treatment assignment is not purely random, but it is influenced by external factors such as patient characteristics, reimbursement policies, and existing guidelines. Therefore, the treatment effect can be estimated only after accounting for confounding factors. Propensity score (PS) methods are a family of methods that is widely used for this purpose. Although they are all based on the estimation of the a posteriori probability of treatment assignment given patient covariates, they estimate the treatment effect from different statistical points of view and are, thus, relatively hard to compare. In this work, we propose a simulation experiment in which a hypothetical cohort of subjects is simulated in seven scenarios of increasing complexity of the associations between covariates and treatment, but where the two main definitions of treatment effect (average treatment effect, ATE, and average effect of the treatment on the treated, ATT) coincide. Our purpose is to compare the performance of a wide array of PS-based methods (matching, stratification, and inverse probability weighting) in estimating the treatment effect and their robustness in different scenarios. We find that inverse probability weighting provides estimates of the treatment effect that are closer to the expected value by weighting all subjects of the starting population. Conversely, matching and stratification ensure that the subpopulation that generated the final estimate is made up of real instances drawn from the starting population, and, thus, provide a higher degree of control on the validity domain of the estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17385v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Poletto, Enrico Longato, Erica Tavazzi, Martina Vettoretti</dc:creator>
    </item>
    <item>
      <title>Dual-criterion Dose Finding Designs Based on Dose-Limiting Toxicity and Tolerability</title>
      <link>https://arxiv.org/abs/2408.17392</link>
      <description>arXiv:2408.17392v1 Announce Type: new 
Abstract: The primary objective of Phase I oncology trials is to assess the safety and tolerability of novel therapeutics. Conventional dose escalation methods identify the maximum tolerated dose (MTD) based on dose-limiting toxicity (DLT). However, as cancer therapies have evolved from chemotherapy to targeted therapies, these traditional methods have become problematic. Many targeted therapies rarely produce DLT and are administered over multiple cycles, potentially resulting in the accumulation of lower-grade toxicities, which can lead to intolerance, such as dose reduction or interruption. To address this issue, we proposed dual-criterion designs that find the MTD based on both DLT and non-DLT-caused intolerance. We considered the model-based design and model-assisted design that allow real-time decision-making in the presence of pending data due to long event assessment windows. Compared to DLT-based methods, our approaches exhibit superior operating characteristics when intolerance is the primary driver for determining the MTD and comparable operating characteristics when DLT is the primary driver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17392v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yunlong Yang, Ying Yuan</dc:creator>
    </item>
    <item>
      <title>Family of multivariate extended skew-elliptical distributions: Statistical properties, inference and application</title>
      <link>https://arxiv.org/abs/2408.17410</link>
      <description>arXiv:2408.17410v1 Announce Type: new 
Abstract: In this paper we propose a family of multivariate asymmetric distributions over an arbitrary subset of set of real numbers which is defined in terms of the well-known elliptically symmetric distributions. We explore essential properties, including the characterization of the density function for various distribution types, as well as other key aspects such as identifiability, quantiles, stochastic representation, conditional and marginal distributions, moments, Kullback-Leibler Divergence, and parameter estimation. A Monte Carlo simulation study is performed for examining the performance of the developed parameter estimation method. Finally, the proposed models are used to analyze socioeconomic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17410v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo, Leonardo Santos, Jo\~ao Monteiros, Felipe Quintino</dc:creator>
    </item>
    <item>
      <title>Weighted Regression with Sybil Networks</title>
      <link>https://arxiv.org/abs/2408.17426</link>
      <description>arXiv:2408.17426v1 Announce Type: new 
Abstract: In many online domains, Sybil networks -- or cases where a single user assumes multiple identities -- is a pervasive feature. This complicates experiments, as off-the-shelf regression estimators at least assume known network topologies (if not fully independent observations) when Sybil network topologies in practice are often unknown. The literature has exclusively focused on techniques to detect Sybil networks, leading many experimenters to subsequently exclude suspected networks entirely before estimating treatment effects. I present a more efficient solution in the presence of these suspected Sybil networks: a weighted regression framework that applies weights based on the probabilities that sets of observations are controlled by single actors. I show in the paper that the MSE-minimizing solution is to set the weight matrix equal to the inverse of the expected network topology. I demonstrate the methodology on simulated data, and then I apply the technique to a competition with suspected Sybil networks run on the Sui blockchain and show reductions in the standard error of the estimate by 6 - 24%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17426v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nihar Shah</dc:creator>
    </item>
    <item>
      <title>Multi-faceted Neuroimaging Data Integration via Analysis of Subspaces</title>
      <link>https://arxiv.org/abs/2408.16791</link>
      <description>arXiv:2408.16791v1 Announce Type: cross 
Abstract: Neuroimaging studies, such as the Human Connectome Project (HCP), often collect multi-faceted and multi-block data to study the complex human brain. However, these data are often analyzed in a pairwise fashion, which can hinder our understanding of how different brain-related measures interact with each other. In this study, we comprehensively analyze the multi-block HCP data using the Data Integration via Analysis of Subspaces (DIVAS) method. We integrate structural and functional brain connectivity, substance use, cognition, and genetics in an exhaustive five-block analysis. This gives rise to the important finding that genetics is the single data modality most predictive of brain connectivity, outside of brain connectivity itself. Nearly 14\% of the variation in functional connectivity (FC) and roughly 12\% of the variation in structural connectivity (SC) is attributed to shared spaces with genetics. Moreover, investigations of shared space loadings provide interpretable associations between particular brain regions and drivers of variability, such as alcohol consumption in the substance-use data block. Novel Jackstraw hypothesis tests are developed for the DIVAS framework to establish statistically significant loadings. For example, in the (FC, SC, and Substance Use) shared space, these novel hypothesis tests highlight largely negative functional and structural connections suggesting the brain's role in physiological responses to increased substance use. Furthermore, our findings have been validated using a subset of genetically relevant siblings or twins not studied in the main analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16791v1</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ackerman, Zhengwu Zhang, Jan Hannig, Jack Prothero, J. S. Marron</dc:creator>
    </item>
    <item>
      <title>On the choice of the two tuning parameters for nonparametric estimation of an elliptical distribution generator</title>
      <link>https://arxiv.org/abs/2408.17087</link>
      <description>arXiv:2408.17087v1 Announce Type: cross 
Abstract: Elliptical distributions are a simple and flexible class of distributions that depend on a one-dimensional function, called the density generator. In this article, we study the non-parametric estimator of this generator that was introduced by Liebscher (2005). This estimator depends on two tuning parameters: a bandwidth $h$ -- as usual in kernel smoothing -- and an additional parameter $a$ that control the behavior near the center of the distribution. We give an explicit expression for the asymptotic MSE at a point $x$, and derive explicit expressions for the optimal tuning parameters $h$ and $a$. Estimation of the derivatives of the generator is also discussed. A simulation study shows the performance of the new methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17087v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Ryan, Alexis Derumigny</dc:creator>
    </item>
    <item>
      <title>cosimmr: an R package for fast fitting of Stable Isotope Mixing Models with covariates</title>
      <link>https://arxiv.org/abs/2408.17230</link>
      <description>arXiv:2408.17230v1 Announce Type: cross 
Abstract: The study of animal diets and the proportional contribution that different foods make to their diets is an important task in ecology. Stable Isotope Mixing Models (SIMMs) are an important tool for studying an animal's diet and understanding how the animal interacts with its environment. We present cosimmr, a new R package designed to include covariates when estimating diet proportions in SIMMs, with simple functions to produce plots and summary statistics. The inclusion of covariates allows for users to perform a more in-depth analysis of their system and to gain new insights into the diets of the organisms being studied. A common problem with the previous generation of SIMMs is that they are very slow to produce a posterior distribution of dietary estimates, especially for more complex model structures, such as when covariates are included. The widely-used Markov chain Monte Carlo (MCMC) algorithm used by many traditional SIMMs often requires a very large number of iterations to reach convergence. In contrast, cosimmr uses Fixed Form Variational Bayes (FFVB), which we demonstrate gives up to an order of magnitude speed improvement with no discernible loss of accuracy. We provide a full mathematical description of the model, which includes corrections for trophic discrimination and concentration dependence, and evaluate its performance against the state of the art MixSIAR model. Whilst MCMC is guaranteed to converge to the posterior distribution in the long term, FFVB converges to an approximation of the posterior distribution, which may lead to sub-optimal performance. However we show that the package produces equivalent results in a fraction of the time for all the examples on which we test. The package is designed to be user-friendly and is based on the existing simmr framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17230v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Govan, Andrew L Jackson, Stuart Bearhop, Richard Inger, Brian C Stock, Brice X Semmens, Eric J Ward, Andrew C Parnell</dc:creator>
    </item>
    <item>
      <title>A functional regression model for heterogeneous BioGeoChemical Argo data in the Southern Ocean</title>
      <link>https://arxiv.org/abs/2211.04012</link>
      <description>arXiv:2211.04012v2 Announce Type: replace 
Abstract: Leveraging available measurements of our environment can help us understand complex processes. One example is Argo Biogeochemical data, which aims to collect measurements of oxygen, nitrate, pH, and other variables at varying depths in the ocean. We focus on the oxygen data in the Southern Ocean, which has implications for ocean biology and the Earth's carbon cycle. Systematic monitoring of such data has only recently begun to be established, and the data is sparse. In contrast, Argo measurements of temperature and salinity are much more abundant. In this work, we introduce and estimate a functional regression model describing dependence in oxygen, temperature, and salinity data at all depths covered by the Argo data simultaneously. Our model elucidates important aspects of the joint distribution of temperature, salinity, and oxygen. Due to fronts that establish distinct spatial zones in the Southern Ocean, we augment this functional regression model with a mixture component. By modelling spatial dependence in the mixture component and in the data itself, we provide predictions onto a grid and improve location estimates of fronts. Our approach is scalable to the size of the Argo data, and we demonstrate its success in cross-validation and a comprehensive interpretation of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.04012v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Korte-Stapff, Drew Yarger, Stilian Stoev, Tailen Hsing</dc:creator>
    </item>
    <item>
      <title>Model selection-based estimation for generalized additive models using mixtures of g-priors: Towards systematization</title>
      <link>https://arxiv.org/abs/2301.10468</link>
      <description>arXiv:2301.10468v4 Announce Type: replace 
Abstract: We explore the estimation of generalized additive models using basis expansion in conjunction with Bayesian model selection. Although Bayesian model selection is useful for regression splines, it has traditionally been applied mainly to Gaussian regression owing to the availability of a tractable marginal likelihood. We extend this method to handle an exponential family of distributions by using the Laplace approximation of the likelihood. Although this approach works well with any Gaussian prior distribution, consensus has not been reached on the best prior for nonparametric regression with basis expansions. Our investigation indicates that the classical unit information prior may not be ideal for nonparametric regression. Instead, we find that mixtures of g-priors are more effective. We evaluate various mixtures of g-priors to assess their performance in estimating generalized additive models. Additionally, we compare several priors for knots to determine the most effective strategy. Our simulation studies demonstrate that model selection-based approaches outperform other Bayesian methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.10468v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gyeonghun Kang, Seonghyun Jeong</dc:creator>
    </item>
    <item>
      <title>Backward Joint Model for the Dynamic Prediction of Both Competing Risk and Longitudinal Outcomes</title>
      <link>https://arxiv.org/abs/2311.00878</link>
      <description>arXiv:2311.00878v2 Announce Type: replace 
Abstract: Joint modeling is a useful approach to dynamic prediction of clinical outcomes using longitudinally measured predictors. When the outcomes are competing risk events, fitting the conventional shared random effects joint model often involves intensive computation, especially when multiple longitudinal biomarkers are be used as predictors, as is often desired in prediction problems. This paper proposes a new joint model for the dynamic prediction of competing risk outcomes. The model factorizes the likelihood into the distribution of the competing risks data and the distribution of longitudinal data given the competing risks data. It extends the basic idea of the recently published backward joint model (BJM) to the competing risk setting, and we call this model crBJM. This model also enables the prediction of future longitudinal data trajectories conditional on being at risk at a future time, a practically important problem that has not been studied in the statistical literature. The model fitting with the EM algorithm is efficient, stable and computationally fast, with a one-dimensional integral in the E-step and convex optimization for most parameters in the M-step, regardless of the number of longitudinal predictors. The model also comes with a consistent albeit less efficient estimation method that can be quickly implemented with standard software, ideal for model building and diagnostics. We study the numerical properties of the proposed method using simulations and illustrate its use in a chronic kidney disease study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00878v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhao Li, Brad C. Astor, Wei Yang, Tom H. Greene, Liang Li</dc:creator>
    </item>
    <item>
      <title>Power calculation for cross-sectional stepped wedge cluster randomized trials with a time-to-event endpoint</title>
      <link>https://arxiv.org/abs/2312.13097</link>
      <description>arXiv:2312.13097v2 Announce Type: replace 
Abstract: Stepped wedge cluster randomized trials (SW-CRTs) are a form of randomized trial whereby clusters are progressively transitioned from control to intervention, with the timing of transition randomized for each cluster. An important task at the design stage is to ensure that the planned trial has sufficient power to observe a clinically meaningful effect size. While methods for determining study power have been well-developed for SW-CRTs with continuous and binary outcomes, limited methods for power calculation are available for SW-CRTs with censored time-to-event outcomes. In this article, we propose a stratified marginal Cox model to account for secular trend in cross-sectional SW-CRTs, and derive an explicit expression of the robust sandwich variance to facilitate power calculations without the need for computationally intensive simulations. Power formulas based on both the Wald and robust score tests are developed and validated via simulation under different finite-sample scenarios. Finally, we illustrate our methods in the context of a SW-CRT testing the effect of a new electronic reminder system on time to catheter removal in hospital settings. We also offer an R Shiny application to facilitate sample size and power calculations using our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13097v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mary M. Ryan, Denise Esserman, Monica Taljaard, Fan Li</dc:creator>
    </item>
    <item>
      <title>Addressing Duplicated Data in Point Process Models</title>
      <link>https://arxiv.org/abs/2405.15192</link>
      <description>arXiv:2405.15192v2 Announce Type: replace 
Abstract: Spatial point process models are widely applied to point pattern data from various fields in the social and environmental sciences. However, a serious hurdle in fitting point process models is the presence of duplicated points, wherein multiple observations share identical spatial coordinates. This often occurs because of decisions made in the geo-coding process, such as assigning representative locations (e.g., aggregate-level centroids) to observations when data producers lack exact location information. Because spatial point process models like the Log-Gaussian Cox Process (LGCP) assume unique locations, researchers often employ {\it ad hoc} solutions (e.g., jittering) to address duplicated data before analysis. As an alternative, this study proposes a Modified Minimum Contrast (MMC) method that adapts the inference procedure to account for the effect of duplicates without needing to alter the data. The proposed MMC method is applied to LGCP models, with simulation results demonstrating the gains of our method relative to existing approaches in terms of parameter estimation. Interestingly, simulation results also show the effect of the geo-coding process on parameter estimates, which can be utilized in the implementation of the MMC method. The MMC approach is then used to infer the spatial clustering characteristics of conflict events in Afghanistan (2008-2009).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15192v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingling Chen, Mikyoung Jun, Scott J. Cook</dc:creator>
    </item>
    <item>
      <title>Bounds on causal effects in $2^{K}$ factorial experiments with non-compliance</title>
      <link>https://arxiv.org/abs/2407.12114</link>
      <description>arXiv:2407.12114v2 Announce Type: replace 
Abstract: Factorial experiments are ubiquitous in the social and biomedical sciences, but when units fail to comply with each assigned factors, identification and estimation of the average treatment effects become impossible. Leveraging an instrumental variables approach, previous studies have shown how to identify and estimate the causal effect of treatment uptake among respondents who comply with treatment. A major caveat is that these identification results rely on strong assumptions on the effect of randomization on treatment uptake. This paper shows how to bound these complier average treatment effects under more mild assumptions on non-compliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12114v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matthew Blackwell, Nicole E. Pashley</dc:creator>
    </item>
    <item>
      <title>Isotonic propensity score matching</title>
      <link>https://arxiv.org/abs/2207.08868</link>
      <description>arXiv:2207.08868v2 Announce Type: replace-cross 
Abstract: We propose a one-to-many matching estimator of the average treatment effect based on propensity scores estimated by isotonic regression. The method relies on the monotonicity assumption on the propensity score function, which can be justified in many applications in economics. We show that the nature of the isotonic estimator can help us to fix many problems of existing matching methods, including efficiency, choice of the number of matches, choice of tuning parameters, robustness to propensity score misspecification, and bootstrap validity. As a by-product, a uniformly consistent isotonic estimator is developed for our proposed matching method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.08868v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mengshan Xu, Taisuke Otsu</dc:creator>
    </item>
    <item>
      <title>Recursive Estimation of Conditional Kernel Mean Embeddings</title>
      <link>https://arxiv.org/abs/2302.05955</link>
      <description>arXiv:2302.05955v2 Announce Type: replace-cross 
Abstract: Kernel mean embeddings, a widely used technique in machine learning, map probability distributions to elements of a reproducing kernel Hilbert space (RKHS). For supervised learning problems, where input-output pairs are observed, the conditional distribution of outputs given the inputs is a key object. The input dependent conditional distribution of an output can be encoded with an RKHS valued function, the conditional kernel mean map. In this paper we present a new recursive algorithm to estimate the conditional kernel mean map in a Hilbert space valued $L_2$ space, that is in a Bochner space. We prove the weak and strong $L_2$ consistency of our recursive estimator under mild conditions. The idea is to generalize Stone's theorem for Hilbert space valued regression in a locally compact Polish space. We present new insights about conditional kernel mean embeddings and give strong asymptotic bounds regarding the convergence of the proposed recursive method. Finally, the results are demonstrated on three application domains: for inputs coming from Euclidean spaces, Riemannian manifolds and locally compact subsets of function spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.05955v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ambrus Tam\'as, Bal\'azs Csan\'ad Cs\'aji</dc:creator>
    </item>
    <item>
      <title>Inference on many jumps in nonparametric panel regression models</title>
      <link>https://arxiv.org/abs/2312.01162</link>
      <description>arXiv:2312.01162v2 Announce Type: replace-cross 
Abstract: We investigate the significance of change-point or jump effects within fully nonparametric regression contexts, with a particular focus on panel data scenarios where data generation processes vary across individual or group units, and error terms may display complex dependency structures. In our setting the threshold effect depends on a specific covariate, and we permit the true nonparametric regression to vary based on additional latent variables. We propose two uniform testing procedures: one to assess the existence of change-point effects and another to evaluate the uniformity of such effects across units. Even though the underlying data generation processes are neither independent nor identically distributed, our approach involves deriving a straightforward analytical expression to approximate the variance-covariance structure of change-point effects under general dependency conditions. Notably, when Gaussian approximations are made to these test statistics, the intricate dependency structures within the data can be safely disregarded owing to the localized nature of the statistics. This finding bears significant implications for obtaining critical values. Through extensive simulations, we demonstrate that our tests exhibit excellent control over size and reasonable power performance in finite samples, irrespective of strong cross-sectional and weak serial dependency within the data. Furthermore, applying our tests to two datasets reveals the existence of significant nonsmooth effects in both cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01162v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Likai Chen, Georg Keilbar, Liangjun Su, Weining Wang</dc:creator>
    </item>
    <item>
      <title>Invariant Causal Prediction with Local Models</title>
      <link>https://arxiv.org/abs/2401.05218</link>
      <description>arXiv:2401.05218v2 Announce Type: replace-cross 
Abstract: We consider the task of identifying the causal parents of a target variable among a set of candidates from observational data. Our main assumption is that the candidate variables are observed in different environments which may, under certain assumptions, be regarded as interventions on the observed system. We assume a linear relationship between target and candidates, which can be different in each environment with the only restriction that the causal structure is invariant across environments. Within our proposed setting we provide sufficient conditions for identifiability of the causal parents and introduce a practical method called L-ICP ($\textbf{L}$ocalized $\textbf{I}$nvariant $\textbf{Ca}$usal $\textbf{P}$rediction), which is based on a hypothesis test for parent identification using a ratio of minimum and maximum statistics. We then show in a simplified setting that the statistical power of L-ICP converges exponentially fast in the sample size, and finally we analyze the behavior of L-ICP experimentally in more general settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05218v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Mey, Rui Manuel Castro</dc:creator>
    </item>
  </channel>
</rss>
