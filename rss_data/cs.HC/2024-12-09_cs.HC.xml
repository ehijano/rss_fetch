<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Dec 2024 05:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Census-Stub Graph Invariant Descriptor</title>
      <link>https://arxiv.org/abs/2412.04582</link>
      <description>arXiv:2412.04582v1 Announce Type: new 
Abstract: An invariant descriptor captures meaningful structural features of networks, useful where traditional visualizations, like node-link views, face challenges like the hairball phenomenon (inscrutable overlap of points and lines). Designing invariant descriptors involves balancing abstraction and information retention, as richer data summaries demand more storage and computational resources. Building on prior work, chiefly the BMatrix -- a matrix descriptor visualized as the invariant network portrait heatmap -- we introduce BFS-Census, a new algorithm computing our Census data structures: Census-Node, Census-Edge, and Census-Stub. Our experiments show Census-Stub, which focuses on stubs (half-edges), has orders of magnitude greater discerning power (ability to tell non-isomorphic graphs apart) than any other descriptor in this study, without a difficult trade-off: the substantial increase in resolution does not come at a commensurate cost in storage space or computation power. We also present new visualizations -- our Hop-Census polylines and Census-Census trajectories -- and evaluate them using real-world graphs, including a sensitivity analysis that shows graph topology change maps to visual Census change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04582v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2024.3513275</arxiv:DOI>
      <dc:creator>Matt I. B. Oddo, Stephen Kobourov, Tamara Munzner</dc:creator>
    </item>
    <item>
      <title>Argumentative Experience: Reducing Confirmation Bias on Controversial Issues through LLM-Generated Multi-Persona Debates</title>
      <link>https://arxiv.org/abs/2412.04629</link>
      <description>arXiv:2412.04629v1 Announce Type: new 
Abstract: Large language models (LLMs) are enabling designers to give life to exciting new user experiences for information access. In this work, we present a system that generates LLM personas to debate a topic of interest from different perspectives. How might information seekers use and benefit from such a system? Can centering information access around diverse viewpoints help to mitigate thorny challenges like confirmation bias in which information seekers over-trust search results matching existing beliefs? How do potential biases and hallucinations in LLMs play out alongside human users who are also fallible and possibly biased?
  Our study exposes participants to multiple viewpoints on controversial issues via a mixed-methods, within-subjects study. We use eye-tracking metrics to quantitatively assess cognitive engagement alongside qualitative feedback. Compared to a baseline search system, we see more creative interactions and diverse information-seeking with our multi-persona debate system, which more effectively reduces user confirmation bias and conviction toward their initial beliefs. Overall, our study contributes to the emerging design space of LLM-based information access systems, specifically investigating the potential of simulated personas to promote greater exposure to information diversity, emulate collective intelligence, and mitigate bias in information seeking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04629v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Li Shi, Houjiang Liu, Yian Wong, Utkarsh Mujumdar, Dan Zhang, Jacek Gwizdka, Matthew Lease</dc:creator>
    </item>
    <item>
      <title>Nafas: Breathing Gymnastics Application</title>
      <link>https://arxiv.org/abs/2412.04667</link>
      <description>arXiv:2412.04667v1 Announce Type: new 
Abstract: Long sessions of computer use introduce physical and mental health risks, particularly for programmers and intensive computer users. Breathing exercises can improve focus, reduce stress, and overall well-being. However, existing tools for such practices are often app-based, requiring users to leave their workspace. In this technical report, we introduce Nafas, a command-line interface (CLI) application designed specifically for computer users, enabling them to perform breathing exercises directly within the terminal. Nafas offers structured breathing programs with various levels tailored to the needs of busy developers and other intensive computer users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04667v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sadra Sabouri, Sepand Haghighi</dc:creator>
    </item>
    <item>
      <title>'Being there together for health': A Systematic Review on the Feasibility, Effectiveness and Design Considerations of Immersive Collaborative Virtual Environments in Health Applications</title>
      <link>https://arxiv.org/abs/2412.04760</link>
      <description>arXiv:2412.04760v1 Announce Type: new 
Abstract: Effectively using immersive multi-user environments for digital applications (via virtual, augmented and mixed reality technologies) beckons the future of healthcare delivery in the metaverse. We aimed to evaluate the feasibility and effectiveness of these environments used in health applications, while identifying their design features.
  We systematically searched MEDLINE, PsycINFO, and Emcare databases for peer-reviewed original reports, published in English, without date restrictions until Aug 30, 2023, and conducted manual citation searching in Feb 2024. All studies using fully immersive extended reality technologies (e.g., head-mounted displays, smart glasses) while engaging more than one participant in an intervention with direct health benefits were included. A qualitative synthesis of findings is reported. The quality of research was assessed using JBI Critical Checklists. The review was pre-registered on PROSPERO (CRD42023479155).
  Of 2862 identified records, 10 studies were eligible. Included studies were mostly conducted with healthy young adults (five studies) and older adults (four studies). While they all used different models of Oculus/Meta headsets, their environments' designs were distinctive and aligned with their objectives. Findings indicated varying degrees of positive health outcomes, for engagement in rehabilitation, meaningful interactions across distances, positive affect, transformative experiences, mental health therapies, and motor skill learning. Participants reported high usability, motivation, enjoyment, presence and copresence. They also expressed the need for more training time with technology.
  Adopting an intentional intervention design, considering factors affecting presence and copresence, as well as integrating co-creation of the program with participants, seems integral to achieving positive health outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04760v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tohid Zarei (University of South Australia), Michelle Emery (University of South Australia), Dimitrios Saredakis (University of South Australia), Gun A. Lee (University of South Australia), Ben Stubbs (University of South Australia), Ancret Szpak (University of South Australia, The University of Adelaide), Tobias Loetscher (University of South Australia)</dc:creator>
    </item>
    <item>
      <title>PERCY: A Multimodal Dataset and Conversational System for Personalized and Emotionally Aware Human-Robot Interaction</title>
      <link>https://arxiv.org/abs/2412.04908</link>
      <description>arXiv:2412.04908v1 Announce Type: new 
Abstract: The integration of conversational agents into our daily lives has become increasingly common, yet many of these agents cannot engage in deep interactions with humans. Despite this, there is a noticeable shortage of datasets that capture multimodal information from human-robot interaction dialogues. To address this gap, we have developed a Personal Emotional Robotic Conversational sYstem (PERCY) and recorded a novel multimodal dataset that encompasses rich embodied interaction data. The process involved asking participants to complete a questionnaire and gathering their profiles on ten topics, such as hobbies and favourite music. Subsequently, we initiated conversations between the robot and the participants, leveraging GPT-4 to generate contextually appropriate responses based on the participant's profile and emotional state, as determined by facial expression recognition and sentiment analysis. Automatic and user evaluations were conducted to assess the overall quality of the collected data. The results of both evaluations indicated a high level of naturalness, engagement, fluency, consistency, and relevance in the conversation, as well as the robot's ability to provide empathetic responses. It is worth noting that the dataset is derived from genuine interactions with the robot, involving participants who provided personal information and conveyed actual emotions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04908v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <category>cs.RO</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammed Althubyani, Zhijin Meng, Shengyuan Xie, Cha Seung, Imran Razzak, Eduardo Benitez Sandoval, Baki Kocaballi, Mahdi Bamdad, Francisco Cruz Naranjo</dc:creator>
    </item>
    <item>
      <title>Bridging Culture and Finance: A Multimodal Analysis of Memecoins in the Web3 Ecosystem</title>
      <link>https://arxiv.org/abs/2412.04913</link>
      <description>arXiv:2412.04913v1 Announce Type: new 
Abstract: Memecoins, driven by social media engagement and cultural narratives, have rapidly grown within the Web3 ecosystem. Unlike traditional cryptocurrencies, they are shaped by humor, memes, and community sentiment. This paper introduces the Coin-Meme dataset, an open-source collection of visual, textual, community, and financial data from the Pump.fun platform on the Solana blockchain. We also propose a multimodal framework to analyze memecoins, uncovering patterns in cultural themes, community interaction, and financial behavior. Through clustering, sentiment analysis, and word cloud visualizations, we identify distinct thematic groups centered on humor, animals, and political satire. Additionally, we provide financial insights by analyzing metrics such as Market Entry Time and Market Capitalization, offering a comprehensive view of memecoins as both cultural artifacts and financial instruments within Web3. The Coin-Meme dataset is publicly available at https://github.com/hwlongCUHK/Coin-Meme.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04913v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hou-Wan Long, Nga-Man Wong, Wei Cai</dc:creator>
    </item>
    <item>
      <title>Preliminary Study on Virtual Reality Framework for Effective Prospective Memory Training: Integration of Visual Imagery and Daily-life Simulations</title>
      <link>https://arxiv.org/abs/2412.04949</link>
      <description>arXiv:2412.04949v1 Announce Type: new 
Abstract: Prospective memory (PM), defining the currently conceived intention of a future action, is crucial for daily functioning, particularly in aging populations. This study develops and validates a virtual reality prospective memory training (VR-PMT) system that integrates visual imagery training (VIT) and virtual reality training (VRT) to enhance the PM abilities of users. The framework is designed to progressively challenge users by simulating real-life PM tasks in a controlled VR environment. The VIT component is designed to improve the generation and utilization of visual imagery by users, while the VRT component provides PM tasks based on time and event cues within a virtual environment.The framework was evaluated on ten healthy adults (university students and elderly participants) over nine weeks. During the initial session, the baseline PM abilities of the participants were assessed using the memory for intentions screening test (MIST). The subsequent sessions alternated between VIT and VRT with increasing task complexity. The MIST scores were significantly positively correlated with task achievement, confirming the efficacy of the system. Imagery abilities were also strongly correlated with task performance, underscoring the importance of visual imagery in PM training.Usability and user experiences, evaluated on the Jikaku-sho Shirabe questionnaire and the user experience questionnaire, indicated an overall positive user experience but higher fatigue levels in elderly participants. This study demonstrates that the VR--PMT system effectively trains and assesses PM abilities by integrating VIT and VRT, supporting its potential for broader applications in clinical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04949v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Satoshi Fukumori, Kayoko Miura, Ayako Takamori, Sadao Otsuka</dc:creator>
    </item>
    <item>
      <title>Level Up or Game Over: Exploring How Dark Patterns Shape Mobile Games</title>
      <link>https://arxiv.org/abs/2412.05039</link>
      <description>arXiv:2412.05039v1 Announce Type: new 
Abstract: This study explores the prevalence of dark patterns in mobile games that exploit players through temporal, monetary, social, and psychological means. Recognizing the ethical concerns and potential harm surrounding these manipulative strategies, we analyze user-generated data of 1496 games to identify relationships between the deployment of dark patterns within "dark" and "healthy" games. Our findings reveal that dark patterns are not only widespread in games typically seen as problematic but are also present in games that may be perceived as benign. This research contributes needed quantitative support to the broader understanding of dark patterns in games. With an emphasis on ethical design, our study highlights current problems of revenue models that can be particularly harmful to vulnerable populations. To this end, we discuss the relevance of community-based approaches to surface harmful design and the necessity for collaboration among players/users and practitioners to promote healthier gaming experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05039v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3701571.3701604</arxiv:DOI>
      <arxiv:journal_reference>International Conference on Mobile and Ubiquitous Multimedia (MUM '24), December 1--4, 2024, Stockholm, Sweden</arxiv:journal_reference>
      <dc:creator>Sam Niknejad, Thomas Mildner, Nima Zargham, Susanne Putze, Rainer Malaka</dc:creator>
    </item>
    <item>
      <title>Information Flows for Athletes' Health and Performance Data</title>
      <link>https://arxiv.org/abs/2412.05055</link>
      <description>arXiv:2412.05055v1 Announce Type: new 
Abstract: Increasing numbers of athletes and sports teams use data collection technologies to improve athletic development and athlete health with the goal of improving competitive performance. Personal data privacy is managed but it is not always a priority for the coaches who are in charge of athletes. There is a pressing need to investigate what are appropriate information flows as described by contextual integrity for these data technologies and these use cases. We propose two main types of information flows for athletes' health and performance data -- team-centric and athlete-centric -- designed to characterize data used for the collective and individual physical, psychological and social development of athletes. We also present a scenario for applying differential privacy to athletes' data and propose two new information flows -- research-centric and community-centric &amp;mdash;which envision larger-scale, more collaborative sharing of athletes' data in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05055v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brad Stenger, Yuanyuan Feng</dc:creator>
    </item>
    <item>
      <title>Metamemory: Exploring the Resilience of Older Internal Migrants</title>
      <link>https://arxiv.org/abs/2412.05119</link>
      <description>arXiv:2412.05119v1 Announce Type: new 
Abstract: Immigration and aging have always been significant topics of discussion in society, concerning the stability and future development of a country and its people. Research in the field of HCI on immigration and aging has primarily focused on their practical needs but has paid less attention to the adaptability issues of older internal migrants moving with their families. In this study, we investigate the challenges older internal migrants face in adapting socially, using metadata surveys and semi-structured interviews to delve into their life struggles and resilience sources. Our findings highlight the older internal migrants' remarkable resilience, particularly evident in their reminiscences. We explore the integration of reminiscences with the metaverse, identifying the necessary conditions to create a "Metamemory". We introduce a novel design for a metaverse scene that bridges past and present experiences. This aims to encourage discussions on enhancing older internal migrants' reminiscence, leveraging the metaverse's positive potential, and devising strategies to more effectively address older internal migrants' concerns in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05119v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>CHI'2024: Workshop of the CHI Conference on Human Factors in Computing Systems, May 11--16, 2024, Honolulu, HI, USA</arxiv:journal_reference>
      <dc:creator>Xiaoxiao Wang, Jingjing Zhang, Huize Wan, Weiwei Zhang, Yuan Yao</dc:creator>
    </item>
    <item>
      <title>Exploring the Use of Drones for Taking Accessible Selfies with Elderly</title>
      <link>https://arxiv.org/abs/2412.05147</link>
      <description>arXiv:2412.05147v1 Announce Type: new 
Abstract: Selfie taking is a popular social pastime, and is an important part of socialising online. This activity is popular with young people but is also becoming more prevalent with older generations. Despite this, there are a number of accessibility issues when taking selfies. In this research, we investigate preferences from elderly citizens when taking a selfie, to understand the current challenges. As a potential solution to address the challenges identified, we propose the use of drones and present a novel concept for hands free selfie taking. With this work, we hope to trigger conversation around how such a technology can be utilised to enable elderly citizens, and more broadly people with physical disabilities, the ability to easily take part in this social pastime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05147v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuan Yao, Weiwei Zhang, Soojeong Yoo, Callum Parker, Jihong Jeung</dc:creator>
    </item>
    <item>
      <title>"If it has an exclamation point, I step away from it, I need facts, not excited feelings": Technologically Mediated Parental COVID Uncertainty</title>
      <link>https://arxiv.org/abs/2412.05181</link>
      <description>arXiv:2412.05181v1 Announce Type: new 
Abstract: As a novel virus, COVID introduced considerable uncertainty into the daily lives of people all over the globe since late 2019. Relying on twenty-three semi-structured interviews with parents whose children contracted COVID, we analyzed how the use of social media moderated parental uncertainty about the symptoms, prognosis, long-term potential health ramifications of infection, vaccination, and other issues. We framed our findings using Mishel's Uncertainty in Illness theory. We propose new components to the theory that account for technological mediation in uncertainty. We also propose design recommendations to help parents cope with health uncertainty using social media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05181v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karen Joy, Michelle Liang, Tawfiq Ammari</dc:creator>
    </item>
    <item>
      <title>Socio-Emotional Response Generation: A Human Evaluation Protocol for LLM-Based Conversational Systems</title>
      <link>https://arxiv.org/abs/2412.04492</link>
      <description>arXiv:2412.04492v1 Announce Type: cross 
Abstract: Conversational systems are now capable of producing impressive and generally relevant responses. However, we have no visibility nor control of the socio-emotional strategies behind state-of-the-art Large Language Models (LLMs), which poses a problem in terms of their transparency and thus their trustworthiness for critical applications. Another issue is that current automated metrics are not able to properly evaluate the quality of generated responses beyond the dataset's ground truth. In this paper, we propose a neural architecture that includes an intermediate step in planning socio-emotional strategies before response generation. We compare the performance of open-source baseline LLMs to the outputs of these same models augmented with our planning module. We also contrast the outputs obtained from automated metrics and evaluation results provided by human annotators. We describe a novel evaluation protocol that includes a coarse-grained consistency evaluation, as well as a finer-grained annotation of the responses on various social and emotional criteria. Our study shows that predicting a sequence of expected strategy labels and using this sequence to generate a response yields better results than a direct end-to-end generation scheme. It also highlights the divergences and the limits of current evaluation metrics for generated content. The code for the annotation platform and the annotated data are made publicly available for the evaluation of future models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04492v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>AHRI 2024, Sep 2024, Glasgow, United Kingdom</arxiv:journal_reference>
      <dc:creator>Lorraine Vanel (IDS, S2A, LTCI), Ariel R. Ramos Vela (IDS, S2A, LTCI), Alya Yacoubi (IDS, S2A, LTCI), Chlo\'e Clavel (IDS, S2A, LTCI)</dc:creator>
    </item>
    <item>
      <title>Parametric-ControlNet: Multimodal Control in Foundation Models for Precise Engineering Design Synthesis</title>
      <link>https://arxiv.org/abs/2412.04707</link>
      <description>arXiv:2412.04707v1 Announce Type: cross 
Abstract: This paper introduces a generative model designed for multimodal control over text-to-image foundation generative AI models such as Stable Diffusion, specifically tailored for engineering design synthesis. Our model proposes parametric, image, and text control modalities to enhance design precision and diversity. Firstly, it handles both partial and complete parametric inputs using a diffusion model that acts as a design autocomplete co-pilot, coupled with a parametric encoder to process the information. Secondly, the model utilizes assembly graphs to systematically assemble input component images, which are then processed through a component encoder to capture essential visual data. Thirdly, textual descriptions are integrated via CLIP encoding, ensuring a comprehensive interpretation of design intent. These diverse inputs are synthesized through a multimodal fusion technique, creating a joint embedding that acts as the input to a module inspired by ControlNet. This integration allows the model to apply robust multimodal control to foundation models, facilitating the generation of complex and precise engineering designs. This approach broadens the capabilities of AI-driven design tools and demonstrates significant advancements in precise control based on diverse data modalities for enhanced design generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04707v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Zhou, Yanxia Zhang, Chenyang Yuan, Frank Permenter, Nikos Arechiga, Matt Klenk, Faez Ahmed</dc:creator>
    </item>
    <item>
      <title>Robots in the Wild: Contextually-Adaptive Human-Robot Interactions in Urban Public Environments</title>
      <link>https://arxiv.org/abs/2412.04728</link>
      <description>arXiv:2412.04728v1 Announce Type: cross 
Abstract: The increasing transition of human-robot interaction (HRI) context from controlled settings to dynamic, real-world public environments calls for enhanced adaptability in robotic systems. This can go beyond algorithmic navigation or traditional HRI strategies in structured settings, requiring the ability to navigate complex public urban systems containing multifaceted dynamics and various socio-technical needs. Therefore, our proposed workshop seeks to extend the boundaries of adaptive HRI research beyond predictable, semi-structured contexts and highlight opportunities for adaptable robot interactions in urban public environments. This half-day workshop aims to explore design opportunities and challenges in creating contextually-adaptive HRI within these spaces and establish a network of interested parties within the OzCHI research community. By fostering ongoing discussions, sharing of insights, and collaborations, we aim to catalyse future research that empowers robots to navigate the inherent uncertainties and complexities of real-world public interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04728v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyan Yu, Yiyuan Wang, Tram Thi Minh Tran, Yi Zhao, Julie Stephany Berrio Perez, Marius Hoggenmuller, Justine Humphry, Lian Loke, Lynn Masuda, Callum Parker, Martin Tomitsch, Stewart Worrall</dc:creator>
    </item>
    <item>
      <title>Question Answering for Decisionmaking in Green Building Design: A Multimodal Data Reasoning Method Driven by Large Language Models</title>
      <link>https://arxiv.org/abs/2412.04741</link>
      <description>arXiv:2412.04741v1 Announce Type: cross 
Abstract: In recent years, the critical role of green buildings in addressing energy consumption and environmental issues has become widely acknowledged. Research indicates that over 40% of potential energy savings can be achieved during the early design stage. Therefore, decision-making in green building design (DGBD), which is based on modeling and performance simulation, is crucial for reducing building energy costs. However, the field of green building encompasses a broad range of specialized knowledge, which involves significant learning costs and results in low decision-making efficiency. Many studies have already applied artificial intelligence (AI) methods to this field. Based on previous research, this study innovatively integrates large language models with DGBD, creating GreenQA, a question answering framework for multimodal data reasoning. Utilizing Retrieval Augmented Generation, Chain of Thought, and Function Call methods, GreenQA enables multimodal question answering, including weather data analysis and visualization, retrieval of green building cases, and knowledge query. Additionally, this study conducted a user survey using the GreenQA web platform. The results showed that 96% of users believed the platform helped improve design efficiency. This study not only effectively supports DGBD but also provides inspiration for AI-assisted design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04741v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihui Li, Xiaoyue Yan, Hao Zhou, Borong Lin</dc:creator>
    </item>
    <item>
      <title>Employee Well-being in the Age of AI: Perceptions, Concerns, Behaviors, and Outcomes</title>
      <link>https://arxiv.org/abs/2412.04796</link>
      <description>arXiv:2412.04796v1 Announce Type: cross 
Abstract: The growing integration of Artificial Intelligence (AI) into Human Resources (HR) processes has transformed the way organizations manage recruitment, performance evaluation, and employee engagement. While AI offers numerous advantages, such as improved efficiency, reduced bias, and hyper-personalization, it raises significant concerns about employee well-being, job security, fairness, and transparency. This study examines how AI shapes employee perceptions, job satisfaction, mental health, and retention. Key findings reveal that while AI can enhance efficiency and reduce bias, it also raises concerns about job security, fairness, and privacy. Transparency in AI systems emerges as a critical factor in fostering trust and positive employee attitudes. AI systems can both support and undermine employee well-being, depending on how they are implemented and perceived. The research introduces an AI-employee well-being Interaction Framework, illustrating how AI influences employee perceptions, behaviors, and outcomes. Organizational strategies, such as clear communication, upskilling programs, and employee involvement in AI implementation, are identified as crucial for mitigating negative impacts and enhancing positive outcomes. The study concludes that the successful integration of AI in HR requires a balanced approach that prioritizes employee well-being, facilitates human-AI collaboration, and ensures ethical and transparent AI practices alongside technological advancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04796v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soheila Sadeghi</dc:creator>
    </item>
    <item>
      <title>Assessing Similarity Measures for the Evaluation of Human-Robot Motion Correspondence</title>
      <link>https://arxiv.org/abs/2412.04820</link>
      <description>arXiv:2412.04820v1 Announce Type: cross 
Abstract: One key area of research in Human-Robot Interaction is solving the human-robot correspondence problem, which asks how a robot can learn to reproduce a human motion demonstration when the human and robot have different dynamics and kinematic structures. Evaluating these correspondence problem solutions often requires the use of qualitative surveys that can be time consuming to design and administer. Additionally, qualitative survey results vary depending on the population of survey participants. In this paper, we propose the use of heterogeneous time-series similarity measures as a quantitative evaluation metric for evaluating motion correspondence to complement these qualitative surveys. To assess the suitability of these measures, we develop a behavioral cloning-based motion correspondence model, and evaluate it with a qualitative survey as well as quantitative measures. By comparing the resulting similarity scores with the human survey results, we identify Gromov Dynamic Time Warping as a promising quantitative measure for evaluating motion correspondence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04820v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Dietzel, Patrick J. Martin</dc:creator>
    </item>
    <item>
      <title>'Debunk-It-Yourself': Health Professionals' Strategies for Responding to Misinformation on TikTok</title>
      <link>https://arxiv.org/abs/2412.04999</link>
      <description>arXiv:2412.04999v1 Announce Type: cross 
Abstract: Misinformation is "sticky" in nature, requiring a considerable effort to undo its influence. One such effort is debunking or exposing the falsity of information. As an abundance of misinformation is on social media, platforms do bear some debunking responsibility in order to preserve their trustworthiness as information providers. A subject of interpretation, platforms poorly meet this responsibility and allow dangerous health misinformation to influence many of their users. This open route to harm did not sit well with health professional users, who recently decided to take the debunking into their own hands. To study this individual debunking effort - which we call 'Debunk-It-Yourself (DIY)' - we conducted an exploratory survey n=14 health professionals who wage a misinformation counter-influence campaign through videos on TikTok. We focused on two topics, nutrition and mental health, which are the ones most often subjected to misinformation on the platform. Our thematic analysis reveals that the counterinfluence follows a common process of initiation, selection, creation, and "stitching" or duetting a debunking video with a misinformation video. The 'Debunk-It-Yourself' effort was underpinned by three unique aspects: (i) it targets trending misinformation claims perceived to be of direct harm to people's health; (ii) it offers a symmetric response to the misinformation; and (iii) it is strictly based on scientific evidence and claimed clinical experience. Contrasting the 'Debunk-It-Yourself' effort with the one TikTok and other platforms (reluctantly) put in moderation, we offer recommendations for a structured response against the misinformation's influence by the users themselves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04999v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filipo Sharevski, Jennifer Vander Loop, Amy Devine, Peter Jachim, Sanchari Das</dc:creator>
    </item>
    <item>
      <title>Integrating Semantic Communication and Human Decision-Making into an End-to-End Sensing-Decision Framework</title>
      <link>https://arxiv.org/abs/2412.05103</link>
      <description>arXiv:2412.05103v1 Announce Type: cross 
Abstract: As early as 1949, Weaver defined communication in a very broad sense to include all procedures by which one mind or technical system can influence another, thus establishing the idea of semantic communication. With the recent success of machine learning in expert assistance systems where sensed information is wirelessly provided to a human to assist task execution, the need to design effective and efficient communications has become increasingly apparent. In particular, semantic communication aims to convey the meaning behind the sensed information relevant for Human Decision-Making (HDM). Regarding the interplay between semantic communication and HDM, many questions remain, such as how to model the entire end-to-end sensing-decision-making process, how to design semantic communication for the HDM and which information should be provided to the HDM. To address these questions, we propose to integrate semantic communication and HDM into one probabilistic end-to-end sensing-decision framework that bridges communications and psychology. In our interdisciplinary framework, we model the human through a HDM process, allowing us to explore how feature extraction from semantic communication can best support human decision-making. In this sense, our study provides new insights for the design/interaction of semantic communication with models of HDM. Our initial analysis shows how semantic communication can balance the level of detail with human cognitive capabilities while demanding less bandwidth, power, and latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05103v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edgar Beck, Hsuan-Yu Lin, Patrick R\"uckert, Yongping Bao, Bettina von Helversen, Sebastian Fehrler, Kirsten Tracht, Armin Dekorsy</dc:creator>
    </item>
    <item>
      <title>SurgBox: Agent-Driven Operating Room Sandbox with Surgery Copilot</title>
      <link>https://arxiv.org/abs/2412.05187</link>
      <description>arXiv:2412.05187v1 Announce Type: cross 
Abstract: Surgical interventions, particularly in neurology, represent complex and high-stakes scenarios that impose substantial cognitive burdens on surgical teams. Although deliberate education and practice can enhance cognitive capabilities, surgical training opportunities remain limited due to patient safety concerns. To address these cognitive challenges in surgical training and operation, we propose SurgBox, an agent-driven sandbox framework to systematically enhance the cognitive capabilities of surgeons in immersive surgical simulations. Specifically, our SurgBox leverages large language models (LLMs) with tailored Retrieval-Augmented Generation (RAG) to authentically replicate various surgical roles, enabling realistic training environments for deliberate practice. In particular, we devise Surgery Copilot, an AI-driven assistant to actively coordinate the surgical information stream and support clinical decision-making, thereby diminishing the cognitive workload of surgical teams during surgery. By incorporating a novel Long-Short Memory mechanism, our Surgery Copilot can effectively balance immediate procedural assistance with comprehensive surgical knowledge. Extensive experiments using real neurosurgical procedure records validate our SurgBox framework in both enhancing surgical cognitive capabilities and supporting clinical decision-making. By providing an integrated solution for training and operational support to address cognitive challenges, our SurgBox framework advances surgical education and practice, potentially transforming surgical outcomes and healthcare quality. The code is available at https://github.com/franciszchen/SurgBox.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05187v1</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinlin Wu, Xusheng Liang, Xuexue Bai, Zhen Chen</dc:creator>
    </item>
    <item>
      <title>AI's assigned gender affects human-AI cooperation</title>
      <link>https://arxiv.org/abs/2412.05214</link>
      <description>arXiv:2412.05214v1 Announce Type: cross 
Abstract: Cooperation between humans and machines is increasingly vital as artificial intelligence (AI) becomes more integrated into daily life. Research indicates that people are often less willing to cooperate with AI agents than with humans, more readily exploiting AI for personal gain. While prior studies have shown that giving AI agents human-like features influences people's cooperation with them, the impact of AI's assigned gender remains underexplored. This study investigates how human cooperation varies based on gender labels assigned to AI agents with which they interact. In the Prisoner's Dilemma game, 402 participants interacted with partners labelled as AI (bot) or humans. The partners were also labelled male, female, non-binary, or gender-neutral. Results revealed that participants tended to exploit female-labelled and distrust male-labelled AI agents more than their human counterparts, reflecting gender biases similar to those in human-human interactions. These findings highlight the significance of gender biases in human-AI interactions that must be considered in future policy, design of interactive AI systems, and regulation of their use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05214v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepideh Bazazi, Jurgis Karpus, Taha Yasseri</dc:creator>
    </item>
    <item>
      <title>Finding Understanding and Support: Navigating Online Communities to Share and Connect at the intersection of Abuse and Foster Care Experiences</title>
      <link>https://arxiv.org/abs/2404.18301</link>
      <description>arXiv:2404.18301v2 Announce Type: replace 
Abstract: Many children in foster care experience trauma that is rooted in unstable family relationships. Other members of the foster care system like foster parents and social workers face secondary trauma. Drawing on 10 years of Reddit data, we used a mixed methods approach to analyze how different members of the foster care system find support and similar experiences at the intersection of two Reddit communities - foster care, and abuse. Users who cross this boundary focus on trauma experiences specific to different roles in foster care. While representing a small number of users, boundary crossing users contribute heavily to both communities, and, compared to matching users, receive higher scores and more replies. We explore the roles boundary crossing users have both in the online community and in the context of foster care. Finally, we present design recommendations that would support trauma survivors find communities more suited to their personal experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18301v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tawfiq Ammari, Eunhye Ahn, Astha Lakhankar, Joyce Lee</dc:creator>
    </item>
    <item>
      <title>Cultural Reflections in Virtual Reality: The Effects of User Ethnicity in Avatar Matching Experiences on Sense of Embodiment</title>
      <link>https://arxiv.org/abs/2407.10412</link>
      <description>arXiv:2407.10412v2 Announce Type: replace 
Abstract: Matching avatar characteristics to a user can impact sense of embodiment (SoE) in VR. However, few studies have examined how participant demographics may interact with these matching effects. We recruited a diverse and racially balanced sample of 78 participants to investigate the differences among participant groups when embodying both demographically matched and unmatched avatars. We found that participant ethnicity emerged as a significant factor, with Asian and Black participants reporting lower total SoE compared to Hispanic participants. Furthermore, we found that user ethnicity significantly influences ownership (a subscale of SoE), with Asian and Black participants exhibiting stronger effects of matched avatar ethnicity compared to White participants. Additionally, Hispanic participants showed no significant differences, suggesting complex dynamics in ethnic-racial identity. Our results also reveal significant main effects of matched avatar ethnicity and gender on SoE, indicating the importance of considering these factors in VR experiences. These findings contribute valuable insights into understanding the complex dynamics shaping VR experiences across different demographic groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10412v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2024.3456196</arxiv:DOI>
      <dc:creator>Tiffany D. Do, Juanita Benjamin, Camille Isabella Protko, Ryan P. McMahan</dc:creator>
    </item>
    <item>
      <title>The Patchkeeper: An Integrated Wearable Electronic Stethoscope with Multiple Sensors</title>
      <link>https://arxiv.org/abs/2407.11837</link>
      <description>arXiv:2407.11837v3 Announce Type: replace 
Abstract: Many parts of human body generate internal sound during biological processes, which are rich sources of information for understanding health and wellbeing. Despite a long history of development and usage of stethoscopes, there is still a lack of proper tools for recording internal body sound together with complementary sensors for long term monitoring. In this paper, we show our development of a wearable electronic stethoscope, coined Patchkeeper (PK), that can be used for internal body sound recording over long periods of time. Patchkeeper also integrates several state-of-the-art biological sensors, including electrocardiogram (ECG), photoplethysmography (PPG), and inertial measurement unit (IMU) sensors. As a wearable device, Patchkeeper can be placed on various parts of the body to collect sound from particular organs, including heart, lung, stomach, and joints etc. We show in this paper that several vital signals can be recorded simultaneously with high quality. As Patchkeeper can be operated directly by the user, e.g. without involving health care professionals, we believe it could be a useful tool for telemedicine and remote diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11837v3</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hongwei Li, Zoran Radivojevic, Maja Hedlund, Anton Fahlgren, Michael S. Eggleston</dc:creator>
    </item>
    <item>
      <title>Awake at the Wheel: Enhancing Automotive Safety through EEG-Based Fatigue Detection</title>
      <link>https://arxiv.org/abs/2408.13929</link>
      <description>arXiv:2408.13929v2 Announce Type: replace 
Abstract: Driver fatigue detection is increasingly recognized as critical for enhancing road safety. This study introduces a method for detecting driver fatigue using the SEED-VIG dataset, a well-established benchmark in EEG-based vigilance analysis. By employing advanced pattern recognition technologies, including machine learning and deep neural networks, EEG signals are meticulously analyzed to discern patterns indicative of fatigue. This methodology combines feature extraction with a classification framework to improve the accuracy of fatigue detection. The proposed NLMDA-Net reached an impressive accuracy of 83.71% in detecting fatigue from EEG signals by incorporating two novel attention modules designed specifically for EEG signals, the channel and depth attention modules. NLMDA-Net effectively integrate features from multiple dimensions, resulting in improved classification performance. This success stems from integrating temporal convolutions and attention mechanisms, which effectively interpret EEG data. Designed to capture both temporal and spatial characteristics of EEG signals, deep learning classifiers have proven superior to traditional methods. The results of this study reveal a substantial enhancement in detection rates over existing models, highlighting the efficacy of the proposed approach for practical applications. The implications of this research are profound, extending beyond academic realms to inform the development of more sophisticated driver assistance systems. Incorporating this fatigue detection algorithm into these systems could significantly reduce fatigue-related incidents on the road, thus fostering safer driving conditions. This paper provides an exhaustive analysis of the dataset, methods employed, results obtained, and the potential real-world applications of the findings, aiming to contribute significantly to advancements in automotive safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13929v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-78195-7_23</arxiv:DOI>
      <dc:creator>Gourav Siddhad, Sayantan Dey, Partha Pratim Roy, Masakazu Iwamura</dc:creator>
    </item>
    <item>
      <title>Neural Networks Meet Neural Activity: Utilizing EEG for Mental Workload Estimation</title>
      <link>https://arxiv.org/abs/2408.13930</link>
      <description>arXiv:2408.13930v2 Announce Type: replace 
Abstract: Electroencephalography (EEG) offers non-invasive, real-time mental workload assessment, which is crucial in high-stakes domains like aviation and medicine and for advancing brain-computer interface (BCI) technologies. This study introduces a customized ConvNeXt architecture, a powerful convolutional neural network, specifically adapted for EEG analysis. ConvNeXt addresses traditional EEG challenges like high dimensionality, noise, and variability, enhancing the precision of mental workload classification. Using the STEW dataset, the proposed ConvNeXt model is evaluated alongside SVM, EEGNet, and TSception on binary (No vs SIMKAP task) and ternary (SIMKAP multitask) class mental workload tasks. Results demonstrated that ConvNeXt significantly outperformed the other models, achieving accuracies of 95.76% for binary and 95.11% for multi-class classification. This demonstrates ConvNeXt's resilience and efficiency for EEG data analysis, establishing new standards for mental workload evaluation. These findings represent a considerable advancement in EEG-based mental workload estimation, laying the foundation for future improvements in cognitive state measurements. This has broad implications for safety, efficiency, and user experience across various scenarios. Integrating powerful neural networks such as ConvNeXt is a critical step forward in non-invasive cognitive monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13930v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-78195-7_22</arxiv:DOI>
      <dc:creator>Gourav Siddhad, Partha Pratim Roy, Byung-Gyu Kim</dc:creator>
    </item>
    <item>
      <title>DrowzEE-G-Mamba: Leveraging EEG and State Space Models for Driver Drowsiness Detection</title>
      <link>https://arxiv.org/abs/2408.16145</link>
      <description>arXiv:2408.16145v2 Announce Type: replace 
Abstract: Driver drowsiness is identified as a critical factor in road accidents, necessitating robust detection systems to enhance road safety. This study proposes a driver drowsiness detection system, DrowzEE-G-Mamba, that combines Electroencephalography (EEG) with State Space Models (SSMs). EEG data, known for its sensitivity to alertness, is used to model driver state transitions between alert and drowsy. Compared to traditional methods, DrowzEE-G-Mamba achieves significantly improved detection rates and reduced false positives. Notably, it achieves a peak accuracy of 83.24% on the SEED-VIG dataset, surpassing existing techniques. The system maintains high accuracy across varying complexities, making it suitable for real-time applications with limited resources. This robustness is attributed to the combination of channel-split, channel-concatenation, and channel-shuffle operations within the architecture, optimizing information flow from EEG data. Additionally, the integration of convolutional layers and SSMs facilitates comprehensive analysis, capturing both local features and long-range dependencies in the EEG signals. These findings suggest the potential of DrowzEE-G-Mamba for enhancing road safety through accurate drowsiness detection. It also paves the way for developing powerful SSM-based AI algorithms in Brain-Computer Interface applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16145v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-78398-2_19</arxiv:DOI>
      <dc:creator>Gourav Siddhad, Sayantan Dey, Partha Pratim Roy</dc:creator>
    </item>
    <item>
      <title>Practicing Stress Relief for the Everyday: Designing Social Simulation Using VR, AR, and LLMs</title>
      <link>https://arxiv.org/abs/2410.01672</link>
      <description>arXiv:2410.01672v3 Announce Type: replace 
Abstract: Stress is an inevitable part of day-to-day life yet many find themselves unable to manage it themselves, particularly when professional or peer support are not always readily available. As self-care becomes increasingly vital for mental well-being, this paper explores the potential of social simulation as a safe, virtual environment for practicing stress relief for everyday situations. Leveraging the immersive capabilities of VR, AR, and LLMs, we developed eight interactive prototypes for various everyday stressful scenarios (e.g. public speaking) then conducted prototype-driven semi-structured interviews with 19 participants. We reveal that people currently lack effective means to support themselves through everyday stress and found that social simulation fills a gap for simulating real environments for training mental health practices. We outline key considerations for future development of simulation for self-care, including risks of trauma from hyper-realism, distrust of LLM-recommended timing for mental health recommendations, and the value of accessibility for self-care interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01672v3</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anna Fang, Hriday Chhabria, Alekhya Maram, Haiyi Zhu</dc:creator>
    </item>
    <item>
      <title>Transit drivers' reflections on the benefits and harms of eye tracking technology</title>
      <link>https://arxiv.org/abs/2410.24131</link>
      <description>arXiv:2410.24131v2 Announce Type: replace 
Abstract: Eye tracking technology offers great potential for improving road safety. It is already being built into vehicles, namely cars and trucks. When this technology is integrated into transit service vehicles, employees, i.e., bus drivers, will be subject to being eye tracked on their job. Although there is much research effort advancing algorithms for eye tracking in transportation, less is known about how end users perceive this technology, especially when interacting with it in an employer-mandated context. In this first study of its kind, we investigated transit bus operators' perceptions of eye tracking technology. From a methodological perspective, we introduce a mixed methods approach where participants experience the technology first-hand and then reflect on their experience while viewing a playback of the recorded data. Thematic analysis of the interview transcripts reveals interesting potential uses of eye tracking in this work context and surfaces transit operators' fears and concerns about this technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24131v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shaina Murphy, Bryce Grame, Ethan Smith, Siva Srinivasan, Eakta Jain</dc:creator>
    </item>
    <item>
      <title>Understanding cyclists' perception of driverless vehicles through eye-tracking and interviews</title>
      <link>https://arxiv.org/abs/2408.10064</link>
      <description>arXiv:2408.10064v4 Announce Type: replace-cross 
Abstract: As automated vehicles (AVs) become increasingly popular, the question arises as to how cyclists will interact with such vehicles. This study investigated (1) whether cyclists spontaneously notice if a vehicle is driverless, (2) how well they perform a driver-detection task when explicitly instructed, and (3) how they carry out these tasks. Using a Wizard-of-Oz method, 37 participants cycled a designated route and encountered an AV multiple times in two experimental sessions. In Session 1, participants cycled the route uninstructed, while in Session 2, they were instructed to verbally report whether they detected the presence or absence of a driver. Additionally, we recorded participants' gaze behaviour with eye-tracking and their responses in post-session interviews. The interviews revealed that 30% of the cyclists spontaneously mentioned the absence of a driver (Session 1), and when instructed (Session 2), they detected the absence and presence of the driver with 93% accuracy. The eye-tracking data showed that cyclists looked more frequently and for longer at the vehicle in Session 2 compared to Session 1. Additionally, participants exhibited intermittent sampling of the vehicle, and they looked at the area in front of the vehicle when it was far away and towards the windshield region when it was closer. The post-session interviews also indicated that participants were curious, but felt safe, and reported a need to receive information about the AV's driving state. In conclusion, cyclists can detect the absence of a driver in the AV, and this detection may influence their perception of safety. Further research is needed to explore these findings in real-world traffic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10064v4</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siri Hegna Berge, Joost de Winter, Dimitra Dodou, Amir Pooyan Afghari, Eleonora Papadimitriou, Nagarjun Reddy, Yongqi Dong, Narayana Raju, Haneen Farah</dc:creator>
    </item>
    <item>
      <title>Bridging Text and Image for Artist Style Transfer via Contrastive Learning</title>
      <link>https://arxiv.org/abs/2410.09566</link>
      <description>arXiv:2410.09566v2 Announce Type: replace-cross 
Abstract: Image style transfer has attracted widespread attention in the past few years. Despite its remarkable results, it requires additional style images available as references, making it less flexible and inconvenient. Using text is the most natural way to describe the style. More importantly, text can describe implicit abstract styles, like styles of specific artists or art movements. In this paper, we propose a Contrastive Learning for Artistic Style Transfer (CLAST) that leverages advanced image-text encoders to control arbitrary style transfer. We introduce a supervised contrastive training strategy to effectively extract style descriptions from the image-text model (i.e., CLIP), which aligns stylization with the text description. To this end, we also propose a novel and efficient adaLN based state space models that explore style-content fusion. Finally, we achieve a text-driven image style transfer. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods in artistic style transfer. More importantly, it does not require online fine-tuning and can render a 512x512 image in 0.03s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09566v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi-Song Liu, Li-Wen Wang, Jun Xiao, Vicky Kalogeiton</dc:creator>
    </item>
    <item>
      <title>Grand Challenges in Immersive Technologies for Cultural Heritage</title>
      <link>https://arxiv.org/abs/2412.02853</link>
      <description>arXiv:2412.02853v2 Announce Type: replace-cross 
Abstract: Cultural heritage, a testament to human history and civilization, has gained increasing recognition for its significance in preservation and dissemination. The integration of immersive technologies has transformed how cultural heritage is presented, enabling audiences to engage with it in more vivid, intuitive, and interactive ways. However, the adoption of these technologies also brings a range of challenges and potential risks. This paper presents a systematic review, with an in-depth analysis of 177 selected papers. We comprehensively examine and categorize current applications, technological approaches, and user devices in immersive cultural heritage presentations, while also highlighting the associated risks and challenges. Furthermore, we identify areas for future research in the immersive presentation of cultural heritage. Our goal is to provide a comprehensive reference for researchers and practitioners, enhancing understanding of the technological applications, risks, and challenges in this field, and encouraging further innovation and development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02853v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanbing Wang, Junyan Du, Yue Li, Lie Zhang, Xiang Li</dc:creator>
    </item>
  </channel>
</rss>
