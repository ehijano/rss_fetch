<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Sep 2024 04:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exploring the Boundaries of Content Moderation in Text-to-Image Generation</title>
      <link>https://arxiv.org/abs/2409.17155</link>
      <description>arXiv:2409.17155v1 Announce Type: new 
Abstract: This paper analyzes the community safety guidelines of five text-to-image (T2I) generation platforms and audits five T2I models, focusing on prompts related to the representation of humans in areas that might lead to societal stigma. While current research primarily focuses on ensuring safety by restricting the generation of harmful content, our study offers a complementary perspective. We argue that the concept of safety is difficult to define and operationalize, reflected in a discrepancy between the officially published safety guidelines and the actual behavior of the T2I models, and leading at times to over-censorship. Our findings call for more transparency and an inclusive dialogue about the platforms' content moderation practices, bearing in mind their global cultural and social impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17155v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Piera Riccio, Georgina Curto, Nuria Oliver</dc:creator>
    </item>
    <item>
      <title>Finite State Machine with Input and Process Render</title>
      <link>https://arxiv.org/abs/2409.17207</link>
      <description>arXiv:2409.17207v1 Announce Type: new 
Abstract: Finite State Machines are a concept widely taught in undergraduate theory of computing courses. Educators typically use tools with static representations of FSMs to help students visualize these objects and processes; however, all existing tools require manual editing by the instructor. In this poster, we created an automatic visualization tool for FSMs that generates videos of FSM simulation, named Finite State Machine with Input and Process Render (FSMIPR). Educators can input any formal definition of an FSM and an input string, and FSMIPR generates an accompanying video of its simulation. We believe that FSMIPR will be beneficial to students who learn difficult computer theory concepts. We conclude with future work currently in-progress with FSMIPR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17207v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sierra Zoe Bennett-Manke, Sebastian Neumann, Ryan E. Dougherty</dc:creator>
    </item>
    <item>
      <title>Data-Centric AI Governance: Addressing the Limitations of Model-Focused Policies</title>
      <link>https://arxiv.org/abs/2409.17216</link>
      <description>arXiv:2409.17216v1 Announce Type: new 
Abstract: Current regulations on powerful AI capabilities are narrowly focused on "foundation" or "frontier" models. However, these terms are vague and inconsistently defined, leading to an unstable foundation for governance efforts. Critically, policy debates often fail to consider the data used with these models, despite the clear link between data and model performance. Even (relatively) "small" models that fall outside the typical definitions of foundation and frontier models can achieve equivalent outcomes when exposed to sufficiently specific datasets. In this work, we illustrate the importance of considering dataset size and content as essential factors in assessing the risks posed by models both today and in the future. More broadly, we emphasize the risk posed by over-regulating reactively and provide a path towards careful, quantitative evaluation of capabilities that can lead to a simplified regulatory environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17216v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritwik Gupta, Leah Walker, Rodolfo Corona, Stephanie Fu, Suzanne Petryk, Janet Napolitano, Trevor Darrell, Andrew W. Reddie</dc:creator>
    </item>
    <item>
      <title>The Technology of Outrage: Bias in Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2409.17336</link>
      <description>arXiv:2409.17336v1 Announce Type: new 
Abstract: Artificial intelligence and machine learning are increasingly used to offload decision making from people. In the past, one of the rationales for this replacement was that machines, unlike people, can be fair and unbiased. Evidence suggests otherwise. We begin by entertaining the ideas that algorithms can replace people and that algorithms cannot be biased. Taken as axioms, these statements quickly lead to absurdity. Spurred on by this result, we investigate the slogans more closely and identify equivocation surrounding the word 'bias.' We diagnose three forms of outrage-intellectual, moral, and political-that are at play when people react emotionally to algorithmic bias. Then we suggest three practical approaches to addressing bias that the AI community could take, which include clarifying the language around bias, developing new auditing methods for intelligent systems, and building certain capabilities into these systems. We conclude by offering a moral regarding the conversations about algorithmic bias that may transfer to other areas of artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17336v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Will Bridewell, Paul F. Bello, Selmer Bringsjord</dc:creator>
    </item>
    <item>
      <title>Tesla's Autopilot: Ethics and Tragedy</title>
      <link>https://arxiv.org/abs/2409.17380</link>
      <description>arXiv:2409.17380v1 Announce Type: new 
Abstract: This case study delves into the ethical ramifications of an incident involving Tesla's Autopilot, emphasizing Tesla Motors' moral responsibility. Using a seven-step ethical decision-making process, it examines user behavior, system constraints, and regulatory implications. This incident prompts a broader evaluation of ethical challenges in the automotive industry's adoption of autonomous technologies, urging a reconsideration of industry norms and legal frameworks. The analysis offers a succinct exploration of ethical considerations in evolving technological landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17380v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aravinda Jatavallabha</dc:creator>
    </item>
    <item>
      <title>Sociotechnical Approach to Enterprise Generative Artificial Intelligence (E-GenAI)</title>
      <link>https://arxiv.org/abs/2409.17408</link>
      <description>arXiv:2409.17408v1 Announce Type: new 
Abstract: In this theoretical article, a sociotechnical approach is proposed to characterize. First, the business ecosystem, focusing on the relationships among Providers, Enterprise, and Customers through SCM, ERP, and CRM platforms to align: (1) Business Intelligence (BI), Fuzzy Logic (FL), and TRIZ (Theory of Inventive Problem Solving), through the OID model, and (2) Knowledge Management (KM) and Imperfect Knowledge Management (IKM), through the OIDK model. Second, the article explores the E-GenAI business ecosystem, which integrates GenAI-based platforms for SCM, ERP, and CRM with GenAI-based platforms for BI, FL, TRIZ, KM, and IKM, to align Large Language Models (LLMs) through the E-GenAI (OID) model. Finally, to understand the dynamics of LLMs, we utilize finite automata to model the relationships between Followers and Followees. This facilitates the construction of LLMs that can identify specific characteristics of users on a social media platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17408v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leoncio Jimenez, Francisco Venegas</dc:creator>
    </item>
    <item>
      <title>Crafting Synthetic Realities: Examining Visual Realism and Misinformation Potential of Photorealistic AI-Generated Images</title>
      <link>https://arxiv.org/abs/2409.17484</link>
      <description>arXiv:2409.17484v1 Announce Type: new 
Abstract: Advances in generative models have created Artificial Intelligence-Generated Images (AIGIs) nearly indistinguishable from real photographs. Leveraging a large corpus of 30,824 AIGIs collected from Instagram and Twitter, and combining quantitative content analysis with qualitative analysis, this study unpacks AI photorealism of AIGIs from four key dimensions, content, human, aesthetic, and production features. We find that photorealistic AIGIs often depict human figures, especially celebrities and politicians, with a high degree of surrealism and aesthetic professionalism, alongside a low degree of overt signals of AI production. This study is the first to empirically investigate photorealistic AIGIs across multiple platforms using a mixed-methods approach. Our findings provide important implications and insights for understanding visual misinformation and mitigating potential risks associated with photorealistic AIGIs. We also propose design recommendations to enhance the responsible use of AIGIs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17484v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiyao Peng, Yingdan Lu, Yilang Peng, Sijia Qian, Xinyi Liu, Cuihua Shen</dc:creator>
    </item>
    <item>
      <title>Estimating The Carbon Footprint Of Digital Agriculture Deployment: A Parametric Bottom-Up Modelling Approach</title>
      <link>https://arxiv.org/abs/2409.17617</link>
      <description>arXiv:2409.17617v1 Announce Type: new 
Abstract: Digitalization appears as a lever to enhance agriculture sustainability. However, existing works on digital agriculture's own sustainability remain scarce, disregarding the environmental effects of deploying digital devices on a large-scale. We propose a bottom-up method to estimate the carbon footprint of digital agriculture scenarios considering deployment of devices over a diversity of farm sizes. It is applied to two use-cases and demonstrates that digital agriculture encompasses a diversity of devices with heterogeneous carbon footprints and that more complex devices yield higher footprints not always compensated by better performances or scaling gains. By emphasizing the necessity of considering the multiplicity of devices, and the territorial distribution of farm sizes when modelling digital agriculture deployments, this study highlights the need for further exploration of the first-order effects of digital technologies in agriculture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17617v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre La Rocca (IUF, LaBRI, UB), Ga\"el Guennebaud (IUF, LaBRI, UB), Aur\'elie Bugeau (IUF, LaBRI, UB), Anne-Laure Ligozat (ENSIIE, LISN, STL)</dc:creator>
    </item>
    <item>
      <title>E-scooter effects on public transport demand: a case study in Santiago, Chile</title>
      <link>https://arxiv.org/abs/2409.17814</link>
      <description>arXiv:2409.17814v1 Announce Type: new 
Abstract: As cities adopt sustainable mobility solutions, electric scooters (e-scooters) offer both challenges and opportunities for public transportation systems. This study, the first in Latin America, examines the effects of e-scooters on public transport demand in Santiago, Chile, focusing on two scenarios: "generation" of trips (trips starting in study zones) and "attraction" of trips (trips ending in study zones). A negative binomial regression model was applied to data from public transport smart cards and e-scooter GPS. The methodology included urban area clustering and a differences-in-differences approach. The findings reveal significant regional differences: in the Central Region, public transport trips decreased by 21.38% in the generation scenario, while bus trips increased by 76.39%. In the Intermediate Region, metro trips increased by 70.05%, and in the Peripheral Region, bus trips increased by 84.64%. These results suggest that e-scooters reduce public transport usage in highly accessible areas but increase it in less accessible regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17814v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniela Opitz, Eduardo Graells-Garrido, Jacqueline Arriagada, Matilde Rivas, Natalia Meza</dc:creator>
    </item>
    <item>
      <title>Implementing a Nordic-Baltic Federated Health Data Network: a case report</title>
      <link>https://arxiv.org/abs/2409.17865</link>
      <description>arXiv:2409.17865v1 Announce Type: new 
Abstract: Background: Centralized collection and processing of healthcare data across national borders pose significant challenges, including privacy concerns, data heterogeneity and legal barriers. To address some of these challenges, we formed an interdisciplinary consortium to develop a feder-ated health data network, comprised of six institutions across five countries, to facilitate Nordic-Baltic cooperation on secondary use of health data. The objective of this report is to offer early insights into our experiences developing this network. Methods: We used a mixed-method ap-proach, combining both experimental design and implementation science to evaluate the factors affecting the implementation of our network. Results: Technically, our experiments indicate that the network functions without significant performance degradation compared to centralized simu-lation. Conclusion: While use of interdisciplinary approaches holds a potential to solve challeng-es associated with establishing such collaborative networks, our findings turn the spotlight on the uncertain regulatory landscape playing catch up and the significant operational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17865v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taridzo Chomutare, Aleksandar Babic, Laura-Maria Peltonen, Silja Elunurm, Peter Lundberg, Arne J\"onsson, Emma Eneling, Ciprian-Virgil Gerstenberger, Troels Siggaard, Raivo Kolde, Oskar Jerdhaf, Martin Hansson, Alexandra Makhlysheva, Miroslav Muzny, Erik Ylip\"a\"a, S{\o}ren Brunak, Hercules Dalianis</dc:creator>
    </item>
    <item>
      <title>Why Companies "Democratise" Artificial Intelligence: The Case of Open Source Software Donations</title>
      <link>https://arxiv.org/abs/2409.17876</link>
      <description>arXiv:2409.17876v1 Announce Type: new 
Abstract: Companies claim to "democratise" artificial intelligence (AI) when they donate AI open source software (OSS) to non-profit foundations or release AI models, among others, but what does this term mean and why do they do it? As the impact of AI on society and the economy grows, understanding the commercial incentives behind AI democratisation efforts is crucial for ensuring these efforts serve broader interests beyond commercial agendas. Towards this end, this study employs a mixed-methods approach to investigate commercial incentives for 43 AI OSS donations to the Linux Foundation. It makes contributions to both research and practice. It contributes a taxonomy of both individual and organisational social, economic, and technological incentives for AI democratisation. In particular, it highlights the role of democratising the governance and control rights of an OSS project (i.e., from one company to open governance) as a structural enabler for downstream goals, such as attracting external contributors, reducing development costs, and influencing industry standards, among others. Furthermore, OSS donations are often championed by individual developers within companies, highlighting the importance of the bottom-up incentives for AI democratisation. The taxonomy provides a framework and toolkit for discerning incentives for other AI democratisation efforts, such as the release of AI models. The paper concludes with a discussion of future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17876v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cailean Osborne</dc:creator>
    </item>
    <item>
      <title>A Policy Report Evaluating the National Assessment Program for Literacy and Numeracy (Naplan) Reform in Australia: The Impacts of High Stakes Assessment on Students</title>
      <link>https://arxiv.org/abs/2409.17959</link>
      <description>arXiv:2409.17959v1 Announce Type: new 
Abstract: The National Assessment Program for Literacy and Numeracy (NAPLAN) Reform in Australia, launched in 2008, has emerged as the country's most significant and contentious reform. However, due to its high-stakes nature and standardization, testing presents various challenges. These challenges include the combination of accountability with the 'My School' website, overlooking higher-order cognitive abilities, exacerbating students' anxiety and stress, and creating inequity for Language Background Other Than English (LBOTE) students. This report assesses the achievements and obstacles of the NAPLAN reform, proposing recommendations such as transitioning to online testing, enhancing content and platforms, increasing public assessment literacy, and investing more in LBOTE education. These suggestions aim to strike a balance between standardized testing and authentic educational pursuits, adapting to the evolving needs of students to create a fair, inclusive educational environment that addresses the demands of the 21st century.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17959v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Computer Science &amp; Information Technology (CS &amp; IT),ISSN : 2231 - 5403,Volume 14, Number 17, September 2024 link:https://airccse.org/csit/V14N17.html</arxiv:journal_reference>
      <dc:creator>Wenya Zhang</dc:creator>
    </item>
    <item>
      <title>Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models</title>
      <link>https://arxiv.org/abs/2409.17990</link>
      <description>arXiv:2409.17990v1 Announce Type: new 
Abstract: This paper proposes temporally aligned Large Language Models (LLMs) as a tool for longitudinal analysis of social media data. We fine-tune Temporal Adapters for Llama 3 8B on full timelines from a panel of British Twitter users, and extract longitudinal aggregates of emotions and attitudes with established questionnaires. We validate our estimates against representative British survey data and find strong positive, significant correlations for several collective emotions. The obtained estimates are robust across multiple training seeds and prompt formulations, and in line with collective emotions extracted using a traditional classification model trained on labeled data. To the best of our knowledge, this is the first work to extend the analysis of affect in LLMs to a longitudinal setting through Temporal Adapters. Our work enables new approaches towards the longitudinal analysis of social media data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17990v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georg Ahnert, Max Pellert, David Garcia, Markus Strohmaier</dc:creator>
    </item>
    <item>
      <title>Confident Teacher, Confident Student? A Novel User Study Design for Investigating the Didactic Potential of Explanations and their Impact on Uncertainty</title>
      <link>https://arxiv.org/abs/2409.17157</link>
      <description>arXiv:2409.17157v1 Announce Type: cross 
Abstract: Evaluating the quality of explanations in Explainable Artificial Intelligence (XAI) is to this day a challenging problem, with ongoing debate in the research community. While some advocate for establishing standardized offline metrics, others emphasize the importance of human-in-the-loop (HIL) evaluation. Here we propose an experimental design to evaluate the potential of XAI in human-AI collaborative settings as well as the potential of XAI for didactics. In a user study with 1200 participants we investigate the impact of explanations on human performance on a challenging visual task - annotation of biological species in complex taxonomies. Our results demonstrate the potential of XAI in complex visual annotation tasks: users become more accurate in their annotations and demonstrate less uncertainty with AI assistance. The increase in accuracy was, however, not significantly different when users were shown the mere prediction of the model compared to when also providing an explanation. We also find negative effects of explanations: users tend to replicate the model's predictions more often when shown explanations, even when those predictions are wrong. When evaluating the didactic effects of explanations in collaborative human-AI settings, we find that users' annotations are not significantly better after performing annotation with AI assistance. This suggests that explanations in visual human-AI collaboration do not appear to induce lasting learning effects. All code and experimental data can be found in our GitHub repository: https://github.com/TeodorChiaburu/beexplainable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17157v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Teodor Chiaburu, Frank Hau{\ss}er, Felix Bie{\ss}mann</dc:creator>
    </item>
    <item>
      <title>Don't Trust A Single Gerrymandering Metric</title>
      <link>https://arxiv.org/abs/2409.17186</link>
      <description>arXiv:2409.17186v1 Announce Type: cross 
Abstract: In recent years, in an effort to promote fairness in the election process, a wide variety of techniques and metrics have been proposed to determine whether a map is a partisan gerrymander. The most accessible measures, requiring easily obtained data, are metrics such as the Mean-Median Difference, Efficiency Gap, Declination, and GEO metric. But for most of these metrics, researchers have struggled to describe, given no additional information, how a value of that metric on a single map indicates the presence or absence of gerrymandering.
  Our main result is that each of these metrics is gameable when used as a single, isolated quantity to detect gerrymandering (or the lack thereof). That is, for each of the four metrics, we can find district plans for a given state with an extremely large number of Democratic-won (or Republican-won) districts while the metric value of that plan falls within a reasonable, predetermined bound. We do this by using a hill-climbing method to generate district plans that are constrained by the bounds on the metric but also maximize or nearly maximize the number of districts won by a party.
  In addition, extreme values of the Mean-Median Difference do not necessarily correspond to maps with an extreme number of districts won. Thus, the Mean- Median Difference metric is particularly misleading, as it cannot distinguish more extreme maps from less extreme maps. The other metrics are more nuanced, but when assessed on an ensemble, none perform substantially differently from simply measuring number of districts won by a fixed party.
  One clear consequence of these results is that they demonstrate the folly of specifying a priori bounds on a metric that a redistricting commission must meet in order to avoid gerrymandering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17186v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Ratliff, Stephanie Somersille, Ellen Veomett</dc:creator>
    </item>
    <item>
      <title>Plurals: A System for Guiding LLMs Via Simulated Social Ensembles</title>
      <link>https://arxiv.org/abs/2409.17213</link>
      <description>arXiv:2409.17213v1 Announce Type: cross 
Abstract: Recent debates raised concerns that language models may favor certain viewpoints. But what if the solution is not to aim for a 'view from nowhere' but rather to leverage different viewpoints? We introduce Plurals, a system and Python library for pluralistic AI deliberation. Plurals consists of Agents (LLMs, optionally with personas) which deliberate within customizable Structures, with Moderators overseeing deliberation. Plurals is a generator of simulated social ensembles. Plurals integrates with government datasets to create nationally representative personas, includes deliberation templates inspired by democratic deliberation theory, and allows users to customize both information-sharing structures and deliberation behavior within Structures. Six case studies demonstrate fidelity to theoretical constructs and efficacy. Three randomized experiments show simulated focus groups produced output resonant with an online sample of the relevant audiences (chosen over zero-shot generation in 75% of trials). Plurals is both a paradigm and a concrete system for pluralistic AI. The Plurals library is available at https://github.com/josh-ashkinaze/plurals and will be continually updated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17213v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Ashkinaze, Emily Fry, Narendra Edara, Eric Gilbert, Ceren Budak</dc:creator>
    </item>
    <item>
      <title>Design and development of desktop braille printing machine at Fablab Nepal</title>
      <link>https://arxiv.org/abs/2409.17272</link>
      <description>arXiv:2409.17272v1 Announce Type: cross 
Abstract: The development of a desktop Braille printing machine aims to create an affordable, user-friendly device for visually impaired users. This document outlines the entire process, from research and requirement analysis to distribution and support, leveraging the content and guidelines from the GitHub repository,https://github.com/fablabnepal1/Desktop-Braille-Printing-Machine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17272v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daya Bandhu Ghimire, Pallab Shrestha</dc:creator>
    </item>
    <item>
      <title>Democratizing Signal Processing and Machine Learning: Math Learning Equity for Elementary and Middle School Students</title>
      <link>https://arxiv.org/abs/2409.17304</link>
      <description>arXiv:2409.17304v1 Announce Type: cross 
Abstract: Signal Processing (SP) and Machine Learning (ML) rely on good math and coding knowledge, in particular, linear algebra, probability, and complex numbers. A good grasp of these relies on scalar algebra learned in middle school. The ability to understand and use scalar algebra well, in turn, relies on a good foundation in basic arithmetic. Because of various systemic barriers, many students are not able to build a strong foundation in arithmetic in elementary school. This leads them to struggle with algebra and everything after that. Since math learning is cumulative, the gap between those without a strong early foundation and everyone else keeps increasing over the school years and becomes difficult to fill in college. In this article we discuss how SP faculty and graduate students can play an important role in starting, and participating in, university-run (or other) out-of-school math support programs to supplement students' learning. Two example programs run by the authors (CyMath at ISU and Ab7G at Purdue) are briefly described. The second goal of this article is to use our perspective as SP, and engineering, educators who have seen the long-term impact of elementary school math teaching policies, to provide some simple almost zero cost suggestions that elementary schools could adopt to improve math learning: (i) more math practice in school, (ii) send small amounts of homework (individual work is critical in math), and (iii) parent awareness (math resources, need for early math foundation, clear in-school test information and sharing of feedback from the tests). In summary, good early math support (in school and through out-of-school programs) can help make SP and ML more accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17304v1</guid>
      <category>math.HO</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Namrata Vaswani, Mohamed Y. Selim, Renee Serrell Gibert</dc:creator>
    </item>
    <item>
      <title>The Interplay of Computing, Ethics, and Policy in Brain-Computer Interface Design</title>
      <link>https://arxiv.org/abs/2409.17445</link>
      <description>arXiv:2409.17445v1 Announce Type: cross 
Abstract: Brain-computer interfaces (BCIs) connect biological neurons in the brain with external systems like prosthetics and computers. They are increasingly incorporating processing capabilities to analyze and stimulate neural activity, and consequently, pose unique design challenges related to ethics, law, and policy. For the first time, this paper articulates how ethical, legal, and policy considerations can shape BCI architecture design, and how the decisions that architects make constrain or expand the ethical, legal, and policy frameworks that can be applied to them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17445v1</guid>
      <category>cs.AR</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>The 1st Workshop on Hot Topics in Ethical Computer Systems, April, 2024</arxiv:journal_reference>
      <dc:creator>Muhammed Ugur, Raghavendra Pradyumna Pothukuchi, Abhishek Bhattacharjee</dc:creator>
    </item>
    <item>
      <title>Intervention strategies for misinformation sharing on social media: A bibliometric analysis</title>
      <link>https://arxiv.org/abs/2409.17637</link>
      <description>arXiv:2409.17637v1 Announce Type: cross 
Abstract: Widely distributed misinformation shared across social media channels is a pressing issue that poses a significant threat to many aspects of society's well-being. Inaccurate shared information causes confusion, can adversely affect mental health, and can lead to mis-informed decision-making. Therefore, it is important to implement proactive measures to intervene and curb the spread of misinformation where possible. This has prompted scholars to investigate a variety of intervention strategies for misinformation sharing on social media. This study explores the typology of intervention strategies for addressing misinformation sharing on social media, identifying 4 important clusters - cognition-based, automated-based, information-based, and hybrid-based. The literature selection process utilized the PRISMA method to ensure a systematic and comprehensive analysis of relevant literature while maintaining transparency and reproducibility. A total of 139 articles published from 2013-2023 were then analyzed. Meanwhile, bibliometric analyses were conducted using performance analysis and science mapping techniques for the typology development. A comparative analysis of the typology was conducted to reveal patterns and evolution in the field. This provides valuable insights for both theory and practical applications. Overall, the study concludes that scholarly contributions to scientific research and publication help to address research gaps and expand knowledge in this field. Understanding the evolution of intervention strategies for misinformation sharing on social media can support future research that contributes to the development of more effective and sustainable solutions to this persistent problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17637v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2024.3469248</arxiv:DOI>
      <dc:creator>Juanita Zainudin, Nazlena Mohamad Ali, Alan F. Smeaton, Mohamad Taha Ijab</dc:creator>
    </item>
    <item>
      <title>AI Delegates with a Dual Focus: Ensuring Privacy and Strategic Self-Disclosure</title>
      <link>https://arxiv.org/abs/2409.17642</link>
      <description>arXiv:2409.17642v1 Announce Type: cross 
Abstract: Large language model (LLM)-based AI delegates are increasingly utilized to act on behalf of users, assisting them with a wide range of tasks through conversational interfaces. Despite their advantages, concerns arise regarding the potential risk of privacy leaks, particularly in scenarios involving social interactions. While existing research has focused on protecting privacy by limiting the access of AI delegates to sensitive user information, many social scenarios require disclosing private details to achieve desired outcomes, necessitating a balance between privacy protection and disclosure. To address this challenge, we conduct a pilot study to investigate user preferences for AI delegates across various social relations and task scenarios, and then propose a novel AI delegate system that enables privacy-conscious self-disclosure. Our user study demonstrates that the proposed AI delegate strategically protects privacy, pioneering its use in diverse and dynamic social interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17642v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Chen, Zhiyang Zhang, Fangkai Yang, Xiaoting Qin, Chao Du, Xi Cheng, Hangxin Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</dc:creator>
    </item>
    <item>
      <title>Computation Pre-Offloading for MEC-Enabled Vehicular Networks via Trajectory Prediction</title>
      <link>https://arxiv.org/abs/2409.17681</link>
      <description>arXiv:2409.17681v1 Announce Type: cross 
Abstract: Task offloading is of paramount importance to efficiently orchestrate vehicular wireless networks, necessitating the availability of information regarding the current network status and computational resources. However, due to the mobility of the vehicles and the limited computational resources for performing task offloading in near-real-time, such schemes may require high latency, thus, become even infeasible. To address this issue, in this paper, we present a Trajectory Prediction-based Pre-offloading Decision (TPPD) algorithm for analyzing the historical trajectories of vehicles to predict their future coordinates, thereby allowing for computational resource allocation in advance. We first utilize the Long Short-Term Memory (LSTM) network model to predict each vehicle's movement trajectory. Then, based on the task requirements and the predicted trajectories, we devise a dynamic resource allocation algorithm using a Double Deep Q-Network (DDQN) that enables the edge server to minimize task processing delay, while ensuring effective utilization of the available computational resources. Our simulation results verify the effectiveness of the proposed approach, showcasing that, as compared with traditional real-time task offloading strategies, the proposed TPPD algorithm significantly reduces task processing delay while improving resource utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17681v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting Zhang, Bo Yang, Zhiwen Yu, Xuelin Cao, George C. Alexandropoulos, Yan Zhang, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>Participatory design: A systematic review and insights for future practice</title>
      <link>https://arxiv.org/abs/2409.17952</link>
      <description>arXiv:2409.17952v1 Announce Type: cross 
Abstract: Participatory Design -- an iterative, flexible design process that uses the close involvement of stakeholders, most often end users -- is growing in use across design disciplines. As an increasing number of practitioners turn to Participatory Design (PD), it has become less rigidly defined, with stakeholders engaged to varying degrees through the use of disjointed techniques. This ambiguous understanding can be counterproductive when discussing PD processes. Our findings synthesize key decisions and approaches from design peers that can support others in engaging in PD practice. We investigated how scholars report the use of Participatory Design in the field through a systematic literature review. We found that a majority of PD literature examined specific case studies of PD (53 of 88 articles), with the design of intangible systems representing the most common design context (61 of 88 articles). Stakeholders most often participated throughout multiple stages of a design process (65 of 88 articles), recruited in a variety of ways and engaged in several of the 14 specific participatory techniques identified. This systematic review provides today's practitioners synthesized learnings from past Participatory Design processes to inform and improve future use of PD, attempting to remedy inequitable design by engaging directly with stakeholders and users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17952v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Wacnik, Shanna Daly, Aditi Verma</dc:creator>
    </item>
    <item>
      <title>Effect of electric vehicles, heat pumps, and solar panels on low-voltage feeders: Evidence from smart meter profiles</title>
      <link>https://arxiv.org/abs/2409.18105</link>
      <description>arXiv:2409.18105v1 Announce Type: cross 
Abstract: Electric vehicles (EVs), heat pumps (HPs) and solar panels are low-carbon technologies (LCTs) that are being connected to the low-voltage grid (LVG) at a rapid pace. One of the main hurdles to understand their impact on the LVG is the lack of recent, large electricity consumption datasets, measured in real-world conditions. We investigated the contribution of LCTs to the size and timing of peaks on LV feeders by using a large dataset of 42,089 smart meter profiles of residential LVG customers. These profiles were measured in 2022 by Fluvius, the distribution system operator (DSO) of Flanders, Belgium. The dataset contains customers that proactively requested higher-resolution smart metering data, and hence is biased towards energy-interested people. LV feeders of different sizes were statistically modelled with a profile sampling approach. For feeders with 40 connections, we found a contribution to the feeder peak of 1.2 kW for a HP, 1.4 kW for an EV and 2.0 kW for an EV charging faster than 6.5 kW. A visual analysis of the feeder-level loads shows that the classical duck curve is replaced by a night-camel curve for feeders with only HPs and a night-dromedary curve for feeders with only EVs charging faster than 6.5 kW. Consumption patterns will continue to change as the energy transition is carried out, because of e.g. dynamic electricity tariffs or increased battery capacities. Our introduced methods are simple to implement, making it a useful tool for DSOs that have access to smart meter data to monitor changing consumption patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18105v1</guid>
      <category>eess.SY</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. Becker, R. Smet, B. Macharis, K. Vanthournout</dc:creator>
    </item>
    <item>
      <title>Human participants in AI research: Ethics and transparency in practice</title>
      <link>https://arxiv.org/abs/2311.01254</link>
      <description>arXiv:2311.01254v3 Announce Type: replace 
Abstract: In recent years, research involving human participants has been critical to advances in artificial intelligence (AI) and machine learning (ML), particularly in the areas of conversational, human-compatible, and cooperative AI. For example, roughly 9% of publications at recent AAAI and NeurIPS conferences indicate the collection of original human data. Yet AI and ML researchers lack guidelines for ethical research practices with human participants. Fewer than one out of every four of these AAAI and NeurIPS papers confirm independent ethical review, the collection of informed consent, or participant compensation. This paper aims to bridge this gap by examining the normative similarities and differences between AI research and related fields that involve human participants. Though psychology, human-computer interaction, and other adjacent fields offer historic lessons and helpful insights, AI research presents several distinct considerations$\unicode{x2014}$namely, participatory design, crowdsourced dataset development, and an expansive role of corporations$\unicode{x2014}$that necessitate a contextual ethics framework. To address these concerns, this manuscript outlines a set of guidelines for ethical and transparent practice with human participants in AI and ML research. Overall, this paper seeks to equip technical researchers with practical knowledge for their work, and to position them for further dialogue with social scientists, behavioral researchers, and ethicists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01254v3</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TTS.2024.3446183</arxiv:DOI>
      <dc:creator>Kevin R. McKee</dc:creator>
    </item>
    <item>
      <title>AI-enhanced Collective Intelligence</title>
      <link>https://arxiv.org/abs/2403.10433</link>
      <description>arXiv:2403.10433v4 Announce Type: replace 
Abstract: Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents' diversity and interactions influence the system's collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10433v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Cui, Taha Yasseri</dc:creator>
    </item>
    <item>
      <title>Monitoring Human Dependence On AI Systems With Reliance Drills</title>
      <link>https://arxiv.org/abs/2409.14055</link>
      <description>arXiv:2409.14055v2 Announce Type: replace 
Abstract: AI systems are assisting humans with an increasingly broad range of intellectual tasks. Humans could be over-reliant on this assistance if they trust AI-generated advice, even though they would make a better decision on their own. To identify real-world instances of over-reliance, this paper proposes the reliance drill: an exercise that tests whether a human can recognise mistakes in AI-generated advice. We introduce a pipeline that organisations could use to implement these drills. As an example, we explain how this approach could be used to limit over-reliance on AI in a medical setting. We conclude by arguing that reliance drills could become a key tool for ensuring humans remain appropriately involved in AI-assisted decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14055v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rosco Hunter, Richard Moulange, Jamie Bernardi, Merlin Stein</dc:creator>
    </item>
    <item>
      <title>Asking an AI for salary negotiation advice is a matter of concern: Controlled experimental perturbation of ChatGPT for protected and non-protected group discrimination on a contextual task with no clear ground truth answers</title>
      <link>https://arxiv.org/abs/2409.15567</link>
      <description>arXiv:2409.15567v2 Announce Type: replace 
Abstract: We conducted controlled experimental bias audits for four versions of ChatGPT, which we asked to recommend an opening offer in salary negotiations for a new hire. We submitted 98,800 prompts to each version, systematically varying the employee's gender, university, and major, and tested prompts in voice of each side of the negotiation: the employee versus employer. We find ChatGPT as a multi-model platform is not robust and consistent enough to be trusted for such a task. We observed statistically significant salary offers when varying gender for all four models, although with smaller gaps than for other attributes tested. The largest gaps were different model versions and between the employee- vs employer-voiced prompts. We also observed substantial gaps when varying university and major, but many of the biases were not consistent across model versions. We tested for fictional and fraudulent universities and found wildly inconsistent results across cases and model versions. We make broader contributions to the AI/ML fairness literature. Our scenario and our experimental design differ from mainstream AI/ML auditing efforts in key ways. Bias audits typically test discrimination for protected classes like gender, which we contrast with testing non-protected classes of university and major. Asking for negotiation advice includes how aggressive one ought to be in a negotiation relative to known empirical salary distributions and scales, which is a deeply contextual and personalized task that has no objective ground truth to validate. These results raise concerns for the specific model versions we tested and ChatGPT as a multi-model platform in continuous development. Our epistemology does not permit us to definitively certify these models as either generally biased or unbiased on the attributes we test, but our study raises matters of concern for stakeholders to further investigate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15567v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>R. Stuart Geiger, Flynn O'Sullivan, Elsie Wang, Jonathan Lo</dc:creator>
    </item>
    <item>
      <title>Empowering Agrifood System with Artificial Intelligence: A Survey of the Progress, Challenges and Opportunities</title>
      <link>https://arxiv.org/abs/2305.01899</link>
      <description>arXiv:2305.01899v2 Announce Type: replace-cross 
Abstract: With the world population rapidly increasing, transforming our agrifood systems to be more productive, efficient, safe, and sustainable is crucial to mitigate potential food shortages. Recently, artificial intelligence (AI) techniques such as deep learning (DL) have demonstrated their strong abilities in various areas, including language, vision, remote sensing (RS), and agrifood systems applications. However, the overall impact of AI on agrifood systems remains unclear. In this paper, we thoroughly review how AI techniques can transform agrifood systems and contribute to the modern agrifood industry. Firstly, we summarize the data acquisition methods in agrifood systems, including acquisition, storage, and processing techniques. Secondly, we present a progress review of AI methods in agrifood systems, specifically in agriculture, animal husbandry, and fishery, covering topics such as agrifood classification, growth monitoring, yield prediction, and quality assessment. Furthermore, we highlight potential challenges and promising research opportunities for transforming modern agrifood systems with AI. We hope this survey could offer an overall picture to newcomers in the field and serve as a starting point for their further research. The project website is https://github.com/Frenkie14/Agrifood-Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.01899v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>eess.IV</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Chen, Liang Lv, Di Wang, Jing Zhang, Yue Yang, Zeyang Zhao, Chen Wang, Xiaowei Guo, Hao Chen, Qingye Wang, Yufei Xu, Qiming Zhang, Bo Du, Liangpei Zhang, Dacheng Tao</dc:creator>
    </item>
    <item>
      <title>Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI Alignment in the Writing Process through Edits</title>
      <link>https://arxiv.org/abs/2409.14509</link>
      <description>arXiv:2409.14509v3 Announce Type: replace-cross 
Abstract: LLM-based applications are helping people write, and LLM-generated text is making its way into social media, journalism, and our classrooms. However, the differences between LLM-generated and human-written text remain unclear. To explore this, we hired professional writers to edit paragraphs in several creative domains. We first found these writers agree on undesirable idiosyncrasies in LLM-generated text, formalizing it into a seven-category taxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP corpus: 1,057 LLM-generated paragraphs edited by professional writers according to our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our study (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms of writing quality, revealing common limitations across model families. Third, we explored automatic editing methods to improve LLM-generated text. A large-scale preference annotation confirms that although experts largely prefer text edited by other experts, automatic editing methods show promise in improving alignment between LLM-generated and human-written text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14509v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu</dc:creator>
    </item>
  </channel>
</rss>
