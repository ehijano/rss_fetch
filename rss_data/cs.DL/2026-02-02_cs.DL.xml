<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Feb 2026 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Constructing BERT Models: How Team Dynamics and Focus Shape AI Model Impact</title>
      <link>https://arxiv.org/abs/2601.22505</link>
      <description>arXiv:2601.22505v1 Announce Type: new 
Abstract: The rapid evolution of AI technologies, exemplified by BERT-family models, has transformed scientific research, yet little is known about their production and recognition dynamics in the scientific system. This study investigates the development and impact of BERT-family models, focusing on team size, topic specialization, and citation patterns behind the models. Using a dataset of 4,208 BERT-related papers from the Papers with Code (PWC) dataset, we analyze how the BERT-family models evolve across methodological generations and how the newness of models is correlated with their production and recognition. Our findings reveal that newer BERT models are developed by larger, more experienced, and institutionally diverse teams, reflecting the increasing complexity of AI research. Additionally, these models exhibit greater topical specialization, targeting niche applications, which aligns with broader trends in scientific specialization. However, newer models receive fewer citations, particularly over the long term, suggesting a "first-mover advantage," where early models like BERT garner disproportionate recognition. These insights highlight the need for equitable evaluation frameworks that value both foundational and incremental innovations. This study underscores the evolving interplay between collaboration, specialization, and recognition in AI research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22505v1</guid>
      <category>cs.DL</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Likun Cao, Kai Li</dc:creator>
    </item>
    <item>
      <title>What Lies Beneath: A Call for Distribution-based Visual Question &amp; Answer Datasets</title>
      <link>https://arxiv.org/abs/2601.22218</link>
      <description>arXiv:2601.22218v1 Announce Type: cross 
Abstract: Visual Question Answering (VQA) has become an important benchmark for assessing how large multimodal models (LMMs) interpret images. However, most VQA datasets focus on real-world images or simple diagrammatic analysis, with few focused on interpreting complex scientific charts. Indeed, many VQA datasets that analyze charts do not contain the underlying data behind those charts or assume a 1-to-1 correspondence between chart marks and underlying data. In reality, charts are transformations (i.e. analysis, simplification, modification) of data. This distinction introduces a reasoning challenge in VQA that the current datasets do not capture. In this paper, we argue for a dedicated VQA benchmark for scientific charts where there is no 1-to-1 correspondence between chart marks and underlying data. To do so, we survey existing VQA datasets and highlight limitations of the current field. We then generate synthetic histogram charts based on ground truth data, and ask both humans and a large reasoning model questions where precise answers depend on access to the underlying data. We release the open-source dataset, including figures, underlying data, distribution parameters used to generate the data, and bounding boxes for all figure marks and text for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22218v1</guid>
      <category>cs.CV</category>
      <category>cs.DL</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jill P. Naiman, Daniel J. Evans, JooYoung Seo</dc:creator>
    </item>
    <item>
      <title>BibAgent: An Agentic Framework for Traceable Miscitation Detection in Scientific Literature</title>
      <link>https://arxiv.org/abs/2601.16993</link>
      <description>arXiv:2601.16993v2 Announce Type: replace 
Abstract: Citations are the bedrock of scientific authority, yet their integrity is compromised by widespread miscitations: ranging from nuanced distortions to fabricated references. Systematic citation verification is currently unfeasible; manual review cannot scale to modern publishing volumes, while existing automated tools are restricted by abstract-only analysis or small-scale, domain-specific datasets in part due to the "paywall barrier" of full-text access. We introduce BibAgent, a scalable, end-to-end agentic framework for automated citation verification. BibAgent integrates retrieval, reasoning, and adaptive evidence aggregation, applying distinct strategies for accessible and paywalled sources. For paywalled references, it leverages a novel Evidence Committee mechanism that infers citation validity via downstream citation consensus. To support systematic evaluation, we contribute a 5-category Miscitation Taxonomy and MisciteBench, a massive cross-disciplinary benchmark comprising 6,350 miscitation samples spanning 254 fields. Our results demonstrate that BibAgent outperforms state-of-the-art Large Language Model (LLM) baselines in citation verification accuracy and interpretability, providing scalable, transparent detection of citation misalignments across the scientific literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16993v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Peiran Li, Fangzhou Lin, Shuo Xing, Xiang Zheng, Xi Hong, Siyuan Yang, Jiashuo Sun, Zhengzhong Tu, Chaoqun Ni</dc:creator>
    </item>
  </channel>
</rss>
