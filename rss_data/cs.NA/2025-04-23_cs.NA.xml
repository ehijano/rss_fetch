<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Apr 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Randomized Proper Orthogonal Decomposition for data-driven reduced order modeling of a two-layer quasi-geostrophic ocean model</title>
      <link>https://arxiv.org/abs/2504.15350</link>
      <description>arXiv:2504.15350v1 Announce Type: new 
Abstract: The two-layer quasi-geostrophic equations (2QGE) serve as a simplified model for simulating wind-driven, stratified ocean flows. However, their numerical simulation remains computationally expensive due to the need for high-resolution meshes to capture a wide range of turbulent scales. This becomes especially problematic when several simulations need to be run because of, e.g., uncertainty in the parameter settings. To address this challenge, we propose a data-driven reduced order model (ROM) for the 2QGE that leverages randomized proper orthogonal decomposition (rPOD) and long short-term memory (LSTM) networks. To efficiently generate the snapshot data required for model construction, we apply a nonlinear filtering stabilization technique that allows for the use of larger mesh sizes compared to a direct numerical simulations (DNS). Thanks to the use of rPOD to extract the dominant modes from the snapshot matrices, we achieve up to 700 times speedup over the use of deterministic POD. LSTM networks are trained with the modal coefficients associated with the snapshots to enable the prediction of the time- and parameter-dependent modal coefficients during the online phase, which is hundreds of thousands of time faster than a DNS. We assess the accuracy and efficiency of our rPOD-LSTM ROM through an extension of a well-known benchmark called double-gyre wind forcing test. The dimension of the parameter space in this test is increased from two to four.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15350v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lander Besabe, Michele Girfoglio, Annalisa Quaini, Gianluigi Rozza</dc:creator>
    </item>
    <item>
      <title>Convergence-rate and error analysis of sectional-volume average method for the collisional breakage equation with multi-dimensional modelling</title>
      <link>https://arxiv.org/abs/2504.15365</link>
      <description>arXiv:2504.15365v1 Announce Type: new 
Abstract: Recent literature reports two sectional techniques, the finite volume method [Das et al., 2020, SIAM J. Sci. Comput., 42(6): B1570-B1598] and the fixed pivot technique [Kushwah et al., 2023, Commun. Nonlinear Sci. Numer. Simul., 121(37): 107244] to solve one-dimensional collision-induced nonlinear particle breakage equation. It is observed that both the methods become inconsistent over random grids. Therefore, we propose a new birth modification strategy, where the newly born particles are proportionately allocated in three adjacent cells, depending upon the average volume in each cell. This modification technique improves the numerical model by making it consistent over random grids. A detailed convergence and error analysis for this new scheme is studied over different possible choices of grids such as uniform, nonuniform, locally-uniform, random and oscillatory grids. In addition, we have also identified the conditions upon kernels for which the convergence rate increases significantly and the scheme achieves second order of convergence over uniform, nonuniform and locally-uniform grids. The enhanced order of accuracy will enable the new model to be easily coupled with CFD-modules. Another significant advancement in the literature is done by extending the discrete model for two-dimensional equation over rectangular grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15365v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Prakrati Kushwah, Anupama Ghorai, Jitraj Saha</dc:creator>
    </item>
    <item>
      <title>On optimality and bounds for internal solutions generated from boundary data-driven Gramians</title>
      <link>https://arxiv.org/abs/2504.15407</link>
      <description>arXiv:2504.15407v1 Announce Type: new 
Abstract: We consider the computation of internal solutions for a time domain plasma wave equation with unknown coefficients from the data obtained by sampling its transfer function at the boundary. The computation is performed by transforming known background snapshots using the Cholesky decomposition of the data-driven Gramian. We show that this approximation is asymptotically close to the projection of the true internal solution onto the subspace of background snapshots. This allows us to derive a generally applicable bound for the error in the approximation of internal fields from boundary data for a time domain plasma wave equation with an unknown potential $q$. For general $q\in L^\infty$, we prove convergence of these data generated internal fields in one dimension for two examples. The first is for piecewise constant initial data and sampling $\tau$ equal to the pulse width. The second is piecewise linear initial data and sampling at half the pulse width. We show that in both cases the data generated solutions converge in $L^2$ at order $\sqrt{\tau}$. We present numerical experiments validating the result and the sharpness of this convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15407v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V. Druskin, S. Moskow, M. Zaslavsky</dc:creator>
    </item>
    <item>
      <title>Derivatives of tree tensor networks and its applications in Runge--Kutta methods</title>
      <link>https://arxiv.org/abs/2504.15516</link>
      <description>arXiv:2504.15516v1 Announce Type: new 
Abstract: Tree tensor networks (TTNs) provide a compact and structured representation of high-dimensional data, making them valuable in various areas of computational mathematics and physics. In this paper, we present a rigorous mathematical framework for expressing high-order derivatives of functional TTNs, both with or without constraints. Our framework decomposes the total derivative of a given TTN into a summation of TTNs, each corresponding to the partial derivatives of the original TTN. Using this decomposition, we derive the Taylor expansion of vector-valued functions subject to ordinary differential equation constraints or algebraic constraints imposed by Runge--Kutta (RK) methods. As a concrete application, we employ this framework to construct order conditions for RK methods. Due to the intrinsic tensor properties of partial derivatives and the separable tensor structure in RK methods, the Taylor expansion of numerical solutions can be obtained in a manner analogous to that of exact solutions using tensor operators. This enables the order conditions of RK methods to be established by directly comparing the Taylor expansions of the exact and numerical solutions, eliminating the need for mathematical induction. For a given function $\vector{f}$, we derive sharper order conditions that go beyond the classical ones, enabling the identification of situations where a standard RK scheme of order {\it p} achieves unexpectedly higher convergence order for the particular function. These results establish new connections between tensor network theory and classical numerical methods, potentially opening new avenues for both analytical exploration and practical computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15516v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyuan He, Zhonghao Sun, Jizu Huang</dc:creator>
    </item>
    <item>
      <title>Minimization of Curve Length through Energy Minimization using Finite Difference and Numerical Integration in Real Coordinate Space</title>
      <link>https://arxiv.org/abs/2504.15566</link>
      <description>arXiv:2504.15566v1 Announce Type: new 
Abstract: The problem of determining the minimal length is garnering attention in various fields such as computer vision, robotics, and machine learning. One solution to this problem involves linearly interpolating the solution of a nonlinear optimization problem that approximates the curve's energy minimization problem using finite differences and numerical integration. This method tends to be easier to implement compared to others. However, it was previously unknown whether this approach successfully minimizes the curve's length under the Riemannian metric in real coordinate spaces. In this paper, we prove that the length of a curve obtained by linear interpolation of the solution to an optimization problem, where the energy of the curve is approximated using finite differences and the trapezoidal rule, converges to the minimal curve length at a rate of $1/2$ in terms of the number of points used in the numerical integration. Similarly, we prove that when using the left-point rule, the approximated curve's length likewise converges to the minimal curve length at a rate of $1/2$ in terms of the number of points used in the numerical integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15566v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.OC</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Akira Kitaoka</dc:creator>
    </item>
    <item>
      <title>Operator Inference for Elliptic Eigenvalue Problems</title>
      <link>https://arxiv.org/abs/2504.15733</link>
      <description>arXiv:2504.15733v1 Announce Type: new 
Abstract: Eigenvalue problems for elliptic operators play an important role in science and engineering applications, where efficient and accurate numerical computation is essential. In this work, we propose a novel operator inference approach for elliptic eigenvalue problems based on neural network approximations that directly maps computational domains to their associated eigenvalues and eigenfunctions. Motivated by existing neural network architectures and the mathematical characteristics of eigenvalue problems, we represent computational domains as pixelated images and decompose the task into two subtasks: eigenvalue prediction and eigenfunction prediction. For the eigenvalue prediction, we design a convolutional neural network (CNN), while for the eigenfunction prediction, we employ a Fourier Neural Operator (FNO). Additionally, we introduce a critical preprocessing module that integrates domain scaling, detailed boundary pixelization, and main-axis alignment. This preprocessing step not only simplifies the learning task but also enhances the performance of the neural networks. Finally, we present numerical results to demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15733v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoqian Li, Jiguang Sun, Zhiwen Zhang</dc:creator>
    </item>
    <item>
      <title>Multilevel lattice-based kernel approximation for elliptic PDEs with random coefficients</title>
      <link>https://arxiv.org/abs/2504.15810</link>
      <description>arXiv:2504.15810v1 Announce Type: new 
Abstract: This paper introduces a multilevel kernel-based approximation method to estimate efficiently solutions to elliptic partial differential equations (PDEs) with periodic random coefficients. Building upon the work of Kaarnioja, Kazashi, Kuo, Nobile, Sloan (Numer. Math., 2022) on kernel interpolation with quasi-Monte Carlo (QMC) lattice point sets, we leverage multilevel techniques to enhance computational efficiency while maintaining a given level of accuracy. In the function space setting with product-type weight parameters, the single-level approximation can achieve an accuracy of $\varepsilon&gt;0$ with cost $\mathcal{O}(\varepsilon^{-\eta-\nu-\theta})$ for positive constants $\eta, \nu, \theta $ depending on the rates of convergence associated with dimension truncation, kernel approximation, and finite element approximation, respectively. Our multilevel approximation can achieve the same $\varepsilon$ accuracy at a reduced cost $\mathcal{O}(\varepsilon^{-\eta-\max(\nu,\theta)})$. Full regularity theory and error analysis are provided, followed by numerical experiments that validate the efficacy of the proposed multilevel approximation in comparison to the single-level approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15810v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander D. Gilbert, Michael B. Giles, Frances Y. Kuo, Ian H. Sloan, Abirami Srikumar</dc:creator>
    </item>
    <item>
      <title>Toward optimal-scaling DFT: stochastic Hartree theory in the thermodynamic and complete basis set limits at arbitrary temperature</title>
      <link>https://arxiv.org/abs/2504.15816</link>
      <description>arXiv:2504.15816v1 Announce Type: new 
Abstract: We present the first mathematical analysis of stochastic density functional theory (DFT) in the context of the Hartree approximation. We motivate our analysis via the notion of nearly-optimal or $\tilde{O}(n)$ scaling with respect to the number $n$ of computational degrees of freedom, independent of the number of electrons, in both the thermodynamic and complete basis set limits. Indeed, the promise of such scaling is the primary motivation for stochastic DFT relative to conventional orbital-based approaches, as well as deterministic orbital-free alternatives. We highlight three key targets for mathematical attention, which are synthesized in our algorithm and analysis. First, we identify a particular stochastic estimator for the Hartree potential whose sample complexity is essentially independent of the discretization size. Second, we reformulate the self-consistent field iteration as a stochastic mirror descent method where the Fermi-Dirac entropy plays the role of the Bregman potential, and we prove a nearly discretization-independent bound on the number of iterations needed to reach fixed accuracy. Third, motivated by the estimator, we introduce a novel pole expansion scheme for the square-root Fermi-Dirac operator, preserving $\tilde{O}(n)$ cost per mirror descent iteration even in the complete basis set limit. Combining these ingredients, we establish nearly-optimal scaling in both limits of interest under reasonable assumptions on the basis sets chosen for discretization. Extensive numerical experiments on problems with as many as $10^{6}$ degrees of freedom validate our algorithm and support the theory of nearly-optimal scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15816v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhang Cai, Michael Lindsey</dc:creator>
    </item>
    <item>
      <title>Understanding the Role of Covariates in Numerical Reconstructions of Real-World Vehicle-to-Pedestrian Collisions</title>
      <link>https://arxiv.org/abs/2504.15951</link>
      <description>arXiv:2504.15951v1 Announce Type: new 
Abstract: Traumatic Brain Injuries (TBIs) are a pressing global public health issue, impacting tens of millions of individuals annually. Vulnerable road users (VRUs), such as pedestrians, are vastly overrepresented in the worldwide TBI statistics. To evaluate the effectiveness of injury prevention measures, researchers often employ Finite Element (FE) models of the human body to virtually simulate the human response to impact in real-world road traffic accident scenarios. However, VRU accidents occur in a highly uncontrolled environment and, in consequence, there is a large amount of variables (covariates), e.g. the vehicle impact speed and VRU body posture, that together dictate the injurious outcome of the collision. At the same time, since FE analysis is a computationally heavy task, researchers often need to apply extensive simplifications to FE models when attempting to predict real-world VRU head trauma. To help researchers make informed decisions when conducting FE accident reconstructions, this literature review aims to create an overarching summary of covariates that have been reported influential in literature. The review provides researchers with an overview of variables proven to have an influence on head injury predictions. The material could potentially be useful as a basis for choosing parameters to include when performing sensitivity analyses of car-to-pedestrian impact simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15951v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia Lindgren, Svein Kleiven, Xiaogai Li</dc:creator>
    </item>
    <item>
      <title>High order treatment of moving curved boundaries: Arbitrary-Lagrangian-Eulerian methods with a shifted boundary polynomials correction</title>
      <link>https://arxiv.org/abs/2504.15963</link>
      <description>arXiv:2504.15963v1 Announce Type: new 
Abstract: In this paper we present a novel approach for the prescription of high order boundary conditions when approximating the solution of the Euler equations for compressible gas dynamics on curved moving domains. When dealing with curved boundaries, the consistency of boundary conditions is a real challenge, and it becomes even more challenging in the context of moving domains discretized with high order Arbitrary-Lagrangian-Eulerian (ALE) schemes. The ALE formulation is particularly well-suited for handling moving and deforming domains, thus allowing for the simulation of complex fluid-structure interaction problems. However, if not properly treated, the imposition of boundary conditions can lead to significant errors in the numerical solution, which can spoil the high order discretization of the underlying mathematical model. In order to tackle this issue, we propose a new method based on the recently developed shifted boundary polynomial correction, which was originally proposed on fixed meshes. The new method is integrated into the space-time corrector step of a direct ALE finite volume method to account for the local curvature of the moving boundary by only exploiting the high order reconstruction polynomial of the finite volume control volume. It relies on a correction based on the extrapolated value of the cell polynomial evaluated at the true geometry, thus not requiring the explicit evaluation of high order Taylor series. This greatly simplifies the treatment of moving curved boundaries, as it allows for the use of standard simplicial meshes, which are much easier to generate and move than curvilinear ones, especially for 3D time-dependent problems. Several numerical experiments are presented demonstrating the high order convergence properties of the new method in the context of compressible flows in moving curved domains, which remain approximated by piecewise linear elements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15963v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Walter Boscheri, Mirco Ciallella</dc:creator>
    </item>
    <item>
      <title>Interpolation error analysis using a new geometric parameter</title>
      <link>https://arxiv.org/abs/2504.16012</link>
      <description>arXiv:2504.16012v1 Announce Type: new 
Abstract: This article presents novel proof methods for estimating interpolation errors, predicated on the understanding that one has already studied foundational error analysis using the finite element method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16012v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroki Ishizaka</dc:creator>
    </item>
    <item>
      <title>Bayesian Parameter Identification in the Landau-de Gennes Theory for Nematic Liquid Crystals</title>
      <link>https://arxiv.org/abs/2504.16029</link>
      <description>arXiv:2504.16029v1 Announce Type: new 
Abstract: This manuscript establishes a pathway to reconstruct material parameters from measurements within the Landau-de Gennes model for nematic liquid crystals. We present a Bayesian approach to this inverse problem and analyse its properties using given, simulated data for benchmark problems of a planar bistable nematic device. In particular, we discuss the accuracy of the Markov chain Monte Carlo approximations, confidence intervals and the limits of identifiability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16029v1</guid>
      <category>math.NA</category>
      <category>cond-mat.soft</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heiko Gimperlein, Ruma R. Maity, Apala Majumdar, Michael Oberguggenberger</dc:creator>
    </item>
    <item>
      <title>Reconstruction of source function in a parabolic equation using partial boundary measurements</title>
      <link>https://arxiv.org/abs/2504.16070</link>
      <description>arXiv:2504.16070v1 Announce Type: new 
Abstract: In this paper, we present the analytical and numerical study of the optimization approach for determining the space-dependent source function in the parabolic inverse source problem using partial boundary measurements. The Lagrangian approach for the solution of the optimization problem is presented, and optimality conditions are derived. The proof of the Fr\'echet differentiability of the regularized Tikhonov functional and the existence result for the solution of the inverse source problem are established. A local stability estimate for the unknown source term is also presented. The numerical examples justify the theoretical investigations using the conjugate gradient method (CGM) in 2D and 3D tests with noisy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16070v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>T. Sharma, L. Beilina, K. Sakthivel</dc:creator>
    </item>
    <item>
      <title>AneuPy: An open source Python tool for creating simulation-ready geometries of abdominal aortic aneurysms</title>
      <link>https://arxiv.org/abs/2504.15285</link>
      <description>arXiv:2504.15285v1 Announce Type: cross 
Abstract: Abdominal aortic aneurysms (AAAs) are localized dilations of the abdominal aorta that can lead to life-threatening rupture if left untreated. AAAs predominantly affect older individuals, with a high mortality rate upon rupture, making early diagnosis and risk assessment critical. The geometric characteristics of an AAA, such as its maximum diameter, asymmetry, and wall thickness, play a crucial role in biomechanical models used to assess rupture risk. Despite the growing use of computational modeling to study AAAs, there is a lack of open source software that facilitates the generation of simulation-ready geometries tailored for biomechanical and hemodynamic analyses. To address this need, we introduce AneuPy, an open-source Python-based tool designed to generate idealized and patient-specific AAA geometrical models. AneuPy provides an efficient and automated approach to aneurysm geometry generation, requiring minimal input data while allowing for flexible parameterization. By streamlining the creation of simulation-ready geometries for finite element analysis (FEA), computational fluid dynamics (CFD), or fluid-structure interaction (FSI) models, AneuPy aims to facilitate research in AAAs and enhance patient-specific risk assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15285v1</guid>
      <category>physics.med-ph</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario de Lucio, Jacobo Diaz, Alberto de Castro, Luis E. Romera</dc:creator>
    </item>
    <item>
      <title>Bayesian Parameter Estimation for Partially Observed McKean-Vlasov Diffusions Using Multilevel Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2504.15588</link>
      <description>arXiv:2504.15588v1 Announce Type: cross 
Abstract: In this article we consider Bayesian estimation of static parameters for a class of partially observed McKean-Vlasov diffusion processes with discrete-time observations over a fixed time interval. This problem features several obstacles to its solution, which include that the posterior density is numerically intractable in continuous-time, even if the transition probabilities are available and even when one uses a time-discretization, the posterior still cannot be used by adopting well-known computational methods such as Markov chain Monte Carlo (MCMC). In this paper we provide a solution to this problem by using new MCMC algorithms which can solve the afore-mentioned issues. This MCMC algorithm is extended to use multilevel Monte Carlo (MLMC) methods. We prove convergence bounds on our parameter estimators and show that the MLMC-based MCMC algorithm reduces the computational cost to achieve a mean square error versus ordinary MCMC by an order of magnitude. We numerically illustrate our results on two models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15588v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ajay Jasra, Amin Wu</dc:creator>
    </item>
    <item>
      <title>Sharp inverse statements for kernel interpolation</title>
      <link>https://arxiv.org/abs/2306.14618</link>
      <description>arXiv:2306.14618v3 Announce Type: replace 
Abstract: While direct statements for kernel based interpolation on regions $\Omega \subset \mathbb{R}^d$ are well researched, far less is known about corresponding inverse statements. The available inverse statements for kernel based interpolation so far are not sharp. In this paper, we derive sharp inverse statements for interpolation using finitely smooth kernels, such as popular radial basis function (RBF) kernels like the class of Mat\'ern or Wendland kernels. In particular, the results show that there is a one-to-one correspondence between the smoothness of a function and its approximation rate via kernel interpolation: If a function can be approximated with a given rate, it has a corresponding smoothness and vice versa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14618v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tizian Wenzel</dc:creator>
    </item>
    <item>
      <title>Some convergence analysis for multicontinuum homogenization</title>
      <link>https://arxiv.org/abs/2401.12799</link>
      <description>arXiv:2401.12799v5 Announce Type: replace 
Abstract: In this paper, we provide an analysis of a recently proposed multicontinuum homogenization technique. The analysis differs from those used in classical homogenization methods for several reasons. First, the cell problems in multicontinuum homogenization use constraint problems and can not be directly substituted into the differential operator. Secondly, the problem contains high contrast that remains in the homogenized problem. The homogenized problem averages the microstructure while containing the small parameter. In this analysis, we first based on our previous techniques, CEM-GMsFEM, to define a CEM-downscaling operator that maps the multicontinuum quantities to an approximated microscopic solution. Following the regularity assumption of the multicontinuum quantities, we construct a downscaling operator and the homogenized multicontinuum equations using the information of linear approximation of the multicontinuum quantities. The error analysis is given by the residual estimate of the homogenized equations and the well-posedness assumption of the homogenized equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12799v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wing Tat Leung</dc:creator>
    </item>
    <item>
      <title>Explicit symmetric low-regularity integrators for the nonlinear Schr\"odinger equation</title>
      <link>https://arxiv.org/abs/2411.07720</link>
      <description>arXiv:2411.07720v3 Announce Type: replace 
Abstract: The numerical approximation of low-regularity solutions to the nonlinear Schr\"odinger equation is notoriously difficult and even more so if structure-preserving schemes are sought. Recent works have been successful in establishing symmetric low-regularity integrators for this equation. However, so far, all prior symmetric low-regularity algorithms are fully implicit, and therefore require the solution of a nonlinear equation at each time step, leading to significant numerical cost in the iteration. In this work, we introduce the first fully explicit (multi-step) symmetric low-regularity integrators for the nonlinear Schr\"odinger equation. We demonstrate the construction of an entire class of such schemes which notably can be used to symmetrise (in explicit form) a large amount of existing low-regularity integrators. We provide rigorous convergence analysis of our schemes and numerical examples demonstrating both the favourable structure preservation properties obtained with our novel schemes, and the significant reduction in computational cost over implicit methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07720v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Feng, Georg Maierhofer, Chushan Wang</dc:creator>
    </item>
    <item>
      <title>Steady-state and transient thermal stress analysis using a polygonal finite element method</title>
      <link>https://arxiv.org/abs/2501.03908</link>
      <description>arXiv:2501.03908v2 Announce Type: replace 
Abstract: This work develops a polygonal finite element method (PFEM) for the analysis of steady-state and transient thermal stresses in two dimensional continua. The method employs Wachspress rational basis functions to construct conforming interpolations over arbitrary convex polygonal meshes, providing enhanced geometric flexibility and accuracy in capturing complex boundary conditions and heterogeneous material behavior. A quadtree-based acceleration strategy is introduced to significantly reduce computational cost through the reuse of precomputed stiffness and mass matrices. The PFEM is implemented in ABAQUS via a user-defined element (UEL) framework. Comprehensive benchmark problems, including multi-scale and non-matching mesh scenarios, are conducted to verify the accuracy, convergence properties, and computational efficiency of the method. Results indicate that the proposed PFEM offers notable advantages over conventional FEM in terms of mesh adaptability, solution quality, and runtime performance. The method shows strong potential for large-scale simulations involving thermal-mechanical coupling, complex geometries, and multi-resolution modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03908v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Yang, Mingjiao Yan, Zongliang Zhang, Dengmiao Hao, Xuedong Chen, Weixiong Chen</dc:creator>
    </item>
    <item>
      <title>The Polynomial Set Associated with a Fixed Number of Matrix-Matrix Multiplications</title>
      <link>https://arxiv.org/abs/2504.01500</link>
      <description>arXiv:2504.01500v2 Announce Type: replace 
Abstract: We consider the problem of computing matrix polynomials $p(X)$, where $X$ is a large dense matrix, with as few matrix-matrix multiplications as possible. More precisely, let $\Pi_{2^{m}}^*$ represent the set of polynomials computable with $m$ matrix-matrix multiplications, but with an arbitrary number of matrix additions and scaling operations. We characterize this set through a tabular parameterization. By deriving equivalence transformations of the tabular representation, we establish new methods that can be used to construct elements of $\Pi_{2^{m}}^*$ and determine general properties of the set. The transformations allow us to eliminate variables and prove that the dimension is bounded by $m^2$. Numerical simulations suggest that this is a sharp bound. Consequently, we have identified a parameterization that, to the best of our knowledge, is the first minimal parameterization. We also conduct a study using computational tools from algebraic geometry to determine the largest degree $d$ such that all polynomials of that degree belong to $\Pi_{2^{m}}^*$, or its closure. In many cases, the computational setup is constructive in the sense that it can also be used to determine a specific evaluation scheme for a given polynomial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01500v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elias Jarlebring, Gustaf Lorentzon</dc:creator>
    </item>
    <item>
      <title>Splitting Method for Stochastic Navier-Stokes Equations</title>
      <link>https://arxiv.org/abs/2504.04360</link>
      <description>arXiv:2504.04360v3 Announce Type: replace 
Abstract: This paper investigates the two-dimensional stochastic steady-state Navier-Stokes(NS) equations with additive random noise. We introduce an innovative splitting method that decomposes the stochastic NS equations into a deterministic NS component and a stochastic equation. We rigorously analyze the proposed splitting method from the perspectives of equivalence, stability, existence and uniqueness of the solution. We also propose a modified splitting scheme, which simplified the stochastic equation by omitting its nonlinear terms. A detailed analysis of the solution properties for this modified approach is provided. Additionally, we discuss the statistical errors with both the original splitting format and the modified scheme. Our theoretical and numerical studies demonstrate that the equivalent splitting scheme exhibits significantly enhanced stability compared to the original stochastic NS equations, enabling more effective handling of nonlinear characteristics. Several numerical experiments were performed to compare the statistical errors of the splitting method and the modified splitting method. Notably, the deterministic NS equation in the splitting method does not require repeated solving, and the stochastic equation in the modified scheme is free of nonlinear terms. These features make the modified splitting method particularly advantageous for large-scale computations, as it significantly improves computational efficiency without compromising accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04360v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Zhu, Yujun Zhu, Ju Ming, Max D. Gunzburger</dc:creator>
    </item>
    <item>
      <title>Computation of shape Taylor expansions</title>
      <link>https://arxiv.org/abs/2504.06621</link>
      <description>arXiv:2504.06621v2 Announce Type: replace 
Abstract: Shape derivative is an important analytical tool for studying scattering problems involving perturbations in scatterers. Many applications, including inverse scattering, optimal design, and uncertainty quantification, are based on shape derivatives. However, computing high order shape derivatives is challenging due to the complexity of shape calculus. This work introduces a comprehensive method for computing shape Taylor expansions in two dimensions using recurrence formulas. The approach is developed under sound-soft, sound-hard, impedance, and transmission boundary conditions. Additionally, we apply the shape Taylor expansion to uncertainty quantification in wave scattering, enabling high order moment estimation for the scattered field under random boundary perturbations. Numerical examples are provided to illustrate the effectiveness of the shape Taylor expansion in achieving high order approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06621v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gang Bao, Jun Lai, Haoran Ma</dc:creator>
    </item>
    <item>
      <title>Maximum bound principle for Q-tensor gradient flow with low regularity integrators</title>
      <link>https://arxiv.org/abs/2504.11676</link>
      <description>arXiv:2504.11676v2 Announce Type: replace 
Abstract: We investigate low-regularity integrator (LRI) methods for the Q-tensor model governing nematic liquid-crystalline semilinear parabolic equation. First- and second-order temporal discretizations are developed using Duhamel's formula, and we rigorously prove that both schemes preserve the maximum bound principle (MBP) and energy dissipation under minimal regularity requirements. Optimal convergence rates are established for the proposed methods. Numerical experiments validate the theoretical findings, demonstrating that the eigenvalues of Q remain strictly confined within the physical range (-1/3},2/3).</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11676v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenshuai Hu, Guanghua Ji</dc:creator>
    </item>
    <item>
      <title>A Fast Direct Solver for Boundary Integral Equations Using Quadrature By Expansion</title>
      <link>https://arxiv.org/abs/2504.13809</link>
      <description>arXiv:2504.13809v2 Announce Type: replace 
Abstract: We construct and analyze a hierarchical direct solver for linear systems arising from the discretization of boundary integral equations using the Quadrature by Expansion (QBX) method. Our scheme builds on the existing theory of Hierarchical Semi-Separable (HSS) matrix operators that contain low-rank off-diagonal submatrices. We use proxy-based approximations of the far-field interactions and the Interpolative Decomposition (ID) to construct compressed HSS operators that are used as fast direct solvers for the original system. We describe a number of modifications to the standard HSS framework that enable compatibility with the QBX family of discretization methods. We establish an error model for the direct solver that is based on a multipole expansion of the QBX-mediated proxy interactions and standard estimates for the ID\@. Based on these theoretical results, we develop an automatic approach for setting scheme parameters based on user-provided error tolerances. The resulting solver seamlessly generalizes across two- and tree-dimensional problems and achieves state-of-the-art asymptotic scaling. We conclude with numerical experiments that support the theoretical expectations for the error and computational cost of the direct solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13809v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandru Fikl, Andreas Kl\"ockner</dc:creator>
    </item>
    <item>
      <title>Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax Optimization</title>
      <link>https://arxiv.org/abs/2303.03984</link>
      <description>arXiv:2303.03984v4 Announce Type: replace-cross 
Abstract: Minimax optimization recently is widely applied in many machine learning tasks such as generative adversarial networks, robust learning and reinforcement learning. In the paper, we study a class of nonconvex-nonconcave minimax optimization with nonsmooth regularization, where the objective function is possibly nonconvex on primal variable $x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition on dual variable $y$. Moreover, we propose a class of enhanced momentum-based gradient descent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic nonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use various adaptive learning rates in updating the variables $x$ and $y$ without relying on any specifical types. Theoretically, we prove that our methods have the best known sample complexity of $\tilde{O}(\epsilon^{-3})$ only requiring one sample at each loop in finding an $\epsilon$-stationary solution. Some numerical experiments on PL-game and Wasserstein-GAN demonstrate the efficiency of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03984v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Feihu Huang, Chunyu Xuan, Xinrui Wang, Siqi Zhang, Songcan Chen</dc:creator>
    </item>
    <item>
      <title>An interacting particle consensus method for constrained global optimization</title>
      <link>https://arxiv.org/abs/2405.00891</link>
      <description>arXiv:2405.00891v3 Announce Type: replace-cross 
Abstract: This paper presents a particle-based optimization method designed for addressing minimization problems with equality constraints, particularly in cases where the loss function exhibits non-differentiability or non-convexity. The proposed method combines components from consensus-based optimization algorithm with a newly introduced forcing term directed at the constraint set. A rigorous mean-field limit of the particle system is derived, and the convergence of the mean-field limit to the constrained minimizer is established. Additionally, we introduce a stable discretized algorithm and conduct various numerical experiments to demonstrate the performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00891v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e A. Carrillo, Shi Jin, Haoyu Zhang, Yuhua Zhu</dc:creator>
    </item>
    <item>
      <title>Benign overfitting in Fixed Dimension via Physics-Informed Learning with Smooth Inductive Bias</title>
      <link>https://arxiv.org/abs/2406.09194</link>
      <description>arXiv:2406.09194v3 Announce Type: replace-cross 
Abstract: Recent advances in machine learning have inspired a surge of research into reconstructing specific quantities of interest from measurements that comply with certain physical laws. These efforts focus on inverse problems that are governed by partial differential equations (PDEs). In this work, we develop an asymptotic Sobolev norm learning curve for kernel ridge(less) regression when addressing (elliptical) linear inverse problems. Our results show that the PDE operators in the inverse problem can stabilize the variance and even behave benign overfitting for fixed-dimensional problems, exhibiting different behaviors from regression problems. Besides, our investigation also demonstrates the impact of various inductive biases introduced by minimizing different Sobolev norms as a form of implicit regularization. For the regularized least squares estimator, we find that all considered inductive biases can achieve the optimal convergence rate, provided the regularization parameter is appropriately chosen. The convergence rate is actually independent to the choice of (smooth enough) inductive bias for both ridge and ridgeless regression. Surprisingly, our smoothness requirement recovered the condition found in Bayesian setting and extend the conclusion to the minimum norm interpolation estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09194v3</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Honam Wong, Wendao Wu, Fanghui Liu, Yiping Lu</dc:creator>
    </item>
    <item>
      <title>Numerical solution by shape optimization method to an inverse shape problem in multi-dimensional advection-diffusion problem with space dependent coefficients</title>
      <link>https://arxiv.org/abs/2504.07796</link>
      <description>arXiv:2504.07796v3 Announce Type: replace-cross 
Abstract: This work focuses on numerically solving a shape identification problem related to advection-diffusion processes with space-dependent coefficients using shape optimization techniques. Two boundary-type cost functionals are considered, and their corresponding variations with respect to shapes are derived using the adjoint method, employing the chain rule approach. This involves firstly utilizing the material derivative of the state system and secondly using its shape derivative. Subsequently, an alternating direction method of multipliers (ADMM) combined with the Sobolev-gradient-descent algorithm is applied to stably solve the shape reconstruction problem. Numerical experiments in two and three dimensions are conducted to demonstrate the feasibility of the methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07796v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elmehdi Cherrat, Lekbir Afraites, Julius Fergy Tiongson Rabago</dc:creator>
    </item>
  </channel>
</rss>
