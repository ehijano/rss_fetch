<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Aug 2024 04:00:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Gaussian Approximation For Non-stationary Time Series with Optimal Rate and Explicit Construction</title>
      <link>https://arxiv.org/abs/2408.02913</link>
      <description>arXiv:2408.02913v1 Announce Type: new 
Abstract: Statistical inference for time series such as curve estimation for time-varying models or testing for existence of change-point have garnered significant attention. However, these works are generally restricted to the assumption of independence and/or stationarity at its best. The main obstacle is that the existing Gaussian approximation results for non-stationary processes only provide an existential proof and thus they are difficult to apply. In this paper, we provide two clear paths to construct such a Gaussian approximation for non-stationary series. While the first one is theoretically more natural, the second one is practically implementable. Our Gaussian approximation results are applicable for a very large class of non-stationary time series, obtain optimal rates and yet have good applicability. Building on such approximations, we also show theoretical results for change-point detection and simultaneous inference in presence of non-stationary errors. Finally we substantiate our theoretical results with simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02913v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soham Bonnerjee, Sayar Karmakar, Wei Biao Wu</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximations for the $k$th coordinate of sums of random vectors</title>
      <link>https://arxiv.org/abs/2408.03039</link>
      <description>arXiv:2408.03039v1 Announce Type: new 
Abstract: We consider the problem of Gaussian approximation for the $\kappa$th coordinate of a sum of high-dimensional random vectors. Such a problem has been studied previously for $\kappa=1$ (i.e., maxima). However, in many applications, a general $\kappa\geq1$ is of great interest, which is addressed in this paper. We make four contributions: 1) we first show that the distribution of the $\kappa$th coordinate of a sum of random vectors, $\boldsymbol{X}= (X_{1},\cdots,X_{p})^{\sf T}= n^{-1/2}\sum_{i=1}^n \boldsymbol{x}_{i}$, can be approximated by that of Gaussian random vectors and derive their Kolmogorov's distributional difference bound; 2) we provide the theoretical justification for estimating the distribution of the $\kappa$th coordinate of a sum of random vectors using a Gaussian multiplier procedure, which multiplies the original vectors with i.i.d. standard Gaussian random variables; 3) we extend the Gaussian approximation result and Gaussian multiplier bootstrap procedure to a more general case where $\kappa$ diverges; 4) we further consider the Gaussian approximation for a square sum of the first $d$ largest coordinates of $\boldsymbol{X}$. All these results allow the dimension $p$ of random vectors to be as large as or much larger than the sample size $n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03039v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixi Ding, Qizhai Li, Yuke Shi, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>A nonparametric test for diurnal variation in spot correlation processes</title>
      <link>https://arxiv.org/abs/2408.02757</link>
      <description>arXiv:2408.02757v1 Announce Type: cross 
Abstract: The association between log-price increments of exchange-traded equities, as measured by their spot correlation estimated from high-frequency data, exhibits a pronounced upward-sloping and almost piecewise linear relationship at the intraday horizon. There is notably lower-on average less positive-correlation in the morning than in the afternoon. We develop a nonparametric testing procedure to detect such deterministic variation in a correlation process. The test statistic has a known distribution under the null hypothesis, whereas it diverges under the alternative. It is robust against stochastic correlation. We run a Monte Carlo simulation to discover the finite sample properties of the test statistic, which are close to the large sample predictions, even for small sample sizes and realistic levels of diurnal variation. In an application, we implement the test on a monthly basis for a high-frequency dataset covering the stock market over an extended period. The test leads to rejection of the null most of the time. This suggests diurnal variation in the correlation process is a nontrivial effect in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02757v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kim Christensen, Ulrich Hounyo, Zhi Liu</dc:creator>
    </item>
    <item>
      <title>A stereographic test of spherical uniformity</title>
      <link>https://arxiv.org/abs/2405.13531</link>
      <description>arXiv:2405.13531v2 Announce Type: replace 
Abstract: We introduce a test of uniformity for (hyper)spherical data motivated by the stereographic projection. The closed-form expression of the test statistic and its null asymptotic distribution are derived using Gegenbauer polynomials. The power against rotationally symmetric local alternatives is provided, and simulations illustrate the non-null asymptotic results. The stereographic test outperforms other tests in a testing scenario with antipodal dependence between observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13531v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spl.2024.110218</arxiv:DOI>
      <arxiv:journal_reference>Statistics &amp; Probability Letters, 215:110218, 2024</arxiv:journal_reference>
      <dc:creator>Alberto Fern\'andez-de-Marcos, Eduardo Garc\'ia-Portugu\'es</dc:creator>
    </item>
    <item>
      <title>Bayesian inference of covariate-parameter relationships for population modelling</title>
      <link>https://arxiv.org/abs/2407.09640</link>
      <description>arXiv:2407.09640v2 Announce Type: replace 
Abstract: We consider population modelling using parametrised ordinary differential equation initial value problems (ODE-IVPs). For each individual drawn randomly from the unknown population distribution, the corresponding parameters for the ODE-IVP cannot be measured directly, but a vector of covariates is given, and one component of the solution to the corresponding ODE-IVP is observed at a fixed finite time grid. The task is to identify a covariate-parameter relationship that maps covariate vectors to parameter vectors. Such settings and problems arise in pharmacokinetics, where the observations are blood drug concentrations, the covariates are clinically observable quantities, and the covariate-parameter relationship is used for personalised drug dosing. For linear homogeneous ODE-IVPs with vector fields defined by matrices that are diagonalisable over $\mathbb{R}$, and for fixed time and random covariate design, we use recent results of Nickl et al. for Bayesian nonlinear statistical inverse problems, to prove posterior contraction and Bernstein--von Mises results for the unknown covariate-parameter relationship. We analytically demonstrate our results on an example from the pharmacokinetics literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09640v2</guid>
      <category>math.ST</category>
      <category>math.CA</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Cheng Lie</dc:creator>
    </item>
    <item>
      <title>A concise proof of Benford's law</title>
      <link>https://arxiv.org/abs/2407.11076</link>
      <description>arXiv:2407.11076v2 Announce Type: replace 
Abstract: This article presents a concise proof of the famous Benford's law when the distribution has a Riemann integrable probability density function and provides a criterion to judge whether a distribution obeys the law. The proof is intuitive and elegant, accessible to anyone with basic knowledge of calculus, revealing that the law originates from the basic property of the human number system. The criterion can bring great convenience to the field of fraud detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11076v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.fmre.2023.01.002</arxiv:DOI>
      <arxiv:journal_reference>Fundamental Research 4 (2024) 841-844</arxiv:journal_reference>
      <dc:creator>Luohan Wang, Bo-Qiang Ma</dc:creator>
    </item>
    <item>
      <title>On the Low-Temperature MCMC threshold: the cases of sparse tensor PCA, sparse regression, and a geometric rule</title>
      <link>https://arxiv.org/abs/2408.00746</link>
      <description>arXiv:2408.00746v2 Announce Type: replace 
Abstract: Over the last years, there has been a significant amount of work studying the power of specific classes of computationally efficient estimators for multiple statistical parametric estimation tasks, including the estimators classes of low-degree polynomials, spectral methods, and others. Despite that, our understanding of the important class of MCMC methods remains quite poorly understood. For instance, for many models of interest, the performance of even zero-temperature (greedy-like) MCMC methods that simply maximize the posterior remains elusive.
  In this work, we provide an easy to check condition under which the low-temperature Metropolis chain maximizes the posterior in polynomial-time with high probability. The result is generally applicable, and in this work, we use it to derive positive MCMC results for two classical sparse estimation tasks: the sparse tensor PCA model and sparse regression. Interestingly, in both cases, we also leverage the Overlap Gap Property framework for inference (Gamarnik, Zadik AoS '22) to prove that our results are tight: no low-temperature local MCMC method can achieve better performance. In particular, our work identifies the "low-temperature (local) MCMC threshold" for both sparse models. Interestingly, in the sparse tensor PCA model our results indicate that low-temperature local MCMC methods significantly underperform compared to other studied time-efficient methods, such as the class of low-degree polynomials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00746v2</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongchen Chen, Conor Sheehan, Ilias Zadik</dc:creator>
    </item>
    <item>
      <title>Spatial autoregressive model with measurement error in covariates</title>
      <link>https://arxiv.org/abs/2402.04593</link>
      <description>arXiv:2402.04593v3 Announce Type: replace-cross 
Abstract: The Spatial AutoRegressive model (SAR) is commonly used in studies involving spatial and network data to estimate the spatial or network peer influence and the effects of covariates on the response, taking into account the dependence among units. While the model can be efficiently estimated with a Quasi maximum likelihood approach (QMLE), the detrimental effect of covariate measurement error on the QMLE and how to remedy it is currently unknown. If covariates are measured with error, then the QMLE may not have the $\sqrt{n}$ convergence and may even be inconsistent even when a node is influenced by only a limited number of other nodes or spatial units. We develop a measurement error-corrected ML estimator (ME-QMLE) for the parameters of the SAR model when covariates are measured with error. The ME-QMLE possesses statistical consistency and asymptotic normality properties and we derive its limiting covariance. We consider two types of applications. The first is when the true covariate is imprecisely measured with replicated measurements or cannot be measured directly, and a proxy is observed instead. The second one involves including latent homophily factors estimated with error from the network for estimating peer influence. Our numerical results verify the bias correction property of the estimator and the accuracy of the standard error estimates in finite samples. We illustrate the method on two real datasets; i) peer influence in GPA for middle school students in New Jersey and ii) county-level death rates from the COVID-19 pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04593v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhadeep Paul, Shanjukta Nath</dc:creator>
    </item>
    <item>
      <title>Conditioning of Banach Space Valued Gaussian Random Variables: An Approximation Approach Based on Martingales</title>
      <link>https://arxiv.org/abs/2404.03453</link>
      <description>arXiv:2404.03453v3 Announce Type: replace-cross 
Abstract: In this paper we investigate the conditional distributions of two Banach space valued, jointly Gaussian random variables. We show that these conditional distributions are again Gaussian and that their means and covariances are determined by a general finite dimensional approximation scheme based upon a martingale approach. In particular, it turns out that the covariance operators occurring in this scheme converge with respect to the nuclear norm and that the conditional probabilities converge weakly. Moreover, we discuss in detail, how our approximation scheme can be implemented in several classes of important Banach spaces such as (reproducing kernel) Hilbert spaces and spaces of continuous functions. As an example, we then apply our general results to the case of Gaussian processes with continuous paths conditioned to partial but infinite observations of their paths. Here we show that conditioning on sufficiently rich, increasing sets of finitely many observations leads to consistent approximations, that is, both the mean and covariance functions converge uniformly and the conditional probabilities converge weakly. Moreover, we discuss how these results improve our understanding of the popular Gaussian processes for machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03453v3</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ingo Steinwart</dc:creator>
    </item>
    <item>
      <title>Convergence, optimization and stability of singular eigenmaps</title>
      <link>https://arxiv.org/abs/2406.19510</link>
      <description>arXiv:2406.19510v2 Announce Type: replace-cross 
Abstract: Eigenmaps are important in analysis, geometry, and machine learning, especially in nonlinear dimension reduction. Approximation of the eigenmaps of a Laplace operator depends crucially on the scaling parameter $\epsilon$. If $\epsilon$ is too small or too large, then the approximation is inaccurate or completely breaks down. However, an analytic expression for the optimal $\epsilon$ is out of reach. In our work, we use some explicitly solvable models and Monte Carlo simulations to find the approximately optimal range of $\epsilon$ that gives, on average, relatively accurate approximation of the eigenmaps. Numerically we can consider several model situations where eigen-coordinates can be computed analytically, including intervals with uniform and weighted measures, squares, tori, spheres, and the Sierpinski gasket. In broader terms, we intend to study eigen-coordinates on weighted Riemannian manifolds, possibly with boundary, and on some metric measure spaces, such as fractals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19510v2</guid>
      <category>math.PR</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernard Akwei, Bobita Atkins, Rachel Bailey, Ashka Dalal, Natalie Dinin, Jonathan Kerby-White, Tess McGuinness, Tonya Patricks, Luke Rogers, Genevieve Romanelli, Yiheng Su, Alexander Teplyaev</dc:creator>
    </item>
  </channel>
</rss>
