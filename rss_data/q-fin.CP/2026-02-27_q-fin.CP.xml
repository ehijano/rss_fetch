<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.CP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.CP</link>
    <description>q-fin.CP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.CP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 05:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Breaking the Dimensional Barrier: Dynamic Portfolio Choice with Parameter Uncertainty via Pontryagin Projection</title>
      <link>https://arxiv.org/abs/2601.03175</link>
      <description>arXiv:2601.03175v3 Announce Type: replace 
Abstract: We study continuous-time CRRA portfolio choice in diffusion markets with estimated and hence uncertain coefficients. Nature draws a latent parameter $\theta \sim q$ at time $0$ and keeps it fixed; the investor never observes $\theta$ and must commit to a single $\theta$-blind policy maximizing an ex-ante objective, treating $q$ as a decision-time input. We propose a simulation-only two-stage solver.Stage 1 (DPO) performs BPTT-based stochastic gradient ascent through an Euler simulator while sampling $\theta$ only inside the simulator. Stage 2 (Pontryagin projection) aggregates costate blocks across $\theta \sim q$ and enforces the $q$-aggregated stationarity condition within the deployable class; the resulting correction can be amortized via interactive distillation. We refer to the full Stage 1 + Stage 2 pipeline as PG-DPO.We prove a uniform conditional BPTT-PMP correspondence and a residual-based policy-gap bound with explicit discretization and Monte Carlo error terms. Experiments on high-dimensional Gaussian drift-uncertainty and factor-driven benchmarks show that projection stabilizes learning and accurately recovers analytic decision-time references, while a model-free PPO baseline remains far from the targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03175v3</guid>
      <category>q-fin.CP</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeonggyu Huh, Hyeng Keun Koo</dc:creator>
    </item>
  </channel>
</rss>
