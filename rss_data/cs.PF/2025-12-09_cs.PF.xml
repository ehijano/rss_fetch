<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Dec 2025 02:45:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization</title>
      <link>https://arxiv.org/abs/2512.06699</link>
      <description>arXiv:2512.06699v1 Announce Type: new 
Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06699v1</guid>
      <category>cs.PF</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Prabhakar</dc:creator>
    </item>
    <item>
      <title>AFarePart: Accuracy-aware Fault-resilient Partitioner for DNN Edge Accelerators</title>
      <link>https://arxiv.org/abs/2512.07449</link>
      <description>arXiv:2512.07449v1 Announce Type: new 
Abstract: Deep Neural Networks (DNNs) are increasingly deployed across distributed and resource-constrained platforms, such as System-on-Chip (SoC) accelerators and edge-cloud systems. DNNs are often partitioned and executed across heterogeneous processing units to optimize latency and energy. However, the reliability of these partitioned models under hardware faults and communication errors remains a critical yet underexplored topic, especially in safety-critical applications. In this paper, we propose an accuracy-aware, fault-resilient DNN partitioning framework targeting multi-objective optimization using NSGA-II, where accuracy degradation under fault conditions is introduced as a core metric alongside energy and latency. Our framework performs runtime fault injection during optimization and utilizes a feedback loop to prioritize fault-tolerant partitioning. We evaluate our approach on benchmark CNNs including AlexNet, SqueezeNet and ResNet18 on hardware accelerators, and demonstrate up to 27.7% improvement in fault tolerance with minimal increase in performance overhead. Our results highlight the importance of incorporating resilience into DNN partitioning, and thereby paving the way for robust AI inference in error-prone environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07449v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mukta Debnath (University of Calcutta, India), Krishnendu Guha (University College Cork, Ireland), Debasri Saha (University of Calcutta, India), Amlan Chakrabarti (University of Calcutta, India), Susmita Sur-Kolay (Indian Statistical Institute, India)</dc:creator>
    </item>
    <item>
      <title>Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses</title>
      <link>https://arxiv.org/abs/2512.06390</link>
      <description>arXiv:2512.06390v1 Announce Type: cross 
Abstract: The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06390v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/APWiMob67231.2025.11269122</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob), Bali, Indonesia, 2025, pp. 180-186</arxiv:journal_reference>
      <dc:creator>Mehrab Hosain, Sabbir Alom Shuvo, Matthew Ogbe, Md Shah Jalal Mazumder, Yead Rahman, Md Azizul Hakim, Anukul Pandey</dc:creator>
    </item>
    <item>
      <title>Block Sparse Flash Attention</title>
      <link>https://arxiv.org/abs/2512.07011</link>
      <description>arXiv:2512.07011v1 Announce Type: cross 
Abstract: Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07011v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Ohayon, Itay Lamprecht, Itay Hubara, Israel Cohen, Daniel Soudry, Noam Elata</dc:creator>
    </item>
    <item>
      <title>Scheduling in Quantum Satellite Networks: Fairness and Performance Optimization</title>
      <link>https://arxiv.org/abs/2512.07108</link>
      <description>arXiv:2512.07108v1 Announce Type: cross 
Abstract: Quantum satellite networks offer a promising solution for achieving long-distance quantum communication by enabling entanglement distribution across global scales. This work formulates and solves the quantum satellite network scheduling problem by optimizing satellite-to-ground station pair assignments under realistic system and environmental constraints. Our framework accounts for limited satellite and ground station resources, fairness, entanglement fidelity thresholds, and real world non-idealities including atmospheric losses, weather and background noise. In addition, we incorporate the complexities of multi-satellite relays enabled via inter-satellite links. We propose an integer linear programming (ILP) based optimization framework that supports multiple scheduling objectives, allowing us to analyze tradeoffs between maximizing total entanglement distribution rate and ensuring fairness across ground station pairs. Our framework can also be used as a benchmark tool to measure the performance of other potential transmission scheduling policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07108v1</guid>
      <category>quant-ph</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashutosh Jayant Dikshit, Naga Lakshmi Anipeddi, Prajit Dhara, Saikat Guha, Deirdre Kilbane, Leandros Tassiulas, Don Towsley, Nitish K. Panigrahy</dc:creator>
    </item>
    <item>
      <title>An\'alisis de rendimiento y eficiencia energ\'etica en el cluster Raspberry Pi Cronos</title>
      <link>https://arxiv.org/abs/2512.07622</link>
      <description>arXiv:2512.07622v1 Announce Type: cross 
Abstract: This article presents an evaluation of the computational performance and energy efficiency of the Cronos cluster, composed of Raspberry Pi4 and 3b microcomputers designed for educational purposes. Experimental tests were performed using the High Performance Linpack (HPL) benchmark, under a resource management environment configured with Slurm and parallel communication via Open MPI. The study focuses on analyzing scalability, stability, and power consumption during the execution of computationally intensive workloads, considering different node configurations. The results show that the cluster achieves a performance of up to 6.91 GFLOPS in homogeneous configurations of 6 Raspberry Pi 4 nodes, and that the use of heterogeneous nodes (including Raspberry Pi 3b) can negatively impact stability and efficiency. Additionally, the total electrical consumption of the system was measured during the runs, allowing for the estimation of the performance-to-consumption ratio (GFLOPS/W) as a comparative metric. This study constitutes a concrete contribution to the design, evaluation, and utilization of low-cost ARM clusters in educational and research contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07622v1</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martha Semken, Mariano Vargas, Ignacio Tula, Giuliana Zorzoli, Andr\'es Rojas Paredes</dc:creator>
    </item>
    <item>
      <title>Game of arrivals at a two queue network with heterogeneous customer routes</title>
      <link>https://arxiv.org/abs/2310.18149</link>
      <description>arXiv:2310.18149v2 Announce Type: replace 
Abstract: We consider a queuing network that opens at a specified time, where customers are non-atomic and belong to different classes. Each class has its own route, and as is typical in the literature, the costs are a linear function of waiting and service completion time. We restrict ourselves to a two class, two queue network: this simplification is well motivated as the diversity in solution structure as a function of problem parameters is substantial even in this simple setting (e.g., a specific routing structure involves eight different regimes), suggesting a combinatorial blow up as the number of queues, routes and customer classes increase. We identify the unique Nash equilibrium customer arrival profile when the customer linear cost preferences are different. This profile is a function of problem parameters including the size of each class, service rates at each queue, and customer cost preferences. When customer cost preferences match, under certain parametric settings, the equilibrium arrival profiles may not be unique and may lie in a convex set. We further make a surprising observation that in some parametric settings, customers in one class may arrive in disjoint intervals. Further, the two classes may arrive in contiguous intervals or in overlapping intervals, and at varying rates within an interval, depending upon the problem parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18149v2</guid>
      <category>cs.PF</category>
      <category>cs.GT</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agniv Bandyopadhyay, Sandeep Juneja</dc:creator>
    </item>
    <item>
      <title>Novel Lower Bounds on M/G/k Scheduling</title>
      <link>https://arxiv.org/abs/2502.08804</link>
      <description>arXiv:2502.08804v3 Announce Type: replace 
Abstract: In queueing systems, effective scheduling algorithms are essential for optimizing performance. Optimal scheduling for the M/G/k queue has been explored in the heavy traffic limit, but much remains unknown in the intermediate load regime.
  In this paper, we give the first framework for proving nontrivial lower bounds on the mean response time of the M/G/k system under arbitrary scheduling policies. Our bounds tighten previous naive lower bounds by more than 60\%, yielding significant improvements particularly for moderate loads. Key to our approach is a new variable-speed queue, which more accurately captures the work completion behavior of multiserver systems. To analyze the expected work of this queue, we develop a novel manner of employing the drift method or the BAR approach, by developing test functions via the solutions to a differential equation.
  We validate our results numerically for systems with up to 5 servers and a range of job size distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08804v3</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyuan Wang, Izzy Grosof</dc:creator>
    </item>
    <item>
      <title>H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference</title>
      <link>https://arxiv.org/abs/2508.16653</link>
      <description>arXiv:2508.16653v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable proficiency in a wide range of natural language processing applications. However, the high energy and latency overhead induced by the KV cache limits the edge deployment, especially for long contexts. Emerging hybrid bonding (HB) technology has been proposed as a promising alternative to conventional near-memory processing (NMP) architectures, offering improved bandwidth efficiency and lower power consumption while exhibiting characteristics of distributed memory. In this paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse attention algorithm-hardware co-design for efficient LLM inference at the edge. At the algorithm level, we propose a hybrid sparse attention scheme with static and dynamic sparsity for different heads to fully leverage the sparsity with high accuracy. At the hardware level, we co-design the hardware to support hybrid sparse attention and propose memory-compute co-placement to address the distributed memory bottleneck. Since different attention heads exhibit different sparse patterns and the attention structure often mismatches the HB architecture, we further develop a load-balancing scheduler with parallel tiled attention to address workload imbalance and optimize the mapping strategy. Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and 6.22~73.48x energy efficiency improvement over baseline HB implementation, with a negligible average accuracy drop of 0.87% on multiple benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16653v2</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zizhuo Fu, Xiaotian Guo, Wenxuan Zeng, Shuzhang Zhong, Yadong Zhang, Peiyu Chen, Runsheng Wang, Le Ye, Meng Li</dc:creator>
    </item>
    <item>
      <title>Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery</title>
      <link>https://arxiv.org/abs/2509.08207</link>
      <description>arXiv:2509.08207v2 Announce Type: replace-cross 
Abstract: Aurora is Argonne National Laboratory's pioneering Exascale supercomputer, designed to accelerate scientific discovery with cutting-edge architectural innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named Ponte Vecchio) on each compute node. Aurora also integrates the Distributed Asynchronous Object Storage (DAOS), a novel exascale storage solution, and leverages Intel's oneAPI programming environment. This paper presents an in-depth exploration of Aurora's node architecture, the HPE Slingshot interconnect, the supporting software ecosystem, and DAOS. We provide insights into standard benchmark performance and applications readiness efforts via Aurora's Early Science Program and the Exascale Computing Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08207v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.CE</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William E. Allcock, Benjamin S. Allen, James Anchell, Victor Anisimov, Thomas Applencourt, Abhishek Bagusetty, Ramesh Balakrishnan, Riccardo Balin, Solomon Bekele, Colleen Bertoni, Cyrus Blackworth, Renzo Bustamante, Kevin Canada, John Carrier, Christopher Chan-nui, Lance C. Cheney, Taylor Childers, Paul Coffman, Susan Coghlan, Tanima Dey, Michael D'Mello, Ashok Emani, Murali Emani, Kyle G. Felker, Sam Foreman, Olivier Franza, Longfei Gao, Marta Garc\'ia, Mar\'ia Garzar\'an, Balazs Gerofi, Yasaman Ghadar, Subrata Goswami, Neha Gupta, Kevin Harms, V\"ain\"o Hatanp\"a\"a, Brian Holland, Carissa Holohan, Brian Homerding, Khalid Hossain, Xue Hu, Louise Huot, Huda Ibeid, Joseph A. Insley, Sai Jayanthi, Hong Jiang, Wei Jiang, Xiao-Yong Jin, Jeongnim Kim, Christopher Knight, Panagiotis Kourdis, Kalyan Kumaran, JaeHyuk Kwack, Janghaeng Lee, Ti Leggett, Ben Lenard, Chris Lewis, Nevin Liber, Johann Lombardi, Raymond M. Loy, Ye Luo, Bethany Lusch, Nilakantan Mahadevan, Beth Markey, Victor A. Mateevitsi, Gordon McPheeters, Ryan Milner, Jerome Mitchell, Vitali A. Morozov, Servesh Muralidharan, Tom Musta, Mrigendra Nagar, Vikram Narayana, Marieme Ngom, Anthony-Trung Nguyen, Nathan Nichols, Aditya Nishtala, James C. Osborn, Michael E. Papka, Scott Parker, Saumil S. Patel, Julia Piotrowska, Adrian C. Pope, Sucheta Raghunanda, Esteban Rangel, Paul M. Rich, Katherine M. Riley, Silvio Rizzi, Kris Rowe, Varuni Sastry, Adam Scovel, Filippo Simini, Haritha Siddabathuni Som, Patrick Steinbrecher, Rick Stevens, Xinmin Tian, Peter Upton, Thomas Uram, Archit K. Vasan, \'Alvaro V\'azquez-Mayagoitia, Kaushik Velusamy, Brice Videau, Venkatram Vishwanath, Brian Whitney, Timothy J. Williams, Michael Woodacre, Sam Zeltner, Chuanjun Zhang, Gengbin Zheng, Huihuo Zheng</dc:creator>
    </item>
  </channel>
</rss>
