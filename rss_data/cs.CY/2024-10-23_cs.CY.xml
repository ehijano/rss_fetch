<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Oct 2024 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Uncovering Regional Defaults from Photorealistic Forests in Text-to-Image Generation with DALL-E 2</title>
      <link>https://arxiv.org/abs/2410.17255</link>
      <description>arXiv:2410.17255v1 Announce Type: new 
Abstract: Regional defaults describe the emerging phenomenon that text-to-image (T2I) foundation models used in generative AI are prone to over-proportionally depicting certain geographic regions to the exclusion of others. In this work, we introduce a scalable evaluation for uncovering such regional defaults. The evaluation consists of region hierarchy--based image generation and cross-level similarity comparisons. We carry out an experiment by prompting DALL-E 2, a state-of-the-art T2I generation model capable of generating photorealistic images, to depict a forest. We select forest as an object class that displays regional variation and can be characterized using spatial statistics. For a region in the hierarchy, our experiment reveals the regional defaults implicit in DALL-E 2, along with their scale-dependent nature and spatial relationships. In addition, we discover that the implicit defaults do not necessarily correspond to the most widely forested regions in reality. Our findings underscore a need for further investigation into the geography of T2I generation and other forms of generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17255v1</guid>
      <category>cs.CY</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zilong Liu, Krzysztof Janowicz, Kitty Currier, Meilin Shi</dc:creator>
    </item>
    <item>
      <title>Code-Driven Law NO, Normware SI!</title>
      <link>https://arxiv.org/abs/2410.17257</link>
      <description>arXiv:2410.17257v1 Announce Type: new 
Abstract: With the digitalization of society, the interest, the debates and the research efforts concerning "code", "law", "artificial intelligence", and their various relationships, have been widely increasing. Yet, most arguments primarily focus on contemporary computational methods and artifacts (inferential models constructed via machine-learning methods, rule-based systems, smart contracts, ...), rather than attempting to identify more fundamental mechanisms. Aiming to go beyond this conceptual limitation, this paper introduces and elaborates on "normware" as an explicit additional stance -- complementary to software and hardware -- for the interpretation and the design of artificial devices. By means of a few examples, we argue that normware-centred views provide a more adequate abstraction to study and design interactions between computational systems and human institutions, and may help with the design and development of technical interventions within wider socio-technical views.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17257v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Sileno</dc:creator>
    </item>
    <item>
      <title>Evaluation of Systems Programming Exercises through Tailored Static Analysis</title>
      <link>https://arxiv.org/abs/2410.17260</link>
      <description>arXiv:2410.17260v1 Announce Type: new 
Abstract: In large programming classes, it takes a significant effort from teachers to evaluate exercises and provide detailed feedback. In systems programming, test cases are not sufficient to assess exercises, since concurrency and resource management bugs are difficult to reproduce. This paper presents an experience report on static analysis for the automatic evaluation of systems programming exercises. We design systems programming assignments with static analysis rules that are tailored for each assignment, to provide detailed and accurate feedback. Our evaluation shows that static analysis can identify a significant number of erroneous submissions missed by test cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17260v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Natella</dc:creator>
    </item>
    <item>
      <title>FairFML: Fair Federated Machine Learning with a Case Study on Reducing Gender Disparities in Cardiac Arrest Outcome Prediction</title>
      <link>https://arxiv.org/abs/2410.17269</link>
      <description>arXiv:2410.17269v1 Announce Type: new 
Abstract: Objective: Mitigating algorithmic disparities is a critical challenge in healthcare research, where ensuring equity and fairness is paramount. While large-scale healthcare data exist across multiple institutions, cross-institutional collaborations often face privacy constraints, highlighting the need for privacy-preserving solutions that also promote fairness.
  Materials and Methods: In this study, we present Fair Federated Machine Learning (FairFML), a model-agnostic solution designed to reduce algorithmic bias in cross-institutional healthcare collaborations while preserving patient privacy. As a proof of concept, we validated FairFML using a real-world clinical case study focused on reducing gender disparities in cardiac arrest outcome prediction.
  Results: We demonstrate that the proposed FairFML framework enhances fairness in federated learning (FL) models without compromising predictive performance. Our findings show that FairFML improves model fairness by up to 65% compared to the centralized model, while maintaining performance comparable to both local and centralized models, as measured by receiver operating characteristic analysis.
  Discussion and Conclusion: FairFML offers a promising and flexible solution for FL collaborations, with its adaptability allowing seamless integration with various FL frameworks and models, from traditional statistical methods to deep learning techniques. This makes FairFML a robust approach for developing fairer FL models across diverse clinical and biomedical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17269v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siqi Li, Qiming Wu, Xin Li, Di Miao, Chuan Hong, Wenjun Gu, Yuqing Shang, Yohei Okada, Michael Hao Chen, Mengying Yan, Yilin Ning, Marcus Eng Hock Ong, Nan Liu</dc:creator>
    </item>
    <item>
      <title>Legal Theory for Pluralistic Alignment</title>
      <link>https://arxiv.org/abs/2410.17271</link>
      <description>arXiv:2410.17271v1 Announce Type: new 
Abstract: Legal theory can address two related key problems of alignment: pluralism and specification. Alignment researchers must determine how to specify what is concretely meant by vague principles like helpfulness and fairness and they must ensure that their techniques do not exclude alternative perspectives on life and values. The law faces these same problems. Leading legal theories suggest the law solves these problems through the interaction of rules and cases, where general rules promulgated by a democratic authority are given specific content through their application over time. Concrete applications allow for convergence on practical meaning while preserving space for disagreement on values. These approaches suggest improvements to existing democratic alignment processes that use AI to create cases that give content to rules, allowing for more pluralist alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17271v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas A. Caputo</dc:creator>
    </item>
    <item>
      <title>Military Applications of Machine Learning: A Bibliometric Perspective</title>
      <link>https://arxiv.org/abs/2410.17272</link>
      <description>arXiv:2410.17272v1 Announce Type: new 
Abstract: The military environment generates a large amount of data of great importance, which makes necessary the use of machine learning for its processing. Its ability to learn and predict possible scenarios by analyzing the huge volume of information generated provides automatic learning and decision support. This paper aims to present a model of a machine learning architecture applied to a military organization, carried out and supported by a bibliometric study applied to an architecture model of a nonmilitary organization. For this purpose, a bibliometric analysis up to the year 2021 was carried out, making a strategic diagram and interpreting the results. The information used has been extracted from one of the main databases widely accepted by the scientific community, ISI WoS. No direct military sources were used. This work is divided into five parts: the study of previous research related to machine learning in the military world; the explanation of our research methodology using the SciMat, Excel and VosViewer tools; the use of this methodology based on data mining, preprocessing, cluster normalization, a strategic diagram and the analysis of its results to investigate machine learning in the military context; based on these results, a conceptual architecture of the practical use of ML in the military context is drawn up; and, finally, we present the conclusions, where we will see the most important areas and the latest advances in machine learning applied, in this case, to a military environment, to analyze a large set of data, providing utility, machine learning and decision support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17272v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/math10091397</arxiv:DOI>
      <arxiv:journal_reference>Mathematics, vol. 10, no. 9, Art. no. 9, Jan. 2022</arxiv:journal_reference>
      <dc:creator>Jos\'e Javier Gal\'an, Ram\'on Alberto Carrasco, Antonio LaTorre</dc:creator>
    </item>
    <item>
      <title>Behavior Matters: An Alternative Perspective on Promoting Responsible Data Science</title>
      <link>https://arxiv.org/abs/2410.17273</link>
      <description>arXiv:2410.17273v1 Announce Type: new 
Abstract: Data science pipelines inform and influence many daily decisions, from what we buy to who we work for and even where we live. When designed incorrectly, these pipelines can easily propagate social inequity and harm. Traditional solutions are technical in nature; e.g., mitigating biased algorithms. In this vision paper, we introduce a novel lens for promoting responsible data science using theories of behavior change that emphasize not only technical solutions but also the behavioral responsibility of practitioners. By integrating behavior change theories from cognitive psychology with data science workflow knowledge and ethics guidelines, we present a new perspective on responsible data science. We present example data science interventions in machine learning and visual data analysis, contextualized in behavior change theories that could be implemented to interrupt and redirect potentially suboptimal or negligent practices while reinforcing ethically conscious behaviors. We conclude with a call to action to our community to explore this new research area of behavior change interventions for responsible data science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17273v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziwei Dong, Ameya Patil, Yuichi Shoda, Leilani Battle, Emily Wall</dc:creator>
    </item>
    <item>
      <title>Automated decision-making and artificial intelligence at European borders and their risks for human rights</title>
      <link>https://arxiv.org/abs/2410.17278</link>
      <description>arXiv:2410.17278v1 Announce Type: new 
Abstract: Many countries use automated decision-making (ADM) systems, often based on artificial intelligence (AI), to manage migration at their borders. This interdisciplinary paper explores two questions. What are the main ways that automated decision-making is used at EU borders? Does such automated decision-making bring risks related to human rights, and if so: which risks? The paper introduces a taxonomy of four types of ADM systems at EU borders. Three types are used at borders: systems for (1) identification and verification by checking biometrics, (2) risk assessment, and (3) border monitoring. In addition, (4) polygraphs and emotion detectors are being tested at EU borders. We discuss three categories of risks of such automated decision-making, namely risks related to the human rights to (1) privacy and data protection, (2) nondiscrimination, and (3) a fair trial and effective remedies. The paper is largely based on a literature review that we conducted about the use of automated decision-making at borders. The paper combines insights from several disciplines, including social sciences, law, computer science, and migration studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17278v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Yang, Frederik Zuiderveen Borgesius, Pascal Beckers, Evelien Brouwer</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Survey and Classification of Evaluation Criteria for Trustworthy Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2410.17281</link>
      <description>arXiv:2410.17281v1 Announce Type: new 
Abstract: This paper presents a systematic review of the literature on evaluation criteria for Trustworthy Artificial Intelligence (TAI), with a focus on the seven EU principles of TAI. This systematic literature review identifies and analyses current evaluation criteria, maps them to the EU TAI principles and proposes a new classification system for each principle. The findings reveal both a need for and significant barriers to standardising criteria for TAI evaluation. The proposed classification contributes to the development, selection and standardization of evaluation criteria for TAI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17281v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louise McCormack, Malika Bendechache</dc:creator>
    </item>
    <item>
      <title>The Internet of Forgotten Things: European Cybersecurity Regulation and IoT Manufacturer Cessation</title>
      <link>https://arxiv.org/abs/2410.17296</link>
      <description>arXiv:2410.17296v1 Announce Type: new 
Abstract: Many modern consumer devices rely on network connections and cloud services to perform their core functions. This dependency is especially present in Internet of Things (IoT) devices, which combine hardware and software with network connections (e.g., a 'smart' doorbell with a camera). This paper argues that current European product legislation, which aims to protect consumers of, inter alia, IoT devices, has a blind spot for an increasing problem in the competitive IoT market: manufacturer cessation. Without the manufacturer's cloud servers, many IoT devices cannot perform core functions such as data analysis. If an IoT manufacturer ceases their operations, consumers of the manufacturer's devices are thus often left with a dysfunctional device and, as the paper shows, hardly any legal remedies. This paper therefore investigates three properties that could support legislators in finding a solution for IoT manufacturer cessation: i) pre-emptive measures, aimed at ii) manufacturer-independent iii) collective control. The paper finally shows how these three properties already align with current legislative processes surrounding 'interoperability' and open-source software development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17296v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mattis van 't Schip</dc:creator>
    </item>
    <item>
      <title>ICT Sector Greenhouse Gas Emissions -- Issues and Trends</title>
      <link>https://arxiv.org/abs/2410.17388</link>
      <description>arXiv:2410.17388v1 Announce Type: new 
Abstract: As Information and Communication Technology (ICT) use has become more prevalent, there has been a growing concern in how its associated greenhouse gas emissions will impact the climate. Estimating such ICT emissions is a difficult undertaking due to its complexity, its rapidly changing nature, and the lack of accurate and up-to-date data on individual stakeholder emissions. In this paper we provide a framework for estimating ICT's carbon footprint and identify some of the issues that impede the task. We attempt to gain greater insight into the factors affecting the ICT sector by drawing on a number of interviews with industry experts. We conclude that more accurate emissions estimates will only be possible with a more more detailed, industry informed, understanding of the whole ICT landscape and much more transparent reporting of energy usage and emissions data by ICT stakeholders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17388v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Peter Garraghan, John Hutchinson, Adrian Friday</dc:creator>
    </item>
    <item>
      <title>From an attention economy to an ecology of attending. A manifesto</title>
      <link>https://arxiv.org/abs/2410.17421</link>
      <description>arXiv:2410.17421v1 Announce Type: new 
Abstract: As the signatories of this manifesto, we denounce the attention economy as inhumane and a threat to our sociopolitical and ecological well-being. We endorse policymakers' efforts to address the negative consequences of the attention economy's technology, but add that these approaches are often limited in their criticism of the systemic context of human attention. Starting from Buddhist philosophy, we advocate a broader approach: an ecology of attending, that centers on conceptualizing, designing, and using attention (1) in an embedded way and (2) focused on the alleviating of suffering. With 'embedded' we mean that attention is not a neutral, isolated mechanism but a meaning-engendering part of an 'ecology' of bodily, sociotechnical and moral frameworks. With 'focused on the alleviation of suffering' we explicitly move away from the (often implicit) conception of attention as a tool for gratifying desires.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17421v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gunter Bombaerts (Eindhoven University of Technology), Tom Hannes (Eindhoven University of Technology), Martin Adam (University of Victoria), Alessandra Aloisi (University of Oxford), Joel Anderson (Utrecht University), Lawrence Berger (Marist College), Stefano Davide Bettera (European Buddhist Union), Enrico Campo (University of Milan), Laura Candiotto (University of Pardubice), Silvia Caprioglio Panizza (University of Pardubice), Yves Citton (University of Paris 8), Diego D'Angelo (Wurzburg University), Matthew Dennis (Eindhoven University of Technology), Natalie Depraz (University of Rouen), Peter Doran (Queens University Belfast), Wolfgang Drechsler (Tallinn University of Technology), Bill Duane (Bill Duane and Associates), William Edelglass (Barre Center for Buddhist Studies), Iris Eisenberger (University of Vienna), Beverley Foulks McGuire (University of North Carolina Wilmington), Antony Fredriksson (University of Pardubice), Karamjit S. Gill (University of Brighton), Peter D. Hershock (East-West Center), Soraj Hongladarom (Chulalongkorn University), Beth Jacobs (independent writer), Gabor Karsai (Dharma Gate Buddhist College), Thomas Lennerfors (Uppsala University), Jeanne Lim (Being AI Corporation), Chien-Te Lin (Tzu-Chi University), Mark Losoncz (University of Belgrade), David Loy (independent researcher), Lavinia Marin (TU Delft), Bence Peter Marosan (Budapest Business University), Chiara Mascarello (Italian Buddhist Union Research Centre), David McMahan (Franklin &amp; Marshall College), Jin Y. Park (American University), Nina Petek (University of Ljubljana), Anna Puzio (University of Twente), Katrien Schaubroek (University of Antwerp), Jens Schlieter (University of Bern), Brian Schroeder (Rochester Institute of Technology), Shobhit Shakya (Tallinn University of Technology), Juewei Shi (Nan Tien Institute), Elizaveta Solomonova (McGill University), Francesco Tormen (Italian Buddhist Union Research Centre), Jitendra Uttam (Jawaharlal Nehru University), Marieke Van Vugt (University of Groningen), Sebastjan Voros (University of Ljubljana), Maren Wehrle (Erasmus University Rotterdam), Galit Wellner (Tel Aviv University), Jason M. Wirth (Seattle University), Olaf Witkowski (University of Tokyo), Apiradee Wongkitrungrueng (Mahidol University), Dale S. Wright (Occidental College), Yutong Zheng (The Chinese University of Hong Kong)</dc:creator>
    </item>
    <item>
      <title>A Time-Aware Approach to Early Detection of Anorexia: UNSL at eRisk 2024</title>
      <link>https://arxiv.org/abs/2410.17963</link>
      <description>arXiv:2410.17963v1 Announce Type: new 
Abstract: The eRisk laboratory aims to address issues related to early risk detection on the Web. In this year's edition, three tasks were proposed, where Task 2 was about early detection of signs of anorexia. Early risk detection is a problem where precision and speed are two crucial objectives. Our research group solved Task 2 by defining a CPI+DMC approach, addressing both objectives independently, and a time-aware approach, where precision and speed are considered a combined single-objective. We implemented the last approach by explicitly integrating time during the learning process, considering the ERDE{\theta} metric as the training objective. It also allowed us to incorporate temporal metrics to validate and select the optimal models. We achieved outstanding results for the ERDE50 metric and ranking-based metrics, demonstrating consistency in solving ERD problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17963v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>CEUR Workshop Proceedings 2024, vol. 3740, pp. 902-907</arxiv:journal_reference>
      <dc:creator>Horacio Thompson, Marcelo Errecalde</dc:creator>
    </item>
    <item>
      <title>An Effective Theory of Bias Amplification</title>
      <link>https://arxiv.org/abs/2410.17263</link>
      <description>arXiv:2410.17263v1 Announce Type: cross 
Abstract: Machine learning models may capture and amplify biases present in data, leading to disparate test performance across social groups. To better understand, evaluate, and mitigate these possible biases, a deeper theoretical understanding of how model design choices and data distribution properties could contribute to bias is needed. In this work, we contribute a precise analytical theory in the context of ridge regression, both with and without random projections, where the former models neural networks in a simplified regime. Our theory offers a unified and rigorous explanation of machine learning bias, providing insights into phenomena such as bias amplification and minority-group bias in various feature and parameter regimes. For example, we demonstrate that there may be an optimal regularization penalty or training time to avoid bias amplification, and there can be fundamental differences in test error between groups that do not vanish with increased parameterization. Importantly, our theoretical predictions align with several empirical observations reported in the literature. We extensively empirically validate our theory on diverse synthetic and semi-synthetic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17263v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arjun Subramonian, Sam Bell, Levent Sagun, Elvis Dohmatob</dc:creator>
    </item>
    <item>
      <title>Metrics for Assessing Inclusivity and Empowerment of People for Supporting the Design of Inclusive Product Lifecycles</title>
      <link>https://arxiv.org/abs/2410.17287</link>
      <description>arXiv:2410.17287v1 Announce Type: cross 
Abstract: Design of an inclusive product lifecycle is important for empowering people (stakeholders) with their meaningful inclusion in lifecycle processes. The aim is to use this as an enabler for transition to sustainability by balancing the power relations among the stakeholders. Design of an inclusive product lifecycle for empowerment requires that the nature of inclusion of stakeholders in the lifecycle is such that it leads to their empowerment. Empowerment processes provide opportunities for people to increase their power to sustain the development of inclusive product lifecycles. Analysing power relations is to balance the amount of power of stakeholders with their inclusion in different functions in an inclusive lifecycle design. Inclusivity addresses the context of the lifecycle process to determine who can be included in which phases of the lifecycle and the diversity of people to be empowered. We apply a novel empowerment and inclusivity framework to a series of real-life case studies from literature to identify the major dimensions of empowerment and inclusivity. By analysing the relationships between the dimensions of empowerment and inclusivity, we propose specific metrics for inclusivity and empowerment that have strong causal connections, indicating the kinds of inclusion that should lead to greater empowerment in product lifecycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17287v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naz Yaldiz, Amaresh Chakrabarti</dc:creator>
    </item>
    <item>
      <title>Literature Meets Data: A Synergistic Approach to Hypothesis Generation</title>
      <link>https://arxiv.org/abs/2410.17309</link>
      <description>arXiv:2410.17309v1 Announce Type: cross 
Abstract: AI holds promise for transforming scientific processes, including hypothesis generation. Prior work on hypothesis generation can be broadly categorized into theory-driven and data-driven approaches. While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other. To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation. We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97\% over few-shot, 15.75\% over literature-based alone, and 3.37\% over data-driven alone). Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection. Our results show that human accuracy improves significantly by 7.44\% and 14.19\% on these tasks, respectively. These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17309v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan</dc:creator>
    </item>
    <item>
      <title>Are Large Language Models Ready for Travel Planning?</title>
      <link>https://arxiv.org/abs/2410.17333</link>
      <description>arXiv:2410.17333v1 Announce Type: cross 
Abstract: While large language models (LLMs) show promise in hospitality and tourism, their ability to provide unbiased service across demographic groups remains unclear. This paper explores gender and ethnic biases when LLMs are utilized as travel planning assistants. To investigate this issue, we apply machine learning techniques to analyze travel suggestions generated from three open-source LLMs. Our findings reveal that the performance of race and gender classifiers substantially exceeds random chance, indicating differences in how LLMs engage with varied subgroups. Specifically, outputs align with cultural expectations tied to certain races and genders. To minimize the effect of these stereotypes, we used a stop-word classification strategy, which decreased identifiable differences, with no disrespectful terms found. However, hallucinations related to African American and gender minority groups were noted. In conclusion, while LLMs can generate travel plans seemingly free from bias, it remains essential to verify the accuracy and appropriateness of their recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17333v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ruiping Ren, Xing Yao, Shu Cole, Haining Wang</dc:creator>
    </item>
    <item>
      <title>End-to-End Optimization and Learning of Fair Court Schedules</title>
      <link>https://arxiv.org/abs/2410.17415</link>
      <description>arXiv:2410.17415v1 Announce Type: cross 
Abstract: Criminal courts across the United States handle millions of cases every year, and the scheduling of those cases must accommodate a diverse set of constraints, including the preferences and availability of courts, prosecutors, and defense teams. When criminal court schedules are formed, defendants' scheduling preferences often take the least priority, although defendants may face significant consequences (including arrest or detention) for missed court dates. Additionally, studies indicate that defendants' nonappearances impose costs on the courts and other system stakeholders. To address these issues, courts and commentators have begun to recognize that pretrial outcomes for defendants and for the system would be improved with greater attention to court processes, including \emph{court scheduling practices}. There is thus a need for fair criminal court pretrial scheduling systems that account for defendants' preferences and availability, but the collection of such data poses logistical challenges. Furthermore, optimizing schedules fairly across various parties' preferences is a complex optimization problem, even when such data is available. In an effort to construct such a fair scheduling system under data uncertainty, this paper proposes a joint optimization and learning framework that combines machine learning models trained end-to-end with efficient matching algorithms. This framework aims to produce court scheduling schedules that optimize a principled measure of fairness, balancing the availability and preferences of all parties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17415v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>My H Dinh, James Kotary, Lauryn P. Gouldin, William Yeoh, Ferdinando Fioretto</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence in Brazilian News: A Mixed-Methods Analysis</title>
      <link>https://arxiv.org/abs/2410.17423</link>
      <description>arXiv:2410.17423v1 Announce Type: cross 
Abstract: The current surge in Artificial Intelligence (AI) interest, reflected in heightened media coverage since 2009, has sparked significant debate on AI's implications for privacy, social justice, workers' rights, and democracy. The media plays a crucial role in shaping public perception and acceptance of AI technologies. However, research into how AI appears in media has primarily focused on anglophone contexts, leaving a gap in understanding how AI is represented globally. This study addresses this gap by analyzing 3,560 news articles from Brazilian media published between July 1, 2023, and February 29, 2024, from 13 popular online news outlets. Using Computational Grounded Theory (CGT), the study applies Latent Dirichlet Allocation (LDA), BERTopic, and Named-Entity Recognition to investigate the main topics in AI coverage and the entities represented. The findings reveal that Brazilian news coverage of AI is dominated by topics related to applications in the workplace and product launches, with limited space for societal concerns, which mostly focus on deepfakes and electoral integrity. The analysis also highlights a significant presence of industry-related entities, indicating a strong influence of corporate agendas in the country's news. This study underscores the need for a more critical and nuanced discussion of AI's societal impacts in Brazilian media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17423v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Hernandes, Giulio Corsi</dc:creator>
    </item>
    <item>
      <title>Revisiting Technical Bias Mitigation Strategies</title>
      <link>https://arxiv.org/abs/2410.17433</link>
      <description>arXiv:2410.17433v1 Announce Type: cross 
Abstract: Efforts to mitigate bias and enhance fairness in the artificial intelligence (AI) community have predominantly focused on technical solutions. While numerous reviews have addressed bias in AI, this review uniquely focuses on the practical limitations of technical solutions in healthcare settings, providing a structured analysis across five key dimensions affecting their real-world implementation: who defines bias and fairness; which mitigation strategy to use and prioritize among dozens that are inconsistent and incompatible; when in the AI development stages the solutions are most effective; for which populations; and the context in which the solutions are designed. We illustrate each limitation with empirical studies focusing on healthcare and biomedical applications. Moreover, we discuss how value-sensitive AI, a framework derived from technology design, can engage stakeholders and ensure that their values are embodied in bias and fairness mitigation solutions. Finally, we discuss areas that require further investigation and provide practical recommendations to address the limitations covered in the study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17433v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abdoul Jalil Djiberou Mahamadou, Artem A. Trotsyuk</dc:creator>
    </item>
    <item>
      <title>Data Obfuscation through Latent Space Projection (LSP) for Privacy-Preserving AI Governance: Case Studies in Medical Diagnosis and Finance Fraud Detection</title>
      <link>https://arxiv.org/abs/2410.17459</link>
      <description>arXiv:2410.17459v1 Announce Type: cross 
Abstract: As AI systems increasingly integrate into critical societal sectors, the demand for robust privacy-preserving methods has escalated. This paper introduces Data Obfuscation through Latent Space Projection (LSP), a novel technique aimed at enhancing AI governance and ensuring Responsible AI compliance. LSP uses machine learning to project sensitive data into a latent space, effectively obfuscating it while preserving essential features for model training and inference. Unlike traditional privacy methods like differential privacy or homomorphic encryption, LSP transforms data into an abstract, lower-dimensional form, achieving a delicate balance between data utility and privacy. Leveraging autoencoders and adversarial training, LSP separates sensitive from non-sensitive information, allowing for precise control over privacy-utility trade-offs. We validate LSP's effectiveness through experiments on benchmark datasets and two real-world case studies: healthcare cancer diagnosis and financial fraud analysis. Our results show LSP achieves high performance (98.7% accuracy in image classification) while providing strong privacy (97.3% protection against sensitive attribute inference), outperforming traditional anonymization and privacy-preserving methods. The paper also examines LSP's alignment with global AI governance frameworks, such as GDPR, CCPA, and HIPAA, highlighting its contribution to fairness, transparency, and accountability. By embedding privacy within the machine learning pipeline, LSP offers a promising approach to developing AI systems that respect privacy while delivering valuable insights. We conclude by discussing future research directions, including theoretical privacy guarantees, integration with federated learning, and enhancing latent space interpretability, positioning LSP as a critical tool for ethical AI advancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17459v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahesh Vaijainthymala Krishnamoorthy</dc:creator>
    </item>
    <item>
      <title>BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers</title>
      <link>https://arxiv.org/abs/2410.17492</link>
      <description>arXiv:2410.17492v1 Announce Type: cross 
Abstract: Attacking fairness is crucial because compromised models can introduce biased outcomes, undermining trust and amplifying inequalities in sensitive applications like hiring, healthcare, and law enforcement. This highlights the urgent need to understand how fairness mechanisms can be exploited and to develop defenses that ensure both fairness and robustness. We introduce BadFair, a novel backdoored fairness attack methodology. BadFair stealthily crafts a model that operates with accuracy and fairness under regular conditions but, when activated by certain triggers, discriminates and produces incorrect results for specific groups. This type of attack is particularly stealthy and dangerous, as it circumvents existing fairness detection methods, maintaining an appearance of fairness in normal use. Our findings reveal that BadFair achieves a more than 85% attack success rate in attacks aimed at target groups on average while only incurring a minimal accuracy loss. Moreover, it consistently exhibits a significant discrimination score, distinguishing between pre-defined target and non-target attacked groups across various datasets and models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17492v1</guid>
      <category>cs.CR</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Xue, Qian Lou, Mengxin Zheng</dc:creator>
    </item>
    <item>
      <title>DisenGCD: A Meta Multigraph-assisted Disentangled Graph Learning Framework for Cognitive Diagnosis</title>
      <link>https://arxiv.org/abs/2410.17564</link>
      <description>arXiv:2410.17564v1 Announce Type: cross 
Abstract: Existing graph learning-based cognitive diagnosis (CD) methods have made relatively good results, but their student, exercise, and concept representations are learned and exchanged in an implicit unified graph, which makes the interaction-agnostic exercise and concept representations be learned poorly, failing to provide high robustness against noise in students' interactions. Besides, lower-order exercise latent representations obtained in shallow layers are not well explored when learning the student representation. To tackle the issues, this paper suggests a meta multigraph-assisted disentangled graph learning framework for CD (DisenGCD), which learns three types of representations on three disentangled graphs: student-exercise-concept interaction, exercise-concept relation, and concept dependency graphs, respectively. Specifically, the latter two graphs are first disentangled from the interaction graph. Then, the student representation is learned from the interaction graph by a devised meta multigraph learning module; multiple learnable propagation paths in this module enable current student latent representation to access lower-order exercise latent representations, which can lead to more effective nad robust student representations learned; the exercise and concept representations are learned on the relation and dependency graphs by graph attention modules. Finally, a novel diagnostic function is devised to handle three disentangled representations for prediction. Experiments show better performance and robustness of DisenGCD than state-of-the-art CD methods and demonstrate the effectiveness of the disentangled learning framework and meta multigraph module. The source code is available at \textcolor{red}{\url{https://github.com/BIMK/Intelligent-Education/tree/main/DisenGCD}}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17564v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shangshang Yang, Mingyang Chen, Ziwen Wang, Xiaoshan Yu, Panpan Zhang, Haiping Ma, Xingyi Zhang</dc:creator>
    </item>
    <item>
      <title>Extending and Applying Automated HERMES Software Publication Workflows</title>
      <link>https://arxiv.org/abs/2410.17614</link>
      <description>arXiv:2410.17614v1 Announce Type: cross 
Abstract: Research software is an import output of research and must be published according to the FAIR Principles for Research Software. This can be achieved by publishing software with metadata under a persistent identifier. HERMES is a tool that leverages continuous integration to automate the publication of software with rich metadata. In this work, we describe the HERMES workflow itself, and how to extend it to meet the needs of specific research software metadata or infrastructure. We introduce the HERMES plugin architecture and provide the example of creating a new HERMES plugin that harvests metadata from a metadata source in source code repositories. We show how to use HERMES as an end user, both via the command line interface, and as a step in a continuous integration pipeline. Finally, we report three informal case studies whose results provide a preliminary evaluation of the feasibility and applicability of HERMES workflows, and the extensibility of the hermes software package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17614v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sophie Kernchen, Michael Meinel, Stephan Druskat, Michael Fritzsche, David Pape, Oliver Bertuch</dc:creator>
    </item>
    <item>
      <title>Mapping the Media Landscape: Predicting Factual Reporting and Political Bias Through Web Interactions</title>
      <link>https://arxiv.org/abs/2410.17655</link>
      <description>arXiv:2410.17655v1 Announce Type: cross 
Abstract: Bias assessment of news sources is paramount for professionals, organizations, and researchers who rely on truthful evidence for information gathering and reporting. While certain bias indicators are discernible from content analysis, descriptors like political bias and fake news pose greater challenges. In this paper, we propose an extension to a recently presented news media reliability estimation method that focuses on modeling outlets and their longitudinal web interactions. Concretely, we assess the classification performance of four reinforcement learning strategies on a large news media hyperlink graph. Our experiments, targeting two challenging bias descriptors, factual reporting and political bias, showed a significant performance improvement at the source media level. Additionally, we validate our methods on the CLEF 2023 CheckThat! Lab challenge, outperforming the reported results in both, F1-score and the official MAE metric. Furthermore, we contribute by releasing the largest annotated dataset of news source media, categorized with factual reporting and political bias labels. Our findings suggest that profiling news media sources based on their hyperlink interactions over time is feasible, offering a bird's-eye view of evolving media landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17655v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-71736-9_7</arxiv:DOI>
      <dc:creator>Dairazalia S\'anchez-Cort\'es, Sergio Burdisso, Esa\'u Villatoro-Tello, Petr Motlicek</dc:creator>
    </item>
    <item>
      <title>Local Contrastive Editing of Gender Stereotypes</title>
      <link>https://arxiv.org/abs/2410.17739</link>
      <description>arXiv:2410.17739v1 Announce Type: cross 
Abstract: Stereotypical bias encoded in language models (LMs) poses a threat to safe language technology, yet our understanding of how bias manifests in the parameters of LMs remains incomplete. We introduce local contrastive editing that enables the localization and editing of a subset of weights in a target model in relation to a reference model. We deploy this approach to identify and modify subsets of weights that are associated with gender stereotypes in LMs. Through a series of experiments, we demonstrate that local contrastive editing can precisely localize and control a small subset (&lt; 0.5%) of weights that encode gender bias. Our work (i) advances our understanding of how stereotypical biases can manifest in the parameter space of LMs and (ii) opens up new avenues for developing parameter-efficient strategies for controlling model properties in a contrastive manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17739v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marlene Lutz, Rochelle Choenni, Markus Strohmaier, Anne Lauscher</dc:creator>
    </item>
    <item>
      <title>Optimizing Travel Itineraries with AI Algorithms in a Microservices Architecture: Balancing Cost, Time, Preferences, and Sustainability</title>
      <link>https://arxiv.org/abs/2410.17943</link>
      <description>arXiv:2410.17943v1 Announce Type: cross 
Abstract: The objective of this research is how an implementation of AI algorithms in the microservices architecture enhances travel itineraries by cost, time, user preferences, and environmental sustainability. It uses machine learning models for both cost forecasting and personalization, genetic algorithm for optimization of the itinerary, and heuristics for sustainability checking. Primary evaluated parameters consist of latency, ability to satisfy user preferences, cost and environmental concern. The experimental results demonstrate an average of 4.5 seconds of response time on 1000 concurrent users and 92% of user preferences accuracy. The cost efficiency is proved, with 95% of provided trips being within the limits of the budget declared by the user. The system also implements some measures to alleviate negative externalities related to travel and 60% of offered travel plans had green options incorporated, resulting in the average 15% lower carbon emissions than the traditional travel plans offered. The genetic algorithm with time complexity O(g.p.f) provides the optimal solution in 100 generations. Every iteration improves the quality of the solution by 5%, thus enabling its effective use in optimization problems where time is measured in seconds. Finally, the system is designed to be fault-tolerant with functional 99.9% availability which allows the provision of services even when requirements are exceeded. Travel optimization platform is turned dynamic and efficient by this microservices based architecture which provides enhanced scaling, allows asynchronous communication and real time changes. Because of the incorporation of Ai, cost control and eco-friendliness approaches, the system addresses the different user needs in the present days travel business.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17943v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biman Barua, M. Shamim Kaiser</dc:creator>
    </item>
    <item>
      <title>The Web unpacked: a quantitative analysis of global Web usage</title>
      <link>https://arxiv.org/abs/2404.17095</link>
      <description>arXiv:2404.17095v2 Announce Type: replace 
Abstract: This paper presents a comprehensive analysis of global web usage patterns based on data from SimilarWeb, a leading source for estimating web traffic. Leveraging a dataset comprising over 250,000 websites, we estimate the total web traffic and investigate its distribution among domains and industry sectors. We detail the characteristics of the top 116 domains, which comprise an estimated one-third of all web traffic. Our analysis scrutinizes various attributes of these domains, including their content sources and types, access requirements, offline presence, and ownership features. Our analysis reveals a significant concentration of web traffic, with a diminutive number of top websites capturing the majority of visits. Search engines, news and media, social networks, streaming, and adult content emerge as primary attractors of web traffic, which is also highly concentrated on platforms and USA-owned websites. Much of the traffic goes to for-profit but mostly free-of-charge websites, highlighting the dominance of business models not based on paywalls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17095v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Henrique S. Xavier</dc:creator>
    </item>
    <item>
      <title>The Impact of Knowledge Silos on Responsible AI Practices in Journalism</title>
      <link>https://arxiv.org/abs/2410.01138</link>
      <description>arXiv:2410.01138v2 Announce Type: replace 
Abstract: The effective adoption of responsible AI practices in journalism requires a concerted effort to bridge different perspectives, including technological, editorial, journalistic, and managerial. Among the many challenges that could impact information sharing around responsible AI inside news organizations are knowledge silos, where information is isolated within one part of the organization and not easily shared with others. This study aims to explore if, and if so, how, knowledge silos affect the adoption of responsible AI practices in journalism through a cross-case study of four major Dutch media outlets. We examine the individual and organizational barriers to AI knowledge sharing and the extent to which knowledge silos could impede the operationalization of responsible AI initiatives inside newsrooms. To address this question, we conducted 14 semi-structured interviews with editors, managers, and journalists at de Telegraaf, de Volkskrant, the Nederlandse Omroep Stichting (NOS), and RTL Nederland. The interviews aimed to uncover insights into the existence of knowledge silos, their effects on responsible AI practice adoption, and the organizational practices influencing these dynamics. Our results emphasize the importance of creating better structures for sharing information on AI across all layers of news organizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01138v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tom\'as Dodds, Astrid Vandendaele, Felix M. Simon, Natali Helberger, Valeria Resendez, Wang Ngai Yeung</dc:creator>
    </item>
    <item>
      <title>On Catastrophic Inheritance of Large Foundation Models</title>
      <link>https://arxiv.org/abs/2402.01909</link>
      <description>arXiv:2402.01909v2 Announce Type: replace-cross 
Abstract: Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inheritance on downstream tasks, and how to Mitigate it. UIM aims to unite both the machine learning and social sciences communities for more responsible and promising AI development and deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01909v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Chen, Bhiksha Raj, Xing Xie, Jindong Wang</dc:creator>
    </item>
    <item>
      <title>Generative AI Security: Challenges and Countermeasures</title>
      <link>https://arxiv.org/abs/2402.12617</link>
      <description>arXiv:2402.12617v2 Announce Type: replace-cross 
Abstract: Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12617v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Banghua Zhu, Norman Mu, Jiantao Jiao, David Wagner</dc:creator>
    </item>
    <item>
      <title>STAR: SocioTechnical Approach to Red Teaming Language Models</title>
      <link>https://arxiv.org/abs/2406.11757</link>
      <description>arXiv:2406.11757v4 Announce Type: replace-cross 
Abstract: This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11757v4</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Weidinger, John Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac</dc:creator>
    </item>
    <item>
      <title>AskBeacon -- Performing genomic data exchange and analytics with natural language</title>
      <link>https://arxiv.org/abs/2410.16700</link>
      <description>arXiv:2410.16700v2 Announce Type: replace-cross 
Abstract: Enabling clinicians and researchers to directly interact with global genomic data resources by removing technological barriers is vital for medical genomics. AskBeacon enables Large Language Models to be applied to securely shared cohorts via the GA4GH Beacon protocol. By simply "asking" Beacon, actionable insights can be gained, analyzed and made publication-ready.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16700v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>q-bio.GN</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anuradha Wickramarachchi, Shakila Tonni, Sonali Majumdar, Sarvnaz Karimi, Sulev K\~oks, Brendan Hosking, Jordi Rambla, Natalie A. Twine, Yatish Jain, Denis C. Bauer</dc:creator>
    </item>
  </channel>
</rss>
