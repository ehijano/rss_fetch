<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Fully Dynamic Strongly Connected Components in Planar Digraphs</title>
      <link>https://arxiv.org/abs/2406.10420</link>
      <description>arXiv:2406.10420v1 Announce Type: new 
Abstract: In this paper, we consider maintaining strongly connected components (SCCs) of a directed planar graph subject to edge insertions and deletions. We show a data structure maintaining an implicit representation of the SCCs within $\tilde{O}(n^{6/7})$ worst-case time per update. The data structure supports, in $O(\log^2{n})$ time, reporting vertices of any specified SCC (with constant overhead per reported vertex) and aggregating vertex information (e.g., computing the maximum label) over all the vertices of that SCC. Furthermore, it can maintain global information about the structure of SCCs, such as the number of SCCs or the size of the largest SCC.
  To the best of our knowledge, no fully dynamic SCCs data structures with sublinear update time have been previously known for any major subclass of digraphs. Our result should be contrasted with the known $n^{1-o(1)}$ amortized update time lower bound conditional on SETH, which holds even for dynamically maintaining whether a general digraph has more than two SCCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10420v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Karczmarz, Marcin Smulewicz</dc:creator>
    </item>
    <item>
      <title>Scheduling two types of jobs with minimum makespan</title>
      <link>https://arxiv.org/abs/2406.10467</link>
      <description>arXiv:2406.10467v1 Announce Type: new 
Abstract: We consider scheduling two types of jobs (A-job and B-job) to $p$ machines and minimizing their makespan. A group of same type of jobs processed consecutively by a machine is called a batch. For machine $v$, processing $x$ A-jobs in a batch takes $k^A_vx^2$ time units for a given speed $k^A_v$, and processing $x$ B-jobs in a batch takes $k^B_vx^2$ time units for a given speed $k^B_v$. We give an $O(n^2p\log(n))$ algorithm based on dynamic programming and binary search for solving this problem, where $n$ denotes the maximal number of A-jobs and B-jobs to be distributed to the machines. Our algorithm also fits the easier linear case where each batch of length $x$ of $A$-jobs takes $k^A_v x$ time units and each batch of length $x$ of $B$-jobs takes $k^B_vx$ time units. The running time is the same as the above case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10467v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Song Cao, Kai Jin</dc:creator>
    </item>
    <item>
      <title>Scalable Temporal Motif Densest Subnetwork Discovery</title>
      <link>https://arxiv.org/abs/2406.10608</link>
      <description>arXiv:2406.10608v1 Announce Type: new 
Abstract: Finding dense subnetworks, with density based on edges or more complex structures, such as subgraphs or $k$-cliques, is a fundamental algorithmic problem with many applications. While the problem has been studied extensively in static networks, much remains to be explored for temporal networks.
  In this work we introduce the novel problem of identifying the temporal motif densest subnetwork, i.e., the densest subnetwork with respect to temporal motifs, which are high-order patterns characterizing temporal networks. This problem significantly differs from analogous formulations for dense temporal (or static) subnetworks as these do not account for temporal motifs. Identifying temporal motifs is an extremely challenging task, and thus, efficient methods are required. To this end, we design two novel randomized approximation algorithms with rigorous probabilistic guarantees that provide high-quality solutions. We perform extensive experiments showing that our methods outperform baselines. Furthermore, our algorithms scale on networks with up to billions of temporal edges, while baselines cannot handle such large networks. We use our techniques to analyze a financial network and show that our formulation reveals important network structures, such as bursty temporal events and communities of users with similar interests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10608v1</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671889</arxiv:DOI>
      <dc:creator>Ilie Sarpe, Fabio Vandin, Aristides Gionis</dc:creator>
    </item>
    <item>
      <title>String Partition for Building Big BWTs</title>
      <link>https://arxiv.org/abs/2406.10610</link>
      <description>arXiv:2406.10610v1 Announce Type: new 
Abstract: The Burrows-Wheeler transform (BWT) is a reversible text transformation used extensively in text compression, indexing, and bioinformatics, particularly in the alignment of short reads. However, constructing the BWT for long strings poses significant challenges. We introduce a novel approach to partition a long string into shorter substrings, enabling the use of multi-string BWT construction algorithms to process these inputs. The approach partitions based on a prefix of the suffix array and we provide an implementation for DNA sequences. Through comparison with state-of-the-art BWT construction algorithms, we demonstrate a speed improvement of approximately 12% on a real genome dataset consisting of 3.2 billion characters. The proposed partitioning strategy is applicable to strings of any alphabet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10610v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enno Adler, Stefan B\"ottcher, Rita Hartel</dc:creator>
    </item>
    <item>
      <title>Solving Co-Path/Cycle Packing Faster than $3^k$</title>
      <link>https://arxiv.org/abs/2406.10829</link>
      <description>arXiv:2406.10829v1 Announce Type: new 
Abstract: The \textsc{Co-Path/Cycle Packing} problem asks whether we can delete at most $k$ vertices from the input graph such that the remaining graph is a collection of induced paths and cycles. \textsc{Co-Path/Cycle Packing} is a fundamental graph problem that has important applications in bioinformatics. Although this problem has been extensively studied in parameterized algorithms, it seems hard to break the running time bound $3^k$. In 2015, Feng et al. provided an $O^*(3^k)$-time randomized algorithm. Recently, Tsur showed that this problem can be solved in $O^*(3^k)$ time deterministically. In this paper, by combining several techniques such as path decomposition, dynamic programming, and branch-and-search methods, we show that \textsc{Co-Path/Cycle Packing} can be solved in $O^*(2.8192^k)$ time. As a by-product, we also show that the \textsc{$d$-Bounded-Degree Vertex Deletion} problem, a generalization of \textsc{Co-Path/Cycle Packing}, can be solved in $O^*((d + 2)^p)$ time if a path decomposition of width $p$ is given, which implies that \textsc{$d$-Bounded-Degree Vertex Deletion} is FPT with parameter $p+d$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10829v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxi Liu, Mingyu Xiao</dc:creator>
    </item>
    <item>
      <title>Perturbation-Resilient Sets for Dynamic Service Balancing</title>
      <link>https://arxiv.org/abs/2406.11075</link>
      <description>arXiv:2406.11075v1 Announce Type: new 
Abstract: A combinatorial trade is a pair of sets of blocks of elements that can be exchanged while preserving relevant subset intersection constraints. The class of balanced and swap-robust minimal trades was proposed in [1] for exchanging blocks of data chunks stored on distributed storage systems in an access- and load-balanced manner. More precisely, data chunks in the trades of interest are labeled by popularity ranks and the blocks are required to have both balanced overall popularity and stability properties with respect to swaps in chunk popularities. The original construction of such trades relied on computer search and paired balanced sets obtained through iterative combining of smaller sets that have provable stability guarantees. To reduce the substantial gap between the results of prior approaches and the known theoretical lower bound, we present new analytical upper and lower bounds on the minimal disbalance of blocks introduced by limited-magnitude popularity ranking swaps. Our constructive and near-optimal approach relies on pairs of graphs whose vertices are two balanced sets with edges/arcs that capture the balance and potential balance changes induced by limited-magnitude popularity swaps. In particular, we show that if we start with carefully selected balanced trades and limit the magnitude of rank swaps to one, the new upper and lower bound on the maximum block disbalance caused by a swap only differ by a factor of $1.07$. We also extend these results for larger popularity swap magnitudes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11075v1</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Sima, Chao Pan, Olgica Milenkovic</dc:creator>
    </item>
    <item>
      <title>Almost Linear Size Edit Distance Sketch</title>
      <link>https://arxiv.org/abs/2406.11225</link>
      <description>arXiv:2406.11225v1 Announce Type: new 
Abstract: Edit distance is an important measure of string similarity. It counts the number of insertions, deletions and substitutions one has to make to a string $x$ to get a string $y$. In this paper we design an almost linear-size sketching scheme for computing edit distance up to a given threshold $k$. The scheme consists of two algorithms, a sketching algorithm and a recovery algorithm. The sketching algorithm depends on the parameter $k$ and takes as input a string $x$ and a public random string $\rho$ and computes a sketch $sk_{\rho}(x;k)$, which is a digested version of $x$. The recovery algorithm is given two sketches $sk_{\rho}(x;k)$ and $sk_{\rho}(y;k)$ as well as the public random string $\rho$ used to create the two sketches, and (with high probability) if the edit distance $ED(x,y)$ between $x$ and $y$ is at most $k$, will output $ED(x,y)$ together with an optimal sequence of edit operations that transforms $x$ to $y$, and if $ED(x,y) &gt; k$ will output LARGE. The size of the sketch output by the sketching algorithm on input $x$ is $k{2^{O(\sqrt{\log(n)\log\log(n)})}}$ (where $n$ is an upper bound on length of $x$). The sketching and recovery algorithms both run in time polynomial in $n$. The dependence of sketch size on $k$ is information theoretically optimal and improves over the quadratic dependence on $k$ in schemes of Kociumaka, Porat and Starikovskaya (FOCS'2021), and Bhattacharya and Kouck\'y (STOC'2023).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11225v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michal Kouck\'y, Michael Saks</dc:creator>
    </item>
    <item>
      <title>Diagram Control and Model Order for Sugiyama Layouts</title>
      <link>https://arxiv.org/abs/2406.11393</link>
      <description>arXiv:2406.11393v1 Announce Type: new 
Abstract: Graphical WYSIWYG editors programming languages are popular since they allow to control the diagram layout to express intention via secondary notation such as proximity and topology. However, such editors typically require users to do manual layout. Conversely, automatic layout of diagrams typically fails to capture intention because graphs are usually considered to not contain any order. Model order can combine the desire for control of secondary notation with automatic layout, without additional overhead, since the textual model already employs secondary notation. We illustrate how model order can exert control on the example of programming languages SCCharts and Lingua Franca. We also propose a first guidebook how such model order configurations can be extracted for other programming languages with a graphical notation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11393v1</guid>
      <category>cs.DS</category>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\"oren Domr\"os, Reinhard von Hanxleden</dc:creator>
    </item>
    <item>
      <title>Making Old Things New: A Unified Algorithm for Differentially Private Clustering</title>
      <link>https://arxiv.org/abs/2406.11649</link>
      <description>arXiv:2406.11649v1 Announce Type: new 
Abstract: As a staple of data analysis and unsupervised learning, the problem of private clustering has been widely studied under various privacy models. Centralized differential privacy is the first of them, and the problem has also been studied for the local and the shuffle variation. In each case, the goal is to design an algorithm that computes privately a clustering, with the smallest possible error. The study of each variation gave rise to new algorithms: the landscape of private clustering algorithms is therefore quite intricate.
  In this paper, we show that a 20-year-old algorithm can be slightly modified to work for any of these models. This provides a unified picture: while matching almost all previously known results, it allows us to improve some of them and extend it to a new privacy model, the continual observation setting, where the input is changing over time and the algorithm must output a new solution at each time step.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11649v1</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Dupr\'e la Tour, Monika Henzinger, David Saulpic</dc:creator>
    </item>
    <item>
      <title>Quantum Property Testing Algorithm for the Concatenation of Two Palindromes Language</title>
      <link>https://arxiv.org/abs/2406.11270</link>
      <description>arXiv:2406.11270v1 Announce Type: cross 
Abstract: In this paper, we present a quantum property testing algorithm for recognizing a context-free language that is a concatenation of two palindromes $L_{REV}$. The query complexity of our algorithm is $O(\frac{1}{\varepsilon}n^{1/3}\log n)$, where $n$ is the length of an input. It is better than the classical complexity that is $\Theta^*(\sqrt{n})$.
  At the same time, in the general setting, the picture is different a little. Classical query complexity is $\Theta(n)$, and quantum query complexity is $\Theta^*(\sqrt{n})$. So, we obtain polynomial speed-up for both cases (general and property testing).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11270v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-63742-1_10</arxiv:DOI>
      <arxiv:journal_reference>LNCS, Vol. 14776, 2024</arxiv:journal_reference>
      <dc:creator>Kamil Khadiev, Danil Serov</dc:creator>
    </item>
    <item>
      <title>Improved Algorithms for Contextual Dynamic Pricing</title>
      <link>https://arxiv.org/abs/2406.11316</link>
      <description>arXiv:2406.11316v1 Announce Type: cross 
Abstract: In contextual dynamic pricing, a seller sequentially prices goods based on contextual information. Buyers will purchase products only if the prices are below their valuations. The goal of the seller is to design a pricing strategy that collects as much revenue as possible. We focus on two different valuation models. The first assumes that valuations linearly depend on the context and are further distorted by noise. Under minor regularity assumptions, our algorithm achieves an optimal regret bound of $\tilde{\mathcal{O}}(T^{2/3})$, improving the existing results. The second model removes the linearity assumption, requiring only that the expected buyer valuation is $\beta$-H\"older in the context. For this model, our algorithm obtains a regret $\tilde{\mathcal{O}}(T^{d+2\beta/d+3\beta})$, where $d$ is the dimension of the context space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11316v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matilde Tullii, Solenne Gaucher, Nadav Merlis, Vianney Perchet</dc:creator>
    </item>
    <item>
      <title>Approximation Algorithms for Smallest Intersecting Balls</title>
      <link>https://arxiv.org/abs/2406.11369</link>
      <description>arXiv:2406.11369v1 Announce Type: cross 
Abstract: We study a general smallest intersecting ball problem and its soft-margin variant in high-dimensional Euclidean spaces, which only require the input objects to be compact and convex. These two problems link and unify a series of fundamental problems in computational geometry and machine learning, including smallest enclosing ball, polytope distance, intersection radius, $\ell_1$-loss support vector machine, $\ell_1$-loss support vector data description, and so on. Two general approximation algorithms are presented respectively, and implementation details are given for specific inputs of convex polytopes, reduced polytopes, axis-aligned bounding boxes, balls, and ellipsoids. For most of these inputs, our algorithms are the first results in high-dimensional spaces, and also the first approximation methods. To achieve this, we develop a novel framework for approximating zero-sum games in Euclidean Jordan algebra systems, which may be useful in its own right.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11369v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Zheng, Tiow-Seng Tan</dc:creator>
    </item>
    <item>
      <title>Treewidth Inapproximability and Tight ETH Lower Bound</title>
      <link>https://arxiv.org/abs/2406.11628</link>
      <description>arXiv:2406.11628v1 Announce Type: cross 
Abstract: We present a simple, self-contained, linear reduction from 3-SAT to Treewidth. Specifically, it shows that 1.00005-approximating Treewidth is NP-hard, and solving Treewidth exactly requires $2^{\Omega(n)}$ time, unless the Exponential-Time Hypothesis fails. We further derive, under the latter assumption, that there is some constant $\delta &gt; 1$ such that $\delta$-approximating Treewidth requires time $2^{n^{1-o(1)}}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11628v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Edouard Bonnet</dc:creator>
    </item>
    <item>
      <title>Daisy Bloom Filters</title>
      <link>https://arxiv.org/abs/2205.14894</link>
      <description>arXiv:2205.14894v2 Announce Type: replace 
Abstract: A filter is a widely used data structure for storing an approximation of a given set $S$ of elements from some universe $U$ (a countable set).It represents a superset $S'\supseteq S$ that is ''close to $S$'' in the sense that for $x\not\in S$, the probability that $x\in S'$ is bounded by some $\varepsilon &gt; 0$. The advantage of using a Bloom filter, when some false positives are acceptable, is that the space usage becomes smaller than what is required to store $S$ exactly.
  Though filters are well-understood from a worst-case perspective, it is clear that state-of-the-art constructions may not be close to optimal for particular distributions of data and queries. Suppose, for instance, that some elements are in $S$ with probability close to 1. Then it would make sense to always include them in $S'$, saving space by not having to represent these elements in the filter. Questions like this have been raised in the context of Weighted Bloom filters (Bruck, Gao and Jiang, ISIT 2006) and Bloom filter implementations that make use of access to learned components (Vaidya, Knorr, Mitzenmacher, and Krask, ICLR 2021).
  In this paper, we present a lower bound for the expected space that such a filter requires. We also show that the lower bound is asymptotically tight by exhibiting a filter construction that executes queries and insertions in worst-case constant time, and has a false positive rate at most $\varepsilon $ with high probability over input sets drawn from a product distribution. We also present a Bloom filter alternative, which we call the $\textit{Daisy Bloom filter}$, that executes operations faster and uses significantly less space than the standard Bloom filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.14894v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ioana O. Bercea, Jakob B{\ae}k Tejs Houen, Rasmus Pagh</dc:creator>
    </item>
    <item>
      <title>Which $L_p$ norm is the fairest? Approximations for fair facility location across all "$p$"</title>
      <link>https://arxiv.org/abs/2211.14873</link>
      <description>arXiv:2211.14873v3 Announce Type: replace 
Abstract: Fair facility location problems try to balance access costs to open facilities borne by different groups of people by minimizing the $L_p$ norm of these group distances. However, there is no clear choice of "$p$" in the current literature. We present a novel approach to address the challenge of choosing the right notion of fairness. We introduce the concept of portfolios, a set of solutions that contains an approximately optimal solution for each objective in a given class of objectives, such as $L_p$ norms. This concept opens up new possibilities for getting around the "right" notion of fairness for many problems. For $r$ client groups, we demonstrate portfolios of size $\Theta(\log r)$ for the facility location and $k$-clustering problems, with an $O(1)$-approximate solution for each $L_p$ norm. Further, motivated by the Justice40 Initiative that provides rolling budget investments, we impose a refinement-like structure on the portfolio. We develop novel approximation algorithms for these structured portfolios and show experimental evidence of their performance in two US counties. We also present a planning tool that provides potential ways to expand access to US healthcare facilities, which might be of independent interest to policymakers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14873v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Swati Gupta, Jai Moondra, Mohit Singh</dc:creator>
    </item>
    <item>
      <title>Robustness Implies Privacy in Statistical Estimation</title>
      <link>https://arxiv.org/abs/2212.05015</link>
      <description>arXiv:2212.05015v3 Announce Type: replace 
Abstract: We study the relationship between adversarial robustness and differential privacy in high-dimensional algorithmic statistics. We give the first black-box reduction from privacy to robustness which can produce private estimators with optimal tradeoffs among sample complexity, accuracy, and privacy for a wide range of fundamental high-dimensional parameter estimation problems, including mean and covariance estimation. We show that this reduction can be implemented in polynomial time in some important special cases. In particular, using nearly-optimal polynomial-time robust estimators for the mean and covariance of high-dimensional Gaussians which are based on the Sum-of-Squares method, we design the first polynomial-time private estimators for these problems with nearly-optimal samples-accuracy-privacy tradeoffs. Our algorithms are also robust to a nearly optimal fraction of adversarially-corrupted samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05015v3</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel B. Hopkins, Gautam Kamath, Mahbod Majid, Shyam Narayanan</dc:creator>
    </item>
    <item>
      <title>A New and Faster Representation for Counting Integer Points in Parametric Polyhedra</title>
      <link>https://arxiv.org/abs/2310.13788</link>
      <description>arXiv:2310.13788v5 Announce Type: replace 
Abstract: In this paper, we consider the counting function $E_P(y) = |P_{y} \cap Z^{n_x}|$ for a parametric polyhedron $P_{y} = \{x \in R^{n_x} \colon A x \leq b + B y\}$, where $y \in R^{n_y}$. We give a new representation of $E_P(y)$, called a \emph{piece-wise step-polynomial with periodic coefficients}, which is a generalization of piece-wise step-polynomials and integer/rational Ehrhart's quasi-polynomials. It gives the fastest way to calculate $E_P(y)$ in certain scenarios. The most important cases are the following:
  1) We show that, for the parametric polyhedron $P_y$ defined by a standard-form system $A x = y,\, x \geq 0$ with a fixed number of equalities, the function $E_P(y)$ can be represented by a polynomial-time computable function. In turn, such a representation of $E_P(y)$ can be constructed by an $poly\bigl(n, \|A\|_{\infty}\bigr)$-time algorithm;
  2) Assuming again that the number of equalities is fixed, we show that integer/rational Ehrhart's quasi-polynomials of a polytope can be computed by FPT-algorithms, parameterized by sub-determinants of $A$ or its elements;
  3) Our representation of $E_P$ is more efficient than other known approaches, if $A$ has bounded elements, especially if it is sparse in addition.
  Additionally, we provide a discussion about possible applications in the area of compiler optimization. In some "natural" assumptions on a program code, our approach has the fastest complexity bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13788v5</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>D. Gribanov, D. Malyshev, P. Pardalos, N. Zolotykh</dc:creator>
    </item>
    <item>
      <title>Moderate Dimension Reduction for $k$-Center Clustering</title>
      <link>https://arxiv.org/abs/2312.01391</link>
      <description>arXiv:2312.01391v3 Announce Type: replace 
Abstract: The Johnson-Lindenstrauss (JL) Lemma introduced the concept of dimension reduction via a random linear map, which has become a fundamental technique in many computational settings. For a set of $n$ points in $\mathbb{R}^d$ and any fixed $\epsilon&gt;0$, it reduces the dimension $d$ to $O(\log n)$ while preserving, with high probability, all the pairwise Euclidean distances within factor $1+\epsilon$. Perhaps surprisingly, the target dimension can be lower if one only wishes to preserve the optimal value of a certain problem on the pointset, e.g., Euclidean max-cut or $k$-means. However, for some notorious problems, like diameter (aka furthest pair), dimension reduction via the JL map to below $O(\log n)$ does not preserve the optimal value within factor $1+\epsilon$.
  We propose to focus on another regime, of \emph{moderate dimension reduction}, where a problem's value is preserved within factor $\alpha&gt;1$ using target dimension $\tfrac{\log n}{poly(\alpha)}$. We establish the viability of this approach and show that the famous $k$-center problem is $\alpha$-approximated when reducing to dimension $O(\tfrac{\log n}{\alpha^2}+\log k)$. Along the way, we address the diameter problem via the special case $k=1$. Our result extends to several important variants of $k$-center (with outliers, capacities, or fairness constraints), and the bound improves further with the input's doubling dimension.
  While our $poly(\alpha)$-factor improvement in the dimension may seem small, it actually has significant implications for streaming algorithms, and easily yields an algorithm for $k$-center in dynamic geometric streams, that achieves $O(\alpha)$-approximation using space $poly(kdn^{1/\alpha^2})$. This is the first algorithm to beat $O(n)$ space in high dimension $d$, as all previous algorithms require space at least $\exp(d)$. Furthermore, it extends to the $k$-center variants mentioned above.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01391v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.SoCG.2024.64</arxiv:DOI>
      <dc:creator>Shaofeng H. -C. Jiang, Robert Krauthgamer, Shay Sapir</dc:creator>
    </item>
    <item>
      <title>On the adversarial robustness of Locality-Sensitive Hashing in Hamming space</title>
      <link>https://arxiv.org/abs/2402.09707</link>
      <description>arXiv:2402.09707v3 Announce Type: replace 
Abstract: Locality-sensitive hashing~[Indyk,Motwani'98] is a classical data structure for approximate nearest neighbor search. It allows, after a close to linear time preprocessing of the input dataset, to find an approximately nearest neighbor of any fixed query in sublinear time in the dataset size. The resulting data structure is randomized and succeeds with high probability for every fixed query.
  In many modern applications of nearest neighbor search the queries are chosen adaptively. In this paper, we study the robustness of the locality-sensitive hashing to adaptive queries in Hamming space. We present a simple adversary that can, under mild assumptions on the initial point set, provably find a query to the approximate near neighbor search data structure that the data structure fails on. Crucially, our adaptive algorithm finds the hard query exponentially faster than random sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09707v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Kapralov, Mikhail Makarov, Christian Sohler</dc:creator>
    </item>
    <item>
      <title>Multiplicative Weights Update, Area Convexity and Random Coordinate Descent for Densest Subgraph Problems</title>
      <link>https://arxiv.org/abs/2405.18809</link>
      <description>arXiv:2405.18809v2 Announce Type: replace 
Abstract: We study the densest subgraph problem and give algorithms via multiplicative weights update and area convexity that converge in $O\left(\frac{\log m}{\epsilon^{2}}\right)$ and $O\left(\frac{\log m}{\epsilon}\right)$ iterations, respectively, both with nearly-linear time per iteration. Compared with the work by Bahmani et al. (2014), our MWU algorithm uses a very different and much simpler procedure for recovering the dense subgraph from the fractional solution and does not employ a binary search. Compared with the work by Boob et al. (2019), our algorithm via area convexity improves the iteration complexity by a factor $\Delta$ -- the maximum degree in the graph, and matches the fastest theoretical runtime currently known via flows (Chekuri et al., 2022) in total time. Next, we study the dense subgraph decomposition problem and give the first practical iterative algorithm with linear convergence rate $O\left(mn\log\frac{1}{\epsilon}\right)$ via accelerated random coordinate descent. This significantly improves over $O\left(\frac{m\sqrt{mn\Delta}}{\epsilon}\right)$ time of the FISTA-based algorithm by Harb et al. (2022). In the high precision regime $\epsilon\ll\frac{1}{n}$ where we can even recover the exact solution, our algorithm has a total runtime of $O\left(mn\log n\right)$, matching the exact algorithm via parametric flows (Gallo et al., 1989). Empirically, we show that this algorithm is very practical and scales to very large graphs, and its performance is competitive with widely used methods that have significantly weaker theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18809v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ta Duy Nguyen, Alina Ene</dc:creator>
    </item>
    <item>
      <title>A Lower Bound for Light Spanners in General Graphs</title>
      <link>https://arxiv.org/abs/2406.04459</link>
      <description>arXiv:2406.04459v2 Announce Type: replace 
Abstract: A recent upper bound by Le and Solomon [STOC '23] has established that every $n$-node graph has a $(1+\varepsilon)(2k-1)$-spanner with lightness $O(\varepsilon^{-1} n^{1/k})$. This bound is optimal up to its dependence on $\varepsilon$; the remaining open problem is whether this dependence can be improved or perhaps even removed entirely.
  We show that the $\varepsilon$-dependence cannot in fact be completely removed. For constant $k$ and for $\varepsilon:= \Theta(n^{-\frac{1}{2k-1}})$, we show a lower bound on lightness of $$\Omega\left( \varepsilon^{-1/k} n^{1/k} \right).$$ For example, this implies that there are graphs for which any $3$-spanner has lightness $\Omega(n^{2/3})$, improving on the previous lower bound of $\Omega(n^{1/2})$.
  An unusual feature of our lower bound is that it is conditional on the girth conjecture with parameter $k-1$ rather than $k$. We additionally show that this implies certain technical limitations to improving our lower bound further. In particular, under the same conditional, generalizing our lower bound to all $\varepsilon$ or obtaining an optimal $\varepsilon$-dependence are as hard as proving the girth conjecture for all constant $k$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04459v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Greg Bodwin, Jeremy Flics</dc:creator>
    </item>
    <item>
      <title>Rethink Tree Traversal</title>
      <link>https://arxiv.org/abs/2209.04825</link>
      <description>arXiv:2209.04825v5 Announce Type: replace-cross 
Abstract: We will show how to implement binary decision tree traversal in the language of matrix computation. Our main contribution is to propose some equivalent algorithms of binary tree traversal based on a novel matrix representation of the hierarchical structure of the decision tree. Our key idea is to travel the binary decision tree by maximum inner product search. We not only implement decision tree methods without the recursive traverse but also delve into the partitioning nature of tree-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04825v5</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinxiong Zhang</dc:creator>
    </item>
    <item>
      <title>Scalable Algorithm for Finding Balanced Subgraphs with Tolerance in Signed Networks</title>
      <link>https://arxiv.org/abs/2402.05006</link>
      <description>arXiv:2402.05006v2 Announce Type: replace-cross 
Abstract: Signed networks, characterized by edges labeled as either positive or negative, offer nuanced insights into interaction dynamics beyond the capabilities of unsigned graphs. Central to this is the task of identifying the maximum balanced subgraph, crucial for applications like polarized community detection in social networks and portfolio analysis in finance. Traditional models, however, are limited by an assumption of perfect partitioning, which fails to mirror the complexities of real-world data. Addressing this gap, we introduce an innovative generalized balanced subgraph model that incorporates tolerance for irregularities. Our proposed region-based heuristic algorithm, tailored for this NP-hard problem, strikes a balance between low time complexity and high-quality outcomes. Comparative experiments validate its superior performance against leading solutions, delivering enhanced effectiveness (notably larger subgraph sizes) and efficiency (achieving up to 100x speedup) in both traditional and generalized contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05006v2</guid>
      <category>cs.SI</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingbang Chen, Qiuyang Mang, Hangrui Zhou, Richard Peng, Yu Gao, Chenhao Ma</dc:creator>
    </item>
  </channel>
</rss>
