<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Oct 2025 01:45:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment</title>
      <link>https://arxiv.org/abs/2510.05705</link>
      <description>arXiv:2510.05705v1 Announce Type: cross 
Abstract: In the ever-changing realm of research software development, it is crucial for the scientific community to grasp current trends to identify gaps that can potentially hinder scientific progress. The adherence to the FAIR (Findable, Accessible, Interoperable, Reusable) principles can serve as a proxy to understand those trends and provide a mechanism to propose specific actions.
  The Software Observatory at OpenEBench (https://openebench.bsc.es/observatory) is a novel web portal that consolidates software metadata from various sources, offering comprehensive insights into critical research software aspects. Our platform enables users to analyse trends, identify patterns and advancements within the Life Sciences research software ecosystem, and understand its evolution over time. It also evaluates research software according to FAIR principles for research software, providing scores for different indicators.
  Users have the ability to visualise this metadata at different levels of granularity, ranging from the entire software landscape to specific communities to individual software entries through the FAIRsoft Evaluator. Indeed, the FAIRsoft Evaluator component streamlines the assessment process, helping developers efficiently evaluate and obtain guidance to improve their software's FAIRness.
  The Software Observatory represents a valuable resource for researchers and software developers, as well as stakeholders, promoting better software development practices and adherence to FAIR principles for research software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05705v1</guid>
      <category>cs.SE</category>
      <category>cs.DL</category>
      <category>q-bio.OT</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eva Mart\'in del Pico, Josep Llu\'is Gelp\'i, Salvador Capella-Guti\'errez</dc:creator>
    </item>
    <item>
      <title>$\Phi$ index: A standardized scale-independent and field-normalized citation indicator</title>
      <link>https://arxiv.org/abs/2401.00997</link>
      <description>arXiv:2401.00997v3 Announce Type: replace 
Abstract: The Impact Factor (IF), despite its widespread use, suffers from well-known biases, most notably its sensitivity to journal size and its lack of field normalization. As a consequence of size sensitivity, a randomly formed journal of $n$ papers can attain a range of IF values that decreases sharply with size, as $\sim 1/\sqrt{n}$. The Central Limit Theorem, which underlies this effect, also allows us to correct for it by standardizing citation averages for scale and field in an elegant manner analogous to calculating the $z$-score in statistics. We thus introduce the $\Phi$ (Phi) index, defined as $\Phi = (f-\mu) \sqrt{n}/\sigma$, where $f$ is a journal's average citation count (akin to the IF), $n$ is the journal's publication count, and $\mu, \sigma$ represent the mean and standard deviation of citations in the journal's field. This formulation incorporates disparities in journal size and field citation practices. Applying the $\Phi$ index to a broad set of journals, we find that it produces rankings that better align with expert community perception of journal prestige, while boosting high-performing journals from diverse and less-cited fields. The $\Phi$ index thus offers a principled, scale- and field-standardized alternative to current metrics, with direct implications for research evaluation and publishing policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00997v3</guid>
      <category>cs.DL</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Manolis Antonoyiannakis</dc:creator>
    </item>
    <item>
      <title>Cosmos 1.0: a multidimensional map of the emerging technology frontier</title>
      <link>https://arxiv.org/abs/2505.10591</link>
      <description>arXiv:2505.10591v2 Announce Type: replace-cross 
Abstract: This paper introduces the Cosmos 1.0 dataset and describes a novel methodology for creating and mapping a universe of technologies, adjacent concepts, and entities. We utilise various source data that contain a rich diversity and breadth of contemporary knowledge. The Cosmos 1.0 dataset comprises 23,544 technology-adjacent entities (TA23k) with a hierarchical structure and eight categories of external indices. Each entity is represented by a 100-dimensional contextual embedding vector, which we use to assign it to seven thematic tech-clusters (TC7) and three meta tech-clusters (TC3). We manually verify 100 emerging technologies (ET100). This dataset is enriched with additional indices specifically developed to assess the landscape of emerging technologies, including the Technology Awareness Index, Generality Index, Deeptech, and Age of Tech Index. The dataset incorporates extensive metadata sourced from Wikipedia and linked data from third-party sources such as Crunchbase, Google Books, OpenAlex and Google Scholar, which are used to validate the relevance and accuracy of the constructed indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10591v2</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xian Gong, Paul X. McCarthy, Colin Griffith, Claire McFarland, Marian-Andrei Rizoiu</dc:creator>
    </item>
    <item>
      <title>Language Models Surface the Unwritten Code of Science and Society</title>
      <link>https://arxiv.org/abs/2505.18942</link>
      <description>arXiv:2505.18942v3 Announce Type: replace-cross 
Abstract: This paper calls on the research community not only to investigate how human biases are inherited by large language models (LLMs) but also to explore how these biases in LLMs can be leveraged to make society's "unwritten code" - such as implicit stereotypes and heuristics - visible and accessible for critique. We introduce a conceptual framework through a case study in science: uncovering hidden rules in peer review - the factors that reviewers care about but rarely state explicitly due to normative scientific expectations. The idea of the framework is to push LLMs to speak out their heuristics through generating self-consistent hypotheses - why one paper appeared stronger in reviewer scoring - among paired papers submitted to 45 academic conferences, while iteratively searching deeper hypotheses from remaining pairs where existing hypotheses cannot explain. We observed that LLMs' normative priors about the internal characteristics of good science extracted from their self-talk, e.g., theoretical rigor, were systematically updated toward posteriors that emphasize storytelling about external connections, such as how the work is positioned and connected within and across literatures. Human reviewers tend to explicitly reward aspects that moderately align with LLMs' normative priors (correlation = 0.49) but avoid articulating contextualization and storytelling posteriors in their review comments (correlation = -0.14), despite giving implicit reward to them with positive scores. These patterns are robust across different models and out-of-sample judgments. We discuss the broad applicability of our proposed framework, leveraging LLMs as diagnostic tools to amplify and surface the tacit codes underlying human society, enabling public discussion of revealed values and more precisely targeted responsible AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18942v3</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Honglin Bao, Siyang Wu, Jiwoong Choi, Yingrong Mao, James A. Evans</dc:creator>
    </item>
    <item>
      <title>A bibliometric study on mathematical oncology: interdisciplinarity, internationality, collaboration and trending topics</title>
      <link>https://arxiv.org/abs/2506.20684</link>
      <description>arXiv:2506.20684v2 Announce Type: replace-cross 
Abstract: Mathematical oncology is an interdisciplinary research field where the mathematical sciences meet cancer research. Being situated at the intersection of these two fields makes mathematical oncology highly dynamic, as practicing researchers are incentivised to quickly adapt to both technical and medical research advances. Determining the scope of mathematical oncology is therefore not straightforward; however, it is important for purposes related to funding allocation, education, scientific communication, and community organisation. To address this issue, we here conduct a bibliometric analysis of mathematical oncology. We compare our results to the broader field of mathematical biology, and position our findings within theoretical science of science frameworks.
  Based on article metadata and citation flows, our results provide evidence that mathematical oncology has undergone a significant evolution since the 1960s marked by increased interactions with other disciplines, geographical expansion, larger research teams, and greater diversity in studied topics. The latter finding contributes to the greater discussion on which models different research communities consider to be valuable in the era of big data and machine learning. Further, the results presented in this study quantitatively motivate that international collaboration networks should be supported to enable new countries to enter and remain in the field, and that mathematical oncology benefits both mathematics and the life sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20684v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.DL</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kira Pugh, Linn\'ea Gyllingberg, Stanislav Stratiev, Sara Hamis</dc:creator>
    </item>
  </channel>
</rss>
