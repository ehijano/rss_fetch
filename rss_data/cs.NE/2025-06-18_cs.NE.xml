<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Jun 2025 04:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Extending Spike-Timing Dependent Plasticity to Learning Synaptic Delays</title>
      <link>https://arxiv.org/abs/2506.14984</link>
      <description>arXiv:2506.14984v1 Announce Type: new 
Abstract: Synaptic delays play a crucial role in biological neuronal networks, where their modulation has been observed in mammalian learning processes. In the realm of neuromorphic computing, although spiking neural networks (SNNs) aim to emulate biology more closely than traditional artificial neural networks do, synaptic delays are rarely incorporated into their simulation. We introduce a novel learning rule for simultaneously learning synaptic connection strengths and delays, by extending spike-timing dependent plasticity (STDP), a Hebbian method commonly used for learning synaptic weights. We validate our approach by extending a widely-used SNN model for classification trained with unsupervised learning. Then we demonstrate the effectiveness of our new method by comparing it against another existing methods for co-learning synaptic weights and delays as well as against STDP without synaptic delays. Results demonstrate that our proposed method consistently achieves superior performance across a variety of test scenarios. Furthermore, our experimental results yield insight into the interplay between synaptic efficacy and delay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14984v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marissa Dominijanni, Alexander Ororbia, Kenneth W. Regan</dc:creator>
    </item>
    <item>
      <title>The Pitfalls and Potentials of Adding Gene-invariance to Optimal Mixing</title>
      <link>https://arxiv.org/abs/2506.15222</link>
      <description>arXiv:2506.15222v1 Announce Type: new 
Abstract: Optimal Mixing (OM) is a variation operator that integrates local search with genetic recombination. EAs with OM are capable of state-of-the-art optimization in discrete spaces, offering significant advantages over classic recombination-based EAs. This success is partly due to high selection pressure that drives rapid convergence. However, this can also negatively impact population diversity, complicating the solving of hierarchical problems, which feature multiple layers of complexity. While there have been attempts to address this issue, these solutions are often complicated and prone to bias. To overcome this, we propose a solution inspired by the Gene Invariant Genetic Algorithm (GIGA), which preserves gene frequencies in the population throughout the process. This technique is tailored to and integrated with the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA), resulting in GI-GOMEA. The simple, yet elegant changes are found to have striking potential: GI-GOMEA outperforms GOMEA on a range of well-known problems, even when these problems are adjusted for pitfalls - biases in much-used benchmark problems that can be easily exploited by maintaining gene invariance. Perhaps even more notably, GI-GOMEA is also found to be effective at solving hierarchical problems, including newly introduced asymmetric hierarchical trap functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15222v1</guid>
      <category>cs.NE</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3712255.3734293</arxiv:DOI>
      <dc:creator>Anton Bouter, Dirk Thierens, Peter A. N. Bosman</dc:creator>
    </item>
    <item>
      <title>Estimate Hitting Time by Hitting Probability for Elitist Evolutionary Algorithms</title>
      <link>https://arxiv.org/abs/2506.15602</link>
      <description>arXiv:2506.15602v1 Announce Type: new 
Abstract: Drift analysis is a powerful tool for analyzing the time complexity of evolutionary algorithms. However, it requires manual construction of drift functions to bound hitting time for each specific algorithm and problem. To address this limitation, general linear drift functions were introduced for elitist evolutionary algorithms. But calculating linear bound coefficients effectively remains a problem. This paper proposes a new method called drift analysis of hitting probability to compute these coefficients. Each coefficient is interpreted as a bound on the hitting probability of a fitness level, transforming the task of estimating hitting time into estimating hitting probability. A novel drift analysis method is then developed to estimate hitting probability, where paths are introduced to handle multimodal fitness landscapes. Explicit expressions are constructed to compute hitting probability, significantly simplifying the estimation process. One advantage of the proposed method is its ability to estimate both the lower and upper bounds of hitting time and to compare the performance of two algorithms in terms of hitting time. To demonstrate this application, two algorithms for the knapsack problem, each incorporating feasibility rules and greedy repair respectively, are compared. The analysis indicates that neither constraint handling technique consistently outperforms the other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15602v1</guid>
      <category>cs.NE</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun He, Siang Yew Chong, Xin Yao</dc:creator>
    </item>
    <item>
      <title>Flat Channels to Infinity in Neural Loss Landscapes</title>
      <link>https://arxiv.org/abs/2506.14951</link>
      <description>arXiv:2506.14951v1 Announce Type: cross 
Abstract: The loss landscapes of neural networks contain minima and saddle points that may be connected in flat regions or appear in isolation. We identify and characterize a special structure in the loss landscape: channels along which the loss decreases extremely slowly, while the output weights of at least two neurons, $a_i$ and $a_j$, diverge to $\pm$infinity, and their input weight vectors, $\mathbf{w_i}$ and $\mathbf{w_j}$, become equal to each other. At convergence, the two neurons implement a gated linear unit: $a_i\sigma(\mathbf{w_i} \cdot \mathbf{x}) + a_j\sigma(\mathbf{w_j} \cdot \mathbf{x}) \rightarrow \sigma(\mathbf{w} \cdot \mathbf{x}) + (\mathbf{v} \cdot \mathbf{x}) \sigma'(\mathbf{w} \cdot \mathbf{x})$. Geometrically, these channels to infinity are asymptotically parallel to symmetry-induced lines of critical points. Gradient flow solvers, and related optimization methods like SGD or ADAM, reach the channels with high probability in diverse regression settings, but without careful inspection they look like flat local minima with finite parameter values. Our characterization provides a comprehensive picture of these quasi-flat regions in terms of gradient dynamics, geometry, and functional interpretation. The emergence of gated linear units at the end of the channels highlights a surprising aspect of the computational capabilities of fully connected layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14951v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flavio Martinelli, Alexander Van Meegen, Berfin \c{S}im\c{s}ek, Wulfram Gerstner, Johanni Brea</dc:creator>
    </item>
    <item>
      <title>HiPreNets: High-Precision Neural Networks through Progressive Training</title>
      <link>https://arxiv.org/abs/2506.15064</link>
      <description>arXiv:2506.15064v1 Announce Type: cross 
Abstract: Deep neural networks are powerful tools for solving nonlinear problems in science and engineering, but training highly accurate models becomes challenging as problem complexity increases. Non-convex optimization and numerous hyperparameters to tune make performance improvement difficult, and traditional approaches often prioritize minimizing mean squared error (MSE) while overlooking $L^{\infty}$ error, which is the critical focus in many applications. To address these challenges, we present a progressive framework for training and tuning high-precision neural networks (HiPreNets). Our approach refines a previously explored staged training technique for neural networks that improves an existing fully connected neural network by sequentially learning its prediction residuals using additional networks, leading to improved overall accuracy. We discuss how to take advantage of the structure of the residuals to guide the choice of loss function, number of parameters to use, and ways to introduce adaptive data sampling techniques. We validate our framework's effectiveness through several benchmark problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15064v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.NE</category>
      <category>math.NA</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ethan Mulle, Wei Kang, Qi Gong</dc:creator>
    </item>
    <item>
      <title>Generative thermodynamic computing</title>
      <link>https://arxiv.org/abs/2506.15121</link>
      <description>arXiv:2506.15121v1 Announce Type: cross 
Abstract: We introduce a generative modeling framework for thermodynamic computing, in which structured data is synthesized from noise by the natural time evolution of a physical system governed by Langevin dynamics. While conventional diffusion models use neural networks to perform denoising, here the information needed to generate structure from noise is encoded by the dynamics of a thermodynamic system. Training proceeds by maximizing the probability with which the computer generates the reverse of a noising trajectory, which ensures that the computer generates data with minimal heat emission. We demonstrate this framework within a digital simulation of a thermodynamic computer. If realized in analog hardware, such a system would function as a generative model that produces structured samples without the need for artificially-injected noise or active control of denoising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15121v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.NE</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephen Whitelam</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap Between Approximation and Learning via Optimal Approximation by ReLU MLPs of Maximal Regularity</title>
      <link>https://arxiv.org/abs/2409.12335</link>
      <description>arXiv:2409.12335v2 Announce Type: replace-cross 
Abstract: The foundations of deep learning are supported by the seemingly opposing perspectives of approximation or learning theory. The former advocates for large/expressive models that need not generalize, while the latter considers classes that generalize but may be too small/constrained to be universal approximators. Motivated by real-world deep learning implementations that are both expressive and statistically reliable, we ask: "Is there a class of neural networks that is both large enough to be universal but structured enough to generalize?" This paper constructively provides a positive answer to this question by identifying a highly structured class of ReLU multilayer perceptions (MLPs), which are optimal function approximators and are statistically well-behaved. We show that any $(L,\alpha)$-H\"{o}lder function from $[0,1]^d$ to $[-n,n]$ can be approximated to a uniform $\mathcal{O}(1/n)$ error on $[0,1]^d$ with a sparsely connected ReLU MLP with the same H\"{o}lder exponent $\alpha$ and coefficient $L$, of width $\mathcal{O}(dn^{d/\alpha})$, depth $\mathcal{O}(\log(d))$, with $\mathcal{O}(dn^{d/\alpha})$ nonzero parameters, and whose weights and biases take values in $\{0,\pm 1/2\}$ except in the first and last layers which instead have magnitude at-most $n$. Further, our class of MLPs achieves a near-optimal sample complexity of $\mathcal{O}(\log(N)/\sqrt{N})$ when given $N$ i.i.d. normalized sub-Gaussian training samples. We achieve this by fitting together linear pieces perfectly via the Kuhn triangulation, and we introduce a new proof technique which shows that our construction preserves the regularity of not only the H\"{o}lder functions, but also any uniformly continuous function. Our results imply that neural networks can solve the McShane extension problem on suitable finite sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12335v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.NE</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruiyang Hong, Anastasis Kratsios</dc:creator>
    </item>
    <item>
      <title>Oscillatory State-Space Models</title>
      <link>https://arxiv.org/abs/2410.03943</link>
      <description>arXiv:2410.03943v3 Announce Type: replace-cross 
Abstract: We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast associative parallel scans, yields the proposed state-space model. We prove that LinOSS produces stable dynamics only requiring nonnegative diagonal state matrix. This is in stark contrast to many previous state-space models relying heavily on restrictive parameterizations. Moreover, we rigorously show that LinOSS is universal, i.e., it can approximate any continuous and causal operator mapping between time-varying functions, to desired accuracy. In addition, we show that an implicit-explicit discretization of LinOSS perfectly conserves the symmetry of time reversibility of the underlying dynamics. Together, these properties enable efficient modeling of long-range interactions, while ensuring stable and accurate long-horizon forecasting. Finally, our empirical results, spanning a wide range of time-series tasks from mid-range to very long-range classification and regression, as well as long-horizon forecasting, demonstrate that our proposed LinOSS model consistently outperforms state-of-the-art sequence models. Notably, LinOSS outperforms Mamba and LRU by nearly 2x on a sequence modeling task with sequences of length 50k.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03943v3</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>T. Konstantin Rusch, Daniela Rus</dc:creator>
    </item>
    <item>
      <title>Towards net-zero manufacturing: carbon-aware scheduling for GHG emissions reduction</title>
      <link>https://arxiv.org/abs/2503.01325</link>
      <description>arXiv:2503.01325v2 Announce Type: replace-cross 
Abstract: Detailed scheduling has traditionally been optimized for the reduction of makespan and manufacturing costs. However, growing awareness of environmental concerns and increasingly stringent regulations are pushing manufacturing towards reducing the carbon footprint of its operations. Scope 2 emissions, which are the indirect emissions related to the production and consumption of grid electricity, are in fact estimated to be responsible for more than one-third of the global GHG emissions. In this context, carbon-aware scheduling can serve as a powerful way to reduce manufacturing's carbon footprint by considering the time-dependent carbon intensity of the grid and the availability of on-site renewable electricity.
  This study introduces a carbon-aware permutation flow-shop scheduling model designed to reduce scope 2 emissions. The model is formulated as a mixed-integer linear problem, taking into account the forecasted grid generation mix and available on-site renewable electricity, along with the set of jobs to be scheduled and their corresponding power requirements. The objective is to find an optimal day-ahead schedule that minimizes scope 2 emissions. The problem is addressed using a dedicated memetic algorithm, combining evolutionary strategy and local search.
  Results from computational experiments confirm that by considering the dynamic carbon intensity of the grid and on-site renewable electricity availability, substantial reductions in carbon emissions can be achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01325v2</guid>
      <category>math.OC</category>
      <category>cs.NE</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrea Mencaroni, Pieter Leyman, Birger Raa, Stijn De Vuyst, Dieter Claeys</dc:creator>
    </item>
    <item>
      <title>A Bird Song Detector for improving bird identification through Deep Learning: a case study from Do\~nana</title>
      <link>https://arxiv.org/abs/2503.15576</link>
      <description>arXiv:2503.15576v2 Announce Type: replace-cross 
Abstract: Passive Acoustic Monitoring is a key tool for biodiversity conservation, but the large volumes of unsupervised audio it generates present major challenges for extracting meaningful information. Deep Learning offers promising solutions. BirdNET, a widely used bird identification model, has shown success in many study systems but is limited at local scale due to biases in its training data, which focus on specific locations and target sounds rather than entire soundscapes. A key challenge in bird species identification is that many recordings either lack target species or contain overlapping vocalizations, complicating automatic identification. To address these problems, we developed a multi-stage pipeline for automatic bird vocalization identification in Do\~nana National Park (SW Spain), a wetland of high conservation concern. We deployed AudioMoth recorders in three main habitats across nine locations and manually annotated 461 minutes of audio, resulting in 3749 labeled segments spanning 34 classes. We first applied a Bird Song Detector to isolate bird vocalizations using spectrogram-based image processing. Then, species were classified using custom models trained at the local scale. Applying the Bird Song Detector before classification improved species identification, as all models performed better when analyzing only the segments where birds were detected. Specifically, the combination of detector and fine-tuned BirdNET outperformed the baseline without detection. This approach demonstrates the effectiveness of integrating a Bird Song Detector with local classification models. These findings highlight the need to adapt general-purpose tools to specific ecological challenges. Automatically detecting bird species helps track the health of this threatened ecosystem, given birds sensitivity to environmental change, and supports conservation planning to reduce biodiversity loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15576v2</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alba M\'arquez-Rodr\'iguez, Miguel \'Angel Mohedano-Munoz, Manuel J. Mar\'in-Jim\'enez, Eduardo Santamar\'ia-Garc\'ia, Giulia Bastianelli, Pedro Jordano, Irene Mendoza</dc:creator>
    </item>
  </channel>
</rss>
